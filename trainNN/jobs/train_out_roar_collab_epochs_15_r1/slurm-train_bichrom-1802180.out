Training seq
DEVICE = cpu
####################
Total Parameters = 605185
Total Trainable Parameters = 605185
bichrom_seq(
  (conv1d): Conv1d(4, 256, kernel_size=(24,), stride=(1,))
  (relu): ReLU()
  (batchNorm1d): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (maxPool1d): MaxPool1d(kernel_size=15, stride=15, padding=0, dilation=1, ceil_mode=True)
  (lstm): LSTM(256, 32, batch_first=True)
  (tanh): Tanh()
  (model_dense_repeat): Sequential(
    (0): Linear(in_features=32, out_features=512, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.5, inplace=False)
    (3): Linear(in_features=512, out_features=512, bias=True)
    (4): ReLU()
    (5): Dropout(p=0.5, inplace=False)
    (6): Linear(in_features=512, out_features=512, bias=True)
    (7): ReLU()
    (8): Dropout(p=0.5, inplace=False)
  )
  (linear): Linear(in_features=512, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)
####################
EPOCH 1:
  batch 1 loss: 0.6923376321792603
  batch 2 loss: 0.6935523748397827
  batch 3 loss: 0.6944925785064697
  batch 4 loss: 0.6945041716098785
  batch 5 loss: 0.6940896034240722
  batch 6 loss: 0.693725456794103
  batch 7 loss: 0.6939782585416522
  batch 8 loss: 0.694103479385376
  batch 9 loss: 0.6935161881976657
  batch 10 loss: 0.6927070260047913
  batch 11 loss: 0.6921663771976124
  batch 12 loss: 0.6914159804582596
  batch 13 loss: 0.6908458379598764
  batch 14 loss: 0.689667386668069
  batch 15 loss: 0.6884486277898153
  batch 16 loss: 0.6877175867557526
  batch 17 loss: 0.6866646654465619
  batch 18 loss: 0.6856826345125834
  batch 19 loss: 0.6855314028890509
  batch 20 loss: 0.6842699766159057
  batch 21 loss: 0.6835571981611706
  batch 22 loss: 0.6828535172072324
  batch 23 loss: 0.6835532421651094
  batch 24 loss: 0.6827441826462746
  batch 25 loss: 0.6826423048973084
  batch 26 loss: 0.6813067197799683
  batch 27 loss: 0.6803189405688533
  batch 28 loss: 0.6790402595485959
  batch 29 loss: 0.6793794796384615
  batch 30 loss: 0.6783743898073832
  batch 31 loss: 0.6777133941650391
  batch 32 loss: 0.6774962674826384
  batch 33 loss: 0.6764925953113672
  batch 34 loss: 0.6759781714747933
  batch 35 loss: 0.6758232406207494
  batch 36 loss: 0.6748957749870088
  batch 37 loss: 0.6741368335646551
  batch 38 loss: 0.6738950249395872
  batch 39 loss: 0.6728932628264794
  batch 40 loss: 0.6718999534845352
  batch 41 loss: 0.6711378431901699
  batch 42 loss: 0.6714104541710445
  batch 43 loss: 0.671173962049706
  batch 44 loss: 0.670841399918903
  batch 45 loss: 0.6706737849447463
  batch 46 loss: 0.6700577165769495
  batch 47 loss: 0.6704966159577065
  batch 48 loss: 0.6702076370517412
  batch 49 loss: 0.6697972173593483
  batch 50 loss: 0.669578286409378
  batch 51 loss: 0.669309702574038
  batch 52 loss: 0.669028626038478
  batch 53 loss: 0.6689307543466676
  batch 54 loss: 0.6683770351939731
  batch 55 loss: 0.6678138884631071
  batch 56 loss: 0.6670468556029456
  batch 57 loss: 0.6670366211941368
  batch 58 loss: 0.6669934299485437
  batch 59 loss: 0.666081913446976
  batch 60 loss: 0.6657869001229604
  batch 61 loss: 0.6652705776886861
  batch 62 loss: 0.6651399375930909
  batch 63 loss: 0.6642055634468321
  batch 64 loss: 0.6634656963869929
  batch 65 loss: 0.662635941688831
  batch 66 loss: 0.6622083990862875
  batch 67 loss: 0.6616430834158143
  batch 68 loss: 0.6611081466955298
  batch 69 loss: 0.6606627184411754
  batch 70 loss: 0.6598878834928785
  batch 71 loss: 0.6590857959129441
  batch 72 loss: 0.6584812055031458
  batch 73 loss: 0.6577752599977467
  batch 74 loss: 0.6569597463350039
  batch 75 loss: 0.6563497138023376
  batch 76 loss: 0.65583026330722
  batch 77 loss: 0.6552277222856299
  batch 78 loss: 0.6542758215696384
  batch 79 loss: 0.6535036624232425
  batch 80 loss: 0.6527156174182892
  batch 81 loss: 0.6517401278754811
  batch 82 loss: 0.6515950189857949
  batch 83 loss: 0.6510684253221535
  batch 84 loss: 0.6508315986111051
  batch 85 loss: 0.6498082539614509
  batch 86 loss: 0.6498013596202052
  batch 87 loss: 0.6492242080041732
  batch 88 loss: 0.6484101015058431
  batch 89 loss: 0.6475861065843133
  batch 90 loss: 0.6467784689532385
  batch 91 loss: 0.646035081082648
  batch 92 loss: 0.6453186778918557
  batch 93 loss: 0.6447765340087235
  batch 94 loss: 0.6439391989657219
  batch 95 loss: 0.6433808960412678
  batch 96 loss: 0.6425246937821308
  batch 97 loss: 0.6419287741798716
  batch 98 loss: 0.6410881889109709
  batch 99 loss: 0.6404028552951235
  batch 100 loss: 0.6398678195476531
  batch 101 loss: 0.6391599390766408
  batch 102 loss: 0.6384220719337463
  batch 103 loss: 0.6379907646225494
  batch 104 loss: 0.636988259278811
  batch 105 loss: 0.6361438393592834
  batch 106 loss: 0.6354305682317266
  batch 107 loss: 0.6342681997290281
  batch 108 loss: 0.6333648137472294
  batch 109 loss: 0.632646139061779
  batch 110 loss: 0.6323368923230605
  batch 111 loss: 0.6317605923961949
  batch 112 loss: 0.6308186831218856
  batch 113 loss: 0.6301817340133464
  batch 114 loss: 0.629547854787425
  batch 115 loss: 0.6287433162979458
  batch 116 loss: 0.6279158155465948
  batch 117 loss: 0.6269443447773273
  batch 118 loss: 0.6260799957534015
  batch 119 loss: 0.6250776393072945
  batch 120 loss: 0.6241668124993642
  batch 121 loss: 0.6230923479253595
  batch 122 loss: 0.621973310337692
  batch 123 loss: 0.6208798454059818
  batch 124 loss: 0.620604298768505
  batch 125 loss: 0.6195839338302612
  batch 126 loss: 0.6185834511877999
  batch 127 loss: 0.6180262152604231
  batch 128 loss: 0.6170863704755902
  batch 129 loss: 0.6162280232407326
  batch 130 loss: 0.6154649995840513
  batch 131 loss: 0.6146800076688519
  batch 132 loss: 0.6138516876733664
  batch 133 loss: 0.6132762579989612
  batch 134 loss: 0.6123167645575395
  batch 135 loss: 0.611582424905565
  batch 136 loss: 0.6108550622182733
  batch 137 loss: 0.6101459625863681
  batch 138 loss: 0.6094507328841997
  batch 139 loss: 0.6088813822046458
  batch 140 loss: 0.6081019092883383
  batch 141 loss: 0.6073888343699435
  batch 142 loss: 0.606425341585992
  batch 143 loss: 0.6054219045005478
  batch 144 loss: 0.6044613414754471
  batch 145 loss: 0.6036210481462807
  batch 146 loss: 0.6028799769813067
  batch 147 loss: 0.6024373802198034
  batch 148 loss: 0.6018755822568327
  batch 149 loss: 0.6012102533506867
  batch 150 loss: 0.6003883306185405
  batch 151 loss: 0.5996491107719624
  batch 152 loss: 0.5989241943155464
  batch 153 loss: 0.5980356263568978
  batch 154 loss: 0.5973542603579435
  batch 155 loss: 0.5965905558678412
  batch 156 loss: 0.5959078499521965
  batch 157 loss: 0.5953742590299838
  batch 158 loss: 0.5946274565745003
  batch 159 loss: 0.5938395389221
  batch 160 loss: 0.5930437536910176
  batch 161 loss: 0.5922887064285159
  batch 162 loss: 0.5913865398845555
  batch 163 loss: 0.5905154202979035
  batch 164 loss: 0.5897931807288309
  batch 165 loss: 0.5888895244309397
  batch 166 loss: 0.588135781237878
  batch 167 loss: 0.5872393703746225
  batch 168 loss: 0.5866398014837787
  batch 169 loss: 0.5857589064970524
  batch 170 loss: 0.5850623071193695
  batch 171 loss: 0.5843243790648834
  batch 172 loss: 0.583497607777285
  batch 173 loss: 0.5827974826958827
  batch 174 loss: 0.5821240655649668
  batch 175 loss: 0.5813848471641541
  batch 176 loss: 0.5803356561809778
  batch 177 loss: 0.5794197942240763
  batch 178 loss: 0.5787066834026509
  batch 179 loss: 0.5779253655638774
  batch 180 loss: 0.5771885724531279
  batch 181 loss: 0.5766657623796831
  batch 182 loss: 0.5757092375021714
  batch 183 loss: 0.5749373979907219
  batch 184 loss: 0.5740968314525874
  batch 185 loss: 0.573363689635251
  batch 186 loss: 0.5727502923178417
  batch 187 loss: 0.5720918502080887
  batch 188 loss: 0.5713133404546595
  batch 189 loss: 0.5705192122194502
  batch 190 loss: 0.5696269728635487
  batch 191 loss: 0.5688491663383564
  batch 192 loss: 0.5681779252675673
  batch 193 loss: 0.5674401830515096
  batch 194 loss: 0.5668901294162593
  batch 195 loss: 0.5665472321021251
  batch 196 loss: 0.5656954922846386
  batch 197 loss: 0.5649247792771626
  batch 198 loss: 0.5643472242535967
  batch 199 loss: 0.5638024076145498
  batch 200 loss: 0.5631337805092335
  batch 201 loss: 0.5624676681869659
  batch 202 loss: 0.5618172556456953
  batch 203 loss: 0.5611491885972141
  batch 204 loss: 0.560404025894754
  batch 205 loss: 0.5598089888328459
  batch 206 loss: 0.5592623124423536
  batch 207 loss: 0.5585468411445618
  batch 208 loss: 0.5577330314196073
  batch 209 loss: 0.5569171402157779
  batch 210 loss: 0.5563000119867779
  batch 211 loss: 0.5557178020194808
  batch 212 loss: 0.5550582914982202
  batch 213 loss: 0.5544567266260514
  batch 214 loss: 0.5537179914590354
  batch 215 loss: 0.5528412673362466
  batch 216 loss: 0.5520343391431702
  batch 217 loss: 0.5514241830544537
  batch 218 loss: 0.5508346478326605
  batch 219 loss: 0.5502383251712747
  batch 220 loss: 0.5495242942463268
  batch 221 loss: 0.5489356493518364
  batch 222 loss: 0.5484383479431942
  batch 223 loss: 0.547858111526934
  batch 224 loss: 0.5472453700910721
  batch 225 loss: 0.5464692621760898
  batch 226 loss: 0.5459234064395448
  batch 227 loss: 0.5452627445107515
  batch 228 loss: 0.544781083469851
  batch 229 loss: 0.5441465288010227
  batch 230 loss: 0.5437532325153766
  batch 231 loss: 0.5431917282668027
  batch 232 loss: 0.5423073710809494
  batch 233 loss: 0.541720373691919
  batch 234 loss: 0.5413793398019595
  batch 235 loss: 0.5408923906214693
  batch 236 loss: 0.5402920847221956
  batch 237 loss: 0.5397763427048293
  batch 238 loss: 0.5391618000859973
  batch 239 loss: 0.5384566290109227
  batch 240 loss: 0.537994035333395
  batch 241 loss: 0.5376174336152453
  batch 242 loss: 0.5369624196251562
  batch 243 loss: 0.5364768196035314
  batch 244 loss: 0.53609456757053
  batch 245 loss: 0.5356108710473898
  batch 246 loss: 0.5350465872665731
  batch 247 loss: 0.5346276398129791
  batch 248 loss: 0.5342523152789762
  batch 249 loss: 0.5336718520964964
  batch 250 loss: 0.5331421685218811
  batch 251 loss: 0.5326201032357387
  batch 252 loss: 0.5320285384853681
  batch 253 loss: 0.5313558916564987
  batch 254 loss: 0.530764086978642
  batch 255 loss: 0.5302968107017816
  batch 256 loss: 0.5298124075634405
  batch 257 loss: 0.529283935805703
  batch 258 loss: 0.5287080196909202
  batch 259 loss: 0.528312335258285
  batch 260 loss: 0.5277368403398074
  batch 261 loss: 0.527235343081741
  batch 262 loss: 0.5265492926572115
  batch 263 loss: 0.5261117911157499
  batch 264 loss: 0.5255560062148354
  batch 265 loss: 0.5251428715462955
  batch 266 loss: 0.5247164691301217
  batch 267 loss: 0.5244423236963007
  batch 268 loss: 0.5238416899940861
  batch 269 loss: 0.5233468371253032
  batch 270 loss: 0.5228976309299469
  batch 271 loss: 0.522451856699377
  batch 272 loss: 0.522036787122488
  batch 273 loss: 0.5215574994192018
  batch 274 loss: 0.5211287797802556
  batch 275 loss: 0.5207246133414182
  batch 276 loss: 0.5202358248441116
  batch 277 loss: 0.5197372342920475
  batch 278 loss: 0.5191841495337246
  batch 279 loss: 0.5188294737783384
  batch 280 loss: 0.5182541746113981
  batch 281 loss: 0.5177847632733953
  batch 282 loss: 0.517372190952301
  batch 283 loss: 0.5167668603966177
  batch 284 loss: 0.5165580803869476
  batch 285 loss: 0.516114459226006
  batch 286 loss: 0.5157134655889097
  batch 287 loss: 0.5152360034736607
  batch 288 loss: 0.5146722572131289
  batch 289 loss: 0.5144379850489871
  batch 290 loss: 0.5137597519775917
  batch 291 loss: 0.5133457246310112
  batch 292 loss: 0.5129399566005354
  batch 293 loss: 0.5124934861887844
  batch 294 loss: 0.5120662099041906
  batch 295 loss: 0.5116191466986123
  batch 296 loss: 0.5112513823283685
  batch 297 loss: 0.5107790916054337
  batch 298 loss: 0.5103590965471012
  batch 299 loss: 0.5098959169658929
  batch 300 loss: 0.5095119342207909
  batch 301 loss: 0.508988754990885
  batch 302 loss: 0.5086601676530396
  batch 303 loss: 0.5083350971980457
  batch 304 loss: 0.5079113687143514
  batch 305 loss: 0.5073503908563832
  batch 306 loss: 0.5070710683764975
  batch 307 loss: 0.5066806321617835
  batch 308 loss: 0.5063646475409532
  batch 309 loss: 0.5060117822441854
  batch 310 loss: 0.5056275288904867
  batch 311 loss: 0.5053893749353586
  batch 312 loss: 0.5051663421476499
  batch 313 loss: 0.5049264397674476
  batch 314 loss: 0.5045054448637992
  batch 315 loss: 0.5041201924520825
  batch 316 loss: 0.503663191312476
  batch 317 loss: 0.5033515333565252
  batch 318 loss: 0.5029130856383521
  batch 319 loss: 0.502534481705543
  batch 320 loss: 0.5021342370659113
  batch 321 loss: 0.501808630528851
  batch 322 loss: 0.5014401685377086
  batch 323 loss: 0.5011265147765723
  batch 324 loss: 0.5005633914728224
  batch 325 loss: 0.5001673146394583
  batch 326 loss: 0.49980980816062975
  batch 327 loss: 0.4994205323016607
  batch 328 loss: 0.4989025083983817
  batch 329 loss: 0.4986527071173068
  batch 330 loss: 0.49827632786649645
  batch 331 loss: 0.4978696959616554
  batch 332 loss: 0.4974827293351472
  batch 333 loss: 0.4971377304545394
  batch 334 loss: 0.4967194986557532
  batch 335 loss: 0.49622121462181434
  batch 336 loss: 0.49576300542269436
  batch 337 loss: 0.4952720499180191
  batch 338 loss: 0.4948989114994128
  batch 339 loss: 0.49457731341893696
  batch 340 loss: 0.49426830656388226
  batch 341 loss: 0.49373862505658284
  batch 342 loss: 0.49333488941192627
  batch 343 loss: 0.4930223730493217
  batch 344 loss: 0.4926144212137821
  batch 345 loss: 0.4923643419708031
  batch 346 loss: 0.49191530315862225
  batch 347 loss: 0.4916354746742963
  batch 348 loss: 0.49134515131684553
  batch 349 loss: 0.49086835194795386
  batch 350 loss: 0.4905680590016501
  batch 351 loss: 0.4902298206787164
  batch 352 loss: 0.48997086117213423
  batch 353 loss: 0.4896685327416439
  batch 354 loss: 0.48949562678229336
  batch 355 loss: 0.48917854604586747
  batch 356 loss: 0.488857105960337
  batch 357 loss: 0.488485688087987
  batch 358 loss: 0.4881943339409109
  batch 359 loss: 0.4878888742007253
  batch 360 loss: 0.48756091064876983
  batch 361 loss: 0.48719676445725885
  batch 362 loss: 0.4868345912648828
  batch 363 loss: 0.4865510252717441
  batch 364 loss: 0.4861507430508897
  batch 365 loss: 0.485907322903202
  batch 366 loss: 0.4856023790080691
  batch 367 loss: 0.48518111213676285
  batch 368 loss: 0.48485079448184243
  batch 369 loss: 0.48461998615484575
  batch 370 loss: 0.4844300380429706
  batch 371 loss: 0.48416136771520835
  batch 372 loss: 0.4838033191939836
  batch 373 loss: 0.483471045065821
  batch 374 loss: 0.4830737375320598
  batch 375 loss: 0.4827410405476888
  batch 376 loss: 0.4825608787384439
  batch 377 loss: 0.4822507272190377
  batch 378 loss: 0.48189866156489763
  batch 379 loss: 0.48161568294099893
  batch 380 loss: 0.4813859559203449
  batch 381 loss: 0.48102030752524927
  batch 382 loss: 0.48068327686861545
  batch 383 loss: 0.48039405401942004
  batch 384 loss: 0.4800757606669019
  batch 385 loss: 0.4799033612399906
  batch 386 loss: 0.47952972970169444
  batch 387 loss: 0.47929884119859345
  batch 388 loss: 0.4791114478535259
  batch 389 loss: 0.4788840471319184
  batch 390 loss: 0.478571712741485
  batch 391 loss: 0.47827736305458773
  batch 392 loss: 0.47800849203248413
  batch 393 loss: 0.4778028880518508
  batch 394 loss: 0.47756379366223584
  batch 395 loss: 0.477217215601402
  batch 396 loss: 0.476917110082477
  batch 397 loss: 0.47665413050567473
  batch 398 loss: 0.4763946084670685
  batch 399 loss: 0.4761489546836767
  batch 400 loss: 0.47587733261287213
  batch 401 loss: 0.47554969430861627
  batch 402 loss: 0.475353390645625
  batch 403 loss: 0.47516582584558587
  batch 404 loss: 0.4749951715959181
  batch 405 loss: 0.47477694521715613
  batch 406 loss: 0.474590860343919
  batch 407 loss: 0.47431234799380384
  batch 408 loss: 0.47416678363201664
  batch 409 loss: 0.4739276595541201
  batch 410 loss: 0.47373068063724333
  batch 411 loss: 0.4734484984056793
  batch 412 loss: 0.4731049727178314
  batch 413 loss: 0.4728675401672613
  batch 414 loss: 0.47259325300149874
  batch 415 loss: 0.4723213100289724
  batch 416 loss: 0.47208231687545776
  batch 417 loss: 0.47189740525732793
  batch 418 loss: 0.47163220576977616
  batch 419 loss: 0.47138593938014955
  batch 420 loss: 0.4711640578650293
  batch 421 loss: 0.47082748810922165
  batch 422 loss: 0.47068880809144387
  batch 423 loss: 0.470416765686468
  batch 424 loss: 0.4701677663427479
  batch 425 loss: 0.4699834587994744
  batch 426 loss: 0.46972731784195965
  batch 427 loss: 0.4694750151254533
  batch 428 loss: 0.46910703070809906
  batch 429 loss: 0.46895772538262925
  batch 430 loss: 0.46868053203405335
  batch 431 loss: 0.46862036729633394
  batch 432 loss: 0.4684338939410669
  batch 433 loss: 0.4681779241176455
  batch 434 loss: 0.4680183989935756
  batch 435 loss: 0.4677313404521723
  batch 436 loss: 0.4674943904947797
  batch 437 loss: 0.4673522453695467
  batch 438 loss: 0.4672097218771503
  batch 439 loss: 0.4669492781026488
  batch 440 loss: 0.46674170378934254
  batch 441 loss: 0.4664519598154254
  batch 442 loss: 0.46616554071460914
  batch 443 loss: 0.46589363670779677
  batch 444 loss: 0.46557557851344616
  batch 445 loss: 0.46538755974073087
  batch 446 loss: 0.4651316441094394
  batch 447 loss: 0.4648177721756417
  batch 448 loss: 0.4646221378019878
  batch 449 loss: 0.46442229318990474
  batch 450 loss: 0.4641760661204656
  batch 451 loss: 0.463976939012629
  batch 452 loss: 0.46378100320564963
  batch 453 loss: 0.46363534602371537
  batch 454 loss: 0.46353295780225995
  batch 455 loss: 0.4633174595597026
  batch 456 loss: 0.46313908297503203
  batch 457 loss: 0.46293191986741333
  batch 458 loss: 0.46269488887755633
  batch 459 loss: 0.46255673458373625
  batch 460 loss: 0.4624658622171568
  batch 461 loss: 0.46224888260607605
  batch 462 loss: 0.4621117595599327
  batch 463 loss: 0.46187980205914886
  batch 464 loss: 0.4616614780549345
  batch 465 loss: 0.46138451951806264
  batch 466 loss: 0.46100163856289417
  batch 467 loss: 0.4609327849790422
  batch 468 loss: 0.460677799442385
  batch 469 loss: 0.4606411803378733
  batch 470 loss: 0.46046222232757733
  batch 471 loss: 0.4602807243285412
  batch 472 loss: 0.4600181131923603
LOSS train 0.4600181131923603 valid 0.25886887311935425
LOSS train 0.4600181131923603 valid 0.2568291276693344
LOSS train 0.4600181131923603 valid 0.2613801459471385
LOSS train 0.4600181131923603 valid 0.25706394016742706
LOSS train 0.4600181131923603 valid 0.2512557148933411
LOSS train 0.4600181131923603 valid 0.2539215038220088
LOSS train 0.4600181131923603 valid 0.26197344064712524
LOSS train 0.4600181131923603 valid 0.26095015555620193
LOSS train 0.4600181131923603 valid 0.2614128788312276
LOSS train 0.4600181131923603 valid 0.26298144459724426
LOSS train 0.4600181131923603 valid 0.2609117721969431
LOSS train 0.4600181131923603 valid 0.2618173745771249
LOSS train 0.4600181131923603 valid 0.2628400635260802
LOSS train 0.4600181131923603 valid 0.26304148882627487
LOSS train 0.4600181131923603 valid 0.2604205638170242
LOSS train 0.4600181131923603 valid 0.26106100250035524
LOSS train 0.4600181131923603 valid 0.2618586605086046
LOSS train 0.4600181131923603 valid 0.2628255560994148
LOSS train 0.4600181131923603 valid 0.2642438701893154
LOSS train 0.4600181131923603 valid 0.2636219926178455
LOSS train 0.4600181131923603 valid 0.2623136064835957
LOSS train 0.4600181131923603 valid 0.2604356726462191
LOSS train 0.4600181131923603 valid 0.2607763662286427
LOSS train 0.4600181131923603 valid 0.2602532139668862
LOSS train 0.4600181131923603 valid 0.2596117043495178
LOSS train 0.4600181131923603 valid 0.2599964886903763
LOSS train 0.4600181131923603 valid 0.2594832292309514
LOSS train 0.4600181131923603 valid 0.25949436319725855
LOSS train 0.4600181131923603 valid 0.25984768620852766
LOSS train 0.4600181131923603 valid 0.26090886890888215
LOSS train 0.4600181131923603 valid 0.261953124115544
LOSS train 0.4600181131923603 valid 0.26171918865293264
LOSS train 0.4600181131923603 valid 0.2625113942406394
LOSS train 0.4600181131923603 valid 0.26276445914717284
LOSS train 0.4600181131923603 valid 0.26417104942458014
LOSS train 0.4600181131923603 valid 0.2638224752412902
LOSS train 0.4600181131923603 valid 0.26351982194024165
LOSS train 0.4600181131923603 valid 0.26451700925827026
LOSS train 0.4600181131923603 valid 0.26413643054473096
LOSS train 0.4600181131923603 valid 0.2647194929420948
LOSS train 0.4600181131923603 valid 0.26543697714805603
LOSS train 0.4600181131923603 valid 0.26552283338138033
LOSS train 0.4600181131923603 valid 0.2654339234496272
LOSS train 0.4600181131923603 valid 0.26619605042717676
LOSS train 0.4600181131923603 valid 0.26558975014421676
LOSS train 0.4600181131923603 valid 0.26623002310162003
LOSS train 0.4600181131923603 valid 0.26636765199772855
LOSS train 0.4600181131923603 valid 0.2664529914036393
LOSS train 0.4600181131923603 valid 0.2667121890248085
LOSS train 0.4600181131923603 valid 0.2661945185065269
LOSS train 0.4600181131923603 valid 0.2665122499068578
LOSS train 0.4600181131923603 valid 0.26642249725185907
LOSS train 0.4600181131923603 valid 0.26641806594605716
LOSS train 0.4600181131923603 valid 0.2662564640243848
LOSS train 0.4600181131923603 valid 0.26608140116388146
LOSS train 0.4600181131923603 valid 0.26582322136632036
LOSS train 0.4600181131923603 valid 0.26551291529546706
LOSS train 0.4600181131923603 valid 0.265613557192786
LOSS train 0.4600181131923603 valid 0.2658035383891251
LOSS train 0.4600181131923603 valid 0.26566559796531997
LOSS train 0.4600181131923603 valid 0.2658686220157342
LOSS train 0.4600181131923603 valid 0.26646107555397097
LOSS train 0.4600181131923603 valid 0.2664475850169621
LOSS train 0.4600181131923603 valid 0.2669862930197269
LOSS train 0.4600181131923603 valid 0.2674579338385509
LOSS train 0.4600181131923603 valid 0.26759582058046805
LOSS train 0.4600181131923603 valid 0.2670760303735733
LOSS train 0.4600181131923603 valid 0.2671346574583474
LOSS train 0.4600181131923603 valid 0.2666704596384712
LOSS train 0.4600181131923603 valid 0.26655122871909825
LOSS train 0.4600181131923603 valid 0.26633641480560033
LOSS train 0.4600181131923603 valid 0.26643562585943276
LOSS train 0.4600181131923603 valid 0.2665240137952648
LOSS train 0.4600181131923603 valid 0.26623384996845917
LOSS train 0.4600181131923603 valid 0.2660785053173701
LOSS train 0.4600181131923603 valid 0.2663881498339929
LOSS train 0.4600181131923603 valid 0.2664972394704819
LOSS train 0.4600181131923603 valid 0.26652114723737425
LOSS train 0.4600181131923603 valid 0.2666495757389672
LOSS train 0.4600181131923603 valid 0.266270649805665
LOSS train 0.4600181131923603 valid 0.2656105390669387
LOSS train 0.4600181131923603 valid 0.2657485653350993
LOSS train 0.4600181131923603 valid 0.2655593589486846
LOSS train 0.4600181131923603 valid 0.2655286386254288
LOSS train 0.4600181131923603 valid 0.2651210837504443
LOSS train 0.4600181131923603 valid 0.2644143052572428
LOSS train 0.4600181131923603 valid 0.2645344001123275
LOSS train 0.4600181131923603 valid 0.2642780374735594
LOSS train 0.4600181131923603 valid 0.26443244115020453
LOSS train 0.4600181131923603 valid 0.2645636060171657
LOSS train 0.4600181131923603 valid 0.2646096530523929
LOSS train 0.4600181131923603 valid 0.2645957144382207
LOSS train 0.4600181131923603 valid 0.26447333908209236
LOSS train 0.4600181131923603 valid 0.26452820779795344
LOSS train 0.4600181131923603 valid 0.26417911444839676
LOSS train 0.4600181131923603 valid 0.2640264364890754
LOSS train 0.4600181131923603 valid 0.264044159927319
LOSS train 0.4600181131923603 valid 0.2639172863291234
LOSS train 0.4600181131923603 valid 0.2640348746620043
LOSS train 0.4600181131923603 valid 0.2644994108378887
LOSS train 0.4600181131923603 valid 0.2647303187906152
LOSS train 0.4600181131923603 valid 0.264737871493779
LOSS train 0.4600181131923603 valid 0.264750133585004
LOSS train 0.4600181131923603 valid 0.2646065287005443
LOSS train 0.4600181131923603 valid 0.2646702359120051
LOSS train 0.4600181131923603 valid 0.2649786642418717
LOSS train 0.4600181131923603 valid 0.2648688207998454
LOSS train 0.4600181131923603 valid 0.2649510396575486
LOSS train 0.4600181131923603 valid 0.2651800394331643
LOSS train 0.4600181131923603 valid 0.2654854488643733
LOSS train 0.4600181131923603 valid 0.2652921103410893
LOSS train 0.4600181131923603 valid 0.2650884586785521
LOSS train 0.4600181131923603 valid 0.26511077094922025
LOSS train 0.4600181131923603 valid 0.26519992241733953
LOSS train 0.4600181131923603 valid 0.26537450940712637
LOSS train 0.4600181131923603 valid 0.26553823207986765
LOSS train 0.4600181131923603 valid 0.2657672171918755
LOSS train 0.4600181131923603 valid 0.265756135522309
LOSS train 0.4600181131923603 valid 0.26568485458357993
LOSS train 0.4600181131923603 valid 0.265458141018947
LOSS train 0.4600181131923603 valid 0.26533377145932724
LOSS train 0.4600181131923603 valid 0.2652404701612035
LOSS train 0.4600181131923603 valid 0.2653633874606311
LOSS train 0.4600181131923603 valid 0.2654793233640732
LOSS train 0.4600181131923603 valid 0.26554749727249144
LOSS train 0.4600181131923603 valid 0.26560783788325293
LOSS train 0.4600181131923603 valid 0.2655076478409955
LOSS train 0.4600181131923603 valid 0.26573405019007623
LOSS train 0.4600181131923603 valid 0.26588823143825974
LOSS train 0.4600181131923603 valid 0.26589588958483473
LOSS train 0.4600181131923603 valid 0.26586690454082634
LOSS train 0.4600181131923603 valid 0.26579879224300385
LOSS train 0.4600181131923603 valid 0.2655268997178042
LOSS train 0.4600181131923603 valid 0.265646386502394
LOSS train 0.4600181131923603 valid 0.2656555347972446
LOSS train 0.4600181131923603 valid 0.26556700840592384
LOSS train 0.4600181131923603 valid 0.2654167017797484
LOSS train 0.4600181131923603 valid 0.26536448364672455
LOSS train 0.4600181131923603 valid 0.2652407244598265
LOSS train 0.4600181131923603 valid 0.2653686362717833
LOSS train 0.4600181131923603 valid 0.2655016345546601
LOSS train 0.4600181131923603 valid 0.26574933161618003
LOSS train 0.4600181131923603 valid 0.26572061991774953
LOSS train 0.4600181131923603 valid 0.265743760081629
LOSS train 0.4600181131923603 valid 0.26557430696898493
LOSS train 0.4600181131923603 valid 0.26587912654631757
LOSS train 0.4600181131923603 valid 0.26570873743011836
LOSS train 0.4600181131923603 valid 0.2663370735339216
LOSS train 0.4600181131923603 valid 0.2664494210441641
LOSS train 0.4600181131923603 valid 0.2664674401283264
LOSS train 0.4600181131923603 valid 0.266466541203442
LOSS train 0.4600181131923603 valid 0.26630920201147856
LOSS train 0.4600181131923603 valid 0.2664235757262099
LOSS train 0.4600181131923603 valid 0.26646599105813285
LOSS train 0.4600181131923603 valid 0.26645910480330065
LOSS train 0.4600181131923603 valid 0.26680754956144553
LOSS train 0.4600181131923603 valid 0.2667195012045514
LOSS train 0.4600181131923603 valid 0.26682287841280805
LOSS train 0.4600181131923603 valid 0.26678819017215344
LOSS train 0.4600181131923603 valid 0.26673313369974494
LOSS train 0.4600181131923603 valid 0.26672168816468733
LOSS train 0.4600181131923603 valid 0.26665889710923774
LOSS train 0.4600181131923603 valid 0.2666237629447247
LOSS train 0.4600181131923603 valid 0.26652445089889737
LOSS train 0.4600181131923603 valid 0.2664174121437651
LOSS train 0.4600181131923603 valid 0.26654092087803116
LOSS train 0.4600181131923603 valid 0.26661956453037833
LOSS train 0.4600181131923603 valid 0.26652571079986437
LOSS train 0.4600181131923603 valid 0.2665632767437478
LOSS train 0.4600181131923603 valid 0.2666470376884236
LOSS train 0.4600181131923603 valid 0.26658477699547484
LOSS train 0.4600181131923603 valid 0.2665488454838132
LOSS train 0.4600181131923603 valid 0.266651050548333
LOSS train 0.4600181131923603 valid 0.26660032364828834
LOSS train 0.4600181131923603 valid 0.2664755300113133
LOSS train 0.4600181131923603 valid 0.26640988513827324
LOSS train 0.4600181131923603 valid 0.26634543123891796
LOSS train 0.4600181131923603 valid 0.26662382818339914
LOSS train 0.4600181131923603 valid 0.26665972930759024
LOSS train 0.4600181131923603 valid 0.26672465834352704
LOSS train 0.4600181131923603 valid 0.2668952777240816
LOSS train 0.4600181131923603 valid 0.26698870940522834
LOSS train 0.4600181131923603 valid 0.2668864814961543
LOSS train 0.4600181131923603 valid 0.2668321799972783
LOSS train 0.4600181131923603 valid 0.266713671990343
LOSS train 0.4600181131923603 valid 0.26669102082008955
LOSS train 0.4600181131923603 valid 0.26659139273638394
LOSS train 0.4600181131923603 valid 0.2666770376740618
LOSS train 0.4600181131923603 valid 0.26670749871819105
LOSS train 0.4600181131923603 valid 0.2667486054332633
LOSS train 0.4600181131923603 valid 0.26675257195977015
LOSS train 0.4600181131923603 valid 0.2667574320609371
LOSS train 0.4600181131923603 valid 0.26680416718048133
LOSS train 0.4600181131923603 valid 0.26668602404827924
LOSS train 0.4600181131923603 valid 0.26653574865597945
LOSS train 0.4600181131923603 valid 0.26664393357172306
LOSS train 0.4600181131923603 valid 0.26670752087522887
LOSS train 0.4600181131923603 valid 0.26663793886851783
LOSS train 0.4600181131923603 valid 0.2666507952626626
LOSS train 0.4600181131923603 valid 0.2665416000783443
LOSS train 0.4600181131923603 valid 0.2664806399475876
LOSS train 0.4600181131923603 valid 0.2664517102265122
LOSS train 0.4600181131923603 valid 0.2664130454873804
LOSS train 0.4600181131923603 valid 0.2663457925413169
LOSS train 0.4600181131923603 valid 0.2662409055523756
LOSS train 0.4600181131923603 valid 0.266290065588303
LOSS train 0.4600181131923603 valid 0.2662272584323146
LOSS train 0.4600181131923603 valid 0.2661693044579946
LOSS train 0.4600181131923603 valid 0.266024466835711
LOSS train 0.4600181131923603 valid 0.2660597366236505
LOSS train 0.4600181131923603 valid 0.2660402582861236
LOSS train 0.4600181131923603 valid 0.26592741463825387
LOSS train 0.4600181131923603 valid 0.2660101160337108
LOSS train 0.4600181131923603 valid 0.2659253622883948
LOSS train 0.4600181131923603 valid 0.265839257559111
LOSS train 0.4600181131923603 valid 0.26580935350998686
LOSS train 0.4600181131923603 valid 0.2659012765241658
LOSS train 0.4600181131923603 valid 0.2658873563376042
LOSS train 0.4600181131923603 valid 0.26593577786939876
LOSS train 0.4600181131923603 valid 0.26600392752073027
LOSS train 0.4600181131923603 valid 0.26603974968329813
LOSS train 0.4600181131923603 valid 0.26607805376385785
LOSS train 0.4600181131923603 valid 0.26623888434048726
LOSS train 0.4600181131923603 valid 0.26636190293356776
LOSS train 0.4600181131923603 valid 0.2664495909876294
LOSS train 0.4600181131923603 valid 0.26651607038436737
LOSS train 0.4600181131923603 valid 0.2668169887186672
LOSS train 0.4600181131923603 valid 0.2669030548841284
LOSS train 0.4600181131923603 valid 0.2669339292564767
LOSS train 0.4600181131923603 valid 0.26705928295850756
LOSS train 0.4600181131923603 valid 0.2670276044380097
LOSS train 0.4600181131923603 valid 0.26707374914709864
LOSS train 0.4600181131923603 valid 0.2669321037029504
LOSS train 0.4600181131923603 valid 0.26692583832221156
LOSS train 0.4600181131923603 valid 0.2669935669670714
LOSS train 0.4600181131923603 valid 0.26693729216517026
LOSS train 0.4600181131923603 valid 0.2669708828760099
LOSS train 0.4600181131923603 valid 0.2669315082060189
LOSS train 0.4600181131923603 valid 0.2668149646480712
LOSS train 0.4600181131923603 valid 0.2668029697611928
LOSS train 0.4600181131923603 valid 0.26681100425631177
LOSS train 0.4600181131923603 valid 0.2666519935096591
LOSS train 0.4600181131923603 valid 0.2667482877341808
LOSS train 0.4600181131923603 valid 0.2668189813734078
LOSS train 0.4600181131923603 valid 0.26694747045332073
LOSS train 0.4600181131923603 valid 0.26692349451586483
LOSS train 0.4600181131923603 valid 0.2669126226713783
LOSS train 0.4600181131923603 valid 0.26688618327100433
LOSS train 0.4600181131923603 valid 0.26683266377113907
LOSS train 0.4600181131923603 valid 0.2669157596230507
LOSS train 0.4600181131923603 valid 0.2668574619221972
LOSS train 0.4600181131923603 valid 0.2668551964064439
LOSS train 0.4600181131923603 valid 0.266789955525059
LOSS train 0.4600181131923603 valid 0.2667682850806732
LOSS train 0.4600181131923603 valid 0.2668121426128874
LOSS train 0.4600181131923603 valid 0.26678490900667384
LOSS train 0.4600181131923603 valid 0.2666966006565651
LOSS train 0.4600181131923603 valid 0.2667472008471341
LOSS train 0.4600181131923603 valid 0.26669711858379336
LOSS train 0.4600181131923603 valid 0.2667644045100762
LOSS train 0.4600181131923603 valid 0.266810128229788
LOSS train 0.4600181131923603 valid 0.2667741885280791
LOSS train 0.4600181131923603 valid 0.26678295603496494
LOSS train 0.4600181131923603 valid 0.26676235297186807
LOSS train 0.4600181131923603 valid 0.26674451125117965
LOSS train 0.4600181131923603 valid 0.26669018480338547
LOSS train 0.4600181131923603 valid 0.2667922111374609
LOSS train 0.4600181131923603 valid 0.26690628107136755
LOSS train 0.4600181131923603 valid 0.2669980938660611
LOSS train 0.4600181131923603 valid 0.2669441111109875
LOSS train 0.4600181131923603 valid 0.2669903230733097
LOSS train 0.4600181131923603 valid 0.26710375261438246
LOSS train 0.4600181131923603 valid 0.26711662408414777
LOSS train 0.4600181131923603 valid 0.2670968670370805
LOSS train 0.4600181131923603 valid 0.26704622772606934
LOSS train 0.4600181131923603 valid 0.26698683585593663
LOSS train 0.4600181131923603 valid 0.26686577283733587
LOSS train 0.4600181131923603 valid 0.2668189966528536
LOSS train 0.4600181131923603 valid 0.2668350699447816
LOSS train 0.4600181131923603 valid 0.26679905562528544
LOSS train 0.4600181131923603 valid 0.2667052662563494
LOSS train 0.4600181131923603 valid 0.2665918883174024
LOSS train 0.4600181131923603 valid 0.2666480491207682
LOSS train 0.4600181131923603 valid 0.26672999681511395
LOSS train 0.4600181131923603 valid 0.2667780543105644
LOSS train 0.4600181131923603 valid 0.2666743829116955
LOSS train 0.4600181131923603 valid 0.26656738681660297
LOSS train 0.4600181131923603 valid 0.266623985953629
LOSS train 0.4600181131923603 valid 0.26668137177876533
LOSS train 0.4600181131923603 valid 0.2666370025996504
LOSS train 0.4600181131923603 valid 0.2665968899874343
LOSS train 0.4600181131923603 valid 0.26665038003088676
LOSS train 0.4600181131923603 valid 0.26673635443729754
LOSS train 0.4600181131923603 valid 0.26677864688594327
LOSS train 0.4600181131923603 valid 0.2667934488441985
LOSS train 0.4600181131923603 valid 0.2667959478457232
LOSS train 0.4600181131923603 valid 0.26682014959027067
LOSS train 0.4600181131923603 valid 0.2667943700247963
LOSS train 0.4600181131923603 valid 0.2668808580042925
LOSS train 0.4600181131923603 valid 0.2668223112821579
LOSS train 0.4600181131923603 valid 0.2668540658942885
LOSS train 0.4600181131923603 valid 0.26689441827748783
LOSS train 0.4600181131923603 valid 0.2670017310691745
LOSS train 0.4600181131923603 valid 0.2669834357343222
LOSS train 0.4600181131923603 valid 0.2669579194217432
LOSS train 0.4600181131923603 valid 0.2669213388869965
LOSS train 0.4600181131923603 valid 0.2669025883806645
LOSS train 0.4600181131923603 valid 0.2668335580013015
LOSS train 0.4600181131923603 valid 0.26691410176962327
LOSS train 0.4600181131923603 valid 0.2668652183586551
LOSS train 0.4600181131923603 valid 0.266796931384844
LOSS train 0.4600181131923603 valid 0.26681681349873543
LOSS train 0.4600181131923603 valid 0.2668393076227877
LOSS train 0.4600181131923603 valid 0.2669267792990253
LOSS train 0.4600181131923603 valid 0.26695107921721445
LOSS train 0.4600181131923603 valid 0.2668568101959138
LOSS train 0.4600181131923603 valid 0.26687948217715374
LOSS train 0.4600181131923603 valid 0.26694367455798873
LOSS train 0.4600181131923603 valid 0.2669834483846975
LOSS train 0.4600181131923603 valid 0.2669325286056846
LOSS train 0.4600181131923603 valid 0.2669793793067011
LOSS train 0.4600181131923603 valid 0.266944623280386
LOSS train 0.4600181131923603 valid 0.26687458438954487
LOSS train 0.4600181131923603 valid 0.2668413606782754
LOSS train 0.4600181131923603 valid 0.26689962428349717
LOSS train 0.4600181131923603 valid 0.26704770285476204
LOSS train 0.4600181131923603 valid 0.2670937184620341
LOSS train 0.4600181131923603 valid 0.2670622811902587
LOSS train 0.4600181131923603 valid 0.26713857946968367
LOSS train 0.4600181131923603 valid 0.26712048085349976
LOSS train 0.4600181131923603 valid 0.2670426746419549
LOSS train 0.4600181131923603 valid 0.26694360801793005
LOSS train 0.4600181131923603 valid 0.266947163789122
LOSS train 0.4600181131923603 valid 0.267116422201702
LOSS train 0.4600181131923603 valid 0.26708483558092544
LOSS train 0.4600181131923603 valid 0.26706657172845943
LOSS train 0.4600181131923603 valid 0.2670361316964605
LOSS train 0.4600181131923603 valid 0.2670239411603064
LOSS train 0.4600181131923603 valid 0.26707680373944365
LOSS train 0.4600181131923603 valid 0.26710204758188305
LOSS train 0.4600181131923603 valid 0.26703947380491017
LOSS train 0.4600181131923603 valid 0.26702913267222067
LOSS train 0.4600181131923603 valid 0.2670184492891106
LOSS train 0.4600181131923603 valid 0.2671701970488526
LOSS train 0.4600181131923603 valid 0.26719212411106497
LOSS train 0.4600181131923603 valid 0.26712468464595046
LOSS train 0.4600181131923603 valid 0.26704887161165564
LOSS train 0.4600181131923603 valid 0.26703705582981824
LOSS train 0.4600181131923603 valid 0.2670876696748515
LOSS train 0.4600181131923603 valid 0.2670282139522689
LOSS train 0.4600181131923603 valid 0.2669841718045395
LOSS train 0.4600181131923603 valid 0.2669883247210898
LOSS train 0.4600181131923603 valid 0.2670238179711377
LOSS train 0.4600181131923603 valid 0.26706891936265814
LOSS train 0.4600181131923603 valid 0.26714751917711443
LOSS train 0.4600181131923603 valid 0.26721278188771075
LOSS train 0.4600181131923603 valid 0.2671759147353533
LOSS train 0.4600181131923603 valid 0.26710284742729623
LOSS train 0.4600181131923603 valid 0.2670960415123565
LOSS train 0.4600181131923603 valid 0.26708395460413564
LOSS train 0.4600181131923603 valid 0.2670280023212248
LOSS train 0.4600181131923603 valid 0.2671303352680654
LOSS train 0.4600181131923603 valid 0.2670162490494652
LOSS train 0.4600181131923603 valid 0.2670065119125686
LOSS train 0.4600181131923603 valid 0.2669765315235478
LOSS train 0.4600181131923603 valid 0.266903673771952
LOSS train 0.4600181131923603 valid 0.2668254337291302
LOSS train 0.4600181131923603 valid 0.26680712717706745
LOSS train 0.4600181131923603 valid 0.2668616501134909
EPOCH 2:
  batch 1 loss: 0.34759706258773804
  batch 2 loss: 0.35514476895332336
  batch 3 loss: 0.3547836144765218
  batch 4 loss: 0.3616333305835724
  batch 5 loss: 0.3730720281600952
  batch 6 loss: 0.3670874536037445
  batch 7 loss: 0.365825423172542
  batch 8 loss: 0.3633205406367779
  batch 9 loss: 0.36545418368445504
  batch 10 loss: 0.3641920298337936
  batch 11 loss: 0.36185445297848096
  batch 12 loss: 0.3593599771459897
  batch 13 loss: 0.35863049442951495
  batch 14 loss: 0.35915589119706837
  batch 15 loss: 0.36280201077461244
  batch 16 loss: 0.36106340773403645
  batch 17 loss: 0.3585094476447386
  batch 18 loss: 0.3595418747928407
  batch 19 loss: 0.35886895656585693
  batch 20 loss: 0.35684587359428405
  batch 21 loss: 0.3574278652667999
  batch 22 loss: 0.3584078618071296
  batch 23 loss: 0.3584610146024953
  batch 24 loss: 0.357985258102417
  batch 25 loss: 0.35955052614212035
  batch 26 loss: 0.35872647853998035
  batch 27 loss: 0.3588636252615187
  batch 28 loss: 0.35777129765067783
  batch 29 loss: 0.3585647868699041
  batch 30 loss: 0.3573614905277888
  batch 31 loss: 0.35764403785428694
  batch 32 loss: 0.35748325660824776
  batch 33 loss: 0.35755831003189087
  batch 34 loss: 0.3583839211393805
  batch 35 loss: 0.3590541183948517
  batch 36 loss: 0.35928744491603637
  batch 37 loss: 0.3602927439921611
  batch 38 loss: 0.36071588961701645
  batch 39 loss: 0.36059668278082824
  batch 40 loss: 0.36050617769360543
  batch 41 loss: 0.3604834828434921
  batch 42 loss: 0.3613888443935485
  batch 43 loss: 0.36162595485532006
  batch 44 loss: 0.3618393215266141
  batch 45 loss: 0.36159349150127834
  batch 46 loss: 0.3609806545402693
  batch 47 loss: 0.36099148113676843
  batch 48 loss: 0.3601981606334448
  batch 49 loss: 0.35953959214444065
  batch 50 loss: 0.3594389355182648
  batch 51 loss: 0.35934200824475754
  batch 52 loss: 0.3598028152034833
  batch 53 loss: 0.35920104159499117
  batch 54 loss: 0.3592657231622272
  batch 55 loss: 0.35839557322588833
  batch 56 loss: 0.35873331502079964
  batch 57 loss: 0.35825179229702864
  batch 58 loss: 0.35902756812243625
  batch 59 loss: 0.3590653094194703
  batch 60 loss: 0.3585807174444199
  batch 61 loss: 0.3586445685292854
  batch 62 loss: 0.3587156904320563
  batch 63 loss: 0.3583049504529862
  batch 64 loss: 0.35885369731113315
  batch 65 loss: 0.358482440159871
  batch 66 loss: 0.3579270446842367
  batch 67 loss: 0.35848849744939093
  batch 68 loss: 0.35865837291759606
  batch 69 loss: 0.3586005836293317
  batch 70 loss: 0.35874746782439093
  batch 71 loss: 0.3583280561675488
  batch 72 loss: 0.35834406606025165
  batch 73 loss: 0.35813075181556075
  batch 74 loss: 0.3582831262736707
  batch 75 loss: 0.358030454715093
  batch 76 loss: 0.35853656028446396
  batch 77 loss: 0.35854934407519057
  batch 78 loss: 0.3585347303977379
  batch 79 loss: 0.3588112117368964
  batch 80 loss: 0.3587054889649153
  batch 81 loss: 0.35902172216662653
  batch 82 loss: 0.35952581937720135
  batch 83 loss: 0.3595233667327697
  batch 84 loss: 0.35921312904074076
  batch 85 loss: 0.35868578412953545
  batch 86 loss: 0.3591755379078
  batch 87 loss: 0.3589628790986949
  batch 88 loss: 0.3583550084043633
  batch 89 loss: 0.35813168963689485
  batch 90 loss: 0.35775045653184256
  batch 91 loss: 0.3579310280281109
  batch 92 loss: 0.3576486246741336
  batch 93 loss: 0.3575530103457871
  batch 94 loss: 0.3580986568902401
  batch 95 loss: 0.35794871135761863
  batch 96 loss: 0.3578572201852997
  batch 97 loss: 0.3585280622403646
  batch 98 loss: 0.3589055927432313
  batch 99 loss: 0.3589963771478094
  batch 100 loss: 0.3590273267030716
  batch 101 loss: 0.3591144034768095
  batch 102 loss: 0.35919714996627733
  batch 103 loss: 0.359506738128014
  batch 104 loss: 0.3595999866150893
  batch 105 loss: 0.3592048676241012
  batch 106 loss: 0.3597719632792023
  batch 107 loss: 0.35923833212005757
  batch 108 loss: 0.3593827052800744
  batch 109 loss: 0.3591753588903935
  batch 110 loss: 0.3594064799222079
  batch 111 loss: 0.35912804721711994
  batch 112 loss: 0.35885391464190824
  batch 113 loss: 0.3591477535994707
  batch 114 loss: 0.3594415140779395
  batch 115 loss: 0.3594027070895485
  batch 116 loss: 0.35941777763695554
  batch 117 loss: 0.35940291305892486
  batch 118 loss: 0.35921452383873825
  batch 119 loss: 0.3593322509477119
  batch 120 loss: 0.35913346260786055
  batch 121 loss: 0.35885873781748057
  batch 122 loss: 0.35856944518011125
  batch 123 loss: 0.35826884149535887
  batch 124 loss: 0.35873278206394565
  batch 125 loss: 0.3584434115886688
  batch 126 loss: 0.3584513042181257
  batch 127 loss: 0.3588049522065741
  batch 128 loss: 0.35862167715094984
  batch 129 loss: 0.35873329339101334
  batch 130 loss: 0.3587090579363016
  batch 131 loss: 0.35879541853912006
  batch 132 loss: 0.3588555340062488
  batch 133 loss: 0.3589125963530146
  batch 134 loss: 0.3587225573276406
  batch 135 loss: 0.3587187201888473
  batch 136 loss: 0.3587324465898907
  batch 137 loss: 0.35867982841756224
  batch 138 loss: 0.35878969620967255
  batch 139 loss: 0.3591158445790517
  batch 140 loss: 0.35880474533353535
  batch 141 loss: 0.3589341251562673
  batch 142 loss: 0.3588300504734818
  batch 143 loss: 0.3587060285198105
  batch 144 loss: 0.3584246670620309
  batch 145 loss: 0.35830708248861903
  batch 146 loss: 0.35819737898976833
  batch 147 loss: 0.3584558497480795
  batch 148 loss: 0.3583392633779629
  batch 149 loss: 0.3582498129182214
  batch 150 loss: 0.35806768655776977
  batch 151 loss: 0.3580144655625552
  batch 152 loss: 0.35809137789826645
  batch 153 loss: 0.3579280199178683
  batch 154 loss: 0.3580651854152803
  batch 155 loss: 0.3580029260727667
  batch 156 loss: 0.3578798870245616
  batch 157 loss: 0.35822099029638205
  batch 158 loss: 0.35833356972736646
  batch 159 loss: 0.3582719787486694
  batch 160 loss: 0.3579998003318906
  batch 161 loss: 0.35799203007857994
  batch 162 loss: 0.357925953320515
  batch 163 loss: 0.3577147932506046
  batch 164 loss: 0.35780998392076024
  batch 165 loss: 0.3575512970938827
  batch 166 loss: 0.35756668771605893
  batch 167 loss: 0.3573101100450504
  batch 168 loss: 0.35722292427505764
  batch 169 loss: 0.3569612025156529
  batch 170 loss: 0.3568456388571683
  batch 171 loss: 0.3568737815346634
  batch 172 loss: 0.35681994345992113
  batch 173 loss: 0.35672601144437843
  batch 174 loss: 0.3568004337192952
  batch 175 loss: 0.35681707773889815
  batch 176 loss: 0.3565605582500046
  batch 177 loss: 0.35641228698067745
  batch 178 loss: 0.35642717577768174
  batch 179 loss: 0.35653966032592943
  batch 180 loss: 0.3563545352882809
  batch 181 loss: 0.35633863237022695
  batch 182 loss: 0.3561458586008994
  batch 183 loss: 0.35585311934596203
  batch 184 loss: 0.3556274961194266
  batch 185 loss: 0.3554953405985961
  batch 186 loss: 0.3556973866878017
  batch 187 loss: 0.3555977771626437
  batch 188 loss: 0.3553867937719568
  batch 189 loss: 0.3551746203155114
  batch 190 loss: 0.35516165557660556
  batch 191 loss: 0.35512447310367806
  batch 192 loss: 0.3550305717314283
  batch 193 loss: 0.35501578419319707
  batch 194 loss: 0.355275510205436
  batch 195 loss: 0.35538129882934766
  batch 196 loss: 0.3550912655737935
  batch 197 loss: 0.3548766923737405
  batch 198 loss: 0.3549518972033202
  batch 199 loss: 0.35488197611803984
  batch 200 loss: 0.3549390448629856
  batch 201 loss: 0.35491979107334837
  batch 202 loss: 0.3548284385463979
  batch 203 loss: 0.3547621050785328
  batch 204 loss: 0.35454340834243625
  batch 205 loss: 0.3545768324921771
  batch 206 loss: 0.35460116996348484
  batch 207 loss: 0.354551550126882
  batch 208 loss: 0.35436789075342506
  batch 209 loss: 0.3541343686968516
  batch 210 loss: 0.35413506172952197
  batch 211 loss: 0.35419072288472503
  batch 212 loss: 0.3540818221726508
  batch 213 loss: 0.35405294100443524
  batch 214 loss: 0.3540039732355938
  batch 215 loss: 0.35375381763591324
  batch 216 loss: 0.35363968999849427
  batch 217 loss: 0.3536595215995191
  batch 218 loss: 0.3535214812930571
  batch 219 loss: 0.3534891662532336
  batch 220 loss: 0.35346992801536214
  batch 221 loss: 0.3534986980899966
  batch 222 loss: 0.3535271458797627
  batch 223 loss: 0.35352757479577857
  batch 224 loss: 0.35349398718348574
  batch 225 loss: 0.3532872128486633
  batch 226 loss: 0.35329151760160393
  batch 227 loss: 0.35308748063536993
  batch 228 loss: 0.35304228124911324
  batch 229 loss: 0.35287938680190706
  batch 230 loss: 0.35296946971312815
  batch 231 loss: 0.3529604110882912
  batch 232 loss: 0.3526456840336323
  batch 233 loss: 0.3525597858582443
  batch 234 loss: 0.3527225303089517
  batch 235 loss: 0.35270700416666395
  batch 236 loss: 0.3526150234422441
  batch 237 loss: 0.3526870713958257
  batch 238 loss: 0.3526022255170245
  batch 239 loss: 0.35247985987483704
  batch 240 loss: 0.3524621643126011
  batch 241 loss: 0.3525665283697769
  batch 242 loss: 0.35236084559732234
  batch 243 loss: 0.3523668542565632
  batch 244 loss: 0.3524642568386969
  batch 245 loss: 0.3524992136322722
  batch 246 loss: 0.3525236341042247
  batch 247 loss: 0.3526022813098151
  batch 248 loss: 0.3526423935207628
  batch 249 loss: 0.3525617875487929
  batch 250 loss: 0.35253007686138155
  batch 251 loss: 0.35243551854593347
  batch 252 loss: 0.35239931893727133
  batch 253 loss: 0.3523546911982208
  batch 254 loss: 0.3523262669955652
  batch 255 loss: 0.35231595167926716
  batch 256 loss: 0.35230906563811004
  batch 257 loss: 0.35221166937731585
  batch 258 loss: 0.3521912216909172
  batch 259 loss: 0.35221770532342916
  batch 260 loss: 0.3521626059825604
  batch 261 loss: 0.35211972715297424
  batch 262 loss: 0.35184908868702314
  batch 263 loss: 0.35193323080530636
  batch 264 loss: 0.35180531159946415
  batch 265 loss: 0.35176940459125444
  batch 266 loss: 0.35169338082012375
  batch 267 loss: 0.3518849220615201
  batch 268 loss: 0.35176706803378777
  batch 269 loss: 0.35168604646916723
  batch 270 loss: 0.3516752659170716
  batch 271 loss: 0.35160334446773317
  batch 272 loss: 0.3516453224069932
  batch 273 loss: 0.3516162231073275
  batch 274 loss: 0.35164520655670306
  batch 275 loss: 0.3516021927920255
  batch 276 loss: 0.3515135676100634
  batch 277 loss: 0.35139537643009144
  batch 278 loss: 0.35121340234931425
  batch 279 loss: 0.35127570926074914
  batch 280 loss: 0.3510886512696743
  batch 281 loss: 0.35094987847626846
  batch 282 loss: 0.35102196090610316
  batch 283 loss: 0.35086730480615325
  batch 284 loss: 0.35093656455127287
  batch 285 loss: 0.35084675968739026
  batch 286 loss: 0.35085885278828494
  batch 287 loss: 0.3508044139641087
  batch 288 loss: 0.3505950993340876
  batch 289 loss: 0.3507690841145169
  batch 290 loss: 0.3504553261502036
  batch 291 loss: 0.3504495353428359
  batch 292 loss: 0.35048549244665117
  batch 293 loss: 0.3503733017949114
  batch 294 loss: 0.3502259111526061
  batch 295 loss: 0.3501370579509412
  batch 296 loss: 0.35014533493164424
  batch 297 loss: 0.35009468073395367
  batch 298 loss: 0.3500272688649645
  batch 299 loss: 0.3499750961428103
  batch 300 loss: 0.34999441971381506
  batch 301 loss: 0.3498541062258407
  batch 302 loss: 0.34983469575446174
  batch 303 loss: 0.34987318328898337
  batch 304 loss: 0.3497813604772091
  batch 305 loss: 0.3495804365541114
  batch 306 loss: 0.3495919509650835
  batch 307 loss: 0.3495541692556698
  batch 308 loss: 0.34962053606649496
  batch 309 loss: 0.3495783308177318
  batch 310 loss: 0.34952455768662116
  batch 311 loss: 0.3496099634760829
  batch 312 loss: 0.34972544558919394
  batch 313 loss: 0.3498066087690786
  batch 314 loss: 0.34983146475378873
  batch 315 loss: 0.3497760035688915
  batch 316 loss: 0.34964647276114813
  batch 317 loss: 0.3496810023904975
  batch 318 loss: 0.34960856836921766
  batch 319 loss: 0.34950603334507596
  batch 320 loss: 0.349457420501858
  batch 321 loss: 0.3494595064552402
  batch 322 loss: 0.34944058242051496
  batch 323 loss: 0.3494595999688187
  batch 324 loss: 0.3492972335697692
  batch 325 loss: 0.3492519820653475
  batch 326 loss: 0.34923230778951586
  batch 327 loss: 0.34920297294217145
  batch 328 loss: 0.34906632044329877
  batch 329 loss: 0.34916906398480424
  batch 330 loss: 0.34908972319328424
  batch 331 loss: 0.3489946131439728
  batch 332 loss: 0.34888749049011486
  batch 333 loss: 0.3488247209304088
  batch 334 loss: 0.34867827451514627
  batch 335 loss: 0.34849715117198315
  batch 336 loss: 0.3483534154615232
  batch 337 loss: 0.3481855249192665
  batch 338 loss: 0.3481384764408924
  batch 339 loss: 0.34806905796745885
  batch 340 loss: 0.3479966037413653
  batch 341 loss: 0.3478436506738411
  batch 342 loss: 0.34771822610793757
  batch 343 loss: 0.347678832514293
  batch 344 loss: 0.3476464744917182
  batch 345 loss: 0.3477473690889884
  batch 346 loss: 0.34753991766816617
  batch 347 loss: 0.3475410590426036
  batch 348 loss: 0.34757919121405173
  batch 349 loss: 0.34738293136429993
  batch 350 loss: 0.34734917981284
  batch 351 loss: 0.347333695983615
  batch 352 loss: 0.347478083927523
  batch 353 loss: 0.34741876162483065
  batch 354 loss: 0.3475301838190542
  batch 355 loss: 0.3475051544081997
  batch 356 loss: 0.3474955978185943
  batch 357 loss: 0.3473589237497634
  batch 358 loss: 0.34736276846691216
  batch 359 loss: 0.34738593704189097
  batch 360 loss: 0.3473427135911253
  batch 361 loss: 0.34731457793151244
  batch 362 loss: 0.347213516166197
  batch 363 loss: 0.34708668615864
  batch 364 loss: 0.3469486176803872
  batch 365 loss: 0.34695826564749627
  batch 366 loss: 0.34688622402688846
  batch 367 loss: 0.3467550849394837
  batch 368 loss: 0.3466845035714948
  batch 369 loss: 0.34668541487639515
  batch 370 loss: 0.34667511687085434
  batch 371 loss: 0.3466632940537846
  batch 372 loss: 0.34659601002931595
  batch 373 loss: 0.3464948710743288
  batch 374 loss: 0.34643546965989197
  batch 375 loss: 0.3463422416051229
  batch 376 loss: 0.3464233968803223
  batch 377 loss: 0.34633697359884447
  batch 378 loss: 0.3462040047481577
  batch 379 loss: 0.34621856717446864
  batch 380 loss: 0.34623481309727616
  batch 381 loss: 0.3461083136205598
  batch 382 loss: 0.345959572267782
  batch 383 loss: 0.345920670981818
  batch 384 loss: 0.34582073288038373
  batch 385 loss: 0.3459051843587454
  batch 386 loss: 0.3457667077143575
  batch 387 loss: 0.34573892860141525
  batch 388 loss: 0.34579903701531517
  batch 389 loss: 0.3457985118912547
  batch 390 loss: 0.3457064078404353
  batch 391 loss: 0.345701854704591
  batch 392 loss: 0.34567636152615355
  batch 393 loss: 0.34569507048633563
  batch 394 loss: 0.34566404606182566
  batch 395 loss: 0.3455761193474637
  batch 396 loss: 0.34554274387732903
  batch 397 loss: 0.34554384757649687
  batch 398 loss: 0.34551473530992194
  batch 399 loss: 0.3455809431715418
  batch 400 loss: 0.34555773362517356
  batch 401 loss: 0.3454567689699425
  batch 402 loss: 0.3454896073732803
  batch 403 loss: 0.34556768143147454
  batch 404 loss: 0.34558642574466103
  batch 405 loss: 0.34560808885244676
  batch 406 loss: 0.3456492287478424
  batch 407 loss: 0.34556918750523935
  batch 408 loss: 0.34564228655368673
  batch 409 loss: 0.34562547115067105
  batch 410 loss: 0.3456328950277189
  batch 411 loss: 0.3456040322055492
  batch 412 loss: 0.34550857211200936
  batch 413 loss: 0.3455408660753587
  batch 414 loss: 0.3454852741985505
  batch 415 loss: 0.34544477541762664
  batch 416 loss: 0.3453818610511147
  batch 417 loss: 0.345416306234378
  batch 418 loss: 0.3453627851163371
  batch 419 loss: 0.3453197924504701
  batch 420 loss: 0.34534448803890316
  batch 421 loss: 0.34527761841329996
  batch 422 loss: 0.34535983141282156
  batch 423 loss: 0.34532460147607413
  batch 424 loss: 0.34528502800835753
  batch 425 loss: 0.3453013900448294
  batch 426 loss: 0.34521828927624393
  batch 427 loss: 0.3451826096418591
  batch 428 loss: 0.3450744544791284
  batch 429 loss: 0.34509478512899583
  batch 430 loss: 0.34504983799402106
  batch 431 loss: 0.34518239904722314
  batch 432 loss: 0.34522710947526825
  batch 433 loss: 0.34517285895402655
  batch 434 loss: 0.34519271250419353
  batch 435 loss: 0.34510269363721213
  batch 436 loss: 0.34502886570648317
  batch 437 loss: 0.34503369329995914
  batch 438 loss: 0.3450922742145791
  batch 439 loss: 0.3450488621254445
  batch 440 loss: 0.3450209148228168
  batch 441 loss: 0.3449284514061718
  batch 442 loss: 0.34486633788676285
  batch 443 loss: 0.34483067600506423
  batch 444 loss: 0.34469095879309886
  batch 445 loss: 0.34467741800158214
  batch 446 loss: 0.3446287470681785
  batch 447 loss: 0.34448807171527174
  batch 448 loss: 0.34447980066761374
  batch 449 loss: 0.34448189811079966
  batch 450 loss: 0.3444658728440603
  batch 451 loss: 0.3444713247060247
  batch 452 loss: 0.34445605407246443
  batch 453 loss: 0.3445088801531339
  batch 454 loss: 0.34457828130753554
  batch 455 loss: 0.3445667244575836
  batch 456 loss: 0.34459163183183
  batch 457 loss: 0.34457738974683916
  batch 458 loss: 0.3445367206652612
  batch 459 loss: 0.34459369790320304
  batch 460 loss: 0.3446676641702652
  batch 461 loss: 0.34466304482968924
  batch 462 loss: 0.3447423553440994
  batch 463 loss: 0.34469548434212194
  batch 464 loss: 0.34470145873211583
  batch 465 loss: 0.34464838485563953
  batch 466 loss: 0.34446994167284906
  batch 467 loss: 0.34453801150240276
  batch 468 loss: 0.3444784931902193
  batch 469 loss: 0.3445994754844129
  batch 470 loss: 0.34458798632976856
  batch 471 loss: 0.34457050494833863
  batch 472 loss: 0.3444463993912026
LOSS train 0.3444463993912026 valid 0.23392154276371002
LOSS train 0.3444463993912026 valid 0.22542305290699005
LOSS train 0.3444463993912026 valid 0.2270414580901464
LOSS train 0.3444463993912026 valid 0.22314725816249847
LOSS train 0.3444463993912026 valid 0.21613821387290955
LOSS train 0.3444463993912026 valid 0.21791677673657736
LOSS train 0.3444463993912026 valid 0.22718619448798044
LOSS train 0.3444463993912026 valid 0.22608465142548084
LOSS train 0.3444463993912026 valid 0.22637617256906298
LOSS train 0.3444463993912026 valid 0.22789334058761596
LOSS train 0.3444463993912026 valid 0.22598829865455627
LOSS train 0.3444463993912026 valid 0.2271058497329553
LOSS train 0.3444463993912026 valid 0.22784952016977164
LOSS train 0.3444463993912026 valid 0.22772250856672013
LOSS train 0.3444463993912026 valid 0.22458564341068268
LOSS train 0.3444463993912026 valid 0.22524632513523102
LOSS train 0.3444463993912026 valid 0.2258430491475498
LOSS train 0.3444463993912026 valid 0.22683068033721712
LOSS train 0.3444463993912026 valid 0.22849849729161514
LOSS train 0.3444463993912026 valid 0.22769301906228065
LOSS train 0.3444463993912026 valid 0.22628964128948392
LOSS train 0.3444463993912026 valid 0.22492473098364743
LOSS train 0.3444463993912026 valid 0.22502267036748969
LOSS train 0.3444463993912026 valid 0.22402322354416052
LOSS train 0.3444463993912026 valid 0.2234920197725296
LOSS train 0.3444463993912026 valid 0.2244083657860756
LOSS train 0.3444463993912026 valid 0.2242519943802445
LOSS train 0.3444463993912026 valid 0.22467752333198274
LOSS train 0.3444463993912026 valid 0.22524639141970668
LOSS train 0.3444463993912026 valid 0.22638072073459625
LOSS train 0.3444463993912026 valid 0.22698016368573712
LOSS train 0.3444463993912026 valid 0.22671998711302876
LOSS train 0.3444463993912026 valid 0.22721064271348895
LOSS train 0.3444463993912026 valid 0.22712390650721156
LOSS train 0.3444463993912026 valid 0.22875422494752065
LOSS train 0.3444463993912026 valid 0.22833973583247927
LOSS train 0.3444463993912026 valid 0.22818933910614736
LOSS train 0.3444463993912026 valid 0.22922726720571518
LOSS train 0.3444463993912026 valid 0.2285029784991191
LOSS train 0.3444463993912026 valid 0.22876802906394006
LOSS train 0.3444463993912026 valid 0.22936801285278507
LOSS train 0.3444463993912026 valid 0.22935454440968378
LOSS train 0.3444463993912026 valid 0.2294456532528234
LOSS train 0.3444463993912026 valid 0.2300483032383702
LOSS train 0.3444463993912026 valid 0.2294685569074419
LOSS train 0.3444463993912026 valid 0.23016802059567493
LOSS train 0.3444463993912026 valid 0.23042012402351866
LOSS train 0.3444463993912026 valid 0.23025659006088972
LOSS train 0.3444463993912026 valid 0.23051854663965654
LOSS train 0.3444463993912026 valid 0.23004858434200287
LOSS train 0.3444463993912026 valid 0.23030764536530363
LOSS train 0.3444463993912026 valid 0.22994443888847643
LOSS train 0.3444463993912026 valid 0.2300404838795932
LOSS train 0.3444463993912026 valid 0.22998001702405788
LOSS train 0.3444463993912026 valid 0.22996024516495792
LOSS train 0.3444463993912026 valid 0.22971284043576037
LOSS train 0.3444463993912026 valid 0.22947043781740623
LOSS train 0.3444463993912026 valid 0.22944489198512044
LOSS train 0.3444463993912026 valid 0.22973207624281866
LOSS train 0.3444463993912026 valid 0.22949103290836018
LOSS train 0.3444463993912026 valid 0.2297462554740124
LOSS train 0.3444463993912026 valid 0.2305592334558887
LOSS train 0.3444463993912026 valid 0.23074817728428615
LOSS train 0.3444463993912026 valid 0.23144996189512312
LOSS train 0.3444463993912026 valid 0.2318338469817088
LOSS train 0.3444463993912026 valid 0.23191920058293777
LOSS train 0.3444463993912026 valid 0.23131814309910162
LOSS train 0.3444463993912026 valid 0.23120667018434582
LOSS train 0.3444463993912026 valid 0.23041979191095932
LOSS train 0.3444463993912026 valid 0.23041093583617892
LOSS train 0.3444463993912026 valid 0.23028104259094723
LOSS train 0.3444463993912026 valid 0.23053615685138437
LOSS train 0.3444463993912026 valid 0.23060094003808007
LOSS train 0.3444463993912026 valid 0.23029710996795344
LOSS train 0.3444463993912026 valid 0.23017580866813658
LOSS train 0.3444463993912026 valid 0.2304507301826226
LOSS train 0.3444463993912026 valid 0.23047993864331925
LOSS train 0.3444463993912026 valid 0.23061600556740394
LOSS train 0.3444463993912026 valid 0.23081823711908317
LOSS train 0.3444463993912026 valid 0.23033905010670425
LOSS train 0.3444463993912026 valid 0.2295051211560214
LOSS train 0.3444463993912026 valid 0.22956188986214196
LOSS train 0.3444463993912026 valid 0.2293085529502616
LOSS train 0.3444463993912026 valid 0.22925950852887972
LOSS train 0.3444463993912026 valid 0.22887115881723516
LOSS train 0.3444463993912026 valid 0.22819711285275082
LOSS train 0.3444463993912026 valid 0.22843677507734847
LOSS train 0.3444463993912026 valid 0.2280806362290274
LOSS train 0.3444463993912026 valid 0.22828841192668745
LOSS train 0.3444463993912026 valid 0.2284029099676344
LOSS train 0.3444463993912026 valid 0.2286311836688073
LOSS train 0.3444463993912026 valid 0.2287184847113879
LOSS train 0.3444463993912026 valid 0.2285383451049046
LOSS train 0.3444463993912026 valid 0.2285972721082099
LOSS train 0.3444463993912026 valid 0.22820595063661275
LOSS train 0.3444463993912026 valid 0.22813233903919658
LOSS train 0.3444463993912026 valid 0.22812695463293606
LOSS train 0.3444463993912026 valid 0.22814042577329946
LOSS train 0.3444463993912026 valid 0.2283325440654851
LOSS train 0.3444463993912026 valid 0.22869444116950036
LOSS train 0.3444463993912026 valid 0.22899238116080217
LOSS train 0.3444463993912026 valid 0.22908309350411096
LOSS train 0.3444463993912026 valid 0.22907778808792817
LOSS train 0.3444463993912026 valid 0.22903752169356897
LOSS train 0.3444463993912026 valid 0.22913096291678292
LOSS train 0.3444463993912026 valid 0.22939755050641186
LOSS train 0.3444463993912026 valid 0.2292044449632413
LOSS train 0.3444463993912026 valid 0.22933115147882038
LOSS train 0.3444463993912026 valid 0.22969446843917216
LOSS train 0.3444463993912026 valid 0.23001487498933618
LOSS train 0.3444463993912026 valid 0.2297843482580271
LOSS train 0.3444463993912026 valid 0.22949741581188782
LOSS train 0.3444463993912026 valid 0.22958199033694984
LOSS train 0.3444463993912026 valid 0.22958199666780338
LOSS train 0.3444463993912026 valid 0.22967551143273063
LOSS train 0.3444463993912026 valid 0.23004013942233448
LOSS train 0.3444463993912026 valid 0.23030095630221897
LOSS train 0.3444463993912026 valid 0.23023857428865918
LOSS train 0.3444463993912026 valid 0.23013354938070313
LOSS train 0.3444463993912026 valid 0.22981595781942207
LOSS train 0.3444463993912026 valid 0.22965330328823122
LOSS train 0.3444463993912026 valid 0.22947283138017185
LOSS train 0.3444463993912026 valid 0.22955996500767342
LOSS train 0.3444463993912026 valid 0.22973285423171136
LOSS train 0.3444463993912026 valid 0.2297450498342514
LOSS train 0.3444463993912026 valid 0.229833227419664
LOSS train 0.3444463993912026 valid 0.22964859137854238
LOSS train 0.3444463993912026 valid 0.2298687583534047
LOSS train 0.3444463993912026 valid 0.22995993876179985
LOSS train 0.3444463993912026 valid 0.2299285685786834
LOSS train 0.3444463993912026 valid 0.2297547592689063
LOSS train 0.3444463993912026 valid 0.22956463355909695
LOSS train 0.3444463993912026 valid 0.22930446558428885
LOSS train 0.3444463993912026 valid 0.22938141148926608
LOSS train 0.3444463993912026 valid 0.22931477074269896
LOSS train 0.3444463993912026 valid 0.22921995062600165
LOSS train 0.3444463993912026 valid 0.2291119436930566
LOSS train 0.3444463993912026 valid 0.22904836451229843
LOSS train 0.3444463993912026 valid 0.2289285576172012
LOSS train 0.3444463993912026 valid 0.2290421649813652
LOSS train 0.3444463993912026 valid 0.2291637935959701
LOSS train 0.3444463993912026 valid 0.22943720410407428
LOSS train 0.3444463993912026 valid 0.2294364421517699
LOSS train 0.3444463993912026 valid 0.2294755188955201
LOSS train 0.3444463993912026 valid 0.22930258820796834
LOSS train 0.3444463993912026 valid 0.22953306097690374
LOSS train 0.3444463993912026 valid 0.22934977933257616
LOSS train 0.3444463993912026 valid 0.2300026158021914
LOSS train 0.3444463993912026 valid 0.23007398793761363
LOSS train 0.3444463993912026 valid 0.23009378443161646
LOSS train 0.3444463993912026 valid 0.23010308764233495
LOSS train 0.3444463993912026 valid 0.22997356255195642
LOSS train 0.3444463993912026 valid 0.23004711424213609
LOSS train 0.3444463993912026 valid 0.23009025257129173
LOSS train 0.3444463993912026 valid 0.2301140712153527
LOSS train 0.3444463993912026 valid 0.23043446854138985
LOSS train 0.3444463993912026 valid 0.2303951883771617
LOSS train 0.3444463993912026 valid 0.2304673404256
LOSS train 0.3444463993912026 valid 0.2304645692402462
LOSS train 0.3444463993912026 valid 0.23034096183255315
LOSS train 0.3444463993912026 valid 0.23035119242549684
LOSS train 0.3444463993912026 valid 0.23020868757624685
LOSS train 0.3444463993912026 valid 0.23017832892804058
LOSS train 0.3444463993912026 valid 0.23006395795723286
LOSS train 0.3444463993912026 valid 0.22993090893283036
LOSS train 0.3444463993912026 valid 0.23009329405893764
LOSS train 0.3444463993912026 valid 0.23023782161895387
LOSS train 0.3444463993912026 valid 0.2301563180628277
LOSS train 0.3444463993912026 valid 0.2302710759569202
LOSS train 0.3444463993912026 valid 0.23034051481415244
LOSS train 0.3444463993912026 valid 0.23025964165640156
LOSS train 0.3444463993912026 valid 0.23007861880022426
LOSS train 0.3444463993912026 valid 0.2301563381976475
LOSS train 0.3444463993912026 valid 0.2300852483202671
LOSS train 0.3444463993912026 valid 0.2300488122020449
LOSS train 0.3444463993912026 valid 0.22993039754642683
LOSS train 0.3444463993912026 valid 0.22987041087810603
LOSS train 0.3444463993912026 valid 0.23015160364716242
LOSS train 0.3444463993912026 valid 0.23012513788052777
LOSS train 0.3444463993912026 valid 0.23021401431825425
LOSS train 0.3444463993912026 valid 0.2304068122120852
LOSS train 0.3444463993912026 valid 0.23051791548073947
LOSS train 0.3444463993912026 valid 0.23041292968017807
LOSS train 0.3444463993912026 valid 0.23040779882475085
LOSS train 0.3444463993912026 valid 0.2302933692932129
LOSS train 0.3444463993912026 valid 0.2302335649728775
LOSS train 0.3444463993912026 valid 0.23010165439570013
LOSS train 0.3444463993912026 valid 0.2301306218067382
LOSS train 0.3444463993912026 valid 0.2301165127249622
LOSS train 0.3444463993912026 valid 0.2301935829614338
LOSS train 0.3444463993912026 valid 0.2301372957479267
LOSS train 0.3444463993912026 valid 0.23007653219004473
LOSS train 0.3444463993912026 valid 0.23014241719492975
LOSS train 0.3444463993912026 valid 0.23003028817090793
LOSS train 0.3444463993912026 valid 0.2298321856902196
LOSS train 0.3444463993912026 valid 0.22996798857134215
LOSS train 0.3444463993912026 valid 0.2300611381149534
LOSS train 0.3444463993912026 valid 0.22989659380130092
LOSS train 0.3444463993912026 valid 0.22988156757163042
LOSS train 0.3444463993912026 valid 0.22966337002813816
LOSS train 0.3444463993912026 valid 0.22953158431681828
LOSS train 0.3444463993912026 valid 0.2295178968717556
LOSS train 0.3444463993912026 valid 0.2294445462121165
LOSS train 0.3444463993912026 valid 0.2293710511396913
LOSS train 0.3444463993912026 valid 0.22926066365183853
LOSS train 0.3444463993912026 valid 0.2292771343756648
LOSS train 0.3444463993912026 valid 0.22917775485826575
LOSS train 0.3444463993912026 valid 0.22913035642928803
LOSS train 0.3444463993912026 valid 0.22897511442597404
LOSS train 0.3444463993912026 valid 0.22899299363295236
LOSS train 0.3444463993912026 valid 0.22899424358849277
LOSS train 0.3444463993912026 valid 0.22886489015424027
LOSS train 0.3444463993912026 valid 0.22894507744502574
LOSS train 0.3444463993912026 valid 0.2288622420942672
LOSS train 0.3444463993912026 valid 0.22874578584072203
LOSS train 0.3444463993912026 valid 0.22867326562603316
LOSS train 0.3444463993912026 valid 0.22868870617607223
LOSS train 0.3444463993912026 valid 0.22866019046087876
LOSS train 0.3444463993912026 valid 0.22870376896640482
LOSS train 0.3444463993912026 valid 0.22880435260859402
LOSS train 0.3444463993912026 valid 0.2288161887707214
LOSS train 0.3444463993912026 valid 0.2288079728280102
LOSS train 0.3444463993912026 valid 0.22889993055786254
LOSS train 0.3444463993912026 valid 0.2290814660622605
LOSS train 0.3444463993912026 valid 0.22912613438235388
LOSS train 0.3444463993912026 valid 0.2291736217067305
LOSS train 0.3444463993912026 valid 0.22949670189540292
LOSS train 0.3444463993912026 valid 0.22959670313356215
LOSS train 0.3444463993912026 valid 0.22962156862150634
LOSS train 0.3444463993912026 valid 0.22977275874303735
LOSS train 0.3444463993912026 valid 0.22976764327003843
LOSS train 0.3444463993912026 valid 0.22981589017757054
LOSS train 0.3444463993912026 valid 0.22972214388233397
LOSS train 0.3444463993912026 valid 0.2297591707135877
LOSS train 0.3444463993912026 valid 0.22978867596768318
LOSS train 0.3444463993912026 valid 0.22970844015984213
LOSS train 0.3444463993912026 valid 0.22976672661958364
LOSS train 0.3444463993912026 valid 0.22975748927653336
LOSS train 0.3444463993912026 valid 0.22962426472159111
LOSS train 0.3444463993912026 valid 0.22960925189157327
LOSS train 0.3444463993912026 valid 0.22966258036150478
LOSS train 0.3444463993912026 valid 0.22949629843481317
LOSS train 0.3444463993912026 valid 0.22962985062059552
LOSS train 0.3444463993912026 valid 0.2297417156886859
LOSS train 0.3444463993912026 valid 0.22983175169448464
LOSS train 0.3444463993912026 valid 0.22981102225499425
LOSS train 0.3444463993912026 valid 0.22980101979695833
LOSS train 0.3444463993912026 valid 0.2297740104578195
LOSS train 0.3444463993912026 valid 0.22966639254705973
LOSS train 0.3444463993912026 valid 0.22975511628389358
LOSS train 0.3444463993912026 valid 0.22968605255700678
LOSS train 0.3444463993912026 valid 0.22971222248105777
LOSS train 0.3444463993912026 valid 0.22961884307060318
LOSS train 0.3444463993912026 valid 0.2296454263484384
LOSS train 0.3444463993912026 valid 0.229723428569588
LOSS train 0.3444463993912026 valid 0.22972323436988518
LOSS train 0.3444463993912026 valid 0.22962696212506944
LOSS train 0.3444463993912026 valid 0.22966138827939367
LOSS train 0.3444463993912026 valid 0.2296049509729658
LOSS train 0.3444463993912026 valid 0.22965836570813106
LOSS train 0.3444463993912026 valid 0.2297439362810946
LOSS train 0.3444463993912026 valid 0.22971069875336786
LOSS train 0.3444463993912026 valid 0.22971164409651956
LOSS train 0.3444463993912026 valid 0.22973285908951904
LOSS train 0.3444463993912026 valid 0.22968643026531868
LOSS train 0.3444463993912026 valid 0.22962067830831484
LOSS train 0.3444463993912026 valid 0.22973277294233943
LOSS train 0.3444463993912026 valid 0.22988973613550415
LOSS train 0.3444463993912026 valid 0.22999977644491373
LOSS train 0.3444463993912026 valid 0.22996590418948068
LOSS train 0.3444463993912026 valid 0.2300114385866151
LOSS train 0.3444463993912026 valid 0.23015495625269763
LOSS train 0.3444463993912026 valid 0.2301720970279568
LOSS train 0.3444463993912026 valid 0.2301231698289405
LOSS train 0.3444463993912026 valid 0.2301142560893839
LOSS train 0.3444463993912026 valid 0.23001131636724956
LOSS train 0.3444463993912026 valid 0.2298760205399689
LOSS train 0.3444463993912026 valid 0.22978519498229885
LOSS train 0.3444463993912026 valid 0.22980483698802182
LOSS train 0.3444463993912026 valid 0.2297814667224884
LOSS train 0.3444463993912026 valid 0.22968872761174877
LOSS train 0.3444463993912026 valid 0.22954340997739886
LOSS train 0.3444463993912026 valid 0.22956896207357885
LOSS train 0.3444463993912026 valid 0.22966535662261534
LOSS train 0.3444463993912026 valid 0.22970630997105648
LOSS train 0.3444463993912026 valid 0.22959417276657546
LOSS train 0.3444463993912026 valid 0.22945758087709806
LOSS train 0.3444463993912026 valid 0.22949999653630787
LOSS train 0.3444463993912026 valid 0.22956462147739107
LOSS train 0.3444463993912026 valid 0.229537999732741
LOSS train 0.3444463993912026 valid 0.2294907398854744
LOSS train 0.3444463993912026 valid 0.22949517268228203
LOSS train 0.3444463993912026 valid 0.2295122601778434
LOSS train 0.3444463993912026 valid 0.22956691428917606
LOSS train 0.3444463993912026 valid 0.22964589575589714
LOSS train 0.3444463993912026 valid 0.22965421049377402
LOSS train 0.3444463993912026 valid 0.22968212698244486
LOSS train 0.3444463993912026 valid 0.22968026000581332
LOSS train 0.3444463993912026 valid 0.2297278006240277
LOSS train 0.3444463993912026 valid 0.2297299613058567
LOSS train 0.3444463993912026 valid 0.2297683837900922
LOSS train 0.3444463993912026 valid 0.2298063165600726
LOSS train 0.3444463993912026 valid 0.22992675852264113
LOSS train 0.3444463993912026 valid 0.22990456076436921
LOSS train 0.3444463993912026 valid 0.22983986574118254
LOSS train 0.3444463993912026 valid 0.2298014592988039
LOSS train 0.3444463993912026 valid 0.22978295482913524
LOSS train 0.3444463993912026 valid 0.22971364255849416
LOSS train 0.3444463993912026 valid 0.22979841996165154
LOSS train 0.3444463993912026 valid 0.22972647845745087
LOSS train 0.3444463993912026 valid 0.22963355448085013
LOSS train 0.3444463993912026 valid 0.22963521156746608
LOSS train 0.3444463993912026 valid 0.22963665644772135
LOSS train 0.3444463993912026 valid 0.22969223554157148
LOSS train 0.3444463993912026 valid 0.2297203259335624
LOSS train 0.3444463993912026 valid 0.22964309830265708
LOSS train 0.3444463993912026 valid 0.22967990208123384
LOSS train 0.3444463993912026 valid 0.22973147498549157
LOSS train 0.3444463993912026 valid 0.22977283517581915
LOSS train 0.3444463993912026 valid 0.22973299217410387
LOSS train 0.3444463993912026 valid 0.229789951740767
LOSS train 0.3444463993912026 valid 0.22975220558991344
LOSS train 0.3444463993912026 valid 0.22971009179350024
LOSS train 0.3444463993912026 valid 0.2296683418530005
LOSS train 0.3444463993912026 valid 0.22969960955473093
LOSS train 0.3444463993912026 valid 0.22985798147924108
LOSS train 0.3444463993912026 valid 0.22992116240186428
LOSS train 0.3444463993912026 valid 0.2298555574947741
LOSS train 0.3444463993912026 valid 0.2299567703780432
LOSS train 0.3444463993912026 valid 0.22993199147961357
LOSS train 0.3444463993912026 valid 0.2298717415494861
LOSS train 0.3444463993912026 valid 0.22979068643895975
LOSS train 0.3444463993912026 valid 0.2297821092802483
LOSS train 0.3444463993912026 valid 0.22995124485142932
LOSS train 0.3444463993912026 valid 0.22991451167348606
LOSS train 0.3444463993912026 valid 0.2298855127411939
LOSS train 0.3444463993912026 valid 0.22985643387194557
LOSS train 0.3444463993912026 valid 0.22984922179103603
LOSS train 0.3444463993912026 valid 0.22988758502104992
LOSS train 0.3444463993912026 valid 0.22991571798920632
LOSS train 0.3444463993912026 valid 0.2298559827451482
LOSS train 0.3444463993912026 valid 0.22982428580173972
LOSS train 0.3444463993912026 valid 0.22985254210737635
LOSS train 0.3444463993912026 valid 0.2300269818150027
LOSS train 0.3444463993912026 valid 0.23003220916658207
LOSS train 0.3444463993912026 valid 0.22994070620722853
LOSS train 0.3444463993912026 valid 0.2298516954485896
LOSS train 0.3444463993912026 valid 0.2298114994733498
LOSS train 0.3444463993912026 valid 0.2298597441105583
LOSS train 0.3444463993912026 valid 0.22978319172348294
LOSS train 0.3444463993912026 valid 0.2297254171211835
LOSS train 0.3444463993912026 valid 0.22973365933549675
LOSS train 0.3444463993912026 valid 0.22977239581579528
LOSS train 0.3444463993912026 valid 0.2298390861537497
LOSS train 0.3444463993912026 valid 0.22994413136596412
LOSS train 0.3444463993912026 valid 0.23001659967088967
LOSS train 0.3444463993912026 valid 0.23001574042464504
LOSS train 0.3444463993912026 valid 0.2299690381857936
LOSS train 0.3444463993912026 valid 0.22996274425458776
LOSS train 0.3444463993912026 valid 0.22991934017174773
LOSS train 0.3444463993912026 valid 0.22986336386765138
LOSS train 0.3444463993912026 valid 0.22996256438737417
LOSS train 0.3444463993912026 valid 0.22983740413812895
LOSS train 0.3444463993912026 valid 0.22987215441989375
LOSS train 0.3444463993912026 valid 0.22982531790047475
LOSS train 0.3444463993912026 valid 0.2297542683902334
LOSS train 0.3444463993912026 valid 0.22967177288408824
LOSS train 0.3444463993912026 valid 0.22965067479273546
LOSS train 0.3444463993912026 valid 0.22969647700870585
EPOCH 3:
  batch 1 loss: 0.3282783031463623
  batch 2 loss: 0.3235241621732712
  batch 3 loss: 0.3269861737887065
  batch 4 loss: 0.3232015371322632
  batch 5 loss: 0.33343303203582764
  batch 6 loss: 0.3274633636077245
  batch 7 loss: 0.3297427296638489
  batch 8 loss: 0.3318156562745571
  batch 9 loss: 0.3333059118853675
  batch 10 loss: 0.33285732567310333
  batch 11 loss: 0.33181704445318744
  batch 12 loss: 0.3294341067473094
  batch 13 loss: 0.32865068545708287
  batch 14 loss: 0.3288401791027614
  batch 15 loss: 0.3315395891666412
  batch 16 loss: 0.3300401959568262
  batch 17 loss: 0.32680292515193715
  batch 18 loss: 0.32735058003001744
  batch 19 loss: 0.3272457279657063
  batch 20 loss: 0.3254251301288605
  batch 21 loss: 0.32619524285906837
  batch 22 loss: 0.3267718418077989
  batch 23 loss: 0.3270874153012815
  batch 24 loss: 0.326257411390543
  batch 25 loss: 0.3278182685375214
  batch 26 loss: 0.32656593506152815
  batch 27 loss: 0.3269641763634152
  batch 28 loss: 0.3254140391945839
  batch 29 loss: 0.32675966414911994
  batch 30 loss: 0.3258980502684911
  batch 31 loss: 0.3261778691122609
  batch 32 loss: 0.3258160427212715
  batch 33 loss: 0.32556439168525464
  batch 34 loss: 0.3260357467567219
  batch 35 loss: 0.3264136689049857
  batch 36 loss: 0.3270983116494285
  batch 37 loss: 0.32808241892505335
  batch 38 loss: 0.32833397623739746
  batch 39 loss: 0.3278591915582999
  batch 40 loss: 0.3278947710990906
  batch 41 loss: 0.3276754930251982
  batch 42 loss: 0.32859874197414946
  batch 43 loss: 0.3288357964781828
  batch 44 loss: 0.3286791348999197
  batch 45 loss: 0.3284643438127306
  batch 46 loss: 0.3280106348835904
  batch 47 loss: 0.32803643510696734
  batch 48 loss: 0.32686609340210754
  batch 49 loss: 0.3264935223423705
  batch 50 loss: 0.3263045597076416
  batch 51 loss: 0.3259329877647699
  batch 52 loss: 0.32629696509012807
  batch 53 loss: 0.3257931161601588
  batch 54 loss: 0.32573055503544984
  batch 55 loss: 0.32520611719651654
  batch 56 loss: 0.3252174561577184
  batch 57 loss: 0.32448849081993103
  batch 58 loss: 0.3254569398945776
  batch 59 loss: 0.3257262726961556
  batch 60 loss: 0.32527331362167994
  batch 61 loss: 0.3253701163119957
  batch 62 loss: 0.3254102593468082
  batch 63 loss: 0.32492520695640925
  batch 64 loss: 0.3256107531487942
  batch 65 loss: 0.3251713042075817
  batch 66 loss: 0.32498179285815265
  batch 67 loss: 0.3255767252907824
  batch 68 loss: 0.3257688507437706
  batch 69 loss: 0.32550374623658
  batch 70 loss: 0.3257719052689416
  batch 71 loss: 0.3255779885909927
  batch 72 loss: 0.325623568561342
  batch 73 loss: 0.3256830393451534
  batch 74 loss: 0.32598066048042196
  batch 75 loss: 0.32571099440256757
  batch 76 loss: 0.3263975228918226
  batch 77 loss: 0.3264861462952255
  batch 78 loss: 0.3264584969251584
  batch 79 loss: 0.32687831490854674
  batch 80 loss: 0.3268178664147854
  batch 81 loss: 0.32708211371928086
  batch 82 loss: 0.3275944503342233
  batch 83 loss: 0.3277563682521682
  batch 84 loss: 0.3275257498025894
  batch 85 loss: 0.327240209719714
  batch 86 loss: 0.32771799176238303
  batch 87 loss: 0.3277032854913295
  batch 88 loss: 0.327291051433845
  batch 89 loss: 0.3269754135876559
  batch 90 loss: 0.3269635472032759
  batch 91 loss: 0.32726808530943735
  batch 92 loss: 0.3270073536297549
  batch 93 loss: 0.3269630952547955
  batch 94 loss: 0.3272131339032599
  batch 95 loss: 0.32689116032500015
  batch 96 loss: 0.32702776975929737
  batch 97 loss: 0.32767205355093654
  batch 98 loss: 0.3280518641885446
  batch 99 loss: 0.3281020558241642
  batch 100 loss: 0.32800389766693117
  batch 101 loss: 0.3282282107537336
  batch 102 loss: 0.3284837355216344
  batch 103 loss: 0.3288521315287618
  batch 104 loss: 0.3291302945178289
  batch 105 loss: 0.328702739023027
  batch 106 loss: 0.3293228857922104
  batch 107 loss: 0.3288429089795763
  batch 108 loss: 0.32895457054729815
  batch 109 loss: 0.3287388266773399
  batch 110 loss: 0.3287483345378529
  batch 111 loss: 0.32858383494454463
  batch 112 loss: 0.3282285942030804
  batch 113 loss: 0.3289759164362882
  batch 114 loss: 0.32927619652789936
  batch 115 loss: 0.32946772549463355
  batch 116 loss: 0.3295822677941158
  batch 117 loss: 0.3296446542964022
  batch 118 loss: 0.3297272647841502
  batch 119 loss: 0.32989074051881034
  batch 120 loss: 0.3297908852497737
  batch 121 loss: 0.3297476133039175
  batch 122 loss: 0.32969958274090877
  batch 123 loss: 0.32929952188235956
  batch 124 loss: 0.3299207521542426
  batch 125 loss: 0.33001613450050354
  batch 126 loss: 0.3300349064289577
  batch 127 loss: 0.33039659285169887
  batch 128 loss: 0.3303211322054267
  batch 129 loss: 0.3305811260559762
  batch 130 loss: 0.3308849552502999
  batch 131 loss: 0.33103980294620716
  batch 132 loss: 0.33116334428389865
  batch 133 loss: 0.3312030712464698
  batch 134 loss: 0.331190430851125
  batch 135 loss: 0.3310978589234529
  batch 136 loss: 0.3311500715858796
  batch 137 loss: 0.33101638781763343
  batch 138 loss: 0.3312454957892929
  batch 139 loss: 0.33171475190910504
  batch 140 loss: 0.3315966301730701
  batch 141 loss: 0.3317393677454468
  batch 142 loss: 0.3315758463782324
  batch 143 loss: 0.33146233971302325
  batch 144 loss: 0.3313108415653308
  batch 145 loss: 0.3312559444328834
  batch 146 loss: 0.3312243988660917
  batch 147 loss: 0.3315437446240665
  batch 148 loss: 0.3314431980252266
  batch 149 loss: 0.33128849131948995
  batch 150 loss: 0.3311730893452962
  batch 151 loss: 0.33125939827091644
  batch 152 loss: 0.3315036834070557
  batch 153 loss: 0.33141117956903243
  batch 154 loss: 0.3316010834721776
  batch 155 loss: 0.3316447902110315
  batch 156 loss: 0.3315885945772513
  batch 157 loss: 0.3319213847825482
  batch 158 loss: 0.3320156430141835
  batch 159 loss: 0.33190511232652004
  batch 160 loss: 0.3317720964550972
  batch 161 loss: 0.3317552171878933
  batch 162 loss: 0.33170610776653997
  batch 163 loss: 0.33149768585807704
  batch 164 loss: 0.3318386466764822
  batch 165 loss: 0.3316538012388981
  batch 166 loss: 0.33151421256094094
  batch 167 loss: 0.33133402181242755
  batch 168 loss: 0.33128842977540834
  batch 169 loss: 0.3309425718685579
  batch 170 loss: 0.3308198525625117
  batch 171 loss: 0.33086380449651975
  batch 172 loss: 0.33092742014762966
  batch 173 loss: 0.3309013745343754
  batch 174 loss: 0.33105082501625194
  batch 175 loss: 0.33107626608439855
  batch 176 loss: 0.33095616136084904
  batch 177 loss: 0.3308737336915765
  batch 178 loss: 0.3309275173069386
  batch 179 loss: 0.3310557953472244
  batch 180 loss: 0.33089895993471147
  batch 181 loss: 0.3308016758268051
  batch 182 loss: 0.3306693667238885
  batch 183 loss: 0.3303300894674708
  batch 184 loss: 0.33016226554046507
  batch 185 loss: 0.33006564249863496
  batch 186 loss: 0.33034934552126033
  batch 187 loss: 0.3302313428830336
  batch 188 loss: 0.33001245843603255
  batch 189 loss: 0.3298773491193378
  batch 190 loss: 0.32984239035531093
  batch 191 loss: 0.32971670521491486
  batch 192 loss: 0.3297875669474403
  batch 193 loss: 0.3298151116297035
  batch 194 loss: 0.3300433112788446
  batch 195 loss: 0.3301407963801653
  batch 196 loss: 0.3299149685368246
  batch 197 loss: 0.3297710492828776
  batch 198 loss: 0.3298658076861892
  batch 199 loss: 0.32992898369554297
  batch 200 loss: 0.3300503258407116
  batch 201 loss: 0.3299749094751937
  batch 202 loss: 0.32997945541202434
  batch 203 loss: 0.33006013071008505
  batch 204 loss: 0.3298899217271337
  batch 205 loss: 0.330024412928558
  batch 206 loss: 0.33001361367771925
  batch 207 loss: 0.32994944922590025
  batch 208 loss: 0.3297697543524779
  batch 209 loss: 0.3294608672840173
  batch 210 loss: 0.3294786525624139
  batch 211 loss: 0.32948291146359737
  batch 212 loss: 0.3295098886174976
  batch 213 loss: 0.32946914252540876
  batch 214 loss: 0.3294599578480854
  batch 215 loss: 0.3292177667451459
  batch 216 loss: 0.3291671360256495
  batch 217 loss: 0.3291295290267962
  batch 218 loss: 0.3290559549670701
  batch 219 loss: 0.3290856756848287
  batch 220 loss: 0.32904547358101066
  batch 221 loss: 0.3290305628496058
  batch 222 loss: 0.32904407328313534
  batch 223 loss: 0.32904833728957067
  batch 224 loss: 0.32904322086168186
  batch 225 loss: 0.3289226320054796
  batch 226 loss: 0.32892157638494945
  batch 227 loss: 0.32873568660887326
  batch 228 loss: 0.3287304639816284
  batch 229 loss: 0.32859652844058373
  batch 230 loss: 0.3286814103955808
  batch 231 loss: 0.3286840892457343
  batch 232 loss: 0.32841657166337146
  batch 233 loss: 0.3283848909605214
  batch 234 loss: 0.3285488058359195
  batch 235 loss: 0.3286057254101368
  batch 236 loss: 0.3284719299714444
  batch 237 loss: 0.32849931226500984
  batch 238 loss: 0.3283739127531773
  batch 239 loss: 0.3281678851678282
  batch 240 loss: 0.32819409171740216
  batch 241 loss: 0.3282750721294356
  batch 242 loss: 0.3280716342620613
  batch 243 loss: 0.3280822773283892
  batch 244 loss: 0.32807751561774584
  batch 245 loss: 0.32808272680457756
  batch 246 loss: 0.3281251212687996
  batch 247 loss: 0.32825645888865235
  batch 248 loss: 0.3283624884582335
  batch 249 loss: 0.32828870319937126
  batch 250 loss: 0.32824796593189237
  batch 251 loss: 0.32823657288969277
  batch 252 loss: 0.3281897185813813
  batch 253 loss: 0.32804561462326953
  batch 254 loss: 0.3279575547130089
  batch 255 loss: 0.32801592174698324
  batch 256 loss: 0.3280003742547706
  batch 257 loss: 0.32797503401797107
  batch 258 loss: 0.3279919758323551
  batch 259 loss: 0.3279744756037664
  batch 260 loss: 0.3279606328560756
  batch 261 loss: 0.32794133730775094
  batch 262 loss: 0.3277157510964925
  batch 263 loss: 0.3277796454755979
  batch 264 loss: 0.3276864100586284
  batch 265 loss: 0.3275631719040421
  batch 266 loss: 0.3275445546198608
  batch 267 loss: 0.32771991689999896
  batch 268 loss: 0.32759891350322695
  batch 269 loss: 0.3275028001863274
  batch 270 loss: 0.32754499713579815
  batch 271 loss: 0.32754530534973003
  batch 272 loss: 0.3275652213350815
  batch 273 loss: 0.32750838962230056
  batch 274 loss: 0.32760685617036195
  batch 275 loss: 0.32759139787067065
  batch 276 loss: 0.32750643519819644
  batch 277 loss: 0.327469882767123
  batch 278 loss: 0.3272721407224806
  batch 279 loss: 0.327377860691385
  batch 280 loss: 0.3271954134106636
  batch 281 loss: 0.32709272412642887
  batch 282 loss: 0.3271275755572826
  batch 283 loss: 0.32709209541971185
  batch 284 loss: 0.3271164741944259
  batch 285 loss: 0.3270686874264165
  batch 286 loss: 0.32708119658323437
  batch 287 loss: 0.3270615882366792
  batch 288 loss: 0.3268281479055683
  batch 289 loss: 0.32700203549903156
  batch 290 loss: 0.32672137433084947
  batch 291 loss: 0.3266553604316056
  batch 292 loss: 0.32670823882703914
  batch 293 loss: 0.32666714911574796
  batch 294 loss: 0.3265282302892127
  batch 295 loss: 0.3264802545814191
  batch 296 loss: 0.32649753836763873
  batch 297 loss: 0.3264816763625803
  batch 298 loss: 0.32648676963860557
  batch 299 loss: 0.32643575313497947
  batch 300 loss: 0.32650176763534544
  batch 301 loss: 0.3264317280826379
  batch 302 loss: 0.32642598558735375
  batch 303 loss: 0.3264732760171292
  batch 304 loss: 0.32637566710381133
  batch 305 loss: 0.32617668480169576
  batch 306 loss: 0.3262331302649055
  batch 307 loss: 0.32616759346440094
  batch 308 loss: 0.32620006132048446
  batch 309 loss: 0.3261463077901636
  batch 310 loss: 0.3260743963141595
  batch 311 loss: 0.3262138017887471
  batch 312 loss: 0.3263260427957926
  batch 313 loss: 0.3264178022409019
  batch 314 loss: 0.32639194939546523
  batch 315 loss: 0.3263335111595336
  batch 316 loss: 0.3261838419527947
  batch 317 loss: 0.3262017444481233
  batch 318 loss: 0.3261730297931335
  batch 319 loss: 0.32603348544025124
  batch 320 loss: 0.3260525120422244
  batch 321 loss: 0.32602095696785
  batch 322 loss: 0.3260060248908049
  batch 323 loss: 0.32596378355941535
  batch 324 loss: 0.3257611675394906
  batch 325 loss: 0.3257185898377345
  batch 326 loss: 0.3256582674438968
  batch 327 loss: 0.3256322146373422
  batch 328 loss: 0.32550788934274416
  batch 329 loss: 0.32559294495901436
  batch 330 loss: 0.32552832979144475
  batch 331 loss: 0.3254725539612266
  batch 332 loss: 0.32537848708859407
  batch 333 loss: 0.32530366166217906
  batch 334 loss: 0.3251514068977562
  batch 335 loss: 0.32496559842308953
  batch 336 loss: 0.32484993826420533
  batch 337 loss: 0.32465267862339753
  batch 338 loss: 0.3245913426022558
  batch 339 loss: 0.32462227801657706
  batch 340 loss: 0.3245619211126776
  batch 341 loss: 0.3244300713637008
  batch 342 loss: 0.3243152776308227
  batch 343 loss: 0.3242582728320586
  batch 344 loss: 0.3242068236996961
  batch 345 loss: 0.32426410006440204
  batch 346 loss: 0.3240939197857256
  batch 347 loss: 0.3240750330669392
  batch 348 loss: 0.3240869535111833
  batch 349 loss: 0.32393699875875326
  batch 350 loss: 0.32395208495003835
  batch 351 loss: 0.3239083881052131
  batch 352 loss: 0.3240023334087296
  batch 353 loss: 0.32393738399484
  batch 354 loss: 0.32405154078693715
  batch 355 loss: 0.3240187062344081
  batch 356 loss: 0.32398910853969914
  batch 357 loss: 0.3238708817658304
  batch 358 loss: 0.32387302091667775
  batch 359 loss: 0.32383457714468655
  batch 360 loss: 0.3237990510132578
  batch 361 loss: 0.32380728956074595
  batch 362 loss: 0.32367948043412265
  batch 363 loss: 0.32359081211168905
  batch 364 loss: 0.32347255519458223
  batch 365 loss: 0.3234486174093534
  batch 366 loss: 0.32334428781368696
  batch 367 loss: 0.3232063926370657
  batch 368 loss: 0.32309266973448836
  batch 369 loss: 0.32314305310326863
  batch 370 loss: 0.32316864285920116
  batch 371 loss: 0.3231513735579673
  batch 372 loss: 0.32310803995657994
  batch 373 loss: 0.32303248741351887
  batch 374 loss: 0.3229274566479545
  batch 375 loss: 0.3228678068319956
  batch 376 loss: 0.3229227916515888
  batch 377 loss: 0.3229043098754529
  batch 378 loss: 0.32279873564445155
  batch 379 loss: 0.3228481720179555
  batch 380 loss: 0.32286887372794904
  batch 381 loss: 0.3227341838396127
  batch 382 loss: 0.32261528339997636
  batch 383 loss: 0.3225663550217531
  batch 384 loss: 0.32249208182717365
  batch 385 loss: 0.322583660444656
  batch 386 loss: 0.32246279878628686
  batch 387 loss: 0.3224758263244185
  batch 388 loss: 0.32250531240529623
  batch 389 loss: 0.3225012796243849
  batch 390 loss: 0.32246587192400905
  batch 391 loss: 0.32244221397373074
  batch 392 loss: 0.32241762011330954
  batch 393 loss: 0.3224773483573632
  batch 394 loss: 0.32246261411511956
  batch 395 loss: 0.32235671455347087
  batch 396 loss: 0.322341215806176
  batch 397 loss: 0.3223613823391028
  batch 398 loss: 0.32233750617983353
  batch 399 loss: 0.32244488208515004
  batch 400 loss: 0.32243854850530623
  batch 401 loss: 0.32231731820582155
  batch 402 loss: 0.3223218760680203
  batch 403 loss: 0.3224136333903367
  batch 404 loss: 0.32242604160662924
  batch 405 loss: 0.32245434132623085
  batch 406 loss: 0.32249858160617906
  batch 407 loss: 0.32241958562806433
  batch 408 loss: 0.3225075698220262
  batch 409 loss: 0.3224958931904259
  batch 410 loss: 0.32252218243552416
  batch 411 loss: 0.3224882583548553
  batch 412 loss: 0.32241615059885004
  batch 413 loss: 0.3224266144755966
  batch 414 loss: 0.3223974113015161
  batch 415 loss: 0.32238411752574414
  batch 416 loss: 0.3223442840031706
  batch 417 loss: 0.3223375818855185
  batch 418 loss: 0.32230564495593167
  batch 419 loss: 0.3222772955609961
  batch 420 loss: 0.3222513472750073
  batch 421 loss: 0.32218338630544885
  batch 422 loss: 0.322293339041172
  batch 423 loss: 0.3222158166533666
  batch 424 loss: 0.32215980306830044
  batch 425 loss: 0.32218190915444317
  batch 426 loss: 0.32210058904309785
  batch 427 loss: 0.32207442816582443
  batch 428 loss: 0.3219867850714755
  batch 429 loss: 0.32201554621016226
  batch 430 loss: 0.32193148420300594
  batch 431 loss: 0.3220762526767713
  batch 432 loss: 0.3221436509931529
  batch 433 loss: 0.32213181752806175
  batch 434 loss: 0.3221703988890494
  batch 435 loss: 0.3221115258233301
  batch 436 loss: 0.3220384049169514
  batch 437 loss: 0.3220424244960331
  batch 438 loss: 0.3221191304595503
  batch 439 loss: 0.3220788704778719
  batch 440 loss: 0.32206665230068293
  batch 441 loss: 0.32199015503837947
  batch 442 loss: 0.3219676083838778
  batch 443 loss: 0.321955156380113
  batch 444 loss: 0.3218546971544489
  batch 445 loss: 0.3218744792295306
  batch 446 loss: 0.3218229377617216
  batch 447 loss: 0.3217192387554203
  batch 448 loss: 0.32166889363101553
  batch 449 loss: 0.32171424685713973
  batch 450 loss: 0.3216609994570414
  batch 451 loss: 0.3216533315948266
  batch 452 loss: 0.32164508124104646
  batch 453 loss: 0.32172780617183405
  batch 454 loss: 0.32178016604305887
  batch 455 loss: 0.32180847217748454
  batch 456 loss: 0.3218162592435092
  batch 457 loss: 0.3218255666465676
  batch 458 loss: 0.3217741652477256
  batch 459 loss: 0.32180068286415797
  batch 460 loss: 0.32188836627680323
  batch 461 loss: 0.3218842985314557
  batch 462 loss: 0.32191607027084795
  batch 463 loss: 0.3218387591916078
  batch 464 loss: 0.32183374413128557
  batch 465 loss: 0.3217373413424338
  batch 466 loss: 0.3215597435320396
  batch 467 loss: 0.32164311252646066
  batch 468 loss: 0.3216150342845
  batch 469 loss: 0.3218030081565446
  batch 470 loss: 0.3217491124855711
  batch 471 loss: 0.32174634563315446
  batch 472 loss: 0.32162207020920214
LOSS train 0.32162207020920214 valid 0.20388242602348328
LOSS train 0.32162207020920214 valid 0.19535841047763824
LOSS train 0.32162207020920214 valid 0.19586562116940817
LOSS train 0.32162207020920214 valid 0.19089674949645996
LOSS train 0.32162207020920214 valid 0.18342743515968324
LOSS train 0.32162207020920214 valid 0.18663723270098367
LOSS train 0.32162207020920214 valid 0.19458089556012834
LOSS train 0.32162207020920214 valid 0.19276998937129974
LOSS train 0.32162207020920214 valid 0.19332042005327013
LOSS train 0.32162207020920214 valid 0.19502383917570115
LOSS train 0.32162207020920214 valid 0.19305572726509787
LOSS train 0.32162207020920214 valid 0.19377009322245917
LOSS train 0.32162207020920214 valid 0.19395547990615553
LOSS train 0.32162207020920214 valid 0.19427671496357238
LOSS train 0.32162207020920214 valid 0.19170462985833486
LOSS train 0.32162207020920214 valid 0.19204855058342218
LOSS train 0.32162207020920214 valid 0.1920714334529989
LOSS train 0.32162207020920214 valid 0.19331146362755033
LOSS train 0.32162207020920214 valid 0.19560828177552475
LOSS train 0.32162207020920214 valid 0.19501842483878135
LOSS train 0.32162207020920214 valid 0.19373778431188493
LOSS train 0.32162207020920214 valid 0.19277084076946432
LOSS train 0.32162207020920214 valid 0.1929443148167237
LOSS train 0.32162207020920214 valid 0.19215143409868082
LOSS train 0.32162207020920214 valid 0.19133854627609254
LOSS train 0.32162207020920214 valid 0.19198989466978952
LOSS train 0.32162207020920214 valid 0.19191816393975858
LOSS train 0.32162207020920214 valid 0.19234969573361532
LOSS train 0.32162207020920214 valid 0.19301441106302986
LOSS train 0.32162207020920214 valid 0.19416496803363165
LOSS train 0.32162207020920214 valid 0.19507330127300754
LOSS train 0.32162207020920214 valid 0.19478804897516966
LOSS train 0.32162207020920214 valid 0.19553757978208136
LOSS train 0.32162207020920214 valid 0.1956792784964337
LOSS train 0.32162207020920214 valid 0.19688555555684226
LOSS train 0.32162207020920214 valid 0.1966291901965936
LOSS train 0.32162207020920214 valid 0.19662384809674444
LOSS train 0.32162207020920214 valid 0.19760446721001676
LOSS train 0.32162207020920214 valid 0.19686015179524055
LOSS train 0.32162207020920214 valid 0.1970361590385437
LOSS train 0.32162207020920214 valid 0.1976284740901575
LOSS train 0.32162207020920214 valid 0.19767099228643237
LOSS train 0.32162207020920214 valid 0.19763001557006393
LOSS train 0.32162207020920214 valid 0.1979923156852072
LOSS train 0.32162207020920214 valid 0.19764896730581918
LOSS train 0.32162207020920214 valid 0.19821410496597705
LOSS train 0.32162207020920214 valid 0.19848921451162785
LOSS train 0.32162207020920214 valid 0.19837112786869207
LOSS train 0.32162207020920214 valid 0.1986414267092335
LOSS train 0.32162207020920214 valid 0.19807227969169616
LOSS train 0.32162207020920214 valid 0.19843263252108706
LOSS train 0.32162207020920214 valid 0.19812972356493658
LOSS train 0.32162207020920214 valid 0.19837284341173353
LOSS train 0.32162207020920214 valid 0.1980464000392843
LOSS train 0.32162207020920214 valid 0.19819435341791672
LOSS train 0.32162207020920214 valid 0.19799701310694218
LOSS train 0.32162207020920214 valid 0.19779102556538164
LOSS train 0.32162207020920214 valid 0.19755747503247753
LOSS train 0.32162207020920214 valid 0.1979329550670365
LOSS train 0.32162207020920214 valid 0.19775550564130148
LOSS train 0.32162207020920214 valid 0.19798956909140603
LOSS train 0.32162207020920214 valid 0.19886399829579937
LOSS train 0.32162207020920214 valid 0.19899204136833312
LOSS train 0.32162207020920214 valid 0.1996195730753243
LOSS train 0.32162207020920214 valid 0.19993919134140015
LOSS train 0.32162207020920214 valid 0.20006402333577475
LOSS train 0.32162207020920214 valid 0.19964314885993503
LOSS train 0.32162207020920214 valid 0.1995160183047547
LOSS train 0.32162207020920214 valid 0.19887518321258435
LOSS train 0.32162207020920214 valid 0.1990525301013674
LOSS train 0.32162207020920214 valid 0.19900540692705504
LOSS train 0.32162207020920214 valid 0.19927000088824165
LOSS train 0.32162207020920214 valid 0.1993006614789571
LOSS train 0.32162207020920214 valid 0.19911471653628993
LOSS train 0.32162207020920214 valid 0.19893672744433086
LOSS train 0.32162207020920214 valid 0.19929902039860425
LOSS train 0.32162207020920214 valid 0.19928927797001678
LOSS train 0.32162207020920214 valid 0.19933835436136293
LOSS train 0.32162207020920214 valid 0.1994972440260875
LOSS train 0.32162207020920214 valid 0.19894227478653193
LOSS train 0.32162207020920214 valid 0.1981590850117766
LOSS train 0.32162207020920214 valid 0.198239262692812
LOSS train 0.32162207020920214 valid 0.1981053197958383
LOSS train 0.32162207020920214 valid 0.19796467874021756
LOSS train 0.32162207020920214 valid 0.19760218437980204
LOSS train 0.32162207020920214 valid 0.1970406325404034
LOSS train 0.32162207020920214 valid 0.19720465972505766
LOSS train 0.32162207020920214 valid 0.19681529023430563
LOSS train 0.32162207020920214 valid 0.19708741615327557
LOSS train 0.32162207020920214 valid 0.19719452361265818
LOSS train 0.32162207020920214 valid 0.1974220442902911
LOSS train 0.32162207020920214 valid 0.1975195678355901
LOSS train 0.32162207020920214 valid 0.1973323102599831
LOSS train 0.32162207020920214 valid 0.19747098154844123
LOSS train 0.32162207020920214 valid 0.19718348038823982
LOSS train 0.32162207020920214 valid 0.19715588974455991
LOSS train 0.32162207020920214 valid 0.19729929148536368
LOSS train 0.32162207020920214 valid 0.19737615010568074
LOSS train 0.32162207020920214 valid 0.19756346248617077
LOSS train 0.32162207020920214 valid 0.1979002046585083
LOSS train 0.32162207020920214 valid 0.19829758013239002
LOSS train 0.32162207020920214 valid 0.1984260204960318
LOSS train 0.32162207020920214 valid 0.19838549240121564
LOSS train 0.32162207020920214 valid 0.19839921335761362
LOSS train 0.32162207020920214 valid 0.19847554706391835
LOSS train 0.32162207020920214 valid 0.19874755672688754
LOSS train 0.32162207020920214 valid 0.1985260335920013
LOSS train 0.32162207020920214 valid 0.19866118455926576
LOSS train 0.32162207020920214 valid 0.19902083824533934
LOSS train 0.32162207020920214 valid 0.19940253428437493
LOSS train 0.32162207020920214 valid 0.19913674971541842
LOSS train 0.32162207020920214 valid 0.1988896827346512
LOSS train 0.32162207020920214 valid 0.19895225882002737
LOSS train 0.32162207020920214 valid 0.1988789728074743
LOSS train 0.32162207020920214 valid 0.1988768157751664
LOSS train 0.32162207020920214 valid 0.1992107600230595
LOSS train 0.32162207020920214 valid 0.19944278297261295
LOSS train 0.32162207020920214 valid 0.19934813930826673
LOSS train 0.32162207020920214 valid 0.1993469381783189
LOSS train 0.32162207020920214 valid 0.19909403088192146
LOSS train 0.32162207020920214 valid 0.1989097376007679
LOSS train 0.32162207020920214 valid 0.19872931732994611
LOSS train 0.32162207020920214 valid 0.19883057160106132
LOSS train 0.32162207020920214 valid 0.19896395780867146
LOSS train 0.32162207020920214 valid 0.19899298465251922
LOSS train 0.32162207020920214 valid 0.19909210302053937
LOSS train 0.32162207020920214 valid 0.19898364405463062
LOSS train 0.32162207020920214 valid 0.1991474461974576
LOSS train 0.32162207020920214 valid 0.1991792203382004
LOSS train 0.32162207020920214 valid 0.19917658555966158
LOSS train 0.32162207020920214 valid 0.19910832602559156
LOSS train 0.32162207020920214 valid 0.19897409122098575
LOSS train 0.32162207020920214 valid 0.19878103589653073
LOSS train 0.32162207020920214 valid 0.19879275393574986
LOSS train 0.32162207020920214 valid 0.19872337999167267
LOSS train 0.32162207020920214 valid 0.19863399096271572
LOSS train 0.32162207020920214 valid 0.19855297003349248
LOSS train 0.32162207020920214 valid 0.19852592072625092
LOSS train 0.32162207020920214 valid 0.19839956520272672
LOSS train 0.32162207020920214 valid 0.19847442124571119
LOSS train 0.32162207020920214 valid 0.19852677082761805
LOSS train 0.32162207020920214 valid 0.19872971015496993
LOSS train 0.32162207020920214 valid 0.19875041525680703
LOSS train 0.32162207020920214 valid 0.19878332637664345
LOSS train 0.32162207020920214 valid 0.19859154090799133
LOSS train 0.32162207020920214 valid 0.19878827270171415
LOSS train 0.32162207020920214 valid 0.1985864997112832
LOSS train 0.32162207020920214 valid 0.19921496218523463
LOSS train 0.32162207020920214 valid 0.1992732361459092
LOSS train 0.32162207020920214 valid 0.19927729735771815
LOSS train 0.32162207020920214 valid 0.19931563695534965
LOSS train 0.32162207020920214 valid 0.1992260957822988
LOSS train 0.32162207020920214 valid 0.19927677443993638
LOSS train 0.32162207020920214 valid 0.19929780143421966
LOSS train 0.32162207020920214 valid 0.19935530866346052
LOSS train 0.32162207020920214 valid 0.19971231543100798
LOSS train 0.32162207020920214 valid 0.19968556313757685
LOSS train 0.32162207020920214 valid 0.19978835282823706
LOSS train 0.32162207020920214 valid 0.19978708994088681
LOSS train 0.32162207020920214 valid 0.1996727974154055
LOSS train 0.32162207020920214 valid 0.19966460468235964
LOSS train 0.32162207020920214 valid 0.1995684383092103
LOSS train 0.32162207020920214 valid 0.19951367999878397
LOSS train 0.32162207020920214 valid 0.19937585012578382
LOSS train 0.32162207020920214 valid 0.19923690575541872
LOSS train 0.32162207020920214 valid 0.19938179832625103
LOSS train 0.32162207020920214 valid 0.19951282066856316
LOSS train 0.32162207020920214 valid 0.19946201296434515
LOSS train 0.32162207020920214 valid 0.19953643887706057
LOSS train 0.32162207020920214 valid 0.19962993684936972
LOSS train 0.32162207020920214 valid 0.19954927483497306
LOSS train 0.32162207020920214 valid 0.19934509590614674
LOSS train 0.32162207020920214 valid 0.19937091988290664
LOSS train 0.32162207020920214 valid 0.19929764299899683
LOSS train 0.32162207020920214 valid 0.19930554594312394
LOSS train 0.32162207020920214 valid 0.19923604449087923
LOSS train 0.32162207020920214 valid 0.19917201970593404
LOSS train 0.32162207020920214 valid 0.19941434992498228
LOSS train 0.32162207020920214 valid 0.19937677281861865
LOSS train 0.32162207020920214 valid 0.19942992967036036
LOSS train 0.32162207020920214 valid 0.19961828635542433
LOSS train 0.32162207020920214 valid 0.1996988275057667
LOSS train 0.32162207020920214 valid 0.1996026108336579
LOSS train 0.32162207020920214 valid 0.19960673712193966
LOSS train 0.32162207020920214 valid 0.19949582831279652
LOSS train 0.32162207020920214 valid 0.19942301655969313
LOSS train 0.32162207020920214 valid 0.19925276353397472
LOSS train 0.32162207020920214 valid 0.19927398361107138
LOSS train 0.32162207020920214 valid 0.19930805139756078
LOSS train 0.32162207020920214 valid 0.19935932716256693
LOSS train 0.32162207020920214 valid 0.19926712564460894
LOSS train 0.32162207020920214 valid 0.19922393669063845
LOSS train 0.32162207020920214 valid 0.19929156772830944
LOSS train 0.32162207020920214 valid 0.19916792621010357
LOSS train 0.32162207020920214 valid 0.19898872742286094
LOSS train 0.32162207020920214 valid 0.1990979703105226
LOSS train 0.32162207020920214 valid 0.1991698561888661
LOSS train 0.32162207020920214 valid 0.1990343165217024
LOSS train 0.32162207020920214 valid 0.19899683224795453
LOSS train 0.32162207020920214 valid 0.19881898477673532
LOSS train 0.32162207020920214 valid 0.19869881186319227
LOSS train 0.32162207020920214 valid 0.19864295327132292
LOSS train 0.32162207020920214 valid 0.19853604801476296
LOSS train 0.32162207020920214 valid 0.1984871536785481
LOSS train 0.32162207020920214 valid 0.1983618240530898
LOSS train 0.32162207020920214 valid 0.19839869222594697
LOSS train 0.32162207020920214 valid 0.19830554450191737
LOSS train 0.32162207020920214 valid 0.19827604917092964
LOSS train 0.32162207020920214 valid 0.19810722582933435
LOSS train 0.32162207020920214 valid 0.1980924591422081
LOSS train 0.32162207020920214 valid 0.1981211545484326
LOSS train 0.32162207020920214 valid 0.19798053997867512
LOSS train 0.32162207020920214 valid 0.19807632722205398
LOSS train 0.32162207020920214 valid 0.19798029332517464
LOSS train 0.32162207020920214 valid 0.1978881652965102
LOSS train 0.32162207020920214 valid 0.19783975722061264
LOSS train 0.32162207020920214 valid 0.19785163555002433
LOSS train 0.32162207020920214 valid 0.19783232825885125
LOSS train 0.32162207020920214 valid 0.19789525267740363
LOSS train 0.32162207020920214 valid 0.19799418991262263
LOSS train 0.32162207020920214 valid 0.1980449566744032
LOSS train 0.32162207020920214 valid 0.1980398792001578
LOSS train 0.32162207020920214 valid 0.19809747945032846
LOSS train 0.32162207020920214 valid 0.19822436744081123
LOSS train 0.32162207020920214 valid 0.19823811789353687
LOSS train 0.32162207020920214 valid 0.19827230874679785
LOSS train 0.32162207020920214 valid 0.1985281245692711
LOSS train 0.32162207020920214 valid 0.19861495364130588
LOSS train 0.32162207020920214 valid 0.19864054108811258
LOSS train 0.32162207020920214 valid 0.19878146583619324
LOSS train 0.32162207020920214 valid 0.19878763218462725
LOSS train 0.32162207020920214 valid 0.19882669980669843
LOSS train 0.32162207020920214 valid 0.19873695842953712
LOSS train 0.32162207020920214 valid 0.19882430753901473
LOSS train 0.32162207020920214 valid 0.1988260399153892
LOSS train 0.32162207020920214 valid 0.19874383976398888
LOSS train 0.32162207020920214 valid 0.1987887896943193
LOSS train 0.32162207020920214 valid 0.1987903816234164
LOSS train 0.32162207020920214 valid 0.1986516169798424
LOSS train 0.32162207020920214 valid 0.19862778180589277
LOSS train 0.32162207020920214 valid 0.19871700131546907
LOSS train 0.32162207020920214 valid 0.19856932242054584
LOSS train 0.32162207020920214 valid 0.1986852187678647
LOSS train 0.32162207020920214 valid 0.19879361278698093
LOSS train 0.32162207020920214 valid 0.1988925703934261
LOSS train 0.32162207020920214 valid 0.19888024843805205
LOSS train 0.32162207020920214 valid 0.19887934516557315
LOSS train 0.32162207020920214 valid 0.19886020805326202
LOSS train 0.32162207020920214 valid 0.19876321420133353
LOSS train 0.32162207020920214 valid 0.19885798716545106
LOSS train 0.32162207020920214 valid 0.19876164168238164
LOSS train 0.32162207020920214 valid 0.19880506756996352
LOSS train 0.32162207020920214 valid 0.19870520174032144
LOSS train 0.32162207020920214 valid 0.19870414655274293
LOSS train 0.32162207020920214 valid 0.19877096379504483
LOSS train 0.32162207020920214 valid 0.19876493571791798
LOSS train 0.32162207020920214 valid 0.19867505515827744
LOSS train 0.32162207020920214 valid 0.19870969653129578
LOSS train 0.32162207020920214 valid 0.19867336548663475
LOSS train 0.32162207020920214 valid 0.19871663537163
LOSS train 0.32162207020920214 valid 0.19881375242467128
LOSS train 0.32162207020920214 valid 0.19881422101086332
LOSS train 0.32162207020920214 valid 0.19881136205486472
LOSS train 0.32162207020920214 valid 0.1988094894384796
LOSS train 0.32162207020920214 valid 0.19875173799271853
LOSS train 0.32162207020920214 valid 0.1986988943658377
LOSS train 0.32162207020920214 valid 0.19880618897270175
LOSS train 0.32162207020920214 valid 0.1989135647665209
LOSS train 0.32162207020920214 valid 0.19899483562402123
LOSS train 0.32162207020920214 valid 0.1989474392047635
LOSS train 0.32162207020920214 valid 0.19899191292229615
LOSS train 0.32162207020920214 valid 0.19914345262462602
LOSS train 0.32162207020920214 valid 0.19917260285043892
LOSS train 0.32162207020920214 valid 0.1991320424815164
LOSS train 0.32162207020920214 valid 0.19909865243868394
LOSS train 0.32162207020920214 valid 0.19901808502449506
LOSS train 0.32162207020920214 valid 0.19888704251296255
LOSS train 0.32162207020920214 valid 0.19879547045599644
LOSS train 0.32162207020920214 valid 0.19880101218232119
LOSS train 0.32162207020920214 valid 0.1987749982625246
LOSS train 0.32162207020920214 valid 0.1986848930447127
LOSS train 0.32162207020920214 valid 0.19855654408745732
LOSS train 0.32162207020920214 valid 0.19860826980309434
LOSS train 0.32162207020920214 valid 0.19871294891960184
LOSS train 0.32162207020920214 valid 0.1987697387473625
LOSS train 0.32162207020920214 valid 0.19866833106412754
LOSS train 0.32162207020920214 valid 0.19855163067267748
LOSS train 0.32162207020920214 valid 0.19857698497879836
LOSS train 0.32162207020920214 valid 0.19862278347196877
LOSS train 0.32162207020920214 valid 0.19859388609384668
LOSS train 0.32162207020920214 valid 0.19855043445665813
LOSS train 0.32162207020920214 valid 0.19854349928767714
LOSS train 0.32162207020920214 valid 0.19857359024038088
LOSS train 0.32162207020920214 valid 0.19864857110644685
LOSS train 0.32162207020920214 valid 0.19872262068724228
LOSS train 0.32162207020920214 valid 0.19869475142174475
LOSS train 0.32162207020920214 valid 0.19871539058107318
LOSS train 0.32162207020920214 valid 0.1986899513926282
LOSS train 0.32162207020920214 valid 0.19872600248824793
LOSS train 0.32162207020920214 valid 0.19869662513335545
LOSS train 0.32162207020920214 valid 0.19873480954241515
LOSS train 0.32162207020920214 valid 0.19875784272587063
LOSS train 0.32162207020920214 valid 0.19885364504340458
LOSS train 0.32162207020920214 valid 0.19884080317263542
LOSS train 0.32162207020920214 valid 0.19879020823807012
LOSS train 0.32162207020920214 valid 0.19874280272356046
LOSS train 0.32162207020920214 valid 0.19873216830752183
LOSS train 0.32162207020920214 valid 0.19865332084236206
LOSS train 0.32162207020920214 valid 0.1987628872147656
LOSS train 0.32162207020920214 valid 0.19871016365866506
LOSS train 0.32162207020920214 valid 0.19858939078459786
LOSS train 0.32162207020920214 valid 0.19858281218852752
LOSS train 0.32162207020920214 valid 0.19858649382576013
LOSS train 0.32162207020920214 valid 0.1986308411048476
LOSS train 0.32162207020920214 valid 0.1986525469356113
LOSS train 0.32162207020920214 valid 0.19859117354396022
LOSS train 0.32162207020920214 valid 0.19861159151660907
LOSS train 0.32162207020920214 valid 0.19864148459157105
LOSS train 0.32162207020920214 valid 0.19869085735288158
LOSS train 0.32162207020920214 valid 0.19864644529297948
LOSS train 0.32162207020920214 valid 0.19868521972608716
LOSS train 0.32162207020920214 valid 0.1986597994839923
LOSS train 0.32162207020920214 valid 0.19863743097420447
LOSS train 0.32162207020920214 valid 0.19862715679186363
LOSS train 0.32162207020920214 valid 0.19864184306218075
LOSS train 0.32162207020920214 valid 0.19876838580596667
LOSS train 0.32162207020920214 valid 0.19883760618507315
LOSS train 0.32162207020920214 valid 0.19879299270488868
LOSS train 0.32162207020920214 valid 0.19888420941981863
LOSS train 0.32162207020920214 valid 0.19887035276853676
LOSS train 0.32162207020920214 valid 0.19880213698771784
LOSS train 0.32162207020920214 valid 0.19873634811086827
LOSS train 0.32162207020920214 valid 0.1987209802006816
LOSS train 0.32162207020920214 valid 0.1988455718327425
LOSS train 0.32162207020920214 valid 0.19882536139061202
LOSS train 0.32162207020920214 valid 0.1988409666255826
LOSS train 0.32162207020920214 valid 0.19879369486333354
LOSS train 0.32162207020920214 valid 0.19877873118636172
LOSS train 0.32162207020920214 valid 0.1987997602924133
LOSS train 0.32162207020920214 valid 0.19885106279569514
LOSS train 0.32162207020920214 valid 0.1987766855098635
LOSS train 0.32162207020920214 valid 0.19874547082081176
LOSS train 0.32162207020920214 valid 0.19876601219003473
LOSS train 0.32162207020920214 valid 0.19892328912608845
LOSS train 0.32162207020920214 valid 0.19891933137955872
LOSS train 0.32162207020920214 valid 0.19884331278890544
LOSS train 0.32162207020920214 valid 0.19877414972191243
LOSS train 0.32162207020920214 valid 0.1987267884201017
LOSS train 0.32162207020920214 valid 0.1988025867836523
LOSS train 0.32162207020920214 valid 0.19873309688908714
LOSS train 0.32162207020920214 valid 0.1986861156772005
LOSS train 0.32162207020920214 valid 0.19869055929170412
LOSS train 0.32162207020920214 valid 0.1987229532037173
LOSS train 0.32162207020920214 valid 0.1988033427104438
LOSS train 0.32162207020920214 valid 0.19893089566432254
LOSS train 0.32162207020920214 valid 0.199008037032706
LOSS train 0.32162207020920214 valid 0.19898814192506112
LOSS train 0.32162207020920214 valid 0.19893590244334503
LOSS train 0.32162207020920214 valid 0.19892796218229203
LOSS train 0.32162207020920214 valid 0.1988727618422773
LOSS train 0.32162207020920214 valid 0.19880156635907878
LOSS train 0.32162207020920214 valid 0.1988882445910359
LOSS train 0.32162207020920214 valid 0.19878692826456276
LOSS train 0.32162207020920214 valid 0.19880699542361302
LOSS train 0.32162207020920214 valid 0.19876660839335558
LOSS train 0.32162207020920214 valid 0.19869422652030902
LOSS train 0.32162207020920214 valid 0.19861967881953685
LOSS train 0.32162207020920214 valid 0.1985960955043202
LOSS train 0.32162207020920214 valid 0.1986575000654391
EPOCH 4:
  batch 1 loss: 0.3034631907939911
  batch 2 loss: 0.29861193895339966
  batch 3 loss: 0.2982386350631714
  batch 4 loss: 0.29757697880268097
  batch 5 loss: 0.309564483165741
  batch 6 loss: 0.3062124451001485
  batch 7 loss: 0.30819975904056
  batch 8 loss: 0.31245313957333565
  batch 9 loss: 0.3144923018084632
  batch 10 loss: 0.31523028016090393
  batch 11 loss: 0.3160167634487152
  batch 12 loss: 0.3132558912038803
  batch 13 loss: 0.3119544661962069
  batch 14 loss: 0.31197342915194376
  batch 15 loss: 0.31452661951382954
  batch 16 loss: 0.3123627472668886
  batch 17 loss: 0.30919549745671887
  batch 18 loss: 0.30996594164106583
  batch 19 loss: 0.31016821610300166
  batch 20 loss: 0.308041849732399
  batch 21 loss: 0.30915017638887676
  batch 22 loss: 0.3099980923262509
  batch 23 loss: 0.3100668062334475
  batch 24 loss: 0.3093828037381172
  batch 25 loss: 0.31172342419624327
  batch 26 loss: 0.309923645395499
  batch 27 loss: 0.3102339329542937
  batch 28 loss: 0.30841378229004995
  batch 29 loss: 0.3091745273820285
  batch 30 loss: 0.3086012750864029
  batch 31 loss: 0.3097313440615131
  batch 32 loss: 0.3094397922977805
  batch 33 loss: 0.3091220629937721
  batch 34 loss: 0.3091707904549206
  batch 35 loss: 0.30931623152324134
  batch 36 loss: 0.3095180259810554
  batch 37 loss: 0.31053636686221975
  batch 38 loss: 0.31072445605930527
  batch 39 loss: 0.31029288126872134
  batch 40 loss: 0.31002611964941024
  batch 41 loss: 0.31027664934716576
  batch 42 loss: 0.3109592880521502
  batch 43 loss: 0.3113299185453459
  batch 44 loss: 0.3109618913043629
  batch 45 loss: 0.31059759391678704
  batch 46 loss: 0.3102389891510424
  batch 47 loss: 0.3099625402308525
  batch 48 loss: 0.3090843490014474
  batch 49 loss: 0.3085555507212269
  batch 50 loss: 0.30883848965168
  batch 51 loss: 0.30867770080472906
  batch 52 loss: 0.30913913880403227
  batch 53 loss: 0.30870412824288856
  batch 54 loss: 0.30879487428400254
  batch 55 loss: 0.3082573457197709
  batch 56 loss: 0.3080196242247309
  batch 57 loss: 0.3071479044462505
  batch 58 loss: 0.3080522155967252
  batch 59 loss: 0.30820467664023576
  batch 60 loss: 0.30774845778942106
  batch 61 loss: 0.30794049823870423
  batch 62 loss: 0.3078893442307749
  batch 63 loss: 0.30736650834007867
  batch 64 loss: 0.3080659252591431
  batch 65 loss: 0.3073963775084569
  batch 66 loss: 0.30704613797592395
  batch 67 loss: 0.3074180812978033
  batch 68 loss: 0.307730785187553
  batch 69 loss: 0.307614800290785
  batch 70 loss: 0.3079097070864269
  batch 71 loss: 0.307632607473454
  batch 72 loss: 0.30781081898344886
  batch 73 loss: 0.3078211660254492
  batch 74 loss: 0.30796300398336873
  batch 75 loss: 0.3075486222902934
  batch 76 loss: 0.30822456079094035
  batch 77 loss: 0.3082960429903749
  batch 78 loss: 0.30826244484155607
  batch 79 loss: 0.308603000791767
  batch 80 loss: 0.30860442146658895
  batch 81 loss: 0.308777814662015
  batch 82 loss: 0.3092438811209144
  batch 83 loss: 0.30918617815856475
  batch 84 loss: 0.3092428293256533
  batch 85 loss: 0.3091550248510697
  batch 86 loss: 0.30963780089866283
  batch 87 loss: 0.30952049671918497
  batch 88 loss: 0.3090495125136592
  batch 89 loss: 0.3088489573323325
  batch 90 loss: 0.3087591525581148
  batch 91 loss: 0.30880141028991115
  batch 92 loss: 0.3087517405333726
  batch 93 loss: 0.30875999857020636
  batch 94 loss: 0.3088643576236481
  batch 95 loss: 0.30857984329524796
  batch 96 loss: 0.30864993513872224
  batch 97 loss: 0.3091299011535251
  batch 98 loss: 0.3093562412018679
  batch 99 loss: 0.3093448088626669
  batch 100 loss: 0.30919319689273833
  batch 101 loss: 0.30918585900032874
  batch 102 loss: 0.30936798950036365
  batch 103 loss: 0.3097410175985503
  batch 104 loss: 0.3098677654679005
  batch 105 loss: 0.309353081101463
  batch 106 loss: 0.3099452071594742
  batch 107 loss: 0.3094832640942012
  batch 108 loss: 0.309514120221138
  batch 109 loss: 0.30941319082855084
  batch 110 loss: 0.30941854769533333
  batch 111 loss: 0.3093207054310017
  batch 112 loss: 0.3090671207755804
  batch 113 loss: 0.30960843784619224
  batch 114 loss: 0.30981291438403885
  batch 115 loss: 0.310010055873705
  batch 116 loss: 0.3101047161838104
  batch 117 loss: 0.31037741364576876
  batch 118 loss: 0.31049394077163633
  batch 119 loss: 0.3107160883290427
  batch 120 loss: 0.3105409366389116
  batch 121 loss: 0.3104663587306157
  batch 122 loss: 0.3104990056303681
  batch 123 loss: 0.31017254789670307
  batch 124 loss: 0.3106954734652273
  batch 125 loss: 0.31087967205047606
  batch 126 loss: 0.31093239074661616
  batch 127 loss: 0.31141087905628473
  batch 128 loss: 0.3114394142758101
  batch 129 loss: 0.3116419959899991
  batch 130 loss: 0.3118982172929324
  batch 131 loss: 0.31215828725399863
  batch 132 loss: 0.312188483774662
  batch 133 loss: 0.31227338493318485
  batch 134 loss: 0.3122668219591255
  batch 135 loss: 0.3121778896561375
  batch 136 loss: 0.3122077465933912
  batch 137 loss: 0.31208303679514976
  batch 138 loss: 0.3122097033521403
  batch 139 loss: 0.3126337487491772
  batch 140 loss: 0.3124726789338248
  batch 141 loss: 0.31257557023501564
  batch 142 loss: 0.31251560067626794
  batch 143 loss: 0.3124168965366337
  batch 144 loss: 0.3123374978701274
  batch 145 loss: 0.3122474072308376
  batch 146 loss: 0.31223251415442116
  batch 147 loss: 0.3124340093460213
  batch 148 loss: 0.3123569550949174
  batch 149 loss: 0.31209335291145635
  batch 150 loss: 0.31195307632287345
  batch 151 loss: 0.312024297106345
  batch 152 loss: 0.3120610433581628
  batch 153 loss: 0.3119876922345629
  batch 154 loss: 0.3122225889524856
  batch 155 loss: 0.3122445279552091
  batch 156 loss: 0.31220374810389984
  batch 157 loss: 0.31246609065183406
  batch 158 loss: 0.31258139957355546
  batch 159 loss: 0.3125082402484222
  batch 160 loss: 0.3123454885557294
  batch 161 loss: 0.3124014279857185
  batch 162 loss: 0.3123336206247777
  batch 163 loss: 0.3121258994918659
  batch 164 loss: 0.31238250979563087
  batch 165 loss: 0.3121564249197642
  batch 166 loss: 0.31204244853502294
  batch 167 loss: 0.311887739304297
  batch 168 loss: 0.31184059913669315
  batch 169 loss: 0.3115292494819009
  batch 170 loss: 0.31133298821309036
  batch 171 loss: 0.3113690717527044
  batch 172 loss: 0.31144582064345827
  batch 173 loss: 0.311368683859103
  batch 174 loss: 0.31148135011223543
  batch 175 loss: 0.3115702678476061
  batch 176 loss: 0.31143317168409174
  batch 177 loss: 0.3113557536386501
  batch 178 loss: 0.3115240317047312
  batch 179 loss: 0.31166102183597716
  batch 180 loss: 0.31156604670815996
  batch 181 loss: 0.31149273946140354
  batch 182 loss: 0.3113819400032798
  batch 183 loss: 0.31119495434839217
  batch 184 loss: 0.31104943561165227
  batch 185 loss: 0.3110039607898609
  batch 186 loss: 0.31121018881438883
  batch 187 loss: 0.3111402712722513
  batch 188 loss: 0.3109566463117904
  batch 189 loss: 0.3107988448054702
  batch 190 loss: 0.3108023243515115
  batch 191 loss: 0.3107806679778074
  batch 192 loss: 0.31089345070843893
  batch 193 loss: 0.31091905721111
  batch 194 loss: 0.3111460374802658
  batch 195 loss: 0.3112802832554548
  batch 196 loss: 0.3110394823003788
  batch 197 loss: 0.31088832459473975
  batch 198 loss: 0.3109583990140395
  batch 199 loss: 0.31109004374125493
  batch 200 loss: 0.3112183517217636
  batch 201 loss: 0.3112038658626044
  batch 202 loss: 0.311178845049131
  batch 203 loss: 0.3112755062544874
  batch 204 loss: 0.3110545623828383
  batch 205 loss: 0.31115334193880967
  batch 206 loss: 0.3112136181986448
  batch 207 loss: 0.31121510857545237
  batch 208 loss: 0.31108782211175334
  batch 209 loss: 0.310805831657072
  batch 210 loss: 0.3109146292720522
  batch 211 loss: 0.31086922878337697
  batch 212 loss: 0.31091016474759803
  batch 213 loss: 0.31088998955740055
  batch 214 loss: 0.31094934018415826
  batch 215 loss: 0.3106878682624462
  batch 216 loss: 0.31067793029877877
  batch 217 loss: 0.310655115936209
  batch 218 loss: 0.3105891293615376
  batch 219 loss: 0.3106398853264987
  batch 220 loss: 0.3106877152215351
  batch 221 loss: 0.3107601858641767
  batch 222 loss: 0.31081252350463523
  batch 223 loss: 0.3108247342131063
  batch 224 loss: 0.3108665831387043
  batch 225 loss: 0.31078158299128217
  batch 226 loss: 0.3108153817927943
  batch 227 loss: 0.3106227616906691
  batch 228 loss: 0.3105682141163893
  batch 229 loss: 0.3104623593878017
  batch 230 loss: 0.31051212976808135
  batch 231 loss: 0.31051432647746363
  batch 232 loss: 0.31033589857919464
  batch 233 loss: 0.310336978906214
  batch 234 loss: 0.3104965263961727
  batch 235 loss: 0.3105148752953144
  batch 236 loss: 0.3104495623354184
  batch 237 loss: 0.31053590170944795
  batch 238 loss: 0.31043265396807374
  batch 239 loss: 0.31024954620764345
  batch 240 loss: 0.31029871193071207
  batch 241 loss: 0.31034812479592955
  batch 242 loss: 0.31017085626598234
  batch 243 loss: 0.31014220471735354
  batch 244 loss: 0.3100314135434198
  batch 245 loss: 0.31003618203863803
  batch 246 loss: 0.31000444680694644
  batch 247 loss: 0.3101951499216952
  batch 248 loss: 0.3101893056544565
  batch 249 loss: 0.3101135164379595
  batch 250 loss: 0.3101198316812515
  batch 251 loss: 0.3100896425456165
  batch 252 loss: 0.31001664272376467
  batch 253 loss: 0.309862081123435
  batch 254 loss: 0.3097501845341029
  batch 255 loss: 0.30981807708740233
  batch 256 loss: 0.30979915347415954
  batch 257 loss: 0.3098812614665421
  batch 258 loss: 0.3099119757959085
  batch 259 loss: 0.3099069324929742
  batch 260 loss: 0.3099098793589152
  batch 261 loss: 0.3098860228883809
  batch 262 loss: 0.30967834580490605
  batch 263 loss: 0.3096794420322085
  batch 264 loss: 0.3095399083738977
  batch 265 loss: 0.3093814754261161
  batch 266 loss: 0.30934813260135796
  batch 267 loss: 0.3094901354348615
  batch 268 loss: 0.309412171480371
  batch 269 loss: 0.3093434849193105
  batch 270 loss: 0.3093908358503271
  batch 271 loss: 0.3093530368541
  batch 272 loss: 0.30935922462274046
  batch 273 loss: 0.30933224303381784
  batch 274 loss: 0.30941318443221766
  batch 275 loss: 0.30949606592004947
  batch 276 loss: 0.30946873247191525
  batch 277 loss: 0.30944305109633435
  batch 278 loss: 0.3092940154907515
  batch 279 loss: 0.3093048714395065
  batch 280 loss: 0.30912496160183633
  batch 281 loss: 0.3090542244105152
  batch 282 loss: 0.3091047090207431
  batch 283 loss: 0.30910346775509867
  batch 284 loss: 0.30912798065954533
  batch 285 loss: 0.3090845867207176
  batch 286 loss: 0.3090441288439544
  batch 287 loss: 0.3090390832997365
  batch 288 loss: 0.3088465909370118
  batch 289 loss: 0.30897402526185586
  batch 290 loss: 0.30872762398473147
  batch 291 loss: 0.30869778468436804
  batch 292 loss: 0.30872184793426566
  batch 293 loss: 0.3086516530643958
  batch 294 loss: 0.3085273920881505
  batch 295 loss: 0.3084730755474608
  batch 296 loss: 0.308482074657002
  batch 297 loss: 0.3084465235170692
  batch 298 loss: 0.3084525584974545
  batch 299 loss: 0.30844998678634794
  batch 300 loss: 0.3085176283121109
  batch 301 loss: 0.3084923803212239
  batch 302 loss: 0.30849470829726844
  batch 303 loss: 0.3085278957196982
  batch 304 loss: 0.3084301379950423
  batch 305 loss: 0.3082432569050398
  batch 306 loss: 0.30832702242860605
  batch 307 loss: 0.3082301209920392
  batch 308 loss: 0.3082611915933621
  batch 309 loss: 0.3082229676370096
  batch 310 loss: 0.3081756914815595
  batch 311 loss: 0.30829145747365677
  batch 312 loss: 0.30839679915553486
  batch 313 loss: 0.3084919573590397
  batch 314 loss: 0.30847937789312596
  batch 315 loss: 0.3084218908870031
  batch 316 loss: 0.3082356939115856
  batch 317 loss: 0.3082500870577544
  batch 318 loss: 0.30822794856327884
  batch 319 loss: 0.3081255164172582
  batch 320 loss: 0.3081008774694055
  batch 321 loss: 0.3080690396816188
  batch 322 loss: 0.3080400322442469
  batch 323 loss: 0.30801557170532806
  batch 324 loss: 0.30782981161718015
  batch 325 loss: 0.3077565617744739
  batch 326 loss: 0.30772557187299787
  batch 327 loss: 0.30776044334475783
  batch 328 loss: 0.30761182762500716
  batch 329 loss: 0.30771249143185947
  batch 330 loss: 0.3076453171896212
  batch 331 loss: 0.3076020895174263
  batch 332 loss: 0.3075036304183753
  batch 333 loss: 0.3074850993113475
  batch 334 loss: 0.3073469130935783
  batch 335 loss: 0.3071765495770013
  batch 336 loss: 0.30706309163499446
  batch 337 loss: 0.3068685065568024
  batch 338 loss: 0.30687541995175494
  batch 339 loss: 0.30691122991145536
  batch 340 loss: 0.3068916496985099
  batch 341 loss: 0.3067509982942486
  batch 342 loss: 0.3066841383949358
  batch 343 loss: 0.3066665458088366
  batch 344 loss: 0.30666208960289176
  batch 345 loss: 0.30677191373230756
  batch 346 loss: 0.30666641374199377
  batch 347 loss: 0.3066486404505518
  batch 348 loss: 0.3067141940881466
  batch 349 loss: 0.30659369027033917
  batch 350 loss: 0.3066250317437308
  batch 351 loss: 0.3066075411107805
  batch 352 loss: 0.30674595648253505
  batch 353 loss: 0.3067000138523221
  batch 354 loss: 0.3068217279042228
  batch 355 loss: 0.3067983926182062
  batch 356 loss: 0.306822695256619
  batch 357 loss: 0.3067294616325229
  batch 358 loss: 0.30672270958650044
  batch 359 loss: 0.30669661318691327
  batch 360 loss: 0.3066522998114427
  batch 361 loss: 0.3066012088281626
  batch 362 loss: 0.30650175391639795
  batch 363 loss: 0.30636069482351463
  batch 364 loss: 0.30630778382112694
  batch 365 loss: 0.30632243327898523
  batch 366 loss: 0.3062416469790245
  batch 367 loss: 0.30611547941407974
  batch 368 loss: 0.3060070723295212
  batch 369 loss: 0.3060104336518905
  batch 370 loss: 0.30604224712462036
  batch 371 loss: 0.3060389748320104
  batch 372 loss: 0.30598257385915323
  batch 373 loss: 0.30590929110951465
  batch 374 loss: 0.30581662967243295
  batch 375 loss: 0.3057575798034668
  batch 376 loss: 0.3058752681505173
  batch 377 loss: 0.3058773442826157
  batch 378 loss: 0.3057954458017198
  batch 379 loss: 0.3058343701123562
  batch 380 loss: 0.30587824834020516
  batch 381 loss: 0.30579024952227674
  batch 382 loss: 0.30569129370894105
  batch 383 loss: 0.30565522249311444
  batch 384 loss: 0.3055973171722144
  batch 385 loss: 0.3057044726687592
  batch 386 loss: 0.30560425170962674
  batch 387 loss: 0.3056349641900962
  batch 388 loss: 0.3056606923028366
  batch 389 loss: 0.30567075270000643
  batch 390 loss: 0.3056469696454513
  batch 391 loss: 0.30566254707858387
  batch 392 loss: 0.30568418971129824
  batch 393 loss: 0.3057844825373351
  batch 394 loss: 0.30579252783114536
  batch 395 loss: 0.3057127758671966
  batch 396 loss: 0.30574679020980394
  batch 397 loss: 0.3057605095714406
  batch 398 loss: 0.30573009785695293
  batch 399 loss: 0.30583602765150236
  batch 400 loss: 0.30586164705455304
  batch 401 loss: 0.30574682139399045
  batch 402 loss: 0.3057945946704096
  batch 403 loss: 0.30583959388969556
  batch 404 loss: 0.3058784908734926
  batch 405 loss: 0.305899245503508
  batch 406 loss: 0.3059678984980278
  batch 407 loss: 0.3059571973375372
  batch 408 loss: 0.3060517728182615
  batch 409 loss: 0.3060698324864534
  batch 410 loss: 0.30611565447435146
  batch 411 loss: 0.30608352054354626
  batch 412 loss: 0.3060047123038653
  batch 413 loss: 0.3060216576077459
  batch 414 loss: 0.3060133258501689
  batch 415 loss: 0.30600845785026093
  batch 416 loss: 0.3060055784881115
  batch 417 loss: 0.3060114740324821
  batch 418 loss: 0.3059725471946041
  batch 419 loss: 0.3059766786906486
  batch 420 loss: 0.3060090233172689
  batch 421 loss: 0.30598012476522396
  batch 422 loss: 0.30607215664680537
  batch 423 loss: 0.30602657773815994
  batch 424 loss: 0.30596599727869034
  batch 425 loss: 0.305983228262733
  batch 426 loss: 0.3058949520750225
  batch 427 loss: 0.3058858501269052
  batch 428 loss: 0.30582566726430555
  batch 429 loss: 0.3058536497307268
  batch 430 loss: 0.3057885569195415
  batch 431 loss: 0.30594025853061896
  batch 432 loss: 0.3060181931489044
  batch 433 loss: 0.30599154074252616
  batch 434 loss: 0.30601567181024686
  batch 435 loss: 0.3059434868823523
  batch 436 loss: 0.3058854867832376
  batch 437 loss: 0.30592281165057517
  batch 438 loss: 0.3060173919620035
  batch 439 loss: 0.3059752669720009
  batch 440 loss: 0.30598547756671907
  batch 441 loss: 0.30592160295196674
  batch 442 loss: 0.30588494881785294
  batch 443 loss: 0.30586691210824263
  batch 444 loss: 0.30573377956275466
  batch 445 loss: 0.30577604532911534
  batch 446 loss: 0.3057435528112099
  batch 447 loss: 0.3056530367408023
  batch 448 loss: 0.30560198748883394
  batch 449 loss: 0.30565506455494723
  batch 450 loss: 0.3056220804651578
  batch 451 loss: 0.3056221298063938
  batch 452 loss: 0.30561110921270024
  batch 453 loss: 0.30565359441781415
  batch 454 loss: 0.3057456950981186
  batch 455 loss: 0.3057884986911501
  batch 456 loss: 0.3058085556942643
  batch 457 loss: 0.30585114657878876
  batch 458 loss: 0.30582682583399734
  batch 459 loss: 0.3058801994928867
  batch 460 loss: 0.30597088586377064
  batch 461 loss: 0.3059753421653636
  batch 462 loss: 0.3060196628108685
  batch 463 loss: 0.3059617217557734
  batch 464 loss: 0.30596958508650807
  batch 465 loss: 0.30591690511472763
  batch 466 loss: 0.30577227898549625
  batch 467 loss: 0.30582346934705523
  batch 468 loss: 0.3057996329143007
  batch 469 loss: 0.3059421847306335
  batch 470 loss: 0.3059009939115098
  batch 471 loss: 0.3058928194002994
  batch 472 loss: 0.305781692487456
LOSS train 0.305781692487456 valid 0.21880485117435455
LOSS train 0.305781692487456 valid 0.21344802528619766
LOSS train 0.305781692487456 valid 0.21410725017388663
LOSS train 0.305781692487456 valid 0.20836782827973366
LOSS train 0.305781692487456 valid 0.20181281864643097
LOSS train 0.305781692487456 valid 0.20664768666028976
LOSS train 0.305781692487456 valid 0.21440584531852178
LOSS train 0.305781692487456 valid 0.21329070627689362
LOSS train 0.305781692487456 valid 0.21458053920004103
LOSS train 0.305781692487456 valid 0.21635252088308335
LOSS train 0.305781692487456 valid 0.2140202061696486
LOSS train 0.305781692487456 valid 0.21491331855456033
LOSS train 0.305781692487456 valid 0.21517377747939184
LOSS train 0.305781692487456 valid 0.21564240327903203
LOSS train 0.305781692487456 valid 0.21310919324556987
LOSS train 0.305781692487456 valid 0.2131202071905136
LOSS train 0.305781692487456 valid 0.21289359296069427
LOSS train 0.305781692487456 valid 0.2141874838206503
LOSS train 0.305781692487456 valid 0.2164665782137921
LOSS train 0.305781692487456 valid 0.21564425379037858
LOSS train 0.305781692487456 valid 0.2146391293832234
LOSS train 0.305781692487456 valid 0.2135364982214841
LOSS train 0.305781692487456 valid 0.21380404415337936
LOSS train 0.305781692487456 valid 0.21272356684009233
LOSS train 0.305781692487456 valid 0.2121083915233612
LOSS train 0.305781692487456 valid 0.2126867129252507
LOSS train 0.305781692487456 valid 0.2125947712748139
LOSS train 0.305781692487456 valid 0.21306666252868517
LOSS train 0.305781692487456 valid 0.2138780077983593
LOSS train 0.305781692487456 valid 0.21515746812025707
LOSS train 0.305781692487456 valid 0.21627575351345923
LOSS train 0.305781692487456 valid 0.2162750749848783
LOSS train 0.305781692487456 valid 0.21690356460484592
LOSS train 0.305781692487456 valid 0.217259382500368
LOSS train 0.305781692487456 valid 0.21848110471452986
LOSS train 0.305781692487456 valid 0.21814636141061783
LOSS train 0.305781692487456 valid 0.21810266496361913
LOSS train 0.305781692487456 valid 0.21915392381580254
LOSS train 0.305781692487456 valid 0.21864128074584863
LOSS train 0.305781692487456 valid 0.2188720192760229
LOSS train 0.305781692487456 valid 0.21949067602797254
LOSS train 0.305781692487456 valid 0.21951512318281902
LOSS train 0.305781692487456 valid 0.21944431752659554
LOSS train 0.305781692487456 valid 0.21977858313105322
LOSS train 0.305781692487456 valid 0.21937481827206082
LOSS train 0.305781692487456 valid 0.21997282686440842
LOSS train 0.305781692487456 valid 0.22029809400122216
LOSS train 0.305781692487456 valid 0.22009520139545202
LOSS train 0.305781692487456 valid 0.220392074816081
LOSS train 0.305781692487456 valid 0.21984820038080216
LOSS train 0.305781692487456 valid 0.22011861993986018
LOSS train 0.305781692487456 valid 0.2198370752426294
LOSS train 0.305781692487456 valid 0.22012849658165337
LOSS train 0.305781692487456 valid 0.21982161893888755
LOSS train 0.305781692487456 valid 0.2201134817166762
LOSS train 0.305781692487456 valid 0.21976159193686076
LOSS train 0.305781692487456 valid 0.2194582221277973
LOSS train 0.305781692487456 valid 0.21906043974490003
LOSS train 0.305781692487456 valid 0.21956643714743146
LOSS train 0.305781692487456 valid 0.21927712460358936
LOSS train 0.305781692487456 valid 0.21945832010175362
LOSS train 0.305781692487456 valid 0.22036041030960699
LOSS train 0.305781692487456 valid 0.22046600518718598
LOSS train 0.305781692487456 valid 0.22107311780564487
LOSS train 0.305781692487456 valid 0.22137370040783516
LOSS train 0.305781692487456 valid 0.22145106507973236
LOSS train 0.305781692487456 valid 0.22098231315612793
LOSS train 0.305781692487456 valid 0.22092002345358625
LOSS train 0.305781692487456 valid 0.2201804760573567
LOSS train 0.305781692487456 valid 0.22040774609361377
LOSS train 0.305781692487456 valid 0.22044442300225647
LOSS train 0.305781692487456 valid 0.22074716641671127
LOSS train 0.305781692487456 valid 0.22074688521966543
LOSS train 0.305781692487456 valid 0.22061803433540705
LOSS train 0.305781692487456 valid 0.22051651219526927
LOSS train 0.305781692487456 valid 0.22092123741382047
LOSS train 0.305781692487456 valid 0.2208632232693883
LOSS train 0.305781692487456 valid 0.2209167113670936
LOSS train 0.305781692487456 valid 0.22107138513009758
LOSS train 0.305781692487456 valid 0.22046148758381606
LOSS train 0.305781692487456 valid 0.21962109796794843
LOSS train 0.305781692487456 valid 0.21965856395843553
LOSS train 0.305781692487456 valid 0.21958874632795172
LOSS train 0.305781692487456 valid 0.21944057356034005
LOSS train 0.305781692487456 valid 0.21911643129937788
LOSS train 0.305781692487456 valid 0.21853097666834675
LOSS train 0.305781692487456 valid 0.2186898368185964
LOSS train 0.305781692487456 valid 0.21829599446871065
LOSS train 0.305781692487456 valid 0.2185111233357633
LOSS train 0.305781692487456 valid 0.21861530045668284
LOSS train 0.305781692487456 valid 0.21883805914894566
LOSS train 0.305781692487456 valid 0.21895727936340414
LOSS train 0.305781692487456 valid 0.21873757787930068
LOSS train 0.305781692487456 valid 0.21888492161289175
LOSS train 0.305781692487456 valid 0.21849187455679242
LOSS train 0.305781692487456 valid 0.2186310935455064
LOSS train 0.305781692487456 valid 0.21883409769879175
LOSS train 0.305781692487456 valid 0.21891954039432565
LOSS train 0.305781692487456 valid 0.21905695368545225
LOSS train 0.305781692487456 valid 0.21939934343099593
LOSS train 0.305781692487456 valid 0.21977682721496808
LOSS train 0.305781692487456 valid 0.21990417791347877
LOSS train 0.305781692487456 valid 0.21996856155326067
LOSS train 0.305781692487456 valid 0.21999253977376682
LOSS train 0.305781692487456 valid 0.22008783036754245
LOSS train 0.305781692487456 valid 0.22031420862899637
LOSS train 0.305781692487456 valid 0.22004327428675144
LOSS train 0.305781692487456 valid 0.22018650477683102
LOSS train 0.305781692487456 valid 0.22057775838659444
LOSS train 0.305781692487456 valid 0.22094240053133532
LOSS train 0.305781692487456 valid 0.22060318546252208
LOSS train 0.305781692487456 valid 0.22035331784614495
LOSS train 0.305781692487456 valid 0.2203943469619329
LOSS train 0.305781692487456 valid 0.22024338431002802
LOSS train 0.305781692487456 valid 0.22015800476074218
LOSS train 0.305781692487456 valid 0.22041037820022683
LOSS train 0.305781692487456 valid 0.22063077807935894
LOSS train 0.305781692487456 valid 0.2205374334323204
LOSS train 0.305781692487456 valid 0.22053413593969426
LOSS train 0.305781692487456 valid 0.22022585247953733
LOSS train 0.305781692487456 valid 0.22002412538883115
LOSS train 0.305781692487456 valid 0.2198796421289444
LOSS train 0.305781692487456 valid 0.21996319560500666
LOSS train 0.305781692487456 valid 0.22007246796161897
LOSS train 0.305781692487456 valid 0.22008032298088073
LOSS train 0.305781692487456 valid 0.22021752938864722
LOSS train 0.305781692487456 valid 0.22012222505460574
LOSS train 0.305781692487456 valid 0.2203590018907562
LOSS train 0.305781692487456 valid 0.22037009409693784
LOSS train 0.305781692487456 valid 0.22040611001161428
LOSS train 0.305781692487456 valid 0.22036193697962142
LOSS train 0.305781692487456 valid 0.22013889196695705
LOSS train 0.305781692487456 valid 0.21990121497695608
LOSS train 0.305781692487456 valid 0.21995087045786985
LOSS train 0.305781692487456 valid 0.21994570328129662
LOSS train 0.305781692487456 valid 0.21985706095309818
LOSS train 0.305781692487456 valid 0.21980463022733257
LOSS train 0.305781692487456 valid 0.2197798066165136
LOSS train 0.305781692487456 valid 0.21963596129588944
LOSS train 0.305781692487456 valid 0.21971291580370494
LOSS train 0.305781692487456 valid 0.2197547772674696
LOSS train 0.305781692487456 valid 0.21995350078377926
LOSS train 0.305781692487456 valid 0.21999519406915544
LOSS train 0.305781692487456 valid 0.2200629607670837
LOSS train 0.305781692487456 valid 0.21989593248942801
LOSS train 0.305781692487456 valid 0.22013712735616997
LOSS train 0.305781692487456 valid 0.2198800723366186
LOSS train 0.305781692487456 valid 0.2205541128846439
LOSS train 0.305781692487456 valid 0.2205492707106891
LOSS train 0.305781692487456 valid 0.22054066121578217
LOSS train 0.305781692487456 valid 0.22053816756665312
LOSS train 0.305781692487456 valid 0.22047213749273828
LOSS train 0.305781692487456 valid 0.22050438139563291
LOSS train 0.305781692487456 valid 0.22055890633688344
LOSS train 0.305781692487456 valid 0.22062303856495888
LOSS train 0.305781692487456 valid 0.22099738367474997
LOSS train 0.305781692487456 valid 0.22093537231539465
LOSS train 0.305781692487456 valid 0.22107670248686512
LOSS train 0.305781692487456 valid 0.2210961695549623
LOSS train 0.305781692487456 valid 0.22088834727182985
LOSS train 0.305781692487456 valid 0.22083594663913206
LOSS train 0.305781692487456 valid 0.22080067738338752
LOSS train 0.305781692487456 valid 0.22077649155277415
LOSS train 0.305781692487456 valid 0.22066971278045236
LOSS train 0.305781692487456 valid 0.22050493675650973
LOSS train 0.305781692487456 valid 0.22061941799629164
LOSS train 0.305781692487456 valid 0.2207257551347424
LOSS train 0.305781692487456 valid 0.22064759493583724
LOSS train 0.305781692487456 valid 0.22071313258458877
LOSS train 0.305781692487456 valid 0.2208853107164888
LOSS train 0.305781692487456 valid 0.2208181540345588
LOSS train 0.305781692487456 valid 0.22061038337821184
LOSS train 0.305781692487456 valid 0.22067339373806308
LOSS train 0.305781692487456 valid 0.22059102279358897
LOSS train 0.305781692487456 valid 0.22058715658528463
LOSS train 0.305781692487456 valid 0.22051315669986335
LOSS train 0.305781692487456 valid 0.22047250966231027
LOSS train 0.305781692487456 valid 0.22073549291725908
LOSS train 0.305781692487456 valid 0.22071783585921345
LOSS train 0.305781692487456 valid 0.22077612611982558
LOSS train 0.305781692487456 valid 0.22099644340862884
LOSS train 0.305781692487456 valid 0.2210691225725216
LOSS train 0.305781692487456 valid 0.22095785603497198
LOSS train 0.305781692487456 valid 0.22096083021682242
LOSS train 0.305781692487456 valid 0.22081026950398008
LOSS train 0.305781692487456 valid 0.22074258399586524
LOSS train 0.305781692487456 valid 0.22060040659445493
LOSS train 0.305781692487456 valid 0.22062092654882592
LOSS train 0.305781692487456 valid 0.22071696147716865
LOSS train 0.305781692487456 valid 0.22075347861177044
LOSS train 0.305781692487456 valid 0.2206481912061182
LOSS train 0.305781692487456 valid 0.2205959144824495
LOSS train 0.305781692487456 valid 0.22067154310836692
LOSS train 0.305781692487456 valid 0.22055955223508716
LOSS train 0.305781692487456 valid 0.2203134033160332
LOSS train 0.305781692487456 valid 0.2204097315517007
LOSS train 0.305781692487456 valid 0.22048430386836154
LOSS train 0.305781692487456 valid 0.22036026704190959
LOSS train 0.305781692487456 valid 0.22033579565172817
LOSS train 0.305781692487456 valid 0.22011328287422657
LOSS train 0.305781692487456 valid 0.21996701104723992
LOSS train 0.305781692487456 valid 0.21985225313075699
LOSS train 0.305781692487456 valid 0.21976007594557231
LOSS train 0.305781692487456 valid 0.2197016271598199
LOSS train 0.305781692487456 valid 0.21956348884396437
LOSS train 0.305781692487456 valid 0.21959082058911183
LOSS train 0.305781692487456 valid 0.2195346198508129
LOSS train 0.305781692487456 valid 0.2194839448739703
LOSS train 0.305781692487456 valid 0.21931925337565572
LOSS train 0.305781692487456 valid 0.21935520697207678
LOSS train 0.305781692487456 valid 0.21936871641055103
LOSS train 0.305781692487456 valid 0.21921393401780218
LOSS train 0.305781692487456 valid 0.21930257157540659
LOSS train 0.305781692487456 valid 0.21921453806005906
LOSS train 0.305781692487456 valid 0.21911852976610494
LOSS train 0.305781692487456 valid 0.2190552460098708
LOSS train 0.305781692487456 valid 0.2190392797443724
LOSS train 0.305781692487456 valid 0.21903989224805745
LOSS train 0.305781692487456 valid 0.21911299439597892
LOSS train 0.305781692487456 valid 0.21924331364306537
LOSS train 0.305781692487456 valid 0.2193281773258658
LOSS train 0.305781692487456 valid 0.21927510886578946
LOSS train 0.305781692487456 valid 0.2193521026672269
LOSS train 0.305781692487456 valid 0.21951811048867448
LOSS train 0.305781692487456 valid 0.21954274872938792
LOSS train 0.305781692487456 valid 0.2195747956897305
LOSS train 0.305781692487456 valid 0.21983692095930882
LOSS train 0.305781692487456 valid 0.2199475050233958
LOSS train 0.305781692487456 valid 0.21996399161597005
LOSS train 0.305781692487456 valid 0.22011097695516504
LOSS train 0.305781692487456 valid 0.22012151384250425
LOSS train 0.305781692487456 valid 0.22017378212306007
LOSS train 0.305781692487456 valid 0.2200674219576586
LOSS train 0.305781692487456 valid 0.22011454498920685
LOSS train 0.305781692487456 valid 0.22012820345290163
LOSS train 0.305781692487456 valid 0.22004118587000895
LOSS train 0.305781692487456 valid 0.22007080101514165
LOSS train 0.305781692487456 valid 0.2201043658521997
LOSS train 0.305781692487456 valid 0.21997069701490043
LOSS train 0.305781692487456 valid 0.21993815395981073
LOSS train 0.305781692487456 valid 0.220049171526897
LOSS train 0.305781692487456 valid 0.21986794804245974
LOSS train 0.305781692487456 valid 0.22000556141751293
LOSS train 0.305781692487456 valid 0.22014754143406134
LOSS train 0.305781692487456 valid 0.2202473853315626
LOSS train 0.305781692487456 valid 0.22022521320154997
LOSS train 0.305781692487456 valid 0.2202447724607792
LOSS train 0.305781692487456 valid 0.22022752962525813
LOSS train 0.305781692487456 valid 0.22010363303274516
LOSS train 0.305781692487456 valid 0.22019095355272292
LOSS train 0.305781692487456 valid 0.22009855206031723
LOSS train 0.305781692487456 valid 0.2201746656071572
LOSS train 0.305781692487456 valid 0.22005698838723978
LOSS train 0.305781692487456 valid 0.22004017890907648
LOSS train 0.305781692487456 valid 0.22010425732416264
LOSS train 0.305781692487456 valid 0.22011064551770687
LOSS train 0.305781692487456 valid 0.22000910915753258
LOSS train 0.305781692487456 valid 0.2200625106807827
LOSS train 0.305781692487456 valid 0.22002468364579336
LOSS train 0.305781692487456 valid 0.2200451950614269
LOSS train 0.305781692487456 valid 0.22012923252536876
LOSS train 0.305781692487456 valid 0.2201371461380529
LOSS train 0.305781692487456 valid 0.2201134663344336
LOSS train 0.305781692487456 valid 0.2201348076941389
LOSS train 0.305781692487456 valid 0.2200804225678714
LOSS train 0.305781692487456 valid 0.22002514832674114
LOSS train 0.305781692487456 valid 0.22012791160340614
LOSS train 0.305781692487456 valid 0.22024796046872638
LOSS train 0.305781692487456 valid 0.22033888489340317
LOSS train 0.305781692487456 valid 0.22029798797987127
LOSS train 0.305781692487456 valid 0.2203361810463381
LOSS train 0.305781692487456 valid 0.22052851368618362
LOSS train 0.305781692487456 valid 0.2205741919113166
LOSS train 0.305781692487456 valid 0.22054928040852512
LOSS train 0.305781692487456 valid 0.22050331836396997
LOSS train 0.305781692487456 valid 0.22043613905923953
LOSS train 0.305781692487456 valid 0.22028552921024902
LOSS train 0.305781692487456 valid 0.22014431575005003
LOSS train 0.305781692487456 valid 0.2201571640788868
LOSS train 0.305781692487456 valid 0.22015029293085847
LOSS train 0.305781692487456 valid 0.22007881444332014
LOSS train 0.305781692487456 valid 0.21992660567481467
LOSS train 0.305781692487456 valid 0.2199807654105311
LOSS train 0.305781692487456 valid 0.2200863581203239
LOSS train 0.305781692487456 valid 0.2201431895557203
LOSS train 0.305781692487456 valid 0.2200154915868819
LOSS train 0.305781692487456 valid 0.2198915805550818
LOSS train 0.305781692487456 valid 0.21993849354071748
LOSS train 0.305781692487456 valid 0.21998638352927039
LOSS train 0.305781692487456 valid 0.21997141391038894
LOSS train 0.305781692487456 valid 0.21994943455936983
LOSS train 0.305781692487456 valid 0.21990102105965353
LOSS train 0.305781692487456 valid 0.2199210281461579
LOSS train 0.305781692487456 valid 0.2200004826394879
LOSS train 0.305781692487456 valid 0.2200791018999229
LOSS train 0.305781692487456 valid 0.22005230570967133
LOSS train 0.305781692487456 valid 0.22004869173873554
LOSS train 0.305781692487456 valid 0.22003009820344463
LOSS train 0.305781692487456 valid 0.2200503422065722
LOSS train 0.305781692487456 valid 0.22000832498073578
LOSS train 0.305781692487456 valid 0.2200401306548388
LOSS train 0.305781692487456 valid 0.2200703008088055
LOSS train 0.305781692487456 valid 0.2201312879229536
LOSS train 0.305781692487456 valid 0.22013412920856162
LOSS train 0.305781692487456 valid 0.22005752818506272
LOSS train 0.305781692487456 valid 0.22000454965175367
LOSS train 0.305781692487456 valid 0.21997372616774097
LOSS train 0.305781692487456 valid 0.21988732793501445
LOSS train 0.305781692487456 valid 0.22001039393511404
LOSS train 0.305781692487456 valid 0.2199570799546857
LOSS train 0.305781692487456 valid 0.21983427743628095
LOSS train 0.305781692487456 valid 0.21982314886573034
LOSS train 0.305781692487456 valid 0.219829966989569
LOSS train 0.305781692487456 valid 0.21989201526543137
LOSS train 0.305781692487456 valid 0.21989816375195034
LOSS train 0.305781692487456 valid 0.21983013019154343
LOSS train 0.305781692487456 valid 0.21985135084070995
LOSS train 0.305781692487456 valid 0.21989149525293014
LOSS train 0.305781692487456 valid 0.21994485502893274
LOSS train 0.305781692487456 valid 0.21989404964260756
LOSS train 0.305781692487456 valid 0.21991989522522484
LOSS train 0.305781692487456 valid 0.21989272358995046
LOSS train 0.305781692487456 valid 0.21985986229066878
LOSS train 0.305781692487456 valid 0.21982931244520493
LOSS train 0.305781692487456 valid 0.2198243954548469
LOSS train 0.305781692487456 valid 0.21997064013788306
LOSS train 0.305781692487456 valid 0.22004750908697052
LOSS train 0.305781692487456 valid 0.22000934001876088
LOSS train 0.305781692487456 valid 0.22013125856234309
LOSS train 0.305781692487456 valid 0.22011250614216835
LOSS train 0.305781692487456 valid 0.220051947272076
LOSS train 0.305781692487456 valid 0.21995853520480982
LOSS train 0.305781692487456 valid 0.21993590775015834
LOSS train 0.305781692487456 valid 0.22007105338894678
LOSS train 0.305781692487456 valid 0.22005772007935082
LOSS train 0.305781692487456 valid 0.2200765764074666
LOSS train 0.305781692487456 valid 0.21999344176637667
LOSS train 0.305781692487456 valid 0.2199752131510063
LOSS train 0.305781692487456 valid 0.21999261829353714
LOSS train 0.305781692487456 valid 0.22004703572567771
LOSS train 0.305781692487456 valid 0.2199669424826798
LOSS train 0.305781692487456 valid 0.21991601185491907
LOSS train 0.305781692487456 valid 0.2199396464550113
LOSS train 0.305781692487456 valid 0.22008329512941283
LOSS train 0.305781692487456 valid 0.22008531970390374
LOSS train 0.305781692487456 valid 0.21999807410329753
LOSS train 0.305781692487456 valid 0.21990986791055553
LOSS train 0.305781692487456 valid 0.21985987599553733
LOSS train 0.305781692487456 valid 0.21992286062001498
LOSS train 0.305781692487456 valid 0.21982618459633418
LOSS train 0.305781692487456 valid 0.21976759286857397
LOSS train 0.305781692487456 valid 0.2197624774734405
LOSS train 0.305781692487456 valid 0.2198037753361818
LOSS train 0.305781692487456 valid 0.2199010928977007
LOSS train 0.305781692487456 valid 0.2200161290840364
LOSS train 0.305781692487456 valid 0.22008944466040375
LOSS train 0.305781692487456 valid 0.22006029512534955
LOSS train 0.305781692487456 valid 0.22001372369308045
LOSS train 0.305781692487456 valid 0.22001113009984113
LOSS train 0.305781692487456 valid 0.2199444126751688
LOSS train 0.305781692487456 valid 0.21986401493859753
LOSS train 0.305781692487456 valid 0.21997060902540197
LOSS train 0.305781692487456 valid 0.21983537114848775
LOSS train 0.305781692487456 valid 0.21984468072980315
LOSS train 0.305781692487456 valid 0.2198041263100219
LOSS train 0.305781692487456 valid 0.21971729076136656
LOSS train 0.305781692487456 valid 0.21962824853629442
LOSS train 0.305781692487456 valid 0.21960579045116901
LOSS train 0.305781692487456 valid 0.219655886615518
EPOCH 5:
  batch 1 loss: 0.3024725317955017
  batch 2 loss: 0.28990694880485535
  batch 3 loss: 0.2908697724342346
  batch 4 loss: 0.28692761808633804
  batch 5 loss: 0.2942955732345581
  batch 6 loss: 0.29090776046117145
  batch 7 loss: 0.2951654664107731
  batch 8 loss: 0.2972032390534878
  batch 9 loss: 0.29841282963752747
  batch 10 loss: 0.30014713406562804
  batch 11 loss: 0.3003810264847495
  batch 12 loss: 0.29762905339399975
  batch 13 loss: 0.2971547291829036
  batch 14 loss: 0.2988827633006232
  batch 15 loss: 0.3006574789683024
  batch 16 loss: 0.2994711883366108
  batch 17 loss: 0.2963949152651955
  batch 18 loss: 0.2979207709431648
  batch 19 loss: 0.29710204742456736
  batch 20 loss: 0.2947873272001743
  batch 21 loss: 0.29563854776677634
  batch 22 loss: 0.2966078743338585
  batch 23 loss: 0.2954510495714519
  batch 24 loss: 0.2951351994027694
  batch 25 loss: 0.29789405524730683
  batch 26 loss: 0.2962430606667812
  batch 27 loss: 0.2963785954095699
  batch 28 loss: 0.2948381820959704
  batch 29 loss: 0.2952162525777159
  batch 30 loss: 0.2948295290271441
  batch 31 loss: 0.2956553836984019
  batch 32 loss: 0.2953004199080169
  batch 33 loss: 0.2946429600318273
  batch 34 loss: 0.2950170228586477
  batch 35 loss: 0.2957520174128669
  batch 36 loss: 0.29591769600907963
  batch 37 loss: 0.29663289761221084
  batch 38 loss: 0.297348936529536
  batch 39 loss: 0.29708077701238483
  batch 40 loss: 0.296890864148736
  batch 41 loss: 0.29683688173933725
  batch 42 loss: 0.2974590598827317
  batch 43 loss: 0.29720161508682164
  batch 44 loss: 0.297455930235711
  batch 45 loss: 0.29670701457394494
  batch 46 loss: 0.296412765007952
  batch 47 loss: 0.29653014876741046
  batch 48 loss: 0.295505459420383
  batch 49 loss: 0.29511931508171313
  batch 50 loss: 0.29487316697835925
  batch 51 loss: 0.2948628433194815
  batch 52 loss: 0.2950900073807973
  batch 53 loss: 0.2946339527953346
  batch 54 loss: 0.29451250064152257
  batch 55 loss: 0.29406367242336273
  batch 56 loss: 0.2937404505376305
  batch 57 loss: 0.2929851300360864
  batch 58 loss: 0.2939507974119022
  batch 59 loss: 0.2940180526446488
  batch 60 loss: 0.2937612367173036
  batch 61 loss: 0.2937079592317831
  batch 62 loss: 0.2937579791872732
  batch 63 loss: 0.2933900954704436
  batch 64 loss: 0.29421493760310113
  batch 65 loss: 0.29380100346528565
  batch 66 loss: 0.29351954049233236
  batch 67 loss: 0.2937919739911805
  batch 68 loss: 0.2941916797529249
  batch 69 loss: 0.2940456567035205
  batch 70 loss: 0.29434436580964496
  batch 71 loss: 0.29404068937603856
  batch 72 loss: 0.294354103712572
  batch 73 loss: 0.2944595166673399
  batch 74 loss: 0.29461466762665156
  batch 75 loss: 0.2940841740369797
  batch 76 loss: 0.29464136318940864
  batch 77 loss: 0.29455348520309893
  batch 78 loss: 0.2944926495353381
  batch 79 loss: 0.2948165609489513
  batch 80 loss: 0.2947986816987395
  batch 81 loss: 0.2948901449088697
  batch 82 loss: 0.2953182795062298
  batch 83 loss: 0.29544444274471465
  batch 84 loss: 0.2955827945399852
  batch 85 loss: 0.2953716441112406
  batch 86 loss: 0.2955717934772026
  batch 87 loss: 0.29548763663604344
  batch 88 loss: 0.29496124166656623
  batch 89 loss: 0.2949021751626154
  batch 90 loss: 0.29474117507537206
  batch 91 loss: 0.29487531673122236
  batch 92 loss: 0.29462314640050347
  batch 93 loss: 0.29446311423214533
  batch 94 loss: 0.2945357800481167
  batch 95 loss: 0.2944652590312456
  batch 96 loss: 0.2945976103655994
  batch 97 loss: 0.2952528861687355
  batch 98 loss: 0.2955255470409685
  batch 99 loss: 0.29561527494830314
  batch 100 loss: 0.2955493859946728
  batch 101 loss: 0.2955917336858145
  batch 102 loss: 0.2959568195191084
  batch 103 loss: 0.2962755426330474
  batch 104 loss: 0.2963477142441731
  batch 105 loss: 0.2960583268176942
  batch 106 loss: 0.29656144259673245
  batch 107 loss: 0.2960156337401577
  batch 108 loss: 0.2960055063444155
  batch 109 loss: 0.29588254917105405
  batch 110 loss: 0.29586367268453945
  batch 111 loss: 0.2958600509005624
  batch 112 loss: 0.29553982708603144
  batch 113 loss: 0.2960438109868396
  batch 114 loss: 0.29653699604565636
  batch 115 loss: 0.2966245447811873
  batch 116 loss: 0.2968327548226406
  batch 117 loss: 0.29707655934696525
  batch 118 loss: 0.2969216776348777
  batch 119 loss: 0.2971878471244283
  batch 120 loss: 0.2971926183750232
  batch 121 loss: 0.29690265864872734
  batch 122 loss: 0.2967713442249376
  batch 123 loss: 0.2966096211012786
  batch 124 loss: 0.2969851329201652
  batch 125 loss: 0.29715994775295257
  batch 126 loss: 0.29720472780958057
  batch 127 loss: 0.2976722868643408
  batch 128 loss: 0.2976802826160565
  batch 129 loss: 0.2979390026293984
  batch 130 loss: 0.2981546277037034
  batch 131 loss: 0.29843399463264086
  batch 132 loss: 0.2985853368811535
  batch 133 loss: 0.29869287029693004
  batch 134 loss: 0.2987159082471435
  batch 135 loss: 0.2987461594519792
  batch 136 loss: 0.29875651692204613
  batch 137 loss: 0.2986835209775145
  batch 138 loss: 0.29888270007095474
  batch 139 loss: 0.29945258303083105
  batch 140 loss: 0.2994284950196743
  batch 141 loss: 0.2995040159910283
  batch 142 loss: 0.2993616611814835
  batch 143 loss: 0.299361794457569
  batch 144 loss: 0.2992474568179912
  batch 145 loss: 0.2991878015213999
  batch 146 loss: 0.29911047332499124
  batch 147 loss: 0.2994053054626296
  batch 148 loss: 0.2994074834561026
  batch 149 loss: 0.2992638232523963
  batch 150 loss: 0.2990224392215411
  batch 151 loss: 0.2988947776966537
  batch 152 loss: 0.29902451848121064
  batch 153 loss: 0.298993315085087
  batch 154 loss: 0.29947331538061045
  batch 155 loss: 0.29939195484884323
  batch 156 loss: 0.29927625115483236
  batch 157 loss: 0.29953582470963713
  batch 158 loss: 0.2997857442573656
  batch 159 loss: 0.29994566963528685
  batch 160 loss: 0.2999129741452634
  batch 161 loss: 0.30000131498582616
  batch 162 loss: 0.30000855479343436
  batch 163 loss: 0.2998700599911754
  batch 164 loss: 0.29994804075941806
  batch 165 loss: 0.2997785410194686
  batch 166 loss: 0.299626634332789
  batch 167 loss: 0.2995513284099316
  batch 168 loss: 0.29968162155931904
  batch 169 loss: 0.29931973494016206
  batch 170 loss: 0.29916939910720375
  batch 171 loss: 0.2991050954102076
  batch 172 loss: 0.29918192465638005
  batch 173 loss: 0.2992078549944596
  batch 174 loss: 0.29927741539204256
  batch 175 loss: 0.2993707661969321
  batch 176 loss: 0.299302030693401
  batch 177 loss: 0.2993522895931524
  batch 178 loss: 0.29943595876854456
  batch 179 loss: 0.2995543669721934
  batch 180 loss: 0.2995199989941385
  batch 181 loss: 0.2993946437677626
  batch 182 loss: 0.2992974436217612
  batch 183 loss: 0.29907431644820126
  batch 184 loss: 0.29888211061125214
  batch 185 loss: 0.29892965120238224
  batch 186 loss: 0.29906398714870536
  batch 187 loss: 0.29900738357860135
  batch 188 loss: 0.2989119183509908
  batch 189 loss: 0.2988487184047699
  batch 190 loss: 0.2989032150883424
  batch 191 loss: 0.2989469928891247
  batch 192 loss: 0.29912690119817853
  batch 193 loss: 0.299185701448065
  batch 194 loss: 0.299388278851804
  batch 195 loss: 0.2995656261077294
  batch 196 loss: 0.29934994359405676
  batch 197 loss: 0.2991626991838368
  batch 198 loss: 0.2992547387727583
  batch 199 loss: 0.2994618644965953
  batch 200 loss: 0.2995897530019283
  batch 201 loss: 0.29955313262061695
  batch 202 loss: 0.29962507008325934
  batch 203 loss: 0.29974379416169794
  batch 204 loss: 0.299668361889381
  batch 205 loss: 0.29979733039693135
  batch 206 loss: 0.29989843678127215
  batch 207 loss: 0.2999740663357979
  batch 208 loss: 0.29989246680186343
  batch 209 loss: 0.2996060381950944
  batch 210 loss: 0.2997967349631446
  batch 211 loss: 0.2997406434391347
  batch 212 loss: 0.2997405739887705
  batch 213 loss: 0.2998094746204609
  batch 214 loss: 0.29988303014608186
  batch 215 loss: 0.29971332189648653
  batch 216 loss: 0.29965734757758955
  batch 217 loss: 0.2996821295006484
  batch 218 loss: 0.2996818728129798
  batch 219 loss: 0.2996934645524308
  batch 220 loss: 0.29973971532149746
  batch 221 loss: 0.2998488945928634
  batch 222 loss: 0.29987683489515976
  batch 223 loss: 0.2999013595516906
  batch 224 loss: 0.29989716596901417
  batch 225 loss: 0.2998304016060299
  batch 226 loss: 0.2998684903161716
  batch 227 loss: 0.299667650513712
  batch 228 loss: 0.29961646099885303
  batch 229 loss: 0.2995493708739635
  batch 230 loss: 0.29960449286129165
  batch 231 loss: 0.29952327345872853
  batch 232 loss: 0.2993269186338474
  batch 233 loss: 0.2993178765405401
  batch 234 loss: 0.2995297596750096
  batch 235 loss: 0.29958213339460654
  batch 236 loss: 0.2995559713345463
  batch 237 loss: 0.29959443425327414
  batch 238 loss: 0.29954138072598885
  batch 239 loss: 0.2994316868702238
  batch 240 loss: 0.29950224931041397
  batch 241 loss: 0.29951698280469014
  batch 242 loss: 0.29932356915198083
  batch 243 loss: 0.2993071828604726
  batch 244 loss: 0.29919089499067086
  batch 245 loss: 0.29917360057636183
  batch 246 loss: 0.29916830833365277
  batch 247 loss: 0.29931367722600094
  batch 248 loss: 0.2992943604626963
  batch 249 loss: 0.29920456004908763
  batch 250 loss: 0.2991940222978592
  batch 251 loss: 0.29915915126819537
  batch 252 loss: 0.2991215811362342
  batch 253 loss: 0.29893387294569507
  batch 254 loss: 0.2988325766925737
  batch 255 loss: 0.2988541932667003
  batch 256 loss: 0.29880694462917745
  batch 257 loss: 0.29882404428511744
  batch 258 loss: 0.29884965197984564
  batch 259 loss: 0.2988377633472207
  batch 260 loss: 0.29881386653735087
  batch 261 loss: 0.2987168097861425
  batch 262 loss: 0.2984721785169521
  batch 263 loss: 0.2985090955015371
  batch 264 loss: 0.29834797001923574
  batch 265 loss: 0.29811592360712447
  batch 266 loss: 0.298095226175803
  batch 267 loss: 0.29816817589913414
  batch 268 loss: 0.29814451432494976
  batch 269 loss: 0.2980444081210736
  batch 270 loss: 0.29813110850475455
  batch 271 loss: 0.2981618730784342
  batch 272 loss: 0.29817971510483937
  batch 273 loss: 0.29813928329027617
  batch 274 loss: 0.29826783274647095
  batch 275 loss: 0.2983673790368167
  batch 276 loss: 0.2983471666989119
  batch 277 loss: 0.29840790622931523
  batch 278 loss: 0.298279315126028
  batch 279 loss: 0.29829971422858564
  batch 280 loss: 0.29815394995468003
  batch 281 loss: 0.29802598775069483
  batch 282 loss: 0.2981181970090731
  batch 283 loss: 0.2981326297820667
  batch 284 loss: 0.29820067435503006
  batch 285 loss: 0.2981564421402781
  batch 286 loss: 0.29812937162139197
  batch 287 loss: 0.29811836431250754
  batch 288 loss: 0.2979291043140822
  batch 289 loss: 0.2980128654559178
  batch 290 loss: 0.29778881417266256
  batch 291 loss: 0.2977174030444057
  batch 292 loss: 0.2976864767707374
  batch 293 loss: 0.2975835656245007
  batch 294 loss: 0.2974575390519739
  batch 295 loss: 0.2973899040686882
  batch 296 loss: 0.2974621802065018
  batch 297 loss: 0.29742695611936076
  batch 298 loss: 0.29747078527740184
  batch 299 loss: 0.2974275161689739
  batch 300 loss: 0.29751465206344924
  batch 301 loss: 0.2975145538284929
  batch 302 loss: 0.2974651115915633
  batch 303 loss: 0.29753600837964034
  batch 304 loss: 0.2974828821361849
  batch 305 loss: 0.29729959964752195
  batch 306 loss: 0.2973492024770749
  batch 307 loss: 0.29725438647627445
  batch 308 loss: 0.29735264656218613
  batch 309 loss: 0.2973675696980992
  batch 310 loss: 0.2973243299991854
  batch 311 loss: 0.29744226507051963
  batch 312 loss: 0.29755851177450937
  batch 313 loss: 0.29758421072182945
  batch 314 loss: 0.29750523939254175
  batch 315 loss: 0.2974998910275717
  batch 316 loss: 0.297390974586523
  batch 317 loss: 0.29738961005060455
  batch 318 loss: 0.29735736136541424
  batch 319 loss: 0.29730860725465613
  batch 320 loss: 0.29730398347601295
  batch 321 loss: 0.2972916781530945
  batch 322 loss: 0.2972421479521331
  batch 323 loss: 0.2971856988989532
  batch 324 loss: 0.29697683582335344
  batch 325 loss: 0.2968980573690855
  batch 326 loss: 0.29691197965408395
  batch 327 loss: 0.29701554948401376
  batch 328 loss: 0.29685750953489687
  batch 329 loss: 0.29693602523723994
  batch 330 loss: 0.2968949256972833
  batch 331 loss: 0.29689514524446875
  batch 332 loss: 0.2968068935695183
  batch 333 loss: 0.2968142205798948
  batch 334 loss: 0.29671542279556123
  batch 335 loss: 0.2965393460508603
  batch 336 loss: 0.29641944391741637
  batch 337 loss: 0.2962346729609662
  batch 338 loss: 0.29621851955645184
  batch 339 loss: 0.29616382929076135
  batch 340 loss: 0.2960933794870096
  batch 341 loss: 0.295967133863231
  batch 342 loss: 0.295855973199097
  batch 343 loss: 0.2958677682167587
  batch 344 loss: 0.2958576284175695
  batch 345 loss: 0.29598655925280803
  batch 346 loss: 0.29588354908662035
  batch 347 loss: 0.29585024740235605
  batch 348 loss: 0.29590068860300656
  batch 349 loss: 0.29576122850435854
  batch 350 loss: 0.29576635850327354
  batch 351 loss: 0.29574987593709234
  batch 352 loss: 0.2958080706355924
  batch 353 loss: 0.2957713743698158
  batch 354 loss: 0.2959166066878933
  batch 355 loss: 0.29589483540662576
  batch 356 loss: 0.29592221058653984
  batch 357 loss: 0.29581167388196083
  batch 358 loss: 0.29579681978045896
  batch 359 loss: 0.29580010681265245
  batch 360 loss: 0.29574192832741475
  batch 361 loss: 0.2957111974749869
  batch 362 loss: 0.29558978435578265
  batch 363 loss: 0.29550214062544927
  batch 364 loss: 0.2954192338818377
  batch 365 loss: 0.29543152047346716
  batch 366 loss: 0.2953328829401178
  batch 367 loss: 0.29520951609358154
  batch 368 loss: 0.29512035923645547
  batch 369 loss: 0.295183854657137
  batch 370 loss: 0.2951802760765359
  batch 371 loss: 0.29514766900205225
  batch 372 loss: 0.2950552752822317
  batch 373 loss: 0.29494512556545216
  batch 374 loss: 0.29485084925265237
  batch 375 loss: 0.2947814934651057
  batch 376 loss: 0.29488398970917185
  batch 377 loss: 0.2948620170354843
  batch 378 loss: 0.2947613092720824
  batch 379 loss: 0.2948515932210832
  batch 380 loss: 0.2948901859161101
  batch 381 loss: 0.29481137475473046
  batch 382 loss: 0.29468748344020695
  batch 383 loss: 0.2946515504669583
  batch 384 loss: 0.2946082843700424
  batch 385 loss: 0.2946627203133199
  batch 386 loss: 0.2945555856576855
  batch 387 loss: 0.29461449465548345
  batch 388 loss: 0.2946329167131911
  batch 389 loss: 0.2946353468536411
  batch 390 loss: 0.2946458135277797
  batch 391 loss: 0.29469160750851303
  batch 392 loss: 0.2947381504868366
  batch 393 loss: 0.2948465996798668
  batch 394 loss: 0.2948625123667233
  batch 395 loss: 0.2947954897261873
  batch 396 loss: 0.29486091220469185
  batch 397 loss: 0.2948575186864555
  batch 398 loss: 0.2948013550447459
  batch 399 loss: 0.2948863377472512
  batch 400 loss: 0.2949279721453786
  batch 401 loss: 0.29481808328420445
  batch 402 loss: 0.29489110777182365
  batch 403 loss: 0.29496034436279134
  batch 404 loss: 0.2949738215913277
  batch 405 loss: 0.2950020646975364
  batch 406 loss: 0.29508865822859
  batch 407 loss: 0.2951092064307421
  batch 408 loss: 0.29523840133904244
  batch 409 loss: 0.29524929741338296
  batch 410 loss: 0.29531153189699827
  batch 411 loss: 0.2952933667132454
  batch 412 loss: 0.2952392421373465
  batch 413 loss: 0.2952839516049147
  batch 414 loss: 0.2952436553899217
  batch 415 loss: 0.29522328042840384
  batch 416 loss: 0.2952198378670101
  batch 417 loss: 0.2951761887585231
  batch 418 loss: 0.2951377711869313
  batch 419 loss: 0.2951364872492583
  batch 420 loss: 0.29514146443633804
  batch 421 loss: 0.295077393534914
  batch 422 loss: 0.295162920738566
  batch 423 loss: 0.29516330970799104
  batch 424 loss: 0.29512160872372817
  batch 425 loss: 0.2951335121954189
  batch 426 loss: 0.2950439656354452
  batch 427 loss: 0.29504217305144326
  batch 428 loss: 0.2949772605620255
  batch 429 loss: 0.2950084027779964
  batch 430 loss: 0.29492061765388
  batch 431 loss: 0.29510169567890343
  batch 432 loss: 0.2951449000242132
  batch 433 loss: 0.2951367252762268
  batch 434 loss: 0.2951907498701926
  batch 435 loss: 0.2950883295001655
  batch 436 loss: 0.2950679388820031
  batch 437 loss: 0.29514585776378144
  batch 438 loss: 0.29522254052635744
  batch 439 loss: 0.2952111726266105
  batch 440 loss: 0.2952403492886912
  batch 441 loss: 0.29518771614212025
  batch 442 loss: 0.29515718796808793
  batch 443 loss: 0.29512749390327636
  batch 444 loss: 0.2950012252607324
  batch 445 loss: 0.2950729405946946
  batch 446 loss: 0.2950488993472048
  batch 447 loss: 0.2949899716065234
  batch 448 loss: 0.2950007882567921
  batch 449 loss: 0.2950523088464227
  batch 450 loss: 0.2950012516644266
  batch 451 loss: 0.2949832700638972
  batch 452 loss: 0.294964917498616
  batch 453 loss: 0.2950492948951574
  batch 454 loss: 0.295147373976949
  batch 455 loss: 0.29517385131055185
  batch 456 loss: 0.29518360059643
  batch 457 loss: 0.2952130811545729
  batch 458 loss: 0.29516181353007864
  batch 459 loss: 0.29517752837901023
  batch 460 loss: 0.2952831699472407
  batch 461 loss: 0.2952900591938718
  batch 462 loss: 0.29534793903172274
  batch 463 loss: 0.29529531435240425
  batch 464 loss: 0.29530846542710887
  batch 465 loss: 0.29528629827883934
  batch 466 loss: 0.295165116790794
  batch 467 loss: 0.29519092695233146
  batch 468 loss: 0.29516784684398234
  batch 469 loss: 0.29530186099665506
  batch 470 loss: 0.2953023599500352
  batch 471 loss: 0.2952961162080684
  batch 472 loss: 0.29517017592186645
LOSS train 0.29517017592186645 valid 0.24695324897766113
LOSS train 0.29517017592186645 valid 0.23683610558509827
LOSS train 0.29517017592186645 valid 0.24002630015214285
LOSS train 0.29517017592186645 valid 0.23214741796255112
LOSS train 0.29517017592186645 valid 0.22437792718410493
LOSS train 0.29517017592186645 valid 0.23037837694088617
LOSS train 0.29517017592186645 valid 0.237989747098514
LOSS train 0.29517017592186645 valid 0.2372153289616108
LOSS train 0.29517017592186645 valid 0.23939342300097147
LOSS train 0.29517017592186645 valid 0.24183925986289978
LOSS train 0.29517017592186645 valid 0.23866096816279672
LOSS train 0.29517017592186645 valid 0.24073562398552895
LOSS train 0.29517017592186645 valid 0.24082756959475005
LOSS train 0.29517017592186645 valid 0.24085572574819838
LOSS train 0.29517017592186645 valid 0.23825738728046417
LOSS train 0.29517017592186645 valid 0.2388684405013919
LOSS train 0.29517017592186645 valid 0.23894164842717788
LOSS train 0.29517017592186645 valid 0.24031997554832035
LOSS train 0.29517017592186645 valid 0.2430629196919893
LOSS train 0.29517017592186645 valid 0.24206012040376662
LOSS train 0.29517017592186645 valid 0.2408189901283809
LOSS train 0.29517017592186645 valid 0.23960835283452814
LOSS train 0.29517017592186645 valid 0.24009494936984518
LOSS train 0.29517017592186645 valid 0.23907267240186533
LOSS train 0.29517017592186645 valid 0.23819064557552339
LOSS train 0.29517017592186645 valid 0.2392379200229278
LOSS train 0.29517017592186645 valid 0.23895886871549818
LOSS train 0.29517017592186645 valid 0.23963334411382675
LOSS train 0.29517017592186645 valid 0.24035354832123065
LOSS train 0.29517017592186645 valid 0.24227928320566813
LOSS train 0.29517017592186645 valid 0.2432107271686677
LOSS train 0.29517017592186645 valid 0.24297950556501746
LOSS train 0.29517017592186645 valid 0.24389776452021164
LOSS train 0.29517017592186645 valid 0.2440715927411528
LOSS train 0.29517017592186645 valid 0.245333588549069
LOSS train 0.29517017592186645 valid 0.24490979686379433
LOSS train 0.29517017592186645 valid 0.2449200978955707
LOSS train 0.29517017592186645 valid 0.24616571123662748
LOSS train 0.29517017592186645 valid 0.24554733473521012
LOSS train 0.29517017592186645 valid 0.2457391057163477
LOSS train 0.29517017592186645 valid 0.2466874700493929
LOSS train 0.29517017592186645 valid 0.24633584455365226
LOSS train 0.29517017592186645 valid 0.24623871577340503
LOSS train 0.29517017592186645 valid 0.2463959174400026
LOSS train 0.29517017592186645 valid 0.24588747123877208
LOSS train 0.29517017592186645 valid 0.24657853627982346
LOSS train 0.29517017592186645 valid 0.24699855770202392
LOSS train 0.29517017592186645 valid 0.24673871447642645
LOSS train 0.29517017592186645 valid 0.2472141342503684
LOSS train 0.29517017592186645 valid 0.24672074377536773
LOSS train 0.29517017592186645 valid 0.24693756010018142
LOSS train 0.29517017592186645 valid 0.24648512441378373
LOSS train 0.29517017592186645 valid 0.24698620865929802
LOSS train 0.29517017592186645 valid 0.2469791198770205
LOSS train 0.29517017592186645 valid 0.2472820588133552
LOSS train 0.29517017592186645 valid 0.24664602960859025
LOSS train 0.29517017592186645 valid 0.24637271594582943
LOSS train 0.29517017592186645 valid 0.24601533777754883
LOSS train 0.29517017592186645 valid 0.24649559264465914
LOSS train 0.29517017592186645 valid 0.2460607185959816
LOSS train 0.29517017592186645 valid 0.24615383441330957
LOSS train 0.29517017592186645 valid 0.24715339464526023
LOSS train 0.29517017592186645 valid 0.24725764564105443
LOSS train 0.29517017592186645 valid 0.24788725236430764
LOSS train 0.29517017592186645 valid 0.2482116855107821
LOSS train 0.29517017592186645 valid 0.24822246847730695
LOSS train 0.29517017592186645 valid 0.24754921647150124
LOSS train 0.29517017592186645 valid 0.24757311054888895
LOSS train 0.29517017592186645 valid 0.24667990186076233
LOSS train 0.29517017592186645 valid 0.2468728855252266
LOSS train 0.29517017592186645 valid 0.24687521537424814
LOSS train 0.29517017592186645 valid 0.2472782956643237
LOSS train 0.29517017592186645 valid 0.24730716278291728
LOSS train 0.29517017592186645 valid 0.24721357045141426
LOSS train 0.29517017592186645 valid 0.2471700261036555
LOSS train 0.29517017592186645 valid 0.24764650373866684
LOSS train 0.29517017592186645 valid 0.24752915931212438
LOSS train 0.29517017592186645 valid 0.24745876552202764
LOSS train 0.29517017592186645 valid 0.24759147099301784
LOSS train 0.29517017592186645 valid 0.2467828571796417
LOSS train 0.29517017592186645 valid 0.24571892361582062
LOSS train 0.29517017592186645 valid 0.2457465525807404
LOSS train 0.29517017592186645 valid 0.24560906010937977
LOSS train 0.29517017592186645 valid 0.2454740990485464
LOSS train 0.29517017592186645 valid 0.24506166577339172
LOSS train 0.29517017592186645 valid 0.24434609018092932
LOSS train 0.29517017592186645 valid 0.24445385083384896
LOSS train 0.29517017592186645 valid 0.24398245848715305
LOSS train 0.29517017592186645 valid 0.24419856389586844
LOSS train 0.29517017592186645 valid 0.24436919275257324
LOSS train 0.29517017592186645 valid 0.2446562321958961
LOSS train 0.29517017592186645 valid 0.2447808318487976
LOSS train 0.29517017592186645 valid 0.24450871909177432
LOSS train 0.29517017592186645 valid 0.24473519924473255
LOSS train 0.29517017592186645 valid 0.24431508544244265
LOSS train 0.29517017592186645 valid 0.24453743283326426
LOSS train 0.29517017592186645 valid 0.24472829745602362
LOSS train 0.29517017592186645 valid 0.2448215841942904
LOSS train 0.29517017592186645 valid 0.2449672401252419
LOSS train 0.29517017592186645 valid 0.24534555807709693
LOSS train 0.29517017592186645 valid 0.24577870094540097
LOSS train 0.29517017592186645 valid 0.24588784034930022
LOSS train 0.29517017592186645 valid 0.24598351598364634
LOSS train 0.29517017592186645 valid 0.24596026812035304
LOSS train 0.29517017592186645 valid 0.24594616889953613
LOSS train 0.29517017592186645 valid 0.24628076772644836
LOSS train 0.29517017592186645 valid 0.24601478108735841
LOSS train 0.29517017592186645 valid 0.24625077457339675
LOSS train 0.29517017592186645 valid 0.2467038448250622
LOSS train 0.29517017592186645 valid 0.24703170440413735
LOSS train 0.29517017592186645 valid 0.2466054596342482
LOSS train 0.29517017592186645 valid 0.24629187557314122
LOSS train 0.29517017592186645 valid 0.24636384792032495
LOSS train 0.29517017592186645 valid 0.2462920691621931
LOSS train 0.29517017592186645 valid 0.24628017842769623
LOSS train 0.29517017592186645 valid 0.2465364543785309
LOSS train 0.29517017592186645 valid 0.24676120854341066
LOSS train 0.29517017592186645 valid 0.2466447124794378
LOSS train 0.29517017592186645 valid 0.2466987302573789
LOSS train 0.29517017592186645 valid 0.24635182743271192
LOSS train 0.29517017592186645 valid 0.2461896919268222
LOSS train 0.29517017592186645 valid 0.24602823667838924
LOSS train 0.29517017592186645 valid 0.24612851307644107
LOSS train 0.29517017592186645 valid 0.2462901189442604
LOSS train 0.29517017592186645 valid 0.24633975911140443
LOSS train 0.29517017592186645 valid 0.24649805445519704
LOSS train 0.29517017592186645 valid 0.2463626662107903
LOSS train 0.29517017592186645 valid 0.24665736849419773
LOSS train 0.29517017592186645 valid 0.24669325767561448
LOSS train 0.29517017592186645 valid 0.24670098080084873
LOSS train 0.29517017592186645 valid 0.24665697317087013
LOSS train 0.29517017592186645 valid 0.24639334890878561
LOSS train 0.29517017592186645 valid 0.24612129754141757
LOSS train 0.29517017592186645 valid 0.24609855306682302
LOSS train 0.29517017592186645 valid 0.2460477493427418
LOSS train 0.29517017592186645 valid 0.24595478933085413
LOSS train 0.29517017592186645 valid 0.24587031176490506
LOSS train 0.29517017592186645 valid 0.24586274204910666
LOSS train 0.29517017592186645 valid 0.2457633109615861
LOSS train 0.29517017592186645 valid 0.24585159814783505
LOSS train 0.29517017592186645 valid 0.24583586321232168
LOSS train 0.29517017592186645 valid 0.24608311487335555
LOSS train 0.29517017592186645 valid 0.2461412652180745
LOSS train 0.29517017592186645 valid 0.24623187890069353
LOSS train 0.29517017592186645 valid 0.24601122305311005
LOSS train 0.29517017592186645 valid 0.24627983733399272
LOSS train 0.29517017592186645 valid 0.24601884672836383
LOSS train 0.29517017592186645 valid 0.24679156805615168
LOSS train 0.29517017592186645 valid 0.24681764891083607
LOSS train 0.29517017592186645 valid 0.24678433189789453
LOSS train 0.29517017592186645 valid 0.24678010222138158
LOSS train 0.29517017592186645 valid 0.24666405743674227
LOSS train 0.29517017592186645 valid 0.24676276733672697
LOSS train 0.29517017592186645 valid 0.24680497642461355
LOSS train 0.29517017592186645 valid 0.24680033028125764
LOSS train 0.29517017592186645 valid 0.24716006649228242
LOSS train 0.29517017592186645 valid 0.24712967977022668
LOSS train 0.29517017592186645 valid 0.24723589146816277
LOSS train 0.29517017592186645 valid 0.24726881030595527
LOSS train 0.29517017592186645 valid 0.24704048661515116
LOSS train 0.29517017592186645 valid 0.24701738681482233
LOSS train 0.29517017592186645 valid 0.24697024835112655
LOSS train 0.29517017592186645 valid 0.24695019213699856
LOSS train 0.29517017592186645 valid 0.2468728292279127
LOSS train 0.29517017592186645 valid 0.24663553156635978
LOSS train 0.29517017592186645 valid 0.24674264072294694
LOSS train 0.29517017592186645 valid 0.24680883111711033
LOSS train 0.29517017592186645 valid 0.24672048689708823
LOSS train 0.29517017592186645 valid 0.2467815899813669
LOSS train 0.29517017592186645 valid 0.24697161360698588
LOSS train 0.29517017592186645 valid 0.24688466511971768
LOSS train 0.29517017592186645 valid 0.24665754427050435
LOSS train 0.29517017592186645 valid 0.24667879008833385
LOSS train 0.29517017592186645 valid 0.24662879708854632
LOSS train 0.29517017592186645 valid 0.24660693330424172
LOSS train 0.29517017592186645 valid 0.24648212920874357
LOSS train 0.29517017592186645 valid 0.2464703866990946
LOSS train 0.29517017592186645 valid 0.24676894404914942
LOSS train 0.29517017592186645 valid 0.2466965221159951
LOSS train 0.29517017592186645 valid 0.2467659196919865
LOSS train 0.29517017592186645 valid 0.24703638444947934
LOSS train 0.29517017592186645 valid 0.247153082882965
LOSS train 0.29517017592186645 valid 0.24701877695615174
LOSS train 0.29517017592186645 valid 0.24702168328930502
LOSS train 0.29517017592186645 valid 0.2468131739545513
LOSS train 0.29517017592186645 valid 0.2467197122913535
LOSS train 0.29517017592186645 valid 0.24658039252069544
LOSS train 0.29517017592186645 valid 0.24661949094622693
LOSS train 0.29517017592186645 valid 0.24671098645086642
LOSS train 0.29517017592186645 valid 0.24677644881762956
LOSS train 0.29517017592186645 valid 0.24663414209300935
LOSS train 0.29517017592186645 valid 0.24658833684710166
LOSS train 0.29517017592186645 valid 0.24664009520735766
LOSS train 0.29517017592186645 valid 0.24657296558323594
LOSS train 0.29517017592186645 valid 0.24629631554469084
LOSS train 0.29517017592186645 valid 0.24639153336079753
LOSS train 0.29517017592186645 valid 0.24649352629475182
LOSS train 0.29517017592186645 valid 0.24638462172012138
LOSS train 0.29517017592186645 valid 0.24632939112246335
LOSS train 0.29517017592186645 valid 0.24610461935400962
LOSS train 0.29517017592186645 valid 0.24596096464057468
LOSS train 0.29517017592186645 valid 0.24581643592308064
LOSS train 0.29517017592186645 valid 0.24572166022408773
LOSS train 0.29517017592186645 valid 0.24565701800234177
LOSS train 0.29517017592186645 valid 0.24548975083886124
LOSS train 0.29517017592186645 valid 0.24547372027797607
LOSS train 0.29517017592186645 valid 0.24542331969104528
LOSS train 0.29517017592186645 valid 0.24532042544048566
LOSS train 0.29517017592186645 valid 0.24516624739865936
LOSS train 0.29517017592186645 valid 0.24524347086747486
LOSS train 0.29517017592186645 valid 0.24525745820377676
LOSS train 0.29517017592186645 valid 0.2450477438012384
LOSS train 0.29517017592186645 valid 0.24514824928532183
LOSS train 0.29517017592186645 valid 0.24506237093254785
LOSS train 0.29517017592186645 valid 0.2449325847764348
LOSS train 0.29517017592186645 valid 0.24486590252706297
LOSS train 0.29517017592186645 valid 0.24484495269263395
LOSS train 0.29517017592186645 valid 0.24483410234845013
LOSS train 0.29517017592186645 valid 0.2449180532808173
LOSS train 0.29517017592186645 valid 0.2450584895231507
LOSS train 0.29517017592186645 valid 0.24513822766990143
LOSS train 0.29517017592186645 valid 0.24508480311514022
LOSS train 0.29517017592186645 valid 0.24518216431408185
LOSS train 0.29517017592186645 valid 0.24537196861846106
LOSS train 0.29517017592186645 valid 0.2453715263472663
LOSS train 0.29517017592186645 valid 0.2454305471835938
LOSS train 0.29517017592186645 valid 0.24571700440104313
LOSS train 0.29517017592186645 valid 0.2458245409162421
LOSS train 0.29517017592186645 valid 0.24582568253492162
LOSS train 0.29517017592186645 valid 0.24597631122754968
LOSS train 0.29517017592186645 valid 0.24597220825942565
LOSS train 0.29517017592186645 valid 0.2460291708080933
LOSS train 0.29517017592186645 valid 0.24590783441527206
LOSS train 0.29517017592186645 valid 0.2459665511408423
LOSS train 0.29517017592186645 valid 0.24599676208293184
LOSS train 0.29517017592186645 valid 0.24591525580923437
LOSS train 0.29517017592186645 valid 0.24596741657216842
LOSS train 0.29517017592186645 valid 0.2459893909071674
LOSS train 0.29517017592186645 valid 0.24585961098451495
LOSS train 0.29517017592186645 valid 0.2458408698439598
LOSS train 0.29517017592186645 valid 0.24598289364601073
LOSS train 0.29517017592186645 valid 0.24574570419374575
LOSS train 0.29517017592186645 valid 0.24585965888980976
LOSS train 0.29517017592186645 valid 0.24602299244677434
LOSS train 0.29517017592186645 valid 0.24612393841451527
LOSS train 0.29517017592186645 valid 0.2460889149972094
LOSS train 0.29517017592186645 valid 0.24610931441368844
LOSS train 0.29517017592186645 valid 0.24609479987092556
LOSS train 0.29517017592186645 valid 0.24597373987776208
LOSS train 0.29517017592186645 valid 0.24609173727035522
LOSS train 0.29517017592186645 valid 0.2459756983703826
LOSS train 0.29517017592186645 valid 0.24603655981639075
LOSS train 0.29517017592186645 valid 0.24590658399427362
LOSS train 0.29517017592186645 valid 0.24587413433968552
LOSS train 0.29517017592186645 valid 0.24594444188417172
LOSS train 0.29517017592186645 valid 0.2459978248225525
LOSS train 0.29517017592186645 valid 0.24586920527168749
LOSS train 0.29517017592186645 valid 0.24590448411398155
LOSS train 0.29517017592186645 valid 0.24586656292663117
LOSS train 0.29517017592186645 valid 0.2458451216037457
LOSS train 0.29517017592186645 valid 0.24590565258515748
LOSS train 0.29517017592186645 valid 0.24590001585146853
LOSS train 0.29517017592186645 valid 0.24585023335404269
LOSS train 0.29517017592186645 valid 0.24588618636357062
LOSS train 0.29517017592186645 valid 0.2458022618631147
LOSS train 0.29517017592186645 valid 0.2457709277706935
LOSS train 0.29517017592186645 valid 0.24590366509523284
LOSS train 0.29517017592186645 valid 0.24604381854409602
LOSS train 0.29517017592186645 valid 0.24610632502899737
LOSS train 0.29517017592186645 valid 0.24607072180068051
LOSS train 0.29517017592186645 valid 0.2460914115074376
LOSS train 0.29517017592186645 valid 0.24630359138416894
LOSS train 0.29517017592186645 valid 0.2463465650112201
LOSS train 0.29517017592186645 valid 0.24633687326725381
LOSS train 0.29517017592186645 valid 0.2462712663412094
LOSS train 0.29517017592186645 valid 0.24620590256392091
LOSS train 0.29517017592186645 valid 0.24603152334259737
LOSS train 0.29517017592186645 valid 0.2458804597635921
LOSS train 0.29517017592186645 valid 0.24590318887678098
LOSS train 0.29517017592186645 valid 0.24590702056884767
LOSS train 0.29517017592186645 valid 0.2458293765677252
LOSS train 0.29517017592186645 valid 0.2456535169630186
LOSS train 0.29517017592186645 valid 0.24569034892341696
LOSS train 0.29517017592186645 valid 0.24578973703401188
LOSS train 0.29517017592186645 valid 0.24586569196299501
LOSS train 0.29517017592186645 valid 0.24571376601299205
LOSS train 0.29517017592186645 valid 0.24557926700505645
LOSS train 0.29517017592186645 valid 0.24565059536447129
LOSS train 0.29517017592186645 valid 0.2456930814936087
LOSS train 0.29517017592186645 valid 0.2456925889541363
LOSS train 0.29517017592186645 valid 0.24567689119335712
LOSS train 0.29517017592186645 valid 0.24563475779882848
LOSS train 0.29517017592186645 valid 0.2456935614652601
LOSS train 0.29517017592186645 valid 0.24576704944072125
LOSS train 0.29517017592186645 valid 0.2458491917383873
LOSS train 0.29517017592186645 valid 0.24582351522671209
LOSS train 0.29517017592186645 valid 0.24583180183513398
LOSS train 0.29517017592186645 valid 0.2458223498507634
LOSS train 0.29517017592186645 valid 0.24585141366142094
LOSS train 0.29517017592186645 valid 0.2458024554948012
LOSS train 0.29517017592186645 valid 0.24583364438971017
LOSS train 0.29517017592186645 valid 0.2458885345139251
LOSS train 0.29517017592186645 valid 0.2459600656831225
LOSS train 0.29517017592186645 valid 0.24593924784934834
LOSS train 0.29517017592186645 valid 0.2458611790762573
LOSS train 0.29517017592186645 valid 0.2458185989385337
LOSS train 0.29517017592186645 valid 0.24576940618042836
LOSS train 0.29517017592186645 valid 0.24565818310751544
LOSS train 0.29517017592186645 valid 0.24579400938112758
LOSS train 0.29517017592186645 valid 0.24572271190343364
LOSS train 0.29517017592186645 valid 0.24559453393293731
LOSS train 0.29517017592186645 valid 0.2455880944736493
LOSS train 0.29517017592186645 valid 0.24561498302240342
LOSS train 0.29517017592186645 valid 0.2456704921023861
LOSS train 0.29517017592186645 valid 0.2456975573585147
LOSS train 0.29517017592186645 valid 0.24558966399370868
LOSS train 0.29517017592186645 valid 0.2455903934755536
LOSS train 0.29517017592186645 valid 0.2456393631749183
LOSS train 0.29517017592186645 valid 0.24569149151864844
LOSS train 0.29517017592186645 valid 0.24564395523630084
LOSS train 0.29517017592186645 valid 0.24570248519705834
LOSS train 0.29517017592186645 valid 0.24565630369119762
LOSS train 0.29517017592186645 valid 0.2456275924532775
LOSS train 0.29517017592186645 valid 0.24558001545108396
LOSS train 0.29517017592186645 valid 0.24556136415554927
LOSS train 0.29517017592186645 valid 0.24573205192030573
LOSS train 0.29517017592186645 valid 0.24583475296285903
LOSS train 0.29517017592186645 valid 0.2457903615403466
LOSS train 0.29517017592186645 valid 0.2459324730565845
LOSS train 0.29517017592186645 valid 0.24591193036599593
LOSS train 0.29517017592186645 valid 0.24585252104388983
LOSS train 0.29517017592186645 valid 0.24572058529200325
LOSS train 0.29517017592186645 valid 0.2457007140189678
LOSS train 0.29517017592186645 valid 0.24584352675669208
LOSS train 0.29517017592186645 valid 0.24583482293050682
LOSS train 0.29517017592186645 valid 0.24587578144634054
LOSS train 0.29517017592186645 valid 0.24579714937273756
LOSS train 0.29517017592186645 valid 0.24576138585982238
LOSS train 0.29517017592186645 valid 0.24580718721963662
LOSS train 0.29517017592186645 valid 0.24585429412477156
LOSS train 0.29517017592186645 valid 0.24575329071615448
LOSS train 0.29517017592186645 valid 0.24569407053161085
LOSS train 0.29517017592186645 valid 0.24571263581601246
LOSS train 0.29517017592186645 valid 0.24587462686522063
LOSS train 0.29517017592186645 valid 0.24587067283581995
LOSS train 0.29517017592186645 valid 0.24577242751858827
LOSS train 0.29517017592186645 valid 0.2456776528949353
LOSS train 0.29517017592186645 valid 0.24562469955490923
LOSS train 0.29517017592186645 valid 0.24569819444913235
LOSS train 0.29517017592186645 valid 0.24555331089666912
LOSS train 0.29517017592186645 valid 0.24546307873012674
LOSS train 0.29517017592186645 valid 0.24543122727085243
LOSS train 0.29517017592186645 valid 0.24548547058537729
LOSS train 0.29517017592186645 valid 0.2455736609165278
LOSS train 0.29517017592186645 valid 0.24572062677061055
LOSS train 0.29517017592186645 valid 0.2457862821187866
LOSS train 0.29517017592186645 valid 0.2457469287289291
LOSS train 0.29517017592186645 valid 0.24570263876428816
LOSS train 0.29517017592186645 valid 0.24568522760961048
LOSS train 0.29517017592186645 valid 0.24559086540506947
LOSS train 0.29517017592186645 valid 0.24550810476914667
LOSS train 0.29517017592186645 valid 0.24562281818844336
LOSS train 0.29517017592186645 valid 0.24546681960259586
LOSS train 0.29517017592186645 valid 0.24547338350624828
LOSS train 0.29517017592186645 valid 0.24543305018993272
LOSS train 0.29517017592186645 valid 0.24535208586298052
LOSS train 0.29517017592186645 valid 0.2452478738100717
LOSS train 0.29517017592186645 valid 0.24522877136326354
LOSS train 0.29517017592186645 valid 0.24527126502215377
EPOCH 6:
  batch 1 loss: 0.29983529448509216
  batch 2 loss: 0.2850637882947922
  batch 3 loss: 0.28300445278485614
  batch 4 loss: 0.2802145779132843
  batch 5 loss: 0.28593834638595583
  batch 6 loss: 0.28050923844178516
  batch 7 loss: 0.2843481089387621
  batch 8 loss: 0.2880721054971218
  batch 9 loss: 0.28759731517897713
  batch 10 loss: 0.2896320581436157
  batch 11 loss: 0.2901761369271712
  batch 12 loss: 0.286321925620238
  batch 13 loss: 0.28586907340930057
  batch 14 loss: 0.2869489086525781
  batch 15 loss: 0.28929064273834226
  batch 16 loss: 0.28868930973112583
  batch 17 loss: 0.28568458381821127
  batch 18 loss: 0.2869269980324639
  batch 19 loss: 0.2867923488742427
  batch 20 loss: 0.28546358495950697
  batch 21 loss: 0.28548824929055716
  batch 22 loss: 0.2851812378926711
  batch 23 loss: 0.2842131451420162
  batch 24 loss: 0.2836350314319134
  batch 25 loss: 0.28639866471290587
  batch 26 loss: 0.284595508988087
  batch 27 loss: 0.2850470818855144
  batch 28 loss: 0.28339957339423044
  batch 29 loss: 0.28367983472758324
  batch 30 loss: 0.28322809040546415
  batch 31 loss: 0.28453932558336564
  batch 32 loss: 0.28392511513084173
  batch 33 loss: 0.28325314323107403
  batch 34 loss: 0.2832572933505563
  batch 35 loss: 0.2839248699801309
  batch 36 loss: 0.2843309955464469
  batch 37 loss: 0.28487779884725006
  batch 38 loss: 0.28529329676377146
  batch 39 loss: 0.2849707137315701
  batch 40 loss: 0.28486640229821203
  batch 41 loss: 0.28487934790006497
  batch 42 loss: 0.28545535746074857
  batch 43 loss: 0.28541700576626977
  batch 44 loss: 0.28565891832113266
  batch 45 loss: 0.2852809919251336
  batch 46 loss: 0.28496292881343677
  batch 47 loss: 0.2851362539098618
  batch 48 loss: 0.2842109076057871
  batch 49 loss: 0.2840095572933859
  batch 50 loss: 0.28388045698404313
  batch 51 loss: 0.28422465073127373
  batch 52 loss: 0.28442542684765965
  batch 53 loss: 0.28374201340495414
  batch 54 loss: 0.28362168592435344
  batch 55 loss: 0.2832583552057093
  batch 56 loss: 0.28304553776979446
  batch 57 loss: 0.28235629357789693
  batch 58 loss: 0.2831488950499173
  batch 59 loss: 0.2835881391824302
  batch 60 loss: 0.2832468241453171
  batch 61 loss: 0.2834418396480748
  batch 62 loss: 0.28341474263898786
  batch 63 loss: 0.2829050596744295
  batch 64 loss: 0.2837306964211166
  batch 65 loss: 0.28308037565304683
  batch 66 loss: 0.283030911376982
  batch 67 loss: 0.28323469188675954
  batch 68 loss: 0.2832948946777512
  batch 69 loss: 0.28327849334564764
  batch 70 loss: 0.2836679045643125
  batch 71 loss: 0.28343305579373534
  batch 72 loss: 0.28383251817690003
  batch 73 loss: 0.28411747698914513
  batch 74 loss: 0.28427417616586426
  batch 75 loss: 0.28381749510765075
  batch 76 loss: 0.2844240720334806
  batch 77 loss: 0.28432860622158296
  batch 78 loss: 0.28447147516103893
  batch 79 loss: 0.28482057248489767
  batch 80 loss: 0.284721390530467
  batch 81 loss: 0.28489453888233796
  batch 82 loss: 0.2853006378906529
  batch 83 loss: 0.28531730246831133
  batch 84 loss: 0.28530876835187274
  batch 85 loss: 0.28517837734783397
  batch 86 loss: 0.28555758332097253
  batch 87 loss: 0.2853073388680644
  batch 88 loss: 0.2849063128232956
  batch 89 loss: 0.28474581476007954
  batch 90 loss: 0.2847180787059996
  batch 91 loss: 0.2847724124625489
  batch 92 loss: 0.2847053991711658
  batch 93 loss: 0.2844784577046671
  batch 94 loss: 0.28445193425137943
  batch 95 loss: 0.2842857244767641
  batch 96 loss: 0.2844738882655899
  batch 97 loss: 0.28494965139123585
  batch 98 loss: 0.28518916240760256
  batch 99 loss: 0.2853498907402308
  batch 100 loss: 0.2853116762638092
  batch 101 loss: 0.2853506788758948
  batch 102 loss: 0.28578485782239954
  batch 103 loss: 0.2860668993111953
  batch 104 loss: 0.28612242438472235
  batch 105 loss: 0.28578877449035645
  batch 106 loss: 0.28621873906198536
  batch 107 loss: 0.2855913130757965
  batch 108 loss: 0.2855010141653043
  batch 109 loss: 0.28531813279751245
  batch 110 loss: 0.2850661886009303
  batch 111 loss: 0.2850397276180284
  batch 112 loss: 0.28482356920306173
  batch 113 loss: 0.2852572596969858
  batch 114 loss: 0.28560332444153336
  batch 115 loss: 0.28599858944830686
  batch 116 loss: 0.2862653039909642
  batch 117 loss: 0.2864726148864143
  batch 118 loss: 0.28664501251305563
  batch 119 loss: 0.28694221740510284
  batch 120 loss: 0.286921596651276
  batch 121 loss: 0.28679517046971753
  batch 122 loss: 0.28686929591855065
  batch 123 loss: 0.28644583259171585
  batch 124 loss: 0.286768181189414
  batch 125 loss: 0.2869370813369751
  batch 126 loss: 0.2869948387619049
  batch 127 loss: 0.2875170402639494
  batch 128 loss: 0.28758270340040326
  batch 129 loss: 0.2878170179766278
  batch 130 loss: 0.2880361192501508
  batch 131 loss: 0.28829545942881635
  batch 132 loss: 0.28844500468535855
  batch 133 loss: 0.2885641032143643
  batch 134 loss: 0.2886403952516727
  batch 135 loss: 0.28861167850317776
  batch 136 loss: 0.2884427157833296
  batch 137 loss: 0.28826893202579806
  batch 138 loss: 0.28830189182274585
  batch 139 loss: 0.28875963915166236
  batch 140 loss: 0.2887619606086186
  batch 141 loss: 0.2888511085341163
  batch 142 loss: 0.2887508835591061
  batch 143 loss: 0.28872982355264515
  batch 144 loss: 0.28858705754909253
  batch 145 loss: 0.28863633994398447
  batch 146 loss: 0.28862759797540427
  batch 147 loss: 0.28887237253643216
  batch 148 loss: 0.28878946703028036
  batch 149 loss: 0.28866280585327403
  batch 150 loss: 0.2884447511037191
  batch 151 loss: 0.288566479027666
  batch 152 loss: 0.2885255686154491
  batch 153 loss: 0.2883462847447863
  batch 154 loss: 0.2886347882933431
  batch 155 loss: 0.2885727559366534
  batch 156 loss: 0.2885665123661359
  batch 157 loss: 0.28889296331982706
  batch 158 loss: 0.28914785630340817
  batch 159 loss: 0.2892715314274314
  batch 160 loss: 0.2892264327034354
  batch 161 loss: 0.2893743894485213
  batch 162 loss: 0.2894509142195737
  batch 163 loss: 0.28929271818670027
  batch 164 loss: 0.28937669807090993
  batch 165 loss: 0.2891793415401921
  batch 166 loss: 0.2890317951699337
  batch 167 loss: 0.28881408711393436
  batch 168 loss: 0.28894418779583203
  batch 169 loss: 0.28861564001035406
  batch 170 loss: 0.288465522580287
  batch 171 loss: 0.2884317884842555
  batch 172 loss: 0.28852307250679926
  batch 173 loss: 0.2885635766507573
  batch 174 loss: 0.2886395436422578
  batch 175 loss: 0.2886393386125565
  batch 176 loss: 0.2885987583378499
  batch 177 loss: 0.28872829027431834
  batch 178 loss: 0.2888588958074538
  batch 179 loss: 0.2889015497275571
  batch 180 loss: 0.288921432942152
  batch 181 loss: 0.288841772886271
  batch 182 loss: 0.2887341736273451
  batch 183 loss: 0.2885300296414745
  batch 184 loss: 0.2883525327981814
  batch 185 loss: 0.2883737149270805
  batch 186 loss: 0.2885404262010769
  batch 187 loss: 0.2884635682252639
  batch 188 loss: 0.28834619032258685
  batch 189 loss: 0.28827533937951244
  batch 190 loss: 0.2883791903916158
  batch 191 loss: 0.2884789759107909
  batch 192 loss: 0.28865086652028066
  batch 193 loss: 0.2887139546593236
  batch 194 loss: 0.2889376344908144
  batch 195 loss: 0.28911648323902717
  batch 196 loss: 0.2889189290605029
  batch 197 loss: 0.28875983358942314
  batch 198 loss: 0.28885288649436197
  batch 199 loss: 0.289073759661847
  batch 200 loss: 0.2891005616635084
  batch 201 loss: 0.2891341718423426
  batch 202 loss: 0.2892013077836226
  batch 203 loss: 0.28924965293243016
  batch 204 loss: 0.28914275685069607
  batch 205 loss: 0.289233197162791
  batch 206 loss: 0.2893225565842054
  batch 207 loss: 0.28939695627505074
  batch 208 loss: 0.28933955621547425
  batch 209 loss: 0.289079751409412
  batch 210 loss: 0.28924065289043244
  batch 211 loss: 0.2891350663951223
  batch 212 loss: 0.28910609447168856
  batch 213 loss: 0.28907183022566246
  batch 214 loss: 0.28910212316245676
  batch 215 loss: 0.288874816201454
  batch 216 loss: 0.2888747002515528
  batch 217 loss: 0.2889842268783376
  batch 218 loss: 0.28899861298023016
  batch 219 loss: 0.288959460715725
  batch 220 loss: 0.2889945028857751
  batch 221 loss: 0.2891261616713321
  batch 222 loss: 0.2891806143898148
  batch 223 loss: 0.28918837912948675
  batch 224 loss: 0.289217870815524
  batch 225 loss: 0.2891655503378974
  batch 226 loss: 0.2891853718103561
  batch 227 loss: 0.28898966141734356
  batch 228 loss: 0.28886869850388747
  batch 229 loss: 0.2887583808347127
  batch 230 loss: 0.2888425023659416
  batch 231 loss: 0.28879866003990173
  batch 232 loss: 0.2886335691758271
  batch 233 loss: 0.28858235197005866
  batch 234 loss: 0.2887647669030051
  batch 235 loss: 0.28886945108149914
  batch 236 loss: 0.28885586155673204
  batch 237 loss: 0.2889404563461175
  batch 238 loss: 0.2889106971125643
  batch 239 loss: 0.28883526527233205
  batch 240 loss: 0.28887798624734085
  batch 241 loss: 0.28886115390235456
  batch 242 loss: 0.288659412383048
  batch 243 loss: 0.28861267184033806
  batch 244 loss: 0.288455066866562
  batch 245 loss: 0.2884591057592509
  batch 246 loss: 0.2884096691036612
  batch 247 loss: 0.28856140206217284
  batch 248 loss: 0.2885446918587531
  batch 249 loss: 0.28851178491929447
  batch 250 loss: 0.28843657648563387
  batch 251 loss: 0.2883839861330283
  batch 252 loss: 0.28836238017630955
  batch 253 loss: 0.2882132155621947
  batch 254 loss: 0.28810636804798456
  batch 255 loss: 0.28813111022406934
  batch 256 loss: 0.2881092223105952
  batch 257 loss: 0.2881086014124206
  batch 258 loss: 0.2881301747736081
  batch 259 loss: 0.28806239999399225
  batch 260 loss: 0.28801922981555644
  batch 261 loss: 0.28796295952979634
  batch 262 loss: 0.28775807908018125
  batch 263 loss: 0.2877814321010285
  batch 264 loss: 0.2876227508214387
  batch 265 loss: 0.2874440257279378
  batch 266 loss: 0.28738334909417573
  batch 267 loss: 0.2874427002467466
  batch 268 loss: 0.2873895466327667
  batch 269 loss: 0.2873195311172301
  batch 270 loss: 0.28739156943780403
  batch 271 loss: 0.28735675187128495
  batch 272 loss: 0.28735022575539704
  batch 273 loss: 0.2873040782662975
  batch 274 loss: 0.2874384599663045
  batch 275 loss: 0.2875461892648177
  batch 276 loss: 0.287540753574475
  batch 277 loss: 0.28751890485037107
  batch 278 loss: 0.28741396148856596
  batch 279 loss: 0.28747256361882745
  batch 280 loss: 0.28736570296542985
  batch 281 loss: 0.28722859293328484
  batch 282 loss: 0.2873138250082943
  batch 283 loss: 0.2872907004908201
  batch 284 loss: 0.2873489465826834
  batch 285 loss: 0.2873001305157678
  batch 286 loss: 0.2872341448714683
  batch 287 loss: 0.28723570470818244
  batch 288 loss: 0.28704799949708915
  batch 289 loss: 0.2871365963793956
  batch 290 loss: 0.28687487162392716
  batch 291 loss: 0.2868303284817135
  batch 292 loss: 0.28680340231281437
  batch 293 loss: 0.2867341161587946
  batch 294 loss: 0.2866593124915142
  batch 295 loss: 0.2865824192257251
  batch 296 loss: 0.2866814341899511
  batch 297 loss: 0.2866110390284246
  batch 298 loss: 0.28668440878391266
  batch 299 loss: 0.286681529969276
  batch 300 loss: 0.28676509896914165
  batch 301 loss: 0.28681613391023936
  batch 302 loss: 0.2867853202369829
  batch 303 loss: 0.2868637480948231
  batch 304 loss: 0.2868459079610674
  batch 305 loss: 0.286688199932458
  batch 306 loss: 0.28673678027843336
  batch 307 loss: 0.28663679544816967
  batch 308 loss: 0.2867554951120507
  batch 309 loss: 0.286739773879545
  batch 310 loss: 0.28675210538410373
  batch 311 loss: 0.28686099003557225
  batch 312 loss: 0.28703406247764063
  batch 313 loss: 0.2871010753388603
  batch 314 loss: 0.287063171671834
  batch 315 loss: 0.28706797962150876
  batch 316 loss: 0.2870035523760922
  batch 317 loss: 0.2870020123216256
  batch 318 loss: 0.28700941899475063
  batch 319 loss: 0.2869833754988673
  batch 320 loss: 0.286955552501604
  batch 321 loss: 0.28693469787869497
  batch 322 loss: 0.28694380111760975
  batch 323 loss: 0.2869027544378127
  batch 324 loss: 0.2867419450187389
  batch 325 loss: 0.2866588160624871
  batch 326 loss: 0.2867140133688055
  batch 327 loss: 0.28682289967478597
  batch 328 loss: 0.28665942330731126
  batch 329 loss: 0.28676108345615825
  batch 330 loss: 0.28671361167322507
  batch 331 loss: 0.28676485128453133
  batch 332 loss: 0.2867191789828869
  batch 333 loss: 0.28677962800940954
  batch 334 loss: 0.2867038277184178
  batch 335 loss: 0.28656891615533114
  batch 336 loss: 0.28649675548963605
  batch 337 loss: 0.28631213183572807
  batch 338 loss: 0.286324476790146
  batch 339 loss: 0.2862697527647722
  batch 340 loss: 0.2862196922302246
  batch 341 loss: 0.28608892080196535
  batch 342 loss: 0.28600109098424686
  batch 343 loss: 0.2860419838404169
  batch 344 loss: 0.286006543048939
  batch 345 loss: 0.2861532788777697
  batch 346 loss: 0.28609338260627204
  batch 347 loss: 0.2860799793808879
  batch 348 loss: 0.2861860915590292
  batch 349 loss: 0.28609616917832875
  batch 350 loss: 0.28610761851072314
  batch 351 loss: 0.28606871156780805
  batch 352 loss: 0.2861310132271187
  batch 353 loss: 0.2861156714789928
  batch 354 loss: 0.28618101198962853
  batch 355 loss: 0.28619669176323315
  batch 356 loss: 0.2862150188158737
  batch 357 loss: 0.2860941747073032
  batch 358 loss: 0.2861074150524326
  batch 359 loss: 0.28612432087198275
  batch 360 loss: 0.2860501401540306
  batch 361 loss: 0.2859910574233433
  batch 362 loss: 0.2859148420599284
  batch 363 loss: 0.28580367171074733
  batch 364 loss: 0.2857247573006284
  batch 365 loss: 0.2857269968888531
  batch 366 loss: 0.2856367196215958
  batch 367 loss: 0.2854677128531952
  batch 368 loss: 0.28538752590184624
  batch 369 loss: 0.28535440418778396
  batch 370 loss: 0.2854111372619062
  batch 371 loss: 0.28538617636315583
  batch 372 loss: 0.285299834064258
  batch 373 loss: 0.2852146463323854
  batch 374 loss: 0.28509347292191206
  batch 375 loss: 0.28501914151509605
  batch 376 loss: 0.2850996734614068
  batch 377 loss: 0.2851086215410055
  batch 378 loss: 0.28501456901036876
  batch 379 loss: 0.285074675421287
  batch 380 loss: 0.28516185115042486
  batch 381 loss: 0.28508053289780155
  batch 382 loss: 0.2849605588622742
  batch 383 loss: 0.28493102735234305
  batch 384 loss: 0.28489937854465097
  batch 385 loss: 0.2849361468058128
  batch 386 loss: 0.284828334892352
  batch 387 loss: 0.28490956177699167
  batch 388 loss: 0.28493833703171345
  batch 389 loss: 0.28492652304987676
  batch 390 loss: 0.284934034714332
  batch 391 loss: 0.2849857952741101
  batch 392 loss: 0.2850510976752456
  batch 393 loss: 0.28513456725588887
  batch 394 loss: 0.28514489947539295
  batch 395 loss: 0.2850689214996145
  batch 396 loss: 0.28513330853346625
  batch 397 loss: 0.28511498496874754
  batch 398 loss: 0.2850898235437259
  batch 399 loss: 0.2851864348975638
  batch 400 loss: 0.28517960123717784
  batch 401 loss: 0.28508897140584977
  batch 402 loss: 0.2851167463915265
  batch 403 loss: 0.28516566904987356
  batch 404 loss: 0.2852119685768491
  batch 405 loss: 0.2852596918373932
  batch 406 loss: 0.2853500736978254
  batch 407 loss: 0.28532190650188954
  batch 408 loss: 0.2854324755642344
  batch 409 loss: 0.2854756625109605
  batch 410 loss: 0.2855238505616421
  batch 411 loss: 0.28547203769213964
  batch 412 loss: 0.28542900284372485
  batch 413 loss: 0.2854784843635905
  batch 414 loss: 0.28543971201791857
  batch 415 loss: 0.2854626833315355
  batch 416 loss: 0.2854990098410501
  batch 417 loss: 0.28547676715204756
  batch 418 loss: 0.2854513382298524
  batch 419 loss: 0.28546965829939147
  batch 420 loss: 0.2855274236982777
  batch 421 loss: 0.2854792846990594
  batch 422 loss: 0.2855461831977017
  batch 423 loss: 0.2855283430211651
  batch 424 loss: 0.2855049392454467
  batch 425 loss: 0.2855408858902314
  batch 426 loss: 0.2854635529744793
  batch 427 loss: 0.28545610278635447
  batch 428 loss: 0.28538620238688506
  batch 429 loss: 0.2853789247207708
  batch 430 loss: 0.28531208124964735
  batch 431 loss: 0.28546839036272187
  batch 432 loss: 0.2855058421070377
  batch 433 loss: 0.28549922775589015
  batch 434 loss: 0.2855817482287433
  batch 435 loss: 0.28547350284696993
  batch 436 loss: 0.28544763701224546
  batch 437 loss: 0.28551254481145394
  batch 438 loss: 0.2856146823869993
  batch 439 loss: 0.28561829325821514
  batch 440 loss: 0.28565419688820837
  batch 441 loss: 0.28562028756757984
  batch 442 loss: 0.28560435077453633
  batch 443 loss: 0.28559292815877946
  batch 444 loss: 0.28548452993099754
  batch 445 loss: 0.28555209352059313
  batch 446 loss: 0.285525678753051
  batch 447 loss: 0.28544649351763246
  batch 448 loss: 0.28543281385542024
  batch 449 loss: 0.28546285201154464
  batch 450 loss: 0.2854227106107606
  batch 451 loss: 0.28541834328629223
  batch 452 loss: 0.2853978614166247
  batch 453 loss: 0.2854910288622837
  batch 454 loss: 0.2855402112729224
  batch 455 loss: 0.2855416896251532
  batch 456 loss: 0.28554731493064184
  batch 457 loss: 0.28559341762598023
  batch 458 loss: 0.28554687850730387
  batch 459 loss: 0.2855919973198365
  batch 460 loss: 0.2856916722720084
  batch 461 loss: 0.28567296528376623
  batch 462 loss: 0.28569894780715305
  batch 463 loss: 0.2856663389867634
  batch 464 loss: 0.2856849978260439
  batch 465 loss: 0.2856754066803122
  batch 466 loss: 0.2855680230925012
  batch 467 loss: 0.28560174542142186
  batch 468 loss: 0.2856105088105059
  batch 469 loss: 0.2857577684782207
  batch 470 loss: 0.2857592536730969
  batch 471 loss: 0.2857577130356665
  batch 472 loss: 0.2856397949853691
LOSS train 0.2856397949853691 valid 0.24656996130943298
LOSS train 0.2856397949853691 valid 0.23845337331295013
LOSS train 0.2856397949853691 valid 0.24063071111838022
LOSS train 0.2856397949853691 valid 0.23355301097035408
LOSS train 0.2856397949853691 valid 0.22674465775489808
LOSS train 0.2856397949853691 valid 0.23392857611179352
LOSS train 0.2856397949853691 valid 0.24012457047189986
LOSS train 0.2856397949853691 valid 0.23832805454730988
LOSS train 0.2856397949853691 valid 0.24050120843781364
LOSS train 0.2856397949853691 valid 0.242844095826149
LOSS train 0.2856397949853691 valid 0.23949397964911026
LOSS train 0.2856397949853691 valid 0.24102478474378586
LOSS train 0.2856397949853691 valid 0.24134794106850258
LOSS train 0.2856397949853691 valid 0.24182877370289393
LOSS train 0.2856397949853691 valid 0.23946723341941833
LOSS train 0.2856397949853691 valid 0.23979603312909603
LOSS train 0.2856397949853691 valid 0.23996776342391968
LOSS train 0.2856397949853691 valid 0.24136454860369363
LOSS train 0.2856397949853691 valid 0.24379605368564003
LOSS train 0.2856397949853691 valid 0.24275768250226976
LOSS train 0.2856397949853691 valid 0.24167642990748087
LOSS train 0.2856397949853691 valid 0.24021423946727405
LOSS train 0.2856397949853691 valid 0.24075737336407538
LOSS train 0.2856397949853691 valid 0.23980820178985596
LOSS train 0.2856397949853691 valid 0.23885679185390474
LOSS train 0.2856397949853691 valid 0.23979370697186544
LOSS train 0.2856397949853691 valid 0.2394241652003041
LOSS train 0.2856397949853691 valid 0.24023626585091865
LOSS train 0.2856397949853691 valid 0.2407960907138627
LOSS train 0.2856397949853691 valid 0.2424038752913475
LOSS train 0.2856397949853691 valid 0.24322841773110052
LOSS train 0.2856397949853691 valid 0.2428881675004959
LOSS train 0.2856397949853691 valid 0.24359586202737057
LOSS train 0.2856397949853691 valid 0.2435336866799523
LOSS train 0.2856397949853691 valid 0.24496681434767587
LOSS train 0.2856397949853691 valid 0.24440047103497717
LOSS train 0.2856397949853691 valid 0.24441046529525035
LOSS train 0.2856397949853691 valid 0.24551701192793093
LOSS train 0.2856397949853691 valid 0.24508935251297095
LOSS train 0.2856397949853691 valid 0.24541924186050892
LOSS train 0.2856397949853691 valid 0.24645267645033395
LOSS train 0.2856397949853691 valid 0.24610075922239394
LOSS train 0.2856397949853691 valid 0.2460592929707017
LOSS train 0.2856397949853691 valid 0.24631779979575763
LOSS train 0.2856397949853691 valid 0.24563803573449453
LOSS train 0.2856397949853691 valid 0.246263371537561
LOSS train 0.2856397949853691 valid 0.24668712089670466
LOSS train 0.2856397949853691 valid 0.24654789455235004
LOSS train 0.2856397949853691 valid 0.24706250064227045
LOSS train 0.2856397949853691 valid 0.24656820505857469
LOSS train 0.2856397949853691 valid 0.24695878490513445
LOSS train 0.2856397949853691 valid 0.24651720804663804
LOSS train 0.2856397949853691 valid 0.24680506536420785
LOSS train 0.2856397949853691 valid 0.24669097474327795
LOSS train 0.2856397949853691 valid 0.24687085910276932
LOSS train 0.2856397949853691 valid 0.24636120668479375
LOSS train 0.2856397949853691 valid 0.24609853301131934
LOSS train 0.2856397949853691 valid 0.24577729403972626
LOSS train 0.2856397949853691 valid 0.24626561696246518
LOSS train 0.2856397949853691 valid 0.2458306570847829
LOSS train 0.2856397949853691 valid 0.24581494854121913
LOSS train 0.2856397949853691 valid 0.24667005745633955
LOSS train 0.2856397949853691 valid 0.2468730757633845
LOSS train 0.2856397949853691 valid 0.24751670029945672
LOSS train 0.2856397949853691 valid 0.24770707190036773
LOSS train 0.2856397949853691 valid 0.24767225100235504
LOSS train 0.2856397949853691 valid 0.24704853500892868
LOSS train 0.2856397949853691 valid 0.24715938682065292
LOSS train 0.2856397949853691 valid 0.24629048139288806
LOSS train 0.2856397949853691 valid 0.24651773763554438
LOSS train 0.2856397949853691 valid 0.24654285106020915
LOSS train 0.2856397949853691 valid 0.24687140083147419
LOSS train 0.2856397949853691 valid 0.24687804363361776
LOSS train 0.2856397949853691 valid 0.24689304707823573
LOSS train 0.2856397949853691 valid 0.24681483149528505
LOSS train 0.2856397949853691 valid 0.24726437698853643
LOSS train 0.2856397949853691 valid 0.24739133305363842
LOSS train 0.2856397949853691 valid 0.2474426371164811
LOSS train 0.2856397949853691 valid 0.24750837268708628
LOSS train 0.2856397949853691 valid 0.24679077249020337
LOSS train 0.2856397949853691 valid 0.2458127448220312
LOSS train 0.2856397949853691 valid 0.24593825547433482
LOSS train 0.2856397949853691 valid 0.2457396009241242
LOSS train 0.2856397949853691 valid 0.24562787051711762
LOSS train 0.2856397949853691 valid 0.24521978848120746
LOSS train 0.2856397949853691 valid 0.24456785480643428
LOSS train 0.2856397949853691 valid 0.24467883610177313
LOSS train 0.2856397949853691 valid 0.2442400406368754
LOSS train 0.2856397949853691 valid 0.24452367374736272
LOSS train 0.2856397949853691 valid 0.24470021343893475
LOSS train 0.2856397949853691 valid 0.24487968476919028
LOSS train 0.2856397949853691 valid 0.2449386260755684
LOSS train 0.2856397949853691 valid 0.2446241941182844
LOSS train 0.2856397949853691 valid 0.24485614372694747
LOSS train 0.2856397949853691 valid 0.2443837423073618
LOSS train 0.2856397949853691 valid 0.2446581336359183
LOSS train 0.2856397949853691 valid 0.24489737939588802
LOSS train 0.2856397949853691 valid 0.24493118588413512
LOSS train 0.2856397949853691 valid 0.24508038509373714
LOSS train 0.2856397949853691 valid 0.24538801208138467
LOSS train 0.2856397949853691 valid 0.24579169977419446
LOSS train 0.2856397949853691 valid 0.2458687490400146
LOSS train 0.2856397949853691 valid 0.24587589777210384
LOSS train 0.2856397949853691 valid 0.24580012132915166
LOSS train 0.2856397949853691 valid 0.24583488120919184
LOSS train 0.2856397949853691 valid 0.24608188909742068
LOSS train 0.2856397949853691 valid 0.24579903817622462
LOSS train 0.2856397949853691 valid 0.2459811253680123
LOSS train 0.2856397949853691 valid 0.24645274522107677
LOSS train 0.2856397949853691 valid 0.24675776931372556
LOSS train 0.2856397949853691 valid 0.24635681979828053
LOSS train 0.2856397949853691 valid 0.24604720103421382
LOSS train 0.2856397949853691 valid 0.24610060044621998
LOSS train 0.2856397949853691 valid 0.24605640978143925
LOSS train 0.2856397949853691 valid 0.24596141395361526
LOSS train 0.2856397949853691 valid 0.2462924063719552
LOSS train 0.2856397949853691 valid 0.24659121342194387
LOSS train 0.2856397949853691 valid 0.24640505609370894
LOSS train 0.2856397949853691 valid 0.24637783963640197
LOSS train 0.2856397949853691 valid 0.24601815827190876
LOSS train 0.2856397949853691 valid 0.2458436384427646
LOSS train 0.2856397949853691 valid 0.2456990287440722
LOSS train 0.2856397949853691 valid 0.24570120495509326
LOSS train 0.2856397949853691 valid 0.24585708254768002
LOSS train 0.2856397949853691 valid 0.24581012439727784
LOSS train 0.2856397949853691 valid 0.24604756964577568
LOSS train 0.2856397949853691 valid 0.24588363412327655
LOSS train 0.2856397949853691 valid 0.2461878239409998
LOSS train 0.2856397949853691 valid 0.24618871868118758
LOSS train 0.2856397949853691 valid 0.24619565296631593
LOSS train 0.2856397949853691 valid 0.24622454772923738
LOSS train 0.2856397949853691 valid 0.24595044541991118
LOSS train 0.2856397949853691 valid 0.24564716959358157
LOSS train 0.2856397949853691 valid 0.24557227986072427
LOSS train 0.2856397949853691 valid 0.2455115572169975
LOSS train 0.2856397949853691 valid 0.24534924766596625
LOSS train 0.2856397949853691 valid 0.24524271749231938
LOSS train 0.2856397949853691 valid 0.24523521247117416
LOSS train 0.2856397949853691 valid 0.24510587172971354
LOSS train 0.2856397949853691 valid 0.24517945423722268
LOSS train 0.2856397949853691 valid 0.2451918772983213
LOSS train 0.2856397949853691 valid 0.24539242926197993
LOSS train 0.2856397949853691 valid 0.24542684519624378
LOSS train 0.2856397949853691 valid 0.2454682258475158
LOSS train 0.2856397949853691 valid 0.24528275183562576
LOSS train 0.2856397949853691 valid 0.24549834522074215
LOSS train 0.2856397949853691 valid 0.24526458512358115
LOSS train 0.2856397949853691 valid 0.24596575969779813
LOSS train 0.2856397949853691 valid 0.24599429004944412
LOSS train 0.2856397949853691 valid 0.24593553682168326
LOSS train 0.2856397949853691 valid 0.2458656816095706
LOSS train 0.2856397949853691 valid 0.2457818982044333
LOSS train 0.2856397949853691 valid 0.24580919771802193
LOSS train 0.2856397949853691 valid 0.24586095641572753
LOSS train 0.2856397949853691 valid 0.2459129011438739
LOSS train 0.2856397949853691 valid 0.24627009463997987
LOSS train 0.2856397949853691 valid 0.24620540307205954
LOSS train 0.2856397949853691 valid 0.24626514758867554
LOSS train 0.2856397949853691 valid 0.24632127451821692
LOSS train 0.2856397949853691 valid 0.2460585289634764
LOSS train 0.2856397949853691 valid 0.24605824127330544
LOSS train 0.2856397949853691 valid 0.24603361599607232
LOSS train 0.2856397949853691 valid 0.24602716859498638
LOSS train 0.2856397949853691 valid 0.2459261669073163
LOSS train 0.2856397949853691 valid 0.24573308951926953
LOSS train 0.2856397949853691 valid 0.24587255518838583
LOSS train 0.2856397949853691 valid 0.24597855617186268
LOSS train 0.2856397949853691 valid 0.24593155902056468
LOSS train 0.2856397949853691 valid 0.2460408972565239
LOSS train 0.2856397949853691 valid 0.24624042633701773
LOSS train 0.2856397949853691 valid 0.24617878892268355
LOSS train 0.2856397949853691 valid 0.245963116977797
LOSS train 0.2856397949853691 valid 0.2459817432254725
LOSS train 0.2856397949853691 valid 0.2459392179360335
LOSS train 0.2856397949853691 valid 0.24593353254454475
LOSS train 0.2856397949853691 valid 0.24585394434292207
LOSS train 0.2856397949853691 valid 0.24586918310257
LOSS train 0.2856397949853691 valid 0.24613001909148827
LOSS train 0.2856397949853691 valid 0.24606432078936913
LOSS train 0.2856397949853691 valid 0.24615600804487864
LOSS train 0.2856397949853691 valid 0.24640107879322537
LOSS train 0.2856397949853691 valid 0.24645541379084954
LOSS train 0.2856397949853691 valid 0.24631365156564555
LOSS train 0.2856397949853691 valid 0.24628626482318278
LOSS train 0.2856397949853691 valid 0.24607519845704776
LOSS train 0.2856397949853691 valid 0.2459699081317071
LOSS train 0.2856397949853691 valid 0.24583039149881047
LOSS train 0.2856397949853691 valid 0.24592908345004347
LOSS train 0.2856397949853691 valid 0.2460496518662367
LOSS train 0.2856397949853691 valid 0.24612221906059667
LOSS train 0.2856397949853691 valid 0.24599885433444177
LOSS train 0.2856397949853691 valid 0.24592405394650996
LOSS train 0.2856397949853691 valid 0.2460154233973261
LOSS train 0.2856397949853691 valid 0.24589052842449896
LOSS train 0.2856397949853691 valid 0.24563153049884698
LOSS train 0.2856397949853691 valid 0.24570839563194585
LOSS train 0.2856397949853691 valid 0.24576123111744216
LOSS train 0.2856397949853691 valid 0.24570942624951853
LOSS train 0.2856397949853691 valid 0.24568228790508442
LOSS train 0.2856397949853691 valid 0.24545913323760032
LOSS train 0.2856397949853691 valid 0.24530026850415698
LOSS train 0.2856397949853691 valid 0.24516198260359245
LOSS train 0.2856397949853691 valid 0.2450428214566461
LOSS train 0.2856397949853691 valid 0.2449819322164152
LOSS train 0.2856397949853691 valid 0.24483381778728672
LOSS train 0.2856397949853691 valid 0.24482945490230634
LOSS train 0.2856397949853691 valid 0.2448233623723477
LOSS train 0.2856397949853691 valid 0.2446796502201603
LOSS train 0.2856397949853691 valid 0.24454429553171095
LOSS train 0.2856397949853691 valid 0.24463203797737756
LOSS train 0.2856397949853691 valid 0.24460762781554488
LOSS train 0.2856397949853691 valid 0.24440329065019228
LOSS train 0.2856397949853691 valid 0.24447681107711344
LOSS train 0.2856397949853691 valid 0.24438593369499545
LOSS train 0.2856397949853691 valid 0.24423366639503213
LOSS train 0.2856397949853691 valid 0.24415029688841766
LOSS train 0.2856397949853691 valid 0.2441132718928948
LOSS train 0.2856397949853691 valid 0.24410109645729766
LOSS train 0.2856397949853691 valid 0.2441476673296053
LOSS train 0.2856397949853691 valid 0.24430728405714036
LOSS train 0.2856397949853691 valid 0.24440376251531404
LOSS train 0.2856397949853691 valid 0.24434811373551688
LOSS train 0.2856397949853691 valid 0.24447591649577222
LOSS train 0.2856397949853691 valid 0.24464516967002833
LOSS train 0.2856397949853691 valid 0.24467335449324715
LOSS train 0.2856397949853691 valid 0.24475408299306853
LOSS train 0.2856397949853691 valid 0.2450301637996136
LOSS train 0.2856397949853691 valid 0.24511138859548068
LOSS train 0.2856397949853691 valid 0.24514418723281295
LOSS train 0.2856397949853691 valid 0.2452917745579844
LOSS train 0.2856397949853691 valid 0.245288735537818
LOSS train 0.2856397949853691 valid 0.24534625990380501
LOSS train 0.2856397949853691 valid 0.24524217952730318
LOSS train 0.2856397949853691 valid 0.24531248905974576
LOSS train 0.2856397949853691 valid 0.24538926070040845
LOSS train 0.2856397949853691 valid 0.24534873260279833
LOSS train 0.2856397949853691 valid 0.24540866590753385
LOSS train 0.2856397949853691 valid 0.24542822945518655
LOSS train 0.2856397949853691 valid 0.2453003328090931
LOSS train 0.2856397949853691 valid 0.2452924977367123
LOSS train 0.2856397949853691 valid 0.2454051261002592
LOSS train 0.2856397949853691 valid 0.24519728967719825
LOSS train 0.2856397949853691 valid 0.24530939475744348
LOSS train 0.2856397949853691 valid 0.24548144290437462
LOSS train 0.2856397949853691 valid 0.24559732949247165
LOSS train 0.2856397949853691 valid 0.2455860287314508
LOSS train 0.2856397949853691 valid 0.24557875114896519
LOSS train 0.2856397949853691 valid 0.24555418682434865
LOSS train 0.2856397949853691 valid 0.24546462578706474
LOSS train 0.2856397949853691 valid 0.24559675472974776
LOSS train 0.2856397949853691 valid 0.24552045608184253
LOSS train 0.2856397949853691 valid 0.24558837422066265
LOSS train 0.2856397949853691 valid 0.24547091756649167
LOSS train 0.2856397949853691 valid 0.24544863508442255
LOSS train 0.2856397949853691 valid 0.2455188842380748
LOSS train 0.2856397949853691 valid 0.24557462451048195
LOSS train 0.2856397949853691 valid 0.24545462128021373
LOSS train 0.2856397949853691 valid 0.24550501776989117
LOSS train 0.2856397949853691 valid 0.24546892043476398
LOSS train 0.2856397949853691 valid 0.24546813437571893
LOSS train 0.2856397949853691 valid 0.24551601062789274
LOSS train 0.2856397949853691 valid 0.24553676051947906
LOSS train 0.2856397949853691 valid 0.245490286173476
LOSS train 0.2856397949853691 valid 0.2455272124798009
LOSS train 0.2856397949853691 valid 0.24544060848793894
LOSS train 0.2856397949853691 valid 0.24541589023923516
LOSS train 0.2856397949853691 valid 0.24557425165444277
LOSS train 0.2856397949853691 valid 0.24570512026548386
LOSS train 0.2856397949853691 valid 0.24575934944099653
LOSS train 0.2856397949853691 valid 0.24573307660994706
LOSS train 0.2856397949853691 valid 0.24576401078173155
LOSS train 0.2856397949853691 valid 0.24600965009235284
LOSS train 0.2856397949853691 valid 0.24603656889536443
LOSS train 0.2856397949853691 valid 0.2460305286149909
LOSS train 0.2856397949853691 valid 0.24597657160325484
LOSS train 0.2856397949853691 valid 0.24593072508772215
LOSS train 0.2856397949853691 valid 0.24575854805617556
LOSS train 0.2856397949853691 valid 0.24562231086784128
LOSS train 0.2856397949853691 valid 0.24566354188654158
LOSS train 0.2856397949853691 valid 0.24568527527153491
LOSS train 0.2856397949853691 valid 0.24562502538394249
LOSS train 0.2856397949853691 valid 0.2454454393145886
LOSS train 0.2856397949853691 valid 0.24549209139582967
LOSS train 0.2856397949853691 valid 0.24560285511780794
LOSS train 0.2856397949853691 valid 0.24561868754395266
LOSS train 0.2856397949853691 valid 0.24548493450755007
LOSS train 0.2856397949853691 valid 0.2453699923973881
LOSS train 0.2856397949853691 valid 0.2453873994656735
LOSS train 0.2856397949853691 valid 0.24542034847926103
LOSS train 0.2856397949853691 valid 0.24543034819693402
LOSS train 0.2856397949853691 valid 0.2454347256635063
LOSS train 0.2856397949853691 valid 0.24538499019937973
LOSS train 0.2856397949853691 valid 0.2454317169893317
LOSS train 0.2856397949853691 valid 0.24551076523098006
LOSS train 0.2856397949853691 valid 0.24557839637085543
LOSS train 0.2856397949853691 valid 0.24556323397602584
LOSS train 0.2856397949853691 valid 0.24555155544569998
LOSS train 0.2856397949853691 valid 0.24554025071779353
LOSS train 0.2856397949853691 valid 0.24556932067592008
LOSS train 0.2856397949853691 valid 0.24551443894704184
LOSS train 0.2856397949853691 valid 0.24555829612915697
LOSS train 0.2856397949853691 valid 0.24558495221943255
LOSS train 0.2856397949853691 valid 0.24564828229422617
LOSS train 0.2856397949853691 valid 0.2456333779386784
LOSS train 0.2856397949853691 valid 0.2455523964811544
LOSS train 0.2856397949853691 valid 0.245504452901728
LOSS train 0.2856397949853691 valid 0.24544059123589085
LOSS train 0.2856397949853691 valid 0.24533031687333987
LOSS train 0.2856397949853691 valid 0.24547330451628924
LOSS train 0.2856397949853691 valid 0.24542984467360282
LOSS train 0.2856397949853691 valid 0.24531167046050167
LOSS train 0.2856397949853691 valid 0.24530090439396027
LOSS train 0.2856397949853691 valid 0.2453266732608929
LOSS train 0.2856397949853691 valid 0.24538259037361024
LOSS train 0.2856397949853691 valid 0.2454003612200419
LOSS train 0.2856397949853691 valid 0.24530483815300314
LOSS train 0.2856397949853691 valid 0.24532001228174577
LOSS train 0.2856397949853691 valid 0.24539029809101573
LOSS train 0.2856397949853691 valid 0.24544081563673048
LOSS train 0.2856397949853691 valid 0.24538427889347075
LOSS train 0.2856397949853691 valid 0.24544038737303
LOSS train 0.2856397949853691 valid 0.2453915659592759
LOSS train 0.2856397949853691 valid 0.2453763264897438
LOSS train 0.2856397949853691 valid 0.24535082740548217
LOSS train 0.2856397949853691 valid 0.24530902330691998
LOSS train 0.2856397949853691 valid 0.24549020296591192
LOSS train 0.2856397949853691 valid 0.24558189948764417
LOSS train 0.2856397949853691 valid 0.2455631569936508
LOSS train 0.2856397949853691 valid 0.24566278024647373
LOSS train 0.2856397949853691 valid 0.24564814572081423
LOSS train 0.2856397949853691 valid 0.24562485236955914
LOSS train 0.2856397949853691 valid 0.24552074981382094
LOSS train 0.2856397949853691 valid 0.24549832662662585
LOSS train 0.2856397949853691 valid 0.24561125316662702
LOSS train 0.2856397949853691 valid 0.24562196691534413
LOSS train 0.2856397949853691 valid 0.24565235733808505
LOSS train 0.2856397949853691 valid 0.24557291373654713
LOSS train 0.2856397949853691 valid 0.2455443921967371
LOSS train 0.2856397949853691 valid 0.24558920437431617
LOSS train 0.2856397949853691 valid 0.24565455830272506
LOSS train 0.2856397949853691 valid 0.24555728272084268
LOSS train 0.2856397949853691 valid 0.24551056347220962
LOSS train 0.2856397949853691 valid 0.24551084967813408
LOSS train 0.2856397949853691 valid 0.2456557097476582
LOSS train 0.2856397949853691 valid 0.245645474869272
LOSS train 0.2856397949853691 valid 0.24554759397499823
LOSS train 0.2856397949853691 valid 0.24544694504408054
LOSS train 0.2856397949853691 valid 0.24538427882495967
LOSS train 0.2856397949853691 valid 0.2454713055260885
LOSS train 0.2856397949853691 valid 0.24534277285848344
LOSS train 0.2856397949853691 valid 0.24525364443787143
LOSS train 0.2856397949853691 valid 0.2452116598883136
LOSS train 0.2856397949853691 valid 0.24526635475097905
LOSS train 0.2856397949853691 valid 0.24532850326622946
LOSS train 0.2856397949853691 valid 0.24546095917762165
LOSS train 0.2856397949853691 valid 0.245533562317658
LOSS train 0.2856397949853691 valid 0.2454785011312207
LOSS train 0.2856397949853691 valid 0.2454088334014962
LOSS train 0.2856397949853691 valid 0.24537622019607044
LOSS train 0.2856397949853691 valid 0.2452900187836753
LOSS train 0.2856397949853691 valid 0.2452232621795913
LOSS train 0.2856397949853691 valid 0.2453170452081696
LOSS train 0.2856397949853691 valid 0.24517126767267536
LOSS train 0.2856397949853691 valid 0.24515517297518122
LOSS train 0.2856397949853691 valid 0.24510975292284196
LOSS train 0.2856397949853691 valid 0.245036707427658
LOSS train 0.2856397949853691 valid 0.24492559035889785
LOSS train 0.2856397949853691 valid 0.2449133177895261
LOSS train 0.2856397949853691 valid 0.24494891191724194
EPOCH 7:
  batch 1 loss: 0.277462363243103
  batch 2 loss: 0.2681933343410492
  batch 3 loss: 0.26939767599105835
  batch 4 loss: 0.26780328154563904
  batch 5 loss: 0.2724270045757294
  batch 6 loss: 0.26758673787117004
  batch 7 loss: 0.2716083526611328
  batch 8 loss: 0.27701129391789436
  batch 9 loss: 0.27749843067593044
  batch 10 loss: 0.27822511196136473
  batch 11 loss: 0.2790909301150929
  batch 12 loss: 0.27565450221300125
  batch 13 loss: 0.27450302701729995
  batch 14 loss: 0.2752971947193146
  batch 15 loss: 0.27704329093297325
  batch 16 loss: 0.27730753272771835
  batch 17 loss: 0.2749403574887444
  batch 18 loss: 0.27602385977904004
  batch 19 loss: 0.2749833997927214
  batch 20 loss: 0.2739915192127228
  batch 21 loss: 0.27401163180669147
  batch 22 loss: 0.2739407460797917
  batch 23 loss: 0.2732755775037019
  batch 24 loss: 0.27245087549090385
  batch 25 loss: 0.2750828766822815
  batch 26 loss: 0.2734201837044496
  batch 27 loss: 0.27426621428242437
  batch 28 loss: 0.2727513558098248
  batch 29 loss: 0.2733486674982926
  batch 30 loss: 0.2724638710419337
  batch 31 loss: 0.27361850680843475
  batch 32 loss: 0.27316469233483076
  batch 33 loss: 0.27252976551200403
  batch 34 loss: 0.27270760080393625
  batch 35 loss: 0.27299564821379524
  batch 36 loss: 0.2731410753395822
  batch 37 loss: 0.27359326626803426
  batch 38 loss: 0.27432306973557724
  batch 39 loss: 0.27419028832362247
  batch 40 loss: 0.2742778703570366
  batch 41 loss: 0.2740327163440425
  batch 42 loss: 0.2746008861632574
  batch 43 loss: 0.27466723738714705
  batch 44 loss: 0.27492391792210663
  batch 45 loss: 0.27458272245195176
  batch 46 loss: 0.27435450903747394
  batch 47 loss: 0.2746364601114963
  batch 48 loss: 0.2739463479568561
  batch 49 loss: 0.2737901520972349
  batch 50 loss: 0.2736093157529831
  batch 51 loss: 0.2735357898123124
  batch 52 loss: 0.2738632164322413
  batch 53 loss: 0.2732158904930331
  batch 54 loss: 0.27315118025850366
  batch 55 loss: 0.27276124249805106
  batch 56 loss: 0.27240494904773577
  batch 57 loss: 0.27160690384998654
  batch 58 loss: 0.27229563891887665
  batch 59 loss: 0.27256766466771143
  batch 60 loss: 0.2723298589388529
  batch 61 loss: 0.27236310575829176
  batch 62 loss: 0.27220186783421424
  batch 63 loss: 0.27179018325275844
  batch 64 loss: 0.2725997045636177
  batch 65 loss: 0.27194657417444085
  batch 66 loss: 0.2717444815418937
  batch 67 loss: 0.2719616436246616
  batch 68 loss: 0.2722146151696934
  batch 69 loss: 0.27234218690706335
  batch 70 loss: 0.27262835928371976
  batch 71 loss: 0.27240381862076235
  batch 72 loss: 0.2725795726809237
  batch 73 loss: 0.272812110103973
  batch 74 loss: 0.2728712433898771
  batch 75 loss: 0.2724813775221507
  batch 76 loss: 0.2728171266223255
  batch 77 loss: 0.2728207324232374
  batch 78 loss: 0.2727567832439374
  batch 79 loss: 0.27317494267149817
  batch 80 loss: 0.27303875274956224
  batch 81 loss: 0.27312036760059405
  batch 82 loss: 0.2739065158657911
  batch 83 loss: 0.2738917602832059
  batch 84 loss: 0.2741359958336467
  batch 85 loss: 0.2741175921524272
  batch 86 loss: 0.27439703878968263
  batch 87 loss: 0.27433554982316904
  batch 88 loss: 0.2739731174978343
  batch 89 loss: 0.2737875490376119
  batch 90 loss: 0.273707092139456
  batch 91 loss: 0.27392195902027927
  batch 92 loss: 0.2738642022013664
  batch 93 loss: 0.2737736218078162
  batch 94 loss: 0.27380885976426145
  batch 95 loss: 0.2735384988157373
  batch 96 loss: 0.27381549837688607
  batch 97 loss: 0.2742918490134564
  batch 98 loss: 0.27466462643779055
  batch 99 loss: 0.2750162199290112
  batch 100 loss: 0.27514062702655795
  batch 101 loss: 0.2752490374121335
  batch 102 loss: 0.27566707309554606
  batch 103 loss: 0.27597202982717345
  batch 104 loss: 0.27604416757822037
  batch 105 loss: 0.27583443579219635
  batch 106 loss: 0.27640989788298337
  batch 107 loss: 0.2759124106057336
  batch 108 loss: 0.27573453207258825
  batch 109 loss: 0.2757778871770299
  batch 110 loss: 0.27569989277557894
  batch 111 loss: 0.27573535477255917
  batch 112 loss: 0.2756029972806573
  batch 113 loss: 0.2761660831428207
  batch 114 loss: 0.2766551371467741
  batch 115 loss: 0.2769232751234718
  batch 116 loss: 0.2772158809520047
  batch 117 loss: 0.2775039243647176
  batch 118 loss: 0.27742513496491866
  batch 119 loss: 0.27769001579585195
  batch 120 loss: 0.2777755567183097
  batch 121 loss: 0.27760289880362427
  batch 122 loss: 0.27760727960066717
  batch 123 loss: 0.27728259866315175
  batch 124 loss: 0.27754209202624136
  batch 125 loss: 0.2775142985582352
  batch 126 loss: 0.27754659205675125
  batch 127 loss: 0.2779193071164484
  batch 128 loss: 0.27797172509599477
  batch 129 loss: 0.278229724182639
  batch 130 loss: 0.2785087501773467
  batch 131 loss: 0.27875900166180295
  batch 132 loss: 0.27895301072434947
  batch 133 loss: 0.279201729517234
  batch 134 loss: 0.2793473028647366
  batch 135 loss: 0.2793593246627737
  batch 136 loss: 0.2793987821787596
  batch 137 loss: 0.2792677969610604
  batch 138 loss: 0.2792645410999008
  batch 139 loss: 0.27962330444682415
  batch 140 loss: 0.2798049706433501
  batch 141 loss: 0.27981570098839753
  batch 142 loss: 0.2798083944010063
  batch 143 loss: 0.27990440044786546
  batch 144 loss: 0.27986029877016944
  batch 145 loss: 0.2798736916533832
  batch 146 loss: 0.2799256069407071
  batch 147 loss: 0.28027942239427245
  batch 148 loss: 0.2802833094186074
  batch 149 loss: 0.2802392053324104
  batch 150 loss: 0.28007556825876234
  batch 151 loss: 0.2800958100336277
  batch 152 loss: 0.2800864358677676
  batch 153 loss: 0.2798761087500192
  batch 154 loss: 0.28029025500857985
  batch 155 loss: 0.2802541964477108
  batch 156 loss: 0.28021616631975543
  batch 157 loss: 0.2805037998659596
  batch 158 loss: 0.280842227554774
  batch 159 loss: 0.281081275838726
  batch 160 loss: 0.2810104792006314
  batch 161 loss: 0.28114489470579607
  batch 162 loss: 0.2812772734481611
  batch 163 loss: 0.2812897306095603
  batch 164 loss: 0.28131823759616875
  batch 165 loss: 0.28112949268384413
  batch 166 loss: 0.2810201320842088
  batch 167 loss: 0.28087009810758923
  batch 168 loss: 0.2808659893593618
  batch 169 loss: 0.28056310452300415
  batch 170 loss: 0.280465589288403
  batch 171 loss: 0.2804379017903791
  batch 172 loss: 0.2805274971869103
  batch 173 loss: 0.28052033880197935
  batch 174 loss: 0.28059118874799244
  batch 175 loss: 0.28064026704856326
  batch 176 loss: 0.2806177687069232
  batch 177 loss: 0.28071248590272696
  batch 178 loss: 0.280797328590677
  batch 179 loss: 0.2809356436882605
  batch 180 loss: 0.2809267526699437
  batch 181 loss: 0.2808374664566135
  batch 182 loss: 0.28076237300922585
  batch 183 loss: 0.28056449080751245
  batch 184 loss: 0.28035421888141526
  batch 185 loss: 0.2804200931980803
  batch 186 loss: 0.28061633813445286
  batch 187 loss: 0.2805639499808378
  batch 188 loss: 0.2803704191712623
  batch 189 loss: 0.2802720676969599
  batch 190 loss: 0.2804258148921163
  batch 191 loss: 0.28051999398551064
  batch 192 loss: 0.28075893813123304
  batch 193 loss: 0.2807784562283847
  batch 194 loss: 0.2809542535813813
  batch 195 loss: 0.2810759177574745
  batch 196 loss: 0.28091111535928687
  batch 197 loss: 0.2807976375073951
  batch 198 loss: 0.2808802660667535
  batch 199 loss: 0.28103234989559234
  batch 200 loss: 0.2811494264006615
  batch 201 loss: 0.2811562789613335
  batch 202 loss: 0.2811427899811528
  batch 203 loss: 0.2812680361306139
  batch 204 loss: 0.2811571832965402
  batch 205 loss: 0.2812734249161511
  batch 206 loss: 0.2813902981940982
  batch 207 loss: 0.2814823950546375
  batch 208 loss: 0.2814844916932858
  batch 209 loss: 0.28125309544887267
  batch 210 loss: 0.28139537998608183
  batch 211 loss: 0.2813546781856302
  batch 212 loss: 0.2813650220632553
  batch 213 loss: 0.28126149376233417
  batch 214 loss: 0.28129134891189145
  batch 215 loss: 0.28109869596570036
  batch 216 loss: 0.2810135323692251
  batch 217 loss: 0.281006013330776
  batch 218 loss: 0.2810160866297713
  batch 219 loss: 0.28096159707465673
  batch 220 loss: 0.2810137471014803
  batch 221 loss: 0.28107056480187637
  batch 222 loss: 0.28115776275192295
  batch 223 loss: 0.28116664718084805
  batch 224 loss: 0.2811891991379006
  batch 225 loss: 0.28112673163414004
  batch 226 loss: 0.2811758424592229
  batch 227 loss: 0.28096864402031585
  batch 228 loss: 0.28089868891657443
  batch 229 loss: 0.28078729083444354
  batch 230 loss: 0.2808928651654202
  batch 231 loss: 0.28085600813745937
  batch 232 loss: 0.28071110913979597
  batch 233 loss: 0.2806776929566789
  batch 234 loss: 0.2808689459776267
  batch 235 loss: 0.2809546701451565
  batch 236 loss: 0.2809463206489207
  batch 237 loss: 0.2810058983569407
  batch 238 loss: 0.28096295429878876
  batch 239 loss: 0.2809378994808037
  batch 240 loss: 0.2810443735371033
  batch 241 loss: 0.2810293274054389
  batch 242 loss: 0.2808818026515078
  batch 243 loss: 0.28079778888098006
  batch 244 loss: 0.2806580338443889
  batch 245 loss: 0.28072740307876043
  batch 246 loss: 0.28066926195127206
  batch 247 loss: 0.2807086413567848
  batch 248 loss: 0.2806946003749486
  batch 249 loss: 0.28061449019066304
  batch 250 loss: 0.2805344074368477
  batch 251 loss: 0.28050924338904987
  batch 252 loss: 0.2804039412192882
  batch 253 loss: 0.2802555522193079
  batch 254 loss: 0.2801627799516588
  batch 255 loss: 0.2801942687408597
  batch 256 loss: 0.2801668820902705
  batch 257 loss: 0.28013888233366646
  batch 258 loss: 0.28012396221937136
  batch 259 loss: 0.28008561359869466
  batch 260 loss: 0.28005845099687576
  batch 261 loss: 0.2799935539563497
  batch 262 loss: 0.2797946440240809
  batch 263 loss: 0.2797872764642701
  batch 264 loss: 0.27957276819330273
  batch 265 loss: 0.27939044737590935
  batch 266 loss: 0.2793461601424934
  batch 267 loss: 0.279402369807722
  batch 268 loss: 0.27935987216100766
  batch 269 loss: 0.27931317304811515
  batch 270 loss: 0.2793249382464974
  batch 271 loss: 0.2792715693641853
  batch 272 loss: 0.27924418969846826
  batch 273 loss: 0.2792101053200362
  batch 274 loss: 0.27936123306081245
  batch 275 loss: 0.27948407297784633
  batch 276 loss: 0.27945300801724626
  batch 277 loss: 0.27948737989048666
  batch 278 loss: 0.27939876126085256
  batch 279 loss: 0.27944857571073756
  batch 280 loss: 0.27929621525108816
  batch 281 loss: 0.27918157697360285
  batch 282 loss: 0.2792526493575556
  batch 283 loss: 0.279270662909683
  batch 284 loss: 0.27930831389737804
  batch 285 loss: 0.27922792889569936
  batch 286 loss: 0.279173566380164
  batch 287 loss: 0.2791309471433586
  batch 288 loss: 0.2789306396928926
  batch 289 loss: 0.2790162613338253
  batch 290 loss: 0.2787713883765813
  batch 291 loss: 0.27873781195416075
  batch 292 loss: 0.27874696392514936
  batch 293 loss: 0.2787092459791756
  batch 294 loss: 0.27857460922935384
  batch 295 loss: 0.2784714676566043
  batch 296 loss: 0.2785520604937463
  batch 297 loss: 0.2784980059874178
  batch 298 loss: 0.2784623977721938
  batch 299 loss: 0.27836894570385734
  batch 300 loss: 0.27845998406410216
  batch 301 loss: 0.2785015672544308
  batch 302 loss: 0.2784478083154224
  batch 303 loss: 0.27848755969072725
  batch 304 loss: 0.278421616671901
  batch 305 loss: 0.2782526892228205
  batch 306 loss: 0.2783033700258124
  batch 307 loss: 0.2782123154362949
  batch 308 loss: 0.27825010224402724
  batch 309 loss: 0.2782358762033549
  batch 310 loss: 0.2782026136594434
  batch 311 loss: 0.27831385033115313
  batch 312 loss: 0.2784439530223608
  batch 313 loss: 0.27847685519689186
  batch 314 loss: 0.2783978380689955
  batch 315 loss: 0.278397438592381
  batch 316 loss: 0.27830889175021195
  batch 317 loss: 0.27827160652691635
  batch 318 loss: 0.27825539634100294
  batch 319 loss: 0.2782600341265478
  batch 320 loss: 0.27826126678846774
  batch 321 loss: 0.27821331338904726
  batch 322 loss: 0.2781371024917372
  batch 323 loss: 0.2780855093670334
  batch 324 loss: 0.2778498365279333
  batch 325 loss: 0.27774510961312515
  batch 326 loss: 0.2778157997716424
  batch 327 loss: 0.27794788691246547
  batch 328 loss: 0.2777866016982532
  batch 329 loss: 0.2778926018885928
  batch 330 loss: 0.2778590133695891
  batch 331 loss: 0.277883664088667
  batch 332 loss: 0.27785556933966027
  batch 333 loss: 0.2779224610722459
  batch 334 loss: 0.27784760885252924
  batch 335 loss: 0.2777181936733758
  batch 336 loss: 0.2776359257598718
  batch 337 loss: 0.2774397529197376
  batch 338 loss: 0.2774252249644353
  batch 339 loss: 0.2772959283934934
  batch 340 loss: 0.27724184915423394
  batch 341 loss: 0.27713793424392374
  batch 342 loss: 0.2770954286407309
  batch 343 loss: 0.277118804199355
  batch 344 loss: 0.2770989011799873
  batch 345 loss: 0.2772264661564343
  batch 346 loss: 0.2771552376061506
  batch 347 loss: 0.27718594623058607
  batch 348 loss: 0.2772542142748148
  batch 349 loss: 0.2771596907288434
  batch 350 loss: 0.2771893582599504
  batch 351 loss: 0.27716458722566945
  batch 352 loss: 0.2771709271367978
  batch 353 loss: 0.27714986685484055
  batch 354 loss: 0.27725822655326227
  batch 355 loss: 0.27720774475117804
  batch 356 loss: 0.2772502659328198
  batch 357 loss: 0.2771449803220792
  batch 358 loss: 0.2771670701367229
  batch 359 loss: 0.277173564336094
  batch 360 loss: 0.27708736177947785
  batch 361 loss: 0.2770282971231561
  batch 362 loss: 0.2769631200897101
  batch 363 loss: 0.2768816619947265
  batch 364 loss: 0.27680813410616184
  batch 365 loss: 0.2768466211753349
  batch 366 loss: 0.27671834130267625
  batch 367 loss: 0.2765944962686674
  batch 368 loss: 0.27650118080656166
  batch 369 loss: 0.2765119068102462
  batch 370 loss: 0.27648360330510785
  batch 371 loss: 0.27646873240682956
  batch 372 loss: 0.2763924500153911
  batch 373 loss: 0.2763295029027852
  batch 374 loss: 0.27622824218024544
  batch 375 loss: 0.2761506699323654
  batch 376 loss: 0.2761869445205369
  batch 377 loss: 0.27617436279826835
  batch 378 loss: 0.276093279046041
  batch 379 loss: 0.27619769135219757
  batch 380 loss: 0.27624364326659
  batch 381 loss: 0.2761523552297607
  batch 382 loss: 0.27605773531953703
  batch 383 loss: 0.27598954604904585
  batch 384 loss: 0.2759308044721062
  batch 385 loss: 0.27596321032418836
  batch 386 loss: 0.27584421144402704
  batch 387 loss: 0.27591815101853945
  batch 388 loss: 0.27597274865532656
  batch 389 loss: 0.27593983893682533
  batch 390 loss: 0.2759411375874128
  batch 391 loss: 0.27598155188895857
  batch 392 loss: 0.27603214458391373
  batch 393 loss: 0.2761446055186007
  batch 394 loss: 0.2761800778169317
  batch 395 loss: 0.2761659239289127
  batch 396 loss: 0.2762416210213695
  batch 397 loss: 0.27624078147507436
  batch 398 loss: 0.27624709136671755
  batch 399 loss: 0.27633848297117947
  batch 400 loss: 0.27632961127907035
  batch 401 loss: 0.2762619511890887
  batch 402 loss: 0.27628513629460216
  batch 403 loss: 0.2763012277872923
  batch 404 loss: 0.27635638024842385
  batch 405 loss: 0.27638972283881386
  batch 406 loss: 0.27650761736437607
  batch 407 loss: 0.2765362276638462
  batch 408 loss: 0.27666136899999544
  batch 409 loss: 0.2767150072713353
  batch 410 loss: 0.2767466236905354
  batch 411 loss: 0.2767068471061632
  batch 412 loss: 0.2766810189028388
  batch 413 loss: 0.2767173484076022
  batch 414 loss: 0.2766817670657439
  batch 415 loss: 0.2766754041235131
  batch 416 loss: 0.2767408527433872
  batch 417 loss: 0.27670581206429207
  batch 418 loss: 0.27668159415847376
  batch 419 loss: 0.2766721067963466
  batch 420 loss: 0.2767239462761652
  batch 421 loss: 0.27671243565382697
  batch 422 loss: 0.2768091177347147
  batch 423 loss: 0.2768452392261361
  batch 424 loss: 0.27681359353492846
  batch 425 loss: 0.27684471558122076
  batch 426 loss: 0.2767772828469254
  batch 427 loss: 0.276793472521199
  batch 428 loss: 0.27674436450840156
  batch 429 loss: 0.2767193638361417
  batch 430 loss: 0.27665989856387296
  batch 431 loss: 0.2767899259060552
  batch 432 loss: 0.27685764956253545
  batch 433 loss: 0.27687712767382966
  batch 434 loss: 0.276978572804807
  batch 435 loss: 0.2768783178480192
  batch 436 loss: 0.27689523847961645
  batch 437 loss: 0.27698488472119076
  batch 438 loss: 0.277080338669422
  batch 439 loss: 0.27710539526847067
  batch 440 loss: 0.2771619491617788
  batch 441 loss: 0.2771554955887416
  batch 442 loss: 0.2771460427755144
  batch 443 loss: 0.27714945275008546
  batch 444 loss: 0.2770564069820417
  batch 445 loss: 0.2771114166198152
  batch 446 loss: 0.27709237325752795
  batch 447 loss: 0.27703858251939684
  batch 448 loss: 0.2770262540517641
  batch 449 loss: 0.27709591299096303
  batch 450 loss: 0.27702199353112117
  batch 451 loss: 0.2769995771199795
  batch 452 loss: 0.2770121307895247
  batch 453 loss: 0.2770782676885196
  batch 454 loss: 0.2771228545956675
  batch 455 loss: 0.277136335006127
  batch 456 loss: 0.2771183017706662
  batch 457 loss: 0.2771817514228612
  batch 458 loss: 0.27715835983836495
  batch 459 loss: 0.2772209409275346
  batch 460 loss: 0.27732553190511205
  batch 461 loss: 0.2773454123360475
  batch 462 loss: 0.2773904592056811
  batch 463 loss: 0.27735460544765383
  batch 464 loss: 0.2774066822924491
  batch 465 loss: 0.277418049176534
  batch 466 loss: 0.2773409016398401
  batch 467 loss: 0.2774145261761463
  batch 468 loss: 0.27741981921797126
  batch 469 loss: 0.2775550081785808
  batch 470 loss: 0.27757905734346267
  batch 471 loss: 0.2775681458587606
  batch 472 loss: 0.27744322121774745
LOSS train 0.27744322121774745 valid 0.27595317363739014
LOSS train 0.27744322121774745 valid 0.2631769925355911
LOSS train 0.27744322121774745 valid 0.2663830816745758
LOSS train 0.27744322121774745 valid 0.25820109248161316
LOSS train 0.27744322121774745 valid 0.24915491342544555
LOSS train 0.27744322121774745 valid 0.2561873197555542
LOSS train 0.27744322121774745 valid 0.2629787836756025
LOSS train 0.27744322121774745 valid 0.261504914611578
LOSS train 0.27744322121774745 valid 0.2629814081721836
LOSS train 0.27744322121774745 valid 0.2657944470643997
LOSS train 0.27744322121774745 valid 0.2630265273831107
LOSS train 0.27744322121774745 valid 0.26525576661030453
LOSS train 0.27744322121774745 valid 0.2657010738666241
LOSS train 0.27744322121774745 valid 0.26575833771909985
LOSS train 0.27744322121774745 valid 0.26279138724009193
LOSS train 0.27744322121774745 valid 0.2630594912916422
LOSS train 0.27744322121774745 valid 0.26348863805041595
LOSS train 0.27744322121774745 valid 0.26475458674960667
LOSS train 0.27744322121774745 valid 0.267600312044746
LOSS train 0.27744322121774745 valid 0.2665171816945076
LOSS train 0.27744322121774745 valid 0.2658308886346363
LOSS train 0.27744322121774745 valid 0.2640113207426938
LOSS train 0.27744322121774745 valid 0.2645997897438381
LOSS train 0.27744322121774745 valid 0.26359391026198864
LOSS train 0.27744322121774745 valid 0.2625591391324997
LOSS train 0.27744322121774745 valid 0.26341388259942716
LOSS train 0.27744322121774745 valid 0.26274294433770357
LOSS train 0.27744322121774745 valid 0.2634042501449585
LOSS train 0.27744322121774745 valid 0.2638632069373953
LOSS train 0.27744322121774745 valid 0.2658077746629715
LOSS train 0.27744322121774745 valid 0.26681182845946283
LOSS train 0.27744322121774745 valid 0.2663455232977867
LOSS train 0.27744322121774745 valid 0.26707535620891687
LOSS train 0.27744322121774745 valid 0.26692145449273724
LOSS train 0.27744322121774745 valid 0.26844660980360846
LOSS train 0.27744322121774745 valid 0.26771991824110347
LOSS train 0.27744322121774745 valid 0.26776701413296367
LOSS train 0.27744322121774745 valid 0.26866652267543895
LOSS train 0.27744322121774745 valid 0.26797140752657866
LOSS train 0.27744322121774745 valid 0.26828806214034556
LOSS train 0.27744322121774745 valid 0.2693510379006223
LOSS train 0.27744322121774745 valid 0.26901189308790935
LOSS train 0.27744322121774745 valid 0.26900205882482753
LOSS train 0.27744322121774745 valid 0.2693492834540931
LOSS train 0.27744322121774745 valid 0.2686879035499361
LOSS train 0.27744322121774745 valid 0.2693501552161963
LOSS train 0.27744322121774745 valid 0.26965007217640574
LOSS train 0.27744322121774745 valid 0.2696220437064767
LOSS train 0.27744322121774745 valid 0.2702573778069749
LOSS train 0.27744322121774745 valid 0.26954569965600966
LOSS train 0.27744322121774745 valid 0.2698996689389734
LOSS train 0.27744322121774745 valid 0.2694513823550481
LOSS train 0.27744322121774745 valid 0.26975038057228307
LOSS train 0.27744322121774745 valid 0.2694384469478219
LOSS train 0.27744322121774745 valid 0.2696071378209374
LOSS train 0.27744322121774745 valid 0.26900473795831203
LOSS train 0.27744322121774745 valid 0.2686426602957541
LOSS train 0.27744322121774745 valid 0.26838230059064666
LOSS train 0.27744322121774745 valid 0.2689665020522425
LOSS train 0.27744322121774745 valid 0.268426988273859
LOSS train 0.27744322121774745 valid 0.2684773565804372
LOSS train 0.27744322121774745 valid 0.2695135491028909
LOSS train 0.27744322121774745 valid 0.26978717129381874
LOSS train 0.27744322121774745 valid 0.2706403515767306
LOSS train 0.27744322121774745 valid 0.2709032606620055
LOSS train 0.27744322121774745 valid 0.27098509150021005
LOSS train 0.27744322121774745 valid 0.27037025698974954
LOSS train 0.27744322121774745 valid 0.27038518339395523
LOSS train 0.27744322121774745 valid 0.2694603962742764
LOSS train 0.27744322121774745 valid 0.26967248214142664
LOSS train 0.27744322121774745 valid 0.2696663197916998
LOSS train 0.27744322121774745 valid 0.2699549440294504
LOSS train 0.27744322121774745 valid 0.26988406846784563
LOSS train 0.27744322121774745 valid 0.2698520516624322
LOSS train 0.27744322121774745 valid 0.26984896083672844
LOSS train 0.27744322121774745 valid 0.2704064981325677
LOSS train 0.27744322121774745 valid 0.27047540673187803
LOSS train 0.27744322121774745 valid 0.27055960759902614
LOSS train 0.27744322121774745 valid 0.2705255114956747
LOSS train 0.27744322121774745 valid 0.2697327909991145
LOSS train 0.27744322121774745 valid 0.26885391735000375
LOSS train 0.27744322121774745 valid 0.26899678332776555
LOSS train 0.27744322121774745 valid 0.26876588381198513
LOSS train 0.27744322121774745 valid 0.268659402039789
LOSS train 0.27744322121774745 valid 0.26820506558698765
LOSS train 0.27744322121774745 valid 0.2674516007997269
LOSS train 0.27744322121774745 valid 0.26764567127858085
LOSS train 0.27744322121774745 valid 0.2671199410476468
LOSS train 0.27744322121774745 valid 0.2673345459311196
LOSS train 0.27744322121774745 valid 0.2677048838800854
LOSS train 0.27744322121774745 valid 0.26787320172393714
LOSS train 0.27744322121774745 valid 0.2679276725520258
LOSS train 0.27744322121774745 valid 0.26758897641012747
LOSS train 0.27744322121774745 valid 0.2678091914412823
LOSS train 0.27744322121774745 valid 0.2672974925292166
LOSS train 0.27744322121774745 valid 0.26764218260844547
LOSS train 0.27744322121774745 valid 0.26773431743543175
LOSS train 0.27744322121774745 valid 0.2677682023875567
LOSS train 0.27744322121774745 valid 0.26789639574108703
LOSS train 0.27744322121774745 valid 0.2682661733031273
LOSS train 0.27744322121774745 valid 0.26875824739437293
LOSS train 0.27744322121774745 valid 0.26885565414148216
LOSS train 0.27744322121774745 valid 0.2689724848108384
LOSS train 0.27744322121774745 valid 0.2688201210246636
LOSS train 0.27744322121774745 valid 0.2688314210800897
LOSS train 0.27744322121774745 valid 0.26912876682461434
LOSS train 0.27744322121774745 valid 0.2688202128232082
LOSS train 0.27744322121774745 valid 0.2690212307152925
LOSS train 0.27744322121774745 valid 0.26946031801197506
LOSS train 0.27744322121774745 valid 0.26975413154472005
LOSS train 0.27744322121774745 valid 0.2693752169877559
LOSS train 0.27744322121774745 valid 0.26902888329433544
LOSS train 0.27744322121774745 valid 0.2690550591302129
LOSS train 0.27744322121774745 valid 0.26902172426905546
LOSS train 0.27744322121774745 valid 0.26898516533167466
LOSS train 0.27744322121774745 valid 0.2693491055276887
LOSS train 0.27744322121774745 valid 0.2696125944200744
LOSS train 0.27744322121774745 valid 0.2693611373335628
LOSS train 0.27744322121774745 valid 0.2692796365052712
LOSS train 0.27744322121774745 valid 0.26887027161816757
LOSS train 0.27744322121774745 valid 0.26873845982650096
LOSS train 0.27744322121774745 valid 0.26855223088479435
LOSS train 0.27744322121774745 valid 0.26861758505910394
LOSS train 0.27744322121774745 valid 0.2687258132763447
LOSS train 0.27744322121774745 valid 0.26868698084354403
LOSS train 0.27744322121774745 valid 0.26898141926716246
LOSS train 0.27744322121774745 valid 0.2688146323904278
LOSS train 0.27744322121774745 valid 0.26914575265254825
LOSS train 0.27744322121774745 valid 0.2691698690024457
LOSS train 0.27744322121774745 valid 0.2691311844266378
LOSS train 0.27744322121774745 valid 0.26911720576632114
LOSS train 0.27744322121774745 valid 0.2687911219669111
LOSS train 0.27744322121774745 valid 0.26848710434777395
LOSS train 0.27744322121774745 valid 0.26839968940215325
LOSS train 0.27744322121774745 valid 0.26835281804755884
LOSS train 0.27744322121774745 valid 0.268247504663818
LOSS train 0.27744322121774745 valid 0.268092521133214
LOSS train 0.27744322121774745 valid 0.26803457045900647
LOSS train 0.27744322121774745 valid 0.2678383481588295
LOSS train 0.27744322121774745 valid 0.2679493640150343
LOSS train 0.27744322121774745 valid 0.2679445170764382
LOSS train 0.27744322121774745 valid 0.2681557225509429
LOSS train 0.27744322121774745 valid 0.2681852168553359
LOSS train 0.27744322121774745 valid 0.26820298056635594
LOSS train 0.27744322121774745 valid 0.267937722288329
LOSS train 0.27744322121774745 valid 0.2681978567414088
LOSS train 0.27744322121774745 valid 0.26793748231566683
LOSS train 0.27744322121774745 valid 0.2687844624994574
LOSS train 0.27744322121774745 valid 0.26887125126867484
LOSS train 0.27744322121774745 valid 0.2688024182120959
LOSS train 0.27744322121774745 valid 0.268723628477545
LOSS train 0.27744322121774745 valid 0.26864361047352614
LOSS train 0.27744322121774745 valid 0.26867183039780534
LOSS train 0.27744322121774745 valid 0.2687330713326281
LOSS train 0.27744322121774745 valid 0.2688081451962071
LOSS train 0.27744322121774745 valid 0.269170182542159
LOSS train 0.27744322121774745 valid 0.2691308158408305
LOSS train 0.27744322121774745 valid 0.26916869918379605
LOSS train 0.27744322121774745 valid 0.2692256379614836
LOSS train 0.27744322121774745 valid 0.2689795522019267
LOSS train 0.27744322121774745 valid 0.2689403974861832
LOSS train 0.27744322121774745 valid 0.2689514366196997
LOSS train 0.27744322121774745 valid 0.2689639340880458
LOSS train 0.27744322121774745 valid 0.2687983687331037
LOSS train 0.27744322121774745 valid 0.2685630438002673
LOSS train 0.27744322121774745 valid 0.26868427850036736
LOSS train 0.27744322121774745 valid 0.2687653828702287
LOSS train 0.27744322121774745 valid 0.2686614893554222
LOSS train 0.27744322121774745 valid 0.2687132202485609
LOSS train 0.27744322121774745 valid 0.26893477886915207
LOSS train 0.27744322121774745 valid 0.2688441028197606
LOSS train 0.27744322121774745 valid 0.2686473715097405
LOSS train 0.27744322121774745 valid 0.2686819704626337
LOSS train 0.27744322121774745 valid 0.26865260388659334
LOSS train 0.27744322121774745 valid 0.26863405108451843
LOSS train 0.27744322121774745 valid 0.26855562203987077
LOSS train 0.27744322121774745 valid 0.2686086935175341
LOSS train 0.27744322121774745 valid 0.26889012569791815
LOSS train 0.27744322121774745 valid 0.2688174009656107
LOSS train 0.27744322121774745 valid 0.26890785810020235
LOSS train 0.27744322121774745 valid 0.2691756442436197
LOSS train 0.27744322121774745 valid 0.26924795616459063
LOSS train 0.27744322121774745 valid 0.2691103042474861
LOSS train 0.27744322121774745 valid 0.2690637547360814
LOSS train 0.27744322121774745 valid 0.2688575490100964
LOSS train 0.27744322121774745 valid 0.2687686401349242
LOSS train 0.27744322121774745 valid 0.2686334024457371
LOSS train 0.27744322121774745 valid 0.2687425153686645
LOSS train 0.27744322121774745 valid 0.26886630957088775
LOSS train 0.27744322121774745 valid 0.26894558715192896
LOSS train 0.27744322121774745 valid 0.2688149386987636
LOSS train 0.27744322121774745 valid 0.26870399263376993
LOSS train 0.27744322121774745 valid 0.26880835174278894
LOSS train 0.27744322121774745 valid 0.2686464180497779
LOSS train 0.27744322121774745 valid 0.26831941062059156
LOSS train 0.27744322121774745 valid 0.2684378690105312
LOSS train 0.27744322121774745 valid 0.26851572958648506
LOSS train 0.27744322121774745 valid 0.2685000639822748
LOSS train 0.27744322121774745 valid 0.2685054833415765
LOSS train 0.27744322121774745 valid 0.2682612757384777
LOSS train 0.27744322121774745 valid 0.26810262704370036
LOSS train 0.27744322121774745 valid 0.26796638980360316
LOSS train 0.27744322121774745 valid 0.26783453478601765
LOSS train 0.27744322121774745 valid 0.2677701827065617
LOSS train 0.27744322121774745 valid 0.2675779899445976
LOSS train 0.27744322121774745 valid 0.26754619730907736
LOSS train 0.27744322121774745 valid 0.2675154982269674
LOSS train 0.27744322121774745 valid 0.26740083134231657
LOSS train 0.27744322121774745 valid 0.2672914426292529
LOSS train 0.27744322121774745 valid 0.26739292244116464
LOSS train 0.27744322121774745 valid 0.26737665918201065
LOSS train 0.27744322121774745 valid 0.26717083352916643
LOSS train 0.27744322121774745 valid 0.2671961596874004
LOSS train 0.27744322121774745 valid 0.26708856782066487
LOSS train 0.27744322121774745 valid 0.2669380417396856
LOSS train 0.27744322121774745 valid 0.26684400384072904
LOSS train 0.27744322121774745 valid 0.2668566772465332
LOSS train 0.27744322121774745 valid 0.26682905770769905
LOSS train 0.27744322121774745 valid 0.2669035705257224
LOSS train 0.27744322121774745 valid 0.2671021286736835
LOSS train 0.27744322121774745 valid 0.2672187402777003
LOSS train 0.27744322121774745 valid 0.2671442345992939
LOSS train 0.27744322121774745 valid 0.2672896615058317
LOSS train 0.27744322121774745 valid 0.2674621883779764
LOSS train 0.27744322121774745 valid 0.26748329480489097
LOSS train 0.27744322121774745 valid 0.26764439402428347
LOSS train 0.27744322121774745 valid 0.26792624369591866
LOSS train 0.27744322121774745 valid 0.2680306335290273
LOSS train 0.27744322121774745 valid 0.26804221821664204
LOSS train 0.27744322121774745 valid 0.26819001993407376
LOSS train 0.27744322121774745 valid 0.2681812796241793
LOSS train 0.27744322121774745 valid 0.26822071008641146
LOSS train 0.27744322121774745 valid 0.2681147617331902
LOSS train 0.27744322121774745 valid 0.2681836744404247
LOSS train 0.27744322121774745 valid 0.26827293051050066
LOSS train 0.27744322121774745 valid 0.2682530999941341
LOSS train 0.27744322121774745 valid 0.2682989618194757
LOSS train 0.27744322121774745 valid 0.2683288561446326
LOSS train 0.27744322121774745 valid 0.26820844062701427
LOSS train 0.27744322121774745 valid 0.2681470828751723
LOSS train 0.27744322121774745 valid 0.26830344365840136
LOSS train 0.27744322121774745 valid 0.2680701519832138
LOSS train 0.27744322121774745 valid 0.2681703096554603
LOSS train 0.27744322121774745 valid 0.26835427775246196
LOSS train 0.27744322121774745 valid 0.26848952247171987
LOSS train 0.27744322121774745 valid 0.2684511243085551
LOSS train 0.27744322121774745 valid 0.26843823546822737
LOSS train 0.27744322121774745 valid 0.26842408987783617
LOSS train 0.27744322121774745 valid 0.2683339785021472
LOSS train 0.27744322121774745 valid 0.2684895721077919
LOSS train 0.27744322121774745 valid 0.26841216762939774
LOSS train 0.27744322121774745 valid 0.2684926607660831
LOSS train 0.27744322121774745 valid 0.2683644947327173
LOSS train 0.27744322121774745 valid 0.2683341469117037
LOSS train 0.27744322121774745 valid 0.2683840731779734
LOSS train 0.27744322121774745 valid 0.2684740989934653
LOSS train 0.27744322121774745 valid 0.2683255798273977
LOSS train 0.27744322121774745 valid 0.26833801011937536
LOSS train 0.27744322121774745 valid 0.2683047153429635
LOSS train 0.27744322121774745 valid 0.2683031435769338
LOSS train 0.27744322121774745 valid 0.2683472631649039
LOSS train 0.27744322121774745 valid 0.2683409061709433
LOSS train 0.27744322121774745 valid 0.2683098641978471
LOSS train 0.27744322121774745 valid 0.268355315212499
LOSS train 0.27744322121774745 valid 0.2682540494878337
LOSS train 0.27744322121774745 valid 0.2682203174869817
LOSS train 0.27744322121774745 valid 0.26837901012058113
LOSS train 0.27744322121774745 valid 0.26853633132665905
LOSS train 0.27744322121774745 valid 0.26860998770339783
LOSS train 0.27744322121774745 valid 0.2685880523588922
LOSS train 0.27744322121774745 valid 0.26861052116124834
LOSS train 0.27744322121774745 valid 0.26888264841674003
LOSS train 0.27744322121774745 valid 0.2689063998259904
LOSS train 0.27744322121774745 valid 0.26891102326394867
LOSS train 0.27744322121774745 valid 0.26887314368378035
LOSS train 0.27744322121774745 valid 0.26883417772858037
LOSS train 0.27744322121774745 valid 0.26863766625684954
LOSS train 0.27744322121774745 valid 0.26847177706390835
LOSS train 0.27744322121774745 valid 0.26853341424977906
LOSS train 0.27744322121774745 valid 0.2685346855116742
LOSS train 0.27744322121774745 valid 0.26845906658953195
LOSS train 0.27744322121774745 valid 0.2682635938973292
LOSS train 0.27744322121774745 valid 0.26828555804891213
LOSS train 0.27744322121774745 valid 0.2684011725367795
LOSS train 0.27744322121774745 valid 0.2684323555021955
LOSS train 0.27744322121774745 valid 0.2682901084735677
LOSS train 0.27744322121774745 valid 0.2681840573557578
LOSS train 0.27744322121774745 valid 0.2682090369053185
LOSS train 0.27744322121774745 valid 0.268267425234755
LOSS train 0.27744322121774745 valid 0.26827115956051595
LOSS train 0.27744322121774745 valid 0.26825531779490797
LOSS train 0.27744322121774745 valid 0.2682233555369998
LOSS train 0.27744322121774745 valid 0.26829075391178653
LOSS train 0.27744322121774745 valid 0.268381670066694
LOSS train 0.27744322121774745 valid 0.2684384676864592
LOSS train 0.27744322121774745 valid 0.2684139160791764
LOSS train 0.27744322121774745 valid 0.26842074838752294
LOSS train 0.27744322121774745 valid 0.26842316260073806
LOSS train 0.27744322121774745 valid 0.2684786713940643
LOSS train 0.27744322121774745 valid 0.26839870274066924
LOSS train 0.27744322121774745 valid 0.26844267542180034
LOSS train 0.27744322121774745 valid 0.2684624475359127
LOSS train 0.27744322121774745 valid 0.26853058440457084
LOSS train 0.27744322121774745 valid 0.26853064330000626
LOSS train 0.27744322121774745 valid 0.2684330689125374
LOSS train 0.27744322121774745 valid 0.26837100366166994
LOSS train 0.27744322121774745 valid 0.2683038608161945
LOSS train 0.27744322121774745 valid 0.2681880105141695
LOSS train 0.27744322121774745 valid 0.2683593525180539
LOSS train 0.27744322121774745 valid 0.26831414492860917
LOSS train 0.27744322121774745 valid 0.2681710150368344
LOSS train 0.27744322121774745 valid 0.26816193277064043
LOSS train 0.27744322121774745 valid 0.2681964007905497
LOSS train 0.27744322121774745 valid 0.2682614273800971
LOSS train 0.27744322121774745 valid 0.26827817677505433
LOSS train 0.27744322121774745 valid 0.268180186235452
LOSS train 0.27744322121774745 valid 0.2682101632518347
LOSS train 0.27744322121774745 valid 0.2682713064007789
LOSS train 0.27744322121774745 valid 0.26833307668333145
LOSS train 0.27744322121774745 valid 0.268273853790015
LOSS train 0.27744322121774745 valid 0.26835588252061626
LOSS train 0.27744322121774745 valid 0.26830766289870933
LOSS train 0.27744322121774745 valid 0.268284294904201
LOSS train 0.27744322121774745 valid 0.26824467150885384
LOSS train 0.27744322121774745 valid 0.26818270655778736
LOSS train 0.27744322121774745 valid 0.26834689976613213
LOSS train 0.27744322121774745 valid 0.268436458497237
LOSS train 0.27744322121774745 valid 0.2684161635797198
LOSS train 0.27744322121774745 valid 0.2685592898300716
LOSS train 0.27744322121774745 valid 0.2685481907743396
LOSS train 0.27744322121774745 valid 0.2685044904850041
LOSS train 0.27744322121774745 valid 0.26836413338062276
LOSS train 0.27744322121774745 valid 0.2683414754656342
LOSS train 0.27744322121774745 valid 0.26850994765580055
LOSS train 0.27744322121774745 valid 0.268508901747305
LOSS train 0.27744322121774745 valid 0.2685702487915045
LOSS train 0.27744322121774745 valid 0.2684742106174503
LOSS train 0.27744322121774745 valid 0.268470738061081
LOSS train 0.27744322121774745 valid 0.2685425996252921
LOSS train 0.27744322121774745 valid 0.26859324609532076
LOSS train 0.27744322121774745 valid 0.26848599655537314
LOSS train 0.27744322121774745 valid 0.2684461835183595
LOSS train 0.27744322121774745 valid 0.26846798585385695
LOSS train 0.27744322121774745 valid 0.26862640505613283
LOSS train 0.27744322121774745 valid 0.2686412078746851
LOSS train 0.27744322121774745 valid 0.26853555729451206
LOSS train 0.27744322121774745 valid 0.26843838209034043
LOSS train 0.27744322121774745 valid 0.2683711473712291
LOSS train 0.27744322121774745 valid 0.26845132420773493
LOSS train 0.27744322121774745 valid 0.26830133736133577
LOSS train 0.27744322121774745 valid 0.26819544079636576
LOSS train 0.27744322121774745 valid 0.26811866872859275
LOSS train 0.27744322121774745 valid 0.26819234415255594
LOSS train 0.27744322121774745 valid 0.2682708228627841
LOSS train 0.27744322121774745 valid 0.26840214565606185
LOSS train 0.27744322121774745 valid 0.26846957755055323
LOSS train 0.27744322121774745 valid 0.2684227329592745
LOSS train 0.27744322121774745 valid 0.26838327790272304
LOSS train 0.27744322121774745 valid 0.2683522238455775
LOSS train 0.27744322121774745 valid 0.26825081039634013
LOSS train 0.27744322121774745 valid 0.2681580698143412
LOSS train 0.27744322121774745 valid 0.2682519020378919
LOSS train 0.27744322121774745 valid 0.2680761959845041
LOSS train 0.27744322121774745 valid 0.2680515247796263
LOSS train 0.27744322121774745 valid 0.2680226981231611
LOSS train 0.27744322121774745 valid 0.26793894163572074
LOSS train 0.27744322121774745 valid 0.26782672659415313
LOSS train 0.27744322121774745 valid 0.26780232872166065
LOSS train 0.27744322121774745 valid 0.2678320846018106
EPOCH 8:
  batch 1 loss: 0.27959752082824707
  batch 2 loss: 0.2643379345536232
  batch 3 loss: 0.2622427095969518
  batch 4 loss: 0.2613874040544033
  batch 5 loss: 0.2688163787126541
  batch 6 loss: 0.2635826865832011
  batch 7 loss: 0.26583642193249296
  batch 8 loss: 0.2696309871971607
  batch 9 loss: 0.2684389584594303
  batch 10 loss: 0.27178684175014495
  batch 11 loss: 0.2724602357907729
  batch 12 loss: 0.26815132548411685
  batch 13 loss: 0.26655903458595276
  batch 14 loss: 0.26721083905015675
  batch 15 loss: 0.2689828137556712
  batch 16 loss: 0.2687627486884594
  batch 17 loss: 0.2668015825397828
  batch 18 loss: 0.26849470618698335
  batch 19 loss: 0.2678356413778506
  batch 20 loss: 0.2667876504361629
  batch 21 loss: 0.26720860529513585
  batch 22 loss: 0.26682043956084683
  batch 23 loss: 0.265727751928827
  batch 24 loss: 0.26476676265398663
  batch 25 loss: 0.2666542613506317
  batch 26 loss: 0.26508539800460523
  batch 27 loss: 0.2652769143934603
  batch 28 loss: 0.26401882831539425
  batch 29 loss: 0.26421681971385563
  batch 30 loss: 0.2637408624092738
  batch 31 loss: 0.26519266347731313
  batch 32 loss: 0.2643793891184032
  batch 33 loss: 0.2641445393815185
  batch 34 loss: 0.2643964426482425
  batch 35 loss: 0.2648659020662308
  batch 36 loss: 0.26542504835459924
  batch 37 loss: 0.26659480744117015
  batch 38 loss: 0.2671422758384755
  batch 39 loss: 0.26691498015171444
  batch 40 loss: 0.26701109670102596
  batch 41 loss: 0.2669367052432967
  batch 42 loss: 0.2673533988140878
  batch 43 loss: 0.26749391437963
  batch 44 loss: 0.2679056223820556
  batch 45 loss: 0.26814341644446055
  batch 46 loss: 0.2676639343085496
  batch 47 loss: 0.26840733150218393
  batch 48 loss: 0.2678879763310154
  batch 49 loss: 0.2679278604230102
  batch 50 loss: 0.26790650576353076
  batch 51 loss: 0.2680131141461578
  batch 52 loss: 0.2683190975624781
  batch 53 loss: 0.26769708574942824
  batch 54 loss: 0.2682037121719784
  batch 55 loss: 0.2677436498078433
  batch 56 loss: 0.26760474113481386
  batch 57 loss: 0.2669083766247097
  batch 58 loss: 0.26721739743290274
  batch 59 loss: 0.2680022082591461
  batch 60 loss: 0.2678764867285887
  batch 61 loss: 0.2679975611753151
  batch 62 loss: 0.26787196940952734
  batch 63 loss: 0.26735968816848027
  batch 64 loss: 0.26810828410089016
  batch 65 loss: 0.26752991607555976
  batch 66 loss: 0.2673444312178727
  batch 67 loss: 0.2676282509494184
  batch 68 loss: 0.2675936412723625
  batch 69 loss: 0.26792919570985047
  batch 70 loss: 0.26843683400324414
  batch 71 loss: 0.2681979027012704
  batch 72 loss: 0.2681581438001659
  batch 73 loss: 0.2682375375130405
  batch 74 loss: 0.26842394692672267
  batch 75 loss: 0.26795899669329326
  batch 76 loss: 0.2680797322016013
  batch 77 loss: 0.26788945708956036
  batch 78 loss: 0.26771093102601856
  batch 79 loss: 0.2681939654712436
  batch 80 loss: 0.2679928757250309
  batch 81 loss: 0.268016846091659
  batch 82 loss: 0.26842019543415163
  batch 83 loss: 0.26836337023470774
  batch 84 loss: 0.2684370368009522
  batch 85 loss: 0.26847997798639184
  batch 86 loss: 0.26884035558201547
  batch 87 loss: 0.26870138480745515
  batch 88 loss: 0.26834243654527445
  batch 89 loss: 0.26809086692467166
  batch 90 loss: 0.26796900033950805
  batch 91 loss: 0.2680649966984005
  batch 92 loss: 0.2679858201223871
  batch 93 loss: 0.26780265954232985
  batch 94 loss: 0.2677343658944394
  batch 95 loss: 0.26747946080408597
  batch 96 loss: 0.26771253409485024
  batch 97 loss: 0.26830670606229723
  batch 98 loss: 0.26859085748390277
  batch 99 loss: 0.268835268237374
  batch 100 loss: 0.269003281891346
  batch 101 loss: 0.2690805999359282
  batch 102 loss: 0.2695860164422615
  batch 103 loss: 0.2699106549753726
  batch 104 loss: 0.27004061753933245
  batch 105 loss: 0.2698948513893854
  batch 106 loss: 0.2702383202201915
  batch 107 loss: 0.26976974865543507
  batch 108 loss: 0.2696404080424044
  batch 109 loss: 0.2696382639878387
  batch 110 loss: 0.26954571320251985
  batch 111 loss: 0.2696210767503257
  batch 112 loss: 0.2692553274599569
  batch 113 loss: 0.2697597231748885
  batch 114 loss: 0.2702912485651803
  batch 115 loss: 0.2704224186099094
  batch 116 loss: 0.27059895095640213
  batch 117 loss: 0.2710221100312013
  batch 118 loss: 0.2710247624476077
  batch 119 loss: 0.2711260913550353
  batch 120 loss: 0.2711927626281977
  batch 121 loss: 0.27102652229060814
  batch 122 loss: 0.27099814632388414
  batch 123 loss: 0.2706884986501399
  batch 124 loss: 0.2710195948039332
  batch 125 loss: 0.2710466334819794
  batch 126 loss: 0.27106653272159514
  batch 127 loss: 0.2715343931528527
  batch 128 loss: 0.27170777413994074
  batch 129 loss: 0.2718481802663138
  batch 130 loss: 0.27222448610342465
  batch 131 loss: 0.27255330786450216
  batch 132 loss: 0.27277234332128003
  batch 133 loss: 0.27307752334981933
  batch 134 loss: 0.27334840662443816
  batch 135 loss: 0.2734509203169081
  batch 136 loss: 0.27360892602626014
  batch 137 loss: 0.273528681619324
  batch 138 loss: 0.27367840642514435
  batch 139 loss: 0.2741025748441545
  batch 140 loss: 0.2742127069405147
  batch 141 loss: 0.2744412026929517
  batch 142 loss: 0.27439220312615514
  batch 143 loss: 0.27464581494564777
  batch 144 loss: 0.2747956802033716
  batch 145 loss: 0.2746591066492015
  batch 146 loss: 0.2746277488257787
  batch 147 loss: 0.2750328391993127
  batch 148 loss: 0.2751921433452013
  batch 149 loss: 0.2751761792890178
  batch 150 loss: 0.27508431891600293
  batch 151 loss: 0.2750086286999532
  batch 152 loss: 0.27520773755876643
  batch 153 loss: 0.27509043380325915
  batch 154 loss: 0.27548104911655574
  batch 155 loss: 0.27539134813893223
  batch 156 loss: 0.27546927428398377
  batch 157 loss: 0.27580599496319036
  batch 158 loss: 0.2759978348695779
  batch 159 loss: 0.27615530254705895
  batch 160 loss: 0.27602284457534554
  batch 161 loss: 0.276191325476451
  batch 162 loss: 0.2762379044735873
  batch 163 loss: 0.27619014982065543
  batch 164 loss: 0.27618902976193077
  batch 165 loss: 0.27606496413548787
  batch 166 loss: 0.27602650853524724
  batch 167 loss: 0.2758349614764402
  batch 168 loss: 0.2758343596720979
  batch 169 loss: 0.2755831983491514
  batch 170 loss: 0.27534320748904173
  batch 171 loss: 0.27535564692048303
  batch 172 loss: 0.2752962144481581
  batch 173 loss: 0.2753677761795893
  batch 174 loss: 0.2754521754452552
  batch 175 loss: 0.27553268526281627
  batch 176 loss: 0.2754511847584085
  batch 177 loss: 0.2755398815129436
  batch 178 loss: 0.27571576641181883
  batch 179 loss: 0.2758380112201808
  batch 180 loss: 0.2757971885303656
  batch 181 loss: 0.27579668781704664
  batch 182 loss: 0.27575371840170454
  batch 183 loss: 0.27553657658113156
  batch 184 loss: 0.2753906183592651
  batch 185 loss: 0.2756035200647406
  batch 186 loss: 0.2755622996758389
  batch 187 loss: 0.2756781608344399
  batch 188 loss: 0.275479395576614
  batch 189 loss: 0.2753935468102258
  batch 190 loss: 0.2754990067136915
  batch 191 loss: 0.2755515677610617
  batch 192 loss: 0.2757627700921148
  batch 193 loss: 0.2759246704955175
  batch 194 loss: 0.27613705158540885
  batch 195 loss: 0.2762436668842267
  batch 196 loss: 0.27611311182987935
  batch 197 loss: 0.27612956382598974
  batch 198 loss: 0.2762103933427069
  batch 199 loss: 0.27632230902137467
  batch 200 loss: 0.27654539309442044
  batch 201 loss: 0.27666473485047544
  batch 202 loss: 0.27662003077197783
  batch 203 loss: 0.27668443520374486
  batch 204 loss: 0.27659134793223117
  batch 205 loss: 0.2767553686368756
  batch 206 loss: 0.27681267514680197
  batch 207 loss: 0.2768466795844156
  batch 208 loss: 0.27689761833216137
  batch 209 loss: 0.27676361842018565
  batch 210 loss: 0.2768500355027971
  batch 211 loss: 0.2767620497687733
  batch 212 loss: 0.2768238884098125
  batch 213 loss: 0.27675615286043553
  batch 214 loss: 0.27677795597325977
  batch 215 loss: 0.276575001727703
  batch 216 loss: 0.2765102102248757
  batch 217 loss: 0.2764977323844136
  batch 218 loss: 0.27653207130935215
  batch 219 loss: 0.2764864034576503
  batch 220 loss: 0.2765314477411183
  batch 221 loss: 0.2765957532425272
  batch 222 loss: 0.2766791960677585
  batch 223 loss: 0.27672885832765176
  batch 224 loss: 0.27671103538679226
  batch 225 loss: 0.27666023347112867
  batch 226 loss: 0.2766796789601841
  batch 227 loss: 0.2764806905638279
  batch 228 loss: 0.2764034875782958
  batch 229 loss: 0.2762488455361154
  batch 230 loss: 0.27630358500325164
  batch 231 loss: 0.2762715004352264
  batch 232 loss: 0.276139328513166
  batch 233 loss: 0.27616715872492403
  batch 234 loss: 0.2762923875552976
  batch 235 loss: 0.2763433006849695
  batch 236 loss: 0.2762810758993787
  batch 237 loss: 0.2763354713529474
  batch 238 loss: 0.2763223304342823
  batch 239 loss: 0.2762516584231764
  batch 240 loss: 0.27635286320000885
  batch 241 loss: 0.2763519713121826
  batch 242 loss: 0.2762136327579987
  batch 243 loss: 0.2761576230879183
  batch 244 loss: 0.276004858681413
  batch 245 loss: 0.2759305315358298
  batch 246 loss: 0.27583332117495496
  batch 247 loss: 0.27585123509530596
  batch 248 loss: 0.2758080090486234
  batch 249 loss: 0.2757718844346732
  batch 250 loss: 0.2756903052330017
  batch 251 loss: 0.2756043541953858
  batch 252 loss: 0.27549073147395303
  batch 253 loss: 0.2753481784239117
  batch 254 loss: 0.2752929379738222
  batch 255 loss: 0.27523649270628014
  batch 256 loss: 0.27522029721876606
  batch 257 loss: 0.27520030661547695
  batch 258 loss: 0.2752055309193079
  batch 259 loss: 0.275153085474342
  batch 260 loss: 0.27513431878044053
  batch 261 loss: 0.27507655756455274
  batch 262 loss: 0.2749310383246145
  batch 263 loss: 0.2749277324390955
  batch 264 loss: 0.27473082082966965
  batch 265 loss: 0.27454942745982475
  batch 266 loss: 0.2744906912172647
  batch 267 loss: 0.2745120149203454
  batch 268 loss: 0.27446508107345496
  batch 269 loss: 0.27437356502150073
  batch 270 loss: 0.2743647753088563
  batch 271 loss: 0.27428973073009194
  batch 272 loss: 0.27423149040516687
  batch 273 loss: 0.27419450344183505
  batch 274 loss: 0.2743118238927674
  batch 275 loss: 0.2744182956218719
  batch 276 loss: 0.2743992133848909
  batch 277 loss: 0.2744115792241768
  batch 278 loss: 0.2743074297904968
  batch 279 loss: 0.2742971780052322
  batch 280 loss: 0.27415874217237746
  batch 281 loss: 0.27402653420522965
  batch 282 loss: 0.27412402069737724
  batch 283 loss: 0.2741095172010968
  batch 284 loss: 0.2741005242290631
  batch 285 loss: 0.27403401471020883
  batch 286 loss: 0.27397791051364445
  batch 287 loss: 0.2739769374036623
  batch 288 loss: 0.2737704237095184
  batch 289 loss: 0.2738615739510546
  batch 290 loss: 0.27362648685430657
  batch 291 loss: 0.27359243792152077
  batch 292 loss: 0.27361103101340056
  batch 293 loss: 0.273547291196247
  batch 294 loss: 0.27342614414943317
  batch 295 loss: 0.2733101362394074
  batch 296 loss: 0.27338068635278456
  batch 297 loss: 0.2733066846124251
  batch 298 loss: 0.2732945030067591
  batch 299 loss: 0.27328944550110745
  batch 300 loss: 0.2733326872686545
  batch 301 loss: 0.27340226231817394
  batch 302 loss: 0.27335969506707414
  batch 303 loss: 0.27341950731505654
  batch 304 loss: 0.27342850131619917
  batch 305 loss: 0.27328299811629
  batch 306 loss: 0.2732888948099286
  batch 307 loss: 0.2732197301589705
  batch 308 loss: 0.2732642358773715
  batch 309 loss: 0.27324865242424135
  batch 310 loss: 0.27320503611718455
  batch 311 loss: 0.2732920299772281
  batch 312 loss: 0.273390338015862
  batch 313 loss: 0.27348122743371955
  batch 314 loss: 0.27341435982543194
  batch 315 loss: 0.27337412086744156
  batch 316 loss: 0.2733369642232038
  batch 317 loss: 0.27330187508360443
  batch 318 loss: 0.2733199278701027
  batch 319 loss: 0.27332930336925304
  batch 320 loss: 0.2732967400923371
  batch 321 loss: 0.2733461762143073
  batch 322 loss: 0.273255198499801
  batch 323 loss: 0.27318061053199294
  batch 324 loss: 0.27294718717903266
  batch 325 loss: 0.27283236659490145
  batch 326 loss: 0.2729061255974272
  batch 327 loss: 0.27306947558662575
  batch 328 loss: 0.2729317925688697
  batch 329 loss: 0.2730213012376455
  batch 330 loss: 0.2729746812220776
  batch 331 loss: 0.27298447003537435
  batch 332 loss: 0.2729693641325077
  batch 333 loss: 0.27299989603303215
  batch 334 loss: 0.2729918913034622
  batch 335 loss: 0.27287176260307655
  batch 336 loss: 0.27279887579026674
  batch 337 loss: 0.2726231874450378
  batch 338 loss: 0.2725720946958079
  batch 339 loss: 0.2724424299940599
  batch 340 loss: 0.27236888807486087
  batch 341 loss: 0.27230825290477173
  batch 342 loss: 0.2722814873906604
  batch 343 loss: 0.27237727825391395
  batch 344 loss: 0.27240431529664716
  batch 345 loss: 0.27248452914797744
  batch 346 loss: 0.2724112713698707
  batch 347 loss: 0.2724184116901513
  batch 348 loss: 0.2724718174458235
  batch 349 loss: 0.27236422887845846
  batch 350 loss: 0.27241163679531644
  batch 351 loss: 0.2724178503041933
  batch 352 loss: 0.2724363576112823
  batch 353 loss: 0.27242420550446333
  batch 354 loss: 0.272496672673414
  batch 355 loss: 0.272453313142481
  batch 356 loss: 0.27248973432886464
  batch 357 loss: 0.2723680934986147
  batch 358 loss: 0.2724079165045776
  batch 359 loss: 0.27246257920119094
  batch 360 loss: 0.2723870153642363
  batch 361 loss: 0.27235945526747823
  batch 362 loss: 0.2722793921251982
  batch 363 loss: 0.27217901861372074
  batch 364 loss: 0.2720895563098755
  batch 365 loss: 0.27214315390750154
  batch 366 loss: 0.27202936799311245
  batch 367 loss: 0.2719183974678575
  batch 368 loss: 0.2717980112799484
  batch 369 loss: 0.2718020923496262
  batch 370 loss: 0.27174207386938304
  batch 371 loss: 0.27173854804263925
  batch 372 loss: 0.271676677489473
  batch 373 loss: 0.2715798664748189
  batch 374 loss: 0.2715262940781002
  batch 375 loss: 0.27146363655726113
  batch 376 loss: 0.27146013517011985
  batch 377 loss: 0.271454027698274
  batch 378 loss: 0.27134842240305806
  batch 379 loss: 0.27143821135988966
  batch 380 loss: 0.2714745749768458
  batch 381 loss: 0.27141091415262597
  batch 382 loss: 0.2712844420634015
  batch 383 loss: 0.27123131390962524
  batch 384 loss: 0.27121095545589924
  batch 385 loss: 0.27127382221159996
  batch 386 loss: 0.27115621560595815
  batch 387 loss: 0.27121637403502946
  batch 388 loss: 0.27130103756472007
  batch 389 loss: 0.27124959973565715
  batch 390 loss: 0.27117570226009075
  batch 391 loss: 0.27123921042513055
  batch 392 loss: 0.2712719093019865
  batch 393 loss: 0.2713716977273538
  batch 394 loss: 0.27140681218677365
  batch 395 loss: 0.27137171075313904
  batch 396 loss: 0.2714048985278968
  batch 397 loss: 0.271414510023684
  batch 398 loss: 0.2713895836217919
  batch 399 loss: 0.2714820477299224
  batch 400 loss: 0.2714523430913687
  batch 401 loss: 0.2713216828958055
  batch 402 loss: 0.2713602551106197
  batch 403 loss: 0.27137157701824793
  batch 404 loss: 0.2713886192898349
  batch 405 loss: 0.27141826973285205
  batch 406 loss: 0.271534877641154
  batch 407 loss: 0.27148019791528105
  batch 408 loss: 0.2715921587511605
  batch 409 loss: 0.27163361614082726
  batch 410 loss: 0.27168251523157444
  batch 411 loss: 0.2716684889619368
  batch 412 loss: 0.2716500101592934
  batch 413 loss: 0.2717342264860075
  batch 414 loss: 0.2717197049355161
  batch 415 loss: 0.2717389275510627
  batch 416 loss: 0.27176731258917314
  batch 417 loss: 0.27173592268134195
  batch 418 loss: 0.2716908419959283
  batch 419 loss: 0.2716555212022012
  batch 420 loss: 0.27168101384526205
  batch 421 loss: 0.2716947141282632
  batch 422 loss: 0.27182825170987024
  batch 423 loss: 0.27185769262888754
  batch 424 loss: 0.271855660415483
  batch 425 loss: 0.27188976091497086
  batch 426 loss: 0.27182489486647327
  batch 427 loss: 0.27187919309602687
  batch 428 loss: 0.2718137034049658
  batch 429 loss: 0.27180886435342
  batch 430 loss: 0.27173762238302895
  batch 431 loss: 0.2718661807114453
  batch 432 loss: 0.2719212414489852
  batch 433 loss: 0.2718631079028991
  batch 434 loss: 0.27193616869674847
  batch 435 loss: 0.27187793731004345
  batch 436 loss: 0.2718922641777664
  batch 437 loss: 0.2719085656055324
  batch 438 loss: 0.2719971199762331
  batch 439 loss: 0.27202719826768906
  batch 440 loss: 0.2720288041979074
  batch 441 loss: 0.2720003591305545
  batch 442 loss: 0.27195726432816475
  batch 443 loss: 0.2719714042497273
  batch 444 loss: 0.27188409915244255
  batch 445 loss: 0.27195432859190394
  batch 446 loss: 0.2719300972974354
  batch 447 loss: 0.271863442872728
  batch 448 loss: 0.2718066612058984
  batch 449 loss: 0.2718796564436171
  batch 450 loss: 0.27179710464345086
  batch 451 loss: 0.27177066374115827
  batch 452 loss: 0.271793628853243
  batch 453 loss: 0.27184734179352554
  batch 454 loss: 0.27190108337985264
  batch 455 loss: 0.2718867114284536
  batch 456 loss: 0.27187278334116727
  batch 457 loss: 0.27193188298844145
  batch 458 loss: 0.2719300575066342
  batch 459 loss: 0.2719749037931168
  batch 460 loss: 0.2720669370306575
  batch 461 loss: 0.2720866594746418
  batch 462 loss: 0.27210853184327416
  batch 463 loss: 0.27206901697437913
  batch 464 loss: 0.2720991480003657
  batch 465 loss: 0.2720902177915778
  batch 466 loss: 0.27198842414075214
  batch 467 loss: 0.2720302057789531
  batch 468 loss: 0.27204379245129406
  batch 469 loss: 0.27221088524439185
  batch 470 loss: 0.27222647879351963
  batch 471 loss: 0.27222548317630835
  batch 472 loss: 0.27212503830255086
LOSS train 0.27212503830255086 valid 0.254153847694397
LOSS train 0.27212503830255086 valid 0.2464931607246399
LOSS train 0.27212503830255086 valid 0.248872309923172
LOSS train 0.27212503830255086 valid 0.24209655821323395
LOSS train 0.27212503830255086 valid 0.2350095421075821
LOSS train 0.27212503830255086 valid 0.24021798620621362
LOSS train 0.27212503830255086 valid 0.24660808912345342
LOSS train 0.27212503830255086 valid 0.24430730193853378
LOSS train 0.27212503830255086 valid 0.24626502725813124
LOSS train 0.27212503830255086 valid 0.24874950349330902
LOSS train 0.27212503830255086 valid 0.24639135870066556
LOSS train 0.27212503830255086 valid 0.2476600781083107
LOSS train 0.27212503830255086 valid 0.24830758342376122
LOSS train 0.27212503830255086 valid 0.24820801509278162
LOSS train 0.27212503830255086 valid 0.24483396609624228
LOSS train 0.27212503830255086 valid 0.24493656866252422
LOSS train 0.27212503830255086 valid 0.24532214157721577
LOSS train 0.27212503830255086 valid 0.24681572284963396
LOSS train 0.27212503830255086 valid 0.2495961832372766
LOSS train 0.27212503830255086 valid 0.24887214004993438
LOSS train 0.27212503830255086 valid 0.24807144630522954
LOSS train 0.27212503830255086 valid 0.24633408744226803
LOSS train 0.27212503830255086 valid 0.24727217075617416
LOSS train 0.27212503830255086 valid 0.24628168903291225
LOSS train 0.27212503830255086 valid 0.2449455040693283
LOSS train 0.27212503830255086 valid 0.24556773041303342
LOSS train 0.27212503830255086 valid 0.24505575994650522
LOSS train 0.27212503830255086 valid 0.24564661351697786
LOSS train 0.27212503830255086 valid 0.2460929643491219
LOSS train 0.27212503830255086 valid 0.24784661879142125
LOSS train 0.27212503830255086 valid 0.24877440881344579
LOSS train 0.27212503830255086 valid 0.2483593812212348
LOSS train 0.27212503830255086 valid 0.24901606729536346
LOSS train 0.27212503830255086 valid 0.2489381029325373
LOSS train 0.27212503830255086 valid 0.2501701533794403
LOSS train 0.27212503830255086 valid 0.24984312264455688
LOSS train 0.27212503830255086 valid 0.24998179601656423
LOSS train 0.27212503830255086 valid 0.25077958130522776
LOSS train 0.27212503830255086 valid 0.25016900705985534
LOSS train 0.27212503830255086 valid 0.2505658183246851
LOSS train 0.27212503830255086 valid 0.2516166447866254
LOSS train 0.27212503830255086 valid 0.2512348652595565
LOSS train 0.27212503830255086 valid 0.2511577512635741
LOSS train 0.27212503830255086 valid 0.25145344876430253
LOSS train 0.27212503830255086 valid 0.2509224401579963
LOSS train 0.27212503830255086 valid 0.25137363827746845
LOSS train 0.27212503830255086 valid 0.25160259515681166
LOSS train 0.27212503830255086 valid 0.25174026067058247
LOSS train 0.27212503830255086 valid 0.2522605712316474
LOSS train 0.27212503830255086 valid 0.2517798081040382
LOSS train 0.27212503830255086 valid 0.25216881521776613
LOSS train 0.27212503830255086 valid 0.25163611425803256
LOSS train 0.27212503830255086 valid 0.25199790214592555
LOSS train 0.27212503830255086 valid 0.25175022306265654
LOSS train 0.27212503830255086 valid 0.2518539981408553
LOSS train 0.27212503830255086 valid 0.25140461990875856
LOSS train 0.27212503830255086 valid 0.251060537863196
LOSS train 0.27212503830255086 valid 0.2508753461056742
LOSS train 0.27212503830255086 valid 0.25147511544874157
LOSS train 0.27212503830255086 valid 0.25095886265238126
LOSS train 0.27212503830255086 valid 0.25104997465845014
LOSS train 0.27212503830255086 valid 0.25195433079235013
LOSS train 0.27212503830255086 valid 0.2523261523909039
LOSS train 0.27212503830255086 valid 0.25315194739960134
LOSS train 0.27212503830255086 valid 0.2533190431503149
LOSS train 0.27212503830255086 valid 0.253333010682554
LOSS train 0.27212503830255086 valid 0.2528301106015248
LOSS train 0.27212503830255086 valid 0.2527784440009033
LOSS train 0.27212503830255086 valid 0.2519812985606816
LOSS train 0.27212503830255086 valid 0.2521461346319744
LOSS train 0.27212503830255086 valid 0.25220767587003573
LOSS train 0.27212503830255086 valid 0.25242198465598953
LOSS train 0.27212503830255086 valid 0.25235802731285356
LOSS train 0.27212503830255086 valid 0.25236803031450994
LOSS train 0.27212503830255086 valid 0.25242896934350334
LOSS train 0.27212503830255086 valid 0.252890799978846
LOSS train 0.27212503830255086 valid 0.2530918816080341
LOSS train 0.27212503830255086 valid 0.2531764994447048
LOSS train 0.27212503830255086 valid 0.2531207403804682
LOSS train 0.27212503830255086 valid 0.25240051820874215
LOSS train 0.27212503830255086 valid 0.2516322623432418
LOSS train 0.27212503830255086 valid 0.25172839604499864
LOSS train 0.27212503830255086 valid 0.251582142997937
LOSS train 0.27212503830255086 valid 0.25145713257647695
LOSS train 0.27212503830255086 valid 0.2511102450244567
LOSS train 0.27212503830255086 valid 0.2504523041289906
LOSS train 0.27212503830255086 valid 0.25062723594835434
LOSS train 0.27212503830255086 valid 0.25013996626843105
LOSS train 0.27212503830255086 valid 0.25026642271641936
LOSS train 0.27212503830255086 valid 0.2506249745686849
LOSS train 0.27212503830255086 valid 0.2507238001613827
LOSS train 0.27212503830255086 valid 0.2507955571231635
LOSS train 0.27212503830255086 valid 0.25050980461541045
LOSS train 0.27212503830255086 valid 0.2507408806618224
LOSS train 0.27212503830255086 valid 0.25033676279218575
LOSS train 0.27212503830255086 valid 0.2506424679110448
LOSS train 0.27212503830255086 valid 0.2507332842989066
LOSS train 0.27212503830255086 valid 0.2507038545243594
LOSS train 0.27212503830255086 valid 0.2508676864884116
LOSS train 0.27212503830255086 valid 0.2511901494860649
LOSS train 0.27212503830255086 valid 0.2516400825859296
LOSS train 0.27212503830255086 valid 0.2517060143690483
LOSS train 0.27212503830255086 valid 0.25180020378631296
LOSS train 0.27212503830255086 valid 0.2516584930798182
LOSS train 0.27212503830255086 valid 0.2516713305598214
LOSS train 0.27212503830255086 valid 0.25192803130397257
LOSS train 0.27212503830255086 valid 0.2516936650064504
LOSS train 0.27212503830255086 valid 0.25191929370716765
LOSS train 0.27212503830255086 valid 0.2523598601238443
LOSS train 0.27212503830255086 valid 0.25264481888575985
LOSS train 0.27212503830255086 valid 0.252364758569915
LOSS train 0.27212503830255086 valid 0.25200935719268663
LOSS train 0.27212503830255086 valid 0.2519906007610591
LOSS train 0.27212503830255086 valid 0.2519675456641013
LOSS train 0.27212503830255086 valid 0.25181895015032396
LOSS train 0.27212503830255086 valid 0.2521078467112163
LOSS train 0.27212503830255086 valid 0.25236501410985607
LOSS train 0.27212503830255086 valid 0.25217506693581404
LOSS train 0.27212503830255086 valid 0.2520849157531722
LOSS train 0.27212503830255086 valid 0.25175801453491053
LOSS train 0.27212503830255086 valid 0.2516462805349965
LOSS train 0.27212503830255086 valid 0.2515025328173012
LOSS train 0.27212503830255086 valid 0.2515152597088155
LOSS train 0.27212503830255086 valid 0.2516758544070105
LOSS train 0.27212503830255086 valid 0.2516208488941193
LOSS train 0.27212503830255086 valid 0.2518505468254998
LOSS train 0.27212503830255086 valid 0.2517628655658932
LOSS train 0.27212503830255086 valid 0.25212880852632225
LOSS train 0.27212503830255086 valid 0.25210657253745916
LOSS train 0.27212503830255086 valid 0.25210162951396065
LOSS train 0.27212503830255086 valid 0.2520636099440451
LOSS train 0.27212503830255086 valid 0.25169612309246353
LOSS train 0.27212503830255086 valid 0.25150722747010396
LOSS train 0.27212503830255086 valid 0.25149384307772366
LOSS train 0.27212503830255086 valid 0.25141329798433515
LOSS train 0.27212503830255086 valid 0.2513202348833575
LOSS train 0.27212503830255086 valid 0.2511715574638687
LOSS train 0.27212503830255086 valid 0.25111167964296066
LOSS train 0.27212503830255086 valid 0.25098057820213787
LOSS train 0.27212503830255086 valid 0.25111438344631876
LOSS train 0.27212503830255086 valid 0.2511367500885159
LOSS train 0.27212503830255086 valid 0.2513416599429829
LOSS train 0.27212503830255086 valid 0.251339927315712
LOSS train 0.27212503830255086 valid 0.2513432739716437
LOSS train 0.27212503830255086 valid 0.25108861717684516
LOSS train 0.27212503830255086 valid 0.251364845323236
LOSS train 0.27212503830255086 valid 0.2511158783216866
LOSS train 0.27212503830255086 valid 0.25188895949238055
LOSS train 0.27212503830255086 valid 0.25191845699844745
LOSS train 0.27212503830255086 valid 0.25184899220863977
LOSS train 0.27212503830255086 valid 0.25174478554962487
LOSS train 0.27212503830255086 valid 0.2516189386185847
LOSS train 0.27212503830255086 valid 0.25164801038168616
LOSS train 0.27212503830255086 valid 0.2517100537752176
LOSS train 0.27212503830255086 valid 0.25183749429641233
LOSS train 0.27212503830255086 valid 0.2521810334844467
LOSS train 0.27212503830255086 valid 0.2521282103221128
LOSS train 0.27212503830255086 valid 0.2521468722556211
LOSS train 0.27212503830255086 valid 0.2521653746096593
LOSS train 0.27212503830255086 valid 0.2519100827164948
LOSS train 0.27212503830255086 valid 0.2518661219325865
LOSS train 0.27212503830255086 valid 0.2519206482871079
LOSS train 0.27212503830255086 valid 0.2518817403016646
LOSS train 0.27212503830255086 valid 0.2517184352547657
LOSS train 0.27212503830255086 valid 0.2515007236690232
LOSS train 0.27212503830255086 valid 0.251639976289617
LOSS train 0.27212503830255086 valid 0.2516789305174422
LOSS train 0.27212503830255086 valid 0.2515578820769276
LOSS train 0.27212503830255086 valid 0.25160095227893287
LOSS train 0.27212503830255086 valid 0.25181712464374656
LOSS train 0.27212503830255086 valid 0.25171877469932824
LOSS train 0.27212503830255086 valid 0.25151330327918364
LOSS train 0.27212503830255086 valid 0.2515265079419737
LOSS train 0.27212503830255086 valid 0.25147217846122283
LOSS train 0.27212503830255086 valid 0.2514389207533428
LOSS train 0.27212503830255086 valid 0.25135075190866535
LOSS train 0.27212503830255086 valid 0.25139338493683916
LOSS train 0.27212503830255086 valid 0.25166963785886765
LOSS train 0.27212503830255086 valid 0.25159822079722444
LOSS train 0.27212503830255086 valid 0.2516802219880952
LOSS train 0.27212503830255086 valid 0.25192095890888194
LOSS train 0.27212503830255086 valid 0.2519958882213949
LOSS train 0.27212503830255086 valid 0.25185525824463434
LOSS train 0.27212503830255086 valid 0.2517885693713375
LOSS train 0.27212503830255086 valid 0.25160813605463184
LOSS train 0.27212503830255086 valid 0.25149265668725457
LOSS train 0.27212503830255086 valid 0.2513608740453414
LOSS train 0.27212503830255086 valid 0.25145240674944636
LOSS train 0.27212503830255086 valid 0.2515693995214644
LOSS train 0.27212503830255086 valid 0.2516659618208283
LOSS train 0.27212503830255086 valid 0.25156506529341194
LOSS train 0.27212503830255086 valid 0.2514940230951955
LOSS train 0.27212503830255086 valid 0.251557519630447
LOSS train 0.27212503830255086 valid 0.2514056908254771
LOSS train 0.27212503830255086 valid 0.25109904354963547
LOSS train 0.27212503830255086 valid 0.25121304932601596
LOSS train 0.27212503830255086 valid 0.2512946553187927
LOSS train 0.27212503830255086 valid 0.25130231144151305
LOSS train 0.27212503830255086 valid 0.2513521163158081
LOSS train 0.27212503830255086 valid 0.2511182987689972
LOSS train 0.27212503830255086 valid 0.2509732353153513
LOSS train 0.27212503830255086 valid 0.2508583972507184
LOSS train 0.27212503830255086 valid 0.25075117399539854
LOSS train 0.27212503830255086 valid 0.25067372517842873
LOSS train 0.27212503830255086 valid 0.2505037653736952
LOSS train 0.27212503830255086 valid 0.25047880570286685
LOSS train 0.27212503830255086 valid 0.2504491351076946
LOSS train 0.27212503830255086 valid 0.25034823891921687
LOSS train 0.27212503830255086 valid 0.2502352272351963
LOSS train 0.27212503830255086 valid 0.2502869234908195
LOSS train 0.27212503830255086 valid 0.25030287980185867
LOSS train 0.27212503830255086 valid 0.25016011867039606
LOSS train 0.27212503830255086 valid 0.25020303811545663
LOSS train 0.27212503830255086 valid 0.25007860383419234
LOSS train 0.27212503830255086 valid 0.24994996210863424
LOSS train 0.27212503830255086 valid 0.24987083087088885
LOSS train 0.27212503830255086 valid 0.24986840551075298
LOSS train 0.27212503830255086 valid 0.24986192303786584
LOSS train 0.27212503830255086 valid 0.24995138962247057
LOSS train 0.27212503830255086 valid 0.25016278062354436
LOSS train 0.27212503830255086 valid 0.250280575986901
LOSS train 0.27212503830255086 valid 0.25025069008807876
LOSS train 0.27212503830255086 valid 0.2503789184205735
LOSS train 0.27212503830255086 valid 0.25055172074852244
LOSS train 0.27212503830255086 valid 0.25058004266685907
LOSS train 0.27212503830255086 valid 0.2507140465674147
LOSS train 0.27212503830255086 valid 0.2509296652635289
LOSS train 0.27212503830255086 valid 0.25100953099236156
LOSS train 0.27212503830255086 valid 0.25102138395653023
LOSS train 0.27212503830255086 valid 0.25109624260145685
LOSS train 0.27212503830255086 valid 0.25113148987293243
LOSS train 0.27212503830255086 valid 0.25114642179988583
LOSS train 0.27212503830255086 valid 0.2510242392448908
LOSS train 0.27212503830255086 valid 0.25109692650218296
LOSS train 0.27212503830255086 valid 0.25118444898027054
LOSS train 0.27212503830255086 valid 0.25114887264572966
LOSS train 0.27212503830255086 valid 0.2511612962192624
LOSS train 0.27212503830255086 valid 0.2511815638477061
LOSS train 0.27212503830255086 valid 0.2510790099159943
LOSS train 0.27212503830255086 valid 0.25097820597390336
LOSS train 0.27212503830255086 valid 0.251113130096578
LOSS train 0.27212503830255086 valid 0.25086834521825646
LOSS train 0.27212503830255086 valid 0.25098089781808264
LOSS train 0.27212503830255086 valid 0.2511651082361331
LOSS train 0.27212503830255086 valid 0.25126896427602186
LOSS train 0.27212503830255086 valid 0.2512157779277825
LOSS train 0.27212503830255086 valid 0.25120687810515585
LOSS train 0.27212503830255086 valid 0.2511684097229473
LOSS train 0.27212503830255086 valid 0.2510921053259248
LOSS train 0.27212503830255086 valid 0.2512344228625297
LOSS train 0.27212503830255086 valid 0.25118780118298245
LOSS train 0.27212503830255086 valid 0.25124659749960143
LOSS train 0.27212503830255086 valid 0.25111610362887854
LOSS train 0.27212503830255086 valid 0.2510699609250534
LOSS train 0.27212503830255086 valid 0.251105316304693
LOSS train 0.27212503830255086 valid 0.2511921395198442
LOSS train 0.27212503830255086 valid 0.25104493152074775
LOSS train 0.27212503830255086 valid 0.25104274623846823
LOSS train 0.27212503830255086 valid 0.25099325812921564
LOSS train 0.27212503830255086 valid 0.25098811433865476
LOSS train 0.27212503830255086 valid 0.25102582905027604
LOSS train 0.27212503830255086 valid 0.2510570004468656
LOSS train 0.27212503830255086 valid 0.25104911098462096
LOSS train 0.27212503830255086 valid 0.25106255653681175
LOSS train 0.27212503830255086 valid 0.25097840225921486
LOSS train 0.27212503830255086 valid 0.2509523239126779
LOSS train 0.27212503830255086 valid 0.2510898429133026
LOSS train 0.27212503830255086 valid 0.2512457827801135
LOSS train 0.27212503830255086 valid 0.2512931684135947
LOSS train 0.27212503830255086 valid 0.2512630943346907
LOSS train 0.27212503830255086 valid 0.25130976186687215
LOSS train 0.27212503830255086 valid 0.2515380489366019
LOSS train 0.27212503830255086 valid 0.25157105415077
LOSS train 0.27212503830255086 valid 0.2515838473056355
LOSS train 0.27212503830255086 valid 0.2515393473343416
LOSS train 0.27212503830255086 valid 0.2514998792414216
LOSS train 0.27212503830255086 valid 0.2513075680186172
LOSS train 0.27212503830255086 valid 0.2511498726529183
LOSS train 0.27212503830255086 valid 0.2511984208151431
LOSS train 0.27212503830255086 valid 0.2511828939829554
LOSS train 0.27212503830255086 valid 0.2511187936063339
LOSS train 0.27212503830255086 valid 0.2509319339977934
LOSS train 0.27212503830255086 valid 0.2509580524671204
LOSS train 0.27212503830255086 valid 0.2510607457181937
LOSS train 0.27212503830255086 valid 0.2511074156614772
LOSS train 0.27212503830255086 valid 0.25097860911717784
LOSS train 0.27212503830255086 valid 0.2508886052548678
LOSS train 0.27212503830255086 valid 0.2509477984988027
LOSS train 0.27212503830255086 valid 0.25097277131460116
LOSS train 0.27212503830255086 valid 0.2509665360224658
LOSS train 0.27212503830255086 valid 0.2509581402609848
LOSS train 0.27212503830255086 valid 0.25094260209023134
LOSS train 0.27212503830255086 valid 0.25100986456098
LOSS train 0.27212503830255086 valid 0.2511024660095066
LOSS train 0.27212503830255086 valid 0.2511610438763085
LOSS train 0.27212503830255086 valid 0.2511540022552819
LOSS train 0.27212503830255086 valid 0.2511644741200437
LOSS train 0.27212503830255086 valid 0.25117545324884005
LOSS train 0.27212503830255086 valid 0.2512225142390433
LOSS train 0.27212503830255086 valid 0.2511504356563091
LOSS train 0.27212503830255086 valid 0.2512049028940771
LOSS train 0.27212503830255086 valid 0.2512004684336138
LOSS train 0.27212503830255086 valid 0.251268460215515
LOSS train 0.27212503830255086 valid 0.2512517903294218
LOSS train 0.27212503830255086 valid 0.25114195087894065
LOSS train 0.27212503830255086 valid 0.25109023776124506
LOSS train 0.27212503830255086 valid 0.2510241287053602
LOSS train 0.27212503830255086 valid 0.2509134736928073
LOSS train 0.27212503830255086 valid 0.25107224261490657
LOSS train 0.27212503830255086 valid 0.2510182045159801
LOSS train 0.27212503830255086 valid 0.2509130204988829
LOSS train 0.27212503830255086 valid 0.25092575068657214
LOSS train 0.27212503830255086 valid 0.25096163229820445
LOSS train 0.27212503830255086 valid 0.25102891056400956
LOSS train 0.27212503830255086 valid 0.251044325790708
LOSS train 0.27212503830255086 valid 0.2509597331945655
LOSS train 0.27212503830255086 valid 0.2509945630557153
LOSS train 0.27212503830255086 valid 0.251054011379023
LOSS train 0.27212503830255086 valid 0.2511393004170032
LOSS train 0.27212503830255086 valid 0.25108586908318103
LOSS train 0.27212503830255086 valid 0.2511714428106201
LOSS train 0.27212503830255086 valid 0.2511126604972419
LOSS train 0.27212503830255086 valid 0.2510758834449868
LOSS train 0.27212503830255086 valid 0.25105740861208353
LOSS train 0.27212503830255086 valid 0.25098772943019865
LOSS train 0.27212503830255086 valid 0.2511325960784602
LOSS train 0.27212503830255086 valid 0.25122282369238885
LOSS train 0.27212503830255086 valid 0.2511828585368831
LOSS train 0.27212503830255086 valid 0.25130792999340046
LOSS train 0.27212503830255086 valid 0.25129481174729085
LOSS train 0.27212503830255086 valid 0.25125599596255493
LOSS train 0.27212503830255086 valid 0.25112938889897013
LOSS train 0.27212503830255086 valid 0.25110697938694254
LOSS train 0.27212503830255086 valid 0.25125387208368966
LOSS train 0.27212503830255086 valid 0.25125472398836224
LOSS train 0.27212503830255086 valid 0.2513010353293447
LOSS train 0.27212503830255086 valid 0.25119839365475255
LOSS train 0.27212503830255086 valid 0.251166004577332
LOSS train 0.27212503830255086 valid 0.2512241802384368
LOSS train 0.27212503830255086 valid 0.2512710927163853
LOSS train 0.27212503830255086 valid 0.25116683778699894
LOSS train 0.27212503830255086 valid 0.25114750570198247
LOSS train 0.27212503830255086 valid 0.2511626244373294
LOSS train 0.27212503830255086 valid 0.2513095196336508
LOSS train 0.27212503830255086 valid 0.2513191430896953
LOSS train 0.27212503830255086 valid 0.251233802968367
LOSS train 0.27212503830255086 valid 0.2511593652218151
LOSS train 0.27212503830255086 valid 0.2511244107080602
LOSS train 0.27212503830255086 valid 0.25119644710874145
LOSS train 0.27212503830255086 valid 0.25104320147207804
LOSS train 0.27212503830255086 valid 0.2509736762597011
LOSS train 0.27212503830255086 valid 0.2508947880176658
LOSS train 0.27212503830255086 valid 0.25096695082890075
LOSS train 0.27212503830255086 valid 0.25104553403995805
LOSS train 0.27212503830255086 valid 0.25115095179685404
LOSS train 0.27212503830255086 valid 0.2511986583220155
LOSS train 0.27212503830255086 valid 0.2511446105749333
LOSS train 0.27212503830255086 valid 0.2510589303334332
LOSS train 0.27212503830255086 valid 0.25101916442176425
LOSS train 0.27212503830255086 valid 0.2509308065391249
LOSS train 0.27212503830255086 valid 0.2508338268774038
LOSS train 0.27212503830255086 valid 0.2509272732458062
LOSS train 0.27212503830255086 valid 0.2507759843319244
LOSS train 0.27212503830255086 valid 0.2507590108684131
LOSS train 0.27212503830255086 valid 0.2507409682829086
LOSS train 0.27212503830255086 valid 0.2506660868724187
LOSS train 0.27212503830255086 valid 0.2505590654605091
LOSS train 0.27212503830255086 valid 0.2505405582892506
LOSS train 0.27212503830255086 valid 0.2505585147840221
EPOCH 9:
  batch 1 loss: 0.2747518718242645
  batch 2 loss: 0.26165972650051117
  batch 3 loss: 0.2510162442922592
  batch 4 loss: 0.25125132873654366
  batch 5 loss: 0.25878391563892367
  batch 6 loss: 0.255700059235096
  batch 7 loss: 0.258039191365242
  batch 8 loss: 0.26540389470756054
  batch 9 loss: 0.263531400097741
  batch 10 loss: 0.2666282683610916
  batch 11 loss: 0.2679653682491996
  batch 12 loss: 0.26515039304892224
  batch 13 loss: 0.2639417052268982
  batch 14 loss: 0.2648018641131265
  batch 15 loss: 0.267751936117808
  batch 16 loss: 0.2671991139650345
  batch 17 loss: 0.2647486346609452
  batch 18 loss: 0.26596047315332627
  batch 19 loss: 0.26600010928354767
  batch 20 loss: 0.2642966188490391
  batch 21 loss: 0.2645745100010009
  batch 22 loss: 0.2647090141068805
  batch 23 loss: 0.2634786224883536
  batch 24 loss: 0.2625375123073657
  batch 25 loss: 0.26445273458957674
  batch 26 loss: 0.2629828665118951
  batch 27 loss: 0.263279824897095
  batch 28 loss: 0.26222665927239824
  batch 29 loss: 0.2623796185542797
  batch 30 loss: 0.261913721760114
  batch 31 loss: 0.26277642384652167
  batch 32 loss: 0.2622684072703123
  batch 33 loss: 0.26223304054953833
  batch 34 loss: 0.26234301837051616
  batch 35 loss: 0.26269538658005853
  batch 36 loss: 0.26319198393159443
  batch 37 loss: 0.2634726488912428
  batch 38 loss: 0.26390508052549866
  batch 39 loss: 0.2635541504774338
  batch 40 loss: 0.2633397288620472
  batch 41 loss: 0.26308963647702843
  batch 42 loss: 0.26310609706810545
  batch 43 loss: 0.26287901748058407
  batch 44 loss: 0.26376218551939185
  batch 45 loss: 0.26330748167302875
  batch 46 loss: 0.26292453220357065
  batch 47 loss: 0.26289412474378626
  batch 48 loss: 0.2623714391763012
  batch 49 loss: 0.2625967394940707
  batch 50 loss: 0.26248559027910234
  batch 51 loss: 0.2622983978659499
  batch 52 loss: 0.2625791605275411
  batch 53 loss: 0.2618229889644767
  batch 54 loss: 0.26175741465003405
  batch 55 loss: 0.26094895980574867
  batch 56 loss: 0.26047478296927046
  batch 57 loss: 0.25975333075774343
  batch 58 loss: 0.2603071260041204
  batch 59 loss: 0.2605345239073543
  batch 60 loss: 0.2602251666287581
  batch 61 loss: 0.26038682534069313
  batch 62 loss: 0.26009280090370485
  batch 63 loss: 0.2594114650809576
  batch 64 loss: 0.26021655555814505
  batch 65 loss: 0.2598238949592297
  batch 66 loss: 0.259623722596602
  batch 67 loss: 0.25987673250596915
  batch 68 loss: 0.26038741670987187
  batch 69 loss: 0.260527051013449
  batch 70 loss: 0.26081815702574596
  batch 71 loss: 0.26025881271966744
  batch 72 loss: 0.2606659461226728
  batch 73 loss: 0.26068768068535686
  batch 74 loss: 0.2606179742394267
  batch 75 loss: 0.26023765524228415
  batch 76 loss: 0.2605387662586413
  batch 77 loss: 0.2602303558355802
  batch 78 loss: 0.26011929909388226
  batch 79 loss: 0.26051299594625643
  batch 80 loss: 0.26036566514521836
  batch 81 loss: 0.2603441169600428
  batch 82 loss: 0.26077610853968597
  batch 83 loss: 0.26079940993383705
  batch 84 loss: 0.26086306909010526
  batch 85 loss: 0.26062371047104105
  batch 86 loss: 0.2608060918228571
  batch 87 loss: 0.26061194604155663
  batch 88 loss: 0.2603273635560816
  batch 89 loss: 0.2603149481034011
  batch 90 loss: 0.26021584636635253
  batch 91 loss: 0.2604499318442502
  batch 92 loss: 0.2604248970746994
  batch 93 loss: 0.26028650854864427
  batch 94 loss: 0.2602249512330015
  batch 95 loss: 0.2600551818546496
  batch 96 loss: 0.26026435910413664
  batch 97 loss: 0.26067698677790535
  batch 98 loss: 0.26107026241263565
  batch 99 loss: 0.26131498633009015
  batch 100 loss: 0.26142208635807035
  batch 101 loss: 0.2614322546685096
  batch 102 loss: 0.2618005798727858
  batch 103 loss: 0.2622223004554082
  batch 104 loss: 0.2624092549085617
  batch 105 loss: 0.2622088507527397
  batch 106 loss: 0.26279621880571796
  batch 107 loss: 0.2623876748240997
  batch 108 loss: 0.26215588453191296
  batch 109 loss: 0.2619750181195933
  batch 110 loss: 0.2616909153082154
  batch 111 loss: 0.2617072158300125
  batch 112 loss: 0.26153738437486546
  batch 113 loss: 0.26170123427842573
  batch 114 loss: 0.2620023777871801
  batch 115 loss: 0.2620488149964291
  batch 116 loss: 0.26212405143626805
  batch 117 loss: 0.2623950017568393
  batch 118 loss: 0.26229057536791944
  batch 119 loss: 0.2623914185942722
  batch 120 loss: 0.26223644614219666
  batch 121 loss: 0.26209403586781715
  batch 122 loss: 0.2621124714124398
  batch 123 loss: 0.26180060498598146
  batch 124 loss: 0.26213159039616585
  batch 125 loss: 0.26205466520786286
  batch 126 loss: 0.26193338880936307
  batch 127 loss: 0.26246471679586125
  batch 128 loss: 0.2624694799305871
  batch 129 loss: 0.2626172321480374
  batch 130 loss: 0.2630316063761711
  batch 131 loss: 0.2631959170103073
  batch 132 loss: 0.26334936905539397
  batch 133 loss: 0.2635476251966075
  batch 134 loss: 0.26376342918001006
  batch 135 loss: 0.2638660736657955
  batch 136 loss: 0.2638640328146079
  batch 137 loss: 0.26362158553878756
  batch 138 loss: 0.2636985430035038
  batch 139 loss: 0.2642335736279865
  batch 140 loss: 0.26429951286741665
  batch 141 loss: 0.26441613928223334
  batch 142 loss: 0.2643777596908556
  batch 143 loss: 0.26429468674676404
  batch 144 loss: 0.264344859143926
  batch 145 loss: 0.2643077799986149
  batch 146 loss: 0.264291557649227
  batch 147 loss: 0.2646328911286633
  batch 148 loss: 0.26471391632347496
  batch 149 loss: 0.26476661780936606
  batch 150 loss: 0.264613448480765
  batch 151 loss: 0.2645721399034096
  batch 152 loss: 0.264733454996818
  batch 153 loss: 0.2645610179386887
  batch 154 loss: 0.26488305221904407
  batch 155 loss: 0.2648791322785039
  batch 156 loss: 0.26498518616725236
  batch 157 loss: 0.2653241087296966
  batch 158 loss: 0.2655350056630147
  batch 159 loss: 0.265623373617916
  batch 160 loss: 0.2655650105327368
  batch 161 loss: 0.2657733098690554
  batch 162 loss: 0.2659334639708201
  batch 163 loss: 0.2659413887679211
  batch 164 loss: 0.26607318004456965
  batch 165 loss: 0.26595599813894794
  batch 166 loss: 0.26591671105608883
  batch 167 loss: 0.2657814480052023
  batch 168 loss: 0.2658778163826182
  batch 169 loss: 0.2656321688692951
  batch 170 loss: 0.2654105477473315
  batch 171 loss: 0.26547582787379886
  batch 172 loss: 0.26539459426042644
  batch 173 loss: 0.2654052164857787
  batch 174 loss: 0.26555408126321334
  batch 175 loss: 0.2656303799152374
  batch 176 loss: 0.2655552488497712
  batch 177 loss: 0.2656603085119172
  batch 178 loss: 0.2658065693097168
  batch 179 loss: 0.2660060819633846
  batch 180 loss: 0.2660721437798606
  batch 181 loss: 0.26605458993938086
  batch 182 loss: 0.2659954666108875
  batch 183 loss: 0.2657825681355482
  batch 184 loss: 0.2655989313903062
  batch 185 loss: 0.2656545634205277
  batch 186 loss: 0.26569330387858936
  batch 187 loss: 0.26561864079001113
  batch 188 loss: 0.2655051837259151
  batch 189 loss: 0.26546908639095446
  batch 190 loss: 0.26559092763223147
  batch 191 loss: 0.26569920887497706
  batch 192 loss: 0.26599831009904545
  batch 193 loss: 0.2661370817243744
  batch 194 loss: 0.26635468190478295
  batch 195 loss: 0.2665772575598497
  batch 196 loss: 0.26657394517441185
  batch 197 loss: 0.2665414372979082
  batch 198 loss: 0.26666827108522856
  batch 199 loss: 0.2668121332798771
  batch 200 loss: 0.2670150975883007
  batch 201 loss: 0.2671443920823472
  batch 202 loss: 0.267156992218282
  batch 203 loss: 0.2673440055894147
  batch 204 loss: 0.2673184924265918
  batch 205 loss: 0.26757017068746614
  batch 206 loss: 0.2675592112309724
  batch 207 loss: 0.2675936795087252
  batch 208 loss: 0.2677268019089332
  batch 209 loss: 0.26763148121001046
  batch 210 loss: 0.26774713460888183
  batch 211 loss: 0.2677314748272512
  batch 212 loss: 0.2678124766164231
  batch 213 loss: 0.26788071812318526
  batch 214 loss: 0.26797683237590525
  batch 215 loss: 0.2677330551452415
  batch 216 loss: 0.2676864153946991
  batch 217 loss: 0.26789687161896086
  batch 218 loss: 0.267977008784036
  batch 219 loss: 0.26796152990430455
  batch 220 loss: 0.2681893433359536
  batch 221 loss: 0.2685297507101594
  batch 222 loss: 0.2685534491598069
  batch 223 loss: 0.26864912016776643
  batch 224 loss: 0.2687465567807002
  batch 225 loss: 0.268787259194586
  batch 226 loss: 0.2688474028115779
  batch 227 loss: 0.2686244186982184
  batch 228 loss: 0.2685827142967467
  batch 229 loss: 0.268494128139779
  batch 230 loss: 0.2685592138248941
  batch 231 loss: 0.2685632428307554
  batch 232 loss: 0.26840603467205476
  batch 233 loss: 0.2684334933501968
  batch 234 loss: 0.26852575084592545
  batch 235 loss: 0.2685799658298492
  batch 236 loss: 0.26849379062147466
  batch 237 loss: 0.2685955368013825
  batch 238 loss: 0.2686213302512129
  batch 239 loss: 0.2686267031286551
  batch 240 loss: 0.2687614936381578
  batch 241 loss: 0.2688290537392945
  batch 242 loss: 0.2687238315659121
  batch 243 loss: 0.2687201279181021
  batch 244 loss: 0.26862812347587994
  batch 245 loss: 0.26861202035631454
  batch 246 loss: 0.268507456573529
  batch 247 loss: 0.2685381425657736
  batch 248 loss: 0.26847801231328516
  batch 249 loss: 0.2684405049286693
  batch 250 loss: 0.2683499456048012
  batch 251 loss: 0.26833197527910135
  batch 252 loss: 0.2682379606579031
  batch 253 loss: 0.26810271696375293
  batch 254 loss: 0.26809004846754975
  batch 255 loss: 0.2680718258899801
  batch 256 loss: 0.26803475379711017
  batch 257 loss: 0.26799240227107407
  batch 258 loss: 0.2679788690320281
  batch 259 loss: 0.26796859922786476
  batch 260 loss: 0.26795381175783967
  batch 261 loss: 0.2678440923435021
  batch 262 loss: 0.2677007382380143
  batch 263 loss: 0.2677505136442728
  batch 264 loss: 0.26757951780702127
  batch 265 loss: 0.2674458702217858
  batch 266 loss: 0.2674287573847556
  batch 267 loss: 0.26742633289835427
  batch 268 loss: 0.2673378896468611
  batch 269 loss: 0.26729180162502486
  batch 270 loss: 0.26731890864946223
  batch 271 loss: 0.2672996318098364
  batch 272 loss: 0.2672690311446786
  batch 273 loss: 0.26731586625506154
  batch 274 loss: 0.267480306327343
  batch 275 loss: 0.26758055497299543
  batch 276 loss: 0.26757058441854903
  batch 277 loss: 0.2676484663886714
  batch 278 loss: 0.2676327233370259
  batch 279 loss: 0.26766973130378247
  batch 280 loss: 0.2675162831587451
  batch 281 loss: 0.2673801723536223
  batch 282 loss: 0.2674674504826255
  batch 283 loss: 0.2674277710198514
  batch 284 loss: 0.26741156380780984
  batch 285 loss: 0.2674371700537832
  batch 286 loss: 0.26731767927433225
  batch 287 loss: 0.26730332131585177
  batch 288 loss: 0.2670967585096757
  batch 289 loss: 0.26718834496286914
  batch 290 loss: 0.26704938185626065
  batch 291 loss: 0.2670526503492467
  batch 292 loss: 0.26711059912835083
  batch 293 loss: 0.26708683493601176
  batch 294 loss: 0.2669502260024045
  batch 295 loss: 0.26685995888912073
  batch 296 loss: 0.2669195754805932
  batch 297 loss: 0.26680175884805546
  batch 298 loss: 0.2667434274550252
  batch 299 loss: 0.2667073057248042
  batch 300 loss: 0.26676199634869896
  batch 301 loss: 0.26680087855091916
  batch 302 loss: 0.266748052263102
  batch 303 loss: 0.2668335256796859
  batch 304 loss: 0.266773911683183
  batch 305 loss: 0.26665048565043775
  batch 306 loss: 0.266728718442465
  batch 307 loss: 0.26662848346202306
  batch 308 loss: 0.26670233658575393
  batch 309 loss: 0.2666330102674398
  batch 310 loss: 0.2665952215752294
  batch 311 loss: 0.2667218589035261
  batch 312 loss: 0.26683825851441956
  batch 313 loss: 0.2668823958300173
  batch 314 loss: 0.26684123595618897
  batch 315 loss: 0.2668137953867988
  batch 316 loss: 0.2667453042879889
  batch 317 loss: 0.26673012313782607
  batch 318 loss: 0.2667380800037264
  batch 319 loss: 0.2667441738062891
  batch 320 loss: 0.26671684281900526
  batch 321 loss: 0.26672007166708
  batch 322 loss: 0.2666651673198487
  batch 323 loss: 0.2665979596022113
  batch 324 loss: 0.26634523852003944
  batch 325 loss: 0.26628361564416153
  batch 326 loss: 0.26637150456934616
  batch 327 loss: 0.26644291690001065
  batch 328 loss: 0.2662886480641801
  batch 329 loss: 0.26636077318691553
  batch 330 loss: 0.26627876835339
  batch 331 loss: 0.2662945166905481
  batch 332 loss: 0.26622447842754515
  batch 333 loss: 0.26627650619806115
  batch 334 loss: 0.26623856713493427
  batch 335 loss: 0.2661415150806085
  batch 336 loss: 0.2660795371269896
  batch 337 loss: 0.26586134727347854
  batch 338 loss: 0.26581492411667074
  batch 339 loss: 0.265671547447334
  batch 340 loss: 0.2656156364170944
  batch 341 loss: 0.265531102626904
  batch 342 loss: 0.2654571927953185
  batch 343 loss: 0.2655225835805732
  batch 344 loss: 0.26558702161838843
  batch 345 loss: 0.2656622094520624
  batch 346 loss: 0.2655799751509132
  batch 347 loss: 0.2655853379184987
  batch 348 loss: 0.2656386510907919
  batch 349 loss: 0.26553945486730013
  batch 350 loss: 0.2655678171770913
  batch 351 loss: 0.2655881920940856
  batch 352 loss: 0.2656261489133943
  batch 353 loss: 0.26562733057538107
  batch 354 loss: 0.2657243041163784
  batch 355 loss: 0.26565207167410515
  batch 356 loss: 0.26561434064688305
  batch 357 loss: 0.2654795255564174
  batch 358 loss: 0.26550371899118635
  batch 359 loss: 0.2655596503557269
  batch 360 loss: 0.26554477930896814
  batch 361 loss: 0.26545907771653415
  batch 362 loss: 0.26538961607118994
  batch 363 loss: 0.2653651650451103
  batch 364 loss: 0.26528214597767524
  batch 365 loss: 0.2653084744329322
  batch 366 loss: 0.2651970150366507
  batch 367 loss: 0.26514081730017547
  batch 368 loss: 0.2650650272388821
  batch 369 loss: 0.2650807796293481
  batch 370 loss: 0.26508043343956406
  batch 371 loss: 0.265062414491273
  batch 372 loss: 0.26500658763031804
  batch 373 loss: 0.265004633019501
  batch 374 loss: 0.26490518116058515
  batch 375 loss: 0.26481021972497304
  batch 376 loss: 0.2648353595286608
  batch 377 loss: 0.26485224945633734
  batch 378 loss: 0.26475645700302075
  batch 379 loss: 0.2648823021033823
  batch 380 loss: 0.26497212338604426
  batch 381 loss: 0.2649259099497257
  batch 382 loss: 0.26481944030023996
  batch 383 loss: 0.26477897163315167
  batch 384 loss: 0.2647522824894016
  batch 385 loss: 0.2648154821875808
  batch 386 loss: 0.26466458136399174
  batch 387 loss: 0.26473068725077065
  batch 388 loss: 0.26485085130198716
  batch 389 loss: 0.2648194917599769
  batch 390 loss: 0.26488807724836544
  batch 391 loss: 0.26490663350238214
  batch 392 loss: 0.2649547854172332
  batch 393 loss: 0.2650808236602002
  batch 394 loss: 0.26514689242325457
  batch 395 loss: 0.2651431824964813
  batch 396 loss: 0.2652411888720411
  batch 397 loss: 0.26525195363336607
  batch 398 loss: 0.26524009196153236
  batch 399 loss: 0.26534024504641246
  batch 400 loss: 0.2653357398882508
  batch 401 loss: 0.2652064376415457
  batch 402 loss: 0.2652763137547531
  batch 403 loss: 0.265320570497004
  batch 404 loss: 0.2653809744901586
  batch 405 loss: 0.26540959650351675
  batch 406 loss: 0.26554002231124585
  batch 407 loss: 0.2655316415890429
  batch 408 loss: 0.2656532517616071
  batch 409 loss: 0.2657218315709191
  batch 410 loss: 0.26577965431823963
  batch 411 loss: 0.26578260066300413
  batch 412 loss: 0.26581353444642236
  batch 413 loss: 0.2658685838699918
  batch 414 loss: 0.2658402042616393
  batch 415 loss: 0.26583953496203366
  batch 416 loss: 0.2658599323163239
  batch 417 loss: 0.2658693319101699
  batch 418 loss: 0.2658401482769747
  batch 419 loss: 0.2658063240509329
  batch 420 loss: 0.26590513342193195
  batch 421 loss: 0.26598379628250546
  batch 422 loss: 0.26622022557738834
  batch 423 loss: 0.2662328589117555
  batch 424 loss: 0.2662712618497745
  batch 425 loss: 0.2663457180822597
  batch 426 loss: 0.2663107644154432
  batch 427 loss: 0.2663483855944886
  batch 428 loss: 0.26629613802951074
  batch 429 loss: 0.26634922995295
  batch 430 loss: 0.2663077042892922
  batch 431 loss: 0.26647150768896266
  batch 432 loss: 0.2665410999691597
  batch 433 loss: 0.26651351986663724
  batch 434 loss: 0.2665834819322907
  batch 435 loss: 0.2665345456750914
  batch 436 loss: 0.26655224570577296
  batch 437 loss: 0.26656249469014
  batch 438 loss: 0.2666141209468994
  batch 439 loss: 0.2666461757280289
  batch 440 loss: 0.2666740439493548
  batch 441 loss: 0.2666470888588164
  batch 442 loss: 0.2666324403018973
  batch 443 loss: 0.2666511239975893
  batch 444 loss: 0.26657225010362834
  batch 445 loss: 0.2666362274228857
  batch 446 loss: 0.2666378127486182
  batch 447 loss: 0.26656756858430986
  batch 448 loss: 0.26650835048141225
  batch 449 loss: 0.266517959352591
  batch 450 loss: 0.26645646313826243
  batch 451 loss: 0.2664060218635525
  batch 452 loss: 0.26639857275032364
  batch 453 loss: 0.26645433461955575
  batch 454 loss: 0.26652031106308166
  batch 455 loss: 0.266521529509471
  batch 456 loss: 0.2665218337716764
  batch 457 loss: 0.26656610732266367
  batch 458 loss: 0.26654749523083715
  batch 459 loss: 0.2666250177717936
  batch 460 loss: 0.26673322974339775
  batch 461 loss: 0.26672484294194204
  batch 462 loss: 0.26675282721911675
  batch 463 loss: 0.26673349499959925
  batch 464 loss: 0.2667454858792239
  batch 465 loss: 0.266686382537247
  batch 466 loss: 0.2666016312819694
  batch 467 loss: 0.26668172241916493
  batch 468 loss: 0.2667086937297613
  batch 469 loss: 0.2668649634000843
  batch 470 loss: 0.2668685165808556
  batch 471 loss: 0.2668837733139658
  batch 472 loss: 0.26675185337031293
LOSS train 0.26675185337031293 valid 0.2329353392124176
LOSS train 0.26675185337031293 valid 0.2267632931470871
LOSS train 0.26675185337031293 valid 0.22789546847343445
LOSS train 0.26675185337031293 valid 0.2211124375462532
LOSS train 0.26675185337031293 valid 0.21398310363292694
LOSS train 0.26675185337031293 valid 0.21940929194291434
LOSS train 0.26675185337031293 valid 0.22489445975848607
LOSS train 0.26675185337031293 valid 0.223563389852643
LOSS train 0.26675185337031293 valid 0.22619973288642037
LOSS train 0.26675185337031293 valid 0.22879322469234467
LOSS train 0.26675185337031293 valid 0.2259987403046001
LOSS train 0.26675185337031293 valid 0.22738361358642578
LOSS train 0.26675185337031293 valid 0.2277080611540721
LOSS train 0.26675185337031293 valid 0.22822019564253943
LOSS train 0.26675185337031293 valid 0.22528632978598276
LOSS train 0.26675185337031293 valid 0.22556469682604074
LOSS train 0.26675185337031293 valid 0.2257325859630809
LOSS train 0.26675185337031293 valid 0.22689681003491083
LOSS train 0.26675185337031293 valid 0.2296623783676248
LOSS train 0.26675185337031293 valid 0.22902523055672647
LOSS train 0.26675185337031293 valid 0.22849946788379125
LOSS train 0.26675185337031293 valid 0.22690126570788297
LOSS train 0.26675185337031293 valid 0.22792728050895358
LOSS train 0.26675185337031293 valid 0.2272539393355449
LOSS train 0.26675185337031293 valid 0.22592859148979186
LOSS train 0.26675185337031293 valid 0.22660759206001574
LOSS train 0.26675185337031293 valid 0.22641190058655208
LOSS train 0.26675185337031293 valid 0.22684088934745109
LOSS train 0.26675185337031293 valid 0.22724558669945288
LOSS train 0.26675185337031293 valid 0.22887853880723316
LOSS train 0.26675185337031293 valid 0.22947199113907352
LOSS train 0.26675185337031293 valid 0.22921389155089855
LOSS train 0.26675185337031293 valid 0.22960787650310632
LOSS train 0.26675185337031293 valid 0.22952058210092433
LOSS train 0.26675185337031293 valid 0.23101521474974496
LOSS train 0.26675185337031293 valid 0.23076534478200805
LOSS train 0.26675185337031293 valid 0.23092245652868942
LOSS train 0.26675185337031293 valid 0.23152699047013334
LOSS train 0.26675185337031293 valid 0.23115603511150068
LOSS train 0.26675185337031293 valid 0.23146781250834464
LOSS train 0.26675185337031293 valid 0.23246408017670236
LOSS train 0.26675185337031293 valid 0.23204981606631053
LOSS train 0.26675185337031293 valid 0.23211475721625394
LOSS train 0.26675185337031293 valid 0.2324863767082041
LOSS train 0.26675185337031293 valid 0.2320264309644699
LOSS train 0.26675185337031293 valid 0.23249571705641953
LOSS train 0.26675185337031293 valid 0.2325762301049334
LOSS train 0.26675185337031293 valid 0.23261910087118545
LOSS train 0.26675185337031293 valid 0.2330482015196158
LOSS train 0.26675185337031293 valid 0.2326012670993805
LOSS train 0.26675185337031293 valid 0.23297368311414532
LOSS train 0.26675185337031293 valid 0.23254072952729005
LOSS train 0.26675185337031293 valid 0.23283082443588185
LOSS train 0.26675185337031293 valid 0.23257896055777869
LOSS train 0.26675185337031293 valid 0.23258110122247175
LOSS train 0.26675185337031293 valid 0.2321598862430879
LOSS train 0.26675185337031293 valid 0.23196620533340856
LOSS train 0.26675185337031293 valid 0.23166973683340797
LOSS train 0.26675185337031293 valid 0.23222040523916987
LOSS train 0.26675185337031293 valid 0.23167412479718527
LOSS train 0.26675185337031293 valid 0.23174507940401795
LOSS train 0.26675185337031293 valid 0.23256264338570257
LOSS train 0.26675185337031293 valid 0.2330138319068485
LOSS train 0.26675185337031293 valid 0.23385015316307545
LOSS train 0.26675185337031293 valid 0.23408000331658582
LOSS train 0.26675185337031293 valid 0.23405314033681696
LOSS train 0.26675185337031293 valid 0.23357927754743776
LOSS train 0.26675185337031293 valid 0.23344209505354657
LOSS train 0.26675185337031293 valid 0.23266735111457715
LOSS train 0.26675185337031293 valid 0.23279094334159578
LOSS train 0.26675185337031293 valid 0.23280569176438828
LOSS train 0.26675185337031293 valid 0.23302178705732027
LOSS train 0.26675185337031293 valid 0.23294214700182822
LOSS train 0.26675185337031293 valid 0.232992559872769
LOSS train 0.26675185337031293 valid 0.2330947548151016
LOSS train 0.26675185337031293 valid 0.2335583608793585
LOSS train 0.26675185337031293 valid 0.23368500869769554
LOSS train 0.26675185337031293 valid 0.23364513978744164
LOSS train 0.26675185337031293 valid 0.23356419254707383
LOSS train 0.26675185337031293 valid 0.23297463189810513
LOSS train 0.26675185337031293 valid 0.23222170255066435
LOSS train 0.26675185337031293 valid 0.23236243335939036
LOSS train 0.26675185337031293 valid 0.23222955845924745
LOSS train 0.26675185337031293 valid 0.23219482068504607
LOSS train 0.26675185337031293 valid 0.23182748468483197
LOSS train 0.26675185337031293 valid 0.2312055645987045
LOSS train 0.26675185337031293 valid 0.23134545810606288
LOSS train 0.26675185337031293 valid 0.23088665899227967
LOSS train 0.26675185337031293 valid 0.23106085668119153
LOSS train 0.26675185337031293 valid 0.23137091348568598
LOSS train 0.26675185337031293 valid 0.2314657564019109
LOSS train 0.26675185337031293 valid 0.23151202694229459
LOSS train 0.26675185337031293 valid 0.23119015911574003
LOSS train 0.26675185337031293 valid 0.23146049107642883
LOSS train 0.26675185337031293 valid 0.23103877666749453
LOSS train 0.26675185337031293 valid 0.2312570414505899
LOSS train 0.26675185337031293 valid 0.2313672502323524
LOSS train 0.26675185337031293 valid 0.23134224071186416
LOSS train 0.26675185337031293 valid 0.23147306884780075
LOSS train 0.26675185337031293 valid 0.23174591854214668
LOSS train 0.26675185337031293 valid 0.2321051959649171
LOSS train 0.26675185337031293 valid 0.2322485758101239
LOSS train 0.26675185337031293 valid 0.23237329591246483
LOSS train 0.26675185337031293 valid 0.2322645879422243
LOSS train 0.26675185337031293 valid 0.2322127442984354
LOSS train 0.26675185337031293 valid 0.23240715138754756
LOSS train 0.26675185337031293 valid 0.23215333643917727
LOSS train 0.26675185337031293 valid 0.2323189983489337
LOSS train 0.26675185337031293 valid 0.23278969457936943
LOSS train 0.26675185337031293 valid 0.23308598087592558
LOSS train 0.26675185337031293 valid 0.23283630421569756
LOSS train 0.26675185337031293 valid 0.2325172315218619
LOSS train 0.26675185337031293 valid 0.23256115153827497
LOSS train 0.26675185337031293 valid 0.23248294100426792
LOSS train 0.26675185337031293 valid 0.2323317915201187
LOSS train 0.26675185337031293 valid 0.2326297144694575
LOSS train 0.26675185337031293 valid 0.2328883678230465
LOSS train 0.26675185337031293 valid 0.23276706065161754
LOSS train 0.26675185337031293 valid 0.23264449108548524
LOSS train 0.26675185337031293 valid 0.23227226100862025
LOSS train 0.26675185337031293 valid 0.23215845522801737
LOSS train 0.26675185337031293 valid 0.2320379746741936
LOSS train 0.26675185337031293 valid 0.23206464546482738
LOSS train 0.26675185337031293 valid 0.23218690916415183
LOSS train 0.26675185337031293 valid 0.23214335227012634
LOSS train 0.26675185337031293 valid 0.2323344787434926
LOSS train 0.26675185337031293 valid 0.23223138395256884
LOSS train 0.26675185337031293 valid 0.23258490138687193
LOSS train 0.26675185337031293 valid 0.23257666179376055
LOSS train 0.26675185337031293 valid 0.23264216322165268
LOSS train 0.26675185337031293 valid 0.23264736687864057
LOSS train 0.26675185337031293 valid 0.2323093538482984
LOSS train 0.26675185337031293 valid 0.23211460353288435
LOSS train 0.26675185337031293 valid 0.23209853481445739
LOSS train 0.26675185337031293 valid 0.2320232586728202
LOSS train 0.26675185337031293 valid 0.2319596797666129
LOSS train 0.26675185337031293 valid 0.23181777996738462
LOSS train 0.26675185337031293 valid 0.2317278446710628
LOSS train 0.26675185337031293 valid 0.23163641421057338
LOSS train 0.26675185337031293 valid 0.23179321310349874
LOSS train 0.26675185337031293 valid 0.23177573856309797
LOSS train 0.26675185337031293 valid 0.23196502397177923
LOSS train 0.26675185337031293 valid 0.23193710114989247
LOSS train 0.26675185337031293 valid 0.23192001723994812
LOSS train 0.26675185337031293 valid 0.231713850847606
LOSS train 0.26675185337031293 valid 0.23190425611930351
LOSS train 0.26675185337031293 valid 0.23171012732041937
LOSS train 0.26675185337031293 valid 0.23242120636073318
LOSS train 0.26675185337031293 valid 0.2324519721453622
LOSS train 0.26675185337031293 valid 0.23237491577863692
LOSS train 0.26675185337031293 valid 0.23230545735911817
LOSS train 0.26675185337031293 valid 0.2322546551494222
LOSS train 0.26675185337031293 valid 0.2322499437464608
LOSS train 0.26675185337031293 valid 0.23232773633359313
LOSS train 0.26675185337031293 valid 0.23248342938961522
LOSS train 0.26675185337031293 valid 0.23279802797314447
LOSS train 0.26675185337031293 valid 0.23277485541477325
LOSS train 0.26675185337031293 valid 0.23277975741443754
LOSS train 0.26675185337031293 valid 0.23275854864960197
LOSS train 0.26675185337031293 valid 0.2325181556865573
LOSS train 0.26675185337031293 valid 0.23250758851537054
LOSS train 0.26675185337031293 valid 0.23254224952356314
LOSS train 0.26675185337031293 valid 0.2324851855544225
LOSS train 0.26675185337031293 valid 0.23239235189266322
LOSS train 0.26675185337031293 valid 0.232156641645865
LOSS train 0.26675185337031293 valid 0.23229329781719002
LOSS train 0.26675185337031293 valid 0.23237923323037382
LOSS train 0.26675185337031293 valid 0.23230221778863952
LOSS train 0.26675185337031293 valid 0.2323402088774732
LOSS train 0.26675185337031293 valid 0.2325794593376272
LOSS train 0.26675185337031293 valid 0.2324786354400958
LOSS train 0.26675185337031293 valid 0.2322695887712545
LOSS train 0.26675185337031293 valid 0.2322966847461083
LOSS train 0.26675185337031293 valid 0.23223216960142398
LOSS train 0.26675185337031293 valid 0.2321860660825457
LOSS train 0.26675185337031293 valid 0.23212862378832969
LOSS train 0.26675185337031293 valid 0.232188513386721
LOSS train 0.26675185337031293 valid 0.23241631150915382
LOSS train 0.26675185337031293 valid 0.2323648430948151
LOSS train 0.26675185337031293 valid 0.23242177524500424
LOSS train 0.26675185337031293 valid 0.23265688697606818
LOSS train 0.26675185337031293 valid 0.23268699506809423
LOSS train 0.26675185337031293 valid 0.2325279877974036
LOSS train 0.26675185337031293 valid 0.23246846923037714
LOSS train 0.26675185337031293 valid 0.2323081793817314
LOSS train 0.26675185337031293 valid 0.23221565839103472
LOSS train 0.26675185337031293 valid 0.23213323106102765
LOSS train 0.26675185337031293 valid 0.23223343000132987
LOSS train 0.26675185337031293 valid 0.23234532544852565
LOSS train 0.26675185337031293 valid 0.23242632723168322
LOSS train 0.26675185337031293 valid 0.23233661032159916
LOSS train 0.26675185337031293 valid 0.23226480550753573
LOSS train 0.26675185337031293 valid 0.23232931292427636
LOSS train 0.26675185337031293 valid 0.23219114211723976
LOSS train 0.26675185337031293 valid 0.23191067622258113
LOSS train 0.26675185337031293 valid 0.23204200045794857
LOSS train 0.26675185337031293 valid 0.23215788148986508
LOSS train 0.26675185337031293 valid 0.23216269142699963
LOSS train 0.26675185337031293 valid 0.2321964280689182
LOSS train 0.26675185337031293 valid 0.2319884093850851
LOSS train 0.26675185337031293 valid 0.23184837119199744
LOSS train 0.26675185337031293 valid 0.23175014813642691
LOSS train 0.26675185337031293 valid 0.23165254236148497
LOSS train 0.26675185337031293 valid 0.23155355672625935
LOSS train 0.26675185337031293 valid 0.23138968144975058
LOSS train 0.26675185337031293 valid 0.23134058381168587
LOSS train 0.26675185337031293 valid 0.23125348488489786
LOSS train 0.26675185337031293 valid 0.23120880800370985
LOSS train 0.26675185337031293 valid 0.2311370255844445
LOSS train 0.26675185337031293 valid 0.23118792857442583
LOSS train 0.26675185337031293 valid 0.23119913204021364
LOSS train 0.26675185337031293 valid 0.2310417987546831
LOSS train 0.26675185337031293 valid 0.23108014247506997
LOSS train 0.26675185337031293 valid 0.23098097358630082
LOSS train 0.26675185337031293 valid 0.23083858406820962
LOSS train 0.26675185337031293 valid 0.23074068577477225
LOSS train 0.26675185337031293 valid 0.23075271827009966
LOSS train 0.26675185337031293 valid 0.2307517537419949
LOSS train 0.26675185337031293 valid 0.2308357850601684
LOSS train 0.26675185337031293 valid 0.2309761945496906
LOSS train 0.26675185337031293 valid 0.23107804445659413
LOSS train 0.26675185337031293 valid 0.2310216010019586
LOSS train 0.26675185337031293 valid 0.2311443566875073
LOSS train 0.26675185337031293 valid 0.23129073896312288
LOSS train 0.26675185337031293 valid 0.23129697647359637
LOSS train 0.26675185337031293 valid 0.2314157357389948
LOSS train 0.26675185337031293 valid 0.23162547023023278
LOSS train 0.26675185337031293 valid 0.23166551460561
LOSS train 0.26675185337031293 valid 0.23168131221069518
LOSS train 0.26675185337031293 valid 0.23177274899638217
LOSS train 0.26675185337031293 valid 0.23176775537269972
LOSS train 0.26675185337031293 valid 0.23180527927289749
LOSS train 0.26675185337031293 valid 0.23168035222481248
LOSS train 0.26675185337031293 valid 0.23172625542705894
LOSS train 0.26675185337031293 valid 0.2318005564364981
LOSS train 0.26675185337031293 valid 0.23175673729787438
LOSS train 0.26675185337031293 valid 0.23177551594464588
LOSS train 0.26675185337031293 valid 0.23178148351046218
LOSS train 0.26675185337031293 valid 0.2317057919053353
LOSS train 0.26675185337031293 valid 0.2316290104140838
LOSS train 0.26675185337031293 valid 0.23176893505318036
LOSS train 0.26675185337031293 valid 0.23152106267607903
LOSS train 0.26675185337031293 valid 0.23163466842331512
LOSS train 0.26675185337031293 valid 0.23180544968755518
LOSS train 0.26675185337031293 valid 0.23191945048011078
LOSS train 0.26675185337031293 valid 0.2318835168228886
LOSS train 0.26675185337031293 valid 0.23186375924691496
LOSS train 0.26675185337031293 valid 0.23185701013332413
LOSS train 0.26675185337031293 valid 0.2318073812139082
LOSS train 0.26675185337031293 valid 0.23193713158369064
LOSS train 0.26675185337031293 valid 0.2318882743319667
LOSS train 0.26675185337031293 valid 0.23198123317625788
LOSS train 0.26675185337031293 valid 0.23184207723781525
LOSS train 0.26675185337031293 valid 0.23181510968940464
LOSS train 0.26675185337031293 valid 0.2318807237288531
LOSS train 0.26675185337031293 valid 0.2319378411048092
LOSS train 0.26675185337031293 valid 0.23180956431168062
LOSS train 0.26675185337031293 valid 0.2317774205129276
LOSS train 0.26675185337031293 valid 0.23175795899855123
LOSS train 0.26675185337031293 valid 0.23173427020127957
LOSS train 0.26675185337031293 valid 0.23177496677842632
LOSS train 0.26675185337031293 valid 0.23179560057989512
LOSS train 0.26675185337031293 valid 0.23176183690363011
LOSS train 0.26675185337031293 valid 0.23177139308642258
LOSS train 0.26675185337031293 valid 0.23169389181541947
LOSS train 0.26675185337031293 valid 0.23165357689884372
LOSS train 0.26675185337031293 valid 0.2317355117212967
LOSS train 0.26675185337031293 valid 0.23188054567175126
LOSS train 0.26675185337031293 valid 0.23194108576579608
LOSS train 0.26675185337031293 valid 0.23187958388416854
LOSS train 0.26675185337031293 valid 0.23190076638191828
LOSS train 0.26675185337031293 valid 0.23211064030799797
LOSS train 0.26675185337031293 valid 0.23208237327498832
LOSS train 0.26675185337031293 valid 0.2320793646856816
LOSS train 0.26675185337031293 valid 0.23204037444158035
LOSS train 0.26675185337031293 valid 0.2319873407267142
LOSS train 0.26675185337031293 valid 0.2318301404211065
LOSS train 0.26675185337031293 valid 0.23167615737632025
LOSS train 0.26675185337031293 valid 0.23174078500826298
LOSS train 0.26675185337031293 valid 0.23171938737588268
LOSS train 0.26675185337031293 valid 0.2316697951525556
LOSS train 0.26675185337031293 valid 0.231491837465594
LOSS train 0.26675185337031293 valid 0.23153318312058602
LOSS train 0.26675185337031293 valid 0.23163004167063136
LOSS train 0.26675185337031293 valid 0.23164182305335998
LOSS train 0.26675185337031293 valid 0.23155336405013824
LOSS train 0.26675185337031293 valid 0.23146902289540094
LOSS train 0.26675185337031293 valid 0.2315322308180233
LOSS train 0.26675185337031293 valid 0.23153021519159364
LOSS train 0.26675185337031293 valid 0.23154986372281766
LOSS train 0.26675185337031293 valid 0.23156205910382813
LOSS train 0.26675185337031293 valid 0.23155668944324534
LOSS train 0.26675185337031293 valid 0.23159916103903344
LOSS train 0.26675185337031293 valid 0.2317096382176795
LOSS train 0.26675185337031293 valid 0.2317904886552843
LOSS train 0.26675185337031293 valid 0.23178203071694117
LOSS train 0.26675185337031293 valid 0.23178548062289203
LOSS train 0.26675185337031293 valid 0.2317938949137726
LOSS train 0.26675185337031293 valid 0.23184705672654818
LOSS train 0.26675185337031293 valid 0.23178309892614682
LOSS train 0.26675185337031293 valid 0.23185583805523047
LOSS train 0.26675185337031293 valid 0.2318375359801267
LOSS train 0.26675185337031293 valid 0.2319036463994791
LOSS train 0.26675185337031293 valid 0.2318810800972738
LOSS train 0.26675185337031293 valid 0.23179266149880456
LOSS train 0.26675185337031293 valid 0.2317230961973371
LOSS train 0.26675185337031293 valid 0.23166835492519292
LOSS train 0.26675185337031293 valid 0.23158661295454225
LOSS train 0.26675185337031293 valid 0.23173309722764593
LOSS train 0.26675185337031293 valid 0.23169403249217618
LOSS train 0.26675185337031293 valid 0.23157578491129677
LOSS train 0.26675185337031293 valid 0.2315900517293276
LOSS train 0.26675185337031293 valid 0.23160415416517957
LOSS train 0.26675185337031293 valid 0.23166159355336693
LOSS train 0.26675185337031293 valid 0.23166325267345186
LOSS train 0.26675185337031293 valid 0.23158353699159018
LOSS train 0.26675185337031293 valid 0.23159886802596624
LOSS train 0.26675185337031293 valid 0.23165691775158517
LOSS train 0.26675185337031293 valid 0.2317093233329749
LOSS train 0.26675185337031293 valid 0.23165415236726403
LOSS train 0.26675185337031293 valid 0.2317265732637447
LOSS train 0.26675185337031293 valid 0.23168164940539354
LOSS train 0.26675185337031293 valid 0.23164946911135695
LOSS train 0.26675185337031293 valid 0.23165562804098483
LOSS train 0.26675185337031293 valid 0.23156156429877647
LOSS train 0.26675185337031293 valid 0.23169020458233136
LOSS train 0.26675185337031293 valid 0.23177377246206324
LOSS train 0.26675185337031293 valid 0.23174866543310443
LOSS train 0.26675185337031293 valid 0.23186148662335243
LOSS train 0.26675185337031293 valid 0.23185310937238462
LOSS train 0.26675185337031293 valid 0.23182413675273653
LOSS train 0.26675185337031293 valid 0.23173182287130012
LOSS train 0.26675185337031293 valid 0.23171375360753801
LOSS train 0.26675185337031293 valid 0.23184561448361346
LOSS train 0.26675185337031293 valid 0.23185405393144978
LOSS train 0.26675185337031293 valid 0.23188844886386678
LOSS train 0.26675185337031293 valid 0.23177379011577243
LOSS train 0.26675185337031293 valid 0.23174463829697942
LOSS train 0.26675185337031293 valid 0.23178318643991927
LOSS train 0.26675185337031293 valid 0.23183461541638656
LOSS train 0.26675185337031293 valid 0.23175303686049678
LOSS train 0.26675185337031293 valid 0.2317434411212715
LOSS train 0.26675185337031293 valid 0.23175409848940268
LOSS train 0.26675185337031293 valid 0.23189976425884767
LOSS train 0.26675185337031293 valid 0.23191964539928714
LOSS train 0.26675185337031293 valid 0.23182373808298498
LOSS train 0.26675185337031293 valid 0.23175832277072608
LOSS train 0.26675185337031293 valid 0.23171916484147653
LOSS train 0.26675185337031293 valid 0.23178994800778036
LOSS train 0.26675185337031293 valid 0.23166225471666882
LOSS train 0.26675185337031293 valid 0.23160959283510843
LOSS train 0.26675185337031293 valid 0.23151429598643022
LOSS train 0.26675185337031293 valid 0.23157999527353065
LOSS train 0.26675185337031293 valid 0.23163838448834284
LOSS train 0.26675185337031293 valid 0.23175093984939682
LOSS train 0.26675185337031293 valid 0.23179875644907522
LOSS train 0.26675185337031293 valid 0.2317544585432993
LOSS train 0.26675185337031293 valid 0.23168241378315335
LOSS train 0.26675185337031293 valid 0.23164964207865735
LOSS train 0.26675185337031293 valid 0.23155204504728316
LOSS train 0.26675185337031293 valid 0.23146370622920198
LOSS train 0.26675185337031293 valid 0.23155160042462428
LOSS train 0.26675185337031293 valid 0.23140032626380605
LOSS train 0.26675185337031293 valid 0.23135818025240532
LOSS train 0.26675185337031293 valid 0.23131108675917533
LOSS train 0.26675185337031293 valid 0.23123863648847154
LOSS train 0.26675185337031293 valid 0.2311387946397797
LOSS train 0.26675185337031293 valid 0.23110333044567835
LOSS train 0.26675185337031293 valid 0.2311398339707677
EPOCH 10:
  batch 1 loss: 0.2697732150554657
  batch 2 loss: 0.2554500699043274
  batch 3 loss: 0.2540274163087209
  batch 4 loss: 0.25062545761466026
  batch 5 loss: 0.2526539355516434
  batch 6 loss: 0.2503019745151202
  batch 7 loss: 0.2513765458549772
  batch 8 loss: 0.25580775551497936
  batch 9 loss: 0.2575195249583986
  batch 10 loss: 0.2586349919438362
  batch 11 loss: 0.26028167795051227
  batch 12 loss: 0.2567201741039753
  batch 13 loss: 0.25593973581607526
  batch 14 loss: 0.2570082651717322
  batch 15 loss: 0.25905316869417827
  batch 16 loss: 0.258339986205101
  batch 17 loss: 0.2573806415585911
  batch 18 loss: 0.2575954645872116
  batch 19 loss: 0.2571192289653577
  batch 20 loss: 0.2558937147259712
  batch 21 loss: 0.2562061221826644
  batch 22 loss: 0.25614565069025214
  batch 23 loss: 0.25533629241197003
  batch 24 loss: 0.2540341770897309
  batch 25 loss: 0.25638349950313566
  batch 26 loss: 0.2555107915630707
  batch 27 loss: 0.25600949536871026
  batch 28 loss: 0.2551072048289435
  batch 29 loss: 0.2557912253100297
  batch 30 loss: 0.2557783047358195
  batch 31 loss: 0.2570327164665345
  batch 32 loss: 0.2566936630755663
  batch 33 loss: 0.25646124825333105
  batch 34 loss: 0.25704341951538534
  batch 35 loss: 0.2572799955095564
  batch 36 loss: 0.25756875508361393
  batch 37 loss: 0.25761448370443807
  batch 38 loss: 0.2584160472217359
  batch 39 loss: 0.2577966310274907
  batch 40 loss: 0.2575116377323866
  batch 41 loss: 0.25726887256633946
  batch 42 loss: 0.257378430948371
  batch 43 loss: 0.2574595590663511
  batch 44 loss: 0.257729192010381
  batch 45 loss: 0.25765896605120764
  batch 46 loss: 0.25743753884149634
  batch 47 loss: 0.2579382042935554
  batch 48 loss: 0.25757833073536557
  batch 49 loss: 0.2578077577814764
  batch 50 loss: 0.25776011884212496
  batch 51 loss: 0.2574314612383936
  batch 52 loss: 0.25766396780426687
  batch 53 loss: 0.257113830942028
  batch 54 loss: 0.25691927279587146
  batch 55 loss: 0.25638972331177107
  batch 56 loss: 0.2561227101832628
  batch 57 loss: 0.25542561474599335
  batch 58 loss: 0.2558368721912647
  batch 59 loss: 0.2562669076151767
  batch 60 loss: 0.25593008622527125
  batch 61 loss: 0.2562290847790046
  batch 62 loss: 0.2559834841278292
  batch 63 loss: 0.2551020213535854
  batch 64 loss: 0.25574141973629594
  batch 65 loss: 0.2553136412913983
  batch 66 loss: 0.2551028434977387
  batch 67 loss: 0.25534238076921717
  batch 68 loss: 0.2558767572045326
  batch 69 loss: 0.25584546033886896
  batch 70 loss: 0.25620257897036414
  batch 71 loss: 0.255672693462439
  batch 72 loss: 0.2558237055523528
  batch 73 loss: 0.25568077825520136
  batch 74 loss: 0.25555728396048416
  batch 75 loss: 0.25497629404067995
  batch 76 loss: 0.25514890880961166
  batch 77 loss: 0.25479548143876063
  batch 78 loss: 0.25454043730711323
  batch 79 loss: 0.2549199835409092
  batch 80 loss: 0.2547294460237026
  batch 81 loss: 0.2549947367774116
  batch 82 loss: 0.25531488802374863
  batch 83 loss: 0.2552430893642357
  batch 84 loss: 0.2553175107708999
  batch 85 loss: 0.2552251151379417
  batch 86 loss: 0.2554990612836771
  batch 87 loss: 0.25540354539608134
  batch 88 loss: 0.2552269460125403
  batch 89 loss: 0.2551209536496173
  batch 90 loss: 0.25505384869045683
  batch 91 loss: 0.25539944198105363
  batch 92 loss: 0.255274858487689
  batch 93 loss: 0.2550550107994387
  batch 94 loss: 0.25522157470596596
  batch 95 loss: 0.25509346362791563
  batch 96 loss: 0.2551990677602589
  batch 97 loss: 0.25543669496000426
  batch 98 loss: 0.2556855020170309
  batch 99 loss: 0.25584778806777914
  batch 100 loss: 0.2558498503267765
  batch 101 loss: 0.25566970623365726
  batch 102 loss: 0.2560447252848569
  batch 103 loss: 0.2564563160961114
  batch 104 loss: 0.2565738478532204
  batch 105 loss: 0.25639306207497914
  batch 106 loss: 0.2569184217531726
  batch 107 loss: 0.2563426580941566
  batch 108 loss: 0.25624070758069
  batch 109 loss: 0.2559500666113075
  batch 110 loss: 0.25563219460574066
  batch 111 loss: 0.2554072873280929
  batch 112 loss: 0.25521095456289394
  batch 113 loss: 0.2554694927107971
  batch 114 loss: 0.2558703410782312
  batch 115 loss: 0.2560556993536327
  batch 116 loss: 0.2561892457306385
  batch 117 loss: 0.25636646062390417
  batch 118 loss: 0.2563982480915926
  batch 119 loss: 0.2567292250254575
  batch 120 loss: 0.2566576413810253
  batch 121 loss: 0.25647223870123714
  batch 122 loss: 0.25647066203785723
  batch 123 loss: 0.2560471619774656
  batch 124 loss: 0.25630430648884467
  batch 125 loss: 0.25604638612270353
  batch 126 loss: 0.2559895110981805
  batch 127 loss: 0.2563659222576562
  batch 128 loss: 0.25652035092934966
  batch 129 loss: 0.25677397269611213
  batch 130 loss: 0.25712808393515074
  batch 131 loss: 0.2573053647998635
  batch 132 loss: 0.2573720555413853
  batch 133 loss: 0.25748416714202194
  batch 134 loss: 0.2577079025222294
  batch 135 loss: 0.2577722414776131
  batch 136 loss: 0.25782475751989026
  batch 137 loss: 0.2576051405529036
  batch 138 loss: 0.25766861967850424
  batch 139 loss: 0.2579792121545874
  batch 140 loss: 0.25792521334120205
  batch 141 loss: 0.2579627913152072
  batch 142 loss: 0.25801948646844275
  batch 143 loss: 0.2579389605280403
  batch 144 loss: 0.2579900795179937
  batch 145 loss: 0.2579036014861074
  batch 146 loss: 0.2577340714123151
  batch 147 loss: 0.25797526173445645
  batch 148 loss: 0.25804977451224587
  batch 149 loss: 0.2581607896409579
  batch 150 loss: 0.2580903829137484
  batch 151 loss: 0.2580413876582455
  batch 152 loss: 0.2581692776201587
  batch 153 loss: 0.2579532326046937
  batch 154 loss: 0.2581246208834958
  batch 155 loss: 0.25799164791261
  batch 156 loss: 0.2580792411015584
  batch 157 loss: 0.25849572925051306
  batch 158 loss: 0.25868793473213536
  batch 159 loss: 0.25870011512588403
  batch 160 loss: 0.25860949978232384
  batch 161 loss: 0.2587837989656081
  batch 162 loss: 0.258921503652761
  batch 163 loss: 0.25888347516030624
  batch 164 loss: 0.25895908329545
  batch 165 loss: 0.2588193557479165
  batch 166 loss: 0.2588794865162976
  batch 167 loss: 0.25876363299920885
  batch 168 loss: 0.25881262292109786
  batch 169 loss: 0.2585837975056214
  batch 170 loss: 0.25826981120249803
  batch 171 loss: 0.25836634095649275
  batch 172 loss: 0.25834800337636193
  batch 173 loss: 0.25838742021880395
  batch 174 loss: 0.2585716074568102
  batch 175 loss: 0.2586020987374442
  batch 176 loss: 0.25857199406759307
  batch 177 loss: 0.25862316913523914
  batch 178 loss: 0.25873283874452785
  batch 179 loss: 0.2589638802592315
  batch 180 loss: 0.2589937369028727
  batch 181 loss: 0.2589927893646514
  batch 182 loss: 0.25901442717064865
  batch 183 loss: 0.2588010097299117
  batch 184 loss: 0.25862930318259675
  batch 185 loss: 0.2586762823768564
  batch 186 loss: 0.25880201938011316
  batch 187 loss: 0.25875221431892825
  batch 188 loss: 0.25861554743444665
  batch 189 loss: 0.25858297305447714
  batch 190 loss: 0.2587605408147762
  batch 191 loss: 0.25882244835661344
  batch 192 loss: 0.25902145146392286
  batch 193 loss: 0.25911077818413475
  batch 194 loss: 0.25932397693395615
  batch 195 loss: 0.2595090866853029
  batch 196 loss: 0.259526931585706
  batch 197 loss: 0.25961084542843293
  batch 198 loss: 0.25976014746861026
  batch 199 loss: 0.2597741311668751
  batch 200 loss: 0.2599485928565264
  batch 201 loss: 0.2600840059530676
  batch 202 loss: 0.26005875274981605
  batch 203 loss: 0.259997597101874
  batch 204 loss: 0.2598932640371369
  batch 205 loss: 0.26019700656576855
  batch 206 loss: 0.260252854850107
  batch 207 loss: 0.2602883935118643
  batch 208 loss: 0.26041201552232873
  batch 209 loss: 0.2603117778683393
  batch 210 loss: 0.2604691547297296
  batch 211 loss: 0.2604175853362016
  batch 212 loss: 0.2604239859249232
  batch 213 loss: 0.26047570863520036
  batch 214 loss: 0.26059736861525296
  batch 215 loss: 0.26035745289436607
  batch 216 loss: 0.2603399585932493
  batch 217 loss: 0.2604828311413664
  batch 218 loss: 0.26057435496958026
  batch 219 loss: 0.26055540688778167
  batch 220 loss: 0.26071204793724145
  batch 221 loss: 0.2610268458656596
  batch 222 loss: 0.2611276005973687
  batch 223 loss: 0.26122366159218846
  batch 224 loss: 0.2612572347612253
  batch 225 loss: 0.26127926210562386
  batch 226 loss: 0.2613754084548064
  batch 227 loss: 0.2611559258552375
  batch 228 loss: 0.2610577820032312
  batch 229 loss: 0.26098331015182896
  batch 230 loss: 0.2610633443231168
  batch 231 loss: 0.26104446548920174
  batch 232 loss: 0.26091653532509146
  batch 233 loss: 0.260925375980369
  batch 234 loss: 0.2610577407300982
  batch 235 loss: 0.26110687040268105
  batch 236 loss: 0.26108037200519596
  batch 237 loss: 0.26115709417479954
  batch 238 loss: 0.2611743142875303
  batch 239 loss: 0.261218696583265
  batch 240 loss: 0.2613923034320275
  batch 241 loss: 0.26151642500117606
  batch 242 loss: 0.26147194760890047
  batch 243 loss: 0.26146748796902564
  batch 244 loss: 0.2613412949149726
  batch 245 loss: 0.2612959361806208
  batch 246 loss: 0.26118871763469725
  batch 247 loss: 0.26119999798685917
  batch 248 loss: 0.2611155732865295
  batch 249 loss: 0.26107932890514773
  batch 250 loss: 0.2609833186864853
  batch 251 loss: 0.26091322112843335
  batch 252 loss: 0.2608285409117502
  batch 253 loss: 0.26070213017491956
  batch 254 loss: 0.26073425005036077
  batch 255 loss: 0.2607467106159996
  batch 256 loss: 0.2607227278058417
  batch 257 loss: 0.26073867111586413
  batch 258 loss: 0.26072119817484257
  batch 259 loss: 0.2607297880654169
  batch 260 loss: 0.2607416989138493
  batch 261 loss: 0.260665867235012
  batch 262 loss: 0.2605578054794828
  batch 263 loss: 0.26056425885329226
  batch 264 loss: 0.260369250042872
  batch 265 loss: 0.26025635571974626
  batch 266 loss: 0.2602345647108286
  batch 267 loss: 0.26025718768660944
  batch 268 loss: 0.2601881685541637
  batch 269 loss: 0.26009603498372
  batch 270 loss: 0.26015400770637725
  batch 271 loss: 0.2601184890608946
  batch 272 loss: 0.260096021270489
  batch 273 loss: 0.2601288390574438
  batch 274 loss: 0.2602831882825733
  batch 275 loss: 0.2604010518572547
  batch 276 loss: 0.2603883567819561
  batch 277 loss: 0.2605035656303275
  batch 278 loss: 0.26044025341812654
  batch 279 loss: 0.2604537401148068
  batch 280 loss: 0.2603136600660426
  batch 281 loss: 0.2602719388292353
  batch 282 loss: 0.2604156491075847
  batch 283 loss: 0.26036985239586646
  batch 284 loss: 0.26031979299347163
  batch 285 loss: 0.2602808630257322
  batch 286 loss: 0.2602270053816842
  batch 287 loss: 0.26025181261089203
  batch 288 loss: 0.26004146437885034
  batch 289 loss: 0.26010723453285783
  batch 290 loss: 0.25995298870678607
  batch 291 loss: 0.2599242530327892
  batch 292 loss: 0.25997414123522095
  batch 293 loss: 0.25998253850806696
  batch 294 loss: 0.25984789917663653
  batch 295 loss: 0.2597451705548723
  batch 296 loss: 0.2597923415234765
  batch 297 loss: 0.25972742735336124
  batch 298 loss: 0.25964270532131195
  batch 299 loss: 0.25974540447311656
  batch 300 loss: 0.25980967392524085
  batch 301 loss: 0.2599373103376243
  batch 302 loss: 0.259922931032465
  batch 303 loss: 0.2600218927112743
  batch 304 loss: 0.25999997026826205
  batch 305 loss: 0.2599559623686994
  batch 306 loss: 0.2601061608471902
  batch 307 loss: 0.26004402878230093
  batch 308 loss: 0.2601199461655183
  batch 309 loss: 0.26002599882462263
  batch 310 loss: 0.2600036310572778
  batch 311 loss: 0.2601362518558932
  batch 312 loss: 0.26030130923176426
  batch 313 loss: 0.26033096067821637
  batch 314 loss: 0.26027316182472143
  batch 315 loss: 0.26024924757934753
  batch 316 loss: 0.26018238902280605
  batch 317 loss: 0.26017888360407077
  batch 318 loss: 0.26020953719908335
  batch 319 loss: 0.26017954772729485
  batch 320 loss: 0.2601674780715257
  batch 321 loss: 0.2601841697904551
  batch 322 loss: 0.26018692141178973
  batch 323 loss: 0.2600922044308931
  batch 324 loss: 0.2598710810696637
  batch 325 loss: 0.2598149897502019
  batch 326 loss: 0.25992263442168206
  batch 327 loss: 0.26001482564010386
  batch 328 loss: 0.25983392856106524
  batch 329 loss: 0.2600102116634056
  batch 330 loss: 0.25989649756388233
  batch 331 loss: 0.2599302571705821
  batch 332 loss: 0.2598637255828783
  batch 333 loss: 0.2599255375944458
  batch 334 loss: 0.25987488432915623
  batch 335 loss: 0.25972976052939
  batch 336 loss: 0.2596734796340267
  batch 337 loss: 0.25946520653605815
  batch 338 loss: 0.2593952312007458
  batch 339 loss: 0.2592914519848022
  batch 340 loss: 0.2592249331667143
  batch 341 loss: 0.2591303269272326
  batch 342 loss: 0.25905183127574755
  batch 343 loss: 0.25903971719150987
  batch 344 loss: 0.25913997821856377
  batch 345 loss: 0.25920657796272334
  batch 346 loss: 0.2590917094766749
  batch 347 loss: 0.25910736272589274
  batch 348 loss: 0.2591404166372343
  batch 349 loss: 0.25903163572778676
  batch 350 loss: 0.25904869845935274
  batch 351 loss: 0.259046779059277
  batch 352 loss: 0.2590799985284155
  batch 353 loss: 0.2591089976070285
  batch 354 loss: 0.2591749030654713
  batch 355 loss: 0.2591255185469775
  batch 356 loss: 0.25911211624239266
  batch 357 loss: 0.2589858013207839
  batch 358 loss: 0.2590351298701164
  batch 359 loss: 0.259078176738824
  batch 360 loss: 0.2590930146475633
  batch 361 loss: 0.2590019897328189
  batch 362 loss: 0.2589169862108995
  batch 363 loss: 0.2589286415948027
  batch 364 loss: 0.25886882657369414
  batch 365 loss: 0.2588958613268317
  batch 366 loss: 0.2587985063755447
  batch 367 loss: 0.2587276176227211
  batch 368 loss: 0.25865917106199526
  batch 369 loss: 0.2586863495227767
  batch 370 loss: 0.25868074648283623
  batch 371 loss: 0.2586604727487358
  batch 372 loss: 0.25857830376073876
  batch 373 loss: 0.2585284018005187
  batch 374 loss: 0.2584251768608144
  batch 375 loss: 0.25832085851828257
  batch 376 loss: 0.25831357290611623
  batch 377 loss: 0.2582956951319065
  batch 378 loss: 0.25819852262262316
  batch 379 loss: 0.25836586189458743
  batch 380 loss: 0.2584408422833995
  batch 381 loss: 0.258382065871882
  batch 382 loss: 0.25827002970023927
  batch 383 loss: 0.25825721959221143
  batch 384 loss: 0.25824548040206236
  batch 385 loss: 0.2583100192732625
  batch 386 loss: 0.2581452755884803
  batch 387 loss: 0.2582134740759236
  batch 388 loss: 0.2583242961272751
  batch 389 loss: 0.25828034865519073
  batch 390 loss: 0.25826019843419395
  batch 391 loss: 0.2583270159066486
  batch 392 loss: 0.2583990588936271
  batch 393 loss: 0.2585071316202178
  batch 394 loss: 0.2585997355317101
  batch 395 loss: 0.2585951410516908
  batch 396 loss: 0.2587005638263442
  batch 397 loss: 0.2587155960218732
  batch 398 loss: 0.25871802888922957
  batch 399 loss: 0.2588178151681608
  batch 400 loss: 0.25879815109074117
  batch 401 loss: 0.25867413549203233
  batch 402 loss: 0.2586895965447473
  batch 403 loss: 0.25868859765103674
  batch 404 loss: 0.2587468412769313
  batch 405 loss: 0.25879315244562834
  batch 406 loss: 0.2589196583687378
  batch 407 loss: 0.2588894916034741
  batch 408 loss: 0.25902909102539223
  batch 409 loss: 0.2590746731294688
  batch 410 loss: 0.2591150173690261
  batch 411 loss: 0.25912387458802427
  batch 412 loss: 0.25912951645486565
  batch 413 loss: 0.2591973119223666
  batch 414 loss: 0.25917484352122183
  batch 415 loss: 0.25922380488321006
  batch 416 loss: 0.2592722018822454
  batch 417 loss: 0.25922809166016336
  batch 418 loss: 0.25917123956828597
  batch 419 loss: 0.2592287609144725
  batch 420 loss: 0.25926949289583023
  batch 421 loss: 0.25936613676100617
  batch 422 loss: 0.25962237632387625
  batch 423 loss: 0.2596100990794793
  batch 424 loss: 0.2596416281501077
  batch 425 loss: 0.25975522854748895
  batch 426 loss: 0.25977530284946515
  batch 427 loss: 0.25981904716346527
  batch 428 loss: 0.259768308044594
  batch 429 loss: 0.25980849637018216
  batch 430 loss: 0.25978639437708745
  batch 431 loss: 0.25995466043943594
  batch 432 loss: 0.2600401579919789
  batch 433 loss: 0.2600436680036109
  batch 434 loss: 0.2600892300841995
  batch 435 loss: 0.26004930159141276
  batch 436 loss: 0.26008708323907415
  batch 437 loss: 0.260111041145412
  batch 438 loss: 0.26020399532089494
  batch 439 loss: 0.2602227444257714
  batch 440 loss: 0.2602543644607067
  batch 441 loss: 0.26022139641027603
  batch 442 loss: 0.26021197771999094
  batch 443 loss: 0.26022332129709874
  batch 444 loss: 0.26012962234315573
  batch 445 loss: 0.260210741202483
  batch 446 loss: 0.2602003430905898
  batch 447 loss: 0.26014070172837916
  batch 448 loss: 0.2600772018610899
  batch 449 loss: 0.26008132083511565
  batch 450 loss: 0.2599970738093058
  batch 451 loss: 0.2599394588407022
  batch 452 loss: 0.2599195223466485
  batch 453 loss: 0.25991974629576897
  batch 454 loss: 0.25996083913920737
  batch 455 loss: 0.2599747060419439
  batch 456 loss: 0.25996555198441473
  batch 457 loss: 0.26001577747728943
  batch 458 loss: 0.2599913384196019
  batch 459 loss: 0.260043834419292
  batch 460 loss: 0.2601839414109354
  batch 461 loss: 0.2602229817182537
  batch 462 loss: 0.2602611117677771
  batch 463 loss: 0.26024844264108726
  batch 464 loss: 0.2602760038370716
  batch 465 loss: 0.2602214213020058
  batch 466 loss: 0.2601118931314976
  batch 467 loss: 0.26018844043927736
  batch 468 loss: 0.26022906970774007
  batch 469 loss: 0.26034467840499714
  batch 470 loss: 0.26034905479309406
  batch 471 loss: 0.26035224916828664
  batch 472 loss: 0.26022702013537036
LOSS train 0.26022702013537036 valid 0.24609310925006866
LOSS train 0.26022702013537036 valid 0.2373257651925087
LOSS train 0.26022702013537036 valid 0.24068949123223624
LOSS train 0.26022702013537036 valid 0.2337791658937931
LOSS train 0.26022702013537036 valid 0.22625038027763367
LOSS train 0.26022702013537036 valid 0.2328998645146688
LOSS train 0.26022702013537036 valid 0.23891540936061315
LOSS train 0.26022702013537036 valid 0.23550830222666264
LOSS train 0.26022702013537036 valid 0.23853296869330937
LOSS train 0.26022702013537036 valid 0.24123525768518447
LOSS train 0.26022702013537036 valid 0.23773388158191333
LOSS train 0.26022702013537036 valid 0.23912869145472845
LOSS train 0.26022702013537036 valid 0.23939596231167132
LOSS train 0.26022702013537036 valid 0.24010483494826726
LOSS train 0.26022702013537036 valid 0.23710237840811413
LOSS train 0.26022702013537036 valid 0.2370413150638342
LOSS train 0.26022702013537036 valid 0.23771704470410066
LOSS train 0.26022702013537036 valid 0.23886752294169533
LOSS train 0.26022702013537036 valid 0.24133959726283424
LOSS train 0.26022702013537036 valid 0.2407292902469635
LOSS train 0.26022702013537036 valid 0.2400527284258888
LOSS train 0.26022702013537036 valid 0.23859670893712479
LOSS train 0.26022702013537036 valid 0.23975980022679205
LOSS train 0.26022702013537036 valid 0.23919619930287203
LOSS train 0.26022702013537036 valid 0.23767963647842408
LOSS train 0.26022702013537036 valid 0.23871784943800706
LOSS train 0.26022702013537036 valid 0.23840548870740114
LOSS train 0.26022702013537036 valid 0.2389416614813464
LOSS train 0.26022702013537036 valid 0.23925036859923396
LOSS train 0.26022702013537036 valid 0.2412297303477923
LOSS train 0.26022702013537036 valid 0.24156013179209926
LOSS train 0.26022702013537036 valid 0.24125953810289502
LOSS train 0.26022702013537036 valid 0.24190360894708923
LOSS train 0.26022702013537036 valid 0.2416696864015916
LOSS train 0.26022702013537036 valid 0.24327569093023027
LOSS train 0.26022702013537036 valid 0.24300770544343525
LOSS train 0.26022702013537036 valid 0.24307619478251483
LOSS train 0.26022702013537036 valid 0.2435218244791031
LOSS train 0.26022702013537036 valid 0.24304201434820127
LOSS train 0.26022702013537036 valid 0.2435859613120556
LOSS train 0.26022702013537036 valid 0.24443703889846802
LOSS train 0.26022702013537036 valid 0.24407422684487842
LOSS train 0.26022702013537036 valid 0.24407208658928095
LOSS train 0.26022702013537036 valid 0.2446735365824266
LOSS train 0.26022702013537036 valid 0.24436401493019527
LOSS train 0.26022702013537036 valid 0.244745550920134
LOSS train 0.26022702013537036 valid 0.24478338214945286
LOSS train 0.26022702013537036 valid 0.24486817885190248
LOSS train 0.26022702013537036 valid 0.2454469796954369
LOSS train 0.26022702013537036 valid 0.24491673231124877
LOSS train 0.26022702013537036 valid 0.24536758018474952
LOSS train 0.26022702013537036 valid 0.24503327562258795
LOSS train 0.26022702013537036 valid 0.24529669644697658
LOSS train 0.26022702013537036 valid 0.24494557634547906
LOSS train 0.26022702013537036 valid 0.24491968452930452
LOSS train 0.26022702013537036 valid 0.2445100410176175
LOSS train 0.26022702013537036 valid 0.24423756254346748
LOSS train 0.26022702013537036 valid 0.24389958946869292
LOSS train 0.26022702013537036 valid 0.24430760589696593
LOSS train 0.26022702013537036 valid 0.24361199115713436
LOSS train 0.26022702013537036 valid 0.24370697849109524
LOSS train 0.26022702013537036 valid 0.24445387648959313
LOSS train 0.26022702013537036 valid 0.24494047557550763
LOSS train 0.26022702013537036 valid 0.24582852819003165
LOSS train 0.26022702013537036 valid 0.24593364940239834
LOSS train 0.26022702013537036 valid 0.24588565505815274
LOSS train 0.26022702013537036 valid 0.2454564042055785
LOSS train 0.26022702013537036 valid 0.24530749066787608
LOSS train 0.26022702013537036 valid 0.24445132587267004
LOSS train 0.26022702013537036 valid 0.24462137605462755
LOSS train 0.26022702013537036 valid 0.2446759781787093
LOSS train 0.26022702013537036 valid 0.24490335562990773
LOSS train 0.26022702013537036 valid 0.2448536124947953
LOSS train 0.26022702013537036 valid 0.24489091155496803
LOSS train 0.26022702013537036 valid 0.2449650424718857
LOSS train 0.26022702013537036 valid 0.24540221397029727
LOSS train 0.26022702013537036 valid 0.24567070545314193
LOSS train 0.26022702013537036 valid 0.24557527211996225
LOSS train 0.26022702013537036 valid 0.24542772864239126
LOSS train 0.26022702013537036 valid 0.2447744006291032
LOSS train 0.26022702013537036 valid 0.24397167600231406
LOSS train 0.26022702013537036 valid 0.24413188347002354
LOSS train 0.26022702013537036 valid 0.24398251901189966
LOSS train 0.26022702013537036 valid 0.24392704470526605
LOSS train 0.26022702013537036 valid 0.2435992470558952
LOSS train 0.26022702013537036 valid 0.24298466638077137
LOSS train 0.26022702013537036 valid 0.24309780097555841
LOSS train 0.26022702013537036 valid 0.24254863116551528
LOSS train 0.26022702013537036 valid 0.242747903037607
LOSS train 0.26022702013537036 valid 0.24308899955617058
LOSS train 0.26022702013537036 valid 0.2431656274494234
LOSS train 0.26022702013537036 valid 0.24320359430883243
LOSS train 0.26022702013537036 valid 0.24284068567137565
LOSS train 0.26022702013537036 valid 0.24312223002631614
LOSS train 0.26022702013537036 valid 0.24269496233839738
LOSS train 0.26022702013537036 valid 0.24290695569167534
LOSS train 0.26022702013537036 valid 0.24299529747864634
LOSS train 0.26022702013537036 valid 0.2429599129423803
LOSS train 0.26022702013537036 valid 0.2430903435957552
LOSS train 0.26022702013537036 valid 0.2433714148402214
LOSS train 0.26022702013537036 valid 0.24370804842155758
LOSS train 0.26022702013537036 valid 0.24381941293968873
LOSS train 0.26022702013537036 valid 0.24400759148366244
LOSS train 0.26022702013537036 valid 0.2438606548194702
LOSS train 0.26022702013537036 valid 0.24385483534563154
LOSS train 0.26022702013537036 valid 0.24406002869583526
LOSS train 0.26022702013537036 valid 0.24372097153529942
LOSS train 0.26022702013537036 valid 0.243877950641844
LOSS train 0.26022702013537036 valid 0.24435133015343902
LOSS train 0.26022702013537036 valid 0.2446201275695454
LOSS train 0.26022702013537036 valid 0.24425216878319647
LOSS train 0.26022702013537036 valid 0.24390542467257806
LOSS train 0.26022702013537036 valid 0.24388944416974498
LOSS train 0.26022702013537036 valid 0.24382965802623516
LOSS train 0.26022702013537036 valid 0.24371630717878756
LOSS train 0.26022702013537036 valid 0.24403938946538958
LOSS train 0.26022702013537036 valid 0.24431172000546741
LOSS train 0.26022702013537036 valid 0.2441424319552163
LOSS train 0.26022702013537036 valid 0.2439641187421414
LOSS train 0.26022702013537036 valid 0.24353228012720743
LOSS train 0.26022702013537036 valid 0.2433535762069639
LOSS train 0.26022702013537036 valid 0.2432770092712074
LOSS train 0.26022702013537036 valid 0.24330468693884408
LOSS train 0.26022702013537036 valid 0.24345197949197986
LOSS train 0.26022702013537036 valid 0.2433880636692047
LOSS train 0.26022702013537036 valid 0.24357643510614121
LOSS train 0.26022702013537036 valid 0.24339285915292155
LOSS train 0.26022702013537036 valid 0.24384186533279717
LOSS train 0.26022702013537036 valid 0.24380902911341468
LOSS train 0.26022702013537036 valid 0.24382250584088838
LOSS train 0.26022702013537036 valid 0.24378942942801324
LOSS train 0.26022702013537036 valid 0.2434575031652595
LOSS train 0.26022702013537036 valid 0.24327292500581957
LOSS train 0.26022702013537036 valid 0.24324425165332966
LOSS train 0.26022702013537036 valid 0.24320030013720195
LOSS train 0.26022702013537036 valid 0.24317197326351614
LOSS train 0.26022702013537036 valid 0.2430151011386927
LOSS train 0.26022702013537036 valid 0.242900634697382
LOSS train 0.26022702013537036 valid 0.2428127482211847
LOSS train 0.26022702013537036 valid 0.24294852763414382
LOSS train 0.26022702013537036 valid 0.24289661941798865
LOSS train 0.26022702013537036 valid 0.24307749766699024
LOSS train 0.26022702013537036 valid 0.2430529194278317
LOSS train 0.26022702013537036 valid 0.24310790085130268
LOSS train 0.26022702013537036 valid 0.24284717553648455
LOSS train 0.26022702013537036 valid 0.2430388022163143
LOSS train 0.26022702013537036 valid 0.242821045169214
LOSS train 0.26022702013537036 valid 0.2435850969238861
LOSS train 0.26022702013537036 valid 0.24358283153316318
LOSS train 0.26022702013537036 valid 0.24349607557058334
LOSS train 0.26022702013537036 valid 0.243398651085942
LOSS train 0.26022702013537036 valid 0.243300020008495
LOSS train 0.26022702013537036 valid 0.2433296878353443
LOSS train 0.26022702013537036 valid 0.24335736355611257
LOSS train 0.26022702013537036 valid 0.243451409859042
LOSS train 0.26022702013537036 valid 0.24380662989539978
LOSS train 0.26022702013537036 valid 0.24376480812859383
LOSS train 0.26022702013537036 valid 0.24376488392111623
LOSS train 0.26022702013537036 valid 0.24374188435902386
LOSS train 0.26022702013537036 valid 0.24341889806091785
LOSS train 0.26022702013537036 valid 0.24342789783240845
LOSS train 0.26022702013537036 valid 0.2434802528148816
LOSS train 0.26022702013537036 valid 0.2434465467381331
LOSS train 0.26022702013537036 valid 0.2433255330273291
LOSS train 0.26022702013537036 valid 0.24308659696217738
LOSS train 0.26022702013537036 valid 0.24326830895909343
LOSS train 0.26022702013537036 valid 0.2433252567481138
LOSS train 0.26022702013537036 valid 0.24324676793600833
LOSS train 0.26022702013537036 valid 0.24328114982892776
LOSS train 0.26022702013537036 valid 0.24351271303261027
LOSS train 0.26022702013537036 valid 0.24338006764127498
LOSS train 0.26022702013537036 valid 0.243162210944087
LOSS train 0.26022702013537036 valid 0.24322820002633022
LOSS train 0.26022702013537036 valid 0.2431406925121943
LOSS train 0.26022702013537036 valid 0.24310636852468764
LOSS train 0.26022702013537036 valid 0.24306733872402797
LOSS train 0.26022702013537036 valid 0.24311858365091227
LOSS train 0.26022702013537036 valid 0.2433616766768895
LOSS train 0.26022702013537036 valid 0.24330034602287762
LOSS train 0.26022702013537036 valid 0.24331217863493496
LOSS train 0.26022702013537036 valid 0.24352117722535002
LOSS train 0.26022702013537036 valid 0.24357681623199484
LOSS train 0.26022702013537036 valid 0.24338026735626284
LOSS train 0.26022702013537036 valid 0.2432875559543786
LOSS train 0.26022702013537036 valid 0.24311916481804202
LOSS train 0.26022702013537036 valid 0.2430785238262146
LOSS train 0.26022702013537036 valid 0.24291736110646456
LOSS train 0.26022702013537036 valid 0.24301168005517187
LOSS train 0.26022702013537036 valid 0.243159250449882
LOSS train 0.26022702013537036 valid 0.24325993970820778
LOSS train 0.26022702013537036 valid 0.24314957825925337
LOSS train 0.26022702013537036 valid 0.24311082740314305
LOSS train 0.26022702013537036 valid 0.24322087337007176
LOSS train 0.26022702013537036 valid 0.24304927440033747
LOSS train 0.26022702013537036 valid 0.24271925076460227
LOSS train 0.26022702013537036 valid 0.24291087489347069
LOSS train 0.26022702013537036 valid 0.2430190869091731
LOSS train 0.26022702013537036 valid 0.24302987302794601
LOSS train 0.26022702013537036 valid 0.24307137952378047
LOSS train 0.26022702013537036 valid 0.2428474970161915
LOSS train 0.26022702013537036 valid 0.24272507310506716
LOSS train 0.26022702013537036 valid 0.24258445100028916
LOSS train 0.26022702013537036 valid 0.24250925306616158
LOSS train 0.26022702013537036 valid 0.24240413293534635
LOSS train 0.26022702013537036 valid 0.24221830498881455
LOSS train 0.26022702013537036 valid 0.24221269460847078
LOSS train 0.26022702013537036 valid 0.24209571129457963
LOSS train 0.26022702013537036 valid 0.24206582729059917
LOSS train 0.26022702013537036 valid 0.24197126554245013
LOSS train 0.26022702013537036 valid 0.24202187125171934
LOSS train 0.26022702013537036 valid 0.24203583181469362
LOSS train 0.26022702013537036 valid 0.24192261175726945
LOSS train 0.26022702013537036 valid 0.241964298514693
LOSS train 0.26022702013537036 valid 0.24185698235703407
LOSS train 0.26022702013537036 valid 0.24172312395517218
LOSS train 0.26022702013537036 valid 0.2416146087149779
LOSS train 0.26022702013537036 valid 0.2416084052368243
LOSS train 0.26022702013537036 valid 0.2416157138730408
LOSS train 0.26022702013537036 valid 0.2417149607721529
LOSS train 0.26022702013537036 valid 0.24186047627167268
LOSS train 0.26022702013537036 valid 0.24195032556671903
LOSS train 0.26022702013537036 valid 0.241893081514685
LOSS train 0.26022702013537036 valid 0.24203501207411557
LOSS train 0.26022702013537036 valid 0.2421606776437589
LOSS train 0.26022702013537036 valid 0.24221437732378642
LOSS train 0.26022702013537036 valid 0.24232436373697974
LOSS train 0.26022702013537036 valid 0.2425358890174244
LOSS train 0.26022702013537036 valid 0.24259290974913983
LOSS train 0.26022702013537036 valid 0.24260802039933518
LOSS train 0.26022702013537036 valid 0.24268874163212983
LOSS train 0.26022702013537036 valid 0.24266815391969887
LOSS train 0.26022702013537036 valid 0.24272289666636238
LOSS train 0.26022702013537036 valid 0.24258347770175198
LOSS train 0.26022702013537036 valid 0.24265696872503328
LOSS train 0.26022702013537036 valid 0.24270987066816777
LOSS train 0.26022702013537036 valid 0.242642109676943
LOSS train 0.26022702013537036 valid 0.24266394413221737
LOSS train 0.26022702013537036 valid 0.24270017213430725
LOSS train 0.26022702013537036 valid 0.24261353941143307
LOSS train 0.26022702013537036 valid 0.24253029661873976
LOSS train 0.26022702013537036 valid 0.2426719858438642
LOSS train 0.26022702013537036 valid 0.24240044301206415
LOSS train 0.26022702013537036 valid 0.24253881419146503
LOSS train 0.26022702013537036 valid 0.2427397844244222
LOSS train 0.26022702013537036 valid 0.24285715733255658
LOSS train 0.26022702013537036 valid 0.24278950315665423
LOSS train 0.26022702013537036 valid 0.24276357056640904
LOSS train 0.26022702013537036 valid 0.24272539322414705
LOSS train 0.26022702013537036 valid 0.2426941271527225
LOSS train 0.26022702013537036 valid 0.2428610862493515
LOSS train 0.26022702013537036 valid 0.2428197633341489
LOSS train 0.26022702013537036 valid 0.24287955681719478
LOSS train 0.26022702013537036 valid 0.24272455970053616
LOSS train 0.26022702013537036 valid 0.24271847313548636
LOSS train 0.26022702013537036 valid 0.24280548674218794
LOSS train 0.26022702013537036 valid 0.24286333663621917
LOSS train 0.26022702013537036 valid 0.24270711131828768
LOSS train 0.26022702013537036 valid 0.24265101269003034
LOSS train 0.26022702013537036 valid 0.24261735730649883
LOSS train 0.26022702013537036 valid 0.2426026612520218
LOSS train 0.26022702013537036 valid 0.2426463345001484
LOSS train 0.26022702013537036 valid 0.24268992813943907
LOSS train 0.26022702013537036 valid 0.24263350535254968
LOSS train 0.26022702013537036 valid 0.24265292230429072
LOSS train 0.26022702013537036 valid 0.24256976584218584
LOSS train 0.26022702013537036 valid 0.24253736615629123
LOSS train 0.26022702013537036 valid 0.24263255973433734
LOSS train 0.26022702013537036 valid 0.24280623035199606
LOSS train 0.26022702013537036 valid 0.24285469567022358
LOSS train 0.26022702013537036 valid 0.24278613124732618
LOSS train 0.26022702013537036 valid 0.2428262510312879
LOSS train 0.26022702013537036 valid 0.24305387164520867
LOSS train 0.26022702013537036 valid 0.24301847165975815
LOSS train 0.26022702013537036 valid 0.24302281140193452
LOSS train 0.26022702013537036 valid 0.2429652917926962
LOSS train 0.26022702013537036 valid 0.24287232637837314
LOSS train 0.26022702013537036 valid 0.24270190605187675
LOSS train 0.26022702013537036 valid 0.24249490323684197
LOSS train 0.26022702013537036 valid 0.24254051246096156
LOSS train 0.26022702013537036 valid 0.2424949502306325
LOSS train 0.26022702013537036 valid 0.24243840532794966
LOSS train 0.26022702013537036 valid 0.24226850704521152
LOSS train 0.26022702013537036 valid 0.24227929210073113
LOSS train 0.26022702013537036 valid 0.2423532829737999
LOSS train 0.26022702013537036 valid 0.2423685128228706
LOSS train 0.26022702013537036 valid 0.24224775914962476
LOSS train 0.26022702013537036 valid 0.24214901440234965
LOSS train 0.26022702013537036 valid 0.24221519411851963
LOSS train 0.26022702013537036 valid 0.24221751557914442
LOSS train 0.26022702013537036 valid 0.2422501770586803
LOSS train 0.26022702013537036 valid 0.24230119545025514
LOSS train 0.26022702013537036 valid 0.24231114299738243
LOSS train 0.26022702013537036 valid 0.24234923842417094
LOSS train 0.26022702013537036 valid 0.24245180047693707
LOSS train 0.26022702013537036 valid 0.24252484228651403
LOSS train 0.26022702013537036 valid 0.24249983629262126
LOSS train 0.26022702013537036 valid 0.24250360084101807
LOSS train 0.26022702013537036 valid 0.24253308368009208
LOSS train 0.26022702013537036 valid 0.24259370012227507
LOSS train 0.26022702013537036 valid 0.24254448955257735
LOSS train 0.26022702013537036 valid 0.2426383758205116
LOSS train 0.26022702013537036 valid 0.24261228853700967
LOSS train 0.26022702013537036 valid 0.24270101757136114
LOSS train 0.26022702013537036 valid 0.24269835636215775
LOSS train 0.26022702013537036 valid 0.24258990874055955
LOSS train 0.26022702013537036 valid 0.24250025005122416
LOSS train 0.26022702013537036 valid 0.2424441152267425
LOSS train 0.26022702013537036 valid 0.2423438893800432
LOSS train 0.26022702013537036 valid 0.24248994356413103
LOSS train 0.26022702013537036 valid 0.2424517451755462
LOSS train 0.26022702013537036 valid 0.24235758990336845
LOSS train 0.26022702013537036 valid 0.242383337364747
LOSS train 0.26022702013537036 valid 0.24241146873742248
LOSS train 0.26022702013537036 valid 0.24247983363783285
LOSS train 0.26022702013537036 valid 0.24249489662193116
LOSS train 0.26022702013537036 valid 0.2424124114781241
LOSS train 0.26022702013537036 valid 0.24243940187365476
LOSS train 0.26022702013537036 valid 0.2425033303457986
LOSS train 0.26022702013537036 valid 0.24255100644681146
LOSS train 0.26022702013537036 valid 0.24249361935071648
LOSS train 0.26022702013537036 valid 0.24257836883127504
LOSS train 0.26022702013537036 valid 0.24251353643510654
LOSS train 0.26022702013537036 valid 0.24247699772979453
LOSS train 0.26022702013537036 valid 0.24248983802986734
LOSS train 0.26022702013537036 valid 0.24240138989228469
LOSS train 0.26022702013537036 valid 0.24256550086422202
LOSS train 0.26022702013537036 valid 0.24264744853754655
LOSS train 0.26022702013537036 valid 0.24262991006963136
LOSS train 0.26022702013537036 valid 0.24276201001235417
LOSS train 0.26022702013537036 valid 0.24276468893795303
LOSS train 0.26022702013537036 valid 0.24272764336846747
LOSS train 0.26022702013537036 valid 0.2426369337881186
LOSS train 0.26022702013537036 valid 0.24261379389612525
LOSS train 0.26022702013537036 valid 0.24273741695873752
LOSS train 0.26022702013537036 valid 0.24275288359442754
LOSS train 0.26022702013537036 valid 0.24276631201306978
LOSS train 0.26022702013537036 valid 0.24263994444371684
LOSS train 0.26022702013537036 valid 0.2425955154839352
LOSS train 0.26022702013537036 valid 0.24261375365004076
LOSS train 0.26022702013537036 valid 0.24266800389570348
LOSS train 0.26022702013537036 valid 0.24258003303032816
LOSS train 0.26022702013537036 valid 0.2425624962177193
LOSS train 0.26022702013537036 valid 0.24259045255253683
LOSS train 0.26022702013537036 valid 0.2427562862028216
LOSS train 0.26022702013537036 valid 0.24276669310486834
LOSS train 0.26022702013537036 valid 0.24266995928880108
LOSS train 0.26022702013537036 valid 0.2426006462886629
LOSS train 0.26022702013537036 valid 0.24255353083898282
LOSS train 0.26022702013537036 valid 0.24262851365657795
LOSS train 0.26022702013537036 valid 0.2424937897494861
LOSS train 0.26022702013537036 valid 0.24242727679234963
LOSS train 0.26022702013537036 valid 0.2423274437473579
LOSS train 0.26022702013537036 valid 0.24238309010905518
LOSS train 0.26022702013537036 valid 0.24244814816504548
LOSS train 0.26022702013537036 valid 0.24254767575734099
LOSS train 0.26022702013537036 valid 0.24259786039925693
LOSS train 0.26022702013537036 valid 0.24254967042711936
LOSS train 0.26022702013537036 valid 0.24247001839916132
LOSS train 0.26022702013537036 valid 0.24245232364427413
LOSS train 0.26022702013537036 valid 0.24233991586499745
LOSS train 0.26022702013537036 valid 0.24224373273076774
LOSS train 0.26022702013537036 valid 0.24233230499766809
LOSS train 0.26022702013537036 valid 0.2421760244638795
LOSS train 0.26022702013537036 valid 0.24211688650833382
LOSS train 0.26022702013537036 valid 0.2420860223982432
LOSS train 0.26022702013537036 valid 0.24202293467000535
LOSS train 0.26022702013537036 valid 0.24192651152123548
LOSS train 0.26022702013537036 valid 0.24188686526663924
LOSS train 0.26022702013537036 valid 0.2419323040864009
EPOCH 11:
  batch 1 loss: 0.2431235909461975
  batch 2 loss: 0.23719283938407898
  batch 3 loss: 0.2398246427377065
  batch 4 loss: 0.2412741258740425
  batch 5 loss: 0.2452946901321411
  batch 6 loss: 0.24266590674718222
  batch 7 loss: 0.24561100346701487
  batch 8 loss: 0.24839850887656212
  batch 9 loss: 0.24903720948431227
  batch 10 loss: 0.2508216291666031
  batch 11 loss: 0.2525883533737876
  batch 12 loss: 0.2503533524771531
  batch 13 loss: 0.2492255293405973
  batch 14 loss: 0.24979946868760244
  batch 15 loss: 0.25130834579467776
  batch 16 loss: 0.2504165666177869
  batch 17 loss: 0.24869817845961628
  batch 18 loss: 0.2495085663265652
  batch 19 loss: 0.24860919619861402
  batch 20 loss: 0.24767818301916122
  batch 21 loss: 0.24731450421469553
  batch 22 loss: 0.24775318665937943
  batch 23 loss: 0.24699317242788232
  batch 24 loss: 0.24623573633531728
  batch 25 loss: 0.24836306869983674
  batch 26 loss: 0.24727198424247596
  batch 27 loss: 0.2473660326666302
  batch 28 loss: 0.24591033001031196
  batch 29 loss: 0.24676322166262002
  batch 30 loss: 0.2468327537178993
  batch 31 loss: 0.2484808790106927
  batch 32 loss: 0.24814238445833325
  batch 33 loss: 0.2482636458042896
  batch 34 loss: 0.24887309574029026
  batch 35 loss: 0.2489211929695947
  batch 36 loss: 0.248750867942969
  batch 37 loss: 0.24890176589424545
  batch 38 loss: 0.2499071916467265
  batch 39 loss: 0.24937061239511538
  batch 40 loss: 0.24927774481475354
  batch 41 loss: 0.24910901268807853
  batch 42 loss: 0.24948046569313323
  batch 43 loss: 0.24942063384277877
  batch 44 loss: 0.24967048317193985
  batch 45 loss: 0.2499053630563948
  batch 46 loss: 0.24994088450203772
  batch 47 loss: 0.25062900464585486
  batch 48 loss: 0.2501097097992897
  batch 49 loss: 0.2502813819719821
  batch 50 loss: 0.2503497952222824
  batch 51 loss: 0.2500154037101596
  batch 52 loss: 0.25052631073273146
  batch 53 loss: 0.24977498993558703
  batch 54 loss: 0.2500158137193433
  batch 55 loss: 0.249310347979719
  batch 56 loss: 0.24891227909496852
  batch 57 loss: 0.24806733586286245
  batch 58 loss: 0.24831918113190551
  batch 59 loss: 0.24842392009193615
  batch 60 loss: 0.24820570573210715
  batch 61 loss: 0.24830810090557473
  batch 62 loss: 0.2481857824229425
  batch 63 loss: 0.24724674745211525
  batch 64 loss: 0.24795022094622254
  batch 65 loss: 0.2476709755567404
  batch 66 loss: 0.24735006893222983
  batch 67 loss: 0.2477127080088231
  batch 68 loss: 0.24809110668652198
  batch 69 loss: 0.24808056933292444
  batch 70 loss: 0.24823911445481436
  batch 71 loss: 0.24796862585443846
  batch 72 loss: 0.24825515184137556
  batch 73 loss: 0.24813312924888037
  batch 74 loss: 0.2483149901837916
  batch 75 loss: 0.24785793999830882
  batch 76 loss: 0.2481928124631706
  batch 77 loss: 0.24791642771912858
  batch 78 loss: 0.24773603295668578
  batch 79 loss: 0.24807024115248572
  batch 80 loss: 0.24786166623234748
  batch 81 loss: 0.24809084115204988
  batch 82 loss: 0.24853081855832077
  batch 83 loss: 0.24842121317444077
  batch 84 loss: 0.2483998623987039
  batch 85 loss: 0.24821971654891967
  batch 86 loss: 0.24854927354080733
  batch 87 loss: 0.24842132199769731
  batch 88 loss: 0.2481180808760903
  batch 89 loss: 0.24824626894479387
  batch 90 loss: 0.24813197602828344
  batch 91 loss: 0.24847719941165422
  batch 92 loss: 0.2483863448319228
  batch 93 loss: 0.24818266014898976
  batch 94 loss: 0.24837765034208906
  batch 95 loss: 0.24824466485726207
  batch 96 loss: 0.2484339503571391
  batch 97 loss: 0.2487813876461737
  batch 98 loss: 0.24909099389095696
  batch 99 loss: 0.24932477420026605
  batch 100 loss: 0.24942256957292558
  batch 101 loss: 0.24934000986637456
  batch 102 loss: 0.24983450711942187
  batch 103 loss: 0.25030051388786834
  batch 104 loss: 0.25038222481424993
  batch 105 loss: 0.2503037773427509
  batch 106 loss: 0.25096872983113777
  batch 107 loss: 0.2504616032972514
  batch 108 loss: 0.25032798269832574
  batch 109 loss: 0.2502697857968304
  batch 110 loss: 0.24991898374124008
  batch 111 loss: 0.249836961159835
  batch 112 loss: 0.24958860368600913
  batch 113 loss: 0.2498606071535465
  batch 114 loss: 0.2501888429386574
  batch 115 loss: 0.2504502962464872
  batch 116 loss: 0.2506786556593303
  batch 117 loss: 0.2509309937301864
  batch 118 loss: 0.25091460321919395
  batch 119 loss: 0.2512420789033425
  batch 120 loss: 0.2513569874068101
  batch 121 loss: 0.2512397128195802
  batch 122 loss: 0.25116504289087704
  batch 123 loss: 0.25082875957818535
  batch 124 loss: 0.25105261550314967
  batch 125 loss: 0.25089249408245085
  batch 126 loss: 0.25079778558205046
  batch 127 loss: 0.25129194839263524
  batch 128 loss: 0.25143031461630017
  batch 129 loss: 0.251592225460119
  batch 130 loss: 0.2519032163115648
  batch 131 loss: 0.252194462955453
  batch 132 loss: 0.2522686030151266
  batch 133 loss: 0.2524809526993816
  batch 134 loss: 0.25270727043276403
  batch 135 loss: 0.2528429948621326
  batch 136 loss: 0.2529508069376735
  batch 137 loss: 0.25275011143110093
  batch 138 loss: 0.25290197144815885
  batch 139 loss: 0.25332865322665343
  batch 140 loss: 0.2534001549439771
  batch 141 loss: 0.25358621860649566
  batch 142 loss: 0.2535956910168621
  batch 143 loss: 0.2534726679950327
  batch 144 loss: 0.2536335489195254
  batch 145 loss: 0.25367943140967136
  batch 146 loss: 0.2535885647959905
  batch 147 loss: 0.25386155847789477
  batch 148 loss: 0.2540311783149436
  batch 149 loss: 0.2541565501049861
  batch 150 loss: 0.2540842890739441
  batch 151 loss: 0.2540216153820619
  batch 152 loss: 0.25417438286699745
  batch 153 loss: 0.253904223149898
  batch 154 loss: 0.25410379904818226
  batch 155 loss: 0.2541502140222057
  batch 156 loss: 0.2541789719118522
  batch 157 loss: 0.2545150925589215
  batch 158 loss: 0.2546393481802337
  batch 159 loss: 0.2546366347269442
  batch 160 loss: 0.254549218993634
  batch 161 loss: 0.25473124612562403
  batch 162 loss: 0.25489797608719933
  batch 163 loss: 0.25487501352476927
  batch 164 loss: 0.2550228689683647
  batch 165 loss: 0.2549398955973712
  batch 166 loss: 0.2550135711769024
  batch 167 loss: 0.25495231829717485
  batch 168 loss: 0.25499433864440235
  batch 169 loss: 0.2547761874615088
  batch 170 loss: 0.25453464818351407
  batch 171 loss: 0.2546742465238125
  batch 172 loss: 0.2545255186945893
  batch 173 loss: 0.25462090624550177
  batch 174 loss: 0.254839152097702
  batch 175 loss: 0.2548485108784267
  batch 176 loss: 0.25474917592311447
  batch 177 loss: 0.25484531553788375
  batch 178 loss: 0.25499225843153644
  batch 179 loss: 0.25523401447181593
  batch 180 loss: 0.25520670819613667
  batch 181 loss: 0.2551836321202431
  batch 182 loss: 0.25524905708792445
  batch 183 loss: 0.25498640007985746
  batch 184 loss: 0.2548214418246694
  batch 185 loss: 0.2548671107839894
  batch 186 loss: 0.2548556782064899
  batch 187 loss: 0.2547857119915957
  batch 188 loss: 0.254614660714535
  batch 189 loss: 0.25458372687851943
  batch 190 loss: 0.25472113623430853
  batch 191 loss: 0.2547368763785088
  batch 192 loss: 0.2548966155542682
  batch 193 loss: 0.2550766825521548
  batch 194 loss: 0.25529915808709625
  batch 195 loss: 0.25540288190046945
  batch 196 loss: 0.2553952895104885
  batch 197 loss: 0.255514329535707
  batch 198 loss: 0.2558042025475791
  batch 199 loss: 0.25589828941989784
  batch 200 loss: 0.25599922783672807
  batch 201 loss: 0.25606335981271755
  batch 202 loss: 0.2560513116198011
  batch 203 loss: 0.2559623100193851
  batch 204 loss: 0.25578506831444947
  batch 205 loss: 0.2560993369032697
  batch 206 loss: 0.2560446197257459
  batch 207 loss: 0.2560477520244709
  batch 208 loss: 0.2560733254425801
  batch 209 loss: 0.25589666590451055
  batch 210 loss: 0.25603257438966204
  batch 211 loss: 0.2559268401838592
  batch 212 loss: 0.25595094031601584
  batch 213 loss: 0.25602282681655436
  batch 214 loss: 0.25609961005850373
  batch 215 loss: 0.2558847045482591
  batch 216 loss: 0.2558531403127644
  batch 217 loss: 0.2558934782644571
  batch 218 loss: 0.2558793080372548
  batch 219 loss: 0.2558955930547627
  batch 220 loss: 0.2560410729186101
  batch 221 loss: 0.25614580437878137
  batch 222 loss: 0.25630399000805776
  batch 223 loss: 0.2563318729801563
  batch 224 loss: 0.25624417373910546
  batch 225 loss: 0.2562489267852571
  batch 226 loss: 0.25633738493233654
  batch 227 loss: 0.25615449105327875
  batch 228 loss: 0.2560111499930683
  batch 229 loss: 0.25591052645679124
  batch 230 loss: 0.2559347704700802
  batch 231 loss: 0.25585261051788993
  batch 232 loss: 0.2557081427296688
  batch 233 loss: 0.25571472309689663
  batch 234 loss: 0.2557696837645311
  batch 235 loss: 0.2558390191260804
  batch 236 loss: 0.2557329408571882
  batch 237 loss: 0.25586084292156286
  batch 238 loss: 0.25587711533328067
  batch 239 loss: 0.25584879380389736
  batch 240 loss: 0.25598684176802633
  batch 241 loss: 0.25615685906153
  batch 242 loss: 0.2560986281672785
  batch 243 loss: 0.2561039572635305
  batch 244 loss: 0.25615501892371256
  batch 245 loss: 0.2560760935958551
  batch 246 loss: 0.2559726048654657
  batch 247 loss: 0.25601669090238177
  batch 248 loss: 0.2561171855777502
  batch 249 loss: 0.25609125256777765
  batch 250 loss: 0.256024657189846
  batch 251 loss: 0.2561183958295807
  batch 252 loss: 0.2560352480837277
  batch 253 loss: 0.2559329731308896
  batch 254 loss: 0.2560014758171059
  batch 255 loss: 0.2560356694693659
  batch 256 loss: 0.2560263851773925
  batch 257 loss: 0.2560690488912716
  batch 258 loss: 0.2560936067571012
  batch 259 loss: 0.2561776095490658
  batch 260 loss: 0.25626225637701844
  batch 261 loss: 0.25618590323175966
  batch 262 loss: 0.2560798773215017
  batch 263 loss: 0.2561480939501592
  batch 264 loss: 0.2559229832594142
  batch 265 loss: 0.2558355531602536
  batch 266 loss: 0.2558705793287521
  batch 267 loss: 0.25591363196962336
  batch 268 loss: 0.255878014351005
  batch 269 loss: 0.2558182100713475
  batch 270 loss: 0.25591541920547134
  batch 271 loss: 0.25593576258618894
  batch 272 loss: 0.2559052483343026
  batch 273 loss: 0.25590037524481835
  batch 274 loss: 0.25607380956193826
  batch 275 loss: 0.2562606792016463
  batch 276 loss: 0.25629693194144015
  batch 277 loss: 0.2563395283927986
  batch 278 loss: 0.2563313677799787
  batch 279 loss: 0.25641637079177365
  batch 280 loss: 0.2563643302236285
  batch 281 loss: 0.2562948809505782
  batch 282 loss: 0.2563693721560722
  batch 283 loss: 0.25645539654227956
  batch 284 loss: 0.2563810320702237
  batch 285 loss: 0.25650395965367034
  batch 286 loss: 0.25648448960139203
  batch 287 loss: 0.2564883530659127
  batch 288 loss: 0.2562978688834442
  batch 289 loss: 0.2564280797247243
  batch 290 loss: 0.2563112309266781
  batch 291 loss: 0.2563540397845593
  batch 292 loss: 0.2564853289968347
  batch 293 loss: 0.2565426052430384
  batch 294 loss: 0.2564175122347819
  batch 295 loss: 0.25631713084245134
  batch 296 loss: 0.2564325611031539
  batch 297 loss: 0.25636726838571056
  batch 298 loss: 0.25627651265603585
  batch 299 loss: 0.25632846230846584
  batch 300 loss: 0.25636406257748606
  batch 301 loss: 0.25644819963215987
  batch 302 loss: 0.2564183907299642
  batch 303 loss: 0.25651306046707795
  batch 304 loss: 0.25648973725344004
  batch 305 loss: 0.25642025060341006
  batch 306 loss: 0.25656569403371
  batch 307 loss: 0.25654762546480286
  batch 308 loss: 0.2566177700827648
  batch 309 loss: 0.2565683188658316
  batch 310 loss: 0.2565312969107782
  batch 311 loss: 0.2566226610608423
  batch 312 loss: 0.2568231132359077
  batch 313 loss: 0.25690951000768153
  batch 314 loss: 0.2569071168352844
  batch 315 loss: 0.25687476032310064
  batch 316 loss: 0.25679335587575464
  batch 317 loss: 0.25689168800878975
  batch 318 loss: 0.256911131992655
  batch 319 loss: 0.25689404245267466
  batch 320 loss: 0.2569510757457465
  batch 321 loss: 0.2569938149314803
  batch 322 loss: 0.25699461584690936
  batch 323 loss: 0.25693066141369175
  batch 324 loss: 0.256785467956905
  batch 325 loss: 0.2566697352207624
  batch 326 loss: 0.2567135931615449
  batch 327 loss: 0.2569232640918971
  batch 328 loss: 0.25682784771410433
  batch 329 loss: 0.25694372072408267
  batch 330 loss: 0.25690494057807056
  batch 331 loss: 0.25692789517680686
  batch 332 loss: 0.25688212194356574
  batch 333 loss: 0.25700179395732936
  batch 334 loss: 0.25701799337378517
  batch 335 loss: 0.2569290357739178
  batch 336 loss: 0.256921742909721
  batch 337 loss: 0.2567676483081427
  batch 338 loss: 0.2567521489319011
  batch 339 loss: 0.25667891807436594
  batch 340 loss: 0.25666245349189815
  batch 341 loss: 0.256616288484716
  batch 342 loss: 0.2565717126368082
  batch 343 loss: 0.2565984970105285
  batch 344 loss: 0.25672176939456964
  batch 345 loss: 0.25684272456860197
  batch 346 loss: 0.25672217759955135
  batch 347 loss: 0.2567835105341518
  batch 348 loss: 0.2568660820780814
  batch 349 loss: 0.25680237906026976
  batch 350 loss: 0.2568401053122112
  batch 351 loss: 0.2568452206262496
  batch 352 loss: 0.2568923805586316
  batch 353 loss: 0.25698040159836033
  batch 354 loss: 0.2571057827122467
  batch 355 loss: 0.2570953321289009
  batch 356 loss: 0.25708312624960805
  batch 357 loss: 0.25701521165898533
  batch 358 loss: 0.2570965942723791
  batch 359 loss: 0.2571204763767115
  batch 360 loss: 0.25710521042346957
  batch 361 loss: 0.2570190446198482
  batch 362 loss: 0.257044092314678
  batch 363 loss: 0.25695901764325857
  batch 364 loss: 0.25691528262181595
  batch 365 loss: 0.2569376042852663
  batch 366 loss: 0.2568522042752615
  batch 367 loss: 0.2568151924038453
  batch 368 loss: 0.2567824946878397
  batch 369 loss: 0.2567889849910245
  batch 370 loss: 0.2567477482798937
  batch 371 loss: 0.2567795108190444
  batch 372 loss: 0.2567344618740902
  batch 373 loss: 0.25661918920422366
  batch 374 loss: 0.2564797975561198
  batch 375 loss: 0.25638544229666393
  batch 376 loss: 0.2563533063660911
  batch 377 loss: 0.2563528154905026
  batch 378 loss: 0.25629715215434473
  batch 379 loss: 0.25638154962918375
  batch 380 loss: 0.25644565989312373
  batch 381 loss: 0.2563791751783351
  batch 382 loss: 0.2562595554359296
  batch 383 loss: 0.2561636041879031
  batch 384 loss: 0.2561546315749486
  batch 385 loss: 0.256228114103342
  batch 386 loss: 0.2561023709378712
  batch 387 loss: 0.2561295927340978
  batch 388 loss: 0.25622477052138026
  batch 389 loss: 0.25614356653702597
  batch 390 loss: 0.25611873341676517
  batch 391 loss: 0.25616012307841457
  batch 392 loss: 0.25623201096088305
  batch 393 loss: 0.2564736271405038
  batch 394 loss: 0.2565816741833832
  batch 395 loss: 0.2565056178765961
  batch 396 loss: 0.2566236572509462
  batch 397 loss: 0.2566820200279318
  batch 398 loss: 0.25668796301636865
  batch 399 loss: 0.25675431995612935
  batch 400 loss: 0.256786219291389
  batch 401 loss: 0.25674078700845676
  batch 402 loss: 0.25675037992534355
  batch 403 loss: 0.25674021399346536
  batch 404 loss: 0.25680189181377394
  batch 405 loss: 0.2568585955066445
  batch 406 loss: 0.2569885331715269
  batch 407 loss: 0.2569352966122311
  batch 408 loss: 0.2570773940168175
  batch 409 loss: 0.25713780158014926
  batch 410 loss: 0.2571955690296685
  batch 411 loss: 0.25716967307883165
  batch 412 loss: 0.2571651669020213
  batch 413 loss: 0.2572675682514112
  batch 414 loss: 0.2573010184698635
  batch 415 loss: 0.25728780006069735
  batch 416 loss: 0.2573410853958474
  batch 417 loss: 0.2572570373471692
  batch 418 loss: 0.25722077050420084
  batch 419 loss: 0.2571678657022467
  batch 420 loss: 0.25722178401691576
  batch 421 loss: 0.2571256680330018
  batch 422 loss: 0.25733541552489403
  batch 423 loss: 0.25729700232510305
  batch 424 loss: 0.2572937603547888
  batch 425 loss: 0.25731698036193845
  batch 426 loss: 0.2572594831037409
  batch 427 loss: 0.25731555943047973
  batch 428 loss: 0.2573037031585368
  batch 429 loss: 0.2573014616896778
  batch 430 loss: 0.25725746334985244
  batch 431 loss: 0.2574377343438894
  batch 432 loss: 0.2574877431961121
  batch 433 loss: 0.2574369883633651
  batch 434 loss: 0.2574866929430566
  batch 435 loss: 0.25738482119023115
  batch 436 loss: 0.25742108791793156
  batch 437 loss: 0.2574986955939496
  batch 438 loss: 0.2575819252152421
  batch 439 loss: 0.2576057682140542
  batch 440 loss: 0.2576365900310603
  batch 441 loss: 0.25759919910203843
  batch 442 loss: 0.257570884750979
  batch 443 loss: 0.25757260341407484
  batch 444 loss: 0.2574701907599832
  batch 445 loss: 0.2575651298413116
  batch 446 loss: 0.2575594374884939
  batch 447 loss: 0.2574914403809797
  batch 448 loss: 0.25745383014769424
  batch 449 loss: 0.2575251775687946
  batch 450 loss: 0.2574536864293946
  batch 451 loss: 0.25739632090945996
  batch 452 loss: 0.25736347048551633
  batch 453 loss: 0.2574231718352299
  batch 454 loss: 0.25745121251775305
  batch 455 loss: 0.25746530978889254
  batch 456 loss: 0.25745638391297115
  batch 457 loss: 0.2575213233216847
  batch 458 loss: 0.2575265457080962
  batch 459 loss: 0.2576410872029843
  batch 460 loss: 0.2577628477436045
  batch 461 loss: 0.25779099335768735
  batch 462 loss: 0.25782898078104116
  batch 463 loss: 0.25784483851289647
  batch 464 loss: 0.2578814234905716
  batch 465 loss: 0.25783674245880495
  batch 466 loss: 0.2577620234676185
  batch 467 loss: 0.25781308695259136
  batch 468 loss: 0.2578283618084895
  batch 469 loss: 0.2579604196967855
  batch 470 loss: 0.2579667889374368
  batch 471 loss: 0.2580130423266923
  batch 472 loss: 0.25788301822997756
LOSS train 0.25788301822997756 valid 0.2469213306903839
LOSS train 0.25788301822997756 valid 0.23168569058179855
LOSS train 0.25788301822997756 valid 0.2370548496643702
LOSS train 0.25788301822997756 valid 0.2289615198969841
LOSS train 0.25788301822997756 valid 0.22113630175590515
LOSS train 0.25788301822997756 valid 0.2272486388683319
LOSS train 0.25788301822997756 valid 0.23409179278782435
LOSS train 0.25788301822997756 valid 0.23000656254589558
LOSS train 0.25788301822997756 valid 0.2332160489426719
LOSS train 0.25788301822997756 valid 0.23503813594579698
LOSS train 0.25788301822997756 valid 0.23163734105500308
LOSS train 0.25788301822997756 valid 0.2329993136227131
LOSS train 0.25788301822997756 valid 0.23295677739840287
LOSS train 0.25788301822997756 valid 0.2337021284869739
LOSS train 0.25788301822997756 valid 0.23094203372796376
LOSS train 0.25788301822997756 valid 0.2309160279110074
LOSS train 0.25788301822997756 valid 0.23187019193873687
LOSS train 0.25788301822997756 valid 0.23299694226847756
LOSS train 0.25788301822997756 valid 0.2355910665110538
LOSS train 0.25788301822997756 valid 0.23489618077874183
LOSS train 0.25788301822997756 valid 0.23416254704906828
LOSS train 0.25788301822997756 valid 0.23267355493523859
LOSS train 0.25788301822997756 valid 0.23365217641643857
LOSS train 0.25788301822997756 valid 0.2330562217781941
LOSS train 0.25788301822997756 valid 0.23163955569267272
LOSS train 0.25788301822997756 valid 0.23291378754835862
LOSS train 0.25788301822997756 valid 0.2321456300991553
LOSS train 0.25788301822997756 valid 0.23238556406327657
LOSS train 0.25788301822997756 valid 0.23247904870016822
LOSS train 0.25788301822997756 valid 0.23431861648956934
LOSS train 0.25788301822997756 valid 0.23489446457355254
LOSS train 0.25788301822997756 valid 0.23460367554798722
LOSS train 0.25788301822997756 valid 0.2353872751647776
LOSS train 0.25788301822997756 valid 0.23515698050751405
LOSS train 0.25788301822997756 valid 0.2364616274833679
LOSS train 0.25788301822997756 valid 0.23639195412397385
LOSS train 0.25788301822997756 valid 0.23657331877463572
LOSS train 0.25788301822997756 valid 0.23690988907688543
LOSS train 0.25788301822997756 valid 0.23641740282376608
LOSS train 0.25788301822997756 valid 0.2369471736252308
LOSS train 0.25788301822997756 valid 0.23777336827138576
LOSS train 0.25788301822997756 valid 0.23735841860373816
LOSS train 0.25788301822997756 valid 0.2373502264882243
LOSS train 0.25788301822997756 valid 0.23776143416762352
LOSS train 0.25788301822997756 valid 0.23749580350187088
LOSS train 0.25788301822997756 valid 0.23811161161764807
LOSS train 0.25788301822997756 valid 0.238270399418283
LOSS train 0.25788301822997756 valid 0.23828539128104845
LOSS train 0.25788301822997756 valid 0.2388601333511119
LOSS train 0.25788301822997756 valid 0.23845614343881608
LOSS train 0.25788301822997756 valid 0.23881313350855135
LOSS train 0.25788301822997756 valid 0.2386749521470987
LOSS train 0.25788301822997756 valid 0.2389723149672994
LOSS train 0.25788301822997756 valid 0.23862951635210602
LOSS train 0.25788301822997756 valid 0.23858721093697982
LOSS train 0.25788301822997756 valid 0.2382257240159171
LOSS train 0.25788301822997756 valid 0.2380349189043045
LOSS train 0.25788301822997756 valid 0.23778818213734135
LOSS train 0.25788301822997756 valid 0.23817131756725957
LOSS train 0.25788301822997756 valid 0.23752404873569807
LOSS train 0.25788301822997756 valid 0.23769067862971885
LOSS train 0.25788301822997756 valid 0.23852452323321374
LOSS train 0.25788301822997756 valid 0.23904948740724533
LOSS train 0.25788301822997756 valid 0.2399101892951876
LOSS train 0.25788301822997756 valid 0.24000472380564764
LOSS train 0.25788301822997756 valid 0.23997334097370956
LOSS train 0.25788301822997756 valid 0.239479380534656
LOSS train 0.25788301822997756 valid 0.23940055883106062
LOSS train 0.25788301822997756 valid 0.2386321555013242
LOSS train 0.25788301822997756 valid 0.23867358501468386
LOSS train 0.25788301822997756 valid 0.23859079769799407
LOSS train 0.25788301822997756 valid 0.23879527528252867
LOSS train 0.25788301822997756 valid 0.2387204157979521
LOSS train 0.25788301822997756 valid 0.23875278174071699
LOSS train 0.25788301822997756 valid 0.23880734264850617
LOSS train 0.25788301822997756 valid 0.23922236793135343
LOSS train 0.25788301822997756 valid 0.2394309305138402
LOSS train 0.25788301822997756 valid 0.2393335115451079
LOSS train 0.25788301822997756 valid 0.2392682580253746
LOSS train 0.25788301822997756 valid 0.23857257924973965
LOSS train 0.25788301822997756 valid 0.23779865878599662
LOSS train 0.25788301822997756 valid 0.23801270163640742
LOSS train 0.25788301822997756 valid 0.2378029433718647
LOSS train 0.25788301822997756 valid 0.23768908051507814
LOSS train 0.25788301822997756 valid 0.23731242733843186
LOSS train 0.25788301822997756 valid 0.2366092809757521
LOSS train 0.25788301822997756 valid 0.23670360805659457
LOSS train 0.25788301822997756 valid 0.23616547357629647
LOSS train 0.25788301822997756 valid 0.23633146403210886
LOSS train 0.25788301822997756 valid 0.2366133373644617
LOSS train 0.25788301822997756 valid 0.2367214321435153
LOSS train 0.25788301822997756 valid 0.23677299884350403
LOSS train 0.25788301822997756 valid 0.23654511154338878
LOSS train 0.25788301822997756 valid 0.23679137959125193
LOSS train 0.25788301822997756 valid 0.23636091947555543
LOSS train 0.25788301822997756 valid 0.2364557875941197
LOSS train 0.25788301822997756 valid 0.23647811440462918
LOSS train 0.25788301822997756 valid 0.23644829024465716
LOSS train 0.25788301822997756 valid 0.23654621779316604
LOSS train 0.25788301822997756 valid 0.2367867025732994
LOSS train 0.25788301822997756 valid 0.23710355811780043
LOSS train 0.25788301822997756 valid 0.2372015136129716
LOSS train 0.25788301822997756 valid 0.23735560578049966
LOSS train 0.25788301822997756 valid 0.23723266932826775
LOSS train 0.25788301822997756 valid 0.23722434015501112
LOSS train 0.25788301822997756 valid 0.23748210297440583
LOSS train 0.25788301822997756 valid 0.23717887824940903
LOSS train 0.25788301822997756 valid 0.23734305219517815
LOSS train 0.25788301822997756 valid 0.2378485719545172
LOSS train 0.25788301822997756 valid 0.23819682869044217
LOSS train 0.25788301822997756 valid 0.2378159466090503
LOSS train 0.25788301822997756 valid 0.23749968076923064
LOSS train 0.25788301822997756 valid 0.23742589605065573
LOSS train 0.25788301822997756 valid 0.23736328161076495
LOSS train 0.25788301822997756 valid 0.23725336764169777
LOSS train 0.25788301822997756 valid 0.23755284013419314
LOSS train 0.25788301822997756 valid 0.2378405360584585
LOSS train 0.25788301822997756 valid 0.23765783959021003
LOSS train 0.25788301822997756 valid 0.23752759622425593
LOSS train 0.25788301822997756 valid 0.2370786218593518
LOSS train 0.25788301822997756 valid 0.23690388714971622
LOSS train 0.25788301822997756 valid 0.23675457217165682
LOSS train 0.25788301822997756 valid 0.23675388691386556
LOSS train 0.25788301822997756 valid 0.23691769989748154
LOSS train 0.25788301822997756 valid 0.2368492624759674
LOSS train 0.25788301822997756 valid 0.23704902662171257
LOSS train 0.25788301822997756 valid 0.23690061763984951
LOSS train 0.25788301822997756 valid 0.2372736610705033
LOSS train 0.25788301822997756 valid 0.23725711317487466
LOSS train 0.25788301822997756 valid 0.23739544646098062
LOSS train 0.25788301822997756 valid 0.23743657631273488
LOSS train 0.25788301822997756 valid 0.23711157601439592
LOSS train 0.25788301822997756 valid 0.23698486432545168
LOSS train 0.25788301822997756 valid 0.23697804893130686
LOSS train 0.25788301822997756 valid 0.23691585682056568
LOSS train 0.25788301822997756 valid 0.23688487533260794
LOSS train 0.25788301822997756 valid 0.23675867860769703
LOSS train 0.25788301822997756 valid 0.23664506827143655
LOSS train 0.25788301822997756 valid 0.23648715480197247
LOSS train 0.25788301822997756 valid 0.2366518515561308
LOSS train 0.25788301822997756 valid 0.23658627578130004
LOSS train 0.25788301822997756 valid 0.23675002126206815
LOSS train 0.25788301822997756 valid 0.23677392137217354
LOSS train 0.25788301822997756 valid 0.2368486381860243
LOSS train 0.25788301822997756 valid 0.23662653575683462
LOSS train 0.25788301822997756 valid 0.23690639079025347
LOSS train 0.25788301822997756 valid 0.23671525716781616
LOSS train 0.25788301822997756 valid 0.23743284332591133
LOSS train 0.25788301822997756 valid 0.2374253640038855
LOSS train 0.25788301822997756 valid 0.23736301869153975
LOSS train 0.25788301822997756 valid 0.23724645445283676
LOSS train 0.25788301822997756 valid 0.23709327256993243
LOSS train 0.25788301822997756 valid 0.2370983811180576
LOSS train 0.25788301822997756 valid 0.2370890902427884
LOSS train 0.25788301822997756 valid 0.2372048532770526
LOSS train 0.25788301822997756 valid 0.23763927368399423
LOSS train 0.25788301822997756 valid 0.2376244829338827
LOSS train 0.25788301822997756 valid 0.23760026250081726
LOSS train 0.25788301822997756 valid 0.23755849082514924
LOSS train 0.25788301822997756 valid 0.2372891333885491
LOSS train 0.25788301822997756 valid 0.23728623347623007
LOSS train 0.25788301822997756 valid 0.23727279984288746
LOSS train 0.25788301822997756 valid 0.2372627546268007
LOSS train 0.25788301822997756 valid 0.23709341065912712
LOSS train 0.25788301822997756 valid 0.23690979291092265
LOSS train 0.25788301822997756 valid 0.23715856931654802
LOSS train 0.25788301822997756 valid 0.23725499159204747
LOSS train 0.25788301822997756 valid 0.23718329714167685
LOSS train 0.25788301822997756 valid 0.23725995932810406
LOSS train 0.25788301822997756 valid 0.2375341559157652
LOSS train 0.25788301822997756 valid 0.23739669058057997
LOSS train 0.25788301822997756 valid 0.23717841046840646
LOSS train 0.25788301822997756 valid 0.23720746930968556
LOSS train 0.25788301822997756 valid 0.23711814277473536
LOSS train 0.25788301822997756 valid 0.23709062142031534
LOSS train 0.25788301822997756 valid 0.23704670725220983
LOSS train 0.25788301822997756 valid 0.23715329035527288
LOSS train 0.25788301822997756 valid 0.2373361453581392
LOSS train 0.25788301822997756 valid 0.23725831725077923
LOSS train 0.25788301822997756 valid 0.23727329307132297
LOSS train 0.25788301822997756 valid 0.23745182041305204
LOSS train 0.25788301822997756 valid 0.23751503376515357
LOSS train 0.25788301822997756 valid 0.2373125430990438
LOSS train 0.25788301822997756 valid 0.23720451217630636
LOSS train 0.25788301822997756 valid 0.23708624380665858
LOSS train 0.25788301822997756 valid 0.23705623923770844
LOSS train 0.25788301822997756 valid 0.23693095115735568
LOSS train 0.25788301822997756 valid 0.23703993452673264
LOSS train 0.25788301822997756 valid 0.23717523338618102
LOSS train 0.25788301822997756 valid 0.23725063604743857
LOSS train 0.25788301822997756 valid 0.23713321203648732
LOSS train 0.25788301822997756 valid 0.23704563632297018
LOSS train 0.25788301822997756 valid 0.2371836856702449
LOSS train 0.25788301822997756 valid 0.23699650506383366
LOSS train 0.25788301822997756 valid 0.2366656813866053
LOSS train 0.25788301822997756 valid 0.23678352303650915
LOSS train 0.25788301822997756 valid 0.23689399288995616
LOSS train 0.25788301822997756 valid 0.23690508102828806
LOSS train 0.25788301822997756 valid 0.2369516165412251
LOSS train 0.25788301822997756 valid 0.23676353842020034
LOSS train 0.25788301822997756 valid 0.23661130763108457
LOSS train 0.25788301822997756 valid 0.23652477082934711
LOSS train 0.25788301822997756 valid 0.23644642693362214
LOSS train 0.25788301822997756 valid 0.2363719688180615
LOSS train 0.25788301822997756 valid 0.23620208181985994
LOSS train 0.25788301822997756 valid 0.23617402623289996
LOSS train 0.25788301822997756 valid 0.23602963048190886
LOSS train 0.25788301822997756 valid 0.23598945054870385
LOSS train 0.25788301822997756 valid 0.2358976884179138
LOSS train 0.25788301822997756 valid 0.2359587374187651
LOSS train 0.25788301822997756 valid 0.23597476860923225
LOSS train 0.25788301822997756 valid 0.23581421825121035
LOSS train 0.25788301822997756 valid 0.23585479227310055
LOSS train 0.25788301822997756 valid 0.23575509381349954
LOSS train 0.25788301822997756 valid 0.23562150244102922
LOSS train 0.25788301822997756 valid 0.23556537450187737
LOSS train 0.25788301822997756 valid 0.23560066267092655
LOSS train 0.25788301822997756 valid 0.2356185837896592
LOSS train 0.25788301822997756 valid 0.23570092056439892
LOSS train 0.25788301822997756 valid 0.2358509062366052
LOSS train 0.25788301822997756 valid 0.23593328926897697
LOSS train 0.25788301822997756 valid 0.23587913972300453
LOSS train 0.25788301822997756 valid 0.2360316926588392
LOSS train 0.25788301822997756 valid 0.23613859513508423
LOSS train 0.25788301822997756 valid 0.23616363988982306
LOSS train 0.25788301822997756 valid 0.23627156876357255
LOSS train 0.25788301822997756 valid 0.2365092723117526
LOSS train 0.25788301822997756 valid 0.23654455292904586
LOSS train 0.25788301822997756 valid 0.2365903153560047
LOSS train 0.25788301822997756 valid 0.23670561436725698
LOSS train 0.25788301822997756 valid 0.23668900009615595
LOSS train 0.25788301822997756 valid 0.23675033764849449
LOSS train 0.25788301822997756 valid 0.2366153909285181
LOSS train 0.25788301822997756 valid 0.2366610739985083
LOSS train 0.25788301822997756 valid 0.23672923699338386
LOSS train 0.25788301822997756 valid 0.23667098000898198
LOSS train 0.25788301822997756 valid 0.23670616406428663
LOSS train 0.25788301822997756 valid 0.23674434673886338
LOSS train 0.25788301822997756 valid 0.23663316105948332
LOSS train 0.25788301822997756 valid 0.23652577288448812
LOSS train 0.25788301822997756 valid 0.23666015564158743
LOSS train 0.25788301822997756 valid 0.23640937358140945
LOSS train 0.25788301822997756 valid 0.23649734694035454
LOSS train 0.25788301822997756 valid 0.23671234771609306
LOSS train 0.25788301822997756 valid 0.23682816521245606
LOSS train 0.25788301822997756 valid 0.23680435157403712
LOSS train 0.25788301822997756 valid 0.2367667436961703
LOSS train 0.25788301822997756 valid 0.2367469210538172
LOSS train 0.25788301822997756 valid 0.23669646321290946
LOSS train 0.25788301822997756 valid 0.23685041278600694
LOSS train 0.25788301822997756 valid 0.23681475809845792
LOSS train 0.25788301822997756 valid 0.23685281877479855
LOSS train 0.25788301822997756 valid 0.23673760095839444
LOSS train 0.25788301822997756 valid 0.2367334432723954
LOSS train 0.25788301822997756 valid 0.2367984391894995
LOSS train 0.25788301822997756 valid 0.23687793023418635
LOSS train 0.25788301822997756 valid 0.23673166056319433
LOSS train 0.25788301822997756 valid 0.23667018242584642
LOSS train 0.25788301822997756 valid 0.2366336134755013
LOSS train 0.25788301822997756 valid 0.23663609377466716
LOSS train 0.25788301822997756 valid 0.23668433817182008
LOSS train 0.25788301822997756 valid 0.23667666668655307
LOSS train 0.25788301822997756 valid 0.2366336280968706
LOSS train 0.25788301822997756 valid 0.2366259413573778
LOSS train 0.25788301822997756 valid 0.23652885460628653
LOSS train 0.25788301822997756 valid 0.23648967184966668
LOSS train 0.25788301822997756 valid 0.23658423365725115
LOSS train 0.25788301822997756 valid 0.23674520182965406
LOSS train 0.25788301822997756 valid 0.2368346816101925
LOSS train 0.25788301822997756 valid 0.23679222034083472
LOSS train 0.25788301822997756 valid 0.23682770765355593
LOSS train 0.25788301822997756 valid 0.2370592906304142
LOSS train 0.25788301822997756 valid 0.23705303516143408
LOSS train 0.25788301822997756 valid 0.2370656716236233
LOSS train 0.25788301822997756 valid 0.23699832542376084
LOSS train 0.25788301822997756 valid 0.23693597538099773
LOSS train 0.25788301822997756 valid 0.23674097845485495
LOSS train 0.25788301822997756 valid 0.23655219799537452
LOSS train 0.25788301822997756 valid 0.23657756958383813
LOSS train 0.25788301822997756 valid 0.23652233917798315
LOSS train 0.25788301822997756 valid 0.23647679712000266
LOSS train 0.25788301822997756 valid 0.23630611651332667
LOSS train 0.25788301822997756 valid 0.23634337315289797
LOSS train 0.25788301822997756 valid 0.23641612758518946
LOSS train 0.25788301822997756 valid 0.2364395221074422
LOSS train 0.25788301822997756 valid 0.23634665516706613
LOSS train 0.25788301822997756 valid 0.23623890697125358
LOSS train 0.25788301822997756 valid 0.23629761026758286
LOSS train 0.25788301822997756 valid 0.23633711070956656
LOSS train 0.25788301822997756 valid 0.23635107987913592
LOSS train 0.25788301822997756 valid 0.23637293647859514
LOSS train 0.25788301822997756 valid 0.2364046214888357
LOSS train 0.25788301822997756 valid 0.2364290923693888
LOSS train 0.25788301822997756 valid 0.23653330765411157
LOSS train 0.25788301822997756 valid 0.2365931273011838
LOSS train 0.25788301822997756 valid 0.23654515865082676
LOSS train 0.25788301822997756 valid 0.236573014797185
LOSS train 0.25788301822997756 valid 0.23658308146783968
LOSS train 0.25788301822997756 valid 0.23664935685719135
LOSS train 0.25788301822997756 valid 0.23660922388235728
LOSS train 0.25788301822997756 valid 0.23669679319343695
LOSS train 0.25788301822997756 valid 0.23668576762178875
LOSS train 0.25788301822997756 valid 0.23678920818830873
LOSS train 0.25788301822997756 valid 0.23679358943512566
LOSS train 0.25788301822997756 valid 0.23668976626435265
LOSS train 0.25788301822997756 valid 0.23657839949808868
LOSS train 0.25788301822997756 valid 0.23653553833984786
LOSS train 0.25788301822997756 valid 0.2364506329131591
LOSS train 0.25788301822997756 valid 0.23658298891531998
LOSS train 0.25788301822997756 valid 0.2365865044536129
LOSS train 0.25788301822997756 valid 0.23646741310116562
LOSS train 0.25788301822997756 valid 0.2364856828099642
LOSS train 0.25788301822997756 valid 0.2365074234362989
LOSS train 0.25788301822997756 valid 0.23657614377084052
LOSS train 0.25788301822997756 valid 0.23659584702007355
LOSS train 0.25788301822997756 valid 0.23649562121946602
LOSS train 0.25788301822997756 valid 0.23651269736350147
LOSS train 0.25788301822997756 valid 0.2365598827600479
LOSS train 0.25788301822997756 valid 0.23663343054747507
LOSS train 0.25788301822997756 valid 0.2365869479253888
LOSS train 0.25788301822997756 valid 0.23662876089413962
LOSS train 0.25788301822997756 valid 0.23660547078026006
LOSS train 0.25788301822997756 valid 0.23657384214940086
LOSS train 0.25788301822997756 valid 0.23661599441626927
LOSS train 0.25788301822997756 valid 0.23653759076045108
LOSS train 0.25788301822997756 valid 0.23670157223391386
LOSS train 0.25788301822997756 valid 0.2368092923353936
LOSS train 0.25788301822997756 valid 0.23678461116988483
LOSS train 0.25788301822997756 valid 0.23691903276646392
LOSS train 0.25788301822997756 valid 0.236899584441474
LOSS train 0.25788301822997756 valid 0.23688647371584556
LOSS train 0.25788301822997756 valid 0.2367994211703898
LOSS train 0.25788301822997756 valid 0.23676836141594895
LOSS train 0.25788301822997756 valid 0.23689439857077454
LOSS train 0.25788301822997756 valid 0.23689655746096996
LOSS train 0.25788301822997756 valid 0.23692147642196643
LOSS train 0.25788301822997756 valid 0.236796467744632
LOSS train 0.25788301822997756 valid 0.23677010027438225
LOSS train 0.25788301822997756 valid 0.23678318808915694
LOSS train 0.25788301822997756 valid 0.23683168782907374
LOSS train 0.25788301822997756 valid 0.23675198796207952
LOSS train 0.25788301822997756 valid 0.2367506563140635
LOSS train 0.25788301822997756 valid 0.2367366609698482
LOSS train 0.25788301822997756 valid 0.23689308286059735
LOSS train 0.25788301822997756 valid 0.23689315189485965
LOSS train 0.25788301822997756 valid 0.23679464314714332
LOSS train 0.25788301822997756 valid 0.23675664636690266
LOSS train 0.25788301822997756 valid 0.23672941108715945
LOSS train 0.25788301822997756 valid 0.23683147209250824
LOSS train 0.25788301822997756 valid 0.23670440839869636
LOSS train 0.25788301822997756 valid 0.2366427640374909
LOSS train 0.25788301822997756 valid 0.2365272268652916
LOSS train 0.25788301822997756 valid 0.23661338430288492
LOSS train 0.25788301822997756 valid 0.23666942245879416
LOSS train 0.25788301822997756 valid 0.2367813461263415
LOSS train 0.25788301822997756 valid 0.23682295849149146
LOSS train 0.25788301822997756 valid 0.23678620439283654
LOSS train 0.25788301822997756 valid 0.23671343298264721
LOSS train 0.25788301822997756 valid 0.23668500324477723
LOSS train 0.25788301822997756 valid 0.23659375641081068
LOSS train 0.25788301822997756 valid 0.2365157323943611
LOSS train 0.25788301822997756 valid 0.2366022627738958
LOSS train 0.25788301822997756 valid 0.23646698810016484
LOSS train 0.25788301822997756 valid 0.2364235646858975
LOSS train 0.25788301822997756 valid 0.23639961689302366
LOSS train 0.25788301822997756 valid 0.2363466113154354
LOSS train 0.25788301822997756 valid 0.236226286119921
LOSS train 0.25788301822997756 valid 0.23619861593065056
LOSS train 0.25788301822997756 valid 0.23623320593581937
EPOCH 12:
  batch 1 loss: 0.25405654311180115
  batch 2 loss: 0.2468802109360695
  batch 3 loss: 0.24990619719028473
  batch 4 loss: 0.2493702583014965
  batch 5 loss: 0.25379485785961153
  batch 6 loss: 0.25068532923857373
  batch 7 loss: 0.25147328632218496
  batch 8 loss: 0.2533227205276489
  batch 9 loss: 0.2560812466674381
  batch 10 loss: 0.2574430167675018
  batch 11 loss: 0.2594588263468309
  batch 12 loss: 0.25589822232723236
  batch 13 loss: 0.254775203191317
  batch 14 loss: 0.25430849513837267
  batch 15 loss: 0.255735237399737
  batch 16 loss: 0.25551159027963877
  batch 17 loss: 0.25335947029730854
  batch 18 loss: 0.2538833088344998
  batch 19 loss: 0.2524379899627284
  batch 20 loss: 0.25125395357608793
  batch 21 loss: 0.2517028890904926
  batch 22 loss: 0.2521983493458141
  batch 23 loss: 0.25136477662169415
  batch 24 loss: 0.250070055325826
  batch 25 loss: 0.2520104789733887
  batch 26 loss: 0.2506561657557121
  batch 27 loss: 0.2506915110128897
  batch 28 loss: 0.2492211881492819
  batch 29 loss: 0.2496377625342073
  batch 30 loss: 0.24992208828528722
  batch 31 loss: 0.2508233175162346
  batch 32 loss: 0.25015058275312185
  batch 33 loss: 0.24971909098552936
  batch 34 loss: 0.25006594771848006
  batch 35 loss: 0.2498125502041408
  batch 36 loss: 0.24935329581300417
  batch 37 loss: 0.24918136363093918
  batch 38 loss: 0.24999996353136866
  batch 39 loss: 0.24960012007982302
  batch 40 loss: 0.24950486198067665
  batch 41 loss: 0.24904606981975277
  batch 42 loss: 0.2501022375765301
  batch 43 loss: 0.24952113870964493
  batch 44 loss: 0.24961173365061934
  batch 45 loss: 0.249574276804924
  batch 46 loss: 0.24944164085647333
  batch 47 loss: 0.2500258262487168
  batch 48 loss: 0.24946175701916218
  batch 49 loss: 0.24960198937630168
  batch 50 loss: 0.2493436187505722
  batch 51 loss: 0.24911205324472166
  batch 52 loss: 0.2492703847013987
  batch 53 loss: 0.24855773881921228
  batch 54 loss: 0.2483483999967575
  batch 55 loss: 0.2474862423810092
  batch 56 loss: 0.24733422749808856
  batch 57 loss: 0.2463544667289968
  batch 58 loss: 0.24648381384282275
  batch 59 loss: 0.24641421058420407
  batch 60 loss: 0.24561995143691698
  batch 61 loss: 0.24591032965261428
  batch 62 loss: 0.24593042414034566
  batch 63 loss: 0.24502395471883198
  batch 64 loss: 0.24557482381351292
  batch 65 loss: 0.245164715555998
  batch 66 loss: 0.244951428789081
  batch 67 loss: 0.2453010749461046
  batch 68 loss: 0.2458478843464571
  batch 69 loss: 0.24578660877718442
  batch 70 loss: 0.24586181747061867
  batch 71 loss: 0.24547130415137386
  batch 72 loss: 0.2456895982225736
  batch 73 loss: 0.24557617163821444
  batch 74 loss: 0.24588063459944082
  batch 75 loss: 0.24540793836116792
  batch 76 loss: 0.24563081189990044
  batch 77 loss: 0.24517468450131355
  batch 78 loss: 0.24494594488388452
  batch 79 loss: 0.2452611402620243
  batch 80 loss: 0.24502640943974258
  batch 81 loss: 0.24502738813559213
  batch 82 loss: 0.24551727659091716
  batch 83 loss: 0.24542367368577475
  batch 84 loss: 0.2454379809399446
  batch 85 loss: 0.2453737257158055
  batch 86 loss: 0.24576696112405422
  batch 87 loss: 0.24583994400227208
  batch 88 loss: 0.24550446952608498
  batch 89 loss: 0.24540949587741595
  batch 90 loss: 0.24545132451587254
  batch 91 loss: 0.24573646436680804
  batch 92 loss: 0.24572527748735054
  batch 93 loss: 0.24545905570830068
  batch 94 loss: 0.24562772189049012
  batch 95 loss: 0.24540900258641493
  batch 96 loss: 0.24553063713635007
  batch 97 loss: 0.24586567789623417
  batch 98 loss: 0.24622563242304082
  batch 99 loss: 0.2464513566457864
  batch 100 loss: 0.24647224202752113
  batch 101 loss: 0.24645269861315736
  batch 102 loss: 0.24677687971030965
  batch 103 loss: 0.24717316841616213
  batch 104 loss: 0.2473326941522268
  batch 105 loss: 0.2470347918215252
  batch 106 loss: 0.24747386968360757
  batch 107 loss: 0.24697295736486666
  batch 108 loss: 0.24680217383084474
  batch 109 loss: 0.24671532241029476
  batch 110 loss: 0.24641524973240767
  batch 111 loss: 0.24641169151207348
  batch 112 loss: 0.24608678117926633
  batch 113 loss: 0.24613317438458973
  batch 114 loss: 0.24662290578871443
  batch 115 loss: 0.24662121171536652
  batch 116 loss: 0.24682155200119676
  batch 117 loss: 0.24721155436629924
  batch 118 loss: 0.24705461400040127
  batch 119 loss: 0.2472718110104569
  batch 120 loss: 0.24742281859119733
  batch 121 loss: 0.24747619303790006
  batch 122 loss: 0.24725099628577468
  batch 123 loss: 0.24694166290081612
  batch 124 loss: 0.24727973822624452
  batch 125 loss: 0.2471279468536377
  batch 126 loss: 0.24688050623924013
  batch 127 loss: 0.24745372242814911
  batch 128 loss: 0.24748111492954195
  batch 129 loss: 0.2477545368579007
  batch 130 loss: 0.24801570979448465
  batch 131 loss: 0.24830793356167452
  batch 132 loss: 0.24839258871295236
  batch 133 loss: 0.24869472191746073
  batch 134 loss: 0.24895485479440263
  batch 135 loss: 0.24913779541298195
  batch 136 loss: 0.24930713238085017
  batch 137 loss: 0.24916962398229725
  batch 138 loss: 0.24933678473251453
  batch 139 loss: 0.2495814718359666
  batch 140 loss: 0.24953315800854137
  batch 141 loss: 0.24973873646124034
  batch 142 loss: 0.24977040028488132
  batch 143 loss: 0.24961139886946113
  batch 144 loss: 0.2497420208528638
  batch 145 loss: 0.24984014414507769
  batch 146 loss: 0.24969470327439375
  batch 147 loss: 0.24987747444182026
  batch 148 loss: 0.2501038422657026
  batch 149 loss: 0.25033073387290006
  batch 150 loss: 0.2503373059630394
  batch 151 loss: 0.2502835165980636
  batch 152 loss: 0.25048527964635897
  batch 153 loss: 0.25026938660082476
  batch 154 loss: 0.25046647688398116
  batch 155 loss: 0.25035946686421673
  batch 156 loss: 0.25046102559337247
  batch 157 loss: 0.25078360888229057
  batch 158 loss: 0.2509374334653722
  batch 159 loss: 0.25092406186667626
  batch 160 loss: 0.25087119480594994
  batch 161 loss: 0.25104748564107077
  batch 162 loss: 0.2512024246432163
  batch 163 loss: 0.25117312234603556
  batch 164 loss: 0.2512946352362633
  batch 165 loss: 0.25122635635462676
  batch 166 loss: 0.25133531208497933
  batch 167 loss: 0.2512537918226448
  batch 168 loss: 0.25131169165528955
  batch 169 loss: 0.2510553459973025
  batch 170 loss: 0.250699723731069
  batch 171 loss: 0.25067706789538174
  batch 172 loss: 0.2507358121490756
  batch 173 loss: 0.2507600018571567
  batch 174 loss: 0.2508933486780901
  batch 175 loss: 0.2509901020356587
  batch 176 loss: 0.25099768447266385
  batch 177 loss: 0.2510734415155346
  batch 178 loss: 0.251128047788411
  batch 179 loss: 0.25138196457364703
  batch 180 loss: 0.25138985903726685
  batch 181 loss: 0.25133916315782134
  batch 182 loss: 0.25138952222826716
  batch 183 loss: 0.2511974533236092
  batch 184 loss: 0.2511369836233232
  batch 185 loss: 0.2511680520064122
  batch 186 loss: 0.2512560731140516
  batch 187 loss: 0.25133690405338205
  batch 188 loss: 0.2511249331242226
  batch 189 loss: 0.2510627017765449
  batch 190 loss: 0.25120621794148495
  batch 191 loss: 0.25112159846652865
  batch 192 loss: 0.25115755627242226
  batch 193 loss: 0.2513196428977146
  batch 194 loss: 0.2515991691922404
  batch 195 loss: 0.251644537005669
  batch 196 loss: 0.2515904968034248
  batch 197 loss: 0.2516381626353046
  batch 198 loss: 0.2519322238636739
  batch 199 loss: 0.25191536860250346
  batch 200 loss: 0.25203731387853623
  batch 201 loss: 0.2520930019777213
  batch 202 loss: 0.2520984482057024
  batch 203 loss: 0.2519667819509365
  batch 204 loss: 0.2517683592935403
  batch 205 loss: 0.25200549051529025
  batch 206 loss: 0.25197381924078305
  batch 207 loss: 0.25198835393656854
  batch 208 loss: 0.2520321018707294
  batch 209 loss: 0.251839317630923
  batch 210 loss: 0.2519530221110299
  batch 211 loss: 0.2518107466646845
  batch 212 loss: 0.2518297926575508
  batch 213 loss: 0.2518328177117406
  batch 214 loss: 0.251921898641876
  batch 215 loss: 0.2516582857730777
  batch 216 loss: 0.25169487780442945
  batch 217 loss: 0.25176381954949023
  batch 218 loss: 0.2517310886071363
  batch 219 loss: 0.2517681029969699
  batch 220 loss: 0.2518761502748186
  batch 221 loss: 0.2519670022963399
  batch 222 loss: 0.2520597585820937
  batch 223 loss: 0.25209353302893617
  batch 224 loss: 0.2520021252068026
  batch 225 loss: 0.2519326380226347
  batch 226 loss: 0.2519927765283964
  batch 227 loss: 0.25175290446449483
  batch 228 loss: 0.2516228330501339
  batch 229 loss: 0.25153160550708853
  batch 230 loss: 0.2515293923409089
  batch 231 loss: 0.2514097485449407
  batch 232 loss: 0.25133441690484
  batch 233 loss: 0.25124849820341677
  batch 234 loss: 0.2513137771787807
  batch 235 loss: 0.2513759651082627
  batch 236 loss: 0.2512842088432635
  batch 237 loss: 0.25144518540881355
  batch 238 loss: 0.2514387357886098
  batch 239 loss: 0.2514338536990736
  batch 240 loss: 0.25159553152819475
  batch 241 loss: 0.2517018386189868
  batch 242 loss: 0.25167292147135933
  batch 243 loss: 0.2517282020898513
  batch 244 loss: 0.2517246783512538
  batch 245 loss: 0.251688013636336
  batch 246 loss: 0.25158386816823386
  batch 247 loss: 0.25157602366648224
  batch 248 loss: 0.25162317651894783
  batch 249 loss: 0.2515586292647932
  batch 250 loss: 0.2513750019669533
  batch 251 loss: 0.25142956237156555
  batch 252 loss: 0.25135583909494535
  batch 253 loss: 0.25125024612006464
  batch 254 loss: 0.25118328583991434
  batch 255 loss: 0.25120411585359015
  batch 256 loss: 0.25129889184609056
  batch 257 loss: 0.25138497387388803
  batch 258 loss: 0.2513822064150211
  batch 259 loss: 0.2514655080311087
  batch 260 loss: 0.2516571260415591
  batch 261 loss: 0.25168550322795735
  batch 262 loss: 0.25163673399058917
  batch 263 loss: 0.2516667811136282
  batch 264 loss: 0.2515951765870506
  batch 265 loss: 0.2516522137061605
  batch 266 loss: 0.2516558166397245
  batch 267 loss: 0.25170264211710025
  batch 268 loss: 0.2517783937058342
  batch 269 loss: 0.25175285012527027
  batch 270 loss: 0.2518050303613698
  batch 271 loss: 0.251830400521025
  batch 272 loss: 0.25183731596916914
  batch 273 loss: 0.25183038118776385
  batch 274 loss: 0.25199028300325366
  batch 275 loss: 0.2521547515283931
  batch 276 loss: 0.25229397227150807
  batch 277 loss: 0.25236334261696264
  batch 278 loss: 0.25234528974020226
  batch 279 loss: 0.25246592068971274
  batch 280 loss: 0.2524436480764832
  batch 281 loss: 0.2523931709260703
  batch 282 loss: 0.2524343872746677
  batch 283 loss: 0.2524127031383582
  batch 284 loss: 0.25240054228146314
  batch 285 loss: 0.2523877430903284
  batch 286 loss: 0.2524403337311078
  batch 287 loss: 0.2524308386268516
  batch 288 loss: 0.2522096935038765
  batch 289 loss: 0.25237286853955276
  batch 290 loss: 0.2521761417902749
  batch 291 loss: 0.2521583538796894
  batch 292 loss: 0.2522761720194392
  batch 293 loss: 0.25238353609021613
  batch 294 loss: 0.25229843411924074
  batch 295 loss: 0.2522339211682142
  batch 296 loss: 0.2523946336193665
  batch 297 loss: 0.25241594001500295
  batch 298 loss: 0.2523598951981372
  batch 299 loss: 0.25231856820376025
  batch 300 loss: 0.2524331953624884
  batch 301 loss: 0.2524470687407592
  batch 302 loss: 0.2524346359419507
  batch 303 loss: 0.25268605336694433
  batch 304 loss: 0.2526689970767812
  batch 305 loss: 0.25261649826511007
  batch 306 loss: 0.2527624205533975
  batch 307 loss: 0.2527035021917828
  batch 308 loss: 0.25273573790471277
  batch 309 loss: 0.2527121551889432
  batch 310 loss: 0.2527137474186959
  batch 311 loss: 0.25283502847243733
  batch 312 loss: 0.25306121675440896
  batch 313 loss: 0.2531624019812471
  batch 314 loss: 0.2531582576928625
  batch 315 loss: 0.2532062799684585
  batch 316 loss: 0.2531673182509368
  batch 317 loss: 0.2531271069677846
  batch 318 loss: 0.2531884699216429
  batch 319 loss: 0.25322298565441537
  batch 320 loss: 0.25323031744919716
  batch 321 loss: 0.2532575124427908
  batch 322 loss: 0.253227135640864
  batch 323 loss: 0.2531812731695618
  batch 324 loss: 0.25300357844910504
  batch 325 loss: 0.2529145190807489
  batch 326 loss: 0.2529194898491988
  batch 327 loss: 0.2529642096626649
  batch 328 loss: 0.2528516714256711
  batch 329 loss: 0.25294269829716726
  batch 330 loss: 0.2528492419557138
  batch 331 loss: 0.25286614845346467
  batch 332 loss: 0.25274936455380487
  batch 333 loss: 0.252859124684477
  batch 334 loss: 0.25279473660591834
  batch 335 loss: 0.2527405521762905
  batch 336 loss: 0.2527083054717098
  batch 337 loss: 0.2525091958328949
  batch 338 loss: 0.25254126836562296
  batch 339 loss: 0.25245202137129846
  batch 340 loss: 0.2523974650484674
  batch 341 loss: 0.25225283096263135
  batch 342 loss: 0.25222691065735287
  batch 343 loss: 0.25218583308920567
  batch 344 loss: 0.2522305957106657
  batch 345 loss: 0.25232305224391
  batch 346 loss: 0.2522800011355753
  batch 347 loss: 0.2523581970923229
  batch 348 loss: 0.2523869723509783
  batch 349 loss: 0.2523266495042681
  batch 350 loss: 0.2524471829192979
  batch 351 loss: 0.2524728167515535
  batch 352 loss: 0.2525785768150606
  batch 353 loss: 0.2525907529058943
  batch 354 loss: 0.25267367161768306
  batch 355 loss: 0.2526916545461601
  batch 356 loss: 0.25272239238191185
  batch 357 loss: 0.2525949015253398
  batch 358 loss: 0.25265419587243204
  batch 359 loss: 0.25270816347559183
  batch 360 loss: 0.2526781926138533
  batch 361 loss: 0.25259656580861584
  batch 362 loss: 0.25254989061401695
  batch 363 loss: 0.2524819980393428
  batch 364 loss: 0.2523657923543846
  batch 365 loss: 0.2523146790184387
  batch 366 loss: 0.252213859655818
  batch 367 loss: 0.25216168010916956
  batch 368 loss: 0.25210936869616096
  batch 369 loss: 0.2520960412697417
  batch 370 loss: 0.2520585321091317
  batch 371 loss: 0.25207952821672447
  batch 372 loss: 0.2520178797985277
  batch 373 loss: 0.2519019937627117
  batch 374 loss: 0.25183972537039434
  batch 375 loss: 0.251765371521314
  batch 376 loss: 0.2517239537407109
  batch 377 loss: 0.25169133819857076
  batch 378 loss: 0.2515836278045619
  batch 379 loss: 0.25165731976875216
  batch 380 loss: 0.25164751226180476
  batch 381 loss: 0.2515669224143341
  batch 382 loss: 0.2514571827587657
  batch 383 loss: 0.2513588770406987
  batch 384 loss: 0.2513313175877556
  batch 385 loss: 0.25139131124143477
  batch 386 loss: 0.25125009296317175
  batch 387 loss: 0.25125939859130275
  batch 388 loss: 0.2513855025556284
  batch 389 loss: 0.25129827408833494
  batch 390 loss: 0.2512729486211752
  batch 391 loss: 0.2513037719156431
  batch 392 loss: 0.2513281663081476
  batch 393 loss: 0.2514700091446326
  batch 394 loss: 0.2515709284825373
  batch 395 loss: 0.25147075871878033
  batch 396 loss: 0.2515160459460634
  batch 397 loss: 0.251538031572659
  batch 398 loss: 0.2515352126491729
  batch 399 loss: 0.2515643748424406
  batch 400 loss: 0.2515918403863907
  batch 401 loss: 0.2515447463403616
  batch 402 loss: 0.2515318219489719
  batch 403 loss: 0.25150685552184104
  batch 404 loss: 0.2515579542827488
  batch 405 loss: 0.251608055204521
  batch 406 loss: 0.251752979172568
  batch 407 loss: 0.2517201093915639
  batch 408 loss: 0.2518763789432306
  batch 409 loss: 0.2519267440060823
  batch 410 loss: 0.2519826093461455
  batch 411 loss: 0.2519495969416161
  batch 412 loss: 0.25192802334294734
  batch 413 loss: 0.25199721930390695
  batch 414 loss: 0.25202562692372693
  batch 415 loss: 0.25201991182493877
  batch 416 loss: 0.252035302002556
  batch 417 loss: 0.2519836916411809
  batch 418 loss: 0.2519467919708439
  batch 419 loss: 0.2518848793176592
  batch 420 loss: 0.25189805804263976
  batch 421 loss: 0.251814077946749
  batch 422 loss: 0.251965870612888
  batch 423 loss: 0.25195545990275436
  batch 424 loss: 0.25194387770486326
  batch 425 loss: 0.2519949463535758
  batch 426 loss: 0.25192776910016235
  batch 427 loss: 0.2519715372516623
  batch 428 loss: 0.2519426821757143
  batch 429 loss: 0.2519601653297464
  batch 430 loss: 0.2519315534206324
  batch 431 loss: 0.2520961969261103
  batch 432 loss: 0.2521658603584877
  batch 433 loss: 0.2521497998859811
  batch 434 loss: 0.2521950582617439
  batch 435 loss: 0.252098296942382
  batch 436 loss: 0.2521558271703917
  batch 437 loss: 0.25228334622334014
  batch 438 loss: 0.2523875096685266
  batch 439 loss: 0.2524185542161481
  batch 440 loss: 0.25245324410498143
  batch 441 loss: 0.2524145742595331
  batch 442 loss: 0.25238839995402557
  batch 443 loss: 0.2523890463614302
  batch 444 loss: 0.25230702432292
  batch 445 loss: 0.25242519870902713
  batch 446 loss: 0.2524309041053725
  batch 447 loss: 0.2523850854944596
  batch 448 loss: 0.252330738884796
  batch 449 loss: 0.25240974871644994
  batch 450 loss: 0.2523568910029199
  batch 451 loss: 0.2522792994249158
  batch 452 loss: 0.25225013675811014
  batch 453 loss: 0.2523381001178266
  batch 454 loss: 0.25238905784710913
  batch 455 loss: 0.25246789281839854
  batch 456 loss: 0.25248189868503496
  batch 457 loss: 0.25258491131226396
  batch 458 loss: 0.25261930909869973
  batch 459 loss: 0.2527255483117758
  batch 460 loss: 0.25287219988911047
  batch 461 loss: 0.2529278655178893
  batch 462 loss: 0.25296347510892075
  batch 463 loss: 0.25303010616781385
  batch 464 loss: 0.2530704202001979
  batch 465 loss: 0.25300108037328206
  batch 466 loss: 0.2529082088511389
  batch 467 loss: 0.2530043180034809
  batch 468 loss: 0.2530958131592498
  batch 469 loss: 0.25322213108097313
  batch 470 loss: 0.25326708200130055
  batch 471 loss: 0.25335756298589607
  batch 472 loss: 0.25324976756789924
LOSS train 0.25324976756789924 valid 0.2462950050830841
LOSS train 0.25324976756789924 valid 0.23036764562129974
LOSS train 0.25324976756789924 valid 0.23575066030025482
LOSS train 0.25324976756789924 valid 0.22706405818462372
LOSS train 0.25324976756789924 valid 0.2178390085697174
LOSS train 0.25324976756789924 valid 0.22356838981310526
LOSS train 0.25324976756789924 valid 0.2302580007484981
LOSS train 0.25324976756789924 valid 0.22708813101053238
LOSS train 0.25324976756789924 valid 0.22997361752721998
LOSS train 0.25324976756789924 valid 0.23160890340805054
LOSS train 0.25324976756789924 valid 0.22766568985852328
LOSS train 0.25324976756789924 valid 0.22868790104985237
LOSS train 0.25324976756789924 valid 0.22818512182969314
LOSS train 0.25324976756789924 valid 0.22909227865082876
LOSS train 0.25324976756789924 valid 0.2261533002058665
LOSS train 0.25324976756789924 valid 0.22634412068873644
LOSS train 0.25324976756789924 valid 0.22696323692798615
LOSS train 0.25324976756789924 valid 0.22820840775966644
LOSS train 0.25324976756789924 valid 0.23043198177689
LOSS train 0.25324976756789924 valid 0.22993918135762215
LOSS train 0.25324976756789924 valid 0.22938834272679828
LOSS train 0.25324976756789924 valid 0.2280238060788675
LOSS train 0.25324976756789924 valid 0.22907275674135788
LOSS train 0.25324976756789924 valid 0.22845117499430975
LOSS train 0.25324976756789924 valid 0.2271645474433899
LOSS train 0.25324976756789924 valid 0.2282497940155176
LOSS train 0.25324976756789924 valid 0.22771244468512358
LOSS train 0.25324976756789924 valid 0.2280530259013176
LOSS train 0.25324976756789924 valid 0.22791050939724364
LOSS train 0.25324976756789924 valid 0.22946789065996806
LOSS train 0.25324976756789924 valid 0.2300438573283534
LOSS train 0.25324976756789924 valid 0.2298654643818736
LOSS train 0.25324976756789924 valid 0.23048972631945755
LOSS train 0.25324976756789924 valid 0.23039742953637066
LOSS train 0.25324976756789924 valid 0.23179183091436115
LOSS train 0.25324976756789924 valid 0.23150304125414956
LOSS train 0.25324976756789924 valid 0.23158090984499133
LOSS train 0.25324976756789924 valid 0.23199261960230375
LOSS train 0.25324976756789924 valid 0.23167980825289702
LOSS train 0.25324976756789924 valid 0.23201070688664913
LOSS train 0.25324976756789924 valid 0.2329974708760657
LOSS train 0.25324976756789924 valid 0.23269400816588176
LOSS train 0.25324976756789924 valid 0.23260168800520342
LOSS train 0.25324976756789924 valid 0.2332088022746823
LOSS train 0.25324976756789924 valid 0.23287947078545887
LOSS train 0.25324976756789924 valid 0.2333018426661906
LOSS train 0.25324976756789924 valid 0.23349834566420696
LOSS train 0.25324976756789924 valid 0.2335570463910699
LOSS train 0.25324976756789924 valid 0.23414649251772432
LOSS train 0.25324976756789924 valid 0.23372605741024016
LOSS train 0.25324976756789924 valid 0.23399738441495335
LOSS train 0.25324976756789924 valid 0.23380302637815475
LOSS train 0.25324976756789924 valid 0.23408684162598736
LOSS train 0.25324976756789924 valid 0.23374299898191733
LOSS train 0.25324976756789924 valid 0.2337437543002042
LOSS train 0.25324976756789924 valid 0.233327985608152
LOSS train 0.25324976756789924 valid 0.23302198292916282
LOSS train 0.25324976756789924 valid 0.23265550927869205
LOSS train 0.25324976756789924 valid 0.23296020192615055
LOSS train 0.25324976756789924 valid 0.2323400266468525
LOSS train 0.25324976756789924 valid 0.2325130288229614
LOSS train 0.25324976756789924 valid 0.2333110347390175
LOSS train 0.25324976756789924 valid 0.23387978781783392
LOSS train 0.25324976756789924 valid 0.23472866020165384
LOSS train 0.25324976756789924 valid 0.2348499453984774
LOSS train 0.25324976756789924 valid 0.23491750341473203
LOSS train 0.25324976756789924 valid 0.23439385397220724
LOSS train 0.25324976756789924 valid 0.2342039092498667
LOSS train 0.25324976756789924 valid 0.23353207521680472
LOSS train 0.25324976756789924 valid 0.23365113990647451
LOSS train 0.25324976756789924 valid 0.23358846277418271
LOSS train 0.25324976756789924 valid 0.2337573909511169
LOSS train 0.25324976756789924 valid 0.23363611142929286
LOSS train 0.25324976756789924 valid 0.23363937598627968
LOSS train 0.25324976756789924 valid 0.23368427515029908
LOSS train 0.25324976756789924 valid 0.23407100807679326
LOSS train 0.25324976756789924 valid 0.23442695938147506
LOSS train 0.25324976756789924 valid 0.23441947690951517
LOSS train 0.25324976756789924 valid 0.2344377780262428
LOSS train 0.25324976756789924 valid 0.23375603798776864
LOSS train 0.25324976756789924 valid 0.23296778051205624
LOSS train 0.25324976756789924 valid 0.23318617427494467
LOSS train 0.25324976756789924 valid 0.23290705555174723
LOSS train 0.25324976756789924 valid 0.23275345581628026
LOSS train 0.25324976756789924 valid 0.23239524872863995
LOSS train 0.25324976756789924 valid 0.2317668620583623
LOSS train 0.25324976756789924 valid 0.23183301450877353
LOSS train 0.25324976756789924 valid 0.2313149178569967
LOSS train 0.25324976756789924 valid 0.2313732099667024
LOSS train 0.25324976756789924 valid 0.23165056175655788
LOSS train 0.25324976756789924 valid 0.23177991496337638
LOSS train 0.25324976756789924 valid 0.2318283484681793
LOSS train 0.25324976756789924 valid 0.2315696628503902
LOSS train 0.25324976756789924 valid 0.2318418060845517
LOSS train 0.25324976756789924 valid 0.23138146416137093
LOSS train 0.25324976756789924 valid 0.23153358542670807
LOSS train 0.25324976756789924 valid 0.23162984679040222
LOSS train 0.25324976756789924 valid 0.23156958438304007
LOSS train 0.25324976756789924 valid 0.23174441598280512
LOSS train 0.25324976756789924 valid 0.23192584693431853
LOSS train 0.25324976756789924 valid 0.2321647072192466
LOSS train 0.25324976756789924 valid 0.2322276572094244
LOSS train 0.25324976756789924 valid 0.23239678650805093
LOSS train 0.25324976756789924 valid 0.23229805938899517
LOSS train 0.25324976756789924 valid 0.23230178554852804
LOSS train 0.25324976756789924 valid 0.2325626606086515
LOSS train 0.25324976756789924 valid 0.23232201493789104
LOSS train 0.25324976756789924 valid 0.23239978320068783
LOSS train 0.25324976756789924 valid 0.23289835726449248
LOSS train 0.25324976756789924 valid 0.23323990025303581
LOSS train 0.25324976756789924 valid 0.2328594286699553
LOSS train 0.25324976756789924 valid 0.23262445641947643
LOSS train 0.25324976756789924 valid 0.23257127245970532
LOSS train 0.25324976756789924 valid 0.23248816320770666
LOSS train 0.25324976756789924 valid 0.23234220691349194
LOSS train 0.25324976756789924 valid 0.2325399345878897
LOSS train 0.25324976756789924 valid 0.23287044631110299
LOSS train 0.25324976756789924 valid 0.23272078017057
LOSS train 0.25324976756789924 valid 0.23265267445259735
LOSS train 0.25324976756789924 valid 0.2322568235297998
LOSS train 0.25324976756789924 valid 0.23208465852028082
LOSS train 0.25324976756789924 valid 0.23193346818939584
LOSS train 0.25324976756789924 valid 0.231943480367583
LOSS train 0.25324976756789924 valid 0.23213617263301725
LOSS train 0.25324976756789924 valid 0.23208327341079713
LOSS train 0.25324976756789924 valid 0.2322537326623523
LOSS train 0.25324976756789924 valid 0.23209343439950716
LOSS train 0.25324976756789924 valid 0.23255662596784532
LOSS train 0.25324976756789924 valid 0.23249228229356367
LOSS train 0.25324976756789924 valid 0.23258296996355057
LOSS train 0.25324976756789924 valid 0.23258632705866836
LOSS train 0.25324976756789924 valid 0.23229439917838934
LOSS train 0.25324976756789924 valid 0.23216165445352854
LOSS train 0.25324976756789924 valid 0.23216306306977771
LOSS train 0.25324976756789924 valid 0.23213758258907882
LOSS train 0.25324976756789924 valid 0.23214199229636612
LOSS train 0.25324976756789924 valid 0.23198771672527285
LOSS train 0.25324976756789924 valid 0.23189703839412634
LOSS train 0.25324976756789924 valid 0.2317077399586602
LOSS train 0.25324976756789924 valid 0.23182246940476553
LOSS train 0.25324976756789924 valid 0.2317452185542871
LOSS train 0.25324976756789924 valid 0.23193508680437652
LOSS train 0.25324976756789924 valid 0.2319412254370176
LOSS train 0.25324976756789924 valid 0.23200012184679508
LOSS train 0.25324976756789924 valid 0.2317817758897255
LOSS train 0.25324976756789924 valid 0.23204566752665665
LOSS train 0.25324976756789924 valid 0.23188856233950375
LOSS train 0.25324976756789924 valid 0.23266568407416344
LOSS train 0.25324976756789924 valid 0.23262765863597792
LOSS train 0.25324976756789924 valid 0.23260113428036372
LOSS train 0.25324976756789924 valid 0.23241637726098496
LOSS train 0.25324976756789924 valid 0.23224282696058876
LOSS train 0.25324976756789924 valid 0.23219733719342675
LOSS train 0.25324976756789924 valid 0.23218883419191683
LOSS train 0.25324976756789924 valid 0.23234143468641466
LOSS train 0.25324976756789924 valid 0.2327440333289978
LOSS train 0.25324976756789924 valid 0.23273627849141504
LOSS train 0.25324976756789924 valid 0.2326995388993734
LOSS train 0.25324976756789924 valid 0.23269777852784163
LOSS train 0.25324976756789924 valid 0.23236670587211847
LOSS train 0.25324976756789924 valid 0.23240627329912245
LOSS train 0.25324976756789924 valid 0.232383073296076
LOSS train 0.25324976756789924 valid 0.23235922667877806
LOSS train 0.25324976756789924 valid 0.23222720913770722
LOSS train 0.25324976756789924 valid 0.23200330183361517
LOSS train 0.25324976756789924 valid 0.2322487677615809
LOSS train 0.25324976756789924 valid 0.232364037704325
LOSS train 0.25324976756789924 valid 0.23226148679497696
LOSS train 0.25324976756789924 valid 0.23232311601116812
LOSS train 0.25324976756789924 valid 0.23258842519100975
LOSS train 0.25324976756789924 valid 0.23243940111837888
LOSS train 0.25324976756789924 valid 0.23214299534988958
LOSS train 0.25324976756789924 valid 0.2321330022088365
LOSS train 0.25324976756789924 valid 0.23201374555456228
LOSS train 0.25324976756789924 valid 0.2319935498918806
LOSS train 0.25324976756789924 valid 0.23196092214096675
LOSS train 0.25324976756789924 valid 0.23205729931761315
LOSS train 0.25324976756789924 valid 0.2322026410799348
LOSS train 0.25324976756789924 valid 0.23214364518000427
LOSS train 0.25324976756789924 valid 0.23215559158060287
LOSS train 0.25324976756789924 valid 0.23240924227303564
LOSS train 0.25324976756789924 valid 0.23243482754780695
LOSS train 0.25324976756789924 valid 0.2322409011301447
LOSS train 0.25324976756789924 valid 0.23222324244030143
LOSS train 0.25324976756789924 valid 0.23207882054754206
LOSS train 0.25324976756789924 valid 0.2320764618374968
LOSS train 0.25324976756789924 valid 0.23194367107860545
LOSS train 0.25324976756789924 valid 0.2320198375493922
LOSS train 0.25324976756789924 valid 0.23214174278829464
LOSS train 0.25324976756789924 valid 0.23222299312290393
LOSS train 0.25324976756789924 valid 0.23214436295144844
LOSS train 0.25324976756789924 valid 0.23204613073418537
LOSS train 0.25324976756789924 valid 0.23222152572221708
LOSS train 0.25324976756789924 valid 0.23203685449570724
LOSS train 0.25324976756789924 valid 0.23172258322055522
LOSS train 0.25324976756789924 valid 0.23182424325115827
LOSS train 0.25324976756789924 valid 0.23195037548312075
LOSS train 0.25324976756789924 valid 0.23194692554798993
LOSS train 0.25324976756789924 valid 0.232003098157183
LOSS train 0.25324976756789924 valid 0.2317890080809593
LOSS train 0.25324976756789924 valid 0.23165052135785422
LOSS train 0.25324976756789924 valid 0.23156870340946878
LOSS train 0.25324976756789924 valid 0.2314417390106934
LOSS train 0.25324976756789924 valid 0.23138299314122573
LOSS train 0.25324976756789924 valid 0.23125728870310436
LOSS train 0.25324976756789924 valid 0.2311949451426858
LOSS train 0.25324976756789924 valid 0.23105261197700594
LOSS train 0.25324976756789924 valid 0.23103600819236958
LOSS train 0.25324976756789924 valid 0.23096033702626753
LOSS train 0.25324976756789924 valid 0.23097863112177167
LOSS train 0.25324976756789924 valid 0.23099109113781374
LOSS train 0.25324976756789924 valid 0.23088714051640258
LOSS train 0.25324976756789924 valid 0.23090666000831855
LOSS train 0.25324976756789924 valid 0.23080106121357355
LOSS train 0.25324976756789924 valid 0.23063276132871938
LOSS train 0.25324976756789924 valid 0.23057223256263468
LOSS train 0.25324976756789924 valid 0.23060799990930864
LOSS train 0.25324976756789924 valid 0.23061935795009683
LOSS train 0.25324976756789924 valid 0.2307183390066504
LOSS train 0.25324976756789924 valid 0.23087419203736564
LOSS train 0.25324976756789924 valid 0.230945996973849
LOSS train 0.25324976756789924 valid 0.23090373093748953
LOSS train 0.25324976756789924 valid 0.2310328349270628
LOSS train 0.25324976756789924 valid 0.23115463215591653
LOSS train 0.25324976756789924 valid 0.23114903151988983
LOSS train 0.25324976756789924 valid 0.23125499955057044
LOSS train 0.25324976756789924 valid 0.23154519997241738
LOSS train 0.25324976756789924 valid 0.23154824978688307
LOSS train 0.25324976756789924 valid 0.23159360143815585
LOSS train 0.25324976756789924 valid 0.2316838203564934
LOSS train 0.25324976756789924 valid 0.23171341625642983
LOSS train 0.25324976756789924 valid 0.2317648533217866
LOSS train 0.25324976756789924 valid 0.2316367271887898
LOSS train 0.25324976756789924 valid 0.23166264861057967
LOSS train 0.25324976756789924 valid 0.23172462265542212
LOSS train 0.25324976756789924 valid 0.2316845770357019
LOSS train 0.25324976756789924 valid 0.23168601837590777
LOSS train 0.25324976756789924 valid 0.23172321107958546
LOSS train 0.25324976756789924 valid 0.23163387247957445
LOSS train 0.25324976756789924 valid 0.23152919511000317
LOSS train 0.25324976756789924 valid 0.231667499571915
LOSS train 0.25324976756789924 valid 0.23146495734118233
LOSS train 0.25324976756789924 valid 0.23157732985883092
LOSS train 0.25324976756789924 valid 0.23175508315201665
LOSS train 0.25324976756789924 valid 0.23189602098902878
LOSS train 0.25324976756789924 valid 0.23186662029929278
LOSS train 0.25324976756789924 valid 0.2317805325212749
LOSS train 0.25324976756789924 valid 0.23175854866783466
LOSS train 0.25324976756789924 valid 0.23172160294879393
LOSS train 0.25324976756789924 valid 0.2318266803622246
LOSS train 0.25324976756789924 valid 0.23181511461734772
LOSS train 0.25324976756789924 valid 0.23188223335004987
LOSS train 0.25324976756789924 valid 0.23175146139186362
LOSS train 0.25324976756789924 valid 0.2317505186115663
LOSS train 0.25324976756789924 valid 0.23180463442615434
LOSS train 0.25324976756789924 valid 0.23188933089841157
LOSS train 0.25324976756789924 valid 0.23176148591802279
LOSS train 0.25324976756789924 valid 0.23167142006315927
LOSS train 0.25324976756789924 valid 0.23163498414529338
LOSS train 0.25324976756789924 valid 0.2316473940244088
LOSS train 0.25324976756789924 valid 0.23172017098386627
LOSS train 0.25324976756789924 valid 0.2317174437050601
LOSS train 0.25324976756789924 valid 0.23168966655722137
LOSS train 0.25324976756789924 valid 0.2317161113588196
LOSS train 0.25324976756789924 valid 0.23162899191649455
LOSS train 0.25324976756789924 valid 0.23158981426990122
LOSS train 0.25324976756789924 valid 0.23171036090520436
LOSS train 0.25324976756789924 valid 0.23187880534956704
LOSS train 0.25324976756789924 valid 0.23196021614243106
LOSS train 0.25324976756789924 valid 0.23193717334005567
LOSS train 0.25324976756789924 valid 0.23194325311157538
LOSS train 0.25324976756789924 valid 0.2321447704863899
LOSS train 0.25324976756789924 valid 0.2321314798606621
LOSS train 0.25324976756789924 valid 0.2321115641372047
LOSS train 0.25324976756789924 valid 0.2320369842377576
LOSS train 0.25324976756789924 valid 0.23197999332046162
LOSS train 0.25324976756789924 valid 0.2318029915275126
LOSS train 0.25324976756789924 valid 0.2316275807164556
LOSS train 0.25324976756789924 valid 0.23163614900095061
LOSS train 0.25324976756789924 valid 0.2316166208258697
LOSS train 0.25324976756789924 valid 0.23159230312206566
LOSS train 0.25324976756789924 valid 0.23142588323523813
LOSS train 0.25324976756789924 valid 0.23143601917757162
LOSS train 0.25324976756789924 valid 0.2314949912924162
LOSS train 0.25324976756789924 valid 0.23150705177533
LOSS train 0.25324976756789924 valid 0.23141648438010182
LOSS train 0.25324976756789924 valid 0.23134014649050577
LOSS train 0.25324976756789924 valid 0.23138950827221075
LOSS train 0.25324976756789924 valid 0.2314255364935291
LOSS train 0.25324976756789924 valid 0.23144502023170735
LOSS train 0.25324976756789924 valid 0.23145401744088767
LOSS train 0.25324976756789924 valid 0.2314791526798516
LOSS train 0.25324976756789924 valid 0.2314733411264908
LOSS train 0.25324976756789924 valid 0.23159910657373414
LOSS train 0.25324976756789924 valid 0.23166792887752338
LOSS train 0.25324976756789924 valid 0.23163040752547817
LOSS train 0.25324976756789924 valid 0.23164164006509363
LOSS train 0.25324976756789924 valid 0.23166646248342207
LOSS train 0.25324976756789924 valid 0.231734418101534
LOSS train 0.25324976756789924 valid 0.23170075009266536
LOSS train 0.25324976756789924 valid 0.23179136647338486
LOSS train 0.25324976756789924 valid 0.23177093997696377
LOSS train 0.25324976756789924 valid 0.23186704978691075
LOSS train 0.25324976756789924 valid 0.23187354246252462
LOSS train 0.25324976756789924 valid 0.23177649857567958
LOSS train 0.25324976756789924 valid 0.2316687203018494
LOSS train 0.25324976756789924 valid 0.23162221282429338
LOSS train 0.25324976756789924 valid 0.2315346427842394
LOSS train 0.25324976756789924 valid 0.23164869526636253
LOSS train 0.25324976756789924 valid 0.2316735071039969
LOSS train 0.25324976756789924 valid 0.23154120300552086
LOSS train 0.25324976756789924 valid 0.23159628862944934
LOSS train 0.25324976756789924 valid 0.2315919653009683
LOSS train 0.25324976756789924 valid 0.23164049839707695
LOSS train 0.25324976756789924 valid 0.2316573151993373
LOSS train 0.25324976756789924 valid 0.23154140876818308
LOSS train 0.25324976756789924 valid 0.23154997679901423
LOSS train 0.25324976756789924 valid 0.23158070210765744
LOSS train 0.25324976756789924 valid 0.23166151087859582
LOSS train 0.25324976756789924 valid 0.23163346350193023
LOSS train 0.25324976756789924 valid 0.23172504285414272
LOSS train 0.25324976756789924 valid 0.23169079802421308
LOSS train 0.25324976756789924 valid 0.23166684464958062
LOSS train 0.25324976756789924 valid 0.23170162976523975
LOSS train 0.25324976756789924 valid 0.23163357092784
LOSS train 0.25324976756789924 valid 0.2317971417143301
LOSS train 0.25324976756789924 valid 0.23190336416985283
LOSS train 0.25324976756789924 valid 0.2318970222571274
LOSS train 0.25324976756789924 valid 0.23202014870737825
LOSS train 0.25324976756789924 valid 0.23200189837000587
LOSS train 0.25324976756789924 valid 0.23196216904504782
LOSS train 0.25324976756789924 valid 0.23186461640948272
LOSS train 0.25324976756789924 valid 0.23184391314739938
LOSS train 0.25324976756789924 valid 0.23197038752768567
LOSS train 0.25324976756789924 valid 0.23197771559900313
LOSS train 0.25324976756789924 valid 0.23200344019347713
LOSS train 0.25324976756789924 valid 0.23187565759310735
LOSS train 0.25324976756789924 valid 0.23183308854787307
LOSS train 0.25324976756789924 valid 0.23186256009041384
LOSS train 0.25324976756789924 valid 0.23191672120900714
LOSS train 0.25324976756789924 valid 0.2318292056535346
LOSS train 0.25324976756789924 valid 0.23182999896027193
LOSS train 0.25324976756789924 valid 0.23178832367627336
LOSS train 0.25324976756789924 valid 0.23194924802627676
LOSS train 0.25324976756789924 valid 0.23195680207100466
LOSS train 0.25324976756789924 valid 0.2318541373131592
LOSS train 0.25324976756789924 valid 0.2318464052093132
LOSS train 0.25324976756789924 valid 0.23180728525608435
LOSS train 0.25324976756789924 valid 0.2319238916850705
LOSS train 0.25324976756789924 valid 0.23179044974701746
LOSS train 0.25324976756789924 valid 0.23172262357680545
LOSS train 0.25324976756789924 valid 0.23162454210052436
LOSS train 0.25324976756789924 valid 0.2317076725172929
LOSS train 0.25324976756789924 valid 0.23177319676694225
LOSS train 0.25324976756789924 valid 0.23189251267574204
LOSS train 0.25324976756789924 valid 0.23191453453697516
LOSS train 0.25324976756789924 valid 0.23189207573397821
LOSS train 0.25324976756789924 valid 0.23181307278365396
LOSS train 0.25324976756789924 valid 0.23180922849264649
LOSS train 0.25324976756789924 valid 0.2317247723125749
LOSS train 0.25324976756789924 valid 0.23166166547262768
LOSS train 0.25324976756789924 valid 0.2317582167973176
LOSS train 0.25324976756789924 valid 0.23160583421218495
LOSS train 0.25324976756789924 valid 0.2315617065344538
LOSS train 0.25324976756789924 valid 0.23152985429927095
LOSS train 0.25324976756789924 valid 0.23147588732138358
LOSS train 0.25324976756789924 valid 0.23137813960823767
LOSS train 0.25324976756789924 valid 0.23135447878714488
LOSS train 0.25324976756789924 valid 0.2313923188018282
EPOCH 13:
  batch 1 loss: 0.2613891363143921
  batch 2 loss: 0.2550777345895767
  batch 3 loss: 0.2699791093667348
  batch 4 loss: 0.2695663794875145
  batch 5 loss: 0.2720224499702454
  batch 6 loss: 0.26849099000295
  batch 7 loss: 0.2681847597871508
  batch 8 loss: 0.26791657134890556
  batch 9 loss: 0.26858995689286125
  batch 10 loss: 0.26854006946086884
  batch 11 loss: 0.2671785625544461
  batch 12 loss: 0.2627236309150855
  batch 13 loss: 0.26129307540563435
  batch 14 loss: 0.2603369227477482
  batch 15 loss: 0.26092313329378763
  batch 16 loss: 0.25969316996634007
  batch 17 loss: 0.25730054255794077
  batch 18 loss: 0.2575460034939978
  batch 19 loss: 0.2560881627233405
  batch 20 loss: 0.2541734792292118
  batch 21 loss: 0.25343632059437887
  batch 22 loss: 0.25321828709407285
  batch 23 loss: 0.2517783019853675
  batch 24 loss: 0.2503686137497425
  batch 25 loss: 0.25194262504577636
  batch 26 loss: 0.2502915274638396
  batch 27 loss: 0.24954050448205736
  batch 28 loss: 0.24784473329782486
  batch 29 loss: 0.24768284271503316
  batch 30 loss: 0.24719230234622955
  batch 31 loss: 0.24816148415688546
  batch 32 loss: 0.24772347696125507
  batch 33 loss: 0.24713215773755853
  batch 34 loss: 0.24733278944211848
  batch 35 loss: 0.2472124755382538
  batch 36 loss: 0.24673384841945437
  batch 37 loss: 0.246660986864889
  batch 38 loss: 0.24761494641241275
  batch 39 loss: 0.24684327420515892
  batch 40 loss: 0.24681484438478946
  batch 41 loss: 0.24651670310555435
  batch 42 loss: 0.24683760461353121
  batch 43 loss: 0.24637213938458022
  batch 44 loss: 0.2462313405492089
  batch 45 loss: 0.24613799452781676
  batch 46 loss: 0.2462374166302059
  batch 47 loss: 0.24645939786383447
  batch 48 loss: 0.24550737999379635
  batch 49 loss: 0.24573488138159927
  batch 50 loss: 0.2454660415649414
  batch 51 loss: 0.24535007219688565
  batch 52 loss: 0.24545983339731509
  batch 53 loss: 0.24467452517095603
  batch 54 loss: 0.2443691122311133
  batch 55 loss: 0.24327797808430413
  batch 56 loss: 0.24273870193532535
  batch 57 loss: 0.241912386396475
  batch 58 loss: 0.2419893798129312
  batch 59 loss: 0.24201557999950343
  batch 60 loss: 0.241396447767814
  batch 61 loss: 0.2414992407697146
  batch 62 loss: 0.2414267214555894
  batch 63 loss: 0.24072305787177312
  batch 64 loss: 0.24093789886683226
  batch 65 loss: 0.2405814679769369
  batch 66 loss: 0.24057514197898633
  batch 67 loss: 0.24131837932031547
  batch 68 loss: 0.24150383866885128
  batch 69 loss: 0.24167045043862384
  batch 70 loss: 0.2419189534017018
  batch 71 loss: 0.2418118505410745
  batch 72 loss: 0.24203111562463972
  batch 73 loss: 0.24187591590293467
  batch 74 loss: 0.24226776047332868
  batch 75 loss: 0.24177819927533467
  batch 76 loss: 0.24208449846819827
  batch 77 loss: 0.2421102032258913
  batch 78 loss: 0.2418857782314985
  batch 79 loss: 0.2422365353831762
  batch 80 loss: 0.24225467406213283
  batch 81 loss: 0.24250771123685955
  batch 82 loss: 0.24304369346397678
  batch 83 loss: 0.24317953241876808
  batch 84 loss: 0.24339756724380313
  batch 85 loss: 0.24333869779811185
  batch 86 loss: 0.24375358882338502
  batch 87 loss: 0.24383047017557868
  batch 88 loss: 0.2435587387193333
  batch 89 loss: 0.24358701069703262
  batch 90 loss: 0.24352636420064502
  batch 91 loss: 0.2438551529750719
  batch 92 loss: 0.2437971870860328
  batch 93 loss: 0.24346878772140831
  batch 94 loss: 0.2436932664602361
  batch 95 loss: 0.24351658523082734
  batch 96 loss: 0.2435539442424973
  batch 97 loss: 0.24383633714361289
  batch 98 loss: 0.24419241140083392
  batch 99 loss: 0.24442977495867796
  batch 100 loss: 0.2445192241668701
  batch 101 loss: 0.24455875025527313
  batch 102 loss: 0.2450403680696207
  batch 103 loss: 0.2454042262533336
  batch 104 loss: 0.2455590792859976
  batch 105 loss: 0.245314817627271
  batch 106 loss: 0.24589933075432507
  batch 107 loss: 0.24546427063852827
  batch 108 loss: 0.2453434826047332
  batch 109 loss: 0.24515202928573712
  batch 110 loss: 0.2449797662821683
  batch 111 loss: 0.2449075125896179
  batch 112 loss: 0.2446157102073942
  batch 113 loss: 0.24489451325045222
  batch 114 loss: 0.245253418621264
  batch 115 loss: 0.24531983111215674
  batch 116 loss: 0.24552931379655313
  batch 117 loss: 0.24594878972086132
  batch 118 loss: 0.24592818547103365
  batch 119 loss: 0.24611591867038182
  batch 120 loss: 0.24627135867873828
  batch 121 loss: 0.24626520349959696
  batch 122 loss: 0.24621257076009376
  batch 123 loss: 0.24590605763885065
  batch 124 loss: 0.24618285101267598
  batch 125 loss: 0.24595113575458527
  batch 126 loss: 0.24577895883056852
  batch 127 loss: 0.24627433130590934
  batch 128 loss: 0.24650230177212507
  batch 129 loss: 0.24674790577833042
  batch 130 loss: 0.24706202367177377
  batch 131 loss: 0.24745123725356036
  batch 132 loss: 0.24768876002141924
  batch 133 loss: 0.2479065879619211
  batch 134 loss: 0.24813879014395956
  batch 135 loss: 0.24843081511833048
  batch 136 loss: 0.24859708887250984
  batch 137 loss: 0.24854322626207867
  batch 138 loss: 0.2488858148023702
  batch 139 loss: 0.24930847966842515
  batch 140 loss: 0.24923580916864532
  batch 141 loss: 0.24941241307884243
  batch 142 loss: 0.24950625054853062
  batch 143 loss: 0.24933673055855543
  batch 144 loss: 0.24930106206900543
  batch 145 loss: 0.24923865363515657
  batch 146 loss: 0.24945609475651834
  batch 147 loss: 0.24996324420786228
  batch 148 loss: 0.24997211368502797
  batch 149 loss: 0.2501370224776684
  batch 150 loss: 0.25048935910065967
  batch 151 loss: 0.25091093561507216
  batch 152 loss: 0.2509729787707329
  batch 153 loss: 0.2507687560678308
  batch 154 loss: 0.25138387732304535
  batch 155 loss: 0.25130417289272433
  batch 156 loss: 0.25141892047264636
  batch 157 loss: 0.25217812285301794
  batch 158 loss: 0.2523748872778084
  batch 159 loss: 0.2524845169025397
  batch 160 loss: 0.2526610093191266
  batch 161 loss: 0.2530771551295097
  batch 162 loss: 0.2533717849004416
  batch 163 loss: 0.2535793967773578
  batch 164 loss: 0.2539114897570959
  batch 165 loss: 0.2541470858183774
  batch 166 loss: 0.25427183293434513
  batch 167 loss: 0.254219901686657
  batch 168 loss: 0.2544266614353373
  batch 169 loss: 0.2542892205115606
  batch 170 loss: 0.25420126020908357
  batch 171 loss: 0.2542376446793651
  batch 172 loss: 0.25439975220103594
  batch 173 loss: 0.25448975277084834
  batch 174 loss: 0.25464552521020517
  batch 175 loss: 0.2546319556236267
  batch 176 loss: 0.2546304808082906
  batch 177 loss: 0.25479214339606504
  batch 178 loss: 0.2548678649610348
  batch 179 loss: 0.2551338171492742
  batch 180 loss: 0.25524230781528684
  batch 181 loss: 0.25526347558801343
  batch 182 loss: 0.2552288320365843
  batch 183 loss: 0.25504867366102874
  batch 184 loss: 0.2549420226689266
  batch 185 loss: 0.25510775068321745
  batch 186 loss: 0.2551302636663119
  batch 187 loss: 0.2550519141443273
  batch 188 loss: 0.2549033853760425
  batch 189 loss: 0.2547952795312518
  batch 190 loss: 0.25479582508927895
  batch 191 loss: 0.25475487518685025
  batch 192 loss: 0.25481072238956887
  batch 193 loss: 0.25488240376037635
  batch 194 loss: 0.25508079762311325
  batch 195 loss: 0.25515099305372974
  batch 196 loss: 0.25496277365149284
  batch 197 loss: 0.2549111078384564
  batch 198 loss: 0.2550662749192931
  batch 199 loss: 0.2550638771087081
  batch 200 loss: 0.255127015337348
  batch 201 loss: 0.2551303060345389
  batch 202 loss: 0.2550686288735654
  batch 203 loss: 0.25496604179807486
  batch 204 loss: 0.2547501485575648
  batch 205 loss: 0.2548720039245559
  batch 206 loss: 0.2547819500843298
  batch 207 loss: 0.2547389332223054
  batch 208 loss: 0.2547336235069312
  batch 209 loss: 0.2544815563128896
  batch 210 loss: 0.2545430061363039
  batch 211 loss: 0.25437440154676755
  batch 212 loss: 0.2543388671188984
  batch 213 loss: 0.25429967997219644
  batch 214 loss: 0.254320986360033
  batch 215 loss: 0.2540346589199332
  batch 216 loss: 0.253996509782694
  batch 217 loss: 0.25396449259623954
  batch 218 loss: 0.2538550176204891
  batch 219 loss: 0.253877467216422
  batch 220 loss: 0.25393069562586873
  batch 221 loss: 0.2539292896225442
  batch 222 loss: 0.25395626742560584
  batch 223 loss: 0.2539256732292774
  batch 224 loss: 0.2538137803120272
  batch 225 loss: 0.2537354622946845
  batch 226 loss: 0.25384465163260433
  batch 227 loss: 0.2535589131227149
  batch 228 loss: 0.2533762449104535
  batch 229 loss: 0.2532559480339159
  batch 230 loss: 0.25321508263764175
  batch 231 loss: 0.2530941338121117
  batch 232 loss: 0.2529024059145615
  batch 233 loss: 0.25281435897166127
  batch 234 loss: 0.2528524884683454
  batch 235 loss: 0.25277728368627267
  batch 236 loss: 0.25266285909939623
  batch 237 loss: 0.2526892177163297
  batch 238 loss: 0.2526588808713841
  batch 239 loss: 0.2525934683603223
  batch 240 loss: 0.2527438610171278
  batch 241 loss: 0.25279221019062265
  batch 242 loss: 0.252751603291547
  batch 243 loss: 0.2527058207326465
  batch 244 loss: 0.2526060999905477
  batch 245 loss: 0.2525430033401567
  batch 246 loss: 0.2524191149608876
  batch 247 loss: 0.2523511246389706
  batch 248 loss: 0.2523269381133779
  batch 249 loss: 0.25228944277188864
  batch 250 loss: 0.2520785974264145
  batch 251 loss: 0.2520679478863796
  batch 252 loss: 0.25196011104280985
  batch 253 loss: 0.25188135994752875
  batch 254 loss: 0.25177371572321794
  batch 255 loss: 0.2517619757091298
  batch 256 loss: 0.2518105849158019
  batch 257 loss: 0.25191291540513244
  batch 258 loss: 0.25185212614231334
  batch 259 loss: 0.2519032564517614
  batch 260 loss: 0.2520485860223953
  batch 261 loss: 0.25210934323597683
  batch 262 loss: 0.2520917025572471
  batch 263 loss: 0.25209448653708844
  batch 264 loss: 0.2519732987451734
  batch 265 loss: 0.2520112836698316
  batch 266 loss: 0.25203634994594676
  batch 267 loss: 0.2519997006721711
  batch 268 loss: 0.25193950500506074
  batch 269 loss: 0.25186907259061875
  batch 270 loss: 0.2519163487134156
  batch 271 loss: 0.2517984755804618
  batch 272 loss: 0.25182489108513384
  batch 273 loss: 0.25176528565612905
  batch 274 loss: 0.25192932534391865
  batch 275 loss: 0.25198104305700825
  batch 276 loss: 0.25203437790058664
  batch 277 loss: 0.25206331752697914
  batch 278 loss: 0.2520045989708935
  batch 279 loss: 0.2520853400657681
  batch 280 loss: 0.2520542461425066
  batch 281 loss: 0.2520275531714497
  batch 282 loss: 0.25208622227746547
  batch 283 loss: 0.2520304564452424
  batch 284 loss: 0.2519740727795681
  batch 285 loss: 0.25196215693365065
  batch 286 loss: 0.2517936254193733
  batch 287 loss: 0.25177905861716654
  batch 288 loss: 0.2515918353262047
  batch 289 loss: 0.2517156658184982
  batch 290 loss: 0.25146564368543955
  batch 291 loss: 0.2513878624054165
  batch 292 loss: 0.25135014012251816
  batch 293 loss: 0.25135391737007035
  batch 294 loss: 0.25128213762223317
  batch 295 loss: 0.2512224093332129
  batch 296 loss: 0.2512816426319045
  batch 297 loss: 0.2513165959605464
  batch 298 loss: 0.2513163790806828
  batch 299 loss: 0.25117757926417833
  batch 300 loss: 0.25125257670879364
  batch 301 loss: 0.25125672215243117
  batch 302 loss: 0.25125406494993247
  batch 303 loss: 0.25128958917687044
  batch 304 loss: 0.2512024293996786
  batch 305 loss: 0.2510628914735356
  batch 306 loss: 0.2510961931612756
  batch 307 loss: 0.250998358627484
  batch 308 loss: 0.25096836444232373
  batch 309 loss: 0.25102514877288473
  batch 310 loss: 0.25094057081207155
  batch 311 loss: 0.25099248498965687
  batch 312 loss: 0.25119775142043066
  batch 313 loss: 0.2512507139684293
  batch 314 loss: 0.25121770922545417
  batch 315 loss: 0.25118057737274774
  batch 316 loss: 0.25118478368731995
  batch 317 loss: 0.25116770519443116
  batch 318 loss: 0.25114648048795246
  batch 319 loss: 0.25115525484271933
  batch 320 loss: 0.2511117022950202
  batch 321 loss: 0.25111175126561497
  batch 322 loss: 0.25106482480808817
  batch 323 loss: 0.25100918709309106
  batch 324 loss: 0.2508251420509668
  batch 325 loss: 0.25071416753989
  batch 326 loss: 0.2507260585306612
  batch 327 loss: 0.25081413665313607
  batch 328 loss: 0.2506264699395837
  batch 329 loss: 0.2507194547395938
  batch 330 loss: 0.250599736652591
  batch 331 loss: 0.2505627888567138
  batch 332 loss: 0.2504319506865668
  batch 333 loss: 0.2505365126394295
  batch 334 loss: 0.25051060510788137
  batch 335 loss: 0.2503861989992768
  batch 336 loss: 0.2503169974134791
  batch 337 loss: 0.2501498433797225
  batch 338 loss: 0.25013313565910217
  batch 339 loss: 0.25000860403596825
  batch 340 loss: 0.2499627877245931
  batch 341 loss: 0.24980822699335664
  batch 342 loss: 0.24976773146126005
  batch 343 loss: 0.24973595633277393
  batch 344 loss: 0.24974933487558088
  batch 345 loss: 0.24987501169460408
  batch 346 loss: 0.24980369016441994
  batch 347 loss: 0.24986548773321707
  batch 348 loss: 0.24988631874151612
  batch 349 loss: 0.24982736654643684
  batch 350 loss: 0.24986062624624797
  batch 351 loss: 0.24985255793458716
  batch 352 loss: 0.24993780047886752
  batch 353 loss: 0.24998533603321055
  batch 354 loss: 0.250082960110263
  batch 355 loss: 0.25010923923740924
  batch 356 loss: 0.2501345605411556
  batch 357 loss: 0.25001977200434655
  batch 358 loss: 0.2500805659583827
  batch 359 loss: 0.2501345178460012
  batch 360 loss: 0.2500767561296622
  batch 361 loss: 0.2500149085085808
  batch 362 loss: 0.24996940733978104
  batch 363 loss: 0.2498902082032737
  batch 364 loss: 0.2498204597568774
  batch 365 loss: 0.2497746622317458
  batch 366 loss: 0.24966168680477663
  batch 367 loss: 0.24956124870751145
  batch 368 loss: 0.24948962676622297
  batch 369 loss: 0.2495370214466803
  batch 370 loss: 0.24946402516719457
  batch 371 loss: 0.2494538625215263
  batch 372 loss: 0.24935182524464464
  batch 373 loss: 0.24920309936233245
  batch 374 loss: 0.24912056099762891
  batch 375 loss: 0.2490236004193624
  batch 376 loss: 0.2489470833555815
  batch 377 loss: 0.24890245513036965
  batch 378 loss: 0.2488071259367403
  batch 379 loss: 0.24890933140601207
  batch 380 loss: 0.2489416659662598
  batch 381 loss: 0.24885856940990358
  batch 382 loss: 0.24873166297274735
  batch 383 loss: 0.24863506589016776
  batch 384 loss: 0.2486045880941674
  batch 385 loss: 0.24866154437715357
  batch 386 loss: 0.24850165851684433
  batch 387 loss: 0.2484875048007768
  batch 388 loss: 0.24862473580947855
  batch 389 loss: 0.2485072683246093
  batch 390 loss: 0.24845494192380171
  batch 391 loss: 0.2484789410667956
  batch 392 loss: 0.24850082199792473
  batch 393 loss: 0.24860780597036425
  batch 394 loss: 0.24873600128640982
  batch 395 loss: 0.2486301788423635
  batch 396 loss: 0.24857440572043862
  batch 397 loss: 0.24857969702190957
  batch 398 loss: 0.24861072501794776
  batch 399 loss: 0.24861447094825276
  batch 400 loss: 0.2486432258412242
  batch 401 loss: 0.2485769270289866
  batch 402 loss: 0.24858639248419756
  batch 403 loss: 0.2485831055244796
  batch 404 loss: 0.248619179693189
  batch 405 loss: 0.24863786579650127
  batch 406 loss: 0.2487885438162705
  batch 407 loss: 0.2487529228448282
  batch 408 loss: 0.2488268149980143
  batch 409 loss: 0.2488613100244247
  batch 410 loss: 0.248902317736207
  batch 411 loss: 0.24886844928972332
  batch 412 loss: 0.24880491106689556
  batch 413 loss: 0.24885962925147778
  batch 414 loss: 0.24887338291475739
  batch 415 loss: 0.24886533065014574
  batch 416 loss: 0.2488986152009322
  batch 417 loss: 0.24884254108372925
  batch 418 loss: 0.2487924446186951
  batch 419 loss: 0.24870904024717064
  batch 420 loss: 0.2487184746279603
  batch 421 loss: 0.248636319348478
  batch 422 loss: 0.24875501589187513
  batch 423 loss: 0.24879902488514605
  batch 424 loss: 0.24876120018790354
  batch 425 loss: 0.24882299914079553
  batch 426 loss: 0.24875888252901918
  batch 427 loss: 0.24880625174950102
  batch 428 loss: 0.24879132907524287
  batch 429 loss: 0.2488184548341311
  batch 430 loss: 0.24879677049642385
  batch 431 loss: 0.24897328990656373
  batch 432 loss: 0.24904089238218688
  batch 433 loss: 0.24906389312022828
  batch 434 loss: 0.24912933177799673
  batch 435 loss: 0.2490571913705475
  batch 436 loss: 0.24908307888502373
  batch 437 loss: 0.24919125720756277
  batch 438 loss: 0.2493653398466437
  batch 439 loss: 0.24939877422768325
  batch 440 loss: 0.24941535033285617
  batch 441 loss: 0.24938466320502786
  batch 442 loss: 0.24935926490239968
  batch 443 loss: 0.24936064620319393
  batch 444 loss: 0.24926660975088943
  batch 445 loss: 0.24935866098725393
  batch 446 loss: 0.24939525294464265
  batch 447 loss: 0.24936668401463186
  batch 448 loss: 0.2493161710777453
  batch 449 loss: 0.24939635780978042
  batch 450 loss: 0.2493747369448344
  batch 451 loss: 0.24934828777006618
  batch 452 loss: 0.24927688970238762
  batch 453 loss: 0.2493088576967353
  batch 454 loss: 0.24940506103017782
  batch 455 loss: 0.24941038174943608
  batch 456 loss: 0.2494370753697136
  batch 457 loss: 0.24955321512545747
  batch 458 loss: 0.24955812480512143
  batch 459 loss: 0.2496398127676355
  batch 460 loss: 0.24982071110735768
  batch 461 loss: 0.24986579872005157
  batch 462 loss: 0.249912627589651
  batch 463 loss: 0.24994748895163155
  batch 464 loss: 0.2500074183375671
  batch 465 loss: 0.24994675875991904
  batch 466 loss: 0.2498751342808228
  batch 467 loss: 0.24997352251914634
  batch 468 loss: 0.250080765503594
  batch 469 loss: 0.25017086694489665
  batch 470 loss: 0.2501847394603364
  batch 471 loss: 0.25025963194810663
  batch 472 loss: 0.2501749948803651
LOSS train 0.2501749948803651 valid 0.29362916946411133
LOSS train 0.2501749948803651 valid 0.2727973163127899
LOSS train 0.2501749948803651 valid 0.2763429383436839
LOSS train 0.2501749948803651 valid 0.2696745991706848
LOSS train 0.2501749948803651 valid 0.25944340527057647
LOSS train 0.2501749948803651 valid 0.2649914249777794
LOSS train 0.2501749948803651 valid 0.2743382304906845
LOSS train 0.2501749948803651 valid 0.2705148532986641
LOSS train 0.2501749948803651 valid 0.27399031983481514
LOSS train 0.2501749948803651 valid 0.2752198874950409
LOSS train 0.2501749948803651 valid 0.2705122164704583
LOSS train 0.2501749948803651 valid 0.2721499813099702
LOSS train 0.2501749948803651 valid 0.271100898201649
LOSS train 0.2501749948803651 valid 0.2724677249789238
LOSS train 0.2501749948803651 valid 0.2697577625513077
LOSS train 0.2501749948803651 valid 0.2699517412111163
LOSS train 0.2501749948803651 valid 0.2703314590103486
LOSS train 0.2501749948803651 valid 0.2720813925067584
LOSS train 0.2501749948803651 valid 0.27454674479208496
LOSS train 0.2501749948803651 valid 0.27413603141903875
LOSS train 0.2501749948803651 valid 0.2736197773899351
LOSS train 0.2501749948803651 valid 0.2720038030635227
LOSS train 0.2501749948803651 valid 0.27341399309427844
LOSS train 0.2501749948803651 valid 0.27257988167305786
LOSS train 0.2501749948803651 valid 0.2711484932899475
LOSS train 0.2501749948803651 valid 0.2727422507909628
LOSS train 0.2501749948803651 valid 0.27208854423628914
LOSS train 0.2501749948803651 valid 0.2722714277250426
LOSS train 0.2501749948803651 valid 0.2721690155308822
LOSS train 0.2501749948803651 valid 0.27403405209382375
LOSS train 0.2501749948803651 valid 0.2750961261410867
LOSS train 0.2501749948803651 valid 0.27501253318041563
LOSS train 0.2501749948803651 valid 0.2757004535559452
LOSS train 0.2501749948803651 valid 0.27564382816062255
LOSS train 0.2501749948803651 valid 0.2770238195146833
LOSS train 0.2501749948803651 valid 0.27676571077770656
LOSS train 0.2501749948803651 valid 0.27681922751504023
LOSS train 0.2501749948803651 valid 0.27716087275429774
LOSS train 0.2501749948803651 valid 0.2768821670458867
LOSS train 0.2501749948803651 valid 0.27724759206175803
LOSS train 0.2501749948803651 valid 0.27804685147797187
LOSS train 0.2501749948803651 valid 0.2776784520773661
LOSS train 0.2501749948803651 valid 0.2775421045547308
LOSS train 0.2501749948803651 valid 0.2783572192896496
LOSS train 0.2501749948803651 valid 0.27789929641617667
LOSS train 0.2501749948803651 valid 0.2785944504582364
LOSS train 0.2501749948803651 valid 0.27880099606006703
LOSS train 0.2501749948803651 valid 0.2788419791807731
LOSS train 0.2501749948803651 valid 0.27946213860901037
LOSS train 0.2501749948803651 valid 0.2789400887489319
LOSS train 0.2501749948803651 valid 0.27936239102307486
LOSS train 0.2501749948803651 valid 0.27921301470353055
LOSS train 0.2501749948803651 valid 0.2794651659029835
LOSS train 0.2501749948803651 valid 0.2790408167574141
LOSS train 0.2501749948803651 valid 0.27903799468820745
LOSS train 0.2501749948803651 valid 0.2785028448062284
LOSS train 0.2501749948803651 valid 0.2781179542081398
LOSS train 0.2501749948803651 valid 0.2777669768908928
LOSS train 0.2501749948803651 valid 0.27811295450743984
LOSS train 0.2501749948803651 valid 0.27744759023189547
LOSS train 0.2501749948803651 valid 0.2777377586872851
LOSS train 0.2501749948803651 valid 0.2787647896235989
LOSS train 0.2501749948803651 valid 0.27944945153735934
LOSS train 0.2501749948803651 valid 0.2803864083252847
LOSS train 0.2501749948803651 valid 0.2805516155866476
LOSS train 0.2501749948803651 valid 0.28068767758932983
LOSS train 0.2501749948803651 valid 0.280092176883968
LOSS train 0.2501749948803651 valid 0.27985847412663345
LOSS train 0.2501749948803651 valid 0.27903275494126306
LOSS train 0.2501749948803651 valid 0.2790406872119222
LOSS train 0.2501749948803651 valid 0.27900134068979343
LOSS train 0.2501749948803651 valid 0.2792408306979471
LOSS train 0.2501749948803651 valid 0.27907141328674473
LOSS train 0.2501749948803651 valid 0.2791169243487152
LOSS train 0.2501749948803651 valid 0.27927452663580576
LOSS train 0.2501749948803651 valid 0.27976335840005623
LOSS train 0.2501749948803651 valid 0.28005269537498423
LOSS train 0.2501749948803651 valid 0.28001840775593734
LOSS train 0.2501749948803651 valid 0.28004921331435817
LOSS train 0.2501749948803651 valid 0.27923869285732505
LOSS train 0.2501749948803651 valid 0.27830134865678385
LOSS train 0.2501749948803651 valid 0.2784305723702035
LOSS train 0.2501749948803651 valid 0.2781256369079452
LOSS train 0.2501749948803651 valid 0.27808165018047604
LOSS train 0.2501749948803651 valid 0.27758635615601257
LOSS train 0.2501749948803651 valid 0.2768768221139908
LOSS train 0.2501749948803651 valid 0.27707962914444934
LOSS train 0.2501749948803651 valid 0.27646976573900744
LOSS train 0.2501749948803651 valid 0.27666857101944053
LOSS train 0.2501749948803651 valid 0.2769097990459866
LOSS train 0.2501749948803651 valid 0.2770730892380515
LOSS train 0.2501749948803651 valid 0.2770589415145957
LOSS train 0.2501749948803651 valid 0.2768542779389248
LOSS train 0.2501749948803651 valid 0.2771280528383052
LOSS train 0.2501749948803651 valid 0.27666105377046685
LOSS train 0.2501749948803651 valid 0.2769052016859253
LOSS train 0.2501749948803651 valid 0.2769699563685152
LOSS train 0.2501749948803651 valid 0.27693534961768557
LOSS train 0.2501749948803651 valid 0.27716819324878733
LOSS train 0.2501749948803651 valid 0.2774342212080956
LOSS train 0.2501749948803651 valid 0.2777087464190946
LOSS train 0.2501749948803651 valid 0.277745175595377
LOSS train 0.2501749948803651 valid 0.27791958700105984
LOSS train 0.2501749948803651 valid 0.27783863361065203
LOSS train 0.2501749948803651 valid 0.2778681570575351
LOSS train 0.2501749948803651 valid 0.2781466546486009
LOSS train 0.2501749948803651 valid 0.2778333296007085
LOSS train 0.2501749948803651 valid 0.27787658806752275
LOSS train 0.2501749948803651 valid 0.27844556550913996
LOSS train 0.2501749948803651 valid 0.2787761220877821
LOSS train 0.2501749948803651 valid 0.2783033525621569
LOSS train 0.2501749948803651 valid 0.2779965126620872
LOSS train 0.2501749948803651 valid 0.27794792219600845
LOSS train 0.2501749948803651 valid 0.27788615252887994
LOSS train 0.2501749948803651 valid 0.27775239218836245
LOSS train 0.2501749948803651 valid 0.2780358822181307
LOSS train 0.2501749948803651 valid 0.27847341161507827
LOSS train 0.2501749948803651 valid 0.27826332540835363
LOSS train 0.2501749948803651 valid 0.2781775974425949
LOSS train 0.2501749948803651 valid 0.2777155759433905
LOSS train 0.2501749948803651 valid 0.277499159998145
LOSS train 0.2501749948803651 valid 0.2772687763952818
LOSS train 0.2501749948803651 valid 0.2772550999633665
LOSS train 0.2501749948803651 valid 0.2775592578034247
LOSS train 0.2501749948803651 valid 0.2774525737762451
LOSS train 0.2501749948803651 valid 0.2776748046042427
LOSS train 0.2501749948803651 valid 0.27747875710171976
LOSS train 0.2501749948803651 valid 0.278082265984267
LOSS train 0.2501749948803651 valid 0.2780171179956244
LOSS train 0.2501749948803651 valid 0.27807727043445296
LOSS train 0.2501749948803651 valid 0.27804390073732566
LOSS train 0.2501749948803651 valid 0.27767346731641074
LOSS train 0.2501749948803651 valid 0.2775298395103082
LOSS train 0.2501749948803651 valid 0.27757383705075106
LOSS train 0.2501749948803651 valid 0.2775540318753984
LOSS train 0.2501749948803651 valid 0.27759211313198595
LOSS train 0.2501749948803651 valid 0.27741245675261006
LOSS train 0.2501749948803651 valid 0.2773049136866694
LOSS train 0.2501749948803651 valid 0.27702681773858107
LOSS train 0.2501749948803651 valid 0.277191010756152
LOSS train 0.2501749948803651 valid 0.2770715966715035
LOSS train 0.2501749948803651 valid 0.277317689128325
LOSS train 0.2501749948803651 valid 0.2772335653955286
LOSS train 0.2501749948803651 valid 0.2772858743038442
LOSS train 0.2501749948803651 valid 0.2770464025694748
LOSS train 0.2501749948803651 valid 0.2773666216494286
LOSS train 0.2501749948803651 valid 0.2772273940293967
LOSS train 0.2501749948803651 valid 0.27810192390068156
LOSS train 0.2501749948803651 valid 0.2780114594321923
LOSS train 0.2501749948803651 valid 0.2779825031757355
LOSS train 0.2501749948803651 valid 0.2778103235146857
LOSS train 0.2501749948803651 valid 0.2775958206896719
LOSS train 0.2501749948803651 valid 0.27752910710238166
LOSS train 0.2501749948803651 valid 0.2774804721211458
LOSS train 0.2501749948803651 valid 0.27766099216476564
LOSS train 0.2501749948803651 valid 0.27811408205292165
LOSS train 0.2501749948803651 valid 0.278057628851028
LOSS train 0.2501749948803651 valid 0.2779628646713269
LOSS train 0.2501749948803651 valid 0.27796019669973626
LOSS train 0.2501749948803651 valid 0.27764884615316987
LOSS train 0.2501749948803651 valid 0.2777419687242982
LOSS train 0.2501749948803651 valid 0.2776651179165016
LOSS train 0.2501749948803651 valid 0.27755947685314836
LOSS train 0.2501749948803651 valid 0.27743139417796603
LOSS train 0.2501749948803651 valid 0.2772225243575645
LOSS train 0.2501749948803651 valid 0.27753278496394673
LOSS train 0.2501749948803651 valid 0.27763988809314316
LOSS train 0.2501749948803651 valid 0.27754819614901427
LOSS train 0.2501749948803651 valid 0.27764274162300945
LOSS train 0.2501749948803651 valid 0.27794829037259605
LOSS train 0.2501749948803651 valid 0.27779725708110986
LOSS train 0.2501749948803651 valid 0.2774672247469425
LOSS train 0.2501749948803651 valid 0.27748766946310255
LOSS train 0.2501749948803651 valid 0.2773361402167671
LOSS train 0.2501749948803651 valid 0.27731293550559455
LOSS train 0.2501749948803651 valid 0.27727257468822325
LOSS train 0.2501749948803651 valid 0.27737999374920363
LOSS train 0.2501749948803651 valid 0.277548997505997
LOSS train 0.2501749948803651 valid 0.27746947076400563
LOSS train 0.2501749948803651 valid 0.27749561543265977
LOSS train 0.2501749948803651 valid 0.27775129206602084
LOSS train 0.2501749948803651 valid 0.2778364029395711
LOSS train 0.2501749948803651 valid 0.27760272566738026
LOSS train 0.2501749948803651 valid 0.2775281207716983
LOSS train 0.2501749948803651 valid 0.2774103225888433
LOSS train 0.2501749948803651 valid 0.277425907311901
LOSS train 0.2501749948803651 valid 0.2772626148506919
LOSS train 0.2501749948803651 valid 0.2773610951101526
LOSS train 0.2501749948803651 valid 0.2775119429542905
LOSS train 0.2501749948803651 valid 0.2775905082100316
LOSS train 0.2501749948803651 valid 0.2775115433163668
LOSS train 0.2501749948803651 valid 0.27740809150661033
LOSS train 0.2501749948803651 valid 0.27761727787670076
LOSS train 0.2501749948803651 valid 0.27743699993054893
LOSS train 0.2501749948803651 valid 0.2770689811462011
LOSS train 0.2501749948803651 valid 0.2772002300741721
LOSS train 0.2501749948803651 valid 0.2773530549809412
LOSS train 0.2501749948803651 valid 0.277378266507929
LOSS train 0.2501749948803651 valid 0.277406555474104
LOSS train 0.2501749948803651 valid 0.2771760542690754
LOSS train 0.2501749948803651 valid 0.27700420027941614
LOSS train 0.2501749948803651 valid 0.27689003826367975
LOSS train 0.2501749948803651 valid 0.2767759495474435
LOSS train 0.2501749948803651 valid 0.27675366021838843
LOSS train 0.2501749948803651 valid 0.27659581870567507
LOSS train 0.2501749948803651 valid 0.27653350789570114
LOSS train 0.2501749948803651 valid 0.276395796433739
LOSS train 0.2501749948803651 valid 0.2763672291945953
LOSS train 0.2501749948803651 valid 0.27632660244070173
LOSS train 0.2501749948803651 valid 0.2764315982659658
LOSS train 0.2501749948803651 valid 0.27645100702606673
LOSS train 0.2501749948803651 valid 0.27635191310689133
LOSS train 0.2501749948803651 valid 0.27637360927084803
LOSS train 0.2501749948803651 valid 0.2763138077804975
LOSS train 0.2501749948803651 valid 0.27611086382422334
LOSS train 0.2501749948803651 valid 0.27603725274955787
LOSS train 0.2501749948803651 valid 0.2760975288081279
LOSS train 0.2501749948803651 valid 0.2761034407746901
LOSS train 0.2501749948803651 valid 0.2761821753630355
LOSS train 0.2501749948803651 valid 0.2763626045801423
LOSS train 0.2501749948803651 valid 0.2764189802953021
LOSS train 0.2501749948803651 valid 0.2763740651242368
LOSS train 0.2501749948803651 valid 0.2764970292692227
LOSS train 0.2501749948803651 valid 0.27663490708385197
LOSS train 0.2501749948803651 valid 0.2766718896230062
LOSS train 0.2501749948803651 valid 0.27682786850275193
LOSS train 0.2501749948803651 valid 0.27715952989813514
LOSS train 0.2501749948803651 valid 0.2772119451771703
LOSS train 0.2501749948803651 valid 0.27728147316707796
LOSS train 0.2501749948803651 valid 0.2774128776529561
LOSS train 0.2501749948803651 valid 0.27740479457429995
LOSS train 0.2501749948803651 valid 0.2774920365933714
LOSS train 0.2501749948803651 valid 0.27738971822763203
LOSS train 0.2501749948803651 valid 0.27744843575180084
LOSS train 0.2501749948803651 valid 0.27752251472878964
LOSS train 0.2501749948803651 valid 0.2774789333343506
LOSS train 0.2501749948803651 valid 0.27747333841987804
LOSS train 0.2501749948803651 valid 0.2775223046040335
LOSS train 0.2501749948803651 valid 0.2774236238401804
LOSS train 0.2501749948803651 valid 0.27737319096922874
LOSS train 0.2501749948803651 valid 0.27751660977656417
LOSS train 0.2501749948803651 valid 0.2772826291928607
LOSS train 0.2501749948803651 valid 0.27741027902428506
LOSS train 0.2501749948803651 valid 0.27761204265913025
LOSS train 0.2501749948803651 valid 0.2777573767365242
LOSS train 0.2501749948803651 valid 0.2777465170597642
LOSS train 0.2501749948803651 valid 0.27762624977329964
LOSS train 0.2501749948803651 valid 0.2775769291986381
LOSS train 0.2501749948803651 valid 0.2775128915247668
LOSS train 0.2501749948803651 valid 0.2776155967116356
LOSS train 0.2501749948803651 valid 0.27760965480984917
LOSS train 0.2501749948803651 valid 0.27764777561265325
LOSS train 0.2501749948803651 valid 0.27753232231017627
LOSS train 0.2501749948803651 valid 0.2775166036458466
LOSS train 0.2501749948803651 valid 0.2775655341498992
LOSS train 0.2501749948803651 valid 0.27768286439822987
LOSS train 0.2501749948803651 valid 0.27751177143493977
LOSS train 0.2501749948803651 valid 0.2774531729230585
LOSS train 0.2501749948803651 valid 0.277433295277555
LOSS train 0.2501749948803651 valid 0.2774394364311145
LOSS train 0.2501749948803651 valid 0.27748363125369924
LOSS train 0.2501749948803651 valid 0.27747151141858284
LOSS train 0.2501749948803651 valid 0.27745237844525183
LOSS train 0.2501749948803651 valid 0.27746261407931644
LOSS train 0.2501749948803651 valid 0.27735820356405005
LOSS train 0.2501749948803651 valid 0.277322767484457
LOSS train 0.2501749948803651 valid 0.2774356749843569
LOSS train 0.2501749948803651 valid 0.2776191553740359
LOSS train 0.2501749948803651 valid 0.27771927554811243
LOSS train 0.2501749948803651 valid 0.2777052937834351
LOSS train 0.2501749948803651 valid 0.2777142400451252
LOSS train 0.2501749948803651 valid 0.277907215497073
LOSS train 0.2501749948803651 valid 0.2779048257695013
LOSS train 0.2501749948803651 valid 0.27790428295622777
LOSS train 0.2501749948803651 valid 0.27784093954346395
LOSS train 0.2501749948803651 valid 0.27778020101612894
LOSS train 0.2501749948803651 valid 0.2775496629063403
LOSS train 0.2501749948803651 valid 0.27731115531792744
LOSS train 0.2501749948803651 valid 0.2773312174825258
LOSS train 0.2501749948803651 valid 0.2773292919886964
LOSS train 0.2501749948803651 valid 0.2772833549573328
LOSS train 0.2501749948803651 valid 0.2770979623211191
LOSS train 0.2501749948803651 valid 0.2771271632543301
LOSS train 0.2501749948803651 valid 0.27720234228271834
LOSS train 0.2501749948803651 valid 0.27721170061512995
LOSS train 0.2501749948803651 valid 0.27709976641031414
LOSS train 0.2501749948803651 valid 0.27697822140068956
LOSS train 0.2501749948803651 valid 0.27706917602982783
LOSS train 0.2501749948803651 valid 0.27710060914495
LOSS train 0.2501749948803651 valid 0.2771217905241868
LOSS train 0.2501749948803651 valid 0.27713390191396076
LOSS train 0.2501749948803651 valid 0.2771425177789714
LOSS train 0.2501749948803651 valid 0.27714930520529635
LOSS train 0.2501749948803651 valid 0.2773166935460097
LOSS train 0.2501749948803651 valid 0.27740102268881717
LOSS train 0.2501749948803651 valid 0.2773734333144652
LOSS train 0.2501749948803651 valid 0.27740144007133716
LOSS train 0.2501749948803651 valid 0.2774146578255916
LOSS train 0.2501749948803651 valid 0.27749763812907163
LOSS train 0.2501749948803651 valid 0.2774717903137207
LOSS train 0.2501749948803651 valid 0.2775751280032123
LOSS train 0.2501749948803651 valid 0.27756124468433935
LOSS train 0.2501749948803651 valid 0.277687938496618
LOSS train 0.2501749948803651 valid 0.27770714265735524
LOSS train 0.2501749948803651 valid 0.27760953522119364
LOSS train 0.2501749948803651 valid 0.27748558329406126
LOSS train 0.2501749948803651 valid 0.2774320125871062
LOSS train 0.2501749948803651 valid 0.2773122595405424
LOSS train 0.2501749948803651 valid 0.2774363497217882
LOSS train 0.2501749948803651 valid 0.27744157136447967
LOSS train 0.2501749948803651 valid 0.27728074046384865
LOSS train 0.2501749948803651 valid 0.2773593361847676
LOSS train 0.2501749948803651 valid 0.2773298498826286
LOSS train 0.2501749948803651 valid 0.2773928352317233
LOSS train 0.2501749948803651 valid 0.2774204804310723
LOSS train 0.2501749948803651 valid 0.27732899950086315
LOSS train 0.2501749948803651 valid 0.27732689208600797
LOSS train 0.2501749948803651 valid 0.27737897601704925
LOSS train 0.2501749948803651 valid 0.27744996580397446
LOSS train 0.2501749948803651 valid 0.27741814306937157
LOSS train 0.2501749948803651 valid 0.2775201336524197
LOSS train 0.2501749948803651 valid 0.2774960479658583
LOSS train 0.2501749948803651 valid 0.2774658543129824
LOSS train 0.2501749948803651 valid 0.27751194147600067
LOSS train 0.2501749948803651 valid 0.2774274587172728
LOSS train 0.2501749948803651 valid 0.27760747783571665
LOSS train 0.2501749948803651 valid 0.27773777128906424
LOSS train 0.2501749948803651 valid 0.27772686925784845
LOSS train 0.2501749948803651 valid 0.2779387640101569
LOSS train 0.2501749948803651 valid 0.2779028349302032
LOSS train 0.2501749948803651 valid 0.2778453143071552
LOSS train 0.2501749948803651 valid 0.27770454360239477
LOSS train 0.2501749948803651 valid 0.27767734900788144
LOSS train 0.2501749948803651 valid 0.2778371647892598
LOSS train 0.2501749948803651 valid 0.27785646021366117
LOSS train 0.2501749948803651 valid 0.2778811513756712
LOSS train 0.2501749948803651 valid 0.27774754378845146
LOSS train 0.2501749948803651 valid 0.2777166633739979
LOSS train 0.2501749948803651 valid 0.2777317339515967
LOSS train 0.2501749948803651 valid 0.2777626601212165
LOSS train 0.2501749948803651 valid 0.27765601829699404
LOSS train 0.2501749948803651 valid 0.2776442765318162
LOSS train 0.2501749948803651 valid 0.27760500202373584
LOSS train 0.2501749948803651 valid 0.27779057474676955
LOSS train 0.2501749948803651 valid 0.27782143717226776
LOSS train 0.2501749948803651 valid 0.2776812936443125
LOSS train 0.2501749948803651 valid 0.2776502359239787
LOSS train 0.2501749948803651 valid 0.2775956639356312
LOSS train 0.2501749948803651 valid 0.27769619589377953
LOSS train 0.2501749948803651 valid 0.2775212793690818
LOSS train 0.2501749948803651 valid 0.27746626879075315
LOSS train 0.2501749948803651 valid 0.2773554899560457
LOSS train 0.2501749948803651 valid 0.27744438657510717
LOSS train 0.2501749948803651 valid 0.2775307036304878
LOSS train 0.2501749948803651 valid 0.2776474013714723
LOSS train 0.2501749948803651 valid 0.27768344235386744
LOSS train 0.2501749948803651 valid 0.2776520651679079
LOSS train 0.2501749948803651 valid 0.2775712727917639
LOSS train 0.2501749948803651 valid 0.27755806800548743
LOSS train 0.2501749948803651 valid 0.27746136118140485
LOSS train 0.2501749948803651 valid 0.2773842401699346
LOSS train 0.2501749948803651 valid 0.2775047598046492
LOSS train 0.2501749948803651 valid 0.2773291470672802
LOSS train 0.2501749948803651 valid 0.2772821366868831
LOSS train 0.2501749948803651 valid 0.27723553054136774
LOSS train 0.2501749948803651 valid 0.2771658182062738
LOSS train 0.2501749948803651 valid 0.27706577703965785
LOSS train 0.2501749948803651 valid 0.2770382157405433
LOSS train 0.2501749948803651 valid 0.2770502360128775
EPOCH 14:
  batch 1 loss: 0.2593531012535095
  batch 2 loss: 0.24188858270645142
  batch 3 loss: 0.25274281700452167
  batch 4 loss: 0.25740522891283035
  batch 5 loss: 0.2669860780239105
  batch 6 loss: 0.2617708245913188
  batch 7 loss: 0.2631771181310926
  batch 8 loss: 0.26347820460796356
  batch 9 loss: 0.260110757417149
  batch 10 loss: 0.26204964965581895
  batch 11 loss: 0.2615865943106738
  batch 12 loss: 0.25988977154095966
  batch 13 loss: 0.25962717487261844
  batch 14 loss: 0.2593447672469275
  batch 15 loss: 0.26091068585713706
  batch 16 loss: 0.260096394456923
  batch 17 loss: 0.258651225882418
  batch 18 loss: 0.2590869756208526
  batch 19 loss: 0.25854218476697016
  batch 20 loss: 0.2571037620306015
  batch 21 loss: 0.25718019689832416
  batch 22 loss: 0.25670642812143674
  batch 23 loss: 0.2549246892980907
  batch 24 loss: 0.2527529566238324
  batch 25 loss: 0.25548596799373624
  batch 26 loss: 0.254484491279492
  batch 27 loss: 0.2537330124113295
  batch 28 loss: 0.25248781059469494
  batch 29 loss: 0.2524950257663069
  batch 30 loss: 0.2520735944310824
  batch 31 loss: 0.2520380563312961
  batch 32 loss: 0.2516520726494491
  batch 33 loss: 0.2510443338842103
  batch 34 loss: 0.25113556928494396
  batch 35 loss: 0.2508335965020316
  batch 36 loss: 0.2504853854576747
  batch 37 loss: 0.25007602451620875
  batch 38 loss: 0.2505598879958454
  batch 39 loss: 0.24957079000962087
  batch 40 loss: 0.24947970248758794
  batch 41 loss: 0.24895524578850445
  batch 42 loss: 0.24931692615860984
  batch 43 loss: 0.2486435968515485
  batch 44 loss: 0.2486571930348873
  batch 45 loss: 0.2481826017300288
  batch 46 loss: 0.24799787285535232
  batch 47 loss: 0.24796420525997243
  batch 48 loss: 0.24713210947811604
  batch 49 loss: 0.24705078315978146
  batch 50 loss: 0.24667868822813033
  batch 51 loss: 0.2462126679864584
  batch 52 loss: 0.2463569798721717
  batch 53 loss: 0.24540381358479554
  batch 54 loss: 0.2452057875968792
  batch 55 loss: 0.24411732202226466
  batch 56 loss: 0.2434812712350062
  batch 57 loss: 0.24250997001664681
  batch 58 loss: 0.24213464917807742
  batch 59 loss: 0.2424184169809697
  batch 60 loss: 0.24169496074318886
  batch 61 loss: 0.24171820600501825
  batch 62 loss: 0.24148947144708327
  batch 63 loss: 0.2405533128314548
  batch 64 loss: 0.24115691892802715
  batch 65 loss: 0.2406372849757855
  batch 66 loss: 0.2404510396899599
  batch 67 loss: 0.24120456736479232
  batch 68 loss: 0.24134522676467896
  batch 69 loss: 0.24140737108562305
  batch 70 loss: 0.24154491126537322
  batch 71 loss: 0.24144971370697021
  batch 72 loss: 0.24167913446823755
  batch 73 loss: 0.24154660963032343
  batch 74 loss: 0.24167721456772573
  batch 75 loss: 0.24125046173731485
  batch 76 loss: 0.24169694906786868
  batch 77 loss: 0.24144340567774586
  batch 78 loss: 0.24140287247987893
  batch 79 loss: 0.24174531388886367
  batch 80 loss: 0.2415307642892003
  batch 81 loss: 0.24191969909049846
  batch 82 loss: 0.24245587445613814
  batch 83 loss: 0.24255945650209865
  batch 84 loss: 0.2426465041935444
  batch 85 loss: 0.24263501009520363
  batch 86 loss: 0.2429863174640855
  batch 87 loss: 0.2428469090968713
  batch 88 loss: 0.2426258371296254
  batch 89 loss: 0.242520742202073
  batch 90 loss: 0.24248352696498235
  batch 91 loss: 0.24280921165104752
  batch 92 loss: 0.24282143637537956
  batch 93 loss: 0.2426554129649234
  batch 94 loss: 0.24298006216896342
  batch 95 loss: 0.24271495248142042
  batch 96 loss: 0.24268505908548832
  batch 97 loss: 0.2428805981714701
  batch 98 loss: 0.24325894701237583
  batch 99 loss: 0.24351745873990685
  batch 100 loss: 0.24362261533737184
  batch 101 loss: 0.24364411270264352
  batch 102 loss: 0.24395091276542813
  batch 103 loss: 0.24433360574314894
  batch 104 loss: 0.24443767993496016
  batch 105 loss: 0.24419969604128883
  batch 106 loss: 0.2449254073062033
  batch 107 loss: 0.24437205604860715
  batch 108 loss: 0.24438772143589127
  batch 109 loss: 0.24432605463977253
  batch 110 loss: 0.2442751483483748
  batch 111 loss: 0.24406680840629716
  batch 112 loss: 0.2439405910138573
  batch 113 loss: 0.24434250936044
  batch 114 loss: 0.24452838432370572
  batch 115 loss: 0.2445595181506613
  batch 116 loss: 0.24478016793727875
  batch 117 loss: 0.24513062045105502
  batch 118 loss: 0.24502289067890684
  batch 119 loss: 0.24511417771587854
  batch 120 loss: 0.2451586442689101
  batch 121 loss: 0.24517959036117742
  batch 122 loss: 0.24508064936419002
  batch 123 loss: 0.24474404921861198
  batch 124 loss: 0.24484303149004136
  batch 125 loss: 0.2446829138994217
  batch 126 loss: 0.24458684500247713
  batch 127 loss: 0.24479491152162627
  batch 128 loss: 0.2448702130932361
  batch 129 loss: 0.2453389128511266
  batch 130 loss: 0.24566710889339446
  batch 131 loss: 0.24587145834478713
  batch 132 loss: 0.24618074636567722
  batch 133 loss: 0.24653058863223942
  batch 134 loss: 0.2466529289733118
  batch 135 loss: 0.24696415905599242
  batch 136 loss: 0.24720085171215675
  batch 137 loss: 0.24736159539570773
  batch 138 loss: 0.24770251974679422
  batch 139 loss: 0.24809163787382113
  batch 140 loss: 0.24801668705684798
  batch 141 loss: 0.2481078083844895
  batch 142 loss: 0.2479045925215936
  batch 143 loss: 0.24767562507332622
  batch 144 loss: 0.24788371628771225
  batch 145 loss: 0.24773013252636483
  batch 146 loss: 0.24763720094749372
  batch 147 loss: 0.24834342300891876
  batch 148 loss: 0.2483965310494642
  batch 149 loss: 0.24843378525052295
  batch 150 loss: 0.24856405427058537
  batch 151 loss: 0.24904770872845555
  batch 152 loss: 0.2490883160774645
  batch 153 loss: 0.24879308551355125
  batch 154 loss: 0.24923800638356766
  batch 155 loss: 0.24918106284833724
  batch 156 loss: 0.24914856866384164
  batch 157 loss: 0.24937596518522614
  batch 158 loss: 0.25003699298146403
  batch 159 loss: 0.25013121531444527
  batch 160 loss: 0.25008581969887017
  batch 161 loss: 0.25038085628000106
  batch 162 loss: 0.25062965868431847
  batch 163 loss: 0.2507351194422669
  batch 164 loss: 0.25094806929913965
  batch 165 loss: 0.25122125510013465
  batch 166 loss: 0.2515030083167984
  batch 167 loss: 0.25148636941424385
  batch 168 loss: 0.25164617349704105
  batch 169 loss: 0.25149951813489024
  batch 170 loss: 0.2515353791853961
  batch 171 loss: 0.2516974191568051
  batch 172 loss: 0.2515458026251128
  batch 173 loss: 0.25180856978273114
  batch 174 loss: 0.2523126131159136
  batch 175 loss: 0.25248357364109586
  batch 176 loss: 0.2524631063199856
  batch 177 loss: 0.25267859190534064
  batch 178 loss: 0.2529638261440095
  batch 179 loss: 0.25317198729048895
  batch 180 loss: 0.253201811760664
  batch 181 loss: 0.25323144696364747
  batch 182 loss: 0.25335124110455043
  batch 183 loss: 0.25335158749682
  batch 184 loss: 0.2532256583318762
  batch 185 loss: 0.2533360024561753
  batch 186 loss: 0.2533440954422438
  batch 187 loss: 0.25326682212837237
  batch 188 loss: 0.25303846058693336
  batch 189 loss: 0.2528899838684728
  batch 190 loss: 0.25317736142560054
  batch 191 loss: 0.25312327597465817
  batch 192 loss: 0.2531363735130678
  batch 193 loss: 0.25319623985747597
  batch 194 loss: 0.2533877686280565
  batch 195 loss: 0.2534772358643703
  batch 196 loss: 0.2533226994683548
  batch 197 loss: 0.2533670360515565
  batch 198 loss: 0.2535725421827249
  batch 199 loss: 0.25367566261758756
  batch 200 loss: 0.2537853503972292
  batch 201 loss: 0.2538522805888854
  batch 202 loss: 0.2538464158625886
  batch 203 loss: 0.2537626161657531
  batch 204 loss: 0.2535703475130539
  batch 205 loss: 0.25364793444552075
  batch 206 loss: 0.2535733864868729
  batch 207 loss: 0.2535912783969428
  batch 208 loss: 0.25352059727391374
  batch 209 loss: 0.2532197545732608
  batch 210 loss: 0.25331529343412035
  batch 211 loss: 0.25315244213382215
  batch 212 loss: 0.25300668081584965
  batch 213 loss: 0.25301829680030896
  batch 214 loss: 0.2530347803764254
  batch 215 loss: 0.2527115392823552
  batch 216 loss: 0.25264412275067083
  batch 217 loss: 0.2525849233849257
  batch 218 loss: 0.25249857913463486
  batch 219 loss: 0.2525205956474287
  batch 220 loss: 0.2525506003336473
  batch 221 loss: 0.2525944062487572
  batch 222 loss: 0.2525802860120395
  batch 223 loss: 0.2525293019854969
  batch 224 loss: 0.252480084409139
  batch 225 loss: 0.25242681509918635
  batch 226 loss: 0.2525146734793629
  batch 227 loss: 0.25223721951114975
  batch 228 loss: 0.2521086326032354
  batch 229 loss: 0.25196961882853613
  batch 230 loss: 0.2518989899236223
  batch 231 loss: 0.2518386865978117
  batch 232 loss: 0.2516734381045761
  batch 233 loss: 0.25157878985988225
  batch 234 loss: 0.2515506027982785
  batch 235 loss: 0.25149547831809266
  batch 236 loss: 0.2514293977390912
  batch 237 loss: 0.25138684069808526
  batch 238 loss: 0.25131759780044316
  batch 239 loss: 0.2512928039209613
  batch 240 loss: 0.25140099972486496
  batch 241 loss: 0.25145404776596925
  batch 242 loss: 0.2514070226264394
  batch 243 loss: 0.251327585720231
  batch 244 loss: 0.25121644548461086
  batch 245 loss: 0.25119810615267074
  batch 246 loss: 0.25113441801168085
  batch 247 loss: 0.2511127539853818
  batch 248 loss: 0.2510067108537882
  batch 249 loss: 0.25095868278219996
  batch 250 loss: 0.25076805174350736
  batch 251 loss: 0.2506524342820939
  batch 252 loss: 0.25049977507146576
  batch 253 loss: 0.2504587354867355
  batch 254 loss: 0.2504044257280395
  batch 255 loss: 0.2503274151507546
  batch 256 loss: 0.2503005795297213
  batch 257 loss: 0.2505052539045245
  batch 258 loss: 0.25050120696771977
  batch 259 loss: 0.2505758640043524
  batch 260 loss: 0.2506867891893937
  batch 261 loss: 0.250815883507217
  batch 262 loss: 0.2509143000454393
  batch 263 loss: 0.2509961671588992
  batch 264 loss: 0.25089662493855663
  batch 265 loss: 0.2509702930472932
  batch 266 loss: 0.25098165942538053
  batch 267 loss: 0.2510147655612967
  batch 268 loss: 0.2508914945508117
  batch 269 loss: 0.25079834826816855
  batch 270 loss: 0.25084998232346994
  batch 271 loss: 0.2506610993089711
  batch 272 loss: 0.25071442291578827
  batch 273 loss: 0.2506486356476724
  batch 274 loss: 0.25081542667246215
  batch 275 loss: 0.25090501416813243
  batch 276 loss: 0.25095509057459625
  batch 277 loss: 0.2509376540618683
  batch 278 loss: 0.25086423084675835
  batch 279 loss: 0.2509559193712836
  batch 280 loss: 0.25090901777148245
  batch 281 loss: 0.25088293473915696
  batch 282 loss: 0.25086791866214564
  batch 283 loss: 0.2508482498646625
  batch 284 loss: 0.250801518926738
  batch 285 loss: 0.25073042475340657
  batch 286 loss: 0.2505796294841733
  batch 287 loss: 0.25057054277288787
  batch 288 loss: 0.250374478701916
  batch 289 loss: 0.25038547324061805
  batch 290 loss: 0.2501384589178809
  batch 291 loss: 0.25008124502254103
  batch 292 loss: 0.25012279352913164
  batch 293 loss: 0.2501087866647252
  batch 294 loss: 0.25000766391048623
  batch 295 loss: 0.24994811410621062
  batch 296 loss: 0.25003694576790203
  batch 297 loss: 0.25004723010841845
  batch 298 loss: 0.25008103126447473
  batch 299 loss: 0.2499466585774087
  batch 300 loss: 0.24991045777996382
  batch 301 loss: 0.2498301793372512
  batch 302 loss: 0.24979749218320216
  batch 303 loss: 0.2498561271248084
  batch 304 loss: 0.2496984061834059
  batch 305 loss: 0.24956355515073558
  batch 306 loss: 0.24958474729575364
  batch 307 loss: 0.24952039151704272
  batch 308 loss: 0.24954436080796377
  batch 309 loss: 0.2494867336981505
  batch 310 loss: 0.24939423539946157
  batch 311 loss: 0.24945922469019507
  batch 312 loss: 0.24961945462303284
  batch 313 loss: 0.24971302029804682
  batch 314 loss: 0.24965855512459567
  batch 315 loss: 0.24960636066065894
  batch 316 loss: 0.24953463187889208
  batch 317 loss: 0.24950591110468662
  batch 318 loss: 0.249467356306202
  batch 319 loss: 0.24944120028923297
  batch 320 loss: 0.24937345134094357
  batch 321 loss: 0.2494048637764476
  batch 322 loss: 0.249374618348868
  batch 323 loss: 0.24927333075749247
  batch 324 loss: 0.24909541958276135
  batch 325 loss: 0.24895809503702018
  batch 326 loss: 0.24895041733065998
  batch 327 loss: 0.24899076731926803
  batch 328 loss: 0.2487797330519775
  batch 329 loss: 0.24884777233049862
  batch 330 loss: 0.24876717405788826
  batch 331 loss: 0.2487285653990201
  batch 332 loss: 0.24857227351651134
  batch 333 loss: 0.24863175416851904
  batch 334 loss: 0.24856694015914094
  batch 335 loss: 0.24844456377314098
  batch 336 loss: 0.24835208869938338
  batch 337 loss: 0.24817989014907124
  batch 338 loss: 0.24817025851039493
  batch 339 loss: 0.2481047720332413
  batch 340 loss: 0.2480821505188942
  batch 341 loss: 0.24792970004669032
  batch 342 loss: 0.24793030661449098
  batch 343 loss: 0.2479217437370178
  batch 344 loss: 0.24794502313746963
  batch 345 loss: 0.24802006068436996
  batch 346 loss: 0.2479396247536461
  batch 347 loss: 0.24801617186591673
  batch 348 loss: 0.24810694479222956
  batch 349 loss: 0.24803166561106216
  batch 350 loss: 0.24805707884686334
  batch 351 loss: 0.24809210851491348
  batch 352 loss: 0.24821323101324114
  batch 353 loss: 0.24824869510303477
  batch 354 loss: 0.2483246541545216
  batch 355 loss: 0.24832464385200553
  batch 356 loss: 0.24835240409782763
  batch 357 loss: 0.2482761777415663
  batch 358 loss: 0.24832885416859354
  batch 359 loss: 0.24838284396859597
  batch 360 loss: 0.24830772313806745
  batch 361 loss: 0.24830281362969459
  batch 362 loss: 0.2482628694308397
  batch 363 loss: 0.24820603532896227
  batch 364 loss: 0.2481183874328713
  batch 365 loss: 0.24808337529228158
  batch 366 loss: 0.24798277928366688
  batch 367 loss: 0.24789620858771924
  batch 368 loss: 0.24780813879940822
  batch 369 loss: 0.2477820764792967
  batch 370 loss: 0.24772624276779792
  batch 371 loss: 0.24767054512976314
  batch 372 loss: 0.24757043977258025
  batch 373 loss: 0.2474674830567741
  batch 374 loss: 0.24735173351943174
  batch 375 loss: 0.2472704087893168
  batch 376 loss: 0.24723587890571735
  batch 377 loss: 0.24726945272491213
  batch 378 loss: 0.24715983347287254
  batch 379 loss: 0.2472630665295986
  batch 380 loss: 0.24726994021942741
  batch 381 loss: 0.24721245178243934
  batch 382 loss: 0.24709139300110453
  batch 383 loss: 0.24699346616124981
  batch 384 loss: 0.2469142625341192
  batch 385 loss: 0.24696976538602408
  batch 386 loss: 0.24685086012658677
  batch 387 loss: 0.2468391936416774
  batch 388 loss: 0.2469665670056933
  batch 389 loss: 0.24683342739977995
  batch 390 loss: 0.2467329285465754
  batch 391 loss: 0.24675360325809634
  batch 392 loss: 0.2467643231989778
  batch 393 loss: 0.24684288525247694
  batch 394 loss: 0.24689562879691873
  batch 395 loss: 0.24678626694256747
  batch 396 loss: 0.24670865063113395
  batch 397 loss: 0.24669405228695282
  batch 398 loss: 0.24670241634600126
  batch 399 loss: 0.24668127272212714
  batch 400 loss: 0.24670634482055903
  batch 401 loss: 0.24662009569028964
  batch 402 loss: 0.24659861483384127
  batch 403 loss: 0.2466081910334509
  batch 404 loss: 0.2466243510641674
  batch 405 loss: 0.246593259550907
  batch 406 loss: 0.2466849832625812
  batch 407 loss: 0.24666382006257406
  batch 408 loss: 0.24679237301004867
  batch 409 loss: 0.24679383708387248
  batch 410 loss: 0.2468287411259442
  batch 411 loss: 0.24681283508194043
  batch 412 loss: 0.2467655156494923
  batch 413 loss: 0.2468546212224637
  batch 414 loss: 0.24685478372418362
  batch 415 loss: 0.24686329655618552
  batch 416 loss: 0.2469191666907416
  batch 417 loss: 0.24687846655325352
  batch 418 loss: 0.2468050964283601
  batch 419 loss: 0.24670331450379265
  batch 420 loss: 0.2466685942241124
  batch 421 loss: 0.2465427470462056
  batch 422 loss: 0.2466766369568793
  batch 423 loss: 0.24668484672586968
  batch 424 loss: 0.24665453734825243
  batch 425 loss: 0.24671705708784217
  batch 426 loss: 0.2466687863001801
  batch 427 loss: 0.24670814085900086
  batch 428 loss: 0.24670520821742922
  batch 429 loss: 0.2467310611323599
  batch 430 loss: 0.24671733833329623
  batch 431 loss: 0.24686344375881408
  batch 432 loss: 0.24690884279294145
  batch 433 loss: 0.24692634728846868
  batch 434 loss: 0.24701620023783452
  batch 435 loss: 0.24698449520544075
  batch 436 loss: 0.2469969332696648
  batch 437 loss: 0.24709547524184874
  batch 438 loss: 0.24721957950695464
  batch 439 loss: 0.2472529897046252
  batch 440 loss: 0.2472770268605514
  batch 441 loss: 0.2472273365440282
  batch 442 loss: 0.24720692519958204
  batch 443 loss: 0.2472120818785028
  batch 444 loss: 0.247129531191276
  batch 445 loss: 0.24721420708667027
  batch 446 loss: 0.24722564060057226
  batch 447 loss: 0.2471706781878034
  batch 448 loss: 0.2471633025644613
  batch 449 loss: 0.2472051838392139
  batch 450 loss: 0.2471434411406517
  batch 451 loss: 0.24709664909644033
  batch 452 loss: 0.2471194057601743
  batch 453 loss: 0.24704662968649244
  batch 454 loss: 0.2471162654164079
  batch 455 loss: 0.2471254512831405
  batch 456 loss: 0.24717259155422971
  batch 457 loss: 0.24723279368564388
  batch 458 loss: 0.2472531314955528
  batch 459 loss: 0.24729204629111654
  batch 460 loss: 0.24743372911344405
  batch 461 loss: 0.24747166642010082
  batch 462 loss: 0.24753303512995378
  batch 463 loss: 0.247538712414758
  batch 464 loss: 0.2475867072634142
  batch 465 loss: 0.24752322816720573
  batch 466 loss: 0.24750043138413982
  batch 467 loss: 0.24752057000707509
  batch 468 loss: 0.24755345189418548
  batch 469 loss: 0.24771798991445285
  batch 470 loss: 0.2477390353984021
  batch 471 loss: 0.2477685188411907
  batch 472 loss: 0.24766874231271824
LOSS train 0.24766874231271824 valid 0.33277449011802673
LOSS train 0.24766874231271824 valid 0.3142329156398773
LOSS train 0.24766874231271824 valid 0.31790317098299664
LOSS train 0.24766874231271824 valid 0.3130641579627991
LOSS train 0.24766874231271824 valid 0.30224833488464353
LOSS train 0.24766874231271824 valid 0.3060946762561798
LOSS train 0.24766874231271824 valid 0.31446785160473417
LOSS train 0.24766874231271824 valid 0.31036636605858803
LOSS train 0.24766874231271824 valid 0.3142869306935204
LOSS train 0.24766874231271824 valid 0.31600719392299653
LOSS train 0.24766874231271824 valid 0.3116070465608077
LOSS train 0.24766874231271824 valid 0.31407291690508526
LOSS train 0.24766874231271824 valid 0.3140398080532367
LOSS train 0.24766874231271824 valid 0.3150504742349897
LOSS train 0.24766874231271824 valid 0.3115903417269389
LOSS train 0.24766874231271824 valid 0.312793780118227
LOSS train 0.24766874231271824 valid 0.313179471913506
LOSS train 0.24766874231271824 valid 0.3149619284603331
LOSS train 0.24766874231271824 valid 0.31768099885237844
LOSS train 0.24766874231271824 valid 0.3176044225692749
LOSS train 0.24766874231271824 valid 0.3167828420797984
LOSS train 0.24766874231271824 valid 0.31517247042872687
LOSS train 0.24766874231271824 valid 0.3164413584315259
LOSS train 0.24766874231271824 valid 0.3153846077620983
LOSS train 0.24766874231271824 valid 0.31328163266181946
LOSS train 0.24766874231271824 valid 0.3143292780105884
LOSS train 0.24766874231271824 valid 0.3134455691885065
LOSS train 0.24766874231271824 valid 0.313693357365472
LOSS train 0.24766874231271824 valid 0.3134985237286009
LOSS train 0.24766874231271824 valid 0.31582647462685903
LOSS train 0.24766874231271824 valid 0.3168767421476303
LOSS train 0.24766874231271824 valid 0.3167645940557122
LOSS train 0.24766874231271824 valid 0.31778280211217474
LOSS train 0.24766874231271824 valid 0.31771911242428946
LOSS train 0.24766874231271824 valid 0.31939404777118136
LOSS train 0.24766874231271824 valid 0.31903518984715146
LOSS train 0.24766874231271824 valid 0.3191052949106371
LOSS train 0.24766874231271824 valid 0.31940349701203796
LOSS train 0.24766874231271824 valid 0.31923831884677595
LOSS train 0.24766874231271824 valid 0.3195283442735672
LOSS train 0.24766874231271824 valid 0.3203091679549799
LOSS train 0.24766874231271824 valid 0.3201531938144139
LOSS train 0.24766874231271824 valid 0.3200515321520872
LOSS train 0.24766874231271824 valid 0.3210741389881481
LOSS train 0.24766874231271824 valid 0.3204487601915995
LOSS train 0.24766874231271824 valid 0.3210848278325537
LOSS train 0.24766874231271824 valid 0.32126963265398717
LOSS train 0.24766874231271824 valid 0.3213569354265928
LOSS train 0.24766874231271824 valid 0.32208610128383247
LOSS train 0.24766874231271824 valid 0.32143135368824005
LOSS train 0.24766874231271824 valid 0.32210516520574983
LOSS train 0.24766874231271824 valid 0.32204377765838915
LOSS train 0.24766874231271824 valid 0.3222363900463536
LOSS train 0.24766874231271824 valid 0.3217308112868556
LOSS train 0.24766874231271824 valid 0.32183103073727
LOSS train 0.24766874231271824 valid 0.3212427890726498
LOSS train 0.24766874231271824 valid 0.32077427861983315
LOSS train 0.24766874231271824 valid 0.3203253771724372
LOSS train 0.24766874231271824 valid 0.3206640709254701
LOSS train 0.24766874231271824 valid 0.31994974265495935
LOSS train 0.24766874231271824 valid 0.32011517882347107
LOSS train 0.24766874231271824 valid 0.32131850527178857
LOSS train 0.24766874231271824 valid 0.32205567236930605
LOSS train 0.24766874231271824 valid 0.3231873116455972
LOSS train 0.24766874231271824 valid 0.3232939866872934
LOSS train 0.24766874231271824 valid 0.3236075395887548
LOSS train 0.24766874231271824 valid 0.32291206539566836
LOSS train 0.24766874231271824 valid 0.3226196963120909
LOSS train 0.24766874231271824 valid 0.3216649207515993
LOSS train 0.24766874231271824 valid 0.3215948705162321
LOSS train 0.24766874231271824 valid 0.3216555349423852
LOSS train 0.24766874231271824 valid 0.32175807447897065
LOSS train 0.24766874231271824 valid 0.3215836715208341
LOSS train 0.24766874231271824 valid 0.32151390048297673
LOSS train 0.24766874231271824 valid 0.32168381969134013
LOSS train 0.24766874231271824 valid 0.3221259462205987
LOSS train 0.24766874231271824 valid 0.3224865511640326
LOSS train 0.24766874231271824 valid 0.3224066484432954
LOSS train 0.24766874231271824 valid 0.32233155829997
LOSS train 0.24766874231271824 valid 0.32137611545622347
LOSS train 0.24766874231271824 valid 0.3204061812088813
LOSS train 0.24766874231271824 valid 0.32058654035009987
LOSS train 0.24766874231271824 valid 0.3202321963856019
LOSS train 0.24766874231271824 valid 0.32016913486378534
LOSS train 0.24766874231271824 valid 0.3196798524435829
LOSS train 0.24766874231271824 valid 0.31887616251790246
LOSS train 0.24766874231271824 valid 0.31905093069734247
LOSS train 0.24766874231271824 valid 0.3183733045377515
LOSS train 0.24766874231271824 valid 0.31844804829426027
LOSS train 0.24766874231271824 valid 0.3187881526019838
LOSS train 0.24766874231271824 valid 0.3188777146758614
LOSS train 0.24766874231271824 valid 0.31886099246533023
LOSS train 0.24766874231271824 valid 0.31859347544690614
LOSS train 0.24766874231271824 valid 0.31885588105688706
LOSS train 0.24766874231271824 valid 0.3184433334752133
LOSS train 0.24766874231271824 valid 0.3188150155668457
LOSS train 0.24766874231271824 valid 0.31899753090032595
LOSS train 0.24766874231271824 valid 0.3190659126456903
LOSS train 0.24766874231271824 valid 0.31931414778786477
LOSS train 0.24766874231271824 valid 0.31971088200807574
LOSS train 0.24766874231271824 valid 0.31996160657098976
LOSS train 0.24766874231271824 valid 0.3199948878849254
LOSS train 0.24766874231271824 valid 0.32013953166100584
LOSS train 0.24766874231271824 valid 0.32007574719878346
LOSS train 0.24766874231271824 valid 0.3201219697793325
LOSS train 0.24766874231271824 valid 0.320519394750865
LOSS train 0.24766874231271824 valid 0.3201601808873292
LOSS train 0.24766874231271824 valid 0.32024416106718556
LOSS train 0.24766874231271824 valid 0.32078681298352163
LOSS train 0.24766874231271824 valid 0.3212162727659399
LOSS train 0.24766874231271824 valid 0.3207458684573302
LOSS train 0.24766874231271824 valid 0.32032341456839014
LOSS train 0.24766874231271824 valid 0.3202154219678018
LOSS train 0.24766874231271824 valid 0.32017481013348226
LOSS train 0.24766874231271824 valid 0.3200316377308058
LOSS train 0.24766874231271824 valid 0.32036891348403074
LOSS train 0.24766874231271824 valid 0.32084570430282855
LOSS train 0.24766874231271824 valid 0.3206941889504255
LOSS train 0.24766874231271824 valid 0.32059263381637443
LOSS train 0.24766874231271824 valid 0.32014029026031493
LOSS train 0.24766874231271824 valid 0.3199795853993124
LOSS train 0.24766874231271824 valid 0.3197857906095317
LOSS train 0.24766874231271824 valid 0.3198255588853262
LOSS train 0.24766874231271824 valid 0.32018862424358246
LOSS train 0.24766874231271824 valid 0.32007133030891416
LOSS train 0.24766874231271824 valid 0.32032001207745264
LOSS train 0.24766874231271824 valid 0.3201193025731665
LOSS train 0.24766874231271824 valid 0.3207121554296464
LOSS train 0.24766874231271824 valid 0.3206985835419145
LOSS train 0.24766874231271824 valid 0.32077074669874633
LOSS train 0.24766874231271824 valid 0.32066093009846813
LOSS train 0.24766874231271824 valid 0.32019281116398896
LOSS train 0.24766874231271824 valid 0.3199147582054138
LOSS train 0.24766874231271824 valid 0.3199216291975619
LOSS train 0.24766874231271824 valid 0.319962685196488
LOSS train 0.24766874231271824 valid 0.320061033262926
LOSS train 0.24766874231271824 valid 0.3198535300954415
LOSS train 0.24766874231271824 valid 0.3198220114345136
LOSS train 0.24766874231271824 valid 0.31964645025541455
LOSS train 0.24766874231271824 valid 0.31981404721736906
LOSS train 0.24766874231271824 valid 0.31971386476611413
LOSS train 0.24766874231271824 valid 0.31997297200518593
LOSS train 0.24766874231271824 valid 0.31992192014114007
LOSS train 0.24766874231271824 valid 0.31994084475768936
LOSS train 0.24766874231271824 valid 0.3196308721756113
LOSS train 0.24766874231271824 valid 0.3199243961948238
LOSS train 0.24766874231271824 valid 0.3196557567638605
LOSS train 0.24766874231271824 valid 0.32048882523903977
LOSS train 0.24766874231271824 valid 0.32042671830062097
LOSS train 0.24766874231271824 valid 0.3204459313551585
LOSS train 0.24766874231271824 valid 0.3202816638725483
LOSS train 0.24766874231271824 valid 0.31999476018704864
LOSS train 0.24766874231271824 valid 0.3199356328817754
LOSS train 0.24766874231271824 valid 0.3199080097210872
LOSS train 0.24766874231271824 valid 0.3201194130605267
LOSS train 0.24766874231271824 valid 0.3204658488050485
LOSS train 0.24766874231271824 valid 0.3204102969852982
LOSS train 0.24766874231271824 valid 0.3203084102914303
LOSS train 0.24766874231271824 valid 0.3203000889259314
LOSS train 0.24766874231271824 valid 0.31992949042469265
LOSS train 0.24766874231271824 valid 0.3200247783838592
LOSS train 0.24766874231271824 valid 0.3199627923744696
LOSS train 0.24766874231271824 valid 0.3199104399768853
LOSS train 0.24766874231271824 valid 0.3196875602006912
LOSS train 0.24766874231271824 valid 0.31947780937859505
LOSS train 0.24766874231271824 valid 0.31976587172732296
LOSS train 0.24766874231271824 valid 0.31982118051922964
LOSS train 0.24766874231271824 valid 0.3197987696954182
LOSS train 0.24766874231271824 valid 0.3198912618075602
LOSS train 0.24766874231271824 valid 0.3202351045959136
LOSS train 0.24766874231271824 valid 0.32008514209100375
LOSS train 0.24766874231271824 valid 0.31973646927711574
LOSS train 0.24766874231271824 valid 0.31975972428487215
LOSS train 0.24766874231271824 valid 0.3196233716504327
LOSS train 0.24766874231271824 valid 0.3195644875935146
LOSS train 0.24766874231271824 valid 0.31958380798724567
LOSS train 0.24766874231271824 valid 0.3196906311027074
LOSS train 0.24766874231271824 valid 0.31988800476106366
LOSS train 0.24766874231271824 valid 0.31982104655084664
LOSS train 0.24766874231271824 valid 0.31980108585622574
LOSS train 0.24766874231271824 valid 0.32003485843621565
LOSS train 0.24766874231271824 valid 0.32011836996445286
LOSS train 0.24766874231271824 valid 0.3198357390575722
LOSS train 0.24766874231271824 valid 0.3197727775120217
LOSS train 0.24766874231271824 valid 0.3197084521925127
LOSS train 0.24766874231271824 valid 0.31975237352232777
LOSS train 0.24766874231271824 valid 0.31957959634734984
LOSS train 0.24766874231271824 valid 0.3196549329985964
LOSS train 0.24766874231271824 valid 0.3198246621580982
LOSS train 0.24766874231271824 valid 0.31993106054632287
LOSS train 0.24766874231271824 valid 0.3198697925862217
LOSS train 0.24766874231271824 valid 0.31985436876614887
LOSS train 0.24766874231271824 valid 0.32005479530349296
LOSS train 0.24766874231271824 valid 0.319843809629224
LOSS train 0.24766874231271824 valid 0.3193731461579983
LOSS train 0.24766874231271824 valid 0.31957487511087435
LOSS train 0.24766874231271824 valid 0.3196977516872629
LOSS train 0.24766874231271824 valid 0.31973685286562853
LOSS train 0.24766874231271824 valid 0.31978667014507794
LOSS train 0.24766874231271824 valid 0.31950058974325657
LOSS train 0.24766874231271824 valid 0.319333300142739
LOSS train 0.24766874231271824 valid 0.31920792818954674
LOSS train 0.24766874231271824 valid 0.31913744015940304
LOSS train 0.24766874231271824 valid 0.31912890054723797
LOSS train 0.24766874231271824 valid 0.3189468422314016
LOSS train 0.24766874231271824 valid 0.31886122390193844
LOSS train 0.24766874231271824 valid 0.3186928832156647
LOSS train 0.24766874231271824 valid 0.3186310912983922
LOSS train 0.24766874231271824 valid 0.31853152369483234
LOSS train 0.24766874231271824 valid 0.31865371742418835
LOSS train 0.24766874231271824 valid 0.31867537615706004
LOSS train 0.24766874231271824 valid 0.31859797077640045
LOSS train 0.24766874231271824 valid 0.3186506387633337
LOSS train 0.24766874231271824 valid 0.3186396376412606
LOSS train 0.24766874231271824 valid 0.3183883960163871
LOSS train 0.24766874231271824 valid 0.31836117024498956
LOSS train 0.24766874231271824 valid 0.31842905811725125
LOSS train 0.24766874231271824 valid 0.3183848354111024
LOSS train 0.24766874231271824 valid 0.31846934291597917
LOSS train 0.24766874231271824 valid 0.3186509779231115
LOSS train 0.24766874231271824 valid 0.31871935327398293
LOSS train 0.24766874231271824 valid 0.31871670370434857
LOSS train 0.24766874231271824 valid 0.31884691860911024
LOSS train 0.24766874231271824 valid 0.3189619136308985
LOSS train 0.24766874231271824 valid 0.3190421213706334
LOSS train 0.24766874231271824 valid 0.3192138527478792
LOSS train 0.24766874231271824 valid 0.3195647166032623
LOSS train 0.24766874231271824 valid 0.3196393219394642
LOSS train 0.24766874231271824 valid 0.3196259029697643
LOSS train 0.24766874231271824 valid 0.31973510572443836
LOSS train 0.24766874231271824 valid 0.3197166355270328
LOSS train 0.24766874231271824 valid 0.319809242119563
LOSS train 0.24766874231271824 valid 0.3197062842451963
LOSS train 0.24766874231271824 valid 0.3197608631518152
LOSS train 0.24766874231271824 valid 0.3198212203193218
LOSS train 0.24766874231271824 valid 0.31974999133055493
LOSS train 0.24766874231271824 valid 0.3197451311198971
LOSS train 0.24766874231271824 valid 0.31977758281120733
LOSS train 0.24766874231271824 valid 0.31963881008545225
LOSS train 0.24766874231271824 valid 0.3196118978783488
LOSS train 0.24766874231271824 valid 0.3197624557864122
LOSS train 0.24766874231271824 valid 0.3195391275789127
LOSS train 0.24766874231271824 valid 0.31968937716121043
LOSS train 0.24766874231271824 valid 0.31992955402028367
LOSS train 0.24766874231271824 valid 0.32005694307843036
LOSS train 0.24766874231271824 valid 0.3200284422534268
LOSS train 0.24766874231271824 valid 0.3198841003633221
LOSS train 0.24766874231271824 valid 0.3198194260438604
LOSS train 0.24766874231271824 valid 0.3197302584308218
LOSS train 0.24766874231271824 valid 0.3198441674113274
LOSS train 0.24766874231271824 valid 0.3198542796164395
LOSS train 0.24766874231271824 valid 0.3199170897049563
LOSS train 0.24766874231271824 valid 0.31984896910756
LOSS train 0.24766874231271824 valid 0.3198576012816955
LOSS train 0.24766874231271824 valid 0.31992467694422777
LOSS train 0.24766874231271824 valid 0.3200997018138878
LOSS train 0.24766874231271824 valid 0.3199008473046559
LOSS train 0.24766874231271824 valid 0.31987048622018605
LOSS train 0.24766874231271824 valid 0.31985622063336683
LOSS train 0.24766874231271824 valid 0.3198695681989193
LOSS train 0.24766874231271824 valid 0.3198853215609474
LOSS train 0.24766874231271824 valid 0.3198966216153771
LOSS train 0.24766874231271824 valid 0.3198774269438515
LOSS train 0.24766874231271824 valid 0.31990607720658637
LOSS train 0.24766874231271824 valid 0.3197760326120089
LOSS train 0.24766874231271824 valid 0.3197721613192917
LOSS train 0.24766874231271824 valid 0.3198837567023124
LOSS train 0.24766874231271824 valid 0.3200767369968678
LOSS train 0.24766874231271824 valid 0.32019337474413523
LOSS train 0.24766874231271824 valid 0.3201736574371656
LOSS train 0.24766874231271824 valid 0.3201804409057892
LOSS train 0.24766874231271824 valid 0.32040906319504275
LOSS train 0.24766874231271824 valid 0.32040575457798254
LOSS train 0.24766874231271824 valid 0.32041191777391154
LOSS train 0.24766874231271824 valid 0.3203239242055199
LOSS train 0.24766874231271824 valid 0.32028642669320107
LOSS train 0.24766874231271824 valid 0.3200191128124829
LOSS train 0.24766874231271824 valid 0.3197509097002393
LOSS train 0.24766874231271824 valid 0.3197673497768286
LOSS train 0.24766874231271824 valid 0.31976105639977115
LOSS train 0.24766874231271824 valid 0.31972247537132686
LOSS train 0.24766874231271824 valid 0.31951369477290636
LOSS train 0.24766874231271824 valid 0.3195633022081726
LOSS train 0.24766874231271824 valid 0.31965385099322025
LOSS train 0.24766874231271824 valid 0.3196485227137281
LOSS train 0.24766874231271824 valid 0.31951914013890953
LOSS train 0.24766874231271824 valid 0.31939317867523287
LOSS train 0.24766874231271824 valid 0.3195181562979188
LOSS train 0.24766874231271824 valid 0.3195459196621159
LOSS train 0.24766874231271824 valid 0.31958889062034673
LOSS train 0.24766874231271824 valid 0.31963345741283444
LOSS train 0.24766874231271824 valid 0.31967380413249746
LOSS train 0.24766874231271824 valid 0.31968811657851876
LOSS train 0.24766874231271824 valid 0.3198796310493735
LOSS train 0.24766874231271824 valid 0.319950314548056
LOSS train 0.24766874231271824 valid 0.319905291286272
LOSS train 0.24766874231271824 valid 0.31993798971778215
LOSS train 0.24766874231271824 valid 0.31994064887298035
LOSS train 0.24766874231271824 valid 0.3200291691715502
LOSS train 0.24766874231271824 valid 0.3199825242658456
LOSS train 0.24766874231271824 valid 0.3201214198357243
LOSS train 0.24766874231271824 valid 0.32010432707750247
LOSS train 0.24766874231271824 valid 0.32021011235100205
LOSS train 0.24766874231271824 valid 0.3202273378540811
LOSS train 0.24766874231271824 valid 0.3201392299327694
LOSS train 0.24766874231271824 valid 0.3200338944987534
LOSS train 0.24766874231271824 valid 0.3200156133901024
LOSS train 0.24766874231271824 valid 0.3199387245460764
LOSS train 0.24766874231271824 valid 0.32003165067785383
LOSS train 0.24766874231271824 valid 0.3200413523662475
LOSS train 0.24766874231271824 valid 0.3198484541231413
LOSS train 0.24766874231271824 valid 0.3199500056126943
LOSS train 0.24766874231271824 valid 0.31994337828966757
LOSS train 0.24766874231271824 valid 0.3200518592812453
LOSS train 0.24766874231271824 valid 0.3200577695691396
LOSS train 0.24766874231271824 valid 0.3199467290711554
LOSS train 0.24766874231271824 valid 0.3199678786827563
LOSS train 0.24766874231271824 valid 0.32000433644221266
LOSS train 0.24766874231271824 valid 0.32005892105423916
LOSS train 0.24766874231271824 valid 0.3199895326513797
LOSS train 0.24766874231271824 valid 0.32012368506545963
LOSS train 0.24766874231271824 valid 0.3200544155439975
LOSS train 0.24766874231271824 valid 0.3200316004247488
LOSS train 0.24766874231271824 valid 0.3200613727080233
LOSS train 0.24766874231271824 valid 0.32001532953519085
LOSS train 0.24766874231271824 valid 0.32019288933898776
LOSS train 0.24766874231271824 valid 0.3203128956782344
LOSS train 0.24766874231271824 valid 0.3202941394524603
LOSS train 0.24766874231271824 valid 0.3204685900258438
LOSS train 0.24766874231271824 valid 0.3204161058321144
LOSS train 0.24766874231271824 valid 0.32036465627009053
LOSS train 0.24766874231271824 valid 0.32018976564328355
LOSS train 0.24766874231271824 valid 0.32020355197223455
LOSS train 0.24766874231271824 valid 0.32034025350195205
LOSS train 0.24766874231271824 valid 0.32037443380747266
LOSS train 0.24766874231271824 valid 0.32038988842673244
LOSS train 0.24766874231271824 valid 0.320266258247882
LOSS train 0.24766874231271824 valid 0.3202309011972162
LOSS train 0.24766874231271824 valid 0.32024204243073423
LOSS train 0.24766874231271824 valid 0.3202617737738525
LOSS train 0.24766874231271824 valid 0.32010982458717313
LOSS train 0.24766874231271824 valid 0.3200955412279793
LOSS train 0.24766874231271824 valid 0.3200694642765529
LOSS train 0.24766874231271824 valid 0.3202816392186769
LOSS train 0.24766874231271824 valid 0.32029216527075005
LOSS train 0.24766874231271824 valid 0.32013557476632165
LOSS train 0.24766874231271824 valid 0.32007119914124954
LOSS train 0.24766874231271824 valid 0.32003435145678194
LOSS train 0.24766874231271824 valid 0.32015106120389647
LOSS train 0.24766874231271824 valid 0.3199554352675165
LOSS train 0.24766874231271824 valid 0.319860608107344
LOSS train 0.24766874231271824 valid 0.3197376312772659
LOSS train 0.24766874231271824 valid 0.3198499785519862
LOSS train 0.24766874231271824 valid 0.31993600974480313
LOSS train 0.24766874231271824 valid 0.3200709130982278
LOSS train 0.24766874231271824 valid 0.320141923603382
LOSS train 0.24766874231271824 valid 0.3201186683248071
LOSS train 0.24766874231271824 valid 0.3200176872271399
LOSS train 0.24766874231271824 valid 0.32000302483942517
LOSS train 0.24766874231271824 valid 0.3198931809514761
LOSS train 0.24766874231271824 valid 0.31976510493544
LOSS train 0.24766874231271824 valid 0.31992074801941606
LOSS train 0.24766874231271824 valid 0.3197527224449415
LOSS train 0.24766874231271824 valid 0.3197202794253826
LOSS train 0.24766874231271824 valid 0.31966738884579643
LOSS train 0.24766874231271824 valid 0.31958855993923596
LOSS train 0.24766874231271824 valid 0.3194927549248495
LOSS train 0.24766874231271824 valid 0.3194417208838074
LOSS train 0.24766874231271824 valid 0.31945336902852306
EPOCH 15:
  batch 1 loss: 0.28325557708740234
  batch 2 loss: 0.26388034224510193
  batch 3 loss: 0.25700973471005756
  batch 4 loss: 0.2568691596388817
  batch 5 loss: 0.26823516488075255
  batch 6 loss: 0.2616664419571559
  batch 7 loss: 0.2612862501825605
  batch 8 loss: 0.26352470740675926
  batch 9 loss: 0.2592444469531377
  batch 10 loss: 0.2593548968434334
  batch 11 loss: 0.2590381760488857
  batch 12 loss: 0.2591555354495843
  batch 13 loss: 0.25895272424587834
  batch 14 loss: 0.25811791100672316
  batch 15 loss: 0.2596714725097021
  batch 16 loss: 0.25922517012804747
  batch 17 loss: 0.2578872354591594
  batch 18 loss: 0.2590906206104491
  batch 19 loss: 0.25970009283015605
  batch 20 loss: 0.2592705026268959
  batch 21 loss: 0.2586942386059534
  batch 22 loss: 0.25816839120604773
  batch 23 loss: 0.25656862621722015
  batch 24 loss: 0.25469815110166866
  batch 25 loss: 0.2562205731868744
  batch 26 loss: 0.2560436118107576
  batch 27 loss: 0.25600268884941385
  batch 28 loss: 0.25431320177657263
  batch 29 loss: 0.25472602042658576
  batch 30 loss: 0.25470682283242546
  batch 31 loss: 0.2549044922474892
  batch 32 loss: 0.25458926195278764
  batch 33 loss: 0.2547819022879456
  batch 34 loss: 0.25551366148626103
  batch 35 loss: 0.2551434474331992
  batch 36 loss: 0.2547730389568541
  batch 37 loss: 0.2543187246129319
  batch 38 loss: 0.2550043895056373
  batch 39 loss: 0.25424089492895663
  batch 40 loss: 0.25399649478495123
  batch 41 loss: 0.25341854735118585
  batch 42 loss: 0.25336706780252005
  batch 43 loss: 0.2525895393865053
  batch 44 loss: 0.25241144543344324
  batch 45 loss: 0.2518282069100274
  batch 46 loss: 0.2515455192845801
  batch 47 loss: 0.25121871746600943
  batch 48 loss: 0.2502775186051925
  batch 49 loss: 0.2502274336863537
  batch 50 loss: 0.24972450047731398
  batch 51 loss: 0.24916225759422078
  batch 52 loss: 0.24916809080885008
  batch 53 loss: 0.24823279678821564
  batch 54 loss: 0.248022825077728
  batch 55 loss: 0.24676129249009218
  batch 56 loss: 0.2461083810776472
  batch 57 loss: 0.24514322505708327
  batch 58 loss: 0.2448113678858198
  batch 59 loss: 0.24485792118614003
  batch 60 loss: 0.2441708743572235
  batch 61 loss: 0.24419016696390558
  batch 62 loss: 0.24373036598966968
  batch 63 loss: 0.2426021399479064
  batch 64 loss: 0.24317551753483713
  batch 65 loss: 0.242507666349411
  batch 66 loss: 0.2420772731755719
  batch 67 loss: 0.24261651799749973
  batch 68 loss: 0.24301888070562305
  batch 69 loss: 0.24307450803293698
  batch 70 loss: 0.2432022539632661
  batch 71 loss: 0.24279756655155774
  batch 72 loss: 0.24287546881371075
  batch 73 loss: 0.2425529709825777
  batch 74 loss: 0.24271717083615227
  batch 75 loss: 0.2421341629823049
  batch 76 loss: 0.24246393340198616
  batch 77 loss: 0.24187606244118182
  batch 78 loss: 0.24202017810864326
  batch 79 loss: 0.242273097739944
  batch 80 loss: 0.24195918180048465
  batch 81 loss: 0.24215030633373025
  batch 82 loss: 0.24262992201781855
  batch 83 loss: 0.24257421816687985
  batch 84 loss: 0.24268629543838047
  batch 85 loss: 0.242547948921428
  batch 86 loss: 0.24280902154223863
  batch 87 loss: 0.2427069585898827
  batch 88 loss: 0.2423856437883594
  batch 89 loss: 0.24206968941045612
  batch 90 loss: 0.2420458280377918
  batch 91 loss: 0.24202266057114025
  batch 92 loss: 0.24175787441756413
  batch 93 loss: 0.24165372806851582
  batch 94 loss: 0.24200898012582292
  batch 95 loss: 0.2417307839581841
  batch 96 loss: 0.24179428862407804
  batch 97 loss: 0.24210925000844544
  batch 98 loss: 0.2424312013448501
  batch 99 loss: 0.242683494181344
  batch 100 loss: 0.24285313323140145
  batch 101 loss: 0.24292164554100226
  batch 102 loss: 0.24322796612977982
  batch 103 loss: 0.24360084432421378
  batch 104 loss: 0.24372168616033518
  batch 105 loss: 0.24349149011430285
  batch 106 loss: 0.24399540812339424
  batch 107 loss: 0.24351103798808338
  batch 108 loss: 0.24327587501870263
  batch 109 loss: 0.243043882721061
  batch 110 loss: 0.24297297542745416
  batch 111 loss: 0.24300345697918455
  batch 112 loss: 0.24256608209439687
  batch 113 loss: 0.24262730447591935
  batch 114 loss: 0.24316502767696715
  batch 115 loss: 0.24322901186735735
  batch 116 loss: 0.24328753403548536
  batch 117 loss: 0.243670297993554
  batch 118 loss: 0.24375468896607222
  batch 119 loss: 0.24398834840590214
  batch 120 loss: 0.24423622662822406
  batch 121 loss: 0.24444930932738565
  batch 122 loss: 0.24458749690016762
  batch 123 loss: 0.244384493406226
  batch 124 loss: 0.24464788131656184
  batch 125 loss: 0.24455392026901246
  batch 126 loss: 0.24454861500906566
  batch 127 loss: 0.2447890484426904
  batch 128 loss: 0.24491906631737947
  batch 129 loss: 0.24541759953018308
  batch 130 loss: 0.246048654959752
  batch 131 loss: 0.24628977989422457
  batch 132 loss: 0.24657593125646765
  batch 133 loss: 0.2469450903118105
  batch 134 loss: 0.24699827316981643
  batch 135 loss: 0.2471040878030989
  batch 136 loss: 0.2471746857113698
  batch 137 loss: 0.24724262931051044
  batch 138 loss: 0.24764419185078662
  batch 139 loss: 0.24794580910703262
  batch 140 loss: 0.2479523437363761
  batch 141 loss: 0.24814899783607916
  batch 142 loss: 0.248081247361613
  batch 143 loss: 0.24779853599888463
  batch 144 loss: 0.2478015889517135
  batch 145 loss: 0.24759412788111587
  batch 146 loss: 0.24763085509408012
  batch 147 loss: 0.24792304341079427
  batch 148 loss: 0.24802651004614057
  batch 149 loss: 0.2480785401675525
  batch 150 loss: 0.2480193892121315
  batch 151 loss: 0.24827455438130738
  batch 152 loss: 0.24845386914124615
  batch 153 loss: 0.2483321640616149
  batch 154 loss: 0.248556525869803
  batch 155 loss: 0.24850811266130016
  batch 156 loss: 0.24869409050696936
  batch 157 loss: 0.24900980056471125
  batch 158 loss: 0.24904453150833708
  batch 159 loss: 0.24952640173570165
  batch 160 loss: 0.24955571237951518
  batch 161 loss: 0.2497808598213314
  batch 162 loss: 0.24992828107910392
  batch 163 loss: 0.2497854681651285
  batch 164 loss: 0.24979028369231923
  batch 165 loss: 0.2499223533001813
  batch 166 loss: 0.2500358323555395
  batch 167 loss: 0.2498891612369857
  batch 168 loss: 0.24990689151343845
  batch 169 loss: 0.24977811985820003
  batch 170 loss: 0.24948472775080624
  batch 171 loss: 0.24945180789071913
  batch 172 loss: 0.24924306640791338
  batch 173 loss: 0.24929051299315658
  batch 174 loss: 0.24931603618736925
  batch 175 loss: 0.24941702127456666
  batch 176 loss: 0.2492449585009705
  batch 177 loss: 0.24938062490042995
  batch 178 loss: 0.2494898009836004
  batch 179 loss: 0.2495172839590957
  batch 180 loss: 0.24939616728160116
  batch 181 loss: 0.24930946726496048
  batch 182 loss: 0.24933942849491977
  batch 183 loss: 0.24928044336415378
  batch 184 loss: 0.24912668877969618
  batch 185 loss: 0.24928347017313984
  batch 186 loss: 0.24932240782886422
  batch 187 loss: 0.24919447087667843
  batch 188 loss: 0.24893687673388643
  batch 189 loss: 0.2486949540950634
  batch 190 loss: 0.2488122809874384
  batch 191 loss: 0.24891948231851868
  batch 192 loss: 0.24897404884298643
  batch 193 loss: 0.24898450315925125
  batch 194 loss: 0.24923057454763
  batch 195 loss: 0.2494379768004784
  batch 196 loss: 0.24935494735836983
  batch 197 loss: 0.24929170121396255
  batch 198 loss: 0.24953639853482296
  batch 199 loss: 0.24973146564996423
  batch 200 loss: 0.24990082964301108
  batch 201 loss: 0.2499965267691446
  batch 202 loss: 0.24994935965774082
  batch 203 loss: 0.24988552296690167
  batch 204 loss: 0.2496890220425877
  batch 205 loss: 0.24983058608159786
  batch 206 loss: 0.24976508666589423
  batch 207 loss: 0.24970065654763854
  batch 208 loss: 0.24969080849908865
  batch 209 loss: 0.24943052082540887
  batch 210 loss: 0.24944436507565634
  batch 211 loss: 0.24927684516420862
  batch 212 loss: 0.24909828511892623
  batch 213 loss: 0.24910167825054114
  batch 214 loss: 0.24912989877651784
  batch 215 loss: 0.248848942130111
  batch 216 loss: 0.24876796247230637
  batch 217 loss: 0.24870588036451471
  batch 218 loss: 0.24859158127406322
  batch 219 loss: 0.24865465601013131
  batch 220 loss: 0.2486507307399403
  batch 221 loss: 0.24872309508906232
  batch 222 loss: 0.24870943935873271
  batch 223 loss: 0.24867235017197015
  batch 224 loss: 0.24863342461841448
  batch 225 loss: 0.2485098228851954
  batch 226 loss: 0.2485907609879443
  batch 227 loss: 0.24833212681278782
  batch 228 loss: 0.24819356386075941
  batch 229 loss: 0.24802720416581267
  batch 230 loss: 0.24800435771112855
  batch 231 loss: 0.24789307618037962
  batch 232 loss: 0.2476821123012181
  batch 233 loss: 0.24767412545854953
  batch 234 loss: 0.2476250498722761
  batch 235 loss: 0.24756348507201417
  batch 236 loss: 0.24750311192819627
  batch 237 loss: 0.24742270295630026
  batch 238 loss: 0.24741161721093313
  batch 239 loss: 0.24736755263356483
  batch 240 loss: 0.24748772829771043
  batch 241 loss: 0.24755853810251005
  batch 242 loss: 0.24750535399460596
  batch 243 loss: 0.24746185111901398
  batch 244 loss: 0.24743747796680107
  batch 245 loss: 0.2473756945863062
  batch 246 loss: 0.24725226658146557
  batch 247 loss: 0.2472173179450788
  batch 248 loss: 0.24709803827347293
  batch 249 loss: 0.24702164255471593
  batch 250 loss: 0.2468496675491333
  batch 251 loss: 0.24671861725261962
  batch 252 loss: 0.24660095838563784
  batch 253 loss: 0.24655183819914053
  batch 254 loss: 0.24652136170019315
  batch 255 loss: 0.2464760863313488
  batch 256 loss: 0.24641840683761984
  batch 257 loss: 0.24650085621770718
  batch 258 loss: 0.24645388530668363
  batch 259 loss: 0.2465143669756223
  batch 260 loss: 0.246568716947849
  batch 261 loss: 0.24670299522264708
  batch 262 loss: 0.24680655864813855
  batch 263 loss: 0.24689165573609645
  batch 264 loss: 0.2467818401302352
  batch 265 loss: 0.24679977522706087
  batch 266 loss: 0.2468224270899493
  batch 267 loss: 0.24691340349586716
  batch 268 loss: 0.2468159549049477
  batch 269 loss: 0.24670669303507609
  batch 270 loss: 0.24669911982836545
  batch 271 loss: 0.2465323171272489
  batch 272 loss: 0.24651553131201687
  batch 273 loss: 0.246470149292614
  batch 274 loss: 0.24661460377439096
  batch 275 loss: 0.24668377366932956
  batch 276 loss: 0.2466807498232178
  batch 277 loss: 0.24668625804061065
  batch 278 loss: 0.2465876199358659
  batch 279 loss: 0.2466264414103655
  batch 280 loss: 0.246528427994677
  batch 281 loss: 0.24649925063301237
  batch 282 loss: 0.24655163325105153
  batch 283 loss: 0.24655062097121463
  batch 284 loss: 0.246462699183276
  batch 285 loss: 0.24638941444848714
  batch 286 loss: 0.24623806409902507
  batch 287 loss: 0.2461934945828408
  batch 288 loss: 0.24592045187536213
  batch 289 loss: 0.2459577830398784
  batch 290 loss: 0.24572927119403049
  batch 291 loss: 0.24563778102193093
  batch 292 loss: 0.2457330948481821
  batch 293 loss: 0.24572789648693982
  batch 294 loss: 0.24562871699430505
  batch 295 loss: 0.2455297379675558
  batch 296 loss: 0.24562629911343795
  batch 297 loss: 0.2456800612034621
  batch 298 loss: 0.2456346117710107
  batch 299 loss: 0.24556047267539047
  batch 300 loss: 0.24553022508819897
  batch 301 loss: 0.2454824429314794
  batch 302 loss: 0.24541989916207774
  batch 303 loss: 0.24543066544107872
  batch 304 loss: 0.24532346697034021
  batch 305 loss: 0.24522121915074646
  batch 306 loss: 0.24530314128188527
  batch 307 loss: 0.24519347168917766
  batch 308 loss: 0.24524772162367772
  batch 309 loss: 0.24514378968951772
  batch 310 loss: 0.2450949903938078
  batch 311 loss: 0.2451387171864126
  batch 312 loss: 0.24525719608824986
  batch 313 loss: 0.24530416960343004
  batch 314 loss: 0.24526876643015322
  batch 315 loss: 0.24522003597683376
  batch 316 loss: 0.24516449581030048
  batch 317 loss: 0.2451451773335129
  batch 318 loss: 0.24514637451689197
  batch 319 loss: 0.24515440350042242
  batch 320 loss: 0.2450747394002974
  batch 321 loss: 0.24508261921992555
  batch 322 loss: 0.24502727838221544
  batch 323 loss: 0.24492514890044836
  batch 324 loss: 0.24477792121929887
  batch 325 loss: 0.24469500807615427
  batch 326 loss: 0.24471739419033192
  batch 327 loss: 0.2447918182301594
  batch 328 loss: 0.24466878380172136
  batch 329 loss: 0.24472793850674093
  batch 330 loss: 0.2446205884218216
  batch 331 loss: 0.2445957201485187
  batch 332 loss: 0.24452080793050399
  batch 333 loss: 0.24458785977091518
  batch 334 loss: 0.2445398698934538
  batch 335 loss: 0.24445887078989798
  batch 336 loss: 0.24441695253231696
  batch 337 loss: 0.24424201449053223
  batch 338 loss: 0.24422932275124556
  batch 339 loss: 0.2441649597830477
  batch 340 loss: 0.24412273111588814
  batch 341 loss: 0.2439258381768056
  batch 342 loss: 0.2439080356505879
  batch 343 loss: 0.24390722993685274
  batch 344 loss: 0.24391702055757822
  batch 345 loss: 0.24394322802191196
  batch 346 loss: 0.24384581215808845
  batch 347 loss: 0.24393689263107454
  batch 348 loss: 0.24399944497593518
  batch 349 loss: 0.24392378723040692
  batch 350 loss: 0.24395209278379168
  batch 351 loss: 0.24399671671736953
  batch 352 loss: 0.24414470995014365
  batch 353 loss: 0.244175944720044
  batch 354 loss: 0.2442809382906068
  batch 355 loss: 0.2443039893264502
  batch 356 loss: 0.2443564384338561
  batch 357 loss: 0.2442229099634315
  batch 358 loss: 0.2442923987377955
  batch 359 loss: 0.24436155459674952
  batch 360 loss: 0.24427376608881685
  batch 361 loss: 0.24425898046539762
  batch 362 loss: 0.24421766095727845
  batch 363 loss: 0.2441129066645278
  batch 364 loss: 0.24403076794940037
  batch 365 loss: 0.24399348236110113
  batch 366 loss: 0.243889031148022
  batch 367 loss: 0.24377547098927665
  batch 368 loss: 0.2436873575915461
  batch 369 loss: 0.2436659159214516
  batch 370 loss: 0.24359573967553474
  batch 371 loss: 0.24356342225543895
  batch 372 loss: 0.24344350906309262
  batch 373 loss: 0.24333097248390598
  batch 374 loss: 0.24320589248987443
  batch 375 loss: 0.24310869872570037
  batch 376 loss: 0.24307618464561218
  batch 377 loss: 0.24305718168813922
  batch 378 loss: 0.2429819640816835
  batch 379 loss: 0.24301163604517412
  batch 380 loss: 0.24301269505369036
  batch 381 loss: 0.2429617019932414
  batch 382 loss: 0.24286034617436494
  batch 383 loss: 0.24275818867876386
  batch 384 loss: 0.24268051058364412
  batch 385 loss: 0.24274724427755778
  batch 386 loss: 0.24261814124225953
  batch 387 loss: 0.2425742427354019
  batch 388 loss: 0.24267824470382376
  batch 389 loss: 0.2425596697931118
  batch 390 loss: 0.24244233469168344
  batch 391 loss: 0.2424772337574483
  batch 392 loss: 0.242480194652263
  batch 393 loss: 0.24255702244568114
  batch 394 loss: 0.24264505721137003
  batch 395 loss: 0.2425619656149345
  batch 396 loss: 0.24250051966219238
  batch 397 loss: 0.24249016018748584
  batch 398 loss: 0.24249127821706648
  batch 399 loss: 0.24249525256174848
  batch 400 loss: 0.24251369398087264
  batch 401 loss: 0.24242488196365852
  batch 402 loss: 0.24241100044096286
  batch 403 loss: 0.242416937615972
  batch 404 loss: 0.24241802103743695
  batch 405 loss: 0.24237525135646631
  batch 406 loss: 0.24250687765223639
  batch 407 loss: 0.24248028046815343
  batch 408 loss: 0.2425767944796997
  batch 409 loss: 0.24255936434333072
  batch 410 loss: 0.2425958094800391
  batch 411 loss: 0.2425557898488938
  batch 412 loss: 0.24251553087124547
  batch 413 loss: 0.24257389457837722
  batch 414 loss: 0.24258449234536306
  batch 415 loss: 0.24257295806723905
  batch 416 loss: 0.24263787212280127
  batch 417 loss: 0.24263965025794307
  batch 418 loss: 0.24256628712541178
  batch 419 loss: 0.24249610756490567
  batch 420 loss: 0.24246285845126425
  batch 421 loss: 0.24231458241186346
  batch 422 loss: 0.24242373524118938
  batch 423 loss: 0.24241302780648497
  batch 424 loss: 0.24236647106425943
  batch 425 loss: 0.2424449742190978
  batch 426 loss: 0.24240592555820661
  batch 427 loss: 0.2424323768191371
  batch 428 loss: 0.24237167114548594
  batch 429 loss: 0.2423638067879043
  batch 430 loss: 0.242322784592939
  batch 431 loss: 0.2425059826219165
  batch 432 loss: 0.24257362406286928
  batch 433 loss: 0.24260735745793402
  batch 434 loss: 0.24270586202496208
  batch 435 loss: 0.24270114401976267
  batch 436 loss: 0.24269596227538695
  batch 437 loss: 0.24279947752810724
  batch 438 loss: 0.24293288676978247
  batch 439 loss: 0.2429114269911564
  batch 440 loss: 0.24293696440078996
  batch 441 loss: 0.2428667402726993
  batch 442 loss: 0.24286299724789226
  batch 443 loss: 0.24286231694587465
  batch 444 loss: 0.24276136979460716
  batch 445 loss: 0.24283532222335258
  batch 446 loss: 0.24283105739697214
  batch 447 loss: 0.24276490489508482
  batch 448 loss: 0.24273181234353355
  batch 449 loss: 0.2428057090875035
  batch 450 loss: 0.2427570769521925
  batch 451 loss: 0.2427194515643786
  batch 452 loss: 0.24271479466583876
  batch 453 loss: 0.24263165953669855
  batch 454 loss: 0.24267221416145696
  batch 455 loss: 0.24264943999248548
  batch 456 loss: 0.24268820515850134
  batch 457 loss: 0.2427738916039206
  batch 458 loss: 0.2427656299812825
  batch 459 loss: 0.2427846290706809
  batch 460 loss: 0.24292662027089493
  batch 461 loss: 0.242951663029168
  batch 462 loss: 0.24301887448731954
  batch 463 loss: 0.24302728644715016
  batch 464 loss: 0.2430682263240732
  batch 465 loss: 0.2430213721849585
  batch 466 loss: 0.24299655000923018
  batch 467 loss: 0.2430533860875912
  batch 468 loss: 0.24306423958932233
  batch 469 loss: 0.2431461730682011
  batch 470 loss: 0.24315645387198062
  batch 471 loss: 0.2431705317236562
  batch 472 loss: 0.2430465133324013
LOSS train 0.2430465133324013 valid 0.328473836183548
LOSS train 0.2430465133324013 valid 0.30771516263484955
LOSS train 0.2430465133324013 valid 0.3160590132077535
LOSS train 0.2430465133324013 valid 0.3119516000151634
LOSS train 0.2430465133324013 valid 0.3023977160453796
LOSS train 0.2430465133324013 valid 0.30591027438640594
LOSS train 0.2430465133324013 valid 0.3129604629107884
LOSS train 0.2430465133324013 valid 0.3081747330725193
LOSS train 0.2430465133324013 valid 0.3119903372393714
LOSS train 0.2430465133324013 valid 0.31411142349243165
LOSS train 0.2430465133324013 valid 0.309227390722795
LOSS train 0.2430465133324013 valid 0.3119461809595426
LOSS train 0.2430465133324013 valid 0.31241788084690386
LOSS train 0.2430465133324013 valid 0.3135839125939778
LOSS train 0.2430465133324013 valid 0.3100253085295359
LOSS train 0.2430465133324013 valid 0.31107755191624165
LOSS train 0.2430465133324013 valid 0.31134462356567383
LOSS train 0.2430465133324013 valid 0.31375643279817367
LOSS train 0.2430465133324013 valid 0.316940786022889
LOSS train 0.2430465133324013 valid 0.3167925730347633
LOSS train 0.2430465133324013 valid 0.31572606024287997
LOSS train 0.2430465133324013 valid 0.31408434158021753
LOSS train 0.2430465133324013 valid 0.3152403352053269
LOSS train 0.2430465133324013 valid 0.31422583758831024
LOSS train 0.2430465133324013 valid 0.3119385039806366
LOSS train 0.2430465133324013 valid 0.31336300992048705
LOSS train 0.2430465133324013 valid 0.31239262333622686
LOSS train 0.2430465133324013 valid 0.31233749219349455
LOSS train 0.2430465133324013 valid 0.3119189718673969
LOSS train 0.2430465133324013 valid 0.31459731856981915
LOSS train 0.2430465133324013 valid 0.3154908668610357
LOSS train 0.2430465133324013 valid 0.31506779231131077
LOSS train 0.2430465133324013 valid 0.3162878246018381
LOSS train 0.2430465133324013 valid 0.31630591785206513
LOSS train 0.2430465133324013 valid 0.31806837831224716
LOSS train 0.2430465133324013 valid 0.3179309028718207
LOSS train 0.2430465133324013 valid 0.3180843116463842
LOSS train 0.2430465133324013 valid 0.31830347133310216
LOSS train 0.2430465133324013 valid 0.31826643149058026
LOSS train 0.2430465133324013 valid 0.3186841309070587
LOSS train 0.2430465133324013 valid 0.3196247001973594
LOSS train 0.2430465133324013 valid 0.3195890479144596
LOSS train 0.2430465133324013 valid 0.3194126840247664
LOSS train 0.2430465133324013 valid 0.3200715468688445
LOSS train 0.2430465133324013 valid 0.3195523394478692
LOSS train 0.2430465133324013 valid 0.32020367098891217
LOSS train 0.2430465133324013 valid 0.3204117848518047
LOSS train 0.2430465133324013 valid 0.32053464526931447
LOSS train 0.2430465133324013 valid 0.32127085571386377
LOSS train 0.2430465133324013 valid 0.32047875940799714
LOSS train 0.2430465133324013 valid 0.32109783326878266
LOSS train 0.2430465133324013 valid 0.3211491503394567
LOSS train 0.2430465133324013 valid 0.3213056320289396
LOSS train 0.2430465133324013 valid 0.3206758714384503
LOSS train 0.2430465133324013 valid 0.32075385668060996
LOSS train 0.2430465133324013 valid 0.32015503836529596
LOSS train 0.2430465133324013 valid 0.3196702531555243
LOSS train 0.2430465133324013 valid 0.3193466262570743
LOSS train 0.2430465133324013 valid 0.3198576316995136
LOSS train 0.2430465133324013 valid 0.3190746188163757
LOSS train 0.2430465133324013 valid 0.31929840709342333
LOSS train 0.2430465133324013 valid 0.3207279251467797
LOSS train 0.2430465133324013 valid 0.321362763170212
LOSS train 0.2430465133324013 valid 0.3224561996757984
LOSS train 0.2430465133324013 valid 0.3225878284527705
LOSS train 0.2430465133324013 valid 0.3229090575919007
LOSS train 0.2430465133324013 valid 0.32216073283508645
LOSS train 0.2430465133324013 valid 0.3217834080843365
LOSS train 0.2430465133324013 valid 0.3208506120287854
LOSS train 0.2430465133324013 valid 0.320696639588901
LOSS train 0.2430465133324013 valid 0.3208064407529965
LOSS train 0.2430465133324013 valid 0.32086480119162136
LOSS train 0.2430465133324013 valid 0.3204674108387673
LOSS train 0.2430465133324013 valid 0.3203253834634214
LOSS train 0.2430465133324013 valid 0.32067245999972027
LOSS train 0.2430465133324013 valid 0.32129918979971034
LOSS train 0.2430465133324013 valid 0.32170883092013275
LOSS train 0.2430465133324013 valid 0.32160484790802
LOSS train 0.2430465133324013 valid 0.32163096530528007
LOSS train 0.2430465133324013 valid 0.32063389755785465
LOSS train 0.2430465133324013 valid 0.31962225521788185
LOSS train 0.2430465133324013 valid 0.31984476799645073
LOSS train 0.2430465133324013 valid 0.31955479372696705
LOSS train 0.2430465133324013 valid 0.31955347156950403
LOSS train 0.2430465133324013 valid 0.31901099944815914
LOSS train 0.2430465133324013 valid 0.3182316718752994
LOSS train 0.2430465133324013 valid 0.3184488146812066
LOSS train 0.2430465133324013 valid 0.3175947556102818
LOSS train 0.2430465133324013 valid 0.31775447508592286
LOSS train 0.2430465133324013 valid 0.3181553444928593
LOSS train 0.2430465133324013 valid 0.31819866859650875
LOSS train 0.2430465133324013 valid 0.3182151617239351
LOSS train 0.2430465133324013 valid 0.31793074534144455
LOSS train 0.2430465133324013 valid 0.3182486322648982
LOSS train 0.2430465133324013 valid 0.31779478239385706
LOSS train 0.2430465133324013 valid 0.31816316038991016
LOSS train 0.2430465133324013 valid 0.31830096352346166
LOSS train 0.2430465133324013 valid 0.3183627850851234
LOSS train 0.2430465133324013 valid 0.3186008916659789
LOSS train 0.2430465133324013 valid 0.3188636563718319
LOSS train 0.2430465133324013 valid 0.3191378020709104
LOSS train 0.2430465133324013 valid 0.319202214041177
LOSS train 0.2430465133324013 valid 0.319324694646215
LOSS train 0.2430465133324013 valid 0.31911588847064054
LOSS train 0.2430465133324013 valid 0.31915492032255444
LOSS train 0.2430465133324013 valid 0.31949943796081365
LOSS train 0.2430465133324013 valid 0.3191523748302014
LOSS train 0.2430465133324013 valid 0.3192005683150556
LOSS train 0.2430465133324013 valid 0.3197939657016632
LOSS train 0.2430465133324013 valid 0.3202722445130348
LOSS train 0.2430465133324013 valid 0.31976581680345106
LOSS train 0.2430465133324013 valid 0.3193454538871135
LOSS train 0.2430465133324013 valid 0.31922950800013755
LOSS train 0.2430465133324013 valid 0.31918798217125105
LOSS train 0.2430465133324013 valid 0.31919885083385136
LOSS train 0.2430465133324013 valid 0.3194521417648628
LOSS train 0.2430465133324013 valid 0.32004989887404645
LOSS train 0.2430465133324013 valid 0.31992194473238317
LOSS train 0.2430465133324013 valid 0.31973988832045
LOSS train 0.2430465133324013 valid 0.31921084883312384
LOSS train 0.2430465133324013 valid 0.31900133956069787
LOSS train 0.2430465133324013 valid 0.3187046301413755
LOSS train 0.2430465133324013 valid 0.31868871113633723
LOSS train 0.2430465133324013 valid 0.31910949861330373
LOSS train 0.2430465133324013 valid 0.3190110288858414
LOSS train 0.2430465133324013 valid 0.31928897530786576
LOSS train 0.2430465133324013 valid 0.319017475632232
LOSS train 0.2430465133324013 valid 0.3197149211773649
LOSS train 0.2430465133324013 valid 0.3196849385204241
LOSS train 0.2430465133324013 valid 0.31971796303987504
LOSS train 0.2430465133324013 valid 0.31966549897466906
LOSS train 0.2430465133324013 valid 0.3191426275580218
LOSS train 0.2430465133324013 valid 0.3189541093612972
LOSS train 0.2430465133324013 valid 0.3190080202115116
LOSS train 0.2430465133324013 valid 0.31912974615891776
LOSS train 0.2430465133324013 valid 0.3192384612253484
LOSS train 0.2430465133324013 valid 0.31900144961193533
LOSS train 0.2430465133324013 valid 0.3188863138573757
LOSS train 0.2430465133324013 valid 0.31872400587840044
LOSS train 0.2430465133324013 valid 0.3189210366989885
LOSS train 0.2430465133324013 valid 0.31889120050778624
LOSS train 0.2430465133324013 valid 0.3191638783040181
LOSS train 0.2430465133324013 valid 0.31909836516096873
LOSS train 0.2430465133324013 valid 0.31920218954069746
LOSS train 0.2430465133324013 valid 0.31889320046737274
LOSS train 0.2430465133324013 valid 0.31928508320491605
LOSS train 0.2430465133324013 valid 0.31902109938008444
LOSS train 0.2430465133324013 valid 0.319860276640267
LOSS train 0.2430465133324013 valid 0.3197498544550582
LOSS train 0.2430465133324013 valid 0.3197656427820524
LOSS train 0.2430465133324013 valid 0.3196154332516209
LOSS train 0.2430465133324013 valid 0.31930286858819035
LOSS train 0.2430465133324013 valid 0.3191736534335255
LOSS train 0.2430465133324013 valid 0.31912639727453135
LOSS train 0.2430465133324013 valid 0.31941801396108443
LOSS train 0.2430465133324013 valid 0.31979025585147053
LOSS train 0.2430465133324013 valid 0.31973561312362647
LOSS train 0.2430465133324013 valid 0.31967577366512034
LOSS train 0.2430465133324013 valid 0.3196250040013835
LOSS train 0.2430465133324013 valid 0.3192991425283253
LOSS train 0.2430465133324013 valid 0.31944700770126366
LOSS train 0.2430465133324013 valid 0.3194177460707264
LOSS train 0.2430465133324013 valid 0.31939765820108307
LOSS train 0.2430465133324013 valid 0.31915997341275215
LOSS train 0.2430465133324013 valid 0.3189745900305835
LOSS train 0.2430465133324013 valid 0.31928275067763157
LOSS train 0.2430465133324013 valid 0.3193009253925906
LOSS train 0.2430465133324013 valid 0.3193061260417813
LOSS train 0.2430465133324013 valid 0.3194356625073055
LOSS train 0.2430465133324013 valid 0.3197634350727586
LOSS train 0.2430465133324013 valid 0.31958789182336705
LOSS train 0.2430465133324013 valid 0.3192426651542963
LOSS train 0.2430465133324013 valid 0.31929678818738527
LOSS train 0.2430465133324013 valid 0.31920035123482515
LOSS train 0.2430465133324013 valid 0.31914360174110956
LOSS train 0.2430465133324013 valid 0.3191184979778799
LOSS train 0.2430465133324013 valid 0.31923025080376427
LOSS train 0.2430465133324013 valid 0.31942231241571767
LOSS train 0.2430465133324013 valid 0.3193851174089496
LOSS train 0.2430465133324013 valid 0.31940881949332023
LOSS train 0.2430465133324013 valid 0.3196890727097158
LOSS train 0.2430465133324013 valid 0.31976650843581
LOSS train 0.2430465133324013 valid 0.31947534649424214
LOSS train 0.2430465133324013 valid 0.31935995306981646
LOSS train 0.2430465133324013 valid 0.31932351935554193
LOSS train 0.2430465133324013 valid 0.3193566900908306
LOSS train 0.2430465133324013 valid 0.319185340069832
LOSS train 0.2430465133324013 valid 0.3193169145032446
LOSS train 0.2430465133324013 valid 0.3194515817852878
LOSS train 0.2430465133324013 valid 0.3195908388809154
LOSS train 0.2430465133324013 valid 0.31954867092414674
LOSS train 0.2430465133324013 valid 0.3195159086802353
LOSS train 0.2430465133324013 valid 0.31972896813419815
LOSS train 0.2430465133324013 valid 0.31949231604632644
LOSS train 0.2430465133324013 valid 0.3190216982211822
LOSS train 0.2430465133324013 valid 0.31922792118726945
LOSS train 0.2430465133324013 valid 0.31937373493831167
LOSS train 0.2430465133324013 valid 0.31939762221141293
LOSS train 0.2430465133324013 valid 0.3194403701541412
LOSS train 0.2430465133324013 valid 0.3191327834874392
LOSS train 0.2430465133324013 valid 0.31896094444082745
LOSS train 0.2430465133324013 valid 0.3188129242251415
LOSS train 0.2430465133324013 valid 0.31866589166554327
LOSS train 0.2430465133324013 valid 0.3186667768248156
LOSS train 0.2430465133324013 valid 0.3184642014707007
LOSS train 0.2430465133324013 valid 0.3184130118743887
LOSS train 0.2430465133324013 valid 0.31824194974657416
LOSS train 0.2430465133324013 valid 0.31817304034932303
LOSS train 0.2430465133324013 valid 0.3180724070830779
LOSS train 0.2430465133324013 valid 0.3182063902417819
LOSS train 0.2430465133324013 valid 0.31823769021090736
LOSS train 0.2430465133324013 valid 0.31818353869723826
LOSS train 0.2430465133324013 valid 0.3182193490680954
LOSS train 0.2430465133324013 valid 0.3181506614679488
LOSS train 0.2430465133324013 valid 0.31792378612729005
LOSS train 0.2430465133324013 valid 0.31788840275947694
LOSS train 0.2430465133324013 valid 0.31791611290472443
LOSS train 0.2430465133324013 valid 0.31784489552635664
LOSS train 0.2430465133324013 valid 0.31792644448748464
LOSS train 0.2430465133324013 valid 0.31810331161726607
LOSS train 0.2430465133324013 valid 0.3181695453316917
LOSS train 0.2430465133324013 valid 0.3181527539550721
LOSS train 0.2430465133324013 valid 0.31829117826549463
LOSS train 0.2430465133324013 valid 0.3184019757567772
LOSS train 0.2430465133324013 valid 0.31847685476144155
LOSS train 0.2430465133324013 valid 0.31864822563608136
LOSS train 0.2430465133324013 valid 0.3189942629983247
LOSS train 0.2430465133324013 valid 0.31906227904715034
LOSS train 0.2430465133324013 valid 0.3190573152626446
LOSS train 0.2430465133324013 valid 0.31918571610813556
LOSS train 0.2430465133324013 valid 0.3191697568475426
LOSS train 0.2430465133324013 valid 0.3192659212469027
LOSS train 0.2430465133324013 valid 0.31915450230432685
LOSS train 0.2430465133324013 valid 0.31919726111695296
LOSS train 0.2430465133324013 valid 0.3192151315034704
LOSS train 0.2430465133324013 valid 0.3191781241636155
LOSS train 0.2430465133324013 valid 0.3191749088246108
LOSS train 0.2430465133324013 valid 0.31924632661232427
LOSS train 0.2430465133324013 valid 0.3191002891028775
LOSS train 0.2430465133324013 valid 0.31905299704521894
LOSS train 0.2430465133324013 valid 0.31920224533288805
LOSS train 0.2430465133324013 valid 0.3189954566192036
LOSS train 0.2430465133324013 valid 0.31917155580020246
LOSS train 0.2430465133324013 valid 0.31942790129878484
LOSS train 0.2430465133324013 valid 0.31953080460733296
LOSS train 0.2430465133324013 valid 0.3195091898484928
LOSS train 0.2430465133324013 valid 0.3193822760692975
LOSS train 0.2430465133324013 valid 0.31934190847940985
LOSS train 0.2430465133324013 valid 0.31926367470777656
LOSS train 0.2430465133324013 valid 0.31937919110059737
LOSS train 0.2430465133324013 valid 0.31935748796301533
LOSS train 0.2430465133324013 valid 0.3193851472248161
LOSS train 0.2430465133324013 valid 0.31925395490388153
LOSS train 0.2430465133324013 valid 0.31926274117757014
LOSS train 0.2430465133324013 valid 0.31930676774651395
LOSS train 0.2430465133324013 valid 0.31945027998881415
LOSS train 0.2430465133324013 valid 0.3192115212113013
LOSS train 0.2430465133324013 valid 0.3191974217924037
LOSS train 0.2430465133324013 valid 0.31916947674337043
LOSS train 0.2430465133324013 valid 0.3192067180115443
LOSS train 0.2430465133324013 valid 0.31920676310172025
LOSS train 0.2430465133324013 valid 0.3191835557800213
LOSS train 0.2430465133324013 valid 0.31915127148873
LOSS train 0.2430465133324013 valid 0.31919380553969834
LOSS train 0.2430465133324013 valid 0.3190885238490015
LOSS train 0.2430465133324013 valid 0.3190775209463629
LOSS train 0.2430465133324013 valid 0.319209326016769
LOSS train 0.2430465133324013 valid 0.3194025958985535
LOSS train 0.2430465133324013 valid 0.31949698088559075
LOSS train 0.2430465133324013 valid 0.319511767062876
LOSS train 0.2430465133324013 valid 0.31951028444010393
LOSS train 0.2430465133324013 valid 0.31974549410755143
LOSS train 0.2430465133324013 valid 0.3197600074630954
LOSS train 0.2430465133324013 valid 0.31975260306231296
LOSS train 0.2430465133324013 valid 0.3196189509196715
LOSS train 0.2430465133324013 valid 0.31960210069150163
LOSS train 0.2430465133324013 valid 0.31935232237573136
LOSS train 0.2430465133324013 valid 0.319051546426557
LOSS train 0.2430465133324013 valid 0.3190782017284824
LOSS train 0.2430465133324013 valid 0.3190800990909338
LOSS train 0.2430465133324013 valid 0.31899189477077156
LOSS train 0.2430465133324013 valid 0.3187905357133412
LOSS train 0.2430465133324013 valid 0.3188586961148906
LOSS train 0.2430465133324013 valid 0.3189269366713477
LOSS train 0.2430465133324013 valid 0.3189189164784917
LOSS train 0.2430465133324013 valid 0.3187773539782404
LOSS train 0.2430465133324013 valid 0.31865113075394247
LOSS train 0.2430465133324013 valid 0.3187411660845909
LOSS train 0.2430465133324013 valid 0.31876448461959933
LOSS train 0.2430465133324013 valid 0.3188038402076425
LOSS train 0.2430465133324013 valid 0.3188271254710725
LOSS train 0.2430465133324013 valid 0.31889461618784354
LOSS train 0.2430465133324013 valid 0.318948356313917
LOSS train 0.2430465133324013 valid 0.319150439381194
LOSS train 0.2430465133324013 valid 0.31920912432468546
LOSS train 0.2430465133324013 valid 0.3191705642620454
LOSS train 0.2430465133324013 valid 0.31921076077202754
LOSS train 0.2430465133324013 valid 0.3192011423458989
LOSS train 0.2430465133324013 valid 0.3193063502227981
LOSS train 0.2430465133324013 valid 0.3192493700484435
LOSS train 0.2430465133324013 valid 0.31940208761596994
LOSS train 0.2430465133324013 valid 0.319385909826945
LOSS train 0.2430465133324013 valid 0.3194865771744511
LOSS train 0.2430465133324013 valid 0.3195096498943473
LOSS train 0.2430465133324013 valid 0.31941128886136855
LOSS train 0.2430465133324013 valid 0.31930488761928344
LOSS train 0.2430465133324013 valid 0.31930644722831364
LOSS train 0.2430465133324013 valid 0.31921897427021684
LOSS train 0.2430465133324013 valid 0.31932078938461045
LOSS train 0.2430465133324013 valid 0.3193281527969145
LOSS train 0.2430465133324013 valid 0.3191433995962143
LOSS train 0.2430465133324013 valid 0.31920090828759545
LOSS train 0.2430465133324013 valid 0.3192127549324554
LOSS train 0.2430465133324013 valid 0.3193774662294965
LOSS train 0.2430465133324013 valid 0.31937596064711377
LOSS train 0.2430465133324013 valid 0.3192775314084337
LOSS train 0.2430465133324013 valid 0.31929730339569246
LOSS train 0.2430465133324013 valid 0.31934910530001864
LOSS train 0.2430465133324013 valid 0.31939650109755957
LOSS train 0.2430465133324013 valid 0.319295414397493
LOSS train 0.2430465133324013 valid 0.31943614715913377
LOSS train 0.2430465133324013 valid 0.3193590484143044
LOSS train 0.2430465133324013 valid 0.3193642290845375
LOSS train 0.2430465133324013 valid 0.31938630420668623
LOSS train 0.2430465133324013 valid 0.31933538872462053
LOSS train 0.2430465133324013 valid 0.3195310132178061
LOSS train 0.2430465133324013 valid 0.3196222829071389
LOSS train 0.2430465133324013 valid 0.3196179247574835
LOSS train 0.2430465133324013 valid 0.3198154262949268
LOSS train 0.2430465133324013 valid 0.31977427985632056
LOSS train 0.2430465133324013 valid 0.31970391780408125
LOSS train 0.2430465133324013 valid 0.3195259414433715
LOSS train 0.2430465133324013 valid 0.31952264614112386
LOSS train 0.2430465133324013 valid 0.31967159815712604
LOSS train 0.2430465133324013 valid 0.31973907000093316
LOSS train 0.2430465133324013 valid 0.31971301160575377
LOSS train 0.2430465133324013 valid 0.3195940959612764
LOSS train 0.2430465133324013 valid 0.31956711689219675
LOSS train 0.2430465133324013 valid 0.31956768506220307
LOSS train 0.2430465133324013 valid 0.3195701652151697
LOSS train 0.2430465133324013 valid 0.31943021906960395
LOSS train 0.2430465133324013 valid 0.31944944160549266
LOSS train 0.2430465133324013 valid 0.3193993223043642
LOSS train 0.2430465133324013 valid 0.31961741545345895
LOSS train 0.2430465133324013 valid 0.31965367763802627
LOSS train 0.2430465133324013 valid 0.31948862948341866
LOSS train 0.2430465133324013 valid 0.31941713710511455
LOSS train 0.2430465133324013 valid 0.31936448337189083
LOSS train 0.2430465133324013 valid 0.31948130234093924
LOSS train 0.2430465133324013 valid 0.3192966349210058
LOSS train 0.2430465133324013 valid 0.3192042427779603
LOSS train 0.2430465133324013 valid 0.31906862662766466
LOSS train 0.2430465133324013 valid 0.3191691245725405
LOSS train 0.2430465133324013 valid 0.31925673896478396
LOSS train 0.2430465133324013 valid 0.3194248658250755
LOSS train 0.2430465133324013 valid 0.3194889953213461
LOSS train 0.2430465133324013 valid 0.31945360353847846
LOSS train 0.2430465133324013 valid 0.3193463374699294
LOSS train 0.2430465133324013 valid 0.3193362920148127
LOSS train 0.2430465133324013 valid 0.3192328011410104
LOSS train 0.2430465133324013 valid 0.31911389174718935
LOSS train 0.2430465133324013 valid 0.3192375874074783
LOSS train 0.2430465133324013 valid 0.31907143807115634
LOSS train 0.2430465133324013 valid 0.31900813262704963
LOSS train 0.2430465133324013 valid 0.31898171236253764
LOSS train 0.2430465133324013 valid 0.31889618798846103
LOSS train 0.2430465133324013 valid 0.3188237420788253
LOSS train 0.2430465133324013 valid 0.318758013577241
LOSS train 0.2430465133324013 valid 0.3187377321607052
Training bichrom
DEVICE = cpu
####################
Total Parameters = 606342
Total Trainable Parameters = 1157
bimodal_network(
  (base_model): bichrom_seq(
    (conv1d): Conv1d(4, 256, kernel_size=(24,), stride=(1,))
    (relu): ReLU()
    (batchNorm1d): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (maxPool1d): MaxPool1d(kernel_size=15, stride=15, padding=0, dilation=1, ceil_mode=True)
    (lstm): LSTM(256, 32, batch_first=True)
    (tanh): Tanh()
    (model_dense_repeat): Sequential(
      (0): Linear(in_features=32, out_features=512, bias=True)
      (1): ReLU()
      (2): Dropout(p=0.5, inplace=False)
      (3): Linear(in_features=512, out_features=512, bias=True)
      (4): ReLU()
      (5): Dropout(p=0.5, inplace=False)
      (6): Linear(in_features=512, out_features=512, bias=True)
      (7): ReLU()
      (8): Dropout(p=0.5, inplace=False)
    )
    (linear): Linear(in_features=512, out_features=1, bias=True)
    (sigmoid): Sigmoid()
  )
  (linear): Linear(in_features=512, out_features=1, bias=True)
  (tanh): Tanh()
  (model): bichrom_chrom(
    (_reshape): _reshape()
    (conv1d): Conv1d(12, 15, kernel_size=(1,), stride=(1,), padding=valid)
    (relu): ReLU()
    (lstm): LSTM(15, 5, batch_first=True)
    (relu2): ReLU()
    (linear): Linear(in_features=5, out_features=1, bias=True)
    (tanh): Tanh()
  )
  (linear2): Linear(in_features=2, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)
base_model.conv1d.weight False
base_model.conv1d.bias False
base_model.batchNorm1d.weight False
base_model.batchNorm1d.bias False
base_model.lstm.weight_ih_l0 False
base_model.lstm.weight_hh_l0 False
base_model.lstm.bias_ih_l0 False
base_model.lstm.bias_hh_l0 False
base_model.model_dense_repeat.0.weight False
base_model.model_dense_repeat.0.bias False
base_model.model_dense_repeat.3.weight False
base_model.model_dense_repeat.3.bias False
base_model.model_dense_repeat.6.weight False
base_model.model_dense_repeat.6.bias False
base_model.linear.weight False
base_model.linear.bias False
linear.weight True
linear.bias True
model.conv1d.weight True
model.conv1d.bias True
model.lstm.weight_ih_l0 True
model.lstm.weight_hh_l0 True
model.lstm.bias_ih_l0 True
model.lstm.bias_hh_l0 True
model.linear.weight True
model.linear.bias True
linear2.weight True
linear2.bias True
####################
EPOCH 1:
  batch 1 loss: 0.7069182991981506
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.7055682837963104
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.6968507369359335
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.684945359826088
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.678537130355835
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.67133828997612
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.6634785958698818
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.6573535650968552
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.6506581041547987
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.6455515921115875
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.6395514065569098
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.633895163734754
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.6290135842103225
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.6248552373477391
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.6205322782198588
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.6154014207422733
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.6110609278959387
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.6067712174521552
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.6020880184675518
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.5969486191868782
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.5926661590735117
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.5885208398103714
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.5848918103653452
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.581088357915481
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.5772638249397278
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.5738512506851783
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.5699561094796216
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.5657661418829646
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.5615881917805507
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.5580476125081381
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.5548074543476105
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.5516472971066833
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.5476655066013336
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.5445114453049267
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.5418720866952623
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.5382072255015373
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.5351693026117377
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.5319301117407648
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.5285311471193265
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.5253578692674636
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.5223593966263097
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.5197874158620834
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.5174042492411858
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.5142715173688802
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.5116648455460866
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.5086630239434864
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.5056877263048862
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.5029405417541662
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.5003892402259671
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.49779276609420775
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.4952595239760829
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.4926578706273666
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.4903289540758673
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.4879092441664802
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.4854876317761161
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.4832012504339218
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.48118982085010464
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.47883002511386213
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.47687676498445414
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.4747985248764356
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.47274402467930904
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.47047989935644213
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.4683396816253662
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.46637167502194643
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.464431062569985
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.46235236435225513
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.460187623749918
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.4581788269912495
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.4562948385010595
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.4547851434775761
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.4527581367694156
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.45092002633545136
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.44927373773431123
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.447523813795399
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.44615323066711426
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.4446516605584245
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.4428757874222545
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.4410366916503662
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.43965084039712254
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.4383356608450413
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.4369208441104418
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.4353608877920523
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.4335866451981556
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.43216677187454133
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.43077362284940834
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.4290381035832472
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.4277110935627729
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.4262977682731368
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.42483357197782967
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.4237359129720264
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.4223239061596629
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.42096297248550085
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.4199728145394274
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.4189750117824433
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.41792673688185844
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.41671926031510037
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.4153466679386257
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.4142808819911918
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.4132687581910027
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.4120693671703339
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.4108928807891241
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.4095977088984321
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.4083637340554913
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.4072028650687291
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.40586044930276416
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.4047812338707582
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.40373906827418604
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.40269509316594515
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.4017481483997555
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.4008255435661836
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.39962134060558974
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.3983344528824091
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.39714038055554957
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.39630084916165
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.39516720979110054
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.3942684608286825
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.3933575219578213
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.39215711415824245
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.39114551183556306
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.39015488972266515
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.38916339509743303
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.3882101383365569
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.38761716959922293
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.3865857201237832
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.38580578994750975
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.38512333305109114
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.3841723797358866
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.3834285601042211
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.3825933743816938
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.38180257494633013
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.38096460449786584
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.3801504572232564
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.37956871775756207
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.3786593824625015
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.37778613876413414
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.37710172551519733
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.37647415599683776
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.3759321475374526
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.3753426113574625
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.3746891100491796
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.37416423910053065
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.3733442392147763
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.3727341340138362
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.37222382922967273
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.3714256808675569
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.37063798083834454
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.36984802184461735
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.3691078033801672
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.36849773830215404
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.3678755925099055
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.36712077082387656
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.36633217962164627
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.3657328523451986
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.36500224471092224
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.36448448050406673
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.36377891707114685
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.3630733905704158
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.3623951750842831
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.361938920410924
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.36124287974089386
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.3604765166036831
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.3598411077702487
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.3594273930312666
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.3587831491377296
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.35819824121215127
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.35761433785938357
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.35701371488456957
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.3564683951082684
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.3558101496223867
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.35517655970419154
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.354506058563963
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.3538774904296842
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.3531814608615258
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.35260884515170393
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.3520469513961247
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.35145914292132313
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.35094436691642483
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.3503363285674138
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.3499012368850868
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.3493223483363787
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.34880087816912825
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.34819630394270135
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.34751223157989525
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.3469938816421706
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.34642264818823015
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.3459317185064798
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.3454177940593046
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.34487169426172337
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.34432651473100856
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.3437769403583125
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.34333097513433525
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.342741296471407
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.34228613787364465
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.3416412379016581
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.3411938378444085
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.34080698401952275
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.3404535183749223
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.34018284925306685
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.3396705517816783
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.33928820192813874
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.3387251656446884
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.33819517603900173
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.3376630229022115
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.3371993666332142
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.3366674673993413
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.33616759345948116
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.33569651914103593
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.3350883066081084
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.334548726749192
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.3342148980924061
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.3337324027201576
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.3334293399216994
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.33310085031348213
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.3325347669213732
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.332196761009305
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.3315902971835048
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.3311853992499514
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.33079910524394535
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.330426465226635
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.33005831742828545
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.32953975717136763
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.3290761454819559
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.32860764005793586
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.3282087226398289
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.3276889431476593
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.3273329230404533
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.32685383353464403
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.3262776312859435
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.3258448539212281
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.3255055764447088
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.3250615559357069
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.3246003359813115
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.3242230845111634
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.32386126592118514
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.32349105889492846
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.32309020266441973
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.32280306392329655
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.3223734442301157
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.322009339928627
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.3216205406313141
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.321250455322602
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.32080387551922446
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.3204466393945639
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.32005174012213455
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.31972532485212596
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.3194305816195845
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.3189809871950613
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.31857869864231153
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.3183100694513704
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.31802141600847245
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.31775471538424016
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.3172983596367495
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.31691770055312884
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.3164953803570252
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.316198405623436
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.31579594226786867
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.31539467928010667
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.31499266266360765
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.31471115937564365
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.31436578379227564
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.31394059944655245
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.31355191892339984
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.31319369811975456
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.3126985816793008
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.312355486345741
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.31210045818995713
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.31174183539236977
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.31143836938400765
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.3110399800162333
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.31073831849628025
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.31040829007695964
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.31007382240803805
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.30984955840495043
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.309465708034317
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.3090183009342714
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.308887285436841
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.3085704853603556
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.3082686942472732
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.3079896503024631
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.3077120827777045
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.30735300332838106
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.3070372389035022
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.30669166958584804
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.3063713186748431
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.30608337134645697
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.3057500576743713
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.30540774414763633
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.30513437858058345
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.3047670208459082
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.3045076221227646
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.3041523528058095
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.30386413439904175
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.3036198605347819
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.30337334621925743
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.3031397875082695
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.3028157493351279
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.30258928224293874
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.30221908474528547
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.3018135673126648
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.3014920525252819
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.30129287641903885
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.3009996979145814
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.3006585214791125
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.30034051747306395
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.300102934632145
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.2997919006967077
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.2994551067923102
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.29909571148938946
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.2987746255876177
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.2984042328211569
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.29810366068047345
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.2978386313964923
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.29755832245365116
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.297230816903008
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.29694209926658205
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.296757601343001
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.2964871033603085
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.2961637477548617
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.29582786233073866
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.2955578343011439
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.29526466261189305
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.29493366362330337
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.29469127159554154
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.2943850832036984
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.2941167125335106
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.2938025828245227
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.29349072136281096
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.29328096630733186
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.29294683175065234
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.2927477595029455
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.29254638270849187
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.2922372192323926
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.29188232701103967
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.29159634645113686
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.2914310573197123
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.29119321295902845
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.29084810919683834
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.2905370172047051
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.29018750584582664
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.28991063302930664
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.2896207721789212
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.2895256497445162
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.28924817294316807
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.2889737727683644
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.288692781329155
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.2884333946370665
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.2881588165887151
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.28788789704270745
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.2876167621858482
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.2872815445491246
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.2870473805231247
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.2868287170898508
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.286612369943273
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.28641705581192245
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.2862561989837969
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.2859732760639673
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.2857589668669954
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.28559553111232194
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.2853558014113259
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.2850428802685605
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.2847543414527359
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.2845012529943529
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.28426331659322274
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.2840049666772177
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.2837967229216066
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.2835408693454305
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.2833212330247141
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.2831334988384143
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.2829091393043032
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.2826384794470426
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.2823677565450617
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.2820757333069078
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.2818152936470093
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.28160847317088733
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.28137471854686735
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.2812588143142614
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.2810092446263336
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.28080656844629814
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.2804907368088146
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.2801673108025601
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.27991754770904703
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.27968081137584766
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.2795105478471004
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.27922558687472093
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.2789399413319377
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.2787547437715407
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.27858678873349224
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.2783909282195814
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.27821321816095046
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.27798219858071743
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.27776165588585006
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.27758767079485924
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.2774181020548022
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.27721443889558617
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.2770263736761069
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.2767605600182456
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.27646443495522216
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.2763254712125165
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.27602664521314146
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.2758094260469079
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.2755787093889089
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.27533650409374666
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.27509454879955975
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.274898586776292
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.2746785707679796
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.2744214233800108
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.27420954962766725
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.27409782450573117
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.2739016603869739
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.2737732659389333
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.2735446916661993
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.27339468353871005
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.273185275946056
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.27306928338060055
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.2729229458843369
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.27271584324681986
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.2725161965802419
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.27232997215678245
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.2722675165157045
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.27208352830438387
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.27201354570881486
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.27187353930484626
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.27162445056522994
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.27141843890806416
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.27123411925400004
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.2710128650069237
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.27082749203161555
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.2706030916039632
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.27031553642594175
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.2701129244857056
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.269969304206753
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.26975475865657683
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.26951649165594
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.26939541527203154
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.2691555470913306
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.2690583201244884
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.26892709339919846
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.26872680237576296
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.26846746757524703
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.2682696716690605
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.2680813073098254
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.267887640525313
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.26770814637701884
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.267551549785846
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.2674011828524343
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.26723007931303017
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.2670327063641559
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.2669156880250999
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.2667489634590319
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.26652678393655355
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.26633583788993354
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.2662013504093727
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.2659762322771102
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.265799916722701
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.2655953327705572
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.2654800788221652
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.26529806653441174
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.2651432068392179
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.26498187640134024
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.26488928205293155
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.26472551150978774
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.26454229717388816
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.26431662069541073
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.2641944056697961
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.264028748998078
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.26383307070947
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.26368549916039696
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.2635991379746005
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.26346236289437136
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.2633191506596322
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.26316864348774
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.2630997614587768
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.2630997614587768 valid 0.23730695247650146
LOSS train 0.2630997614587768 valid 0.22173859924077988
LOSS train 0.2630997614587768 valid 0.22628305852413177
LOSS train 0.2630997614587768 valid 0.21890337392687798
LOSS train 0.2630997614587768 valid 0.2147966593503952
LOSS train 0.2630997614587768 valid 0.22281135867039362
LOSS train 0.2630997614587768 valid 0.22943351311343058
LOSS train 0.2630997614587768 valid 0.2287450935691595
LOSS train 0.2630997614587768 valid 0.2289798061052958
LOSS train 0.2630997614587768 valid 0.23296574056148528
LOSS train 0.2630997614587768 valid 0.2324175157330253
LOSS train 0.2630997614587768 valid 0.23192360624670982
LOSS train 0.2630997614587768 valid 0.23224037197919992
LOSS train 0.2630997614587768 valid 0.23110615355627878
LOSS train 0.2630997614587768 valid 0.2289338678121567
LOSS train 0.2630997614587768 valid 0.2305675959214568
LOSS train 0.2630997614587768 valid 0.23115688650047078
LOSS train 0.2630997614587768 valid 0.23137407004833221
LOSS train 0.2630997614587768 valid 0.23351701623515078
LOSS train 0.2630997614587768 valid 0.23455251902341842
LOSS train 0.2630997614587768 valid 0.23282500533830552
LOSS train 0.2630997614587768 valid 0.23133792254057797
LOSS train 0.2630997614587768 valid 0.23070286473502283
LOSS train 0.2630997614587768 valid 0.23041623644530773
LOSS train 0.2630997614587768 valid 0.22970618963241576
LOSS train 0.2630997614587768 valid 0.23023998393462256
LOSS train 0.2630997614587768 valid 0.2295783289052822
LOSS train 0.2630997614587768 valid 0.23013352443064963
LOSS train 0.2630997614587768 valid 0.23067672036845108
LOSS train 0.2630997614587768 valid 0.23226156681776047
LOSS train 0.2630997614587768 valid 0.23308836981173484
LOSS train 0.2630997614587768 valid 0.23268772615119815
LOSS train 0.2630997614587768 valid 0.23347516177278577
LOSS train 0.2630997614587768 valid 0.23342813376118154
LOSS train 0.2630997614587768 valid 0.23496814114706857
LOSS train 0.2630997614587768 valid 0.23508949619200495
LOSS train 0.2630997614587768 valid 0.23552176879869924
LOSS train 0.2630997614587768 valid 0.2363591700008041
LOSS train 0.2630997614587768 valid 0.23510583127156282
LOSS train 0.2630997614587768 valid 0.2353186897933483
LOSS train 0.2630997614587768 valid 0.23618211033867625
LOSS train 0.2630997614587768 valid 0.23684880492233096
LOSS train 0.2630997614587768 valid 0.23669887455396874
LOSS train 0.2630997614587768 valid 0.23760079152204774
LOSS train 0.2630997614587768 valid 0.23720666666825613
LOSS train 0.2630997614587768 valid 0.23782968099998392
LOSS train 0.2630997614587768 valid 0.23826402774516572
LOSS train 0.2630997614587768 valid 0.23822003168364367
LOSS train 0.2630997614587768 valid 0.23882071460996354
LOSS train 0.2630997614587768 valid 0.23824583053588866
LOSS train 0.2630997614587768 valid 0.23843137656941132
LOSS train 0.2630997614587768 valid 0.23826057664476907
LOSS train 0.2630997614587768 valid 0.23886795229506944
LOSS train 0.2630997614587768 valid 0.2385518777149695
LOSS train 0.2630997614587768 valid 0.23837294578552246
LOSS train 0.2630997614587768 valid 0.2379249980939286
LOSS train 0.2630997614587768 valid 0.23797505937124552
LOSS train 0.2630997614587768 valid 0.23779398305662747
LOSS train 0.2630997614587768 valid 0.23815992323018736
LOSS train 0.2630997614587768 valid 0.2381047065059344
LOSS train 0.2630997614587768 valid 0.23813255204529057
LOSS train 0.2630997614587768 valid 0.23870906618333632
LOSS train 0.2630997614587768 valid 0.23849101719402133
LOSS train 0.2630997614587768 valid 0.2390249245800078
LOSS train 0.2630997614587768 valid 0.23940977729283847
LOSS train 0.2630997614587768 valid 0.23942431581742835
LOSS train 0.2630997614587768 valid 0.23898219217115374
LOSS train 0.2630997614587768 valid 0.23901770088602514
LOSS train 0.2630997614587768 valid 0.23851657388866812
LOSS train 0.2630997614587768 valid 0.23881729330335344
LOSS train 0.2630997614587768 valid 0.23864493172773174
LOSS train 0.2630997614587768 valid 0.2389043252915144
LOSS train 0.2630997614587768 valid 0.23903521813758433
LOSS train 0.2630997614587768 valid 0.23888002054111376
LOSS train 0.2630997614587768 valid 0.23880241433779398
LOSS train 0.2630997614587768 valid 0.23935082594030782
LOSS train 0.2630997614587768 valid 0.23933460902084003
LOSS train 0.2630997614587768 valid 0.23913099253788972
LOSS train 0.2630997614587768 valid 0.23884447805489165
LOSS train 0.2630997614587768 valid 0.23830146472901106
LOSS train 0.2630997614587768 valid 0.2379246297073953
LOSS train 0.2630997614587768 valid 0.2380065894344958
LOSS train 0.2630997614587768 valid 0.2378011778535613
LOSS train 0.2630997614587768 valid 0.2375777234278974
LOSS train 0.2630997614587768 valid 0.23726363550214208
LOSS train 0.2630997614587768 valid 0.23671312335618708
LOSS train 0.2630997614587768 valid 0.23673910365022463
LOSS train 0.2630997614587768 valid 0.236432533711195
LOSS train 0.2630997614587768 valid 0.23672513513082868
LOSS train 0.2630997614587768 valid 0.23688000871075524
LOSS train 0.2630997614587768 valid 0.23710390401410533
LOSS train 0.2630997614587768 valid 0.2370352127953716
LOSS train 0.2630997614587768 valid 0.23681748970862357
LOSS train 0.2630997614587768 valid 0.23697621676516026
LOSS train 0.2630997614587768 valid 0.2365759275461498
LOSS train 0.2630997614587768 valid 0.23650572324792543
LOSS train 0.2630997614587768 valid 0.23658323533756218
LOSS train 0.2630997614587768 valid 0.2366226250115706
LOSS train 0.2630997614587768 valid 0.23700168141812988
LOSS train 0.2630997614587768 valid 0.23732503220438958
LOSS train 0.2630997614587768 valid 0.23745918023114157
LOSS train 0.2630997614587768 valid 0.23776602788883097
LOSS train 0.2630997614587768 valid 0.23793951472611102
LOSS train 0.2630997614587768 valid 0.23791214866706958
LOSS train 0.2630997614587768 valid 0.23789939766838436
LOSS train 0.2630997614587768 valid 0.2383343903523571
LOSS train 0.2630997614587768 valid 0.23820136571041892
LOSS train 0.2630997614587768 valid 0.2384812419337255
LOSS train 0.2630997614587768 valid 0.2386542549100491
LOSS train 0.2630997614587768 valid 0.23892258690162138
LOSS train 0.2630997614587768 valid 0.2388570477833619
LOSS train 0.2630997614587768 valid 0.23863780764596804
LOSS train 0.2630997614587768 valid 0.23878589272499084
LOSS train 0.2630997614587768 valid 0.23879308397309823
LOSS train 0.2630997614587768 valid 0.23878803227258766
LOSS train 0.2630997614587768 valid 0.2390099041934671
LOSS train 0.2630997614587768 valid 0.23918026507410228
LOSS train 0.2630997614587768 valid 0.23890706265376785
LOSS train 0.2630997614587768 valid 0.23871083494995823
LOSS train 0.2630997614587768 valid 0.23858080841600895
LOSS train 0.2630997614587768 valid 0.23829889445265462
LOSS train 0.2630997614587768 valid 0.2381603087558121
LOSS train 0.2630997614587768 valid 0.23840070667305613
LOSS train 0.2630997614587768 valid 0.23857737164343557
LOSS train 0.2630997614587768 valid 0.23849876022338867
LOSS train 0.2630997614587768 valid 0.23869044984143878
LOSS train 0.2630997614587768 valid 0.2385600319528204
LOSS train 0.2630997614587768 valid 0.238663766765967
LOSS train 0.2630997614587768 valid 0.23883969769921415
LOSS train 0.2630997614587768 valid 0.23882577029558327
LOSS train 0.2630997614587768 valid 0.2387219748196711
LOSS train 0.2630997614587768 valid 0.23849224773320285
LOSS train 0.2630997614587768 valid 0.23833803653268887
LOSS train 0.2630997614587768 valid 0.23831647312018409
LOSS train 0.2630997614587768 valid 0.23825732811733527
LOSS train 0.2630997614587768 valid 0.23827670963809772
LOSS train 0.2630997614587768 valid 0.23811979450448587
LOSS train 0.2630997614587768 valid 0.2381584420800209
LOSS train 0.2630997614587768 valid 0.23786871853492242
LOSS train 0.2630997614587768 valid 0.2380648217030934
LOSS train 0.2630997614587768 valid 0.23799764623878694
LOSS train 0.2630997614587768 valid 0.2380403605565219
LOSS train 0.2630997614587768 valid 0.23794668412708736
LOSS train 0.2630997614587768 valid 0.23794623298777473
LOSS train 0.2630997614587768 valid 0.23776644365540867
LOSS train 0.2630997614587768 valid 0.23804835587331694
LOSS train 0.2630997614587768 valid 0.23788083309218996
LOSS train 0.2630997614587768 valid 0.23856427721880577
LOSS train 0.2630997614587768 valid 0.23862923991760152
LOSS train 0.2630997614587768 valid 0.23854881505171457
LOSS train 0.2630997614587768 valid 0.23867766568992313
LOSS train 0.2630997614587768 valid 0.23864724310605148
LOSS train 0.2630997614587768 valid 0.23860949044134103
LOSS train 0.2630997614587768 valid 0.23860016755469435
LOSS train 0.2630997614587768 valid 0.23862564409932782
LOSS train 0.2630997614587768 valid 0.23890364609467676
LOSS train 0.2630997614587768 valid 0.23900649881666633
LOSS train 0.2630997614587768 valid 0.23915492054782336
LOSS train 0.2630997614587768 valid 0.23916544910496887
LOSS train 0.2630997614587768 valid 0.2391323602758348
LOSS train 0.2630997614587768 valid 0.2391309953809525
LOSS train 0.2630997614587768 valid 0.23899339323426472
LOSS train 0.2630997614587768 valid 0.2388509342824023
LOSS train 0.2630997614587768 valid 0.2388339049023826
LOSS train 0.2630997614587768 valid 0.23877791211460575
LOSS train 0.2630997614587768 valid 0.23877503220216337
LOSS train 0.2630997614587768 valid 0.23895912443449396
LOSS train 0.2630997614587768 valid 0.2390918934806472
LOSS train 0.2630997614587768 valid 0.23926772829696272
LOSS train 0.2630997614587768 valid 0.23935059019747904
LOSS train 0.2630997614587768 valid 0.23936176892609623
LOSS train 0.2630997614587768 valid 0.23916296400996143
LOSS train 0.2630997614587768 valid 0.23933711268998295
LOSS train 0.2630997614587768 valid 0.23933342287595244
LOSS train 0.2630997614587768 valid 0.23920207389763423
LOSS train 0.2630997614587768 valid 0.23910968005657196
LOSS train 0.2630997614587768 valid 0.2392448995072963
LOSS train 0.2630997614587768 valid 0.23932560441199313
LOSS train 0.2630997614587768 valid 0.23928427130150395
LOSS train 0.2630997614587768 valid 0.23926101765698857
LOSS train 0.2630997614587768 valid 0.23939238077039876
LOSS train 0.2630997614587768 valid 0.23940375782958753
LOSS train 0.2630997614587768 valid 0.23934279691651872
LOSS train 0.2630997614587768 valid 0.2392018157178941
LOSS train 0.2630997614587768 valid 0.23913520357093296
LOSS train 0.2630997614587768 valid 0.23913370385285346
LOSS train 0.2630997614587768 valid 0.23890569152679036
LOSS train 0.2630997614587768 valid 0.23890915306958746
LOSS train 0.2630997614587768 valid 0.23894289214775044
LOSS train 0.2630997614587768 valid 0.23894650418507427
LOSS train 0.2630997614587768 valid 0.2389247823136015
LOSS train 0.2630997614587768 valid 0.2388012569087247
LOSS train 0.2630997614587768 valid 0.23882008170216812
LOSS train 0.2630997614587768 valid 0.23865771178424972
LOSS train 0.2630997614587768 valid 0.23846541536160004
LOSS train 0.2630997614587768 valid 0.23854558747641894
LOSS train 0.2630997614587768 valid 0.2387143996766376
LOSS train 0.2630997614587768 valid 0.23863165313848342
LOSS train 0.2630997614587768 valid 0.2386618687130099
LOSS train 0.2630997614587768 valid 0.23853731729090213
LOSS train 0.2630997614587768 valid 0.23843785757152594
LOSS train 0.2630997614587768 valid 0.23841383396693977
LOSS train 0.2630997614587768 valid 0.23840330864114714
LOSS train 0.2630997614587768 valid 0.2385111031140767
LOSS train 0.2630997614587768 valid 0.23839551493888947
LOSS train 0.2630997614587768 valid 0.2385020278466558
LOSS train 0.2630997614587768 valid 0.23840854011008128
LOSS train 0.2630997614587768 valid 0.23836591417113176
LOSS train 0.2630997614587768 valid 0.2382924538907822
LOSS train 0.2630997614587768 valid 0.23830857021468027
LOSS train 0.2630997614587768 valid 0.23826389042969565
LOSS train 0.2630997614587768 valid 0.23813390366311343
LOSS train 0.2630997614587768 valid 0.23810283140117575
LOSS train 0.2630997614587768 valid 0.2380375243793024
LOSS train 0.2630997614587768 valid 0.23798642920893293
LOSS train 0.2630997614587768 valid 0.23787325593056502
LOSS train 0.2630997614587768 valid 0.2378154759582836
LOSS train 0.2630997614587768 valid 0.2377165588080336
LOSS train 0.2630997614587768 valid 0.23781666043958707
LOSS train 0.2630997614587768 valid 0.23786057802763852
LOSS train 0.2630997614587768 valid 0.23784498964769268
LOSS train 0.2630997614587768 valid 0.23786032038765983
LOSS train 0.2630997614587768 valid 0.23795141914500248
LOSS train 0.2630997614587768 valid 0.23806528294725077
LOSS train 0.2630997614587768 valid 0.23825155973434448
LOSS train 0.2630997614587768 valid 0.2382915807245052
LOSS train 0.2630997614587768 valid 0.23847303319607538
LOSS train 0.2630997614587768 valid 0.23855541868690858
LOSS train 0.2630997614587768 valid 0.23853876985056433
LOSS train 0.2630997614587768 valid 0.23858525778936304
LOSS train 0.2630997614587768 valid 0.2386839006628309
LOSS train 0.2630997614587768 valid 0.23870258810448236
LOSS train 0.2630997614587768 valid 0.23863652290960238
LOSS train 0.2630997614587768 valid 0.23875005333087382
LOSS train 0.2630997614587768 valid 0.2388664317257861
LOSS train 0.2630997614587768 valid 0.2387944389829191
LOSS train 0.2630997614587768 valid 0.2387200830983713
LOSS train 0.2630997614587768 valid 0.23867059398849472
LOSS train 0.2630997614587768 valid 0.23854324882499345
LOSS train 0.2630997614587768 valid 0.23850755381087463
LOSS train 0.2630997614587768 valid 0.2386170049425972
LOSS train 0.2630997614587768 valid 0.23848484210238968
LOSS train 0.2630997614587768 valid 0.23861084847783845
LOSS train 0.2630997614587768 valid 0.23874469736560447
LOSS train 0.2630997614587768 valid 0.23875233336370819
LOSS train 0.2630997614587768 valid 0.2386937476997453
LOSS train 0.2630997614587768 valid 0.2388094545375963
LOSS train 0.2630997614587768 valid 0.23877149489858457
LOSS train 0.2630997614587768 valid 0.23874491363404746
LOSS train 0.2630997614587768 valid 0.23874814862012864
LOSS train 0.2630997614587768 valid 0.2386344417157876
LOSS train 0.2630997614587768 valid 0.23874630211364656
LOSS train 0.2630997614587768 valid 0.23867799388796915
LOSS train 0.2630997614587768 valid 0.23862441288908637
LOSS train 0.2630997614587768 valid 0.23864672937813927
LOSS train 0.2630997614587768 valid 0.23868962767301127
LOSS train 0.2630997614587768 valid 0.23853444983523178
LOSS train 0.2630997614587768 valid 0.23872373050959536
LOSS train 0.2630997614587768 valid 0.23871552345835564
LOSS train 0.2630997614587768 valid 0.23875723790663939
LOSS train 0.2630997614587768 valid 0.23880803939026435
LOSS train 0.2630997614587768 valid 0.238826914029267
LOSS train 0.2630997614587768 valid 0.2388191086031185
LOSS train 0.2630997614587768 valid 0.23885580976352547
LOSS train 0.2630997614587768 valid 0.238833305633293
LOSS train 0.2630997614587768 valid 0.23878015869094016
LOSS train 0.2630997614587768 valid 0.23888856038618622
LOSS train 0.2630997614587768 valid 0.23896516726088168
LOSS train 0.2630997614587768 valid 0.23908266414053822
LOSS train 0.2630997614587768 valid 0.23907201880658113
LOSS train 0.2630997614587768 valid 0.23915535854897377
LOSS train 0.2630997614587768 valid 0.2393813119543826
LOSS train 0.2630997614587768 valid 0.23949634259218697
LOSS train 0.2630997614587768 valid 0.23954348456468025
LOSS train 0.2630997614587768 valid 0.23945547504858536
LOSS train 0.2630997614587768 valid 0.2394376689757126
LOSS train 0.2630997614587768 valid 0.23931898902899953
LOSS train 0.2630997614587768 valid 0.23917434637923893
LOSS train 0.2630997614587768 valid 0.23916182076845546
LOSS train 0.2630997614587768 valid 0.23912527832601752
LOSS train 0.2630997614587768 valid 0.23904908838221187
LOSS train 0.2630997614587768 valid 0.2388656623397313
LOSS train 0.2630997614587768 valid 0.2388975538129099
LOSS train 0.2630997614587768 valid 0.2389437124972612
LOSS train 0.2630997614587768 valid 0.23895332771435118
LOSS train 0.2630997614587768 valid 0.2389203794561066
LOSS train 0.2630997614587768 valid 0.23881219291105504
LOSS train 0.2630997614587768 valid 0.2387697891228729
LOSS train 0.2630997614587768 valid 0.23874992276558002
LOSS train 0.2630997614587768 valid 0.2387084572993476
LOSS train 0.2630997614587768 valid 0.23856150335872295
LOSS train 0.2630997614587768 valid 0.23857098710659433
LOSS train 0.2630997614587768 valid 0.23859022003391903
LOSS train 0.2630997614587768 valid 0.23860129146348863
LOSS train 0.2630997614587768 valid 0.23871334811388437
LOSS train 0.2630997614587768 valid 0.23867267209130363
LOSS train 0.2630997614587768 valid 0.2386806268081922
LOSS train 0.2630997614587768 valid 0.23870512268087207
LOSS train 0.2630997614587768 valid 0.23876380407092562
LOSS train 0.2630997614587768 valid 0.238773789058129
LOSS train 0.2630997614587768 valid 0.23876347652701443
LOSS train 0.2630997614587768 valid 0.2387021823928056
LOSS train 0.2630997614587768 valid 0.23885908973689127
LOSS train 0.2630997614587768 valid 0.23884922937539063
LOSS train 0.2630997614587768 valid 0.23881863458234756
LOSS train 0.2630997614587768 valid 0.23877518224755143
LOSS train 0.2630997614587768 valid 0.23878065246712502
LOSS train 0.2630997614587768 valid 0.2386799714007935
LOSS train 0.2630997614587768 valid 0.238797634454221
LOSS train 0.2630997614587768 valid 0.23880539767203793
LOSS train 0.2630997614587768 valid 0.2387316026200819
LOSS train 0.2630997614587768 valid 0.23874741420149803
LOSS train 0.2630997614587768 valid 0.2388045590716048
LOSS train 0.2630997614587768 valid 0.23887501846832834
LOSS train 0.2630997614587768 valid 0.23882409192266918
LOSS train 0.2630997614587768 valid 0.23872706826917733
LOSS train 0.2630997614587768 valid 0.23876785296558958
LOSS train 0.2630997614587768 valid 0.2387884165037353
LOSS train 0.2630997614587768 valid 0.23885763518302044
LOSS train 0.2630997614587768 valid 0.23872259072959423
LOSS train 0.2630997614587768 valid 0.2387960518817664
LOSS train 0.2630997614587768 valid 0.23882740476857062
LOSS train 0.2630997614587768 valid 0.23882123481932072
LOSS train 0.2630997614587768 valid 0.23892454871976818
LOSS train 0.2630997614587768 valid 0.23891517584140484
LOSS train 0.2630997614587768 valid 0.23911264365070437
LOSS train 0.2630997614587768 valid 0.2392343429432732
LOSS train 0.2630997614587768 valid 0.239169479415911
LOSS train 0.2630997614587768 valid 0.2393360156962212
LOSS train 0.2630997614587768 valid 0.23928590453032292
LOSS train 0.2630997614587768 valid 0.23919395116520792
LOSS train 0.2630997614587768 valid 0.23914729170950064
LOSS train 0.2630997614587768 valid 0.2391491251306849
LOSS train 0.2630997614587768 valid 0.23928841047301264
LOSS train 0.2630997614587768 valid 0.23927214194589586
LOSS train 0.2630997614587768 valid 0.2393542728165076
LOSS train 0.2630997614587768 valid 0.2393474092911542
LOSS train 0.2630997614587768 valid 0.23930183343986083
LOSS train 0.2630997614587768 valid 0.23935403250663329
LOSS train 0.2630997614587768 valid 0.23937234817182315
LOSS train 0.2630997614587768 valid 0.23925995376627465
LOSS train 0.2630997614587768 valid 0.23922742419598395
LOSS train 0.2630997614587768 valid 0.23927183570910474
LOSS train 0.2630997614587768 valid 0.23948109778034132
LOSS train 0.2630997614587768 valid 0.23954938740833945
LOSS train 0.2630997614587768 valid 0.23954584053313802
LOSS train 0.2630997614587768 valid 0.2394274653979612
LOSS train 0.2630997614587768 valid 0.23940157363640852
LOSS train 0.2630997614587768 valid 0.2394668909488913
LOSS train 0.2630997614587768 valid 0.23937864733593806
LOSS train 0.2630997614587768 valid 0.23929559653810625
LOSS train 0.2630997614587768 valid 0.2392603471790525
LOSS train 0.2630997614587768 valid 0.23930070129231082
LOSS train 0.2630997614587768 valid 0.2393688103657658
LOSS train 0.2630997614587768 valid 0.2394720500203925
LOSS train 0.2630997614587768 valid 0.23954504908303195
LOSS train 0.2630997614587768 valid 0.23953701939736427
LOSS train 0.2630997614587768 valid 0.23943014821693218
LOSS train 0.2630997614587768 valid 0.23948610281379773
LOSS train 0.2630997614587768 valid 0.23947883393201563
LOSS train 0.2630997614587768 valid 0.23945306072274733
LOSS train 0.2630997614587768 valid 0.2395630687145897
LOSS train 0.2630997614587768 valid 0.23950562685645974
LOSS train 0.2630997614587768 valid 0.2395330310522855
LOSS train 0.2630997614587768 valid 0.23954196760099228
LOSS train 0.2630997614587768 valid 0.23949314442162956
LOSS train 0.2630997614587768 valid 0.23938106548558788
LOSS train 0.2630997614587768 valid 0.2393827836798585
LOSS train 0.2630997614587768 valid 0.23945077898379588
EPOCH 2:
  batch 1 loss: 0.18530026078224182
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.17814038693904877
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.17925305664539337
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.1847774162888527
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.18889423608779907
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.19117143005132675
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.18509776251656668
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.18596306443214417
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.18412919839223227
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.18312025517225267
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.18095788495107132
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.1799820214509964
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.17897029106433576
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.17891998376165116
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.17927292188008626
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.18061881884932518
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.1807723483618568
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.18268804914421505
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.1813797950744629
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.18008167892694474
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.18013171638761247
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.18000079555944962
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.1806288471688395
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.18069112611313662
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.18110983848571777
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.18130771013406607
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.18172453509436715
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.18121413407581194
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.18034922819713067
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.18145299752553304
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.1821228813740515
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.1825830042362213
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.1817676844921979
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.1824409983613912
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.1837298469884055
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.18286877291070092
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.18293819153631055
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.18273352635534187
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.18256817758083344
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.18282993510365486
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.18271309827885976
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.18309522313731058
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.18359268716601437
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.182882219214331
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.18341132567988502
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.1830031755177871
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.18239180268125332
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.1821637867639462
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.18221003972754188
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.18249801218509673
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.18248650256325216
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.18256769042748672
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.18274027868261877
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.18314518917489936
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.18306770378893072
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.18290504281009948
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.1829055782995726
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.18250051372010132
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.18284013084435868
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.18269799475868542
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.1829751510112012
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.18261502803333343
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.18244680998817323
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.18234837730415165
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.18229833107728224
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.18218825080178
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.1818744406771304
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.18188703016323202
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.18202675425488016
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.1821768594639642
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.18198120510074453
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.18198843797047934
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.18197809853782393
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.1820492069866206
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.18231440087159476
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.18236929354699036
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.18214269930666144
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.18190542455667105
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.1821287474300288
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.18240544218569993
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.1824176929615162
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.18209925602848936
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.181671561426427
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.18187532414283072
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.18179520508822272
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.18179324130679286
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.181813164689075
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.18192600611258636
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.18186092644595028
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.18214474187956917
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.1820109255693771
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.18193358863177506
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.18232208522417212
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.18238036889345088
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.18268853880857167
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.1826970617597302
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.18257948326081344
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.1826309207446721
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.18263541417892534
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.182535337805748
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.1825247890878432
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.18247119483410143
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.18210032495480138
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.1820968336497362
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.18169273663134802
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.18173282368565505
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.18175598909364682
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.1816678161698359
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.18173334809071426
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.18197617910125038
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.18178225328793396
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.18143403410379375
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.18116843924585696
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.18129990825004744
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.18100851903791013
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.18092115955619975
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.18107319260254884
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.18095953610994048
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.1808970331394372
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.1809752456843853
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.1807467503981157
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.1807447210198543
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.18105915406855141
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.18092820577083096
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.18098713159561158
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.18111349464881987
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.18112844099679332
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.1811174595495686
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.18111789838750234
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.18106441555114894
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.1809267069547231
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.1808206816062783
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.18116571490925953
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.18109903862672067
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.18085139868436037
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.1808121306273867
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.18088331851210906
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.18089867458827252
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.18106391018243145
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.18120868802070617
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.18127605974251496
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.18116619280526336
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.18118787374529804
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.18130083661526442
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.1811787045207517
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.18111953353636887
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.18112297822423531
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.1810723933819178
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.18119628017380734
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.18122755557298661
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.18107405295040432
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.1809221364949879
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.18091805961007387
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.18075093191552472
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.18083989524072216
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.18075718835760385
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.18068768425731901
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.18058874835319158
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.18073416674661935
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.18078335421159863
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.18068247746606791
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.18059086266123217
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.18063770810519258
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.1805828200971208
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.18062721382487903
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.18063357517302756
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.18046541092638485
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.18047239906376317
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.18024863805291216
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.18021592953625848
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.18010070135718897
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.18005931585334067
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.17992098534727372
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.1798679581147501
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.17992736194814954
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.17982067523354833
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.1797431830295735
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.1796062427122941
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.17976748518770633
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.1796340571509467
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.17958039275849064
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.17944155772636225
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.17922641556771074
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.1791304515917664
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.17902779071717648
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.17903194360194669
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.17897878029448464
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.1788605659248981
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.17881569084982393
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.1787736810351673
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.17872479534586064
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.17862009264839193
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.17868678063308638
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.17843592950363749
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.17843147974747878
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.17850946437339393
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.17859199711211443
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.17876011456805047
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.17874980077671646
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.17881753772497178
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.1787491115319788
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.17868115384094785
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.17856270317079986
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.17858517564394893
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.17847000738469565
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.17835721590565246
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.17830622102615337
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.17816803102883008
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.17817092718975397
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.1782556088197799
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.17822483005399387
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.17830075815601168
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.17836143265307788
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.1782133643315217
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.1782808336407639
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.17810227128642578
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.1781322940978037
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.17813135984293912
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.17818583772607047
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.17817458043044263
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.17804685781174656
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.17802275354797775
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.17793558938888157
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.17797719281432883
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.17783988184399074
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.17782055718445144
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.17776488463521528
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.17755185119938433
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.17745017452271225
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.17755346615677295
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.1775637369651299
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.17747198732505584
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.17742809148049662
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.17736853888401619
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.17736357146121087
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.17727562216884
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.17728485431097732
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.17724974860413736
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.1772103600172817
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.1771992308398088
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.17723887719801354
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.17715485418631025
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.17712193218523584
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.1770862867964096
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.17708042543761585
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.1770759057707903
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.17690718511820805
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.1769180059913666
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.17694324464443698
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.17696693241596223
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.17703569707167577
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.17694350674984946
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.17690070815708325
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.1768459731668938
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.17688692761402505
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.1768127967370674
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.17672251744724898
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.17661211772482524
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.17663312591180838
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.17661554033939655
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.17649389363796775
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.1763981560259375
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.17637602922580994
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.17620146477764304
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.17617879628010516
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.17615508265737304
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.1760963083206491
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.17612548778528597
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.17604010388753671
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.17603765367357818
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.17598846796693837
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.17594467328094385
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.17608813930562128
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.17597028433623976
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.17584636780348692
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.17599600472528001
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.1759978618324879
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.17595913984792697
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.1759829373235771
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.17604156375995705
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.17597397430087325
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.17594125644957764
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.1759214511292562
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.17589473488255286
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.17590427738532685
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.1758781871074563
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.17582537950331326
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.17585076831488144
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.17576714911881616
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.17577495014873043
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.17571282028332608
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.17566946654082977
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.175705471339893
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.17577444640349368
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.17578187579825774
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.17580375498211062
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.1758421128145372
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.17572382597515246
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.17559978615081429
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.1755895154674848
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.1756733725237292
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.175649835623258
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.17559719503712734
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.17554959346001087
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.17554194228570968
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.1755161728539498
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.17546514579449882
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.17541168881701186
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.17537390602252245
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.17525103630558136
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.1752463653923231
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.17518885132785028
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.17519476567023098
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.1751387532159781
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.1751210830514393
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.1752086011103437
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.17515391736564576
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.17508636860165205
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.17502871809708287
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.17502723620273172
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.174979474546382
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.17492642383212628
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.1749645471572876
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.17491516682468813
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.1749314035819127
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.17481635986295946
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.17471314643136587
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.17472944663065235
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.17466605737760074
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.1747205576210311
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.1747652649699381
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.17467431566980948
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.1745410611232122
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.17450921915605397
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.17459908999613863
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.17458728510176852
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.1744807536329643
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.1744126231800875
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.1742800902300528
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.1742553555790116
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.17417506458472645
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.1743077745998812
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.1742715756503903
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.17423583476176097
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.1742120725521143
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.17420383736577338
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.17414979082706683
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.1741198729509595
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.17407049136209624
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.17397263833454676
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.17395528475952965
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.1739686390554363
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.17398666980921715
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.17400673372765718
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.17403944478908057
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.17397854828767562
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.174023553979497
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.17409919539476906
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.17409092980673055
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.17398295675714812
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.17390728128914026
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.1739074328903994
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.17387165453479966
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.17380952188274362
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.1738147736412205
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.1737599313421979
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.17375703968046796
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.17377905413994324
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.17375064958240283
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.17365703518326217
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.1736154804493218
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.17354886462130853
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.17350184553909556
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.17350182989064386
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.17348768162727357
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.1735654412273397
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.17351744199463165
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.17352999518157314
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.17340851927023765
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.17328774113404125
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.17323960105734548
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.17317998288345587
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.1732364714379099
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.17317217926029116
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.1730892539411396
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.17308206408443846
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.17307417799336042
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.17307247006401574
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.17309535852128252
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.17303540339836707
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.17298733925118165
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.17297384173286204
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.17304349734279642
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.17304180891531978
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.17304171923595138
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.17299599140280425
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.17289584338364733
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.17293302621224418
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.1728427380995643
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.17280079059302808
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.1727769173068596
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.1727203231545823
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.17263376535316258
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.17261405414578937
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.17259907921155293
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.17254305410708112
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.17249683753864184
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.17256559942867242
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.17256184384991896
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.17258962541818618
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.1725541278176064
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.17258555973762446
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.17255541646163053
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.17261129992019728
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.17262809481247363
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.17261616000905633
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.17259954659344195
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.17256188888156243
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.17267106722391307
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.17262601749528023
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.17272337566786877
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.1727480199099717
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.17265237667036396
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.17260341465754328
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.1725799834728241
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.17252171984021092
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.1725002407190113
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.1724486766713802
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.17234015631508995
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.1722717433128246
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.17229195613318973
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.17223947067503576
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.17217973876082043
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.17220858347168716
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.17212811770110295
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.17219274350946104
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.1722137801843595
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.1721594952148934
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.1720701425860303
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.17202627337114376
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.1719958713603398
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.17195703274658902
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.1719317116667401
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.171915936678111
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.17190351101119866
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.17191183617163133
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.17187969366576047
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.1719283867028675
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.17189898346872265
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.17181852327452765
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.17179197875861318
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.17178424566456701
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.17170726558888458
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.17167922654209683
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.171628673685776
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.1716779795589677
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.17164380701678586
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.17163109408455646
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.17162398500510032
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.17168359915199488
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.1716571344535418
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.17161175863438355
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.17151080538593153
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.1715260693996117
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.1715290573335463
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.17147584249917017
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.1714567626952104
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.1715172055758472
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.17153464213236055
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.1715568167415071
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.1715393638724734
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.1716307469217454
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.1716307469217454 valid 0.23501423001289368
LOSS train 0.1716307469217454 valid 0.21330252289772034
LOSS train 0.1716307469217454 valid 0.21631463865439096
LOSS train 0.1716307469217454 valid 0.20744768530130386
LOSS train 0.1716307469217454 valid 0.2009672313928604
LOSS train 0.1716307469217454 valid 0.21121338258186975
LOSS train 0.1716307469217454 valid 0.2183826587029866
LOSS train 0.1716307469217454 valid 0.21778067760169506
LOSS train 0.1716307469217454 valid 0.21726631621519724
LOSS train 0.1716307469217454 valid 0.22012419253587723
LOSS train 0.1716307469217454 valid 0.21893139048056168
LOSS train 0.1716307469217454 valid 0.2183269460995992
LOSS train 0.1716307469217454 valid 0.21802155100382292
LOSS train 0.1716307469217454 valid 0.21691472296203887
LOSS train 0.1716307469217454 valid 0.21493824621041616
LOSS train 0.1716307469217454 valid 0.21638820506632328
LOSS train 0.1716307469217454 valid 0.21746455746538498
LOSS train 0.1716307469217454 valid 0.2173953221903907
LOSS train 0.1716307469217454 valid 0.21971794335465683
LOSS train 0.1716307469217454 valid 0.22032563611865044
LOSS train 0.1716307469217454 valid 0.21829686349346525
LOSS train 0.1716307469217454 valid 0.21690964766524054
LOSS train 0.1716307469217454 valid 0.21630722025166388
LOSS train 0.1716307469217454 valid 0.21604425584276518
LOSS train 0.1716307469217454 valid 0.21509639263153077
LOSS train 0.1716307469217454 valid 0.2155541628599167
LOSS train 0.1716307469217454 valid 0.21492889909832566
LOSS train 0.1716307469217454 valid 0.21513887441584043
LOSS train 0.1716307469217454 valid 0.21576547365764093
LOSS train 0.1716307469217454 valid 0.21716861675182977
LOSS train 0.1716307469217454 valid 0.217959875060666
LOSS train 0.1716307469217454 valid 0.21747243637219071
LOSS train 0.1716307469217454 valid 0.21802331823291202
LOSS train 0.1716307469217454 valid 0.2180312810575261
LOSS train 0.1716307469217454 valid 0.2197477195944105
LOSS train 0.1716307469217454 valid 0.2198563867972957
LOSS train 0.1716307469217454 valid 0.22028003028921178
LOSS train 0.1716307469217454 valid 0.22114613809083639
LOSS train 0.1716307469217454 valid 0.21976927381295425
LOSS train 0.1716307469217454 valid 0.2197135515511036
LOSS train 0.1716307469217454 valid 0.22032487683179902
LOSS train 0.1716307469217454 valid 0.22087596782616206
LOSS train 0.1716307469217454 valid 0.22082941649958146
LOSS train 0.1716307469217454 valid 0.22178097983652895
LOSS train 0.1716307469217454 valid 0.2214353905783759
LOSS train 0.1716307469217454 valid 0.2219742507390354
LOSS train 0.1716307469217454 valid 0.22226057439408403
LOSS train 0.1716307469217454 valid 0.2220408127953609
LOSS train 0.1716307469217454 valid 0.22282015973207903
LOSS train 0.1716307469217454 valid 0.22216400533914565
LOSS train 0.1716307469217454 valid 0.22242849743833729
LOSS train 0.1716307469217454 valid 0.22229840061985529
LOSS train 0.1716307469217454 valid 0.22284705374600752
LOSS train 0.1716307469217454 valid 0.22257766486318023
LOSS train 0.1716307469217454 valid 0.2225561502304944
LOSS train 0.1716307469217454 valid 0.2220973428338766
LOSS train 0.1716307469217454 valid 0.2221584876901225
LOSS train 0.1716307469217454 valid 0.22191811995259647
LOSS train 0.1716307469217454 valid 0.22234218049857576
LOSS train 0.1716307469217454 valid 0.22221612681945166
LOSS train 0.1716307469217454 valid 0.22221532146461676
LOSS train 0.1716307469217454 valid 0.22283715418269556
LOSS train 0.1716307469217454 valid 0.22269043870388516
LOSS train 0.1716307469217454 valid 0.22328723990358412
LOSS train 0.1716307469217454 valid 0.2236866682767868
LOSS train 0.1716307469217454 valid 0.22374145776936502
LOSS train 0.1716307469217454 valid 0.22342880737425677
LOSS train 0.1716307469217454 valid 0.2234286310918191
LOSS train 0.1716307469217454 valid 0.22283954702425693
LOSS train 0.1716307469217454 valid 0.22323439227683203
LOSS train 0.1716307469217454 valid 0.22310198591628544
LOSS train 0.1716307469217454 valid 0.2234270496086942
LOSS train 0.1716307469217454 valid 0.22369462578264002
LOSS train 0.1716307469217454 valid 0.22358389923701416
LOSS train 0.1716307469217454 valid 0.22351891338825225
LOSS train 0.1716307469217454 valid 0.2241022216254159
LOSS train 0.1716307469217454 valid 0.2240590424119652
LOSS train 0.1716307469217454 valid 0.2239353908942296
LOSS train 0.1716307469217454 valid 0.223536684166027
LOSS train 0.1716307469217454 valid 0.22298491671681403
LOSS train 0.1716307469217454 valid 0.22253938405602067
LOSS train 0.1716307469217454 valid 0.22269645551355874
LOSS train 0.1716307469217454 valid 0.2223876522607114
LOSS train 0.1716307469217454 valid 0.2221473263842719
LOSS train 0.1716307469217454 valid 0.22173657434828142
LOSS train 0.1716307469217454 valid 0.2211928442120552
LOSS train 0.1716307469217454 valid 0.2211995763682771
LOSS train 0.1716307469217454 valid 0.2208951242606748
LOSS train 0.1716307469217454 valid 0.2212020406562291
LOSS train 0.1716307469217454 valid 0.22139665832122166
LOSS train 0.1716307469217454 valid 0.221709206536576
LOSS train 0.1716307469217454 valid 0.2216143885060497
LOSS train 0.1716307469217454 valid 0.22144234725224074
LOSS train 0.1716307469217454 valid 0.22168092540604004
LOSS train 0.1716307469217454 valid 0.2212251222447345
LOSS train 0.1716307469217454 valid 0.2210834768290321
LOSS train 0.1716307469217454 valid 0.2211134576613141
LOSS train 0.1716307469217454 valid 0.22120817902745032
LOSS train 0.1716307469217454 valid 0.22157754034104973
LOSS train 0.1716307469217454 valid 0.22183196276426315
LOSS train 0.1716307469217454 valid 0.22202721577469664
LOSS train 0.1716307469217454 valid 0.22240127199420742
LOSS train 0.1716307469217454 valid 0.22247873883224228
LOSS train 0.1716307469217454 valid 0.22252656275836322
LOSS train 0.1716307469217454 valid 0.22251021947179522
LOSS train 0.1716307469217454 valid 0.22301159799098969
LOSS train 0.1716307469217454 valid 0.2229125410596901
LOSS train 0.1716307469217454 valid 0.22321492350763744
LOSS train 0.1716307469217454 valid 0.22347644026126337
LOSS train 0.1716307469217454 valid 0.2236948086456819
LOSS train 0.1716307469217454 valid 0.22364731307502264
LOSS train 0.1716307469217454 valid 0.22336280239479883
LOSS train 0.1716307469217454 valid 0.2235059181673337
LOSS train 0.1716307469217454 valid 0.22357592601002307
LOSS train 0.1716307469217454 valid 0.22354451430880506
LOSS train 0.1716307469217454 valid 0.2238725296639163
LOSS train 0.1716307469217454 valid 0.2240068033719674
LOSS train 0.1716307469217454 valid 0.2237081025111473
LOSS train 0.1716307469217454 valid 0.22346549211930827
LOSS train 0.1716307469217454 valid 0.2233153895785411
LOSS train 0.1716307469217454 valid 0.22299471990136074
LOSS train 0.1716307469217454 valid 0.22287694157147017
LOSS train 0.1716307469217454 valid 0.22308536406939591
LOSS train 0.1716307469217454 valid 0.22324314425068517
LOSS train 0.1716307469217454 valid 0.223135209441185
LOSS train 0.1716307469217454 valid 0.22333783538095534
LOSS train 0.1716307469217454 valid 0.22316426437670792
LOSS train 0.1716307469217454 valid 0.22324668103829026
LOSS train 0.1716307469217454 valid 0.22342124003772587
LOSS train 0.1716307469217454 valid 0.22339713940253625
LOSS train 0.1716307469217454 valid 0.22319169126394142
LOSS train 0.1716307469217454 valid 0.2229470917672822
LOSS train 0.1716307469217454 valid 0.22275268919485852
LOSS train 0.1716307469217454 valid 0.22276155784058926
LOSS train 0.1716307469217454 valid 0.22275277199568572
LOSS train 0.1716307469217454 valid 0.22276911380536416
LOSS train 0.1716307469217454 valid 0.2225687925832985
LOSS train 0.1716307469217454 valid 0.22257045375696127
LOSS train 0.1716307469217454 valid 0.22231492871860806
LOSS train 0.1716307469217454 valid 0.22245618158153124
LOSS train 0.1716307469217454 valid 0.222391813465044
LOSS train 0.1716307469217454 valid 0.2224245538384142
LOSS train 0.1716307469217454 valid 0.222355343453534
LOSS train 0.1716307469217454 valid 0.22234703683190876
LOSS train 0.1716307469217454 valid 0.22217306063092987
LOSS train 0.1716307469217454 valid 0.22247884208208893
LOSS train 0.1716307469217454 valid 0.22230364372130154
LOSS train 0.1716307469217454 valid 0.2231110286068272
LOSS train 0.1716307469217454 valid 0.22316801918032986
LOSS train 0.1716307469217454 valid 0.2230827291806539
LOSS train 0.1716307469217454 valid 0.22324692354297007
LOSS train 0.1716307469217454 valid 0.22317483766298546
LOSS train 0.1716307469217454 valid 0.22314175788094015
LOSS train 0.1716307469217454 valid 0.22318734228610992
LOSS train 0.1716307469217454 valid 0.22317283749580383
LOSS train 0.1716307469217454 valid 0.22344270272132677
LOSS train 0.1716307469217454 valid 0.22355753354206206
LOSS train 0.1716307469217454 valid 0.22368557619143137
LOSS train 0.1716307469217454 valid 0.2237008305840522
LOSS train 0.1716307469217454 valid 0.22362971426919104
LOSS train 0.1716307469217454 valid 0.22356540223826532
LOSS train 0.1716307469217454 valid 0.22342733302969991
LOSS train 0.1716307469217454 valid 0.2232602636324116
LOSS train 0.1716307469217454 valid 0.22316948806003825
LOSS train 0.1716307469217454 valid 0.22308380504449207
LOSS train 0.1716307469217454 valid 0.22312837264623986
LOSS train 0.1716307469217454 valid 0.2233645639376726
LOSS train 0.1716307469217454 valid 0.22348639723800479
LOSS train 0.1716307469217454 valid 0.22368736767909936
LOSS train 0.1716307469217454 valid 0.22376453587237527
LOSS train 0.1716307469217454 valid 0.2237428314330285
LOSS train 0.1716307469217454 valid 0.22353529626893442
LOSS train 0.1716307469217454 valid 0.22367384435469015
LOSS train 0.1716307469217454 valid 0.2236349967190589
LOSS train 0.1716307469217454 valid 0.223479785323143
LOSS train 0.1716307469217454 valid 0.22337811745025896
LOSS train 0.1716307469217454 valid 0.2235189471709526
LOSS train 0.1716307469217454 valid 0.22365878597739036
LOSS train 0.1716307469217454 valid 0.22357911329362645
LOSS train 0.1716307469217454 valid 0.22357417485780187
LOSS train 0.1716307469217454 valid 0.22369086544816666
LOSS train 0.1716307469217454 valid 0.22365764888760808
LOSS train 0.1716307469217454 valid 0.22360548719030912
LOSS train 0.1716307469217454 valid 0.22345736752385678
LOSS train 0.1716307469217454 valid 0.2233910783722594
LOSS train 0.1716307469217454 valid 0.22340161161076638
LOSS train 0.1716307469217454 valid 0.2231507455920153
LOSS train 0.1716307469217454 valid 0.22314275167089828
LOSS train 0.1716307469217454 valid 0.22315730343735407
LOSS train 0.1716307469217454 valid 0.22314204848126362
LOSS train 0.1716307469217454 valid 0.22311521941766688
LOSS train 0.1716307469217454 valid 0.22299486266759536
LOSS train 0.1716307469217454 valid 0.22296949062940372
LOSS train 0.1716307469217454 valid 0.2228064994836591
LOSS train 0.1716307469217454 valid 0.2225713080320603
LOSS train 0.1716307469217454 valid 0.22263578629615355
LOSS train 0.1716307469217454 valid 0.22278628781967358
LOSS train 0.1716307469217454 valid 0.22271550379016183
LOSS train 0.1716307469217454 valid 0.22274214927874617
LOSS train 0.1716307469217454 valid 0.2225940027832985
LOSS train 0.1716307469217454 valid 0.22248479546006047
LOSS train 0.1716307469217454 valid 0.22245180171610104
LOSS train 0.1716307469217454 valid 0.2224540929929376
LOSS train 0.1716307469217454 valid 0.22259464940311863
LOSS train 0.1716307469217454 valid 0.22244871782093514
LOSS train 0.1716307469217454 valid 0.22258074002937206
LOSS train 0.1716307469217454 valid 0.22249534963697626
LOSS train 0.1716307469217454 valid 0.22247419183930525
LOSS train 0.1716307469217454 valid 0.22236760958815305
LOSS train 0.1716307469217454 valid 0.22234614867539632
LOSS train 0.1716307469217454 valid 0.2222960022812206
LOSS train 0.1716307469217454 valid 0.22216499941247814
LOSS train 0.1716307469217454 valid 0.22214162678505894
LOSS train 0.1716307469217454 valid 0.22208128883459857
LOSS train 0.1716307469217454 valid 0.2220018611397854
LOSS train 0.1716307469217454 valid 0.2218259021087929
LOSS train 0.1716307469217454 valid 0.22176594846808964
LOSS train 0.1716307469217454 valid 0.22166112243035518
LOSS train 0.1716307469217454 valid 0.2217681930489736
LOSS train 0.1716307469217454 valid 0.22180757441303947
LOSS train 0.1716307469217454 valid 0.2217735540947763
LOSS train 0.1716307469217454 valid 0.22179808749540433
LOSS train 0.1716307469217454 valid 0.22191088603216436
LOSS train 0.1716307469217454 valid 0.22202530563143746
LOSS train 0.1716307469217454 valid 0.22222348855601418
LOSS train 0.1716307469217454 valid 0.2222900999858316
LOSS train 0.1716307469217454 valid 0.222476352547759
LOSS train 0.1716307469217454 valid 0.22256814106776004
LOSS train 0.1716307469217454 valid 0.22253221650831564
LOSS train 0.1716307469217454 valid 0.22258597379145414
LOSS train 0.1716307469217454 valid 0.22270403828197743
LOSS train 0.1716307469217454 valid 0.2227314070252509
LOSS train 0.1716307469217454 valid 0.222695233548148
LOSS train 0.1716307469217454 valid 0.22283093191874334
LOSS train 0.1716307469217454 valid 0.22293348902083457
LOSS train 0.1716307469217454 valid 0.22285349199832497
LOSS train 0.1716307469217454 valid 0.22277052065491174
LOSS train 0.1716307469217454 valid 0.22271954343348993
LOSS train 0.1716307469217454 valid 0.22254833547889438
LOSS train 0.1716307469217454 valid 0.22252919307599464
LOSS train 0.1716307469217454 valid 0.2226449450887585
LOSS train 0.1716307469217454 valid 0.22250353637313056
LOSS train 0.1716307469217454 valid 0.2226715501198553
LOSS train 0.1716307469217454 valid 0.2228261755138147
LOSS train 0.1716307469217454 valid 0.2228075538970986
LOSS train 0.1716307469217454 valid 0.22272732829659936
LOSS train 0.1716307469217454 valid 0.2228509655848206
LOSS train 0.1716307469217454 valid 0.22280436796286412
LOSS train 0.1716307469217454 valid 0.2227876855427003
LOSS train 0.1716307469217454 valid 0.22279204565286637
LOSS train 0.1716307469217454 valid 0.2226569632372533
LOSS train 0.1716307469217454 valid 0.22277253523232446
LOSS train 0.1716307469217454 valid 0.22267372093417429
LOSS train 0.1716307469217454 valid 0.22261348167273004
LOSS train 0.1716307469217454 valid 0.22263881061591353
LOSS train 0.1716307469217454 valid 0.22266063408460468
LOSS train 0.1716307469217454 valid 0.22251369366386056
LOSS train 0.1716307469217454 valid 0.2226990150619847
LOSS train 0.1716307469217454 valid 0.2226957184007269
LOSS train 0.1716307469217454 valid 0.22271233080671385
LOSS train 0.1716307469217454 valid 0.2227998593986263
LOSS train 0.1716307469217454 valid 0.22284603937891603
LOSS train 0.1716307469217454 valid 0.22280727485513505
LOSS train 0.1716307469217454 valid 0.2228443408667138
LOSS train 0.1716307469217454 valid 0.22283221904961567
LOSS train 0.1716307469217454 valid 0.22280995872683992
LOSS train 0.1716307469217454 valid 0.22292292586873086
LOSS train 0.1716307469217454 valid 0.22299853979206796
LOSS train 0.1716307469217454 valid 0.22312686201808177
LOSS train 0.1716307469217454 valid 0.2230957603013074
LOSS train 0.1716307469217454 valid 0.223185305280879
LOSS train 0.1716307469217454 valid 0.22344402656616533
LOSS train 0.1716307469217454 valid 0.22359722814498803
LOSS train 0.1716307469217454 valid 0.2236620947501085
LOSS train 0.1716307469217454 valid 0.22357870080254294
LOSS train 0.1716307469217454 valid 0.22357484439144965
LOSS train 0.1716307469217454 valid 0.22347062142962584
LOSS train 0.1716307469217454 valid 0.22331338858218502
LOSS train 0.1716307469217454 valid 0.22331006194741923
LOSS train 0.1716307469217454 valid 0.22326793021389416
LOSS train 0.1716307469217454 valid 0.22319745964427012
LOSS train 0.1716307469217454 valid 0.222968708855886
LOSS train 0.1716307469217454 valid 0.22302321078709916
LOSS train 0.1716307469217454 valid 0.22309309186440118
LOSS train 0.1716307469217454 valid 0.22308625699135295
LOSS train 0.1716307469217454 valid 0.22306074280213642
LOSS train 0.1716307469217454 valid 0.22295847410524347
LOSS train 0.1716307469217454 valid 0.22291698777634236
LOSS train 0.1716307469217454 valid 0.22289886869567488
LOSS train 0.1716307469217454 valid 0.22287186450999358
LOSS train 0.1716307469217454 valid 0.2227086177499024
LOSS train 0.1716307469217454 valid 0.22268736622717283
LOSS train 0.1716307469217454 valid 0.22270407321713484
LOSS train 0.1716307469217454 valid 0.2227097658985326
LOSS train 0.1716307469217454 valid 0.22282590194273802
LOSS train 0.1716307469217454 valid 0.22275408085536313
LOSS train 0.1716307469217454 valid 0.22279002575159876
LOSS train 0.1716307469217454 valid 0.22278752963014897
LOSS train 0.1716307469217454 valid 0.22284467632953936
LOSS train 0.1716307469217454 valid 0.22289726744095484
LOSS train 0.1716307469217454 valid 0.22287904176601145
LOSS train 0.1716307469217454 valid 0.2228160093458283
LOSS train 0.1716307469217454 valid 0.2229616022837831
LOSS train 0.1716307469217454 valid 0.22294569138045373
LOSS train 0.1716307469217454 valid 0.2229215607779925
LOSS train 0.1716307469217454 valid 0.2229143050955791
LOSS train 0.1716307469217454 valid 0.22290823523695383
LOSS train 0.1716307469217454 valid 0.22280387599746904
LOSS train 0.1716307469217454 valid 0.22292293314023312
LOSS train 0.1716307469217454 valid 0.2229202685336913
LOSS train 0.1716307469217454 valid 0.22284258164201903
LOSS train 0.1716307469217454 valid 0.22285165117146113
LOSS train 0.1716307469217454 valid 0.22294342979645956
LOSS train 0.1716307469217454 valid 0.22304466352531105
LOSS train 0.1716307469217454 valid 0.2229830895151411
LOSS train 0.1716307469217454 valid 0.22289639645362203
LOSS train 0.1716307469217454 valid 0.2229494091464142
LOSS train 0.1716307469217454 valid 0.22298192514001197
LOSS train 0.1716307469217454 valid 0.22306084301217596
LOSS train 0.1716307469217454 valid 0.22291839700192212
LOSS train 0.1716307469217454 valid 0.2230398855476736
LOSS train 0.1716307469217454 valid 0.22309338060780343
LOSS train 0.1716307469217454 valid 0.22308568218175104
LOSS train 0.1716307469217454 valid 0.22319776196906596
LOSS train 0.1716307469217454 valid 0.22317471366662245
LOSS train 0.1716307469217454 valid 0.22339155630099994
LOSS train 0.1716307469217454 valid 0.22352197301497154
LOSS train 0.1716307469217454 valid 0.22343697875919866
LOSS train 0.1716307469217454 valid 0.2236054569120465
LOSS train 0.1716307469217454 valid 0.2235639576659058
LOSS train 0.1716307469217454 valid 0.22346686065377067
LOSS train 0.1716307469217454 valid 0.22340259229741902
LOSS train 0.1716307469217454 valid 0.22343854145244793
LOSS train 0.1716307469217454 valid 0.223566811331018
LOSS train 0.1716307469217454 valid 0.22356072014837122
LOSS train 0.1716307469217454 valid 0.22364021296657266
LOSS train 0.1716307469217454 valid 0.2236310127315606
LOSS train 0.1716307469217454 valid 0.2235934063351366
LOSS train 0.1716307469217454 valid 0.22366286691135362
LOSS train 0.1716307469217454 valid 0.2236809685388032
LOSS train 0.1716307469217454 valid 0.22354778795997418
LOSS train 0.1716307469217454 valid 0.22352756847415053
LOSS train 0.1716307469217454 valid 0.22359984326814422
LOSS train 0.1716307469217454 valid 0.2238025178829598
LOSS train 0.1716307469217454 valid 0.22385763167471126
LOSS train 0.1716307469217454 valid 0.22386653984040883
LOSS train 0.1716307469217454 valid 0.22374534727173512
LOSS train 0.1716307469217454 valid 0.22371589281093116
LOSS train 0.1716307469217454 valid 0.2237639977545998
LOSS train 0.1716307469217454 valid 0.22368730634450912
LOSS train 0.1716307469217454 valid 0.2235952510877892
LOSS train 0.1716307469217454 valid 0.22356537136841903
LOSS train 0.1716307469217454 valid 0.22360591565076757
LOSS train 0.1716307469217454 valid 0.2236930033833967
LOSS train 0.1716307469217454 valid 0.22378868330532398
LOSS train 0.1716307469217454 valid 0.2238461466903767
LOSS train 0.1716307469217454 valid 0.22387517091272927
LOSS train 0.1716307469217454 valid 0.2237520023098205
LOSS train 0.1716307469217454 valid 0.22380258005973688
LOSS train 0.1716307469217454 valid 0.22379218662778536
LOSS train 0.1716307469217454 valid 0.22376321375865355
LOSS train 0.1716307469217454 valid 0.22387760474207652
LOSS train 0.1716307469217454 valid 0.22382065657905967
LOSS train 0.1716307469217454 valid 0.22384021042303726
LOSS train 0.1716307469217454 valid 0.22384212302835022
LOSS train 0.1716307469217454 valid 0.2238108850812
LOSS train 0.1716307469217454 valid 0.22371036727844207
LOSS train 0.1716307469217454 valid 0.22371406456374604
LOSS train 0.1716307469217454 valid 0.22377893320590178
EPOCH 3:
  batch 1 loss: 0.1598057597875595
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.14989278465509415
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.15586563448111215
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.1620573326945305
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.16808864176273347
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.17096395045518875
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.16377784524645125
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.16333533264696598
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.16078709893756443
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.15963881313800812
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.15730696374719794
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.15645084778467813
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.15635217611606306
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.15616066328116826
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.1565121293067932
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.15812727343291044
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.15796417173217325
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.15961402985784742
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.15850348378482618
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.15736818313598633
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.1578558413755326
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.15828898278149692
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.15878407074057538
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.15903485690553984
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.15927470982074737
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.15931378763455611
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.1595879566890222
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.15942583233118057
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.15866207048810763
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.15957994858423868
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.1598124994385627
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.16037950199097395
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.15955376399285864
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.16027087893556147
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.16162814838545664
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.16087388205859396
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.16101245139096235
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.1609546997045216
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.16107425437523767
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.16134162433445454
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.161222929634699
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.16175459112439836
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.16231441081956374
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.16158037090843375
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.1622907708088557
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.16204307811415714
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.16131584092657617
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.16105878415207067
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.1610952576204222
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.16136818408966064
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.16133878774502697
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.16162880710684335
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.16176523155761216
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.16219894532804136
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.16210827366872269
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.1618600639381579
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.16185282327626882
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.16136256862303305
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.16172500485080785
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.1615206537147363
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.16173924676707532
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.16142970563903933
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.16120001626393152
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.16126424889080226
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.1612058985691804
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.16109768256093515
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.1608287698742169
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.1608469026053653
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.1609984850106032
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.1612816176244191
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.16107650520935865
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.16106201149523258
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.1610619593156527
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.1611409525613527
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.16128054281075796
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.1614476816826745
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.1612769349828943
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.16107783829554534
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.16129531720771065
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.16157932225614785
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.1616920784667686
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.16132600187528423
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.16092482550316548
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.16118253270785013
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.16114695317604963
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.16121637994466825
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.16137198315954757
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.16151501069014723
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.16147738243086954
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.16171889338228437
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.16162107491886224
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.16150605597573778
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.1618301554392743
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.16184193006855377
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.16218136580366838
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.1622079067553083
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.1621098759555325
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.16215685922272352
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.162197877963384
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.1621266807615757
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.16215823620262712
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.16210961590210596
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.1617197733625625
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.1617721698175256
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.16140101346231642
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.1615098536998596
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.16165311307272065
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.16155524393198667
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.16164001173109088
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.1619416928426786
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.16175015062630713
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.16144287167117
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.16118733274989425
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.16137699423390522
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.16107186616762825
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.16098195462134376
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.16119811072563514
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.1611029368714761
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.16103196150364996
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.16114630804707605
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.1608841701849433
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.1609147516552542
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.1612807276651142
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.1612069142321425
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.1612772428393364
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.16138785574880857
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.16142518507448708
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.16145805892301723
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.16148888839538708
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.1615259822171468
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.16140275983182528
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.16126730332546163
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.16164642230684595
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.16161694950354633
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.16138382956937508
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.1613797002526767
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.1614922497720614
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.1614330865010835
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.16164441562170606
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.16179254156138215
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.16189083589095596
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.16185186745625146
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.16193778891038227
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.16212481751831043
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.1620153471827507
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.16201983427960578
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.16202132723161153
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.16201471602795897
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.1621861414321317
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.16220301553606986
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.16211181064907287
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.16199648836137434
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.16200543029440773
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.1618060138221685
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.16191686713887798
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.16181946435991007
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.16176096985864033
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.16172114641794674
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.16184606318766215
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.16195071223191917
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.1618826244466053
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.16182708772428242
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.16187848923213644
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.1618236435531843
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.16193133542935054
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.1619045388357467
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.16174014425741698
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.16173452178814582
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.16152546025768538
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.16156825515277246
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.16148501046394048
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.16146166999499464
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.16129411149748488
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.16126558188905663
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.16135713181325367
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.16126724167472936
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.16121877750939567
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.16110414898629938
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.1612902665354686
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.1611888845347696
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.161133940501437
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.16099593252107336
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.16082018495745998
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.16074004336057798
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.16064414651812733
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.16062523693006525
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.16058247440798398
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.16046831253519717
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.16043689283271315
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.16044996941560194
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.1604105088645251
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.16030311301195374
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.16037410184972645
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.1601624092106352
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.16019995361566544
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.16029260334159648
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.1603729691980454
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.16062380369715015
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.16067207087973254
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.16076709862798452
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.16074163474105485
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.16068703913600138
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.16061758110529095
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.1606708908563151
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.1605652167666249
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.1604540816426856
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.160389202264484
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.16027494903224018
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.16034100183316966
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.1604525961691425
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.16044772260985668
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.16055144239568486
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.1606361070918925
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.16051424089296956
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.16064404966526252
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.1604760977336102
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.16056062737780233
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.16055911905858494
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.16064025247341965
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.16070225282826206
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.16060336544368062
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.1605888565180001
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.1605396399383053
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.16062409480634546
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.16050138960282007
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.16053774593546327
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.16054568104938263
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.16037728222446485
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.16026694971791522
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.16042203193773394
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.16050391657140864
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.16046166127740308
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.16041096338258792
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.16036201048737916
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.16037933645730323
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.16033133583427486
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.1603247427424801
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.16032088964300997
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.16028230899173346
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.16028121054793398
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.16038710735274547
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.16031342938788667
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.16031098730647514
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.1602754253039106
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.16030783230552867
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.16033016045282528
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.16023103630373836
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.160296241692718
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.16034155877598796
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.16037020567059518
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.16046684259081267
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.16038714588752814
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.16038105666284033
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.16034994030913968
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.16042009267736884
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.1603733367228415
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.16028737468130394
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.1602145522082037
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.16025341376949923
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.16025157946233565
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.16013164238797295
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.16006005421735858
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.16005445430373963
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.15992303539744832
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.15993547279317424
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.15991472108359622
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.15987128703000394
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.15991783789845546
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.15985908763670123
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.1598923418532919
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.1598737719589054
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.1598719076034339
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.16002399528965408
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.15992047015006525
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.15982467236843975
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.16000773900768894
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.1600225100071852
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.15999367010357568
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.1600481291238125
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.1601223560582314
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.16006913550489738
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.16003280310026297
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.16004500669746433
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.16004712588455475
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.1600889158092047
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.160094333721416
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.16007235119255578
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.1601056109648198
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.16002181695716192
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.16005642236820583
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.16004485206812927
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.1600127338801753
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.16008633118338958
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.16017035427851742
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.16018145071753
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.1602286127823833
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.16030642046571178
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.16021378005811032
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.16012111692424602
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.16010761799911657
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.16018184200176766
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.16015641003945805
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.16013688392469985
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.1601191181444416
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.1601383360927222
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.16012966528145317
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.16011996574529996
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.1600705464793877
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.1600390773230386
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.15993810365757635
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.1599567948669866
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.15991981807523048
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.15994579187882974
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.15990916499571436
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.159911704749342
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.16002264762698096
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.15998763525241558
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.15995024212595052
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.15992775164708076
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.15993493378628046
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.1599066171365735
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.1598625164826094
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.1599050886444632
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.15986178734880171
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.15988299642617887
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.15978919940949218
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.15967377159026785
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.15971650437610904
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.1596875970667981
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.1597419566728852
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.1597994659152276
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.15971478855753518
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.15959678657420046
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.1595766729937342
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.1596731139207954
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.15967138235767683
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.1595843882252979
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.15951739758605787
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.15939674963817485
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.1593789542422575
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.1593051557142364
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.15945776085755978
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.15944444164416532
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.15942620654958625
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.15942325518615003
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.15942019340447608
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.15938626234057313
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.1593509741641324
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.15930561174806687
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.15922655663319996
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.15921878101479295
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.1592338516546244
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.1592601744969911
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.1592942718732155
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.1593186612700073
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.15926006942820015
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.15931346165198906
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.1593902297942332
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.15938390737456531
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.15929239247408178
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.15922557614186464
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.15925834492589888
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.15922548032824987
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.15918625334461967
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.1592041363454845
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.15914598766897545
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.15915873163564978
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.1592069107067326
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.1591967136717747
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.15911832169906512
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.15911680897773114
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.1590664760480004
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.1590266776628213
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.1590591737612046
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.1590719010035197
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.15916114787947624
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.15910385647882516
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.15911750704365432
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.15901476539926981
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.15893216840922833
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.1588903515553224
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.15882955154121234
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.15890671602661866
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.1588609503620925
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.15877643279828035
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.15878604501099783
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.1587810476890522
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.1587857271246996
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.15882589468842606
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.15876956904927889
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.1587476386019336
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.15874922779217668
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.15886829620933413
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.15887545092683758
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.15888155783656277
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.15885033482930275
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.158753574832861
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.1588054319197808
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.15873763805493377
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.15870034903287888
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.1586729894552445
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.1586190607417282
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.15855154975620156
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.15854158565992177
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.15855061989507557
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.1585035219861956
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.1584675033423473
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.15855840173568211
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.15855623067591185
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.15859000599238932
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.1585552760936918
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.15859297782327364
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.15856233091244687
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.15863661547213936
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.158650601628315
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.1586575977002772
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.15865133066686224
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.15860431698378194
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.1587228173999058
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.15869705339982396
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.15881762489003975
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.15886834715779924
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.1587754408905974
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.15875137637738349
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.15873920284649906
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.158688670295365
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.15867731458479123
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.15863294845499168
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.1585442022982733
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.1584962896482889
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.15850829915629738
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.1584536313527712
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.1584039263375637
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.15844131714325346
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.15837187722496604
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.15845581414092572
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.15850327448659413
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.15845313753303328
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.15837077197357843
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.15832674979147585
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.15829235178475476
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.158267810957599
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.1582603732935462
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.15823607752519148
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.15823869529399978
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.1582674476497644
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.1582380082349916
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.15833828846059209
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.15837583488438337
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.15831235897209908
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.1583420225396389
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.15839313239320718
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.15832681935815096
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.1583174431474986
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.15828883764180507
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.1583743271042119
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.15836159899602573
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.158385639449004
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.15839734977042233
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.15848479032840418
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.1584666690540676
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.15843501417732342
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.15835103629152925
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.15836967445974207
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.1583946279780839
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.15835995701953065
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.15836113045187528
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.15843023466439837
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.1584507395495484
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.15848697286019933
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.15848187531192337
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.1586075572423258
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.1586075572423258 valid 0.22423020005226135
LOSS train 0.1586075572423258 valid 0.20230266451835632
LOSS train 0.1586075572423258 valid 0.20567719141642252
LOSS train 0.1586075572423258 valid 0.19527219980955124
LOSS train 0.1586075572423258 valid 0.18834908604621886
LOSS train 0.1586075572423258 valid 0.1988437920808792
LOSS train 0.1586075572423258 valid 0.20559728571346828
LOSS train 0.1586075572423258 valid 0.20521341636776924
LOSS train 0.1586075572423258 valid 0.20460250145859188
LOSS train 0.1586075572423258 valid 0.20719139277935028
LOSS train 0.1586075572423258 valid 0.20565412532199512
LOSS train 0.1586075572423258 valid 0.20515568057696024
LOSS train 0.1586075572423258 valid 0.20480842085985038
LOSS train 0.1586075572423258 valid 0.2035401314496994
LOSS train 0.1586075572423258 valid 0.20169824759165447
LOSS train 0.1586075572423258 valid 0.20312275644391775
LOSS train 0.1586075572423258 valid 0.2041844410054824
LOSS train 0.1586075572423258 valid 0.20416618883609772
LOSS train 0.1586075572423258 valid 0.20646154409960696
LOSS train 0.1586075572423258 valid 0.2067966118454933
LOSS train 0.1586075572423258 valid 0.2048932732570739
LOSS train 0.1586075572423258 valid 0.20353719863024625
LOSS train 0.1586075572423258 valid 0.20289268441822217
LOSS train 0.1586075572423258 valid 0.20268953467408815
LOSS train 0.1586075572423258 valid 0.20175878703594208
LOSS train 0.1586075572423258 valid 0.2022603263075535
LOSS train 0.1586075572423258 valid 0.20173442363739014
LOSS train 0.1586075572423258 valid 0.2015649149460452
LOSS train 0.1586075572423258 valid 0.20195242916715556
LOSS train 0.1586075572423258 valid 0.2033534159262975
LOSS train 0.1586075572423258 valid 0.20408897919039573
LOSS train 0.1586075572423258 valid 0.2036720965988934
LOSS train 0.1586075572423258 valid 0.2040631626591538
LOSS train 0.1586075572423258 valid 0.20410488721202402
LOSS train 0.1586075572423258 valid 0.20587300998823985
LOSS train 0.1586075572423258 valid 0.206069345275561
LOSS train 0.1586075572423258 valid 0.2063645974204347
LOSS train 0.1586075572423258 valid 0.20731530495380102
LOSS train 0.1586075572423258 valid 0.20594720198557928
LOSS train 0.1586075572423258 valid 0.20587439499795437
LOSS train 0.1586075572423258 valid 0.20631691922501819
LOSS train 0.1586075572423258 valid 0.20677508059002103
LOSS train 0.1586075572423258 valid 0.20681604846965435
LOSS train 0.1586075572423258 valid 0.2078136568042365
LOSS train 0.1586075572423258 valid 0.20774435698986055
LOSS train 0.1586075572423258 valid 0.20827347493689993
LOSS train 0.1586075572423258 valid 0.20848520385458114
LOSS train 0.1586075572423258 valid 0.20815468207001686
LOSS train 0.1586075572423258 valid 0.20902091812114326
LOSS train 0.1586075572423258 valid 0.20838744789361954
LOSS train 0.1586075572423258 valid 0.2086370838623421
LOSS train 0.1586075572423258 valid 0.20839121794471374
LOSS train 0.1586075572423258 valid 0.2089523563407502
LOSS train 0.1586075572423258 valid 0.20877017549894475
LOSS train 0.1586075572423258 valid 0.2087600759484551
LOSS train 0.1586075572423258 valid 0.20829121528991631
LOSS train 0.1586075572423258 valid 0.2083964421038042
LOSS train 0.1586075572423258 valid 0.20811323461861447
LOSS train 0.1586075572423258 valid 0.208567192493859
LOSS train 0.1586075572423258 valid 0.20841043815016747
LOSS train 0.1586075572423258 valid 0.2084405033803377
LOSS train 0.1586075572423258 valid 0.20898068191543703
LOSS train 0.1586075572423258 valid 0.20883088050380585
LOSS train 0.1586075572423258 valid 0.20941347908228636
LOSS train 0.1586075572423258 valid 0.20974370722587293
LOSS train 0.1586075572423258 valid 0.2097266548962304
LOSS train 0.1586075572423258 valid 0.20949425337029925
LOSS train 0.1586075572423258 valid 0.20939035976634307
LOSS train 0.1586075572423258 valid 0.20880668966666513
LOSS train 0.1586075572423258 valid 0.20920075178146363
LOSS train 0.1586075572423258 valid 0.20908487619648516
LOSS train 0.1586075572423258 valid 0.20936100536750424
LOSS train 0.1586075572423258 valid 0.20969558864423674
LOSS train 0.1586075572423258 valid 0.20954364799969905
LOSS train 0.1586075572423258 valid 0.20942051589488983
LOSS train 0.1586075572423258 valid 0.21000212921123756
LOSS train 0.1586075572423258 valid 0.20998148368550584
LOSS train 0.1586075572423258 valid 0.20980993849344742
LOSS train 0.1586075572423258 valid 0.20934378931039496
LOSS train 0.1586075572423258 valid 0.20874787587672472
LOSS train 0.1586075572423258 valid 0.20827984257980628
LOSS train 0.1586075572423258 valid 0.20848836113766925
LOSS train 0.1586075572423258 valid 0.20819499244891018
LOSS train 0.1586075572423258 valid 0.20802800233165422
LOSS train 0.1586075572423258 valid 0.20753238551756914
LOSS train 0.1586075572423258 valid 0.20705542141614958
LOSS train 0.1586075572423258 valid 0.20708799876015763
LOSS train 0.1586075572423258 valid 0.2067886699329723
LOSS train 0.1586075572423258 valid 0.20706109652358495
LOSS train 0.1586075572423258 valid 0.2073026183578703
LOSS train 0.1586075572423258 valid 0.20763296281898413
LOSS train 0.1586075572423258 valid 0.20747912675142288
LOSS train 0.1586075572423258 valid 0.20730643198695234
LOSS train 0.1586075572423258 valid 0.20759766770804183
LOSS train 0.1586075572423258 valid 0.20709910298648634
LOSS train 0.1586075572423258 valid 0.20696644748871526
LOSS train 0.1586075572423258 valid 0.2069929166860187
LOSS train 0.1586075572423258 valid 0.20706295374096656
LOSS train 0.1586075572423258 valid 0.20742503546103083
LOSS train 0.1586075572423258 valid 0.20765599578619004
LOSS train 0.1586075572423258 valid 0.20775963262756272
LOSS train 0.1586075572423258 valid 0.2081629695261226
LOSS train 0.1586075572423258 valid 0.20818079357008332
LOSS train 0.1586075572423258 valid 0.20825584691304427
LOSS train 0.1586075572423258 valid 0.2081950353724616
LOSS train 0.1586075572423258 valid 0.20870458503376763
LOSS train 0.1586075572423258 valid 0.2086062283716469
LOSS train 0.1586075572423258 valid 0.2089228976379942
LOSS train 0.1586075572423258 valid 0.2091872611450493
LOSS train 0.1586075572423258 valid 0.20934018289501016
LOSS train 0.1586075572423258 valid 0.20930342966908808
LOSS train 0.1586075572423258 valid 0.20896724678043807
LOSS train 0.1586075572423258 valid 0.20917400467184794
LOSS train 0.1586075572423258 valid 0.20931746862959444
LOSS train 0.1586075572423258 valid 0.20925561889358188
LOSS train 0.1586075572423258 valid 0.20958675110134586
LOSS train 0.1586075572423258 valid 0.20971187274170738
LOSS train 0.1586075572423258 valid 0.20940869213160823
LOSS train 0.1586075572423258 valid 0.20918487413089817
LOSS train 0.1586075572423258 valid 0.20903186313807964
LOSS train 0.1586075572423258 valid 0.20872678182834437
LOSS train 0.1586075572423258 valid 0.20863158646665636
LOSS train 0.1586075572423258 valid 0.20886398561117125
LOSS train 0.1586075572423258 valid 0.20903550977668456
LOSS train 0.1586075572423258 valid 0.20889079558849336
LOSS train 0.1586075572423258 valid 0.20911009846225617
LOSS train 0.1586075572423258 valid 0.20892235120450417
LOSS train 0.1586075572423258 valid 0.20898616698104888
LOSS train 0.1586075572423258 valid 0.2091413407131683
LOSS train 0.1586075572423258 valid 0.2090843339379017
LOSS train 0.1586075572423258 valid 0.20882207621599883
LOSS train 0.1586075572423258 valid 0.208544694903222
LOSS train 0.1586075572423258 valid 0.20829560960593976
LOSS train 0.1586075572423258 valid 0.20834993782328137
LOSS train 0.1586075572423258 valid 0.20835894798790966
LOSS train 0.1586075572423258 valid 0.20839041263303337
LOSS train 0.1586075572423258 valid 0.20815225369738835
LOSS train 0.1586075572423258 valid 0.2081162899300672
LOSS train 0.1586075572423258 valid 0.20788518012427598
LOSS train 0.1586075572423258 valid 0.2080334270639079
LOSS train 0.1586075572423258 valid 0.20794552702007565
LOSS train 0.1586075572423258 valid 0.20792661628253023
LOSS train 0.1586075572423258 valid 0.2078522561521797
LOSS train 0.1586075572423258 valid 0.20785201330565745
LOSS train 0.1586075572423258 valid 0.20769207672826176
LOSS train 0.1586075572423258 valid 0.20799105961437095
LOSS train 0.1586075572423258 valid 0.2078184255329119
LOSS train 0.1586075572423258 valid 0.208676654040008
LOSS train 0.1586075572423258 valid 0.20869524086881805
LOSS train 0.1586075572423258 valid 0.2086102428038915
LOSS train 0.1586075572423258 valid 0.20879684694555423
LOSS train 0.1586075572423258 valid 0.20869922236000238
LOSS train 0.1586075572423258 valid 0.2086822163630155
LOSS train 0.1586075572423258 valid 0.20872222012900687
LOSS train 0.1586075572423258 valid 0.20865893565839336
LOSS train 0.1586075572423258 valid 0.20890273172885943
LOSS train 0.1586075572423258 valid 0.20906202485606928
LOSS train 0.1586075572423258 valid 0.20916283234388014
LOSS train 0.1586075572423258 valid 0.20918765514151855
LOSS train 0.1586075572423258 valid 0.20907839834690095
LOSS train 0.1586075572423258 valid 0.20901255300326377
LOSS train 0.1586075572423258 valid 0.20886906107634673
LOSS train 0.1586075572423258 valid 0.20866919642577142
LOSS train 0.1586075572423258 valid 0.20853648643668105
LOSS train 0.1586075572423258 valid 0.2084353666413914
LOSS train 0.1586075572423258 valid 0.20847638938800397
LOSS train 0.1586075572423258 valid 0.20872455835342407
LOSS train 0.1586075572423258 valid 0.20881904431042217
LOSS train 0.1586075572423258 valid 0.20903301635790153
LOSS train 0.1586075572423258 valid 0.20908903549699223
LOSS train 0.1586075572423258 valid 0.20906475944477215
LOSS train 0.1586075572423258 valid 0.20885489196624868
LOSS train 0.1586075572423258 valid 0.20895300237085088
LOSS train 0.1586075572423258 valid 0.20887835563599377
LOSS train 0.1586075572423258 valid 0.2087624134336199
LOSS train 0.1586075572423258 valid 0.20863669958304276
LOSS train 0.1586075572423258 valid 0.20877620232644054
LOSS train 0.1586075572423258 valid 0.2089777309238241
LOSS train 0.1586075572423258 valid 0.20886736750269735
LOSS train 0.1586075572423258 valid 0.20887660243444972
LOSS train 0.1586075572423258 valid 0.2089868106400769
LOSS train 0.1586075572423258 valid 0.20893538579508497
LOSS train 0.1586075572423258 valid 0.20888640686788193
LOSS train 0.1586075572423258 valid 0.20872297170369522
LOSS train 0.1586075572423258 valid 0.20867521650082357
LOSS train 0.1586075572423258 valid 0.20867092474814383
LOSS train 0.1586075572423258 valid 0.20842815712492735
LOSS train 0.1586075572423258 valid 0.20841607213654417
LOSS train 0.1586075572423258 valid 0.20844673164307126
LOSS train 0.1586075572423258 valid 0.20842783231484263
LOSS train 0.1586075572423258 valid 0.20839313452780558
LOSS train 0.1586075572423258 valid 0.2082690615206957
LOSS train 0.1586075572423258 valid 0.20820606550095613
LOSS train 0.1586075572423258 valid 0.20804868200697849
LOSS train 0.1586075572423258 valid 0.20779289198227419
LOSS train 0.1586075572423258 valid 0.20786064049723196
LOSS train 0.1586075572423258 valid 0.20803494342995174
LOSS train 0.1586075572423258 valid 0.20795822015615426
LOSS train 0.1586075572423258 valid 0.20799291036536346
LOSS train 0.1586075572423258 valid 0.20784431509673595
LOSS train 0.1586075572423258 valid 0.20774840804474864
LOSS train 0.1586075572423258 valid 0.20772065310785087
LOSS train 0.1586075572423258 valid 0.20774164091190095
LOSS train 0.1586075572423258 valid 0.20789073867832913
LOSS train 0.1586075572423258 valid 0.2077498092157085
LOSS train 0.1586075572423258 valid 0.20790292373270663
LOSS train 0.1586075572423258 valid 0.20779225187025208
LOSS train 0.1586075572423258 valid 0.20779890567064285
LOSS train 0.1586075572423258 valid 0.20768179248964957
LOSS train 0.1586075572423258 valid 0.20764675324871426
LOSS train 0.1586075572423258 valid 0.2075815329337007
LOSS train 0.1586075572423258 valid 0.2074230803211905
LOSS train 0.1586075572423258 valid 0.20741594908103136
LOSS train 0.1586075572423258 valid 0.20734543631845545
LOSS train 0.1586075572423258 valid 0.20726167933885442
LOSS train 0.1586075572423258 valid 0.20704398400805615
LOSS train 0.1586075572423258 valid 0.20700276667071926
LOSS train 0.1586075572423258 valid 0.2069298195729562
LOSS train 0.1586075572423258 valid 0.2070333160221849
LOSS train 0.1586075572423258 valid 0.20705712830478495
LOSS train 0.1586075572423258 valid 0.20698442758478192
LOSS train 0.1586075572423258 valid 0.20702110700779133
LOSS train 0.1586075572423258 valid 0.20712501004405084
LOSS train 0.1586075572423258 valid 0.20723270910925098
LOSS train 0.1586075572423258 valid 0.20741435653633541
LOSS train 0.1586075572423258 valid 0.2074952054999571
LOSS train 0.1586075572423258 valid 0.207659408253195
LOSS train 0.1586075572423258 valid 0.2077276620425676
LOSS train 0.1586075572423258 valid 0.2076809839650533
LOSS train 0.1586075572423258 valid 0.20771432570789172
LOSS train 0.1586075572423258 valid 0.20785541019656442
LOSS train 0.1586075572423258 valid 0.2079101588962407
LOSS train 0.1586075572423258 valid 0.20786897620673855
LOSS train 0.1586075572423258 valid 0.20803862746454713
LOSS train 0.1586075572423258 valid 0.2081463063016851
LOSS train 0.1586075572423258 valid 0.20807228212134313
LOSS train 0.1586075572423258 valid 0.2079887623399622
LOSS train 0.1586075572423258 valid 0.20793917067662007
LOSS train 0.1586075572423258 valid 0.20774529207201684
LOSS train 0.1586075572423258 valid 0.20774480948845545
LOSS train 0.1586075572423258 valid 0.20784856589255987
LOSS train 0.1586075572423258 valid 0.20770734265323512
LOSS train 0.1586075572423258 valid 0.20789840179706306
LOSS train 0.1586075572423258 valid 0.20805441606484476
LOSS train 0.1586075572423258 valid 0.20800081984120972
LOSS train 0.1586075572423258 valid 0.20792044160084996
LOSS train 0.1586075572423258 valid 0.20804830764228033
LOSS train 0.1586075572423258 valid 0.20799590028341738
LOSS train 0.1586075572423258 valid 0.2079992646194366
LOSS train 0.1586075572423258 valid 0.20798282825946807
LOSS train 0.1586075572423258 valid 0.20784432236179412
LOSS train 0.1586075572423258 valid 0.20794970725500395
LOSS train 0.1586075572423258 valid 0.20784287861448975
LOSS train 0.1586075572423258 valid 0.20780018206656448
LOSS train 0.1586075572423258 valid 0.20784353350891788
LOSS train 0.1586075572423258 valid 0.2078590837190859
LOSS train 0.1586075572423258 valid 0.20772299547371698
LOSS train 0.1586075572423258 valid 0.2078829286403434
LOSS train 0.1586075572423258 valid 0.20789384393158106
LOSS train 0.1586075572423258 valid 0.20790656839425747
LOSS train 0.1586075572423258 valid 0.2080069079252952
LOSS train 0.1586075572423258 valid 0.20805325034920497
LOSS train 0.1586075572423258 valid 0.20800265069243573
LOSS train 0.1586075572423258 valid 0.20803152352119936
LOSS train 0.1586075572423258 valid 0.20805298123719557
LOSS train 0.1586075572423258 valid 0.2080496718224726
LOSS train 0.1586075572423258 valid 0.20814608366748366
LOSS train 0.1586075572423258 valid 0.20822279110773287
LOSS train 0.1586075572423258 valid 0.20835611362661127
LOSS train 0.1586075572423258 valid 0.20832439125687988
LOSS train 0.1586075572423258 valid 0.20842980035336695
LOSS train 0.1586075572423258 valid 0.20869913845158675
LOSS train 0.1586075572423258 valid 0.2088786034466146
LOSS train 0.1586075572423258 valid 0.20898182858733366
LOSS train 0.1586075572423258 valid 0.20892469969662752
LOSS train 0.1586075572423258 valid 0.2089318552094957
LOSS train 0.1586075572423258 valid 0.20881626644719808
LOSS train 0.1586075572423258 valid 0.20864797019057993
LOSS train 0.1586075572423258 valid 0.20864048573492247
LOSS train 0.1586075572423258 valid 0.20861176310905388
LOSS train 0.1586075572423258 valid 0.20854743042450358
LOSS train 0.1586075572423258 valid 0.20830609084021115
LOSS train 0.1586075572423258 valid 0.2083998224549917
LOSS train 0.1586075572423258 valid 0.20846741032642377
LOSS train 0.1586075572423258 valid 0.2084470932943779
LOSS train 0.1586075572423258 valid 0.20844129207250955
LOSS train 0.1586075572423258 valid 0.20834086850960495
LOSS train 0.1586075572423258 valid 0.20830367169239455
LOSS train 0.1586075572423258 valid 0.20829134452507983
LOSS train 0.1586075572423258 valid 0.20829196633963748
LOSS train 0.1586075572423258 valid 0.20813700280238673
LOSS train 0.1586075572423258 valid 0.2080878501069056
LOSS train 0.1586075572423258 valid 0.20807045662565848
LOSS train 0.1586075572423258 valid 0.20805683356969534
LOSS train 0.1586075572423258 valid 0.20819336569915384
LOSS train 0.1586075572423258 valid 0.2081169404492185
LOSS train 0.1586075572423258 valid 0.2081674710166976
LOSS train 0.1586075572423258 valid 0.20816945879651397
LOSS train 0.1586075572423258 valid 0.20823093650731753
LOSS train 0.1586075572423258 valid 0.2083051917453607
LOSS train 0.1586075572423258 valid 0.20830593617057483
LOSS train 0.1586075572423258 valid 0.2082267856558427
LOSS train 0.1586075572423258 valid 0.20835433806916667
LOSS train 0.1586075572423258 valid 0.20835282535929428
LOSS train 0.1586075572423258 valid 0.20831982390802414
LOSS train 0.1586075572423258 valid 0.20833734354746888
LOSS train 0.1586075572423258 valid 0.20832560340045717
LOSS train 0.1586075572423258 valid 0.2082370299991075
LOSS train 0.1586075572423258 valid 0.20836944636982235
LOSS train 0.1586075572423258 valid 0.20835296737570916
LOSS train 0.1586075572423258 valid 0.20829976165601294
LOSS train 0.1586075572423258 valid 0.2083145939768889
LOSS train 0.1586075572423258 valid 0.20844284390298704
LOSS train 0.1586075572423258 valid 0.20856326585932142
LOSS train 0.1586075572423258 valid 0.2084876433251396
LOSS train 0.1586075572423258 valid 0.20840113567589205
LOSS train 0.1586075572423258 valid 0.20844988600124695
LOSS train 0.1586075572423258 valid 0.2084743396010039
LOSS train 0.1586075572423258 valid 0.20854779722922273
LOSS train 0.1586075572423258 valid 0.2083915331400931
LOSS train 0.1586075572423258 valid 0.20854001884519868
LOSS train 0.1586075572423258 valid 0.2085995618138254
LOSS train 0.1586075572423258 valid 0.20860169080573338
LOSS train 0.1586075572423258 valid 0.20872162425039728
LOSS train 0.1586075572423258 valid 0.20868691160128666
LOSS train 0.1586075572423258 valid 0.20889190424439366
LOSS train 0.1586075572423258 valid 0.20903140816848942
LOSS train 0.1586075572423258 valid 0.2089472773780183
LOSS train 0.1586075572423258 valid 0.2091003697150384
LOSS train 0.1586075572423258 valid 0.20905642541068972
LOSS train 0.1586075572423258 valid 0.20894368308008257
LOSS train 0.1586075572423258 valid 0.20887924605105296
LOSS train 0.1586075572423258 valid 0.2089187497491235
LOSS train 0.1586075572423258 valid 0.2090467184246657
LOSS train 0.1586075572423258 valid 0.20904221263394426
LOSS train 0.1586075572423258 valid 0.20913312987734875
LOSS train 0.1586075572423258 valid 0.20912142336722297
LOSS train 0.1586075572423258 valid 0.2090731247730509
LOSS train 0.1586075572423258 valid 0.2091494312943962
LOSS train 0.1586075572423258 valid 0.20915116216329968
LOSS train 0.1586075572423258 valid 0.20901375869152483
LOSS train 0.1586075572423258 valid 0.20899675995634312
LOSS train 0.1586075572423258 valid 0.20906674905351577
LOSS train 0.1586075572423258 valid 0.20925962734361028
LOSS train 0.1586075572423258 valid 0.20931325252505317
LOSS train 0.1586075572423258 valid 0.20934529533620513
LOSS train 0.1586075572423258 valid 0.20922262781978684
LOSS train 0.1586075572423258 valid 0.2091821756684917
LOSS train 0.1586075572423258 valid 0.20923244790907916
LOSS train 0.1586075572423258 valid 0.20916940348488944
LOSS train 0.1586075572423258 valid 0.2090790683557505
LOSS train 0.1586075572423258 valid 0.2090636835941537
LOSS train 0.1586075572423258 valid 0.20909959601275308
LOSS train 0.1586075572423258 valid 0.2091929721461851
LOSS train 0.1586075572423258 valid 0.2092887135878415
LOSS train 0.1586075572423258 valid 0.2093240325286817
LOSS train 0.1586075572423258 valid 0.20936618770370966
LOSS train 0.1586075572423258 valid 0.20923025323358993
LOSS train 0.1586075572423258 valid 0.20927592663711825
LOSS train 0.1586075572423258 valid 0.20926332763499683
LOSS train 0.1586075572423258 valid 0.20922522812338748
LOSS train 0.1586075572423258 valid 0.2093427723777887
LOSS train 0.1586075572423258 valid 0.20931205440815487
LOSS train 0.1586075572423258 valid 0.209319490980316
LOSS train 0.1586075572423258 valid 0.20930993630461497
LOSS train 0.1586075572423258 valid 0.20928314743471926
LOSS train 0.1586075572423258 valid 0.20919393534881217
LOSS train 0.1586075572423258 valid 0.20919918090752934
LOSS train 0.1586075572423258 valid 0.20926464738723063
EPOCH 4:
  batch 1 loss: 0.15726494789123535
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.1473955437541008
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.1536015123128891
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.15932424366474152
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.16551689505577089
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.16853457192579904
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.16010818843330657
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.15972048509866
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.15667246033747992
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.1553364746272564
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.15266446295109662
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.15089834791918597
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.1508498564362526
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.15017342939972878
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.15034938007593154
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.15184374758973718
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.15172149372451446
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.153630580753088
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.15252986589544698
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.15127488188445568
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.15152882571731294
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.1515720354562456
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.15201070742762607
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.15229979871461788
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.15267224937677384
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.15276376645152384
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.15318857326551719
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.15304509470505373
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.15223575588958016
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.15312619730830193
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.15336327711420675
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.15383113804273307
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.15284755713108814
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.15369348293718169
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.15499045529535838
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.15421109915607506
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.15447497911549904
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.15448897429987005
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.15468694823674667
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.1547787768766284
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.15462573181565215
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.155169599290405
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.15583085823197698
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.15507032349705696
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.1557423836655087
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.15538910322863123
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.1546743911631564
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.15442549840857586
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.1544617250257609
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.15470650315284729
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.1546754445515427
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.15490821319130751
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.15499608752862462
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.15543332640771512
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.15544253912839023
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.15525207854807377
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.15528558534488343
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.15481331636165752
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.15514497105347907
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.154898356149594
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.15505014555376084
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.15467572885174904
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.15452150290920622
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.15463540912605822
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.15457099836606245
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.15447821219762167
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.15425909432902266
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.15435673362191984
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.15467464599920355
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.1549632281064987
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.15477616833129398
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.15477862560914624
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.15471032118960604
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.15482182055711746
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.15494862536589304
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.15508206972950384
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.1549125960120907
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.15468588394996446
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.1549163653126246
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.15518950149416924
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.1553003129399853
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.15489851792411105
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.15449100077511316
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.1547913523834376
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.1547147686867153
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.15478769230634667
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.15495809493051177
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.15519185762175106
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.15518089718698116
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.15543268066313531
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.15537853891050424
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.1553265422420657
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.1556650283996777
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.1556779032850519
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.15602551565358513
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.15608253492973745
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.15600393090358713
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.15601640141436032
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.15607374763548976
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.15594986505806446
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.15601470660750227
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.155991574785873
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.15555744610943842
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.15559233725070953
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.15519305361168725
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.15525835506477445
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.1554239192437903
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.15531551458493428
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.15542249620781032
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.1557353861629963
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.15552183039285042
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.1551836394438786
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.1548718152056753
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.15504147491434164
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.1546982993250308
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.15457953910889297
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.15484944571796647
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.1547943170545465
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.15473304939370194
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.15485875283678371
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.15461291298885976
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.1547001851875274
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.15510030127153163
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.1549796602658687
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.15501345121860505
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.1551704303849311
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.15515430940417793
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.15516285854391754
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.15521625897219016
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.15525375191981977
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.15509518491857835
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.1550001320288037
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.15536404339442575
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.15537253800612777
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.15511530339717866
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.15505538245334344
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.15513311446148115
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.15502028923103775
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.15526809538011072
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.15545671390635626
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.1555381975275405
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.1555294622208031
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.1555945616918844
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.15581146141307223
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.15570028146793102
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.15572265831575002
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.15576755311213383
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.1557743424499357
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.15594052408365594
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.15596705396970112
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.15585788087734323
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.15574144905335024
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.15576261204052594
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.15559165508716138
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.15569127721171225
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.15558511114273316
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.15557115159596607
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.1555372279467462
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.15568819098502584
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.1558024269528687
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.15572554011892828
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.1556803788299914
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.15572822587621724
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.15569296170298647
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.15583360718958306
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.15583791449127427
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.15568337817035036
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.15568750546801657
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.15548317791265848
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.15554236484801068
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.1554403313332134
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.15542460835083974
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.15530354572686156
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.1552858667383934
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.15538701589618412
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.1552804712290791
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.15525248440477135
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.155125248423788
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.15531991001780474
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.15520181403391892
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.1551500747793287
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.15502407774329185
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.1548939816694442
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.15482202403085388
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.15475283736312712
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.15475579875931944
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.15473399207713132
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.15458973413927757
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.15457489917044917
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.1546219775159108
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.154597015398023
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.15451847587246448
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.15458536329510297
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.15435649911613808
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.1543987899636611
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.15446779120485393
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.15454759176461225
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.15477115107756673
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.15481430198529259
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.15491681333631277
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.15486550794460288
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.15480859935431196
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.1547140285666353
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.15478070755945703
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.15465995956484865
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.15454533540652793
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.15452675553767578
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.15438864233258826
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.1544481861320409
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.15455693173266594
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.15454686877993046
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.15465516684893169
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.15473022423859492
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.1545820555466915
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.15471596991599992
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.15457982304333537
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.15465641189967433
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.15467271376230302
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.15475868978048568
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.15481503507630393
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.15474544785815667
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.15474028700778075
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.15466187276246837
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.15474969404749572
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.15462706966532602
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.15464033044676864
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.1546479866958925
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.15445327049676785
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.15434307865310445
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.15448864701649417
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.15459607970533948
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.154583641520605
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.15454319192999935
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.15450525465301979
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.15452722948282324
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.1544436041139445
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.15443328654841532
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.15444245875006965
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.1544050390064467
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.1543942947871983
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.15450251396142595
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.15442097547256256
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.15442176282773784
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.15442980065575387
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.15444943907917763
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.15445914625273488
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.15434407503136738
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.15443876088266412
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.15449090263091894
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.1545290597975254
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.15463801752404863
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.1545486513761774
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.15454069519113647
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.1545089597366457
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.15455589420070834
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.1545114416803699
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.15441575231835072
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.15432347568084104
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.15435413069821693
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.15435625466589745
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.15425363682581547
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.15417183648425203
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.15417830473229913
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.15402721667267155
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.15404078479645386
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.1540325105022219
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.15402677937058473
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.1540716334120996
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.1539909439957718
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.15404152602509216
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.15403175120314108
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.15398286033750458
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.15416543226648163
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.15406175482555898
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.15395983292297882
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.15415196512164414
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.15413683838469888
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.15409290053539995
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.1541176751576444
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.15419352770383868
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.15412936439187502
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.15408080581126485
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.15411852629256334
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.15413661114871502
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.15417778306362923
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.15420207363012786
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.15415486716746454
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.1541895418583105
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.15410301793095973
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.15416692600681864
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.15415630172105999
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.15412839683257554
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.15421672762965993
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.1543019749224186
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.1543137742553727
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.15436205950037046
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.15443666648082058
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.15432880971355725
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.15422730382070893
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.15420546437303226
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.15429955463472791
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.15426552226606582
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.15424548005900368
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.15422282867917889
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.15424304384677137
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.15425118156313117
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.1542547769383421
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.15422986220423276
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.15420419503377095
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.1541236213618709
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.1541308142077118
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.15410029362791625
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.15415721279554093
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.15412865925556535
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.15413641754596952
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.15424042765664148
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.1542169992291965
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.15418475538304766
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.1541520852747382
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.15415272107347847
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.15411137411156176
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.15408268732869107
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.15411770445274495
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.15406608130828833
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.15408765123440668
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.15399519244403195
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.15389076328696824
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.15393213593832603
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.15389007314088496
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.1539591504994667
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.15401827458438555
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.15392877612577147
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.15381777076212852
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.15381479513145493
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.15393756045334375
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.15393893589221297
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.15384490058400865
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.15378473787265418
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.15368028125755906
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.15366998551522984
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.1535778135891534
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.1537532990482467
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.15374531179840292
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.15372925002647694
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.15373358439276183
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.15373768388277534
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.1537136610930179
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.15371351759752322
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.15366101343942576
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.15360452458262444
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.15360123918861404
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.153618075101721
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.15365448912800878
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.15368457864654267
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.1537102576082861
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.15366687472867832
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.15372359341516548
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.1538122059704538
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.1538062239050201
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.15371709004458453
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.15367272324922013
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.15371048493691572
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.1536767228042127
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.15361049490888695
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.15363496597090812
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.15358835737643345
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.15361976101791502
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.15367971601612543
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.15366189999551308
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.15358483656435398
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.1535707342455972
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.15352579440561034
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.15348652170986016
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.15351858454193662
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.15354447156190873
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.15363041800942193
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.15356429199878355
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.1536128009477305
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.1535139496887894
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.15343488584223547
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.1533936414271202
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.15333696825342027
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.153404647119674
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.1533903149732699
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.1534026361905135
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.15349306085103534
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.15348938450178742
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.1535106186476565
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.15354073553281455
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.15349368441563385
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.15349342519669887
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.15349256631215008
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.1536355331805523
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.15367168225915298
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.15368554026265688
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.15367646511606495
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.15359244979824166
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.15367511141315177
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.1536060097186189
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.15359263261780143
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.15360021760264536
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.15354324793860094
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.15347294917991083
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.1534658545572864
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.15347248339726602
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.15344722629547705
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.15341671223063433
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.15352717865550636
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.1535410325254671
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.15356642827391626
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.1535500367228712
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.15359489274328775
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.15358018043471305
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.15364851610024194
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.15367275086871113
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.1536876370306485
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.15368940995680055
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.15366879118044982
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.15379613828189617
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.15377077806208814
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.1538735455932923
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.15392512060101562
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.1538514467066343
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.15382058475658578
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.15382371096050038
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.1537746463163358
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.1537665102534886
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.1537317451890384
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.15364808472858998
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.15359312285517537
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.15360820760168223
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.15356523591887067
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.1535140864774757
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.15354388038958272
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.15348951621302243
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.15357963323866555
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.15362779860911163
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.1535844584782374
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.15351336766755663
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.15347465225918727
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.1534571891604097
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.15343224995546212
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.15342034560966708
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.15339693158596485
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.15341362156225055
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.15343016028537879
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.15341890068262215
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.1534899747930467
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.15348819853999301
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.15342484997378456
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.15343583604845398
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.15343750126462066
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.15338292474515128
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.15336609510335628
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.15332336124483045
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.15339176235883906
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.1533716732485289
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.1533854058801347
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.153399316865894
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.15347768320985464
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.15344089722167903
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.15340057541152616
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.1533178791049747
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.1533359795294959
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.1533656826583288
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.15332182234576844
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.1533024534742633
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.15337832797414216
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.15341447929202368
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.15346221901634907
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.15345455604880226
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.15356611482546492
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.15356611482546492 valid 0.22399136424064636
LOSS train 0.15356611482546492 valid 0.19958537071943283
LOSS train 0.15356611482546492 valid 0.20139046013355255
LOSS train 0.15356611482546492 valid 0.1908152513206005
LOSS train 0.15356611482546492 valid 0.18381452560424805
LOSS train 0.15356611482546492 valid 0.19503548741340637
LOSS train 0.15356611482546492 valid 0.2022478176014764
LOSS train 0.15356611482546492 valid 0.2017994448542595
LOSS train 0.15356611482546492 valid 0.20153102940983242
LOSS train 0.15356611482546492 valid 0.2036408707499504
LOSS train 0.15356611482546492 valid 0.20212773572314868
LOSS train 0.15356611482546492 valid 0.20207051063577333
LOSS train 0.15356611482546492 valid 0.20157826176056495
LOSS train 0.15356611482546492 valid 0.20046811870166234
LOSS train 0.15356611482546492 valid 0.19867306351661682
LOSS train 0.15356611482546492 valid 0.20012232195585966
LOSS train 0.15356611482546492 valid 0.2010768003323499
LOSS train 0.15356611482546492 valid 0.20066028419468138
LOSS train 0.15356611482546492 valid 0.20324905530402534
LOSS train 0.15356611482546492 valid 0.20361944213509559
LOSS train 0.15356611482546492 valid 0.20141448932034628
LOSS train 0.15356611482546492 valid 0.20005090941082349
LOSS train 0.15356611482546492 valid 0.19943438535151276
LOSS train 0.15356611482546492 valid 0.1993613870193561
LOSS train 0.15356611482546492 valid 0.19840299665927888
LOSS train 0.15356611482546492 valid 0.19892620753783447
LOSS train 0.15356611482546492 valid 0.19854018533671344
LOSS train 0.15356611482546492 valid 0.19836820662021637
LOSS train 0.15356611482546492 valid 0.1988306672408663
LOSS train 0.15356611482546492 valid 0.19999950975179673
LOSS train 0.15356611482546492 valid 0.2007414347702457
LOSS train 0.15356611482546492 valid 0.2003680276684463
LOSS train 0.15356611482546492 valid 0.20060314870241916
LOSS train 0.15356611482546492 valid 0.20070847123861313
LOSS train 0.15356611482546492 valid 0.20255635678768158
LOSS train 0.15356611482546492 valid 0.20246919203135702
LOSS train 0.15356611482546492 valid 0.20290599803666812
LOSS train 0.15356611482546492 valid 0.20372390119653
LOSS train 0.15356611482546492 valid 0.20239733121333978
LOSS train 0.15356611482546492 valid 0.2021491676568985
LOSS train 0.15356611482546492 valid 0.20271652428115286
LOSS train 0.15356611482546492 valid 0.20321647645462126
LOSS train 0.15356611482546492 valid 0.20330676089885624
LOSS train 0.15356611482546492 valid 0.20424244383519347
LOSS train 0.15356611482546492 valid 0.20398958292272357
LOSS train 0.15356611482546492 valid 0.20462105228849078
LOSS train 0.15356611482546492 valid 0.20486126713296202
LOSS train 0.15356611482546492 valid 0.20452678576111794
LOSS train 0.15356611482546492 valid 0.20521414705685206
LOSS train 0.15356611482546492 valid 0.2046415004134178
LOSS train 0.15356611482546492 valid 0.20490608203644847
LOSS train 0.15356611482546492 valid 0.204714085906744
LOSS train 0.15356611482546492 valid 0.20528710000919845
LOSS train 0.15356611482546492 valid 0.20500552985403273
LOSS train 0.15356611482546492 valid 0.20505068302154542
LOSS train 0.15356611482546492 valid 0.20467097445258073
LOSS train 0.15356611482546492 valid 0.20482907091316424
LOSS train 0.15356611482546492 valid 0.20457005269568543
LOSS train 0.15356611482546492 valid 0.20500975720963235
LOSS train 0.15356611482546492 valid 0.2047035368780295
LOSS train 0.15356611482546492 valid 0.20475047826766968
LOSS train 0.15356611482546492 valid 0.20535506860863778
LOSS train 0.15356611482546492 valid 0.2051392819673296
LOSS train 0.15356611482546492 valid 0.2057790921535343
LOSS train 0.15356611482546492 valid 0.20615430543055902
LOSS train 0.15356611482546492 valid 0.20618870267362305
LOSS train 0.15356611482546492 valid 0.20600690614821307
LOSS train 0.15356611482546492 valid 0.2058887799434802
LOSS train 0.15356611482546492 valid 0.20525774489278378
LOSS train 0.15356611482546492 valid 0.2057270511984825
LOSS train 0.15356611482546492 valid 0.20563361237586383
LOSS train 0.15356611482546492 valid 0.20595785623623264
LOSS train 0.15356611482546492 valid 0.2062364696231607
LOSS train 0.15356611482546492 valid 0.20605847098537394
LOSS train 0.15356611482546492 valid 0.20603862802187603
LOSS train 0.15356611482546492 valid 0.20664054978834956
LOSS train 0.15356611482546492 valid 0.20657659912264192
LOSS train 0.15356611482546492 valid 0.20645346836401865
LOSS train 0.15356611482546492 valid 0.20600566814971877
LOSS train 0.15356611482546492 valid 0.2053851429373026
LOSS train 0.15356611482546492 valid 0.20484710806681786
LOSS train 0.15356611482546492 valid 0.20510179385906313
LOSS train 0.15356611482546492 valid 0.20476545806390695
LOSS train 0.15356611482546492 valid 0.20452198900637172
LOSS train 0.15356611482546492 valid 0.20400137638344484
LOSS train 0.15356611482546492 valid 0.20350735322680585
LOSS train 0.15356611482546492 valid 0.20349865518082147
LOSS train 0.15356611482546492 valid 0.20324389975179324
LOSS train 0.15356611482546492 valid 0.20344130879037836
LOSS train 0.15356611482546492 valid 0.20368019375536178
LOSS train 0.15356611482546492 valid 0.2040404261468531
LOSS train 0.15356611482546492 valid 0.20395927163569824
LOSS train 0.15356611482546492 valid 0.2037998245608422
LOSS train 0.15356611482546492 valid 0.20404584769238818
LOSS train 0.15356611482546492 valid 0.20351619375379462
LOSS train 0.15356611482546492 valid 0.20340664560596147
LOSS train 0.15356611482546492 valid 0.2034345642500317
LOSS train 0.15356611482546492 valid 0.2035082896449128
LOSS train 0.15356611482546492 valid 0.20388071103529495
LOSS train 0.15356611482546492 valid 0.2041284266114235
LOSS train 0.15356611482546492 valid 0.20427420705851942
LOSS train 0.15356611482546492 valid 0.20469901944492377
LOSS train 0.15356611482546492 valid 0.20468973303304136
LOSS train 0.15356611482546492 valid 0.2047993431870754
LOSS train 0.15356611482546492 valid 0.20477420276119596
LOSS train 0.15356611482546492 valid 0.20530615704801847
LOSS train 0.15356611482546492 valid 0.2051958937511266
LOSS train 0.15356611482546492 valid 0.20551111493949537
LOSS train 0.15356611482546492 valid 0.20583460609847254
LOSS train 0.15356611482546492 valid 0.20597714375365864
LOSS train 0.15356611482546492 valid 0.2059346132718765
LOSS train 0.15356611482546492 valid 0.20557555622820342
LOSS train 0.15356611482546492 valid 0.20571016065314807
LOSS train 0.15356611482546492 valid 0.20586634950156799
LOSS train 0.15356611482546492 valid 0.20584255599457285
LOSS train 0.15356611482546492 valid 0.2062390769093201
LOSS train 0.15356611482546492 valid 0.20636088751320147
LOSS train 0.15356611482546492 valid 0.20601522846747253
LOSS train 0.15356611482546492 valid 0.2057773593594046
LOSS train 0.15356611482546492 valid 0.2055589223901431
LOSS train 0.15356611482546492 valid 0.20522979616133635
LOSS train 0.15356611482546492 valid 0.20511508794104466
LOSS train 0.15356611482546492 valid 0.2053231423705574
LOSS train 0.15356611482546492 valid 0.2054682550651412
LOSS train 0.15356611482546492 valid 0.2053481664657593
LOSS train 0.15356611482546492 valid 0.20560507417198212
LOSS train 0.15356611482546492 valid 0.2054160290111707
LOSS train 0.15356611482546492 valid 0.20545539923477918
LOSS train 0.15356611482546492 valid 0.20559307401494462
LOSS train 0.15356611482546492 valid 0.2055058560692347
LOSS train 0.15356611482546492 valid 0.20522215518787618
LOSS train 0.15356611482546492 valid 0.20498051674980106
LOSS train 0.15356611482546492 valid 0.2047248470379894
LOSS train 0.15356611482546492 valid 0.204783534380927
LOSS train 0.15356611482546492 valid 0.20480253199736279
LOSS train 0.15356611482546492 valid 0.2048452629105133
LOSS train 0.15356611482546492 valid 0.20462542391606492
LOSS train 0.15356611482546492 valid 0.20459777635076773
LOSS train 0.15356611482546492 valid 0.20437779947579335
LOSS train 0.15356611482546492 valid 0.2044879944196769
LOSS train 0.15356611482546492 valid 0.2044010691820307
LOSS train 0.15356611482546492 valid 0.20438654374488643
LOSS train 0.15356611482546492 valid 0.20429473198377168
LOSS train 0.15356611482546492 valid 0.2042660376884871
LOSS train 0.15356611482546492 valid 0.20409423308125857
LOSS train 0.15356611482546492 valid 0.2043933510167958
LOSS train 0.15356611482546492 valid 0.20418115527856917
LOSS train 0.15356611482546492 valid 0.20505545942767248
LOSS train 0.15356611482546492 valid 0.20506804481448743
LOSS train 0.15356611482546492 valid 0.20496964395046235
LOSS train 0.15356611482546492 valid 0.2051529526118411
LOSS train 0.15356611482546492 valid 0.20505282018137605
LOSS train 0.15356611482546492 valid 0.20503322916482788
LOSS train 0.15356611482546492 valid 0.20508379653676764
LOSS train 0.15356611482546492 valid 0.2050366631438655
LOSS train 0.15356611482546492 valid 0.20527567494756135
LOSS train 0.15356611482546492 valid 0.20541239971188224
LOSS train 0.15356611482546492 valid 0.20550470186185232
LOSS train 0.15356611482546492 valid 0.2055406778488519
LOSS train 0.15356611482546492 valid 0.2054092099890113
LOSS train 0.15356611482546492 valid 0.20531939719774708
LOSS train 0.15356611482546492 valid 0.2051690795723303
LOSS train 0.15356611482546492 valid 0.20498518517777964
LOSS train 0.15356611482546492 valid 0.204827272855654
LOSS train 0.15356611482546492 valid 0.20472367026589133
LOSS train 0.15356611482546492 valid 0.20478917191545648
LOSS train 0.15356611482546492 valid 0.20505533023865638
LOSS train 0.15356611482546492 valid 0.20515289609985693
LOSS train 0.15356611482546492 valid 0.20534314947015436
LOSS train 0.15356611482546492 valid 0.20537941105225507
LOSS train 0.15356611482546492 valid 0.20534193419922164
LOSS train 0.15356611482546492 valid 0.20514974699810493
LOSS train 0.15356611482546492 valid 0.2052373866297606
LOSS train 0.15356611482546492 valid 0.20518400785566746
LOSS train 0.15356611482546492 valid 0.2050346976518631
LOSS train 0.15356611482546492 valid 0.20491711820729755
LOSS train 0.15356611482546492 valid 0.20505608441465992
LOSS train 0.15356611482546492 valid 0.20523926758029487
LOSS train 0.15356611482546492 valid 0.2051180670214765
LOSS train 0.15356611482546492 valid 0.20510108346740405
LOSS train 0.15356611482546492 valid 0.20522016970162893
LOSS train 0.15356611482546492 valid 0.20516932370898489
LOSS train 0.15356611482546492 valid 0.20513748870195586
LOSS train 0.15356611482546492 valid 0.20499746639119543
LOSS train 0.15356611482546492 valid 0.20492509965961045
LOSS train 0.15356611482546492 valid 0.20494137183632902
LOSS train 0.15356611482546492 valid 0.2047118442581299
LOSS train 0.15356611482546492 valid 0.2047136349880949
LOSS train 0.15356611482546492 valid 0.2047110184159859
LOSS train 0.15356611482546492 valid 0.20471514086974293
LOSS train 0.15356611482546492 valid 0.20467686809170307
LOSS train 0.15356611482546492 valid 0.20457692490890622
LOSS train 0.15356611482546492 valid 0.20449653870083506
LOSS train 0.15356611482546492 valid 0.20434587272172122
LOSS train 0.15356611482546492 valid 0.20408547375446712
LOSS train 0.15356611482546492 valid 0.20412794424563038
LOSS train 0.15356611482546492 valid 0.20428388138410403
LOSS train 0.15356611482546492 valid 0.20422366863549357
LOSS train 0.15356611482546492 valid 0.20427169077959492
LOSS train 0.15356611482546492 valid 0.20411139763891697
LOSS train 0.15356611482546492 valid 0.2040201754712347
LOSS train 0.15356611482546492 valid 0.2040199062021652
LOSS train 0.15356611482546492 valid 0.20405752516438808
LOSS train 0.15356611482546492 valid 0.204234642579275
LOSS train 0.15356611482546492 valid 0.20406584274478076
LOSS train 0.15356611482546492 valid 0.2042090969467626
LOSS train 0.15356611482546492 valid 0.2041185570223896
LOSS train 0.15356611482546492 valid 0.20411422032003218
LOSS train 0.15356611482546492 valid 0.20400408680359142
LOSS train 0.15356611482546492 valid 0.20396354986088616
LOSS train 0.15356611482546492 valid 0.203866289054613
LOSS train 0.15356611482546492 valid 0.2037106417119503
LOSS train 0.15356611482546492 valid 0.20370538098991198
LOSS train 0.15356611482546492 valid 0.2036469848217251
LOSS train 0.15356611482546492 valid 0.20355913008368293
LOSS train 0.15356611482546492 valid 0.20333282532239402
LOSS train 0.15356611482546492 valid 0.2032721900308187
LOSS train 0.15356611482546492 valid 0.20318426219148372
LOSS train 0.15356611482546492 valid 0.2032897781291509
LOSS train 0.15356611482546492 valid 0.20328973944891582
LOSS train 0.15356611482546492 valid 0.20323264349369982
LOSS train 0.15356611482546492 valid 0.20327464450855512
LOSS train 0.15356611482546492 valid 0.2033897984455519
LOSS train 0.15356611482546492 valid 0.2035031599391784
LOSS train 0.15356611482546492 valid 0.20369989732901256
LOSS train 0.15356611482546492 valid 0.20378108553390586
LOSS train 0.15356611482546492 valid 0.20395982173570978
LOSS train 0.15356611482546492 valid 0.20403774945359482
LOSS train 0.15356611482546492 valid 0.2039964259581795
LOSS train 0.15356611482546492 valid 0.20407767438370247
LOSS train 0.15356611482546492 valid 0.2042082923573333
LOSS train 0.15356611482546492 valid 0.20425248839731874
LOSS train 0.15356611482546492 valid 0.20421283708109877
LOSS train 0.15356611482546492 valid 0.20435242603222528
LOSS train 0.15356611482546492 valid 0.20443452140118212
LOSS train 0.15356611482546492 valid 0.2043448291971522
LOSS train 0.15356611482546492 valid 0.20425707875173302
LOSS train 0.15356611482546492 valid 0.20422324857541493
LOSS train 0.15356611482546492 valid 0.20402650397941158
LOSS train 0.15356611482546492 valid 0.20402164869010447
LOSS train 0.15356611482546492 valid 0.20415892430360882
LOSS train 0.15356611482546492 valid 0.20401091362572898
LOSS train 0.15356611482546492 valid 0.2042102041681117
LOSS train 0.15356611482546492 valid 0.2043826588597454
LOSS train 0.15356611482546492 valid 0.20433332974813423
LOSS train 0.15356611482546492 valid 0.20422712295520595
LOSS train 0.15356611482546492 valid 0.20434841284385094
LOSS train 0.15356611482546492 valid 0.20430524442945758
LOSS train 0.15356611482546492 valid 0.20427967805460276
LOSS train 0.15356611482546492 valid 0.20426206225156784
LOSS train 0.15356611482546492 valid 0.2041105066400125
LOSS train 0.15356611482546492 valid 0.2042172152016844
LOSS train 0.15356611482546492 valid 0.20411156942486292
LOSS train 0.15356611482546492 valid 0.20405708523247185
LOSS train 0.15356611482546492 valid 0.20410395328905068
LOSS train 0.15356611482546492 valid 0.204107420577202
LOSS train 0.15356611482546492 valid 0.2039546042912665
LOSS train 0.15356611482546492 valid 0.20413354922865712
LOSS train 0.15356611482546492 valid 0.20413773716876865
LOSS train 0.15356611482546492 valid 0.20412982421425674
LOSS train 0.15356611482546492 valid 0.20424630170352615
LOSS train 0.15356611482546492 valid 0.20430676149957963
LOSS train 0.15356611482546492 valid 0.20424151063645293
LOSS train 0.15356611482546492 valid 0.20426257671506115
LOSS train 0.15356611482546492 valid 0.20426423442813585
LOSS train 0.15356611482546492 valid 0.20424036967351025
LOSS train 0.15356611482546492 valid 0.20436714589595795
LOSS train 0.15356611482546492 valid 0.20445646795986303
LOSS train 0.15356611482546492 valid 0.20459855250931142
LOSS train 0.15356611482546492 valid 0.20455918439008572
LOSS train 0.15356611482546492 valid 0.20466831287994597
LOSS train 0.15356611482546492 valid 0.2049483759328723
LOSS train 0.15356611482546492 valid 0.20512102020310832
LOSS train 0.15356611482546492 valid 0.20521487197736754
LOSS train 0.15356611482546492 valid 0.2051398945938457
LOSS train 0.15356611482546492 valid 0.20515814779893213
LOSS train 0.15356611482546492 valid 0.20506752258173394
LOSS train 0.15356611482546492 valid 0.20491864382148647
LOSS train 0.15356611482546492 valid 0.20491590652628183
LOSS train 0.15356611482546492 valid 0.20489053050322192
LOSS train 0.15356611482546492 valid 0.20482220222305147
LOSS train 0.15356611482546492 valid 0.20456304781614465
LOSS train 0.15356611482546492 valid 0.20465773812663007
LOSS train 0.15356611482546492 valid 0.20475021272268093
LOSS train 0.15356611482546492 valid 0.204727318464664
LOSS train 0.15356611482546492 valid 0.20469490506432272
LOSS train 0.15356611482546492 valid 0.2046026654982816
LOSS train 0.15356611482546492 valid 0.20457891395522487
LOSS train 0.15356611482546492 valid 0.20457674846517174
LOSS train 0.15356611482546492 valid 0.20456442853500104
LOSS train 0.15356611482546492 valid 0.20439928165825783
LOSS train 0.15356611482546492 valid 0.20435036662710857
LOSS train 0.15356611482546492 valid 0.20435260302377642
LOSS train 0.15356611482546492 valid 0.20436383697654115
LOSS train 0.15356611482546492 valid 0.20447277512590764
LOSS train 0.15356611482546492 valid 0.20438458473497145
LOSS train 0.15356611482546492 valid 0.2044277261904996
LOSS train 0.15356611482546492 valid 0.20440857677451715
LOSS train 0.15356611482546492 valid 0.2044436041029002
LOSS train 0.15356611482546492 valid 0.20452192187309265
LOSS train 0.15356611482546492 valid 0.2045091330213008
LOSS train 0.15356611482546492 valid 0.20443690331369047
LOSS train 0.15356611482546492 valid 0.20455419564797933
LOSS train 0.15356611482546492 valid 0.20454505644738674
LOSS train 0.15356611482546492 valid 0.20452427219171992
LOSS train 0.15356611482546492 valid 0.20453634619518044
LOSS train 0.15356611482546492 valid 0.20452214622148085
LOSS train 0.15356611482546492 valid 0.20442756260563802
LOSS train 0.15356611482546492 valid 0.2045531288802045
LOSS train 0.15356611482546492 valid 0.20454067400386255
LOSS train 0.15356611482546492 valid 0.20446716852701746
LOSS train 0.15356611482546492 valid 0.20447443439983404
LOSS train 0.15356611482546492 valid 0.20458944103778742
LOSS train 0.15356611482546492 valid 0.2046808577646875
LOSS train 0.15356611482546492 valid 0.20462441477510665
LOSS train 0.15356611482546492 valid 0.20455879821807524
LOSS train 0.15356611482546492 valid 0.20461386667817172
LOSS train 0.15356611482546492 valid 0.20463447849143226
LOSS train 0.15356611482546492 valid 0.20472372078036066
LOSS train 0.15356611482546492 valid 0.2045944537501782
LOSS train 0.15356611482546492 valid 0.20475042112334124
LOSS train 0.15356611482546492 valid 0.2048216389943354
LOSS train 0.15356611482546492 valid 0.2047964146262721
LOSS train 0.15356611482546492 valid 0.2049048877792594
LOSS train 0.15356611482546492 valid 0.20486169934272766
LOSS train 0.15356611482546492 valid 0.20506637246330822
LOSS train 0.15356611482546492 valid 0.2051983848441996
LOSS train 0.15356611482546492 valid 0.2050909647581781
LOSS train 0.15356611482546492 valid 0.20523775742590247
LOSS train 0.15356611482546492 valid 0.20521082991000378
LOSS train 0.15356611482546492 valid 0.20509852455461852
LOSS train 0.15356611482546492 valid 0.20503148297408977
LOSS train 0.15356611482546492 valid 0.20506028616213584
LOSS train 0.15356611482546492 valid 0.20517399327126806
LOSS train 0.15356611482546492 valid 0.20517036990443271
LOSS train 0.15356611482546492 valid 0.20526223305967592
LOSS train 0.15356611482546492 valid 0.20526356513139402
LOSS train 0.15356611482546492 valid 0.20522147703629273
LOSS train 0.15356611482546492 valid 0.20530483183783416
LOSS train 0.15356611482546492 valid 0.20531755479819636
LOSS train 0.15356611482546492 valid 0.20516247758942266
LOSS train 0.15356611482546492 valid 0.20514952792235983
LOSS train 0.15356611482546492 valid 0.20523145790524108
LOSS train 0.15356611482546492 valid 0.20544197362696015
LOSS train 0.15356611482546492 valid 0.2055051478354827
LOSS train 0.15356611482546492 valid 0.20552160074083792
LOSS train 0.15356611482546492 valid 0.20540957178609173
LOSS train 0.15356611482546492 valid 0.20537679925047117
LOSS train 0.15356611482546492 valid 0.20542219857602542
LOSS train 0.15356611482546492 valid 0.20535319809402738
LOSS train 0.15356611482546492 valid 0.2052651444646368
LOSS train 0.15356611482546492 valid 0.20524190992794253
LOSS train 0.15356611482546492 valid 0.20527628756447486
LOSS train 0.15356611482546492 valid 0.20538188933821047
LOSS train 0.15356611482546492 valid 0.20547049276425805
LOSS train 0.15356611482546492 valid 0.20550678925735227
LOSS train 0.15356611482546492 valid 0.20556513180418842
LOSS train 0.15356611482546492 valid 0.2054357349289862
LOSS train 0.15356611482546492 valid 0.205467515544639
LOSS train 0.15356611482546492 valid 0.2054391126251883
LOSS train 0.15356611482546492 valid 0.20540129089949863
LOSS train 0.15356611482546492 valid 0.20551432846165493
LOSS train 0.15356611482546492 valid 0.20548103134001583
LOSS train 0.15356611482546492 valid 0.20548523651374564
LOSS train 0.15356611482546492 valid 0.20549086258835988
LOSS train 0.15356611482546492 valid 0.20546926220095224
LOSS train 0.15356611482546492 valid 0.20537902142761189
LOSS train 0.15356611482546492 valid 0.20538833073299864
LOSS train 0.15356611482546492 valid 0.20546174154372074
EPOCH 5:
  batch 1 loss: 0.1430937796831131
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.13916292041540146
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.1444904406865438
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.15003183484077454
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.15751054883003235
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.1612971027692159
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.15362609497138432
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.1530772540718317
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.15034368799792397
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.14871501177549362
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.14608640020543878
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.1445778285463651
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.14470464220413795
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.14424420786755426
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.14449726541837057
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.1457521179690957
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.1457609525498222
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.1476736788948377
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.14701621234416962
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.1457565002143383
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.14613951600733258
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.14649736339395697
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.14681429966636325
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.1471104882657528
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.14716628909111024
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.14747719810559198
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.1481930641112504
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.14827891385981015
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.14741303643275952
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.1483229622244835
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.1484368054136153
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.14900895161554217
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.14816368529290863
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.14908748996608398
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.15044813539300647
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.14973815178705585
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.14979511237627752
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.14982845536188075
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.1500898777292325
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.1501357899978757
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.1499214134201771
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.15043665557390168
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.15114419491485107
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.15049618601121686
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.15121168146530786
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.15093739444146986
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.15036994044451005
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.1500473623163998
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.15009234374275013
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.15032773122191428
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.15034983511648925
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.15062393663594356
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.15069728597717466
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.15093453435434234
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.15084757953882216
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.15068320916699512
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.15061082586384655
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.15012439712882042
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.15050835894831158
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.15027197810510795
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.15038766638665904
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.15010130008862865
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.14991315879992076
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.1500044750282541
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.14993653767384016
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.1498913330336412
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.14963287659990254
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.14977138588095412
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.15006880343392276
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.15031491358365331
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.15009654817027104
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.15006396267563105
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.15006622886412765
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.15020101064363042
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.1502391212185224
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.15041291194134637
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.15024308695808633
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.14996360366543135
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.15011632036936434
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.1503930519334972
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.15063210135256802
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.15020973998598935
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.14989188789244157
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.15021400428598836
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.1502425359452472
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.15025859074883682
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.15046052437746663
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.1506717815487222
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.15069058361683
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.15092883797155487
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.15083505319697516
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.15072064271763616
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.1509769410696081
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.15108671887440883
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.15144288688898086
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.1515823573960612
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.15154480865013967
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.15152264865381376
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.15160591516530875
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.15155507452785968
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.15158537996582466
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.15158397185744024
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.1511492734250513
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.15123171008263642
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.15084144473075867
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.15096112734304284
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.15117249547321104
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.1510189204580254
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.15109708317376058
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.15144682296297768
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.15121219273623046
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.15090358576604299
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.1506289330731451
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.15082273776071115
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.15050771560357964
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.1504133003282136
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.15072591106096903
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.15066398351879443
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.1506414418460942
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.15082371955116589
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.15060594271529804
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.15066274744076807
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.15112979492036308
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.1510410818361467
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.1511121256351471
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.15124525555542537
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.15123801297090184
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.15129744494333863
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.15134641920873362
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.15143258273601531
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.15128356184213215
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.15117257698015732
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.15153361307947258
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.1515225754744971
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.15125725556302955
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.15126095777925322
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.1514056392394713
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.15131179718435675
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.15153142284575127
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.15169720969029835
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.15181289095405145
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.15180696242711914
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.15192583724335357
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.15212923350433508
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.1520653464670839
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.1521060732129502
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.15215981564148753
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.15216745315371333
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.1523481486427704
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.15238260795672734
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.15225284808124137
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.15214545564039758
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.1522042837018281
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.15199911971757937
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.15205681391300693
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.15193112949148202
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.1518836649739818
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.15184905342286145
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.15193026967393528
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.1520198774524033
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.15195204326825112
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.15189347554136207
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.15193708999756655
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.15188181082286487
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.15204214896216536
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.15205538883266678
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.15188857712253126
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.15186409328487657
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.1516519776110113
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.1517134452567381
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.15163225214383755
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.15163298349740895
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.15151467927963058
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.1514763963119737
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.15159220116479055
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.15149632502685895
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.15145681983670273
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.15133466136254622
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.15153678510775112
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.15140072935157353
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.1513545327917647
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.1512114555462376
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.15106188053967523
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.15100782923400402
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.150938593938544
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.150915217736075
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.15089420942380466
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.15075471426578277
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.15073701108574236
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.15081277277908828
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.1508116403019241
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.15071146104795238
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.15078888825801987
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.15057289170235702
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.15065834804987296
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.15074229202404313
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.15084353944069237
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.15102909091446134
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.1510321641687173
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.15113739386200906
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.15112005544835655
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.1511313848123692
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.1510305528717088
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.15111262877197826
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.15098498638083296
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.150890819583703
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.15082635438960532
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.15071487405265754
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.15078982539724506
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.150901508189383
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.1508972623619423
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.1509869594900113
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.1510581974412354
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.15089042018228602
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.1510088626035424
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.15088354092505243
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.15095936551621433
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.15097634432786103
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.1510656567877286
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.15110098800875923
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.15102828277182256
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.1510227443398656
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.15095931745965385
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.15106038662738033
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.1509648682673772
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.15098433077862833
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.1510254043577001
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.15084299634684595
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.150756476227373
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.15089418246694233
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.1509924376810784
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.15096769764505583
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.1509221485244358
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.1508789889705487
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.15090393684011824
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.15082231189234782
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.15083414234441042
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.15082180831863098
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.15083032472363078
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.15083105849723022
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.1509603785034037
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.15091201502922152
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.150889402370394
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.15087731836027787
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.15089200193784674
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.15089490355514898
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.15078411746000955
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.15089278653143876
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.1509245047308355
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.1509574827849865
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.15108945381000222
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.15098211574294265
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.1509895643638999
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.15096991324401277
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.15100602954626083
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.15095827789627947
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.15086918776485242
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.15077646662907082
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.15080243950299774
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.15081285814253184
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.15068268884415828
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.15060713995276517
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.15061184996207858
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.15047999138407636
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.15050753371895484
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.15049377302254052
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.1504758922906404
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.15052329693267594
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.15046274401662962
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.15048591135828585
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.1504609194958782
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.15044182654031935
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.15062780994853694
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.15050093658322836
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.15041213872757825
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.150626205690745
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.15061610708490605
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.1505598559064402
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.15058348407988909
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.15064378101378678
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.1505909250565271
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.15055095116403086
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.15058284826813653
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.15060913234844175
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.15066382343831816
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.15068947708794286
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.15065123408202094
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.1506977026195576
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.1506156861318024
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.1506694629017649
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.1506462975619585
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.15061727133005448
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.15071056195798588
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.15080625905978437
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.15083193943157033
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.15089713578187935
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.15094170112300803
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.1508297874223466
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.1507346905194796
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.15071709503730138
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.1508320835341647
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.15079357445437386
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.15077401850089775
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.1507500997303348
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.15077634732254216
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.15079207353147805
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.1507959451756959
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.15076394446871497
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.15074190010337768
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.15065484760749726
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.15068713427069103
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.1506502202544839
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.15070319825563186
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.1506696992856302
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.15068009171694044
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.15081250353987458
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.15078900286733915
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.15075687579107735
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.15074391287042055
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.15075217753183096
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.15072042996359764
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.1506851549890841
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.1507176315590693
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.15066054373703622
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.15069420880996265
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.15060746178396642
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.15050728268215052
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.15056541798318304
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.15050429092170983
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.1505545917785529
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.15061936498228518
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.15053254929202867
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.15041475262011852
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.15041891271304228
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.15053408212626157
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.15053736808754148
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.15044651525399097
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.15038524433617761
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.15027136298495408
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.15025922123561888
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.15019423518974412
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.15035865728182402
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.15034027651777768
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.15032053299161585
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.15032860794361086
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.15033795139779246
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.15033828772024738
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.15033867070986623
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.1503091044403762
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.15025334087865694
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.1502507943925355
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.15029306830414993
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.1503178710091553
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.1503595129539401
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.15037313005034353
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.1503095475517297
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.15036624004276528
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.1504540424898017
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.1504625461186207
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.15038395223932133
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.15034274816760726
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.150370086075028
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.1503343958376853
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.15027695748914074
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.15034947630069026
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.15032809298953723
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.1503922870885124
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.15050036057262964
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.15048891542161383
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.15046185289283057
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.15048395171560688
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.1504270591602851
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.1504456998315957
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.15049131965653145
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.1505482393304507
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.1506720698299877
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.15064243488112558
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.1507223696502113
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.15067276482453132
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.15058857516238564
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.15057192390828622
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.15052665957293584
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.15064806897708705
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.1506060550843055
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.15053447337893697
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.1505652281209595
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.15053513438202615
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.1505840836649703
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.150635446634881
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.1505921659943385
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.15055671155147846
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.1505722633886094
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.15071355234878966
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.15073936795674
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.15077341474309752
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.1507444694788769
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.15067064743348274
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.1507401829164232
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.1506550286273311
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.15062464315444232
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.15062004227442038
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.1505660686771668
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.1504945597673764
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.15048241842132393
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.15050067958640465
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.1504585305641731
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.15043114526632084
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.1505553658284685
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.1505648248025141
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.15059305849235233
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.1505844277803335
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.15062066740853694
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.15061278509530837
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.1506899852791558
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.1507284953113062
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.15075033066722637
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.1507352670069507
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.15072665575660016
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.15082945255006983
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.15079343093647843
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.15089397760652307
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.15093955226348474
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.15085634994309563
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.15082633287979746
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.15082529814804302
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.1507789273287209
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.15078412313930323
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.15074974406406144
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.15066975969951468
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.15062936412733655
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.15065857878693847
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.15061548845497547
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.15057416377249272
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.15060014978119854
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.15053800464361564
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.15062289384253527
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.15066836211719556
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.15062770849629625
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.1505563542043158
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.15052907576953822
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.15051214287137768
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.150488354923094
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.1504697822723916
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.15045307202449254
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.15046190728297396
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.15048576569009256
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.1504808805166208
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.15055213976717954
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.15054109604626828
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.15047318104240628
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.15047600118654
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.15048370994957147
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.1504434822147782
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.15042205341980847
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.15038984686463744
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.15045160930930523
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.15041956194641815
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.15043140349169484
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.15043727712693558
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.1505193625131379
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.15048369655381574
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.15043686894756375
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.15035698398722427
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.15037143023299246
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.15038707511078928
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.15034836800888884
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.150348200548512
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.1504221331073433
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.15044711770088687
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.15048821436914991
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.1504826861297249
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.15060022143426083
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.15060022143426083 valid 0.22568343579769135
LOSS train 0.15060022143426083 valid 0.19955331832170486
LOSS train 0.15060022143426083 valid 0.20100976526737213
LOSS train 0.15060022143426083 valid 0.1898503340780735
LOSS train 0.15060022143426083 valid 0.1827535778284073
LOSS train 0.15060022143426083 valid 0.1941148762901624
LOSS train 0.15060022143426083 valid 0.2017942454133715
LOSS train 0.15060022143426083 valid 0.20124028250575066
LOSS train 0.15060022143426083 valid 0.20099236567815146
LOSS train 0.15060022143426083 valid 0.2030525177717209
LOSS train 0.15060022143426083 valid 0.20132459158247168
LOSS train 0.15060022143426083 valid 0.20136467243234316
LOSS train 0.15060022143426083 valid 0.20078536294973814
LOSS train 0.15060022143426083 valid 0.19964816740580968
LOSS train 0.15060022143426083 valid 0.1979763090610504
LOSS train 0.15060022143426083 valid 0.19936859235167503
LOSS train 0.15060022143426083 valid 0.2002933218198664
LOSS train 0.15060022143426083 valid 0.1998269549674458
LOSS train 0.15060022143426083 valid 0.20249896849456586
LOSS train 0.15060022143426083 valid 0.2027958743274212
LOSS train 0.15060022143426083 valid 0.20055353144804636
LOSS train 0.15060022143426083 valid 0.19918651472438464
LOSS train 0.15060022143426083 valid 0.1985765488251396
LOSS train 0.15060022143426083 valid 0.1985191876689593
LOSS train 0.15060022143426083 valid 0.19750468730926513
LOSS train 0.15060022143426083 valid 0.19808145211293146
LOSS train 0.15060022143426083 valid 0.19777001550904028
LOSS train 0.15060022143426083 valid 0.19751672074198723
LOSS train 0.15060022143426083 valid 0.19793962199112464
LOSS train 0.15060022143426083 valid 0.1991037497917811
LOSS train 0.15060022143426083 valid 0.199839745317736
LOSS train 0.15060022143426083 valid 0.19950123876333237
LOSS train 0.15060022143426083 valid 0.19972289331031567
LOSS train 0.15060022143426083 valid 0.19988148352679083
LOSS train 0.15060022143426083 valid 0.20184805818966456
LOSS train 0.15060022143426083 valid 0.2017480805516243
LOSS train 0.15060022143426083 valid 0.20216692138362574
LOSS train 0.15060022143426083 valid 0.203012499370073
LOSS train 0.15060022143426083 valid 0.201701609370036
LOSS train 0.15060022143426083 valid 0.20143217369914054
LOSS train 0.15060022143426083 valid 0.20203071323836722
LOSS train 0.15060022143426083 valid 0.2025369412842251
LOSS train 0.15060022143426083 valid 0.2026646213476048
LOSS train 0.15060022143426083 valid 0.20362380959770895
LOSS train 0.15060022143426083 valid 0.20340038471751742
LOSS train 0.15060022143426083 valid 0.20409735551346903
LOSS train 0.15060022143426083 valid 0.20434786284223516
LOSS train 0.15060022143426083 valid 0.20399117035170397
LOSS train 0.15060022143426083 valid 0.20468363105034343
LOSS train 0.15060022143426083 valid 0.20412342071533204
LOSS train 0.15060022143426083 valid 0.20438730190781987
LOSS train 0.15060022143426083 valid 0.2042111846116873
LOSS train 0.15060022143426083 valid 0.20477840838567266
LOSS train 0.15060022143426083 valid 0.20448175569375357
LOSS train 0.15060022143426083 valid 0.20456102978099477
LOSS train 0.15060022143426083 valid 0.20417308168751852
LOSS train 0.15060022143426083 valid 0.20435417142876408
LOSS train 0.15060022143426083 valid 0.20408587851401033
LOSS train 0.15060022143426083 valid 0.2045475022267487
LOSS train 0.15060022143426083 valid 0.20422107254465421
LOSS train 0.15060022143426083 valid 0.20428346121897462
LOSS train 0.15060022143426083 valid 0.2049105996566434
LOSS train 0.15060022143426083 valid 0.2046873259638983
LOSS train 0.15060022143426083 valid 0.20534463343210518
LOSS train 0.15060022143426083 valid 0.205733132820863
LOSS train 0.15060022143426083 valid 0.2057609944181009
LOSS train 0.15060022143426083 valid 0.20557914096028057
LOSS train 0.15060022143426083 valid 0.20545640083796837
LOSS train 0.15060022143426083 valid 0.2047959103964377
LOSS train 0.15060022143426083 valid 0.20527704975434713
LOSS train 0.15060022143426083 valid 0.20516964646292404
LOSS train 0.15060022143426083 valid 0.20550729975932175
LOSS train 0.15060022143426083 valid 0.20577502393559233
LOSS train 0.15060022143426083 valid 0.20558381684728572
LOSS train 0.15060022143426083 valid 0.20557314415772757
LOSS train 0.15060022143426083 valid 0.20616919959062024
LOSS train 0.15060022143426083 valid 0.2061037343817872
LOSS train 0.15060022143426083 valid 0.20597255669343165
LOSS train 0.15060022143426083 valid 0.20549586943433254
LOSS train 0.15060022143426083 valid 0.20485279317945243
LOSS train 0.15060022143426083 valid 0.20432276398311428
LOSS train 0.15060022143426083 valid 0.20456825596530262
LOSS train 0.15060022143426083 valid 0.20421751705278834
LOSS train 0.15060022143426083 valid 0.20396798937803223
LOSS train 0.15060022143426083 valid 0.2034049151574864
LOSS train 0.15060022143426083 valid 0.20292406425226567
LOSS train 0.15060022143426083 valid 0.20292793745282053
LOSS train 0.15060022143426083 valid 0.202676268463785
LOSS train 0.15060022143426083 valid 0.20285715045553915
LOSS train 0.15060022143426083 valid 0.20312075830168194
LOSS train 0.15060022143426083 valid 0.20351342987883222
LOSS train 0.15060022143426083 valid 0.2034367591790531
LOSS train 0.15060022143426083 valid 0.2032644807971934
LOSS train 0.15060022143426083 valid 0.20353854590273918
LOSS train 0.15060022143426083 valid 0.20298648288375454
LOSS train 0.15060022143426083 valid 0.2028739843517542
LOSS train 0.15060022143426083 valid 0.20290340775067045
LOSS train 0.15060022143426083 valid 0.20299570308047898
LOSS train 0.15060022143426083 valid 0.20339107949926397
LOSS train 0.15060022143426083 valid 0.20365027263760566
LOSS train 0.15060022143426083 valid 0.20377904117697537
LOSS train 0.15060022143426083 valid 0.20418961696765003
LOSS train 0.15060022143426083 valid 0.20416292374573866
LOSS train 0.15060022143426083 valid 0.2042641158287342
LOSS train 0.15060022143426083 valid 0.20424576288177854
LOSS train 0.15060022143426083 valid 0.2048102889983159
LOSS train 0.15060022143426083 valid 0.20468401713906048
LOSS train 0.15060022143426083 valid 0.20501949003449194
LOSS train 0.15060022143426083 valid 0.20536067111229678
LOSS train 0.15060022143426083 valid 0.20548772527412934
LOSS train 0.15060022143426083 valid 0.20545735807569177
LOSS train 0.15060022143426083 valid 0.20508812168346985
LOSS train 0.15060022143426083 valid 0.20524318872299868
LOSS train 0.15060022143426083 valid 0.205422893297254
LOSS train 0.15060022143426083 valid 0.20539910650771598
LOSS train 0.15060022143426083 valid 0.20583076736536518
LOSS train 0.15060022143426083 valid 0.20592101937175816
LOSS train 0.15060022143426083 valid 0.20556636647147647
LOSS train 0.15060022143426083 valid 0.20533403553882568
LOSS train 0.15060022143426083 valid 0.20510164548953375
LOSS train 0.15060022143426083 valid 0.20477265193442667
LOSS train 0.15060022143426083 valid 0.20465114822641747
LOSS train 0.15060022143426083 valid 0.20486083638861896
LOSS train 0.15060022143426083 valid 0.20503278377075349
LOSS train 0.15060022143426083 valid 0.20491708064079284
LOSS train 0.15060022143426083 valid 0.20518923834675834
LOSS train 0.15060022143426083 valid 0.20499144072138417
LOSS train 0.15060022143426083 valid 0.20502536941785365
LOSS train 0.15060022143426083 valid 0.20515965491302254
LOSS train 0.15060022143426083 valid 0.20506384189312274
LOSS train 0.15060022143426083 valid 0.20476868646290466
LOSS train 0.15060022143426083 valid 0.20450787099473405
LOSS train 0.15060022143426083 valid 0.20424497822173557
LOSS train 0.15060022143426083 valid 0.2043115742393394
LOSS train 0.15060022143426083 valid 0.20433030172630592
LOSS train 0.15060022143426083 valid 0.2043838754077168
LOSS train 0.15060022143426083 valid 0.20416686493549904
LOSS train 0.15060022143426083 valid 0.20413127476754395
LOSS train 0.15060022143426083 valid 0.2039023808223738
LOSS train 0.15060022143426083 valid 0.20402821866529328
LOSS train 0.15060022143426083 valid 0.20393549677328016
LOSS train 0.15060022143426083 valid 0.20390690569306763
LOSS train 0.15060022143426083 valid 0.20380651283931064
LOSS train 0.15060022143426083 valid 0.2037699715130859
LOSS train 0.15060022143426083 valid 0.2035900827111869
LOSS train 0.15060022143426083 valid 0.20389817695911616
LOSS train 0.15060022143426083 valid 0.20367137722823084
LOSS train 0.15060022143426083 valid 0.20454856421093684
LOSS train 0.15060022143426083 valid 0.20455192369502662
LOSS train 0.15060022143426083 valid 0.2044477857152621
LOSS train 0.15060022143426083 valid 0.2046228763678216
LOSS train 0.15060022143426083 valid 0.2045120809619364
LOSS train 0.15060022143426083 valid 0.20449080264646244
LOSS train 0.15060022143426083 valid 0.2045451652307015
LOSS train 0.15060022143426083 valid 0.20449743126669237
LOSS train 0.15060022143426083 valid 0.2047431998145886
LOSS train 0.15060022143426083 valid 0.20488903334565983
LOSS train 0.15060022143426083 valid 0.2049577707731271
LOSS train 0.15060022143426083 valid 0.20498207436417634
LOSS train 0.15060022143426083 valid 0.20483287181705237
LOSS train 0.15060022143426083 valid 0.20474113200021826
LOSS train 0.15060022143426083 valid 0.20458500970292975
LOSS train 0.15060022143426083 valid 0.20437982340166175
LOSS train 0.15060022143426083 valid 0.2042246587211039
LOSS train 0.15060022143426083 valid 0.20411790121685375
LOSS train 0.15060022143426083 valid 0.20418841302035803
LOSS train 0.15060022143426083 valid 0.20445968308848536
LOSS train 0.15060022143426083 valid 0.20454560583900838
LOSS train 0.15060022143426083 valid 0.2047496859491224
LOSS train 0.15060022143426083 valid 0.2047705567058395
LOSS train 0.15060022143426083 valid 0.20472964783858139
LOSS train 0.15060022143426083 valid 0.20452693789157755
LOSS train 0.15060022143426083 valid 0.20460334962847604
LOSS train 0.15060022143426083 valid 0.2045433123050065
LOSS train 0.15060022143426083 valid 0.20439563180719103
LOSS train 0.15060022143426083 valid 0.2042830160564997
LOSS train 0.15060022143426083 valid 0.2044260029570531
LOSS train 0.15060022143426083 valid 0.20462847802411305
LOSS train 0.15060022143426083 valid 0.20450561339628762
LOSS train 0.15060022143426083 valid 0.20447729958428276
LOSS train 0.15060022143426083 valid 0.2046078224537781
LOSS train 0.15060022143426083 valid 0.20454956853127743
LOSS train 0.15060022143426083 valid 0.2045240916841017
LOSS train 0.15060022143426083 valid 0.20437035952573238
LOSS train 0.15060022143426083 valid 0.20428628953727515
LOSS train 0.15060022143426083 valid 0.2042993611545973
LOSS train 0.15060022143426083 valid 0.20407264707560208
LOSS train 0.15060022143426083 valid 0.2040803575927907
LOSS train 0.15060022143426083 valid 0.20408886387234643
LOSS train 0.15060022143426083 valid 0.20410080528572985
LOSS train 0.15060022143426083 valid 0.20404758846572557
LOSS train 0.15060022143426083 valid 0.20394522712255517
LOSS train 0.15060022143426083 valid 0.20384918454397527
LOSS train 0.15060022143426083 valid 0.2036905678891644
LOSS train 0.15060022143426083 valid 0.2034153522589268
LOSS train 0.15060022143426083 valid 0.20346601109723655
LOSS train 0.15060022143426083 valid 0.20363553004518983
LOSS train 0.15060022143426083 valid 0.20357247336645318
LOSS train 0.15060022143426083 valid 0.2036346593993393
LOSS train 0.15060022143426083 valid 0.20347446925938129
LOSS train 0.15060022143426083 valid 0.2033831452700629
LOSS train 0.15060022143426083 valid 0.20338697606089093
LOSS train 0.15060022143426083 valid 0.2034308445717901
LOSS train 0.15060022143426083 valid 0.2036176475093645
LOSS train 0.15060022143426083 valid 0.20344054342769996
LOSS train 0.15060022143426083 valid 0.20358395764549958
LOSS train 0.15060022143426083 valid 0.20348218796045883
LOSS train 0.15060022143426083 valid 0.2034803616026273
LOSS train 0.15060022143426083 valid 0.20336124600405897
LOSS train 0.15060022143426083 valid 0.20330773137864613
LOSS train 0.15060022143426083 valid 0.2031951007119852
LOSS train 0.15060022143426083 valid 0.2030228093828795
LOSS train 0.15060022143426083 valid 0.20301816203224826
LOSS train 0.15060022143426083 valid 0.20295256188261174
LOSS train 0.15060022143426083 valid 0.2028647456058236
LOSS train 0.15060022143426083 valid 0.20263328210071282
LOSS train 0.15060022143426083 valid 0.202561788569947
LOSS train 0.15060022143426083 valid 0.202475319351625
LOSS train 0.15060022143426083 valid 0.20258995366695265
LOSS train 0.15060022143426083 valid 0.2025790303268216
LOSS train 0.15060022143426083 valid 0.2025180514986159
LOSS train 0.15060022143426083 valid 0.20255979546555528
LOSS train 0.15060022143426083 valid 0.20267114537713773
LOSS train 0.15060022143426083 valid 0.20278049233768666
LOSS train 0.15060022143426083 valid 0.20298956784937117
LOSS train 0.15060022143426083 valid 0.20307901448908106
LOSS train 0.15060022143426083 valid 0.2032557777765039
LOSS train 0.15060022143426083 valid 0.2033287262576714
LOSS train 0.15060022143426083 valid 0.2032867440360082
LOSS train 0.15060022143426083 valid 0.2033792244999305
LOSS train 0.15060022143426083 valid 0.20351089356523572
LOSS train 0.15060022143426083 valid 0.20355718550754004
LOSS train 0.15060022143426083 valid 0.20352261945157604
LOSS train 0.15060022143426083 valid 0.20367234951665258
LOSS train 0.15060022143426083 valid 0.20375056818444678
LOSS train 0.15060022143426083 valid 0.20365984958864874
LOSS train 0.15060022143426083 valid 0.20357153687296034
LOSS train 0.15060022143426083 valid 0.2035420835644257
LOSS train 0.15060022143426083 valid 0.20333566525989996
LOSS train 0.15060022143426083 valid 0.2033356348052621
LOSS train 0.15060022143426083 valid 0.20348220037226855
LOSS train 0.15060022143426083 valid 0.20332399878866417
LOSS train 0.15060022143426083 valid 0.20353164649549335
LOSS train 0.15060022143426083 valid 0.20371591696729424
LOSS train 0.15060022143426083 valid 0.20366324952670506
LOSS train 0.15060022143426083 valid 0.20355219266763547
LOSS train 0.15060022143426083 valid 0.20367453262390878
LOSS train 0.15060022143426083 valid 0.20363578873295937
LOSS train 0.15060022143426083 valid 0.20360316030950432
LOSS train 0.15060022143426083 valid 0.20358214628696442
LOSS train 0.15060022143426083 valid 0.20342585147614498
LOSS train 0.15060022143426083 valid 0.20353670152170317
LOSS train 0.15060022143426083 valid 0.20343028409443353
LOSS train 0.15060022143426083 valid 0.20337601821488283
LOSS train 0.15060022143426083 valid 0.20342244821436264
LOSS train 0.15060022143426083 valid 0.2034304402768612
LOSS train 0.15060022143426083 valid 0.20326620502694573
LOSS train 0.15060022143426083 valid 0.20345429348391156
LOSS train 0.15060022143426083 valid 0.20346082292468393
LOSS train 0.15060022143426083 valid 0.2034456916153431
LOSS train 0.15060022143426083 valid 0.20357367293588047
LOSS train 0.15060022143426083 valid 0.20363807490536276
LOSS train 0.15060022143426083 valid 0.20357003196110743
LOSS train 0.15060022143426083 valid 0.20358842707267313
LOSS train 0.15060022143426083 valid 0.20358960954648145
LOSS train 0.15060022143426083 valid 0.20356840724335576
LOSS train 0.15060022143426083 valid 0.20369139168592876
LOSS train 0.15060022143426083 valid 0.20378555279614322
LOSS train 0.15060022143426083 valid 0.2039308336366065
LOSS train 0.15060022143426083 valid 0.2038907801663434
LOSS train 0.15060022143426083 valid 0.20400074905135096
LOSS train 0.15060022143426083 valid 0.20429355388178544
LOSS train 0.15060022143426083 valid 0.20446961736067748
LOSS train 0.15060022143426083 valid 0.20456933018064846
LOSS train 0.15060022143426083 valid 0.20449560057033192
LOSS train 0.15060022143426083 valid 0.2045130674065887
LOSS train 0.15060022143426083 valid 0.2044245138602997
LOSS train 0.15060022143426083 valid 0.20427679935162016
LOSS train 0.15060022143426083 valid 0.20426937296826353
LOSS train 0.15060022143426083 valid 0.2042425022061382
LOSS train 0.15060022143426083 valid 0.20417087077033902
LOSS train 0.15060022143426083 valid 0.20389802912448315
LOSS train 0.15060022143426083 valid 0.20399725000133784
LOSS train 0.15060022143426083 valid 0.20408714699073577
LOSS train 0.15060022143426083 valid 0.20405980128990978
LOSS train 0.15060022143426083 valid 0.20401965831959998
LOSS train 0.15060022143426083 valid 0.20393113978856117
LOSS train 0.15060022143426083 valid 0.20390988420695066
LOSS train 0.15060022143426083 valid 0.20391400906041418
LOSS train 0.15060022143426083 valid 0.2039003727764919
LOSS train 0.15060022143426083 valid 0.2037361936061243
LOSS train 0.15060022143426083 valid 0.2036838009443185
LOSS train 0.15060022143426083 valid 0.20368127412966902
LOSS train 0.15060022143426083 valid 0.20369175654284807
LOSS train 0.15060022143426083 valid 0.2038015495922606
LOSS train 0.15060022143426083 valid 0.20371352428117315
LOSS train 0.15060022143426083 valid 0.2037582319192212
LOSS train 0.15060022143426083 valid 0.2037396242954587
LOSS train 0.15060022143426083 valid 0.20377442420127
LOSS train 0.15060022143426083 valid 0.20385474349061647
LOSS train 0.15060022143426083 valid 0.20384128357088843
LOSS train 0.15060022143426083 valid 0.2037673176617812
LOSS train 0.15060022143426083 valid 0.20388582235712424
LOSS train 0.15060022143426083 valid 0.20388357816754202
LOSS train 0.15060022143426083 valid 0.2038622015812358
LOSS train 0.15060022143426083 valid 0.20388206513294208
LOSS train 0.15060022143426083 valid 0.20386215306648603
LOSS train 0.15060022143426083 valid 0.2037695962693784
LOSS train 0.15060022143426083 valid 0.2039027273847833
LOSS train 0.15060022143426083 valid 0.20388190568454803
LOSS train 0.15060022143426083 valid 0.20380862085957235
LOSS train 0.15060022143426083 valid 0.2038095350830983
LOSS train 0.15060022143426083 valid 0.20392436569871994
LOSS train 0.15060022143426083 valid 0.20401655474475994
LOSS train 0.15060022143426083 valid 0.20395380362631782
LOSS train 0.15060022143426083 valid 0.2038924623799475
LOSS train 0.15060022143426083 valid 0.20395059686165878
LOSS train 0.15060022143426083 valid 0.20396784594598807
LOSS train 0.15060022143426083 valid 0.20405763420378525
LOSS train 0.15060022143426083 valid 0.2039306905120611
LOSS train 0.15060022143426083 valid 0.20409764084860543
LOSS train 0.15060022143426083 valid 0.2041762835398224
LOSS train 0.15060022143426083 valid 0.2041497524862319
LOSS train 0.15060022143426083 valid 0.20426500335703662
LOSS train 0.15060022143426083 valid 0.2042235423051394
LOSS train 0.15060022143426083 valid 0.20442818306340763
LOSS train 0.15060022143426083 valid 0.20456595477343334
LOSS train 0.15060022143426083 valid 0.2044545746431118
LOSS train 0.15060022143426083 valid 0.2046042963364204
LOSS train 0.15060022143426083 valid 0.20457814128109902
LOSS train 0.15060022143426083 valid 0.2044583536526948
LOSS train 0.15060022143426083 valid 0.20439090952277184
LOSS train 0.15060022143426083 valid 0.2044105560661436
LOSS train 0.15060022143426083 valid 0.20452798943141262
LOSS train 0.15060022143426083 valid 0.204531941974341
LOSS train 0.15060022143426083 valid 0.20462618488818407
LOSS train 0.15060022143426083 valid 0.20462636154377142
LOSS train 0.15060022143426083 valid 0.20458220364250376
LOSS train 0.15060022143426083 valid 0.2046639002354096
LOSS train 0.15060022143426083 valid 0.20467314750832669
LOSS train 0.15060022143426083 valid 0.20451634838259464
LOSS train 0.15060022143426083 valid 0.20450558352191545
LOSS train 0.15060022143426083 valid 0.20458823768286247
LOSS train 0.15060022143426083 valid 0.20480421848248603
LOSS train 0.15060022143426083 valid 0.20487139497114265
LOSS train 0.15060022143426083 valid 0.20488692260201954
LOSS train 0.15060022143426083 valid 0.20477077390687268
LOSS train 0.15060022143426083 valid 0.204734379190138
LOSS train 0.15060022143426083 valid 0.20477950867575015
LOSS train 0.15060022143426083 valid 0.20471154889890125
LOSS train 0.15060022143426083 valid 0.2046181977304638
LOSS train 0.15060022143426083 valid 0.2045982020314444
LOSS train 0.15060022143426083 valid 0.20463725012692785
LOSS train 0.15060022143426083 valid 0.2047451343071663
LOSS train 0.15060022143426083 valid 0.20484105247846793
LOSS train 0.15060022143426083 valid 0.20488306486539626
LOSS train 0.15060022143426083 valid 0.20494543195438653
LOSS train 0.15060022143426083 valid 0.20481170705576848
LOSS train 0.15060022143426083 valid 0.20484488056894795
LOSS train 0.15060022143426083 valid 0.20481676869094373
LOSS train 0.15060022143426083 valid 0.2047729409219816
LOSS train 0.15060022143426083 valid 0.2048843180277071
LOSS train 0.15060022143426083 valid 0.20485217101646191
LOSS train 0.15060022143426083 valid 0.20485412656933397
LOSS train 0.15060022143426083 valid 0.2048619759001144
LOSS train 0.15060022143426083 valid 0.20484396224953438
LOSS train 0.15060022143426083 valid 0.204751746693489
LOSS train 0.15060022143426083 valid 0.2047624617976987
LOSS train 0.15060022143426083 valid 0.20484794219981042
EPOCH 6:
  batch 1 loss: 0.14117324352264404
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.13924625515937805
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.145248810450236
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.1499377116560936
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.15665962398052216
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.1603741149107615
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.1527142162833895
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.15186434984207153
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.1486183007558187
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.14716842770576477
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.14465138790282336
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.14331089394787946
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.14368495860925087
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.1428954755621297
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.14277676194906236
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.14428032049909234
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.14415774143794002
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.14654412161972788
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.14563092236456118
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.14434216655790805
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.1445047262878645
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.1450165310366587
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.14510408464981162
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.14558759052306414
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.1455100330710411
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.1456143480653946
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.14633063282127734
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.14667539378362043
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.1458575065279829
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.14675316984454792
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.14707699538238586
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.14759773737750947
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.14678817535891678
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.14763583549681833
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.1489675364324025
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.1482116598635912
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.148251646475212
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.1482619535374014
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.14862164644858775
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.14872262347489595
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.14857691689962294
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.14904214282120978
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.14983880502540012
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.14896601150658997
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.14964743041329914
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.14937477380685185
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.14871164125964997
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.14853832855199775
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.14864812989015969
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.148850926309824
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.14877341819160125
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.1492096264488422
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.14934762033089152
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.14978251040533738
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.14971145405010744
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.1495086746290326
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.14950520414532276
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.14894679805328107
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.14929942837205984
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.1489684171974659
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.14909027419129356
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.14873415568182546
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.14844697217146555
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.14857627567835152
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.14856649018250978
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.14861534987435196
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.14846640513904058
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.1485093036556945
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.14888584419437076
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.14911267289093563
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.14888347664349516
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.14895703809128868
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.14889928637302086
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.1489765787044087
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.1490419868628184
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.14921649368970016
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.14901414552292266
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.14873153028579858
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.14890494471109367
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.1491103008389473
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.14932585277675112
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.14894330610589282
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.14863862842321396
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.1489209331394661
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.14895835001679028
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.14900358549730722
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.1492507229077405
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.14948166576637464
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.14947079599238514
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.14972653430369165
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.14960243323674569
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.1494964927272952
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.14977242429089802
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.14983658373672912
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.1501917105756308
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.150322510007148
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.15031189993791974
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.15028308780521762
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.15038187643795303
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.15029579736292362
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.1502880622992421
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.15021240134157388
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.1497522968690372
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.14983561663673475
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.14942314142272586
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.14952690815025904
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.14974754408141164
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.149657830182049
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.14972734806734486
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.15005581853064623
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.14985328899310515
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.14952070878020354
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.14923506961987082
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.14949077119429907
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.1491817695291146
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.14910498075187206
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.1493831031724938
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.14931656413916813
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.14930920534524597
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.14945744952807824
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.14927484064309066
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.1493763491877767
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.14985732752375486
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.1497281528408489
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.14981707507371902
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.14993956442626696
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.14995986475484577
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.15000364714069292
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.15003856525633685
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.15009197804790278
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.14998550081753548
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.14988629344963667
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.1502862910466983
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.15026481599727673
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.14999291289735722
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.14999614754582152
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.15011051090529365
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.1500273485114609
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.1502553477561731
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.15042976726378712
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.15049412254746078
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.15045931998273016
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.1505460017002546
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.15078838355839252
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.1506642588253679
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.1507104579104136
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.15083039820599717
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.1508296591204566
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.15106378255674502
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.1510790671904882
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.15099417146862737
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.15089298422007183
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.15095221577516568
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.15078948273674234
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.15081992274330508
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.1506832231504795
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.1506728577386042
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.1506415786622446
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.1507440169089995
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.15084589105099439
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.15078345098480675
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.15072726283544374
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.15077833136532204
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.1507436592222714
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.15086436082016338
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.15082235984414458
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.150672268189356
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.15063525807289851
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.15039692104920832
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.1504721678355161
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.1503840308440359
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.15038031261674192
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.150239131440317
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.1502225560018386
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.1503448899303164
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.15027012066407638
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.1502211424758879
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.15008523892820552
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.1502966472056991
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.1501749715043439
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.1501296376655115
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.14996972311656553
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.14985152321760772
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.14979133811657844
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.14970844734359431
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.14970345034073756
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.14968548786831412
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.14952222812683025
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.14949406998813466
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.14955360387500963
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.14953297782318756
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.1494716495896379
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.14955150046496812
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.14932631370947533
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.1493741704103274
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.14944061659732644
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.14950737712649526
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.1496915833817588
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.14970630826662534
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.14978925652801992
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.14976230219228945
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.14977965791626732
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.1497081355214706
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.14978706610261225
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.14964124342290366
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.1495616854828538
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.14951273914120625
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.14938277615090975
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.14944642461372903
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.14955671947626842
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.14957280880749507
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.14967873291868083
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.14976531875805116
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.1495715661026607
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.14970014213129532
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.14957722842141433
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.14966311027652107
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.14969583115446458
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.1497941681919577
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.1498082975772294
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.14973279051651242
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.14971560937864287
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.14964495225070304
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.14973643169339215
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.14962583313385647
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.149643537052701
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.1496684457696482
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.14947966989456563
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.14935844082499175
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.14951229477706163
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.14967419252251135
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.1496455652703499
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.14959294838199289
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.14955913052599654
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.14958871958103587
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.14949892410787485
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.14950104686529828
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.14949323631384792
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.14947291722357522
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.1494750707099835
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.14959336613223762
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.14950673965629468
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.14949660364983014
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.1495004245003716
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.1495352462238195
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.14954283185363784
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.14940906602602738
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.14952389727677068
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.14955368410631356
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.14959238344430922
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.1497271677768563
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.14965574928219355
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.14965842181279254
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.14965633942386297
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.14971824446145227
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.1496707076439634
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.14958804121045288
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.14949685127236123
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.14951131507236526
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.14952073452564385
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.14941178116647677
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.1493402511052048
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.14932874998098997
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.14919112563471904
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.14920589268769857
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.149184358551314
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.14916942726248658
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.14919814581412877
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.14913655005977056
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.14916308342858597
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.14915185504183998
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.1491422331891954
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.14935696635381643
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.1492391529962094
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.14913011797449804
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.14933951390718203
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.14932136751361702
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.14928200484501372
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.14929700231573487
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.1493569190214787
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.14929373618016464
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.14924431531458882
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.14926161074701552
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.14927650282395558
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.14933765397260063
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.1493665119828461
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.149304902018778
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.14934451301168236
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.14925788244986615
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.14931439885805392
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.1492697492088239
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.14923207538381014
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.1493221440514607
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.149413471876764
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.14941778076907336
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.14949667292672233
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.14954186845508088
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.14944216806516553
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.14934246100011878
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.14933044301966827
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.14944068139474653
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.14940644842603348
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.14939140174353477
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.14937221374068604
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.1493878914928827
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.14939143715633285
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.14939693118830846
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.14936429187171646
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.14933381142836172
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.14926076043517358
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.14928176162998008
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.14923127107799816
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.1492973215425738
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.14926281483594778
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.14927730179495283
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.1493980689563706
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.14937271471561317
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.14932100256940103
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.1493065223845195
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.14932444321457297
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.14929817967418568
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.14925823052940163
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.14929129605415067
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.149218557678439
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.14927319689438892
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.149174559541458
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.1490764982474324
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.14914655326524887
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.14909922555768382
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.14915930412032388
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.14922353139096517
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.14913165183311486
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.1490210964604541
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.1490365552777302
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.14916378557682036
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.1491773659806876
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.14907964268141047
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.14900990954693957
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.14889178473425474
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.14888061751775883
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.14878129932887393
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.14897058520749298
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.14896680154014955
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.14895370035150715
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.1489604738743409
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.14895603521538608
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.14894337239286054
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.14892817318610763
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.1488915134893789
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.14882981615407126
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.1488210879414849
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.14885817023671485
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.14890715471408184
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.14895606470310083
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.14896477267775737
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.14890937218337916
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.14897659103743502
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.14905143591611744
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.14904753892866682
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.14896248607999749
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.14891316760279796
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.1489213528471757
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.14886763096676714
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.14881349690667875
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.14886237676829508
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.1488348324197889
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.14887437252809957
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.14893874161593293
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.1489241686132219
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.14883924948202598
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.14885322974537904
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.14880967917301322
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.14878487475116514
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.14883218167778006
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.14886676541964214
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.1489686721024361
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.1489043218189905
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.1489718805348116
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.1488937733001948
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.14882611042182697
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.1487892547187217
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.1487312707562409
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.14880977962020173
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.1487711668984654
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.14869268941027777
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.1486895456717113
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.1486625165632837
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.1486814527666753
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.14872759007932596
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.14867159966857005
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.14864140933813036
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.14863013827755134
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.1487833302250042
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.14882311067983583
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.1488164217223095
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.14878778545317625
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.14869695492745647
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.14876347181186006
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.14869253167457747
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.148654418643564
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.14864633875223168
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.1485923768683749
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.1485140533475367
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.1485081809840285
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.148504836452596
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.1484714038670063
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.14845515848011584
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.1485773938361044
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.14858758072471268
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.14861642974542408
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.14861494203046
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.148659885200916
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.14864806248186288
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.14872348475931346
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.14875311849706144
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.14878973334382933
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.1487638264787283
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.1487472755166047
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.1488713988220777
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.14883206428161688
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.14895469189185712
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.14900718408667646
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.1489197284899141
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.14888507447574498
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.14889583492980285
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.14884444850851114
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.1488420394111852
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.1488164569312167
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.1487433931905351
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.14868994701740354
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.1487139169550274
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.1486766775035196
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.1486401544500994
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.14867425771764897
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.14862106113598264
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.1487235683230085
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.14878001928465862
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.14875113120362093
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.14867522416462386
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.14863722260025414
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.1486197113314994
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.1485989991558623
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.1485806523609915
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.14856570603342745
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.14857745143804657
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.14860684985938094
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.14861914142132873
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.1491745585974838
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.14968073879159108
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.1498661576708158
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.1501219424929164
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.15041043975316318
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.150514115626712
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.1506483163930771
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.1508106835595854
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.15105605112356052
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.1511484400839983
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.15128194270696182
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.15139626515197338
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.15159239088711532
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.15168932131986557
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.15178640618984834
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.15183794588558616
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.15198872784345313
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.15209427020883048
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.15218055075586098
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.15227105718991485
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.15242634882402217
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.15254256745645486
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.15267397495026283
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.15275137980652462
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.15289779847203674
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.15289779847203674 valid 0.2552807927131653
LOSS train 0.15289779847203674 valid 0.24361132085323334
LOSS train 0.15289779847203674 valid 0.24548946817715964
LOSS train 0.15289779847203674 valid 0.2348194532096386
LOSS train 0.15289779847203674 valid 0.22603093683719636
LOSS train 0.15289779847203674 valid 0.23420881976683935
LOSS train 0.15289779847203674 valid 0.24530323701245443
LOSS train 0.15289779847203674 valid 0.24379261769354343
LOSS train 0.15289779847203674 valid 0.24303297201792398
LOSS train 0.15289779847203674 valid 0.24527562260627747
LOSS train 0.15289779847203674 valid 0.24272599003531717
LOSS train 0.15289779847203674 valid 0.24290437127153078
LOSS train 0.15289779847203674 valid 0.2426069562251751
LOSS train 0.15289779847203674 valid 0.2423007892710822
LOSS train 0.15289779847203674 valid 0.2394992599884669
LOSS train 0.15289779847203674 valid 0.2402229206636548
LOSS train 0.15289779847203674 valid 0.2405126699629952
LOSS train 0.15289779847203674 valid 0.24133721739053726
LOSS train 0.15289779847203674 valid 0.24390756534902672
LOSS train 0.15289779847203674 valid 0.24385184198617935
LOSS train 0.15289779847203674 valid 0.24219310212702977
LOSS train 0.15289779847203674 valid 0.2406016852368008
LOSS train 0.15289779847203674 valid 0.2403667705214542
LOSS train 0.15289779847203674 valid 0.23970285865167776
LOSS train 0.15289779847203674 valid 0.23880644977092744
LOSS train 0.15289779847203674 valid 0.2397055413860541
LOSS train 0.15289779847203674 valid 0.23944044333917122
LOSS train 0.15289779847203674 valid 0.2395487017929554
LOSS train 0.15289779847203674 valid 0.24048765003681183
LOSS train 0.15289779847203674 valid 0.241978952785333
LOSS train 0.15289779847203674 valid 0.2427554875612259
LOSS train 0.15289779847203674 valid 0.24235369032248855
LOSS train 0.15289779847203674 valid 0.24310235317909357
LOSS train 0.15289779847203674 valid 0.24347366787054958
LOSS train 0.15289779847203674 valid 0.244966350708689
LOSS train 0.15289779847203674 valid 0.24486171909504467
LOSS train 0.15289779847203674 valid 0.24486227011358416
LOSS train 0.15289779847203674 valid 0.24632326513528824
LOSS train 0.15289779847203674 valid 0.2451454534744605
LOSS train 0.15289779847203674 valid 0.24515680968761444
LOSS train 0.15289779847203674 valid 0.2459685264564142
LOSS train 0.15289779847203674 valid 0.2464133281083334
LOSS train 0.15289779847203674 valid 0.24641030437724534
LOSS train 0.15289779847203674 valid 0.24727106331424278
LOSS train 0.15289779847203674 valid 0.24703954292668237
LOSS train 0.15289779847203674 valid 0.24791290351878043
LOSS train 0.15289779847203674 valid 0.2482410285701143
LOSS train 0.15289779847203674 valid 0.2478851992636919
LOSS train 0.15289779847203674 valid 0.24833064602345836
LOSS train 0.15289779847203674 valid 0.2476602476835251
LOSS train 0.15289779847203674 valid 0.24821406602859497
LOSS train 0.15289779847203674 valid 0.24789751779574615
LOSS train 0.15289779847203674 valid 0.24820344403104963
LOSS train 0.15289779847203674 valid 0.24799241291152108
LOSS train 0.15289779847203674 valid 0.24814325137571855
LOSS train 0.15289779847203674 valid 0.24757035874894687
LOSS train 0.15289779847203674 valid 0.2474996455405888
LOSS train 0.15289779847203674 valid 0.24716402667349782
LOSS train 0.15289779847203674 valid 0.24764325835947262
LOSS train 0.15289779847203674 valid 0.24752068345745404
LOSS train 0.15289779847203674 valid 0.2478086315706128
LOSS train 0.15289779847203674 valid 0.24840007890616694
LOSS train 0.15289779847203674 valid 0.24844955475557418
LOSS train 0.15289779847203674 valid 0.2490143070463091
LOSS train 0.15289779847203674 valid 0.24933078357806573
LOSS train 0.15289779847203674 valid 0.2493882980762106
LOSS train 0.15289779847203674 valid 0.24891783208099763
LOSS train 0.15289779847203674 valid 0.24878707878729878
LOSS train 0.15289779847203674 valid 0.24809817127559497
LOSS train 0.15289779847203674 valid 0.24842995618070876
LOSS train 0.15289779847203674 valid 0.2482576462584482
LOSS train 0.15289779847203674 valid 0.2485567070543766
LOSS train 0.15289779847203674 valid 0.24879890884438607
LOSS train 0.15289779847203674 valid 0.2485695070511586
LOSS train 0.15289779847203674 valid 0.2484196643034617
LOSS train 0.15289779847203674 valid 0.24914012339554334
LOSS train 0.15289779847203674 valid 0.24915382885313653
LOSS train 0.15289779847203674 valid 0.24901412408321333
LOSS train 0.15289779847203674 valid 0.2489494553095178
LOSS train 0.15289779847203674 valid 0.248274851962924
LOSS train 0.15289779847203674 valid 0.2474485981611558
LOSS train 0.15289779847203674 valid 0.24769355029594609
LOSS train 0.15289779847203674 valid 0.2475023481501154
LOSS train 0.15289779847203674 valid 0.24742782080457323
LOSS train 0.15289779847203674 valid 0.24682692902929643
LOSS train 0.15289779847203674 valid 0.2462346378800481
LOSS train 0.15289779847203674 valid 0.2462710001687894
LOSS train 0.15289779847203674 valid 0.2458806623789397
LOSS train 0.15289779847203674 valid 0.24614828389682128
LOSS train 0.15289779847203674 valid 0.2463208516438802
LOSS train 0.15289779847203674 valid 0.24664443600308764
LOSS train 0.15289779847203674 valid 0.24656692487390144
LOSS train 0.15289779847203674 valid 0.24647423032150473
LOSS train 0.15289779847203674 valid 0.24669158855017195
LOSS train 0.15289779847203674 valid 0.2462740449528945
LOSS train 0.15289779847203674 valid 0.24603501738359532
LOSS train 0.15289779847203674 valid 0.2462454988169916
LOSS train 0.15289779847203674 valid 0.24629756382533483
LOSS train 0.15289779847203674 valid 0.24645783202816743
LOSS train 0.15289779847203674 valid 0.24670572072267533
LOSS train 0.15289779847203674 valid 0.24700393682659263
LOSS train 0.15289779847203674 valid 0.2473494200145497
LOSS train 0.15289779847203674 valid 0.24720487316835274
LOSS train 0.15289779847203674 valid 0.2471709164002767
LOSS train 0.15289779847203674 valid 0.2472332084462756
LOSS train 0.15289779847203674 valid 0.24762868361090715
LOSS train 0.15289779847203674 valid 0.24738585935017773
LOSS train 0.15289779847203674 valid 0.2475492699002778
LOSS train 0.15289779847203674 valid 0.24793255889634475
LOSS train 0.15289779847203674 valid 0.2483249933882193
LOSS train 0.15289779847203674 valid 0.24813792874684204
LOSS train 0.15289779847203674 valid 0.24782872346362897
LOSS train 0.15289779847203674 valid 0.24796989124960603
LOSS train 0.15289779847203674 valid 0.24799510954242004
LOSS train 0.15289779847203674 valid 0.2479710518018059
LOSS train 0.15289779847203674 valid 0.24830322237364177
LOSS train 0.15289779847203674 valid 0.24850423965189192
LOSS train 0.15289779847203674 valid 0.2483640236117072
LOSS train 0.15289779847203674 valid 0.24824202273573195
LOSS train 0.15289779847203674 valid 0.24802611581981182
LOSS train 0.15289779847203674 valid 0.24777145634505374
LOSS train 0.15289779847203674 valid 0.24764091826853205
LOSS train 0.15289779847203674 valid 0.2477847810198621
LOSS train 0.15289779847203674 valid 0.24796120822429657
LOSS train 0.15289779847203674 valid 0.24796737420558929
LOSS train 0.15289779847203674 valid 0.2480356465019877
LOSS train 0.15289779847203674 valid 0.24795255592958196
LOSS train 0.15289779847203674 valid 0.24812390573788434
LOSS train 0.15289779847203674 valid 0.24827189004236414
LOSS train 0.15289779847203674 valid 0.24824150537069029
LOSS train 0.15289779847203674 valid 0.24806335079305955
LOSS train 0.15289779847203674 valid 0.24778939907749495
LOSS train 0.15289779847203674 valid 0.24748746221675014
LOSS train 0.15289779847203674 valid 0.24754133186678387
LOSS train 0.15289779847203674 valid 0.24754319963631807
LOSS train 0.15289779847203674 valid 0.24751961844808915
LOSS train 0.15289779847203674 valid 0.2473837279707846
LOSS train 0.15289779847203674 valid 0.24738411335409552
LOSS train 0.15289779847203674 valid 0.2471688611687516
LOSS train 0.15289779847203674 valid 0.24726568620119777
LOSS train 0.15289779847203674 valid 0.24736525689033753
LOSS train 0.15289779847203674 valid 0.24752631374228168
LOSS train 0.15289779847203674 valid 0.2475121912214306
LOSS train 0.15289779847203674 valid 0.24743924838387304
LOSS train 0.15289779847203674 valid 0.24718621660923135
LOSS train 0.15289779847203674 valid 0.24753817181064658
LOSS train 0.15289779847203674 valid 0.24732721814898406
LOSS train 0.15289779847203674 valid 0.24819332935117386
LOSS train 0.15289779847203674 valid 0.24819767025093104
LOSS train 0.15289779847203674 valid 0.2481788592537244
LOSS train 0.15289779847203674 valid 0.2481953591503055
LOSS train 0.15289779847203674 valid 0.2481281703436061
LOSS train 0.15289779847203674 valid 0.24817055785188488
LOSS train 0.15289779847203674 valid 0.2482143614392776
LOSS train 0.15289779847203674 valid 0.24821415437806038
LOSS train 0.15289779847203674 valid 0.24859628931452066
LOSS train 0.15289779847203674 valid 0.24859066990910061
LOSS train 0.15289779847203674 valid 0.24870963630419743
LOSS train 0.15289779847203674 valid 0.2487411509332417
LOSS train 0.15289779847203674 valid 0.248634862806648
LOSS train 0.15289779847203674 valid 0.2485592653107199
LOSS train 0.15289779847203674 valid 0.24846153641924446
LOSS train 0.15289779847203674 valid 0.24827560087654488
LOSS train 0.15289779847203674 valid 0.24807974496265736
LOSS train 0.15289779847203674 valid 0.24789380142183015
LOSS train 0.15289779847203674 valid 0.24796619365014225
LOSS train 0.15289779847203674 valid 0.2481510614206691
LOSS train 0.15289779847203674 valid 0.24815043753811292
LOSS train 0.15289779847203674 valid 0.24835583867406
LOSS train 0.15289779847203674 valid 0.2484500551924986
LOSS train 0.15289779847203674 valid 0.24839709385445244
LOSS train 0.15289779847203674 valid 0.24817556246768596
LOSS train 0.15289779847203674 valid 0.2482133468106992
LOSS train 0.15289779847203674 valid 0.24815745206399895
LOSS train 0.15289779847203674 valid 0.24808869097914016
LOSS train 0.15289779847203674 valid 0.24798614701086824
LOSS train 0.15289779847203674 valid 0.24803636235705875
LOSS train 0.15289779847203674 valid 0.24828684480672472
LOSS train 0.15289779847203674 valid 0.24825355169160407
LOSS train 0.15289779847203674 valid 0.24829371861285635
LOSS train 0.15289779847203674 valid 0.24844881472337313
LOSS train 0.15289779847203674 valid 0.24852089919559248
LOSS train 0.15289779847203674 valid 0.24840166668097177
LOSS train 0.15289779847203674 valid 0.2483325690516959
LOSS train 0.15289779847203674 valid 0.24829336171214644
LOSS train 0.15289779847203674 valid 0.2482437202206222
LOSS train 0.15289779847203674 valid 0.2480149218902231
LOSS train 0.15289779847203674 valid 0.24800522871156958
LOSS train 0.15289779847203674 valid 0.24806483988724057
LOSS train 0.15289779847203674 valid 0.2481029834402235
LOSS train 0.15289779847203674 valid 0.24798619021607943
LOSS train 0.15289779847203674 valid 0.24785187390322486
LOSS train 0.15289779847203674 valid 0.2478689816949281
LOSS train 0.15289779847203674 valid 0.24772954540154368
LOSS train 0.15289779847203674 valid 0.24751322300006182
LOSS train 0.15289779847203674 valid 0.24758528112148753
LOSS train 0.15289779847203674 valid 0.2476794807438923
LOSS train 0.15289779847203674 valid 0.24758726516456314
LOSS train 0.15289779847203674 valid 0.2475692191764937
LOSS train 0.15289779847203674 valid 0.2474427368491888
LOSS train 0.15289779847203674 valid 0.2473382515397238
LOSS train 0.15289779847203674 valid 0.24730133184111944
LOSS train 0.15289779847203674 valid 0.24726100574279652
LOSS train 0.15289779847203674 valid 0.24732041263989374
LOSS train 0.15289779847203674 valid 0.24719362898570735
LOSS train 0.15289779847203674 valid 0.2472252702539407
LOSS train 0.15289779847203674 valid 0.2471132816179939
LOSS train 0.15289779847203674 valid 0.24709960008756474
LOSS train 0.15289779847203674 valid 0.24688806723464618
LOSS train 0.15289779847203674 valid 0.2468949956553323
LOSS train 0.15289779847203674 valid 0.24686762655233319
LOSS train 0.15289779847203674 valid 0.24667612906053382
LOSS train 0.15289779847203674 valid 0.24673671852534926
LOSS train 0.15289779847203674 valid 0.24663460108442842
LOSS train 0.15289779847203674 valid 0.2465390515188838
LOSS train 0.15289779847203674 valid 0.24639617030819258
LOSS train 0.15289779847203674 valid 0.2463298239466232
LOSS train 0.15289779847203674 valid 0.24631639672528713
LOSS train 0.15289779847203674 valid 0.24637239422972343
LOSS train 0.15289779847203674 valid 0.24640971381555904
LOSS train 0.15289779847203674 valid 0.24642835766481597
LOSS train 0.15289779847203674 valid 0.2464372617033151
LOSS train 0.15289779847203674 valid 0.24653274696236768
LOSS train 0.15289779847203674 valid 0.2466503883978086
LOSS train 0.15289779847203674 valid 0.24669476317034827
LOSS train 0.15289779847203674 valid 0.24677074964331314
LOSS train 0.15289779847203674 valid 0.2469950710230462
LOSS train 0.15289779847203674 valid 0.24708642255057367
LOSS train 0.15289779847203674 valid 0.2470579997811255
LOSS train 0.15289779847203674 valid 0.2471755807166514
LOSS train 0.15289779847203674 valid 0.24725961059460907
LOSS train 0.15289779847203674 valid 0.2473506816373817
LOSS train 0.15289779847203674 valid 0.24721204281620712
LOSS train 0.15289779847203674 valid 0.24732950501717055
LOSS train 0.15289779847203674 valid 0.24745708672290154
LOSS train 0.15289779847203674 valid 0.2474100047375186
LOSS train 0.15289779847203674 valid 0.2473766392544855
LOSS train 0.15289779847203674 valid 0.24731667953379013
LOSS train 0.15289779847203674 valid 0.24711309566657413
LOSS train 0.15289779847203674 valid 0.2470786202698946
LOSS train 0.15289779847203674 valid 0.2471542869118734
LOSS train 0.15289779847203674 valid 0.2469860061017935
LOSS train 0.15289779847203674 valid 0.24715728125699754
LOSS train 0.15289779847203674 valid 0.247298783515809
LOSS train 0.15289779847203674 valid 0.2473605466740472
LOSS train 0.15289779847203674 valid 0.24732177462277374
LOSS train 0.15289779847203674 valid 0.2474196302866646
LOSS train 0.15289779847203674 valid 0.24737070807285846
LOSS train 0.15289779847203674 valid 0.24728330957841682
LOSS train 0.15289779847203674 valid 0.24731292521953582
LOSS train 0.15289779847203674 valid 0.24720453669825398
LOSS train 0.15289779847203674 valid 0.24725945305729669
LOSS train 0.15289779847203674 valid 0.2471343036225662
LOSS train 0.15289779847203674 valid 0.24712247143345556
LOSS train 0.15289779847203674 valid 0.24717420532422907
LOSS train 0.15289779847203674 valid 0.24714429018786177
LOSS train 0.15289779847203674 valid 0.24702019061791758
LOSS train 0.15289779847203674 valid 0.24710693683966187
LOSS train 0.15289779847203674 valid 0.24706895137385512
LOSS train 0.15289779847203674 valid 0.24712969718071132
LOSS train 0.15289779847203674 valid 0.24723702714817733
LOSS train 0.15289779847203674 valid 0.2472375258234621
LOSS train 0.15289779847203674 valid 0.24723089772485507
LOSS train 0.15289779847203674 valid 0.2472493373083346
LOSS train 0.15289779847203674 valid 0.24726687928415694
LOSS train 0.15289779847203674 valid 0.24722566802922943
LOSS train 0.15289779847203674 valid 0.24728869683063878
LOSS train 0.15289779847203674 valid 0.24737539003366854
LOSS train 0.15289779847203674 valid 0.2474569201358632
LOSS train 0.15289779847203674 valid 0.2474212819227466
LOSS train 0.15289779847203674 valid 0.24757689386056358
LOSS train 0.15289779847203674 valid 0.2477969503687585
LOSS train 0.15289779847203674 valid 0.24791344427145445
LOSS train 0.15289779847203674 valid 0.24798271783294468
LOSS train 0.15289779847203674 valid 0.24796593048355797
LOSS train 0.15289779847203674 valid 0.24795471245180006
LOSS train 0.15289779847203674 valid 0.2478180125839874
LOSS train 0.15289779847203674 valid 0.2476978666490788
LOSS train 0.15289779847203674 valid 0.24771281318425278
LOSS train 0.15289779847203674 valid 0.24767793656459877
LOSS train 0.15289779847203674 valid 0.24761999151884873
LOSS train 0.15289779847203674 valid 0.2474384883828197
LOSS train 0.15289779847203674 valid 0.2475346248899669
LOSS train 0.15289779847203674 valid 0.24763141782350942
LOSS train 0.15289779847203674 valid 0.24769641160964967
LOSS train 0.15289779847203674 valid 0.24764299158241365
LOSS train 0.15289779847203674 valid 0.247532771751025
LOSS train 0.15289779847203674 valid 0.24749928671452734
LOSS train 0.15289779847203674 valid 0.2475060156472414
LOSS train 0.15289779847203674 valid 0.24750489669627157
LOSS train 0.15289779847203674 valid 0.24736928586492835
LOSS train 0.15289779847203674 valid 0.2473357878625393
LOSS train 0.15289779847203674 valid 0.24732183392951107
LOSS train 0.15289779847203674 valid 0.24735940060242503
LOSS train 0.15289779847203674 valid 0.24746977474729895
LOSS train 0.15289779847203674 valid 0.24743994320365223
LOSS train 0.15289779847203674 valid 0.2475126070105267
LOSS train 0.15289779847203674 valid 0.24751816754973174
LOSS train 0.15289779847203674 valid 0.24759861044461112
LOSS train 0.15289779847203674 valid 0.24765186473727227
LOSS train 0.15289779847203674 valid 0.24766409966438713
LOSS train 0.15289779847203674 valid 0.24765796377169375
LOSS train 0.15289779847203674 valid 0.24777650292163636
LOSS train 0.15289779847203674 valid 0.24776055237376376
LOSS train 0.15289779847203674 valid 0.24771486994672995
LOSS train 0.15289779847203674 valid 0.24769654207759434
LOSS train 0.15289779847203674 valid 0.24765892278293833
LOSS train 0.15289779847203674 valid 0.24755054044646102
LOSS train 0.15289779847203674 valid 0.24769655313692432
LOSS train 0.15289779847203674 valid 0.2476326410808871
LOSS train 0.15289779847203674 valid 0.24749705530822852
LOSS train 0.15289779847203674 valid 0.2475357540907004
LOSS train 0.15289779847203674 valid 0.2475924013902585
LOSS train 0.15289779847203674 valid 0.24773425765477927
LOSS train 0.15289779847203674 valid 0.24772402038649907
LOSS train 0.15289779847203674 valid 0.2476347779642932
LOSS train 0.15289779847203674 valid 0.24768537278243044
LOSS train 0.15289779847203674 valid 0.2477165769387341
LOSS train 0.15289779847203674 valid 0.24778551970342858
LOSS train 0.15289779847203674 valid 0.24767631888389588
LOSS train 0.15289779847203674 valid 0.24777964277430858
LOSS train 0.15289779847203674 valid 0.24776100001720167
LOSS train 0.15289779847203674 valid 0.24775317431234353
LOSS train 0.15289779847203674 valid 0.2477853117901602
LOSS train 0.15289779847203674 valid 0.2477904051542282
LOSS train 0.15289779847203674 valid 0.2479604664115818
LOSS train 0.15289779847203674 valid 0.24808783291494446
LOSS train 0.15289779847203674 valid 0.24802817826772608
LOSS train 0.15289779847203674 valid 0.24814877471119437
LOSS train 0.15289779847203674 valid 0.2480807413205956
LOSS train 0.15289779847203674 valid 0.2479965354110539
LOSS train 0.15289779847203674 valid 0.247903416269874
LOSS train 0.15289779847203674 valid 0.247900603232799
LOSS train 0.15289779847203674 valid 0.2480896217023541
LOSS train 0.15289779847203674 valid 0.24810275577787144
LOSS train 0.15289779847203674 valid 0.24813742474431083
LOSS train 0.15289779847203674 valid 0.24808219619073216
LOSS train 0.15289779847203674 valid 0.2480266867040177
LOSS train 0.15289779847203674 valid 0.24806443555692656
LOSS train 0.15289779847203674 valid 0.24809417387141902
LOSS train 0.15289779847203674 valid 0.24798487568181282
LOSS train 0.15289779847203674 valid 0.24795946348131748
LOSS train 0.15289779847203674 valid 0.24799715247515686
LOSS train 0.15289779847203674 valid 0.24817863176035326
LOSS train 0.15289779847203674 valid 0.24821336839510047
LOSS train 0.15289779847203674 valid 0.24820224032064395
LOSS train 0.15289779847203674 valid 0.24805242673464398
LOSS train 0.15289779847203674 valid 0.24798806015958733
LOSS train 0.15289779847203674 valid 0.24805924941099816
LOSS train 0.15289779847203674 valid 0.24799708813428878
LOSS train 0.15289779847203674 valid 0.24790764873863286
LOSS train 0.15289779847203674 valid 0.24791718426753173
LOSS train 0.15289779847203674 valid 0.24800132801782646
LOSS train 0.15289779847203674 valid 0.24811934768143346
LOSS train 0.15289779847203674 valid 0.24827719045356966
LOSS train 0.15289779847203674 valid 0.24832109160972446
LOSS train 0.15289779847203674 valid 0.24832273982152217
LOSS train 0.15289779847203674 valid 0.2482340633036704
LOSS train 0.15289779847203674 valid 0.24826704501109534
LOSS train 0.15289779847203674 valid 0.24820624796880617
LOSS train 0.15289779847203674 valid 0.24813583716130982
LOSS train 0.15289779847203674 valid 0.24825963460279432
LOSS train 0.15289779847203674 valid 0.24817299432334164
LOSS train 0.15289779847203674 valid 0.2481980679126886
LOSS train 0.15289779847203674 valid 0.24815166355812387
LOSS train 0.15289779847203674 valid 0.248048512784184
LOSS train 0.15289779847203674 valid 0.2479376094542667
LOSS train 0.15289779847203674 valid 0.24790731926813073
LOSS train 0.15289779847203674 valid 0.2479900473420859
EPOCH 7:
  batch 1 loss: 0.18053816258907318
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.17925607413053513
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.18635016679763794
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.19047603011131287
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.19836429357528687
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.20212879528601965
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.19386074585574015
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.1914005484431982
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.18835721247726017
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.18670469373464585
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.18456710333173926
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.18392708649237952
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.18366515865692726
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.18351336781467711
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.18331601321697236
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.18455928284674883
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.18512609776328592
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.1870720088481903
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.18625282927563316
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.18460495322942733
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.18422922846816836
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.18401964821598746
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.18415162809517072
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.18419294121364752
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.18458260118961334
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.18519665358158258
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.18559437493483225
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.18548859070454324
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.1843604301584178
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.18501261522372564
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.1853154056495236
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.18566006189212203
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.18463142306515665
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.18532526273937786
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.18646334878035953
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.18552414948741594
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.18533765464215665
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.184922430468233
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.18480757451974428
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.18493376895785332
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.1845506916685802
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.18476806722936176
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.18529277520124302
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.1845025433735414
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.18502414094077216
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.18413813185432684
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.18323459745721615
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.1827187662323316
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.18252463760424634
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.18242378443479537
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.18211493451221317
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.18209567494117296
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.18201806410303656
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.18221727786240755
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.18182409676638517
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.18167258240282536
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.18157163053228145
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.18079160821848902
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.18096863895149554
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.18065760682026547
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.18067114763572567
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.1802857921969506
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.1799117558532291
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.17988884239457548
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.17965593750660236
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.17927943621620987
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.17890731456564435
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.17879589778535507
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.17876633699389471
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.1789081396801131
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.17830740209196655
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.17831444388462436
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.17803695879570425
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.17803366804445112
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.1778246215979258
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.17763929598425565
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.1773215015600254
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.17684310655563307
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.1769009530544281
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.17706495113670825
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.17705023031175873
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.1764676636312066
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.17605937317193274
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.1762410861750444
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.1760475507553886
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.17594726799532426
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.17596025559408912
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.17605271410535683
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.17597148893924242
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.17615387903319465
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.17597351247792717
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.17572269216179848
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.176084434954069
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.1761852724755064
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.1764978422930366
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.176439354972293
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.1761503491512279
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.17603485620751672
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.17585698914046238
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.17564908429980278
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.17551658900067357
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.17532742797744041
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.17487538468490527
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.17490671173884317
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.17438714184931348
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.17448529298856574
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.17456063788349382
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.1743413428603499
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.17424985718563063
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.1744748484681953
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.1741620599552318
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.17367637669667602
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.17324469376981785
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.17324048555211016
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.17282647607119186
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.17265038384959616
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.17282504531053397
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.17270207089387765
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.17261444720901362
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.17270035942395529
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.17237008332220977
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.172322650302629
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.17259125302477582
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.17243590109771298
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.17219683039188385
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.17222943535399815
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.17213697602429728
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.17205136362463236
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.17202071845531464
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.1719003547842686
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.17168244219008294
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.17156022053324815
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.17178447992729962
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.17166384910024815
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.17132287687725492
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.17122331612250385
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.17124640245507233
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.17115101822908374
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.17134254109516417
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.17147066933768138
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.1715046556283396
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.17138938332947207
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.17143931453461414
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.17164727290057474
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.1714543132946409
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.1714239409320975
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.17142484676675732
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.17139121004053065
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.17150036160577864
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.17150000820557276
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.1713619866710625
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.17119329844258332
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.17121037305180542
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.1710012759287636
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.17097983956336976
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.1707567396836403
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.17070677155142377
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.17061868819254863
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.1706242484491576
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.17065562177449464
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.17046180626620416
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.1703034370769689
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.17032219542681806
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.17018048210841855
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.17021099193529649
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.17017109988324614
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.1699566189757364
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.16987484019427074
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.16961165469073686
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.1695959921268856
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.16946629003474586
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.16941224333158758
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.16924884846444763
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.16913853277420177
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.16924944494451796
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.16910731529986317
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.16908247869903759
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.16890706942322548
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.16901073467465086
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.1688512935406632
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.16878276997508265
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.16863269686371415
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.16840941642151505
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.1682924987019404
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.16820770308778094
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.16811175452124688
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.1680545418020238
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.1678789899228735
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.1677953633208754
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.16782153295843225
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.16776158368088187
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.1676721058320254
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.1676609713024426
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.16750505121098352
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.16749144280568148
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.16750987520327373
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.1676372153504851
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.16775396646875324
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.16775288226915963
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.16776057429611682
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.16765770345778014
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.16760505758004612
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.1675235183514985
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.16750513995979346
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.16732873182471206
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.16718431178805898
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.16709417961357872
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.16692699384517395
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.16692316282594033
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.1670507482119969
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.16702621469000503
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.16708511346072522
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.16711552434124297
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.16694697890883295
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.16702894554581754
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.16685814148298017
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.16689621214218403
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.1668748582858558
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.1669388139901096
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.16692945544015278
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.1668005549260394
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.16672867764760782
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.16657688104518326
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.16668189064200437
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.16654092424445682
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.16649687389093162
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.16647932924625633
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.16625967340772613
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.16610541959256583
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.16616064640490905
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.16625071674972386
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.16617026526866288
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.16607582370866522
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.16604888821259522
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.16605400132372025
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.1659379383643805
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.16590851583058322
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.1658513279897826
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.16581791764023912
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.1657858991374572
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.165846742967847
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.16576092610181856
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.16571002628332304
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.16563253923029195
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.1656315673978961
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.16562193722986593
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.16546140532744558
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.16553741639419908
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.16554389133989572
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.16555992931127547
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.16570151511179024
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.16560885895575797
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.16555669992101993
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.16551638899121698
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.16554901623258403
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.165491814550478
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.16539405112368588
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.16527060529058293
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.16531806447791317
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.16528244018554689
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.16516937138477047
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.16507588958239738
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.16509264679236105
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.16493426497574104
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.16493260160369694
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.1648953643004249
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.16485124762250242
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.16484766361762337
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.16473616330043092
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.16472024440213487
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.16464337863275486
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.16458570751745036
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.16478301145327398
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.16463999306089686
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.16455062381245872
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.16470760034154291
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.16464774991093128
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.16457721936724168
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.16459128663851796
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.1646731836721301
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.164579554411228
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.16451687014377708
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.16454134978906848
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.16451340627817201
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.16458763759910014
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.1645849111353184
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.16450231418480857
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.16451983166755074
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.16441779238233103
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.1644535956968521
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.16440495513251557
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.16437423277697336
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.16444999388968987
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.1645176583761666
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.1645044061844632
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.16452561698954654
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.16459899922513
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.16446907285775914
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.16430742044412971
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.1642752378930648
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.16434162421975024
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.16431886991424277
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.16429033927103082
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.16425049400545264
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.1642883874109534
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.16425460165524794
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.1642008004679742
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.1641378239909937
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.1640952186821734
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.16397043291118837
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.16397963318123313
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.16390779846085188
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.16395206699451317
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.16389754863016925
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.16389111741667703
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.16398729504192178
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.16394601430896705
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.1638999752138021
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.16386991040927115
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.16385030404198914
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.16377718368330477
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.16376436457778357
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.1638267664209977
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.1637369864102867
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.16376683365840178
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.1636453169895096
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.16354440362263892
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.16356384922273276
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.16351472960054694
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.1635832080335328
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.16361802636316536
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.1635029366217464
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.16337152268435504
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.16336684566950369
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.1634632604335671
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.16347150792855591
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.16336384535012685
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.1632815956099499
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.16314009930883536
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.16311385202057221
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.16298217702209075
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.16320981584184352
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.16318737467854086
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.16316290154273427
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.16314666428859684
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.163128217413074
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.16309219794160007
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.1630820484467964
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.16302395078292206
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.16293642697589739
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.16291513153885165
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.16297149761918595
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.16300092859156745
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.16302840729470308
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.16304059899608853
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.16298294257833046
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.1630330113928859
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.1630907573804842
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.16308733146419765
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.1629745354462001
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.16291321704674955
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.16291875966428393
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.1628603231849421
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.16277946689381048
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.16280294605722165
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.16272813108205145
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.16276547058970142
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.1627987812111235
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.16277359827827956
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.16267704780440073
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.162652370080633
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.1625833497132345
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.16256241382526648
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.1626061347517419
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.16259785721699396
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.16269846440867541
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.1626394685723421
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.16267735449961884
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.16257881172650093
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.1624596136180978
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.1624022325271071
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.16232233986024458
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.16239135453657444
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.16232752722377577
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.16225265326438013
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.16226114390582
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.1622020906025125
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.1622126791050139
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.16222817705498868
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.16215708202276474
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.16210486082469716
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.1621021218597889
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.16219969829378542
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.1621971642456684
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.16219285028644756
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.16213797950985456
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.16202804393672224
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.16207614241532944
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.1619733911110345
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.1619331531599164
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.16189860598999367
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.1618212800639779
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.1617436294742021
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.16169621149944788
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.16169274033587655
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.16165052893860588
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.16160991150858361
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.16171571652532793
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.16170287992085688
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.16171869406612907
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.16170185673845947
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.16172569364453981
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.16170122324698774
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.1617547490314585
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.16178672942052405
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.16178624074046427
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.16173961837228826
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.1617200307583695
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.16184745704075032
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.16176921008598236
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.161895293804359
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.16194134816456746
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.16185036301612854
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.16181498832719507
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.16179928208098693
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.16173169506547597
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.1617050391449582
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.16164805033357343
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.16157821161207897
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.16154241229212563
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.1615589439108311
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.16149546119763894
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.16143098067466435
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.16148189362567691
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.16140143867881818
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.16148903019247798
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.16150820248459788
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.16146019611456622
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.16135932209823017
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.1613112696531144
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.1613059040843224
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.16125663787935654
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.16123312049068123
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.16121102547323382
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.16122439131977853
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.16122289035352355
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.16119849721857366
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.16127904544451407
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.16126827749616585
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.16121447526746327
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.1612167178328973
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.1612273986756274
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.16118209317293652
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.1611845723070237
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.1611289070202754
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.16118658974505307
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.16114928819548902
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.16115170417914745
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.16114569210278962
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.16120326292255652
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.16116817476563236
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.16111946180140302
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.1610199983189482
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.1610412154154017
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.16106312963911282
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.16102014405277154
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.1610117029773602
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.16108403450403458
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.1611244588899714
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.16118049608900192
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.16117537224115586
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.1612734042613183
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.1612734042613183 valid 0.2110176980495453
LOSS train 0.1612734042613183 valid 0.1867867261171341
LOSS train 0.1612734042613183 valid 0.19076562921206155
LOSS train 0.1612734042613183 valid 0.17927595973014832
LOSS train 0.1612734042613183 valid 0.1729758769273758
LOSS train 0.1612734042613183 valid 0.1847591126958529
LOSS train 0.1612734042613183 valid 0.19312578865459987
LOSS train 0.1612734042613183 valid 0.19198800809681416
LOSS train 0.1612734042613183 valid 0.19203633897834355
LOSS train 0.1612734042613183 valid 0.19466110914945603
LOSS train 0.1612734042613183 valid 0.1937076679684899
LOSS train 0.1612734042613183 valid 0.19405212253332138
LOSS train 0.1612734042613183 valid 0.19413728553515214
LOSS train 0.1612734042613183 valid 0.19298656710556575
LOSS train 0.1612734042613183 valid 0.19124667843182883
LOSS train 0.1612734042613183 valid 0.1929110698401928
LOSS train 0.1612734042613183 valid 0.19334996535497553
LOSS train 0.1612734042613183 valid 0.1928651680548986
LOSS train 0.1612734042613183 valid 0.1954229270157061
LOSS train 0.1612734042613183 valid 0.19622377455234527
LOSS train 0.1612734042613183 valid 0.1942655344804128
LOSS train 0.1612734042613183 valid 0.19321893832900308
LOSS train 0.1612734042613183 valid 0.19259359655172928
LOSS train 0.1612734042613183 valid 0.19269989741345248
LOSS train 0.1612734042613183 valid 0.1915779459476471
LOSS train 0.1612734042613183 valid 0.19245598637140715
LOSS train 0.1612734042613183 valid 0.19225999381807116
LOSS train 0.1612734042613183 valid 0.1924101486802101
LOSS train 0.1612734042613183 valid 0.19292062983430666
LOSS train 0.1612734042613183 valid 0.19407457411289214
LOSS train 0.1612734042613183 valid 0.19499620410703844
LOSS train 0.1612734042613183 valid 0.19452301040291786
LOSS train 0.1612734042613183 valid 0.19473218014746002
LOSS train 0.1612734042613183 valid 0.1948777551160139
LOSS train 0.1612734042613183 valid 0.19662701742989677
LOSS train 0.1612734042613183 valid 0.19668578397896555
LOSS train 0.1612734042613183 valid 0.19705681663912697
LOSS train 0.1612734042613183 valid 0.1980622351953858
LOSS train 0.1612734042613183 valid 0.1967458801391797
LOSS train 0.1612734042613183 valid 0.19626343324780465
LOSS train 0.1612734042613183 valid 0.19684582439864554
LOSS train 0.1612734042613183 valid 0.19751130647602536
LOSS train 0.1612734042613183 valid 0.1974859670844189
LOSS train 0.1612734042613183 valid 0.1983963389965621
LOSS train 0.1612734042613183 valid 0.1980284892850452
LOSS train 0.1612734042613183 valid 0.19866513363693072
LOSS train 0.1612734042613183 valid 0.1988080137587608
LOSS train 0.1612734042613183 valid 0.19855083369960388
LOSS train 0.1612734042613183 valid 0.19892988369172934
LOSS train 0.1612734042613183 valid 0.1982554563879967
LOSS train 0.1612734042613183 valid 0.1985090897948134
LOSS train 0.1612734042613183 valid 0.19835384362019026
LOSS train 0.1612734042613183 valid 0.1990193842716937
LOSS train 0.1612734042613183 valid 0.19880360116561255
LOSS train 0.1612734042613183 valid 0.19889582720669832
LOSS train 0.1612734042613183 valid 0.19839056581258774
LOSS train 0.1612734042613183 valid 0.19856827823739304
LOSS train 0.1612734042613183 valid 0.19819657555941878
LOSS train 0.1612734042613183 valid 0.19865877067638657
LOSS train 0.1612734042613183 valid 0.19841497763991356
LOSS train 0.1612734042613183 valid 0.19856168549568926
LOSS train 0.1612734042613183 valid 0.19936463669423135
LOSS train 0.1612734042613183 valid 0.1991736926729717
LOSS train 0.1612734042613183 valid 0.19984454615041614
LOSS train 0.1612734042613183 valid 0.20022148581651542
LOSS train 0.1612734042613183 valid 0.200121095912023
LOSS train 0.1612734042613183 valid 0.19982910111768923
LOSS train 0.1612734042613183 valid 0.19971540197730064
LOSS train 0.1612734042613183 valid 0.1989798593348351
LOSS train 0.1612734042613183 valid 0.19929515974862236
LOSS train 0.1612734042613183 valid 0.19923832655792506
LOSS train 0.1612734042613183 valid 0.19944297087689242
LOSS train 0.1612734042613183 valid 0.19980200451530822
LOSS train 0.1612734042613183 valid 0.19957086725814924
LOSS train 0.1612734042613183 valid 0.19952050010363262
LOSS train 0.1612734042613183 valid 0.19998007581422203
LOSS train 0.1612734042613183 valid 0.19986628092728653
LOSS train 0.1612734042613183 valid 0.19980977074458048
LOSS train 0.1612734042613183 valid 0.19940761335288423
LOSS train 0.1612734042613183 valid 0.19857528824359177
LOSS train 0.1612734042613183 valid 0.19812875820530784
LOSS train 0.1612734042613183 valid 0.19838547361333195
LOSS train 0.1612734042613183 valid 0.19800265151334095
LOSS train 0.1612734042613183 valid 0.19778977671549433
LOSS train 0.1612734042613183 valid 0.1973239383276771
LOSS train 0.1612734042613183 valid 0.19688995962226113
LOSS train 0.1612734042613183 valid 0.1969538682151115
LOSS train 0.1612734042613183 valid 0.19671849500049243
LOSS train 0.1612734042613183 valid 0.1968386899889185
LOSS train 0.1612734042613183 valid 0.1971110141939587
LOSS train 0.1612734042613183 valid 0.19746708067563865
LOSS train 0.1612734042613183 valid 0.19739233880587245
LOSS train 0.1612734042613183 valid 0.19726735914266239
LOSS train 0.1612734042613183 valid 0.19740801193612686
LOSS train 0.1612734042613183 valid 0.1969185528002287
LOSS train 0.1612734042613183 valid 0.19678689974049726
LOSS train 0.1612734042613183 valid 0.19686672106846093
LOSS train 0.1612734042613183 valid 0.19694754374878748
LOSS train 0.1612734042613183 valid 0.19740572707219559
LOSS train 0.1612734042613183 valid 0.19763901069760323
LOSS train 0.1612734042613183 valid 0.19777173379270158
LOSS train 0.1612734042613183 valid 0.1982067925672905
LOSS train 0.1612734042613183 valid 0.19815337976205696
LOSS train 0.1612734042613183 valid 0.19830508042986578
LOSS train 0.1612734042613183 valid 0.19823743857088544
LOSS train 0.1612734042613183 valid 0.1987150453452794
LOSS train 0.1612734042613183 valid 0.19853830100776992
LOSS train 0.1612734042613183 valid 0.19886219170358446
LOSS train 0.1612734042613183 valid 0.19920529712230786
LOSS train 0.1612734042613183 valid 0.19934860332445664
LOSS train 0.1612734042613183 valid 0.1993821743909303
LOSS train 0.1612734042613183 valid 0.19904733382697617
LOSS train 0.1612734042613183 valid 0.19926356306645723
LOSS train 0.1612734042613183 valid 0.1994535906011598
LOSS train 0.1612734042613183 valid 0.19945145664007768
LOSS train 0.1612734042613183 valid 0.19983347710864297
LOSS train 0.1612734042613183 valid 0.199986480240129
LOSS train 0.1612734042613183 valid 0.1997294428489976
LOSS train 0.1612734042613183 valid 0.19952661324949825
LOSS train 0.1612734042613183 valid 0.1993482556194067
LOSS train 0.1612734042613183 valid 0.19914195335600987
LOSS train 0.1612734042613183 valid 0.19906956751327046
LOSS train 0.1612734042613183 valid 0.1992497772462969
LOSS train 0.1612734042613183 valid 0.1993332375441828
LOSS train 0.1612734042613183 valid 0.19919486367702485
LOSS train 0.1612734042613183 valid 0.199457701473009
LOSS train 0.1612734042613183 valid 0.19928599976179168
LOSS train 0.1612734042613183 valid 0.19930496695451438
LOSS train 0.1612734042613183 valid 0.19943054366943447
LOSS train 0.1612734042613183 valid 0.19939388174277087
LOSS train 0.1612734042613183 valid 0.19910230561522127
LOSS train 0.1612734042613183 valid 0.19882560487497936
LOSS train 0.1612734042613183 valid 0.19862890265938035
LOSS train 0.1612734042613183 valid 0.19868147562244046
LOSS train 0.1612734042613183 valid 0.1986599166084219
LOSS train 0.1612734042613183 valid 0.19877643006689408
LOSS train 0.1612734042613183 valid 0.1986095825903607
LOSS train 0.1612734042613183 valid 0.19853901647139285
LOSS train 0.1612734042613183 valid 0.19826569863789373
LOSS train 0.1612734042613183 valid 0.19836955943277904
LOSS train 0.1612734042613183 valid 0.19833555268057695
LOSS train 0.1612734042613183 valid 0.1983169934279482
LOSS train 0.1612734042613183 valid 0.19825875415251806
LOSS train 0.1612734042613183 valid 0.19821524319963324
LOSS train 0.1612734042613183 valid 0.1980913080018142
LOSS train 0.1612734042613183 valid 0.19840880697720673
LOSS train 0.1612734042613183 valid 0.19825459266600964
LOSS train 0.1612734042613183 valid 0.1991391126569864
LOSS train 0.1612734042613183 valid 0.19916863019434397
LOSS train 0.1612734042613183 valid 0.1990645838777224
LOSS train 0.1612734042613183 valid 0.199222870911194
LOSS train 0.1612734042613183 valid 0.1991265279878127
LOSS train 0.1612734042613183 valid 0.1991108354010613
LOSS train 0.1612734042613183 valid 0.19911146444546712
LOSS train 0.1612734042613183 valid 0.19905626610402138
LOSS train 0.1612734042613183 valid 0.19929160454716438
LOSS train 0.1612734042613183 valid 0.19941195921533428
LOSS train 0.1612734042613183 valid 0.19948100572145439
LOSS train 0.1612734042613183 valid 0.19953146248868425
LOSS train 0.1612734042613183 valid 0.19940712256357074
LOSS train 0.1612734042613183 valid 0.19931604662296934
LOSS train 0.1612734042613183 valid 0.19912253090260942
LOSS train 0.1612734042613183 valid 0.19892459413025276
LOSS train 0.1612734042613183 valid 0.19884976562930318
LOSS train 0.1612734042613183 valid 0.1987303058306376
LOSS train 0.1612734042613183 valid 0.1987621204321643
LOSS train 0.1612734042613183 valid 0.19906208142192064
LOSS train 0.1612734042613183 valid 0.1991548297838086
LOSS train 0.1612734042613183 valid 0.1993558643132272
LOSS train 0.1612734042613183 valid 0.19934725007590126
LOSS train 0.1612734042613183 valid 0.19923718779059182
LOSS train 0.1612734042613183 valid 0.19900329437020214
LOSS train 0.1612734042613183 valid 0.1991619168850728
LOSS train 0.1612734042613183 valid 0.19911100073107357
LOSS train 0.1612734042613183 valid 0.1990201665673937
LOSS train 0.1612734042613183 valid 0.19891615135764534
LOSS train 0.1612734042613183 valid 0.19903455186361646
LOSS train 0.1612734042613183 valid 0.19920488631122568
LOSS train 0.1612734042613183 valid 0.19911640890816737
LOSS train 0.1612734042613183 valid 0.19909565150737762
LOSS train 0.1612734042613183 valid 0.19925210801935986
LOSS train 0.1612734042613183 valid 0.19920245361524624
LOSS train 0.1612734042613183 valid 0.1991639086783258
LOSS train 0.1612734042613183 valid 0.19902762916425001
LOSS train 0.1612734042613183 valid 0.19898255378813356
LOSS train 0.1612734042613183 valid 0.19897841950578074
LOSS train 0.1612734042613183 valid 0.19874387796231133
LOSS train 0.1612734042613183 valid 0.19876703881519905
LOSS train 0.1612734042613183 valid 0.1987926453036606
LOSS train 0.1612734042613183 valid 0.1988636597206718
LOSS train 0.1612734042613183 valid 0.198806599016589
LOSS train 0.1612734042613183 valid 0.19875040790066123
LOSS train 0.1612734042613183 valid 0.19867771825333333
LOSS train 0.1612734042613183 valid 0.1985403671860695
LOSS train 0.1612734042613183 valid 0.19830170388405138
LOSS train 0.1612734042613183 valid 0.19832236258959285
LOSS train 0.1612734042613183 valid 0.19845374374825336
LOSS train 0.1612734042613183 valid 0.1983390029030617
LOSS train 0.1612734042613183 valid 0.19836545290060378
LOSS train 0.1612734042613183 valid 0.19825599856674672
LOSS train 0.1612734042613183 valid 0.19817667606458142
LOSS train 0.1612734042613183 valid 0.19817450790122004
LOSS train 0.1612734042613183 valid 0.19821481729669524
LOSS train 0.1612734042613183 valid 0.198410645492521
LOSS train 0.1612734042613183 valid 0.19823176926229058
LOSS train 0.1612734042613183 valid 0.1983649327772335
LOSS train 0.1612734042613183 valid 0.19824655778742067
LOSS train 0.1612734042613183 valid 0.19821168023806351
LOSS train 0.1612734042613183 valid 0.19808336208311564
LOSS train 0.1612734042613183 valid 0.1980604935260046
LOSS train 0.1612734042613183 valid 0.19793274045273027
LOSS train 0.1612734042613183 valid 0.19773045568814818
LOSS train 0.1612734042613183 valid 0.19769455863276558
LOSS train 0.1612734042613183 valid 0.19761195862404654
LOSS train 0.1612734042613183 valid 0.19754243631695592
LOSS train 0.1612734042613183 valid 0.19731549960043696
LOSS train 0.1612734042613183 valid 0.1972183941558759
LOSS train 0.1612734042613183 valid 0.19716366079695727
LOSS train 0.1612734042613183 valid 0.1973255899262755
LOSS train 0.1612734042613183 valid 0.19730091792615978
LOSS train 0.1612734042613183 valid 0.1972294989889024
LOSS train 0.1612734042613183 valid 0.19729895587708499
LOSS train 0.1612734042613183 valid 0.19740952404357928
LOSS train 0.1612734042613183 valid 0.1975407026308988
LOSS train 0.1612734042613183 valid 0.19777345094415877
LOSS train 0.1612734042613183 valid 0.1978843182457232
LOSS train 0.1612734042613183 valid 0.19802979402867707
LOSS train 0.1612734042613183 valid 0.1981288234523514
LOSS train 0.1612734042613183 valid 0.19808383181886383
LOSS train 0.1612734042613183 valid 0.19817062376633934
LOSS train 0.1612734042613183 valid 0.19832748890697183
LOSS train 0.1612734042613183 valid 0.198408800787453
LOSS train 0.1612734042613183 valid 0.19836445856247847
LOSS train 0.1612734042613183 valid 0.19853493386608922
LOSS train 0.1612734042613183 valid 0.19863067776598828
LOSS train 0.1612734042613183 valid 0.19853689755171033
LOSS train 0.1612734042613183 valid 0.19844154529430696
LOSS train 0.1612734042613183 valid 0.19838769796515712
LOSS train 0.1612734042613183 valid 0.1982080124038034
LOSS train 0.1612734042613183 valid 0.19820725626001756
LOSS train 0.1612734042613183 valid 0.19833516202774285
LOSS train 0.1612734042613183 valid 0.19818475417608072
LOSS train 0.1612734042613183 valid 0.19838060837223698
LOSS train 0.1612734042613183 valid 0.1985874659702426
LOSS train 0.1612734042613183 valid 0.1985482561345003
LOSS train 0.1612734042613183 valid 0.19845926131659408
LOSS train 0.1612734042613183 valid 0.19859556365109649
LOSS train 0.1612734042613183 valid 0.19860176874264593
LOSS train 0.1612734042613183 valid 0.19856358627717657
LOSS train 0.1612734042613183 valid 0.19851972395181655
LOSS train 0.1612734042613183 valid 0.19834736244849474
LOSS train 0.1612734042613183 valid 0.1984649354976321
LOSS train 0.1612734042613183 valid 0.19834212399282947
LOSS train 0.1612734042613183 valid 0.19829490622432214
LOSS train 0.1612734042613183 valid 0.19833896370495066
LOSS train 0.1612734042613183 valid 0.1983349997899495
LOSS train 0.1612734042613183 valid 0.1981894935731294
LOSS train 0.1612734042613183 valid 0.19837170128905496
LOSS train 0.1612734042613183 valid 0.1983760530308867
LOSS train 0.1612734042613183 valid 0.19836681296045963
LOSS train 0.1612734042613183 valid 0.19848016966348406
LOSS train 0.1612734042613183 valid 0.19851797694013318
LOSS train 0.1612734042613183 valid 0.19846753679754162
LOSS train 0.1612734042613183 valid 0.1985150414773009
LOSS train 0.1612734042613183 valid 0.1985149859819772
LOSS train 0.1612734042613183 valid 0.19847680936406428
LOSS train 0.1612734042613183 valid 0.19858432865321413
LOSS train 0.1612734042613183 valid 0.19871660232988755
LOSS train 0.1612734042613183 valid 0.1988432830594286
LOSS train 0.1612734042613183 valid 0.19880614115132225
LOSS train 0.1612734042613183 valid 0.1989258604735906
LOSS train 0.1612734042613183 valid 0.199201421593042
LOSS train 0.1612734042613183 valid 0.1993546568321221
LOSS train 0.1612734042613183 valid 0.19943298181913194
LOSS train 0.1612734042613183 valid 0.19935647682710128
LOSS train 0.1612734042613183 valid 0.19937992020361667
LOSS train 0.1612734042613183 valid 0.19928469941934523
LOSS train 0.1612734042613183 valid 0.1991140857982121
LOSS train 0.1612734042613183 valid 0.19907473657934469
LOSS train 0.1612734042613183 valid 0.19902421145566873
LOSS train 0.1612734042613183 valid 0.19898069737432691
LOSS train 0.1612734042613183 valid 0.1987237630160988
LOSS train 0.1612734042613183 valid 0.1987917948738004
LOSS train 0.1612734042613183 valid 0.19887487590312958
LOSS train 0.1612734042613183 valid 0.19887033063068724
LOSS train 0.1612734042613183 valid 0.19882803775005406
LOSS train 0.1612734042613183 valid 0.19872471783634676
LOSS train 0.1612734042613183 valid 0.19870991446077824
LOSS train 0.1612734042613183 valid 0.19870479111027964
LOSS train 0.1612734042613183 valid 0.1987070413499043
LOSS train 0.1612734042613183 valid 0.19851611544381303
LOSS train 0.1612734042613183 valid 0.19849458913484666
LOSS train 0.1612734042613183 valid 0.19849841220387013
LOSS train 0.1612734042613183 valid 0.19851848979791006
LOSS train 0.1612734042613183 valid 0.198652592705468
LOSS train 0.1612734042613183 valid 0.19858551256962725
LOSS train 0.1612734042613183 valid 0.19861957124788754
LOSS train 0.1612734042613183 valid 0.1986052261402943
LOSS train 0.1612734042613183 valid 0.19865603032319443
LOSS train 0.1612734042613183 valid 0.1987427641948064
LOSS train 0.1612734042613183 valid 0.19870860186526149
LOSS train 0.1612734042613183 valid 0.19864476694176528
LOSS train 0.1612734042613183 valid 0.1987636397183925
LOSS train 0.1612734042613183 valid 0.19875971173965618
LOSS train 0.1612734042613183 valid 0.19870940349141106
LOSS train 0.1612734042613183 valid 0.19872011906570858
LOSS train 0.1612734042613183 valid 0.19871159926882007
LOSS train 0.1612734042613183 valid 0.19862495749801784
LOSS train 0.1612734042613183 valid 0.19875856992882046
LOSS train 0.1612734042613183 valid 0.19874197456144518
LOSS train 0.1612734042613183 valid 0.198676467181402
LOSS train 0.1612734042613183 valid 0.1986649442368593
LOSS train 0.1612734042613183 valid 0.19878485884529334
LOSS train 0.1612734042613183 valid 0.19883295998072167
LOSS train 0.1612734042613183 valid 0.1987534406166228
LOSS train 0.1612734042613183 valid 0.19869850566492805
LOSS train 0.1612734042613183 valid 0.19874366638412236
LOSS train 0.1612734042613183 valid 0.19876766401641774
LOSS train 0.1612734042613183 valid 0.19883149292401758
LOSS train 0.1612734042613183 valid 0.1987100281752646
LOSS train 0.1612734042613183 valid 0.19886981521811442
LOSS train 0.1612734042613183 valid 0.1989387275639528
LOSS train 0.1612734042613183 valid 0.19893420660643385
LOSS train 0.1612734042613183 valid 0.19905797166772832
LOSS train 0.1612734042613183 valid 0.19901805602587186
LOSS train 0.1612734042613183 valid 0.1991815460971528
LOSS train 0.1612734042613183 valid 0.19931026609665756
LOSS train 0.1612734042613183 valid 0.19917142672873125
LOSS train 0.1612734042613183 valid 0.19932237285611115
LOSS train 0.1612734042613183 valid 0.19929853894493796
LOSS train 0.1612734042613183 valid 0.19916971514052134
LOSS train 0.1612734042613183 valid 0.1991064993731947
LOSS train 0.1612734042613183 valid 0.19912142120861076
LOSS train 0.1612734042613183 valid 0.1992315882426536
LOSS train 0.1612734042613183 valid 0.19925640772527722
LOSS train 0.1612734042613183 valid 0.1993621839653878
LOSS train 0.1612734042613183 valid 0.19935628692191149
LOSS train 0.1612734042613183 valid 0.19930554653060506
LOSS train 0.1612734042613183 valid 0.19935985740307158
LOSS train 0.1612734042613183 valid 0.19938025535906062
LOSS train 0.1612734042613183 valid 0.19922524083220014
LOSS train 0.1612734042613183 valid 0.1991952542237371
LOSS train 0.1612734042613183 valid 0.1992649100177837
LOSS train 0.1612734042613183 valid 0.1994530401344216
LOSS train 0.1612734042613183 valid 0.19950516746527908
LOSS train 0.1612734042613183 valid 0.19953291329172995
LOSS train 0.1612734042613183 valid 0.19938487935306703
LOSS train 0.1612734042613183 valid 0.19936033265515304
LOSS train 0.1612734042613183 valid 0.19943750878118172
LOSS train 0.1612734042613183 valid 0.19937709246362958
LOSS train 0.1612734042613183 valid 0.1992729162728345
LOSS train 0.1612734042613183 valid 0.19925601628016343
LOSS train 0.1612734042613183 valid 0.19928062494010493
LOSS train 0.1612734042613183 valid 0.1994089624386723
LOSS train 0.1612734042613183 valid 0.19951691933920684
LOSS train 0.1612734042613183 valid 0.19953740517912286
LOSS train 0.1612734042613183 valid 0.19957292493151016
LOSS train 0.1612734042613183 valid 0.1994368633744437
LOSS train 0.1612734042613183 valid 0.19947259414494867
LOSS train 0.1612734042613183 valid 0.19946500944594542
LOSS train 0.1612734042613183 valid 0.19940298997959602
LOSS train 0.1612734042613183 valid 0.199498874994602
LOSS train 0.1612734042613183 valid 0.19947559134034085
LOSS train 0.1612734042613183 valid 0.19949709959737547
LOSS train 0.1612734042613183 valid 0.19948863966824257
LOSS train 0.1612734042613183 valid 0.19946567795641434
LOSS train 0.1612734042613183 valid 0.1993696280737339
LOSS train 0.1612734042613183 valid 0.19937087542822826
LOSS train 0.1612734042613183 valid 0.19945600285601164
EPOCH 8:
  batch 1 loss: 0.1355230063199997
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.14291776716709137
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.15052556494871774
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.15680240094661713
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.16432332396507263
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.16621650755405426
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.15764580134834563
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.15723406616598368
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.1548582836985588
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.15335284695029258
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.15026190267367798
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.14833328065772852
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.14913185915121666
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.14868029792393958
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.1490121697386106
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.15061435801908374
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.1508991696378764
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.15392307647400433
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.1528199068025539
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.1512222219258547
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.15159913223414195
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.15181062743067741
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.15251684998688492
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.1525046139334639
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.15269627183675766
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.15336683879678065
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.15424186239639917
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.1542776059359312
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.15329117225161915
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.15450088158249856
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.15476327629819994
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.15515805152244866
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.15425416082143784
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.15518200857674375
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.15639857075044086
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.1554432376805279
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.1554147454129683
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.15534184147652827
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.15552411877956146
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.15582154151052235
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.1555505970992693
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.15586904418610392
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.15661505609750748
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.15595879693600265
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.15673667871289784
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.1565559955070848
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.15573834088888575
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.15548426487172642
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.15559344100100653
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.15580710723996163
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.155647193246028
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.15610553367206684
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.15624441276743728
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.15688048703251062
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.15678726421161132
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.1566819983667561
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.1565927098456182
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.15594743526187435
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.1563257741726051
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.15603589341044427
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.1562394494893121
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.1558882231193204
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.1558132041541357
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.1559065878391266
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.15581020919176247
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.1557743592244206
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.15563565403667848
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.15584912173011722
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.15615459779898325
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.15639211045844215
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.1561762747210516
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.15640858912633526
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.15633034644878074
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.15656249607737
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.15664542277654012
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.15669193314878563
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.15656189755959946
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.1563677770587114
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.1566392683152911
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.156941657140851
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.15712113715248344
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.15661771141174363
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.15622801602963943
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.15652503419135297
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.15649538539788302
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.1565479493245136
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.15663298168059053
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.15684474031017584
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.15684793948122625
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.15708723391095797
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.1569750367121382
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.15695122848062412
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.15752132933947346
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.15763632659899426
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.15800234542081232
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.15801454389778277
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.15793279850298597
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.15794116098965919
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.15794161074992383
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.15784428007900714
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.15783765299780533
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.15772562028438436
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.15731552416838487
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.15742106406161419
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.15699659273737954
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.15709119599382831
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.1572325579195379
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.15706619482349465
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.15705883270556772
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.15741825699806214
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.1572637155249312
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.15690633015973227
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.1565784579225346
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.15676391810963028
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.1564561274388562
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.15638191015299024
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.15664757285107914
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.1566161564472368
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.15660741155137534
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.15678912693013747
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.15660818124359305
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.15668232427513012
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.15711153819551313
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.15702691620155687
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.15689292484521866
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.15699999040317913
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.15704243078710525
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.15709833608707413
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.15713773049818452
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.15709312701454528
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.1569902062529826
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.15694188682193105
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.15732087010055557
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.157323077710262
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.15702428111323605
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.15699748114189682
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.15712388033849478
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.15706690454828567
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.15730528876507024
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.15754543266126086
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.15759521606543386
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.15757111292070067
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.15768550940743692
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.15792000862873262
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.1578087639192055
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.1578056884750928
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.15789170968694752
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.15793776079206853
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.15811320499285755
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.15824988683064778
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.15816568598052524
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.15807588280815826
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.15812380348934846
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.15797856179150668
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.15805120573889825
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.15789844105258966
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.1578228981441753
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.15775964211059523
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.1578622449119136
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.15797174321487545
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.15789870473538867
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.1578306113312274
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.1578759153379253
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.1578214250323249
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.1579262361381993
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.15798255504972963
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.15777941422905037
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.15779977495826425
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.15758028229665474
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.1576328490586842
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.15755029458400102
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.15753587257376936
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.15735843131652458
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.15731289203482113
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.15741353809833528
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.1573523852804845
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.15730203932288003
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.15717002799671687
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.15732688311092013
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.15718784034252167
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.15716749125093385
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.15706567343447234
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.15686519191564757
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.15678274088903613
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.15674734953287486
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.15673948367757182
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.15672845174284541
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.156593687436048
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.15656825545288267
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.15664497864873786
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.15663413929689618
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.15656907732288042
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.15657595533472268
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.15644479458479538
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.15646145190948096
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.1565143704718473
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.15666595593019186
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.15682584220411802
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.15683674939613246
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.1568772356957197
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.15683026307850928
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.1568391705946167
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.15675519810521543
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.15681777302833164
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.15667922104277263
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.156562433297773
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.15652020714709147
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.15642791578116325
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.15647653408312912
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.15662920624017715
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.1566374308266346
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.156723161210429
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.15679871140511384
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.15666112171433796
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.15676270563935124
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.15661525940177617
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.1566901504306749
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.15667980232643425
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.15677267238157525
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.15677529620853337
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.15665541966846086
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.1566458129668021
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.1565511606867538
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.15669389561350858
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.15657828099197812
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.1565755514048897
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.1566193423475988
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.15640633860439584
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.15625690148256752
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.15642077666909798
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.1565391817714745
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.15651356207271075
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.15649004904113614
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.1564520073051636
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.15647780479902917
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.15639716831935666
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.15638053370050237
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.15638042801693708
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.15635044389564123
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.1563477946134905
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.156451692812423
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.15638138227595771
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.1563689697978428
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.1563344510791243
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.156361970031748
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.15639460144367645
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.15625944371648162
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.15635998553085712
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.15640229471476683
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.15644880735874175
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.15658142474068112
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.15651737058919574
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.15648433448297705
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.15647883678045799
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.1565272625170502
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.15648923249682412
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.15642593545904418
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.15632239538569784
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.15636745515247108
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.1563636327592226
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.15626231675175414
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.15620667333821303
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.15621871011112126
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.1560942091781533
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.15610749848046393
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.15611275282681436
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.1560879633500335
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.15611118442420638
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.15601140617991913
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.1560478485568806
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.15599186798193357
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.15594868025030284
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.15617849716009238
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.15606236232131937
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.15598736998709764
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.15618906280808692
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.15617481269453407
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.15614277636297316
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.15620078912879404
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.15633098214332547
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.15626127458445965
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.15622551107765936
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.15626216832508888
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.15625876747071743
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.15637240788915702
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.15639257292960074
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.15633662208551313
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.156381897861138
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.15630379548637924
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.15635218031961343
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.15630668860018457
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.1562873571345659
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.15640299492626875
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.1565029367449738
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.15651297965797328
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.15654619755475102
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.1566237498463605
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.1565144918358966
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.15638863738862965
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.1563930813719829
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.15649927249085469
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.15648766930233562
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.15649108994420213
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.15647465061690463
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.15652769752701776
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.15650566586968945
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.15646611986505868
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.15643427262155266
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.15640767784276827
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.15630445357772613
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.15633386519656686
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.15629463989096573
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.156343723328921
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.1563020451765531
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.1563193574074715
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.15644714596999598
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.1564320686628766
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.15641292514947225
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.15641178160345293
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.15640500464942306
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.15635925454792576
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.1563551115647236
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.15641516592388183
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.15635021746066619
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.1563867015334276
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.15627503187104236
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.15617253157432656
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.15619830712221744
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.1561507721938261
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.15624991860805135
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.15630938997351512
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.15622001107647476
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.15609791787626506
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.1560861646102931
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.1561993561351477
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.15623102969090855
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.15616222245491576
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.1560932053749025
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.1559710754054134
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.1559539373306667
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.15582764727716222
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.15604836426196042
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.1560243368366022
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.15601587167746106
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.15602173112008882
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.15602769541171935
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.15601443589506644
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.1560093612121097
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.15596396881649352
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.15589019900986126
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.15588851763397202
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.15595616851086644
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.15601613731374145
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.15606210530225167
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.15607561819570165
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.15601880544943086
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.15610039591037927
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.15619017227485193
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.15620448548232613
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.15611298125651146
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.156080132830176
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.15609385143162796
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.15604888428177058
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.15599252230354718
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.15604856875661302
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.1560066039037835
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.15605943497747427
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.15610247077015432
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.15608795706011092
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.1560074546047159
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.1560203617795137
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.1559684721612802
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.15596025403159552
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.15603032819727525
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.15604431994756063
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.15615233834436598
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.15611482136919896
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.15616321756883902
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.15605640287528252
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.15595910517792952
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.15591657705350812
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.15584018502715993
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.15593810795803917
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.15588343876879662
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.1558150348338214
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.15583350026854578
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.15579858907433444
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.15582333067336032
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.1558608413125678
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.1558113869184103
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.1557600225710198
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.155785398747848
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.1559131887122875
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.1559141846704604
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.1559359752679173
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.1559072315617643
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.15579658243758251
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.1558680331437432
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.1557815014979893
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.15574484372511507
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.15573560364748773
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.15567715608732619
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.15560900686174706
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.15556846557203496
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.15556982889955426
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.15552115886302417
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.15549282650909493
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.15561460387692147
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.15561872227049106
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.15564261745752359
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.15563449102234087
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.15566527928469828
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.15563793489416344
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.15570211725462463
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.15575908632522606
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.1557752470163485
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.15574387568149636
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.15572988143876978
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.15588216535160251
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.15581365325266405
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.15593526724752507
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.1559954559103855
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.15591605368728614
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.15589182396893794
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.15588353649658315
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.15583060410730715
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.15581100329372866
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.15576936041709977
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.15570994848624253
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.15569374351653942
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.15570723165146436
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.15565574864201523
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.15560491384796402
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.1556748953660787
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.15561303498073556
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.15570312934936187
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.1557384509277835
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.15569101190662274
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.1556060094055391
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.15557590145617722
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.15556664597623202
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.1555400828733973
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.15552112290269904
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.1555202555851088
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.1555577343601859
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.15557022307552565
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.15555118296037052
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.1556300632489313
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.15563451140322504
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.15559659366806347
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.15560902111612243
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.15562333395721112
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.1555922371366166
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.15560080647665497
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.1555553640801828
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.1556189593679288
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.15559551460542095
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.1556085887726998
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.1556131201664347
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.15568797110539415
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.1556612037106088
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.15561147319612564
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.15551647534251986
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.15554603728754768
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.15557422192506892
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.15554406976750992
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.15554665395994002
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.15561934783417955
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.1556719980641469
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.15570644292425603
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.15571784340398834
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.15582339923386856
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.15582339923386856 valid 0.21183058619499207
LOSS train 0.15582339923386856 valid 0.187180295586586
LOSS train 0.15582339923386856 valid 0.19052714109420776
LOSS train 0.15582339923386856 valid 0.1790619194507599
LOSS train 0.15582339923386856 valid 0.17293221950531007
LOSS train 0.15582339923386856 valid 0.18495329469442368
LOSS train 0.15582339923386856 valid 0.19344397740704672
LOSS train 0.15582339923386856 valid 0.19237715005874634
LOSS train 0.15582339923386856 valid 0.192534315917227
LOSS train 0.15582339923386856 valid 0.1951184928417206
LOSS train 0.15582339923386856 valid 0.19422561743042685
LOSS train 0.15582339923386856 valid 0.1946390556792418
LOSS train 0.15582339923386856 valid 0.19463070997825035
LOSS train 0.15582339923386856 valid 0.1935149601527623
LOSS train 0.15582339923386856 valid 0.19178457458813986
LOSS train 0.15582339923386856 valid 0.1933616641908884
LOSS train 0.15582339923386856 valid 0.1938754083479152
LOSS train 0.15582339923386856 valid 0.19333137820164362
LOSS train 0.15582339923386856 valid 0.19588583237246462
LOSS train 0.15582339923386856 valid 0.19675396308302878
LOSS train 0.15582339923386856 valid 0.19474627077579498
LOSS train 0.15582339923386856 valid 0.19366287236863916
LOSS train 0.15582339923386856 valid 0.19301654398441315
LOSS train 0.15582339923386856 valid 0.19319063611328602
LOSS train 0.15582339923386856 valid 0.19203034460544585
LOSS train 0.15582339923386856 valid 0.19290580027378523
LOSS train 0.15582339923386856 valid 0.19271730328047718
LOSS train 0.15582339923386856 valid 0.19283545389771461
LOSS train 0.15582339923386856 valid 0.1933300644159317
LOSS train 0.15582339923386856 valid 0.19445970604817073
LOSS train 0.15582339923386856 valid 0.19536528568114003
LOSS train 0.15582339923386856 valid 0.19491774402558804
LOSS train 0.15582339923386856 valid 0.1951198966214151
LOSS train 0.15582339923386856 valid 0.19525447926100561
LOSS train 0.15582339923386856 valid 0.19703899196216038
LOSS train 0.15582339923386856 valid 0.19707061722874641
LOSS train 0.15582339923386856 valid 0.19749466551316752
LOSS train 0.15582339923386856 valid 0.19847343509134494
LOSS train 0.15582339923386856 valid 0.1971566921625382
LOSS train 0.15582339923386856 valid 0.19670400470495225
LOSS train 0.15582339923386856 valid 0.19730495098160533
LOSS train 0.15582339923386856 valid 0.1979771220967883
LOSS train 0.15582339923386856 valid 0.1979512260403744
LOSS train 0.15582339923386856 valid 0.19888919253240933
LOSS train 0.15582339923386856 valid 0.19850977096292707
LOSS train 0.15582339923386856 valid 0.19916001189014185
LOSS train 0.15582339923386856 valid 0.19931936517674873
LOSS train 0.15582339923386856 valid 0.19905530971785387
LOSS train 0.15582339923386856 valid 0.1994503252968496
LOSS train 0.15582339923386856 valid 0.19876739859580994
LOSS train 0.15582339923386856 valid 0.19900758359946458
LOSS train 0.15582339923386856 valid 0.19887265792259803
LOSS train 0.15582339923386856 valid 0.19954774121068558
LOSS train 0.15582339923386856 valid 0.1993041706305963
LOSS train 0.15582339923386856 valid 0.19939013069326228
LOSS train 0.15582339923386856 valid 0.19890831410884857
LOSS train 0.15582339923386856 valid 0.19909952711640744
LOSS train 0.15582339923386856 valid 0.19873595314806905
LOSS train 0.15582339923386856 valid 0.19919597420652033
LOSS train 0.15582339923386856 valid 0.19893894493579864
LOSS train 0.15582339923386856 valid 0.19910370326433025
LOSS train 0.15582339923386856 valid 0.1999146671545121
LOSS train 0.15582339923386856 valid 0.1997034956538488
LOSS train 0.15582339923386856 valid 0.20037561375647783
LOSS train 0.15582339923386856 valid 0.2007587357209279
LOSS train 0.15582339923386856 valid 0.20064783118890994
LOSS train 0.15582339923386856 valid 0.20037995167632602
LOSS train 0.15582339923386856 valid 0.20026850634638002
LOSS train 0.15582339923386856 valid 0.1995288440282794
LOSS train 0.15582339923386856 valid 0.19987580861364093
LOSS train 0.15582339923386856 valid 0.19981265697680728
LOSS train 0.15582339923386856 valid 0.20004349967671764
LOSS train 0.15582339923386856 valid 0.20039397661816583
LOSS train 0.15582339923386856 valid 0.2001454691226418
LOSS train 0.15582339923386856 valid 0.2001062540213267
LOSS train 0.15582339923386856 valid 0.2005783011646647
LOSS train 0.15582339923386856 valid 0.20044911972114018
LOSS train 0.15582339923386856 valid 0.20039213334138578
LOSS train 0.15582339923386856 valid 0.19996891689451435
LOSS train 0.15582339923386856 valid 0.19912775829434395
LOSS train 0.15582339923386856 valid 0.19869080737785058
LOSS train 0.15582339923386856 valid 0.19893503116398323
LOSS train 0.15582339923386856 valid 0.19853707052857042
LOSS train 0.15582339923386856 valid 0.19831360574989093
LOSS train 0.15582339923386856 valid 0.19784314877846662
LOSS train 0.15582339923386856 valid 0.1973994455711786
LOSS train 0.15582339923386856 valid 0.19746173986758309
LOSS train 0.15582339923386856 valid 0.19724141620099545
LOSS train 0.15582339923386856 valid 0.19735154124458185
LOSS train 0.15582339923386856 valid 0.1976323902606964
LOSS train 0.15582339923386856 valid 0.19798316208870856
LOSS train 0.15582339923386856 valid 0.19791540665471036
LOSS train 0.15582339923386856 valid 0.197784306862021
LOSS train 0.15582339923386856 valid 0.19793271447749847
LOSS train 0.15582339923386856 valid 0.1974332084781245
LOSS train 0.15582339923386856 valid 0.1973076326151689
LOSS train 0.15582339923386856 valid 0.19737412066189283
LOSS train 0.15582339923386856 valid 0.19748050217725793
LOSS train 0.15582339923386856 valid 0.19796502951419714
LOSS train 0.15582339923386856 valid 0.1982000733911991
LOSS train 0.15582339923386856 valid 0.19831937197411414
LOSS train 0.15582339923386856 valid 0.1987536201874415
LOSS train 0.15582339923386856 valid 0.19870252719203246
LOSS train 0.15582339923386856 valid 0.19885257402291664
LOSS train 0.15582339923386856 valid 0.19879431752931503
LOSS train 0.15582339923386856 valid 0.19927250383035192
LOSS train 0.15582339923386856 valid 0.1991018178028481
LOSS train 0.15582339923386856 valid 0.19943648273194278
LOSS train 0.15582339923386856 valid 0.19978061373080683
LOSS train 0.15582339923386856 valid 0.19990755048665135
LOSS train 0.15582339923386856 valid 0.1999559683037234
LOSS train 0.15582339923386856 valid 0.19961215302880322
LOSS train 0.15582339923386856 valid 0.19982484273150958
LOSS train 0.15582339923386856 valid 0.20002688807353639
LOSS train 0.15582339923386856 valid 0.20002182735049207
LOSS train 0.15582339923386856 valid 0.20041926205158234
LOSS train 0.15582339923386856 valid 0.20056018437075818
LOSS train 0.15582339923386856 valid 0.2002965840747801
LOSS train 0.15582339923386856 valid 0.20008537461276815
LOSS train 0.15582339923386856 valid 0.19990010112524031
LOSS train 0.15582339923386856 valid 0.19969887428047242
LOSS train 0.15582339923386856 valid 0.19963087239226357
LOSS train 0.15582339923386856 valid 0.1998101982886229
LOSS train 0.15582339923386856 valid 0.19989034485432408
LOSS train 0.15582339923386856 valid 0.19974274635314943
LOSS train 0.15582339923386856 valid 0.2000192930064504
LOSS train 0.15582339923386856 valid 0.19984624575911544
LOSS train 0.15582339923386856 valid 0.1998588900314644
LOSS train 0.15582339923386856 valid 0.1999828221724015
LOSS train 0.15582339923386856 valid 0.19994105788377614
LOSS train 0.15582339923386856 valid 0.19964103025334481
LOSS train 0.15582339923386856 valid 0.19935692767753746
LOSS train 0.15582339923386856 valid 0.19916003191829623
LOSS train 0.15582339923386856 valid 0.19920592799560347
LOSS train 0.15582339923386856 valid 0.19919111828009287
LOSS train 0.15582339923386856 valid 0.19931582931210012
LOSS train 0.15582339923386856 valid 0.19914960469642695
LOSS train 0.15582339923386856 valid 0.19907374550466952
LOSS train 0.15582339923386856 valid 0.19879886272142258
LOSS train 0.15582339923386856 valid 0.19891441187688283
LOSS train 0.15582339923386856 valid 0.1988766777811321
LOSS train 0.15582339923386856 valid 0.19885449867013474
LOSS train 0.15582339923386856 valid 0.19879398562691428
LOSS train 0.15582339923386856 valid 0.19875018184797633
LOSS train 0.15582339923386856 valid 0.19862673827286426
LOSS train 0.15582339923386856 valid 0.19895923066220872
LOSS train 0.15582339923386856 valid 0.19879692306323926
LOSS train 0.15582339923386856 valid 0.1996970510966069
LOSS train 0.15582339923386856 valid 0.1997334985124985
LOSS train 0.15582339923386856 valid 0.19962562799453734
LOSS train 0.15582339923386856 valid 0.19978325818942871
LOSS train 0.15582339923386856 valid 0.19968233502617008
LOSS train 0.15582339923386856 valid 0.19965582881488053
LOSS train 0.15582339923386856 valid 0.19965357062491504
LOSS train 0.15582339923386856 valid 0.19959982297112866
LOSS train 0.15582339923386856 valid 0.19983509173377967
LOSS train 0.15582339923386856 valid 0.19995687161661257
LOSS train 0.15582339923386856 valid 0.2000278531561924
LOSS train 0.15582339923386856 valid 0.2000758776312354
LOSS train 0.15582339923386856 valid 0.19994648499414325
LOSS train 0.15582339923386856 valid 0.19985921708693416
LOSS train 0.15582339923386856 valid 0.19966434089489926
LOSS train 0.15582339923386856 valid 0.1994582630373949
LOSS train 0.15582339923386856 valid 0.19938283268271423
LOSS train 0.15582339923386856 valid 0.19926105791872198
LOSS train 0.15582339923386856 valid 0.19930017299680824
LOSS train 0.15582339923386856 valid 0.19960970989244428
LOSS train 0.15582339923386856 valid 0.1997010028433232
LOSS train 0.15582339923386856 valid 0.19989464392323467
LOSS train 0.15582339923386856 valid 0.1998938747188624
LOSS train 0.15582339923386856 valid 0.19978253464949758
LOSS train 0.15582339923386856 valid 0.19954580510425013
LOSS train 0.15582339923386856 valid 0.1997085183514336
LOSS train 0.15582339923386856 valid 0.19966015236816187
LOSS train 0.15582339923386856 valid 0.19956077992916108
LOSS train 0.15582339923386856 valid 0.19945489191873508
LOSS train 0.15582339923386856 valid 0.19957895063411044
LOSS train 0.15582339923386856 valid 0.19975173121757722
LOSS train 0.15582339923386856 valid 0.1996528273188202
LOSS train 0.15582339923386856 valid 0.1996229798429542
LOSS train 0.15582339923386856 valid 0.19978748986404904
LOSS train 0.15582339923386856 valid 0.19973231360807522
LOSS train 0.15582339923386856 valid 0.19969686833235736
LOSS train 0.15582339923386856 valid 0.19955195940059164
LOSS train 0.15582339923386856 valid 0.19950108560355934
LOSS train 0.15582339923386856 valid 0.1994986425965063
LOSS train 0.15582339923386856 valid 0.19926662504035522
LOSS train 0.15582339923386856 valid 0.19928680701141663
LOSS train 0.15582339923386856 valid 0.1993051821750308
LOSS train 0.15582339923386856 valid 0.1993837763604365
LOSS train 0.15582339923386856 valid 0.19932388619602662
LOSS train 0.15582339923386856 valid 0.19926211369844773
LOSS train 0.15582339923386856 valid 0.19918040081935842
LOSS train 0.15582339923386856 valid 0.19904446033472867
LOSS train 0.15582339923386856 valid 0.1988015971886806
LOSS train 0.15582339923386856 valid 0.19882551352588498
LOSS train 0.15582339923386856 valid 0.1989624474556918
LOSS train 0.15582339923386856 valid 0.19884322583675385
LOSS train 0.15582339923386856 valid 0.19887489724398857
LOSS train 0.15582339923386856 valid 0.19875777184963225
LOSS train 0.15582339923386856 valid 0.19867804705800107
LOSS train 0.15582339923386856 valid 0.19867856726788058
LOSS train 0.15582339923386856 valid 0.19872499437167726
LOSS train 0.15582339923386856 valid 0.19892746407319517
LOSS train 0.15582339923386856 valid 0.19875010957078235
LOSS train 0.15582339923386856 valid 0.198883479153647
LOSS train 0.15582339923386856 valid 0.19876582099907641
LOSS train 0.15582339923386856 valid 0.19873122906742188
LOSS train 0.15582339923386856 valid 0.19860815424382972
LOSS train 0.15582339923386856 valid 0.19858622203270596
LOSS train 0.15582339923386856 valid 0.19844646208094194
LOSS train 0.15582339923386856 valid 0.19823967623260785
LOSS train 0.15582339923386856 valid 0.19820235711587986
LOSS train 0.15582339923386856 valid 0.1981124116160045
LOSS train 0.15582339923386856 valid 0.19804210780664933
LOSS train 0.15582339923386856 valid 0.19780870299372408
LOSS train 0.15582339923386856 valid 0.19770609893007762
LOSS train 0.15582339923386856 valid 0.19764287339164577
LOSS train 0.15582339923386856 valid 0.19780508874486027
LOSS train 0.15582339923386856 valid 0.19777759103612466
LOSS train 0.15582339923386856 valid 0.19770814235663522
LOSS train 0.15582339923386856 valid 0.19777866949637732
LOSS train 0.15582339923386856 valid 0.19788865638153436
LOSS train 0.15582339923386856 valid 0.19802003679797053
LOSS train 0.15582339923386856 valid 0.19826472368505266
LOSS train 0.15582339923386856 valid 0.19837705230554648
LOSS train 0.15582339923386856 valid 0.1985260470060525
LOSS train 0.15582339923386856 valid 0.19862624970183038
LOSS train 0.15582339923386856 valid 0.19858064642379378
LOSS train 0.15582339923386856 valid 0.19866831030534662
LOSS train 0.15582339923386856 valid 0.19882339432642057
LOSS train 0.15582339923386856 valid 0.1988986112691205
LOSS train 0.15582339923386856 valid 0.19886253326491737
LOSS train 0.15582339923386856 valid 0.19902790197704592
LOSS train 0.15582339923386856 valid 0.19912185237762775
LOSS train 0.15582339923386856 valid 0.19903036806795557
LOSS train 0.15582339923386856 valid 0.19892920572546463
LOSS train 0.15582339923386856 valid 0.19887961134189316
LOSS train 0.15582339923386856 valid 0.19869862110295555
LOSS train 0.15582339923386856 valid 0.19869592605779568
LOSS train 0.15582339923386856 valid 0.19882738571691316
LOSS train 0.15582339923386856 valid 0.1986724645638269
LOSS train 0.15582339923386856 valid 0.19886956097167216
LOSS train 0.15582339923386856 valid 0.19908067852747244
LOSS train 0.15582339923386856 valid 0.19903961791067706
LOSS train 0.15582339923386856 valid 0.198939668393232
LOSS train 0.15582339923386856 valid 0.19907695984068188
LOSS train 0.15582339923386856 valid 0.19908428390420252
LOSS train 0.15582339923386856 valid 0.1990436410090052
LOSS train 0.15582339923386856 valid 0.19899663650989532
LOSS train 0.15582339923386856 valid 0.1988230528703249
LOSS train 0.15582339923386856 valid 0.1989460036986404
LOSS train 0.15582339923386856 valid 0.1988228651963675
LOSS train 0.15582339923386856 valid 0.1987721764548557
LOSS train 0.15582339923386856 valid 0.19881614195365532
LOSS train 0.15582339923386856 valid 0.1988175121950917
LOSS train 0.15582339923386856 valid 0.19866385812425427
LOSS train 0.15582339923386856 valid 0.1988526159940764
LOSS train 0.15582339923386856 valid 0.19885892826617915
LOSS train 0.15582339923386856 valid 0.1988466612994671
LOSS train 0.15582339923386856 valid 0.19896025384751317
LOSS train 0.15582339923386856 valid 0.19900326658296222
LOSS train 0.15582339923386856 valid 0.19895312139063281
LOSS train 0.15582339923386856 valid 0.1990009853334138
LOSS train 0.15582339923386856 valid 0.19900524700587652
LOSS train 0.15582339923386856 valid 0.19896140106414495
LOSS train 0.15582339923386856 valid 0.1990734448705273
LOSS train 0.15582339923386856 valid 0.19921318213663883
LOSS train 0.15582339923386856 valid 0.19934017684823074
LOSS train 0.15582339923386856 valid 0.19930437450055724
LOSS train 0.15582339923386856 valid 0.1994245824338765
LOSS train 0.15582339923386856 valid 0.1997080908759552
LOSS train 0.15582339923386856 valid 0.19986196266207504
LOSS train 0.15582339923386856 valid 0.1999412375318743
LOSS train 0.15582339923386856 valid 0.1998627362468026
LOSS train 0.15582339923386856 valid 0.19988786364379135
LOSS train 0.15582339923386856 valid 0.1997939317557786
LOSS train 0.15582339923386856 valid 0.1996248308167183
LOSS train 0.15582339923386856 valid 0.19958591306295018
LOSS train 0.15582339923386856 valid 0.199538777289646
LOSS train 0.15582339923386856 valid 0.19949290092729588
LOSS train 0.15582339923386856 valid 0.19922740202634892
LOSS train 0.15582339923386856 valid 0.19929445785052363
LOSS train 0.15582339923386856 valid 0.199382384635613
LOSS train 0.15582339923386856 valid 0.19937666194480763
LOSS train 0.15582339923386856 valid 0.19933373621710532
LOSS train 0.15582339923386856 valid 0.19923631587302643
LOSS train 0.15582339923386856 valid 0.1992264531759752
LOSS train 0.15582339923386856 valid 0.19922123524556937
LOSS train 0.15582339923386856 valid 0.19922361759276225
LOSS train 0.15582339923386856 valid 0.19902947135397658
LOSS train 0.15582339923386856 valid 0.19901040058634054
LOSS train 0.15582339923386856 valid 0.19901892659810622
LOSS train 0.15582339923386856 valid 0.19903926865584184
LOSS train 0.15582339923386856 valid 0.19917412238605953
LOSS train 0.15582339923386856 valid 0.19910530323112333
LOSS train 0.15582339923386856 valid 0.1991352160972377
LOSS train 0.15582339923386856 valid 0.19912213557118538
LOSS train 0.15582339923386856 valid 0.19916894274411792
LOSS train 0.15582339923386856 valid 0.19925402795275052
LOSS train 0.15582339923386856 valid 0.19921679487854144
LOSS train 0.15582339923386856 valid 0.19914853444557315
LOSS train 0.15582339923386856 valid 0.19926812895650517
LOSS train 0.15582339923386856 valid 0.1992656348371192
LOSS train 0.15582339923386856 valid 0.1992168468041498
LOSS train 0.15582339923386856 valid 0.19923294598565383
LOSS train 0.15582339923386856 valid 0.19922350089790766
LOSS train 0.15582339923386856 valid 0.1991375811010986
LOSS train 0.15582339923386856 valid 0.19927234137521208
LOSS train 0.15582339923386856 valid 0.1992579645687534
LOSS train 0.15582339923386856 valid 0.1991903081871689
LOSS train 0.15582339923386856 valid 0.19917400176517475
LOSS train 0.15582339923386856 valid 0.1992937342141764
LOSS train 0.15582339923386856 valid 0.1993424235160943
LOSS train 0.15582339923386856 valid 0.19926162901378813
LOSS train 0.15582339923386856 valid 0.19920720518389834
LOSS train 0.15582339923386856 valid 0.19925534612372847
LOSS train 0.15582339923386856 valid 0.19927562126573525
LOSS train 0.15582339923386856 valid 0.19933746404782357
LOSS train 0.15582339923386856 valid 0.19922027364373207
LOSS train 0.15582339923386856 valid 0.19938238461812338
LOSS train 0.15582339923386856 valid 0.1994503626356954
LOSS train 0.15582339923386856 valid 0.1994425178773632
LOSS train 0.15582339923386856 valid 0.19956946970871936
LOSS train 0.15582339923386856 valid 0.19952915971095744
LOSS train 0.15582339923386856 valid 0.1996948071784037
LOSS train 0.15582339923386856 valid 0.19982569557628865
LOSS train 0.15582339923386856 valid 0.1996825595454472
LOSS train 0.15582339923386856 valid 0.19983319113863277
LOSS train 0.15582339923386856 valid 0.19981310525626847
LOSS train 0.15582339923386856 valid 0.19967796475685615
LOSS train 0.15582339923386856 valid 0.1996136276747089
LOSS train 0.15582339923386856 valid 0.19962931999394126
LOSS train 0.15582339923386856 valid 0.1997409256364771
LOSS train 0.15582339923386856 valid 0.1997641392608187
LOSS train 0.15582339923386856 valid 0.19987632298753374
LOSS train 0.15582339923386856 valid 0.19987447555411816
LOSS train 0.15582339923386856 valid 0.19982387641301522
LOSS train 0.15582339923386856 valid 0.19988090555928098
LOSS train 0.15582339923386856 valid 0.19990045051364339
LOSS train 0.15582339923386856 valid 0.19974210539346565
LOSS train 0.15582339923386856 valid 0.19971614331007004
LOSS train 0.15582339923386856 valid 0.19978768828137622
LOSS train 0.15582339923386856 valid 0.19998151224193184
LOSS train 0.15582339923386856 valid 0.2000385166078374
LOSS train 0.15582339923386856 valid 0.2000671675956318
LOSS train 0.15582339923386856 valid 0.19992046221360693
LOSS train 0.15582339923386856 valid 0.19990072746215196
LOSS train 0.15582339923386856 valid 0.1999722387438859
LOSS train 0.15582339923386856 valid 0.1999110401528222
LOSS train 0.15582339923386856 valid 0.1998053548003194
LOSS train 0.15582339923386856 valid 0.19978643348440528
LOSS train 0.15582339923386856 valid 0.19980963216118366
LOSS train 0.15582339923386856 valid 0.19994480815310936
LOSS train 0.15582339923386856 valid 0.2000526873998239
LOSS train 0.15582339923386856 valid 0.20007617181439077
LOSS train 0.15582339923386856 valid 0.20011707894107542
LOSS train 0.15582339923386856 valid 0.19997755034342823
LOSS train 0.15582339923386856 valid 0.20001517431317598
LOSS train 0.15582339923386856 valid 0.2000042447199424
LOSS train 0.15582339923386856 valid 0.19993632078335885
LOSS train 0.15582339923386856 valid 0.20003244457817868
LOSS train 0.15582339923386856 valid 0.20000950187526786
LOSS train 0.15582339923386856 valid 0.20003233199099918
LOSS train 0.15582339923386856 valid 0.20002819916156872
LOSS train 0.15582339923386856 valid 0.20000838944332197
LOSS train 0.15582339923386856 valid 0.19990989573969828
LOSS train 0.15582339923386856 valid 0.1999128993357653
LOSS train 0.15582339923386856 valid 0.19999975220459262
EPOCH 9:
  batch 1 loss: 0.13848261535167694
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.1398037225008011
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.1488753706216812
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.15479138493537903
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.16304102241992952
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.16469459235668182
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.15712458853210723
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.15685332659631968
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.15418554759687847
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.15296612903475762
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.15027201988480307
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.14812880381941795
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.14908660260530618
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.14827330304043634
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.14833881358305614
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.15025863703340292
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.15039796075400183
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.15305905789136887
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.15182754946382424
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.15015518441796302
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.15056129438536509
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.1509342681277882
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.15155013618261917
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.15182764393587908
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.15203457713127136
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.15264360377421746
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.15375533092904975
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.1536602723811354
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.15274948549681697
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.15369335214296978
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.15406069736326894
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.15442517632618546
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.15358409240390314
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.15428708888152065
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.1556006222963333
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.15482792382438978
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.15465896113498792
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.15448119452125147
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.154600379940791
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.15486738756299018
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.1546309521285499
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.15489329468636287
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.15568495836368826
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.15490822612561964
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.15576494187116624
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.1553376500049363
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.15450785562713096
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.15424615253383914
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.15427538311603117
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.1545222993195057
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.15430751749697855
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.15490826577521288
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.1549339138393132
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.1555001005806305
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.1553882367231629
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.1552157225087285
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.1551920992501995
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.1545910845542776
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.15501361605474503
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.15477347324291865
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.1550094580552617
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.15460523722633238
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.1544802789650266
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.15463623777031898
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.15455603163975937
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.154490180087812
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.15425113888818826
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.15445144036236932
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.1548339845477671
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.15515689764704024
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.15488239262305514
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.15511523828738266
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.15504455729706645
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.15514235641505267
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.15523525098959604
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.1553044640704205
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.1553182890275856
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.15509702016909918
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.15526520413688466
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.15558010507375003
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.155808945680842
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.15538384383771478
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.1550256091069026
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.1553650170209862
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.15533086131600773
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.1553566140490909
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.15547358612904602
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.15564888275482439
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.15567346268825316
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.1558684711654981
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.15573657295861087
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.15565828768455464
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.15615553513009062
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.15630047276933143
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.15666721133809342
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.15672240049267808
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.1566649719919126
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.15674492899252443
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.15676595511460545
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.15668501675128937
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.15671404251957885
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.15668800677738937
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.15629382561711433
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.15641136438800737
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.15596749832232792
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.1561018039454829
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.15627094413911072
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.1560968337925496
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.1561751144878361
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.15653721764683723
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.1563741506354229
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.15604500951511519
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.15575121092585337
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.15592746436595917
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.1555788586968961
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.15545508599486843
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.15575733182267246
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.15570437946056914
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.15569094073872605
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.1558088461558024
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.15560126292311455
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.15567283539987
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.15607782604733134
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.15595759691730623
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.1558248062133789
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.15587447749243843
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.15592301314271342
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.15592767612542957
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.15600475322368534
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.1560444326354907
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.1559049667975375
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.1558482808371385
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.15625421926939398
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.15628760152343493
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.1559578745453446
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.1559488350415931
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.15602405902243008
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.15596434322820193
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.15626995286924375
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.15646632931062154
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.15652711932540786
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.15651392926212768
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.1566505575930322
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.15694731235918072
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.15684667167992428
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.1568331299987558
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.15692809761381474
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.1569618452843782
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.15717201794953956
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.15724166383345922
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.15713004313952086
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.1570150335564425
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.15710410095897376
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.15694125170831558
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.15700420256583922
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.15688680675931466
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.15682548246565897
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.1567311628332621
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.15679423792182273
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.1569001235999167
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.15688779996418806
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.15681406800393705
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.15686751682699823
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.15678529713938877
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.15691594761429412
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.15690207813518592
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.1566796583865217
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.1566898146023353
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.15646817156020001
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.15653187233735533
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.15646535284512225
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.15645176982290523
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.1562887908664742
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.15626330395368324
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.1564263705270631
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.1563591412268579
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.15632459474990598
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.15622602959864595
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.15640758315111672
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.15629163479639424
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.15629350624525745
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.1561725203070667
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.15597883848056115
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.15591219217395005
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.15588113556842545
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.15586518308007588
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.15583123796604534
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.1557074128551052
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.15569515695805272
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.15573845658647387
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.15572430072498572
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.1556460582263147
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.1556611457559729
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.15553059970441552
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.15554735091252206
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.1555992911892886
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.1557424043867794
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.15591388003844203
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.15594319029519307
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.1559780326113105
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.15593246845018804
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.1559338269036005
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.15585246172004144
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.15594128464512966
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.15579276088534333
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.15567275704544725
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.15560314257219793
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.15550349978730083
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.15554420525234852
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.1557149763263407
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.15571559008687594
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.15581813915017642
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.15589434919362896
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.15575470103420944
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.15587751134190447
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.15572430576301283
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.15580469462591381
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.15580659985132173
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.15588934280692715
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.15589618022468957
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.15581129083401477
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.1558023628417973
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.1556989204121812
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.15583678531194373
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.15572632004817327
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.15572180602270946
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.15576162924456702
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.15556201845276774
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.15542983432914492
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.15560231665554253
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.15570510649706895
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.15568049614927892
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.1556569012641395
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.15563832245703435
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.15568138649489016
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.15556079872205095
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.15557507947401658
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.15557152186246478
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.1555325661057209
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.1555410492233932
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.15562594502669647
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.15555546577434895
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.15554787344280094
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.15550384789583135
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.1555174133303214
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.15552422619326328
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.15538986651641637
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.15549698601206463
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.15552738252054735
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.15557200273871422
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.1557572911756922
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.15568729120469282
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.1556700332892742
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.1556621434709688
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.15571199354587817
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.15567714817007072
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.15559973567724228
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.1554998981571475
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.155581339934848
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.15556302193838817
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.15545646843439775
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.15539559793040042
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.15542780301983367
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.15529675575706994
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.15530405154205718
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.15531432496985995
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.15529546345329462
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.15533978467000953
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.1552392844540036
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.15524901452439802
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.15520201868126754
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.15516446237726247
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.15537900444897976
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.1552669405121438
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.1551852192391049
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.1553769852642132
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.1553401009109046
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.1553003722332793
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.15531756766273985
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.15543278820280518
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.15536009191512212
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.15532071050917004
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.15535863947004816
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.15535806562804
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.15546628615835256
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.15551333840597759
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.15546012820370936
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.15550257666553888
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.15541868945615928
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.15549344607982143
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.1554614276564408
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.1554594365242001
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.1555402208971489
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.15563639331938459
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.15563611607935468
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.15567236562335007
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.15575266263199977
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.15563485086364234
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.15551599254675932
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.1554940133045117
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.15561475238431727
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.15559718646849227
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.15559304299035875
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.15557553057901954
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.15561347752809523
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.1556070264112326
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.15557065521175775
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.1555404310634771
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.15551342805515986
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.15540991745168162
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.15544265420011386
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.1554136315647226
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.15547540081670871
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.15544270996929735
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.15545360874088984
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.15556468944289262
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.15553177369035756
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.15549388356834837
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.1555013789045026
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.15547939080279322
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.15543232646965163
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.15541660440912158
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.15546769381215328
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.1554026074109622
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.15543525776037803
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.15530819315577576
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.15520843644754603
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.155249459305551
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.15522476020736173
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.15532745971824183
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.15537983741825082
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.15527913811425847
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.15516061890680152
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.1551665777708599
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.15528696555255064
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.15531081087621196
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.155231113581523
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.15515382964847355
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.15502676396331252
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.1550299909842365
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.15491309648385146
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.1551210215827178
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.15510194972374697
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.15505901871379033
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.1550899954809659
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.1550785353073495
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.15505507957694853
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.15504845655683813
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.1550173627527532
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.15495357219661984
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.1549628377385289
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.1550283780829473
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.15507711381479972
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.15512143193328448
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.15514334532576549
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.15509688121716628
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.1551740386990272
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.1552546942117494
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.1552670113638583
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.15516565800127055
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.1551110227888971
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.15511880183170512
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.15507555950032778
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.15500872125746784
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.15506048923077648
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.1550119094794891
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.15505971779859035
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.15510626625188667
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.15508247942216996
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.15500825806646734
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.15501022144067642
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.15493754419668387
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.1549322659264621
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.1549649886627567
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.15496889672676722
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.15507785748056274
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.1550353024579802
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.15507604195563882
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.15498789356613538
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.15490240586039267
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.15484615266714197
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.15477488894035055
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.15487235650303469
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.1548188226103472
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.1547603199427778
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.1547726067892937
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.15472452525405614
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.15475860877519415
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.15481260980079598
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.15473592713093146
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.15469307279038003
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.15471249643941315
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.15481982742253758
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.15481627622836738
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.1548213217077376
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.15478476366460925
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.154692282266821
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.15474360046824018
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.15465350198865235
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.15463213894516228
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.15460524568682596
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.1545456658251843
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.15447386444798178
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.15442623607445471
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.15443795054783055
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.1543901592052629
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.15436357062044542
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.15448911254312478
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.15449077692474888
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.15450913720741505
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.154494988983565
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.1545330908113313
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.15452497180091268
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.15460447477545716
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.1546531382813511
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.15465972744501555
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.15462305166309687
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.15460810920838533
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.15474144580967386
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.15467708231437774
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.1548038170697287
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.1548489623866375
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.154763856220189
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.1547376902879409
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.15473135786897996
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.15466069069826546
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.15465125089088144
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.15461043616600126
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.1545549196241063
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.15451677971801092
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.15455052481311657
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.1544977447569922
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.1544409919762446
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.15448454135711293
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.15441613368604376
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.15450433005980396
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.1545549713296803
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.15451560077601917
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.15441633984955677
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.15437218021940102
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.15437356509318967
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.15434238462965952
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.15433610767193356
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.154324193594155
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.15434261731217416
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.15435784968292768
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.1543342192434358
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.15441769171905304
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.1544143366056986
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.15436072727044423
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.154347571889207
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.15436779750527535
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.15431582108633407
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.15430817339520098
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.15427006201429683
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.15432839621708058
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.1542956287477418
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.15430865628750565
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.1543278832451191
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.1543924717799477
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.15436683824677788
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.15432247783972589
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.15422389278144094
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.15425725384005184
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.15426851100819086
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.15424589694441643
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.15424439713071705
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.15431893371746072
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.15436243375481318
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.15441883922891414
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.1544039672727038
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.15448829468529104
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.15448829468529104 valid 0.20260784029960632
LOSS train 0.15448829468529104 valid 0.1792176589369774
LOSS train 0.15448829468529104 valid 0.1814132829507192
LOSS train 0.15448829468529104 valid 0.17159736156463623
LOSS train 0.15448829468529104 valid 0.1659451276063919
LOSS train 0.15448829468529104 valid 0.17701022326946259
LOSS train 0.15448829468529104 valid 0.18583590643746511
LOSS train 0.15448829468529104 valid 0.18492122925817966
LOSS train 0.15448829468529104 valid 0.18476583394739363
LOSS train 0.15448829468529104 valid 0.18682981580495833
LOSS train 0.15448829468529104 valid 0.18519933115352283
LOSS train 0.15448829468529104 valid 0.18588148802518845
LOSS train 0.15448829468529104 valid 0.1857946664094925
LOSS train 0.15448829468529104 valid 0.1848181773509298
LOSS train 0.15448829468529104 valid 0.183211420973142
LOSS train 0.15448829468529104 valid 0.18448803387582302
LOSS train 0.15448829468529104 valid 0.18513025168110342
LOSS train 0.15448829468529104 valid 0.18472857193814385
LOSS train 0.15448829468529104 valid 0.18725231525145078
LOSS train 0.15448829468529104 valid 0.18759963735938073
LOSS train 0.15448829468529104 valid 0.18567985083375657
LOSS train 0.15448829468529104 valid 0.18468127128752795
LOSS train 0.15448829468529104 valid 0.18418954442376675
LOSS train 0.15448829468529104 valid 0.1843777485191822
LOSS train 0.15448829468529104 valid 0.18321274161338807
LOSS train 0.15448829468529104 valid 0.18393342139629218
LOSS train 0.15448829468529104 valid 0.18391628673783056
LOSS train 0.15448829468529104 valid 0.183714359998703
LOSS train 0.15448829468529104 valid 0.18411611842698064
LOSS train 0.15448829468529104 valid 0.1851637934645017
LOSS train 0.15448829468529104 valid 0.185957228464465
LOSS train 0.15448829468529104 valid 0.1856348761357367
LOSS train 0.15448829468529104 valid 0.18604802949862045
LOSS train 0.15448829468529104 valid 0.1861593907370287
LOSS train 0.15448829468529104 valid 0.18797687802995955
LOSS train 0.15448829468529104 valid 0.18785925540659162
LOSS train 0.15448829468529104 valid 0.18811856089411555
LOSS train 0.15448829468529104 valid 0.18915308580586784
LOSS train 0.15448829468529104 valid 0.18788639093056703
LOSS train 0.15448829468529104 valid 0.18751154243946075
LOSS train 0.15448829468529104 valid 0.18802302239871607
LOSS train 0.15448829468529104 valid 0.18851273152090253
LOSS train 0.15448829468529104 valid 0.18856550961039786
LOSS train 0.15448829468529104 valid 0.1893599456684156
LOSS train 0.15448829468529104 valid 0.18910239934921264
LOSS train 0.15448829468529104 valid 0.18984127984098767
LOSS train 0.15448829468529104 valid 0.19003804852353764
LOSS train 0.15448829468529104 valid 0.1896894806995988
LOSS train 0.15448829468529104 valid 0.19015450137002127
LOSS train 0.15448829468529104 valid 0.18954136103391647
LOSS train 0.15448829468529104 valid 0.18976077963324153
LOSS train 0.15448829468529104 valid 0.18963241261931565
LOSS train 0.15448829468529104 valid 0.19020297111205334
LOSS train 0.15448829468529104 valid 0.18998007807466719
LOSS train 0.15448829468529104 valid 0.19008243896744467
LOSS train 0.15448829468529104 valid 0.18967715544360025
LOSS train 0.15448829468529104 valid 0.18982262046713577
LOSS train 0.15448829468529104 valid 0.18950570862868737
LOSS train 0.15448829468529104 valid 0.1899638938701759
LOSS train 0.15448829468529104 valid 0.18973787104090054
LOSS train 0.15448829468529104 valid 0.18988874066071432
LOSS train 0.15448829468529104 valid 0.19069206137810985
LOSS train 0.15448829468529104 valid 0.19048358405393268
LOSS train 0.15448829468529104 valid 0.191124010598287
LOSS train 0.15448829468529104 valid 0.19145890176296235
LOSS train 0.15448829468529104 valid 0.19138353837258887
LOSS train 0.15448829468529104 valid 0.19105135638322404
LOSS train 0.15448829468529104 valid 0.19096177882131407
LOSS train 0.15448829468529104 valid 0.19025579008503238
LOSS train 0.15448829468529104 valid 0.19061176627874374
LOSS train 0.15448829468529104 valid 0.19050407514605724
LOSS train 0.15448829468529104 valid 0.1907527879294422
LOSS train 0.15448829468529104 valid 0.19106341367715027
LOSS train 0.15448829468529104 valid 0.19083823324055285
LOSS train 0.15448829468529104 valid 0.1907884802420934
LOSS train 0.15448829468529104 valid 0.19130889973358103
LOSS train 0.15448829468529104 valid 0.19123610641275132
LOSS train 0.15448829468529104 valid 0.19117267582661066
LOSS train 0.15448829468529104 valid 0.1908018898360337
LOSS train 0.15448829468529104 valid 0.19003030024468898
LOSS train 0.15448829468529104 valid 0.1895554142969626
LOSS train 0.15448829468529104 valid 0.18976880882571384
LOSS train 0.15448829468529104 valid 0.18943303500313358
LOSS train 0.15448829468529104 valid 0.18920928133385523
LOSS train 0.15448829468529104 valid 0.18877771952572991
LOSS train 0.15448829468529104 valid 0.18835800401000088
LOSS train 0.15448829468529104 valid 0.18842663847166916
LOSS train 0.15448829468529104 valid 0.18819538300687616
LOSS train 0.15448829468529104 valid 0.18824700492151666
LOSS train 0.15448829468529104 valid 0.18850690060191685
LOSS train 0.15448829468529104 valid 0.18887310172175312
LOSS train 0.15448829468529104 valid 0.18884560984113943
LOSS train 0.15448829468529104 valid 0.1886910664778884
LOSS train 0.15448829468529104 valid 0.1888535090900482
LOSS train 0.15448829468529104 valid 0.18836054880368083
LOSS train 0.15448829468529104 valid 0.1882186490111053
LOSS train 0.15448829468529104 valid 0.18832274718382924
LOSS train 0.15448829468529104 valid 0.1884438273858051
LOSS train 0.15448829468529104 valid 0.18885642079391865
LOSS train 0.15448829468529104 valid 0.18912185713648796
LOSS train 0.15448829468529104 valid 0.18920785631283676
LOSS train 0.15448829468529104 valid 0.18955057070535772
LOSS train 0.15448829468529104 valid 0.18945537232658238
LOSS train 0.15448829468529104 valid 0.18962683313741133
LOSS train 0.15448829468529104 valid 0.18960863110565004
LOSS train 0.15448829468529104 valid 0.19007139020370986
LOSS train 0.15448829468529104 valid 0.18992832038447122
LOSS train 0.15448829468529104 valid 0.19026940578111895
LOSS train 0.15448829468529104 valid 0.19059495641550886
LOSS train 0.15448829468529104 valid 0.19073133766651154
LOSS train 0.15448829468529104 valid 0.19073799762639915
LOSS train 0.15448829468529104 valid 0.19038947844611748
LOSS train 0.15448829468529104 valid 0.1905614372375792
LOSS train 0.15448829468529104 valid 0.1907723905485973
LOSS train 0.15448829468529104 valid 0.1907912748015445
LOSS train 0.15448829468529104 valid 0.19122717760760208
LOSS train 0.15448829468529104 valid 0.1913200884293287
LOSS train 0.15448829468529104 valid 0.19104759395122528
LOSS train 0.15448829468529104 valid 0.19084696358993275
LOSS train 0.15448829468529104 valid 0.19063184633851052
LOSS train 0.15448829468529104 valid 0.1904111021806386
LOSS train 0.15448829468529104 valid 0.19032091390891154
LOSS train 0.15448829468529104 valid 0.1904711707578442
LOSS train 0.15448829468529104 valid 0.19057907392421075
LOSS train 0.15448829468529104 valid 0.19045692551136018
LOSS train 0.15448829468529104 valid 0.19073599647907985
LOSS train 0.15448829468529104 valid 0.19057083153349208
LOSS train 0.15448829468529104 valid 0.19057963881641626
LOSS train 0.15448829468529104 valid 0.19068552669166594
LOSS train 0.15448829468529104 valid 0.1906210782436224
LOSS train 0.15448829468529104 valid 0.19033162557441768
LOSS train 0.15448829468529104 valid 0.19006259739398956
LOSS train 0.15448829468529104 valid 0.18986991596849342
LOSS train 0.15448829468529104 valid 0.1899375021457672
LOSS train 0.15448829468529104 valid 0.18992082564919083
LOSS train 0.15448829468529104 valid 0.1900169814334196
LOSS train 0.15448829468529104 valid 0.18985740285720268
LOSS train 0.15448829468529104 valid 0.1897961068628491
LOSS train 0.15448829468529104 valid 0.1895204901480846
LOSS train 0.15448829468529104 valid 0.18966953424470764
LOSS train 0.15448829468529104 valid 0.18963008210168664
LOSS train 0.15448829468529104 valid 0.18964752859212983
LOSS train 0.15448829468529104 valid 0.18957236951047723
LOSS train 0.15448829468529104 valid 0.1895192265510559
LOSS train 0.15448829468529104 valid 0.18935204364102462
LOSS train 0.15448829468529104 valid 0.18968937505189687
LOSS train 0.15448829468529104 valid 0.18949434252417818
LOSS train 0.15448829468529104 valid 0.19037144218344945
LOSS train 0.15448829468529104 valid 0.19036885835980408
LOSS train 0.15448829468529104 valid 0.19028579185406366
LOSS train 0.15448829468529104 valid 0.19040670132400184
LOSS train 0.15448829468529104 valid 0.19030271313692393
LOSS train 0.15448829468529104 valid 0.19026518072567733
LOSS train 0.15448829468529104 valid 0.19025001162058347
LOSS train 0.15448829468529104 valid 0.1901991440403846
LOSS train 0.15448829468529104 valid 0.19043663889169693
LOSS train 0.15448829468529104 valid 0.19055880577700912
LOSS train 0.15448829468529104 valid 0.19058875418916532
LOSS train 0.15448829468529104 valid 0.19062813179298016
LOSS train 0.15448829468529104 valid 0.1904813432134688
LOSS train 0.15448829468529104 valid 0.19041300217927612
LOSS train 0.15448829468529104 valid 0.19024529878372026
LOSS train 0.15448829468529104 valid 0.19003718798877273
LOSS train 0.15448829468529104 valid 0.18994408718696454
LOSS train 0.15448829468529104 valid 0.18982013987772392
LOSS train 0.15448829468529104 valid 0.18988682114216218
LOSS train 0.15448829468529104 valid 0.19015311909292987
LOSS train 0.15448829468529104 valid 0.19021239167168028
LOSS train 0.15448829468529104 valid 0.19040452904955169
LOSS train 0.15448829468529104 valid 0.19041062777533252
LOSS train 0.15448829468529104 valid 0.19031121045873875
LOSS train 0.15448829468529104 valid 0.1901093207646248
LOSS train 0.15448829468529104 valid 0.19022110337122328
LOSS train 0.15448829468529104 valid 0.19014556902921062
LOSS train 0.15448829468529104 valid 0.19004296932901654
LOSS train 0.15448829468529104 valid 0.18993599272587083
LOSS train 0.15448829468529104 valid 0.19005456187967526
LOSS train 0.15448829468529104 valid 0.19026507227943185
LOSS train 0.15448829468529104 valid 0.1901798581277858
LOSS train 0.15448829468529104 valid 0.19015315936671362
LOSS train 0.15448829468529104 valid 0.19031609841802502
LOSS train 0.15448829468529104 valid 0.19027822726702953
LOSS train 0.15448829468529104 valid 0.19026034557428517
LOSS train 0.15448829468529104 valid 0.19010953726651875
LOSS train 0.15448829468529104 valid 0.1900671851796073
LOSS train 0.15448829468529104 valid 0.19004960061721904
LOSS train 0.15448829468529104 valid 0.18982897914029698
LOSS train 0.15448829468529104 valid 0.18984454378802726
LOSS train 0.15448829468529104 valid 0.18987297593916536
LOSS train 0.15448829468529104 valid 0.1899290698139291
LOSS train 0.15448829468529104 valid 0.1898595373980038
LOSS train 0.15448829468529104 valid 0.18980241815249124
LOSS train 0.15448829468529104 valid 0.18969016186313925
LOSS train 0.15448829468529104 valid 0.18955242510923406
LOSS train 0.15448829468529104 valid 0.18929573457974655
LOSS train 0.15448829468529104 valid 0.18931759620199398
LOSS train 0.15448829468529104 valid 0.18946516914718647
LOSS train 0.15448829468529104 valid 0.18933951847179972
LOSS train 0.15448829468529104 valid 0.1893938655529789
LOSS train 0.15448829468529104 valid 0.18926533229649067
LOSS train 0.15448829468529104 valid 0.1891609674217689
LOSS train 0.15448829468529104 valid 0.18914830478111117
LOSS train 0.15448829468529104 valid 0.18919495085777321
LOSS train 0.15448829468529104 valid 0.18938199306527773
LOSS train 0.15448829468529104 valid 0.1892123208540242
LOSS train 0.15448829468529104 valid 0.18933516921638285
LOSS train 0.15448829468529104 valid 0.18920727203721585
LOSS train 0.15448829468529104 valid 0.18918871650329003
LOSS train 0.15448829468529104 valid 0.1890698300594348
LOSS train 0.15448829468529104 valid 0.18902808349756967
LOSS train 0.15448829468529104 valid 0.18890209714948283
LOSS train 0.15448829468529104 valid 0.18868365961144556
LOSS train 0.15448829468529104 valid 0.18865934874809964
LOSS train 0.15448829468529104 valid 0.1885719622407004
LOSS train 0.15448829468529104 valid 0.18850164524344512
LOSS train 0.15448829468529104 valid 0.18828200155662167
LOSS train 0.15448829468529104 valid 0.18819430367463194
LOSS train 0.15448829468529104 valid 0.18813972940685553
LOSS train 0.15448829468529104 valid 0.18828547518002933
LOSS train 0.15448829468529104 valid 0.1882559601556171
LOSS train 0.15448829468529104 valid 0.18819573845259205
LOSS train 0.15448829468529104 valid 0.18825531287773237
LOSS train 0.15448829468529104 valid 0.18835271273493231
LOSS train 0.15448829468529104 valid 0.18849044306469814
LOSS train 0.15448829468529104 valid 0.18870422740777335
LOSS train 0.15448829468529104 valid 0.18881818381822216
LOSS train 0.15448829468529104 valid 0.18899356734647624
LOSS train 0.15448829468529104 valid 0.18907384018887552
LOSS train 0.15448829468529104 valid 0.18902361529623057
LOSS train 0.15448829468529104 valid 0.18911452811697255
LOSS train 0.15448829468529104 valid 0.1892945200720907
LOSS train 0.15448829468529104 valid 0.18933876623110524
LOSS train 0.15448829468529104 valid 0.18930866028873705
LOSS train 0.15448829468529104 valid 0.18946441447632945
LOSS train 0.15448829468529104 valid 0.18953628742948492
LOSS train 0.15448829468529104 valid 0.18944596966444435
LOSS train 0.15448829468529104 valid 0.1893435343785628
LOSS train 0.15448829468529104 valid 0.189320384829986
LOSS train 0.15448829468529104 valid 0.18913176107107346
LOSS train 0.15448829468529104 valid 0.18910658477495115
LOSS train 0.15448829468529104 valid 0.18924130895078428
LOSS train 0.15448829468529104 valid 0.18907954839389185
LOSS train 0.15448829468529104 valid 0.1892685737506843
LOSS train 0.15448829468529104 valid 0.18947198357982714
LOSS train 0.15448829468529104 valid 0.18944139845517216
LOSS train 0.15448829468529104 valid 0.1893546125753139
LOSS train 0.15448829468529104 valid 0.18947687570141394
LOSS train 0.15448829468529104 valid 0.1894686461216019
LOSS train 0.15448829468529104 valid 0.1894249175207682
LOSS train 0.15448829468529104 valid 0.18939284789562225
LOSS train 0.15448829468529104 valid 0.18923332907289148
LOSS train 0.15448829468529104 valid 0.18935173075823558
LOSS train 0.15448829468529104 valid 0.1892349546841482
LOSS train 0.15448829468529104 valid 0.1891841608004307
LOSS train 0.15448829468529104 valid 0.18922018706798555
LOSS train 0.15448829468529104 valid 0.18921661312924698
LOSS train 0.15448829468529104 valid 0.18905866517167147
LOSS train 0.15448829468529104 valid 0.18922601016454918
LOSS train 0.15448829468529104 valid 0.18923688958970736
LOSS train 0.15448829468529104 valid 0.18921446175529408
LOSS train 0.15448829468529104 valid 0.18934196342225276
LOSS train 0.15448829468529104 valid 0.18939738771842637
LOSS train 0.15448829468529104 valid 0.18932940932734385
LOSS train 0.15448829468529104 valid 0.1893503430553458
LOSS train 0.15448829468529104 valid 0.18935274509888775
LOSS train 0.15448829468529104 valid 0.18930945198114654
LOSS train 0.15448829468529104 valid 0.18941697517361086
LOSS train 0.15448829468529104 valid 0.18952793099764567
LOSS train 0.15448829468529104 valid 0.1896423714090014
LOSS train 0.15448829468529104 valid 0.1896073662572437
LOSS train 0.15448829468529104 valid 0.1897183487336134
LOSS train 0.15448829468529104 valid 0.18999165614299915
LOSS train 0.15448829468529104 valid 0.19013226414338136
LOSS train 0.15448829468529104 valid 0.19019721595257738
LOSS train 0.15448829468529104 valid 0.19012570559978484
LOSS train 0.15448829468529104 valid 0.19014155816124834
LOSS train 0.15448829468529104 valid 0.1900656473442966
LOSS train 0.15448829468529104 valid 0.18992093323374823
LOSS train 0.15448829468529104 valid 0.18988172736646455
LOSS train 0.15448829468529104 valid 0.18985151483544283
LOSS train 0.15448829468529104 valid 0.18979577041393497
LOSS train 0.15448829468529104 valid 0.18953945474527406
LOSS train 0.15448829468529104 valid 0.18961354144996972
LOSS train 0.15448829468529104 valid 0.18969388718021588
LOSS train 0.15448829468529104 valid 0.18969214498473888
LOSS train 0.15448829468529104 valid 0.18964432792125882
LOSS train 0.15448829468529104 valid 0.18956148985267102
LOSS train 0.15448829468529104 valid 0.18955540752762723
LOSS train 0.15448829468529104 valid 0.18956623159374744
LOSS train 0.15448829468529104 valid 0.1895599747269318
LOSS train 0.15448829468529104 valid 0.1894000826269081
LOSS train 0.15448829468529104 valid 0.18938993869272813
LOSS train 0.15448829468529104 valid 0.18938769707805875
LOSS train 0.15448829468529104 valid 0.1894269940145567
LOSS train 0.15448829468529104 valid 0.18955013885841532
LOSS train 0.15448829468529104 valid 0.1894898026225132
LOSS train 0.15448829468529104 valid 0.18953756684506382
LOSS train 0.15448829468529104 valid 0.1895345589708562
LOSS train 0.15448829468529104 valid 0.18957683112609347
LOSS train 0.15448829468529104 valid 0.18963607129951318
LOSS train 0.15448829468529104 valid 0.18962502264204215
LOSS train 0.15448829468529104 valid 0.18956563964683487
LOSS train 0.15448829468529104 valid 0.18968375821416528
LOSS train 0.15448829468529104 valid 0.18968268172619374
LOSS train 0.15448829468529104 valid 0.18963824234536436
LOSS train 0.15448829468529104 valid 0.18965837249861045
LOSS train 0.15448829468529104 valid 0.18964002223181803
LOSS train 0.15448829468529104 valid 0.1895552564218834
LOSS train 0.15448829468529104 valid 0.18967723385898042
LOSS train 0.15448829468529104 valid 0.1896508341114367
LOSS train 0.15448829468529104 valid 0.18957693393303268
LOSS train 0.15448829468529104 valid 0.1895618513942911
LOSS train 0.15448829468529104 valid 0.1896682089557663
LOSS train 0.15448829468529104 valid 0.18973665900386064
LOSS train 0.15448829468529104 valid 0.1896665850565547
LOSS train 0.15448829468529104 valid 0.1896194991290192
LOSS train 0.15448829468529104 valid 0.18966503401377977
LOSS train 0.15448829468529104 valid 0.1896798531261255
LOSS train 0.15448829468529104 valid 0.18973867807735845
LOSS train 0.15448829468529104 valid 0.18963611184153706
LOSS train 0.15448829468529104 valid 0.18979426584789685
LOSS train 0.15448829468529104 valid 0.18984385734852055
LOSS train 0.15448829468529104 valid 0.18981695348171995
LOSS train 0.15448829468529104 valid 0.18992992529622565
LOSS train 0.15448829468529104 valid 0.1898937364954215
LOSS train 0.15448829468529104 valid 0.19005255257150877
LOSS train 0.15448829468529104 valid 0.1901798861881644
LOSS train 0.15448829468529104 valid 0.19005922343945358
LOSS train 0.15448829468529104 valid 0.19019656782364047
LOSS train 0.15448829468529104 valid 0.19018446728587152
LOSS train 0.15448829468529104 valid 0.19007264246483224
LOSS train 0.15448829468529104 valid 0.18999803234564971
LOSS train 0.15448829468529104 valid 0.19000967050547357
LOSS train 0.15448829468529104 valid 0.19012983756775628
LOSS train 0.15448829468529104 valid 0.19015868370657538
LOSS train 0.15448829468529104 valid 0.19027309955674268
LOSS train 0.15448829468529104 valid 0.19026753848048275
LOSS train 0.15448829468529104 valid 0.19021361142661444
LOSS train 0.15448829468529104 valid 0.19027672437703716
LOSS train 0.15448829468529104 valid 0.1902919497779187
LOSS train 0.15448829468529104 valid 0.19015048922768785
LOSS train 0.15448829468529104 valid 0.19012501236726667
LOSS train 0.15448829468529104 valid 0.1901959348741495
LOSS train 0.15448829468529104 valid 0.19039411567757988
LOSS train 0.15448829468529104 valid 0.19045879048281822
LOSS train 0.15448829468529104 valid 0.19047194066850437
LOSS train 0.15448829468529104 valid 0.19033748424620037
LOSS train 0.15448829468529104 valid 0.19029998169120016
LOSS train 0.15448829468529104 valid 0.19036419409991676
LOSS train 0.15448829468529104 valid 0.1902940068287509
LOSS train 0.15448829468529104 valid 0.19019718716541925
LOSS train 0.15448829468529104 valid 0.1901773421982811
LOSS train 0.15448829468529104 valid 0.1902026037023358
LOSS train 0.15448829468529104 valid 0.19032797960322456
LOSS train 0.15448829468529104 valid 0.19044679837747358
LOSS train 0.15448829468529104 valid 0.19048372172656353
LOSS train 0.15448829468529104 valid 0.19052256082369834
LOSS train 0.15448829468529104 valid 0.19039962975589256
LOSS train 0.15448829468529104 valid 0.1904251686917374
LOSS train 0.15448829468529104 valid 0.19039994248499473
LOSS train 0.15448829468529104 valid 0.1903304999843859
LOSS train 0.15448829468529104 valid 0.1904277123346184
LOSS train 0.15448829468529104 valid 0.1903963729574989
LOSS train 0.15448829468529104 valid 0.19039948090174041
LOSS train 0.15448829468529104 valid 0.1903968510766552
LOSS train 0.15448829468529104 valid 0.19037017085757413
LOSS train 0.15448829468529104 valid 0.1902744704070793
LOSS train 0.15448829468529104 valid 0.1902629626388459
LOSS train 0.15448829468529104 valid 0.1903671075698483
EPOCH 10:
  batch 1 loss: 0.13471238315105438
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.14456794410943985
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.14941195646921793
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.15398677811026573
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.16081184148788452
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.1636734902858734
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.15520635460104262
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.15513320453464985
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.15202330549558005
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.15030831098556519
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.14714382182468067
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.1452731260408958
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.14579332734529787
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.14524095984441893
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.14519741982221604
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.14719719486311078
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.14723000324824276
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.14957419741484854
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.14851228069317968
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.14692979864776134
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.14754185045049303
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.14801145852966743
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.14857265061658362
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.14881101281692585
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.14904395788908004
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.14915576204657555
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.14974545521868599
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.1497443038970232
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.14880029369017173
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.14968463107943536
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.14994677396551256
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.15075118211098015
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.15007759156552228
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.15087481084115364
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.152179568367345
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.15127471192843384
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.1513557311248135
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.15145441046670863
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.1516267716502532
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.15179211404174567
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.1515027395472294
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.1517267810801665
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.15277062720337578
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.15179452774199573
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.15259176294008892
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.15260831072278644
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.15177907366701898
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.1516824889307221
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.1519012870837231
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.15219667553901672
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.15216160521787755
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.15244122967123985
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.15248735755119683
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.15319079353853507
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.15308237428014929
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.15304493345320225
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.15299760223480693
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.1522448923567246
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.15276270321870256
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.15263516182700793
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.15284091551772883
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.15246363897477427
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.15240120320093065
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.1525656774174422
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.15265119671821595
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.15259486549731457
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.15242851825792397
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.1525865247582688
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.15294323775215427
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.15313196054526737
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.15290428047448817
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.15299322228464815
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.15296907722949982
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.15301844679020546
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.15305748105049133
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.15321139442293266
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.1530024331885499
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.15272396879318434
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.15295705953730812
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.15318159554153682
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.15329941168979364
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.15288567524857638
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.15246985574443656
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.15277279638463542
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.15265995290349513
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.1526259082521117
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.1528524248593155
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.15312752457843584
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.15315895527601242
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.15335145915548007
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.15331783391289658
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.15328312316990417
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.15374323301097398
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.15392472190742798
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.15428271521078912
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.1543469016905874
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.15439751536883028
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.154435218368866
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.1544338877905499
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.15429949797689915
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.1543297775753654
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.15433293339960716
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.15392905452008387
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.15393980413388747
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.15348393484240486
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.15356050649622702
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.15374841137188616
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.15359521636532414
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.15358038226125437
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.1539021496745673
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.1537075391089594
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.15337009149204409
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.1530459184298473
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.15325036631864414
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.15291760648074357
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.15284480700462028
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.15315459866044867
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.15310629766623854
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.1531062057914854
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.15324810078988474
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.15301295856306377
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.1531040898112
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.1535176695362339
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.15345006880740966
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.1533786780834198
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.1534201716856351
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.15341177923003518
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.15348035795614123
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.15347082728101302
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.1535019522676101
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.15331456611174663
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.1532147964744857
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.15359945382390702
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.15362451525766457
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.15334957236493074
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.15332040130434668
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.15342898117582293
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.15330248612208644
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.15360996121768472
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.1537949262452977
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.15390070791996963
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.15388593334757106
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.15401227339789583
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.15433409608279666
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.15424665464409468
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.15425431519134403
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.1543238773739257
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.15435730459520947
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.15455043390893297
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.15461376582582792
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.15457514999125968
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.15451613357780794
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.15459753451199312
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.15444794079506552
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.15449946863997366
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.1543682706184112
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.15431864725746167
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.15427408749355545
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.1543485516178533
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.15447687353007494
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.15443430512403108
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.1543201937995575
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.15431130969999757
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.154268167413226
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.15437456807403854
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.15436249895088644
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.15421381133223722
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.15420150645964203
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.1539393910289516
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.15404293852693896
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.1539682689291692
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.15398714266890703
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.15381306853425297
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.15375747244762278
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.15390901893377304
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.1538439776155759
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.15377041339705894
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.15365718786468666
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.153850727532496
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.1536991080890099
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.15366985928781784
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.15358711357463847
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.1534401171438681
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.15331609777944244
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.15327146234544547
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.1532208963187151
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.153184320797576
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.15303675255718382
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.1530054168647559
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.15309625217004827
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.15304968100411728
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.15297240666889897
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.1530350300391721
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.15282190425955144
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.1528555337817241
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.15291822773917596
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.15307416585344955
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.15320075186665613
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.1531926156513056
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.15325667876750232
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.1532143510470343
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.1531949880910982
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.15310390370673146
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.1531285959496802
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.15297344392392692
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.1528726834984659
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.15280862404528447
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.1526928640758762
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.15272037934458427
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.15285692385264804
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.15286452404413178
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.15298485249843238
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.15305881370121324
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.15288233151224173
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.15301149799380193
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.152871107544612
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.1529450688493966
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.15297090392047114
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.1530614634082742
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.15307622355493633
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.15300820383550895
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.15299387900410472
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.15288465780794888
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.1530168409725385
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.15290276103549533
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.15290572342619432
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.15290704402366923
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.15272283750145058
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.15262356673786212
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.15279308194699495
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.15291862409094195
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.15286726819287086
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.15283650601114837
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.15281770677648038
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.1528338910417354
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.1527249052868051
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.1527532613855877
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.1527559029079285
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.1527555940290874
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.15275056970616183
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.1528182614641071
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.1527453920811661
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.15271766171043302
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.15271592714258883
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.15272110147135598
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.15276084646461455
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.15262762049914372
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.15275011199616617
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.15278942440168924
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.1528179544210434
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.15299378282045464
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.15292062350208796
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.1529197787696665
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.15292067626329858
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.15297142851586434
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.1529289449681528
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.15283668922543062
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.1527373331577279
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.15277224451188415
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.15274881737736556
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.1526392307774774
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.15255516977938077
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.15257431362744972
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.15245972912419925
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.15248525311362068
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.15244694978446888
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.1524339992455329
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.1524676532109282
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.15238522680053923
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.15239928723485383
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.1523683738114649
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.15235193062792807
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.1525531564549212
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.15244658633957814
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.15237545820799742
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.1525725770363773
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.1525421319025088
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.15247545337719884
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.1525152687210336
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.15260608962603978
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.15254901238183535
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.15251728596416772
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.15253265012490033
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.15252620412010542
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.15261794835851902
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.152649402878918
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.15259306454700045
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.1526223292781247
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.1525418405198721
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.15261268620860988
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.15258948233529054
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.15254316856599834
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.15260878754557197
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.1527100911959499
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.15270311716249432
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.15275070700492407
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.1528029926497527
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.15269984032383702
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.15258131824109866
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.15256363379458587
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.15265732110041716
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.15262213233389602
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.15261106279128456
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.15258590781473017
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.15263171555077443
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.15261618800315202
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.15258658298743277
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.15256369948483905
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.15254596601508583
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.15247011720653503
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.15249125637904623
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.15244630688371566
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.15248794165758278
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.15246141875150857
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.15247646974665777
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.15257972085117538
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.15254103721410317
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.15251164239063952
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.1524901550531761
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.15248156331945212
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.15243570095746317
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.152445131785566
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.15250801235068323
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.15242522722685042
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.15247662106385598
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.15235799397884703
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.1522446623134686
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.1522801179756842
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.1522192117336311
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.15230102414886157
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.1523431759001265
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.15224801389926887
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.15214382013580105
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.1521528469855914
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.15224985880638237
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.15227243313122363
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.15219143271799965
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.15212344849956105
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.1520004036635776
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.15200359082835563
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.15189363916685855
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.1520723756947364
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.1520637551175957
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.1520559985060678
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.15205186901317128
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.15205003723846694
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.15201804348903705
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.1519988365257266
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.1519465494505997
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.15187121474317142
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.1518992370477429
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.1519464150875468
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.1519993853256655
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.15203762582718988
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.15205548034167626
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.1520139282632075
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.15206370715584075
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.15215256167107455
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.15214654103876157
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.15205727946013212
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.15199599633834368
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.15202324349659582
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.15197435516053323
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.15191274113789366
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.15197698137123292
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.1519287494240237
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.1519787584200542
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.15203817356782762
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.15203040674692248
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.15194770647464573
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.15194371420980463
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.1518833143937011
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.15187490312008053
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.15191632905666203
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.1519327731728554
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.1520330953669358
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.1519756457652274
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.15203305824644983
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.15194671990295827
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.1518661482945869
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.15182452991096365
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.15174335260821886
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.1518371891492963
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.1517995617662867
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.1517333105012968
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.1517452293418232
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.1517072689240601
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.15173750098065003
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.1517746620512254
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.1517127010684747
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.15168540073020378
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.15171709428636396
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.15183786664906956
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.15185462199340616
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.15185573161402835
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.1518341043982843
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.1517365462984186
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.1518065768420397
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.15171265355626443
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.1516864561662078
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.15168665643047513
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.1516224050254964
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.1515451734030513
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.15150684575633247
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.1515138766647857
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.15145089561716088
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.1514181666423999
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.1515486218108266
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.1515526877565897
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.15160337588409098
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.15158083926152138
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.15162608541041903
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.1516235870180638
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.15168841507123865
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.15174544263316925
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.15177509765355632
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.15175273678571485
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.1517171156392143
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.1518606565234769
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.15180177035785858
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.15190375127305328
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.15197018551600489
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.15188874015430467
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.15185900967357294
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.15186382728464462
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.15179883272435185
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.15179659003954582
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.15174805111834935
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.15168528847552679
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.15163878266548
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.15167688903357757
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.15161739294934604
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.15157167047399847
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.1516415543579561
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.15158401852709125
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.15167921315024205
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.15173269990720792
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.15170018560333884
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.15161566594127643
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.1515824938870289
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.15158098038970208
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.15155124925582658
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.15154016958066088
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.15152726575799352
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.15154654659916847
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.15156189838399267
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.15154224815851355
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.15162609775351094
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.15161479254541524
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.15154332353009117
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.15155623159360992
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.15156984965490028
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.1515270106774029
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.15151046708816998
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.15147311615419912
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.1515281339617152
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.15149708864986244
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.15150347651352528
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.15151568552507555
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.15159053433200587
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.15156770146849874
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.1515218225908486
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.15142935820197442
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.15145933464297962
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.15147960666687257
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.15146797988164068
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.1514643243727388
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.15153670275950024
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.1515844242532116
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.15162613055807478
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.15161506436179903
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.1517146590907695
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.1517146590907695 valid 0.20929808914661407
LOSS train 0.1517146590907695 valid 0.1848568543791771
LOSS train 0.1517146590907695 valid 0.18609141806761423
LOSS train 0.1517146590907695 valid 0.1763266883790493
LOSS train 0.1517146590907695 valid 0.1711064785718918
LOSS train 0.1517146590907695 valid 0.18126474072535834
LOSS train 0.1517146590907695 valid 0.18976680295807974
LOSS train 0.1517146590907695 valid 0.18945065699517727
LOSS train 0.1517146590907695 valid 0.18935210837258232
LOSS train 0.1517146590907695 valid 0.19218937456607818
LOSS train 0.1517146590907695 valid 0.19029642777009445
LOSS train 0.1517146590907695 valid 0.19093170389533043
LOSS train 0.1517146590907695 valid 0.1907013184749163
LOSS train 0.1517146590907695 valid 0.189683626805033
LOSS train 0.1517146590907695 valid 0.18774926761786143
LOSS train 0.1517146590907695 valid 0.1891737086698413
LOSS train 0.1517146590907695 valid 0.19008052699706135
LOSS train 0.1517146590907695 valid 0.1894727067814933
LOSS train 0.1517146590907695 valid 0.1923445635720303
LOSS train 0.1517146590907695 valid 0.19246929958462716
LOSS train 0.1517146590907695 valid 0.1904995703981036
LOSS train 0.1517146590907695 valid 0.18938861245458777
LOSS train 0.1517146590907695 valid 0.18880146353141122
LOSS train 0.1517146590907695 valid 0.18886053562164307
LOSS train 0.1517146590907695 valid 0.18773469030857087
LOSS train 0.1517146590907695 valid 0.1884120381795443
LOSS train 0.1517146590907695 valid 0.1882882046478766
LOSS train 0.1517146590907695 valid 0.18815324721591814
LOSS train 0.1517146590907695 valid 0.18868413721692973
LOSS train 0.1517146590907695 valid 0.1898364002505938
LOSS train 0.1517146590907695 valid 0.19055585178636736
LOSS train 0.1517146590907695 valid 0.19017770374193788
LOSS train 0.1517146590907695 valid 0.190465151812091
LOSS train 0.1517146590907695 valid 0.19061410120304892
LOSS train 0.1517146590907695 valid 0.19239643684455326
LOSS train 0.1517146590907695 valid 0.1922881259686417
LOSS train 0.1517146590907695 valid 0.19259272153313095
LOSS train 0.1517146590907695 valid 0.19355025573780663
LOSS train 0.1517146590907695 valid 0.19232483055347052
LOSS train 0.1517146590907695 valid 0.19220652431249619
LOSS train 0.1517146590907695 valid 0.19267753057363557
LOSS train 0.1517146590907695 valid 0.19313983832086837
LOSS train 0.1517146590907695 valid 0.19322938662628794
LOSS train 0.1517146590907695 valid 0.19409351254051382
LOSS train 0.1517146590907695 valid 0.1938606278763877
LOSS train 0.1517146590907695 valid 0.19454966258743536
LOSS train 0.1517146590907695 valid 0.1948036713168976
LOSS train 0.1517146590907695 valid 0.1945150609438618
LOSS train 0.1517146590907695 valid 0.19510325029188272
LOSS train 0.1517146590907695 valid 0.19448148667812348
LOSS train 0.1517146590907695 valid 0.19479955732822418
LOSS train 0.1517146590907695 valid 0.1946654930137671
LOSS train 0.1517146590907695 valid 0.19512704436509115
LOSS train 0.1517146590907695 valid 0.19486073625308495
LOSS train 0.1517146590907695 valid 0.19484271786429666
LOSS train 0.1517146590907695 valid 0.19443933186786516
LOSS train 0.1517146590907695 valid 0.19453613826057367
LOSS train 0.1517146590907695 valid 0.1942552338386404
LOSS train 0.1517146590907695 valid 0.19465857491654864
LOSS train 0.1517146590907695 valid 0.19455758010347685
LOSS train 0.1517146590907695 valid 0.19475398933301208
LOSS train 0.1517146590907695 valid 0.19548756749399246
LOSS train 0.1517146590907695 valid 0.19528438552977548
LOSS train 0.1517146590907695 valid 0.1959210995119065
LOSS train 0.1517146590907695 valid 0.1963687186057751
LOSS train 0.1517146590907695 valid 0.196285039638028
LOSS train 0.1517146590907695 valid 0.19593376788630415
LOSS train 0.1517146590907695 valid 0.19585193584070487
LOSS train 0.1517146590907695 valid 0.19515202788339145
LOSS train 0.1517146590907695 valid 0.1955606730920928
LOSS train 0.1517146590907695 valid 0.19537469597769455
LOSS train 0.1517146590907695 valid 0.19571198253995842
LOSS train 0.1517146590907695 valid 0.19601748618361067
LOSS train 0.1517146590907695 valid 0.19578388816601522
LOSS train 0.1517146590907695 valid 0.19563907464345295
LOSS train 0.1517146590907695 valid 0.1961177281643215
LOSS train 0.1517146590907695 valid 0.19604710460483254
LOSS train 0.1517146590907695 valid 0.19597290265254486
LOSS train 0.1517146590907695 valid 0.19563595573358897
LOSS train 0.1517146590907695 valid 0.19495151229202748
LOSS train 0.1517146590907695 valid 0.19445834612404858
LOSS train 0.1517146590907695 valid 0.19458498642212008
LOSS train 0.1517146590907695 valid 0.19427475674324726
LOSS train 0.1517146590907695 valid 0.19403376589928353
LOSS train 0.1517146590907695 valid 0.19350446550285114
LOSS train 0.1517146590907695 valid 0.19307621499133665
LOSS train 0.1517146590907695 valid 0.19310786566515079
LOSS train 0.1517146590907695 valid 0.192859169265086
LOSS train 0.1517146590907695 valid 0.19294350719853734
LOSS train 0.1517146590907695 valid 0.1932075189219581
LOSS train 0.1517146590907695 valid 0.19356772205331824
LOSS train 0.1517146590907695 valid 0.19351121605090474
LOSS train 0.1517146590907695 valid 0.1933273405477565
LOSS train 0.1517146590907695 valid 0.19357228929058035
LOSS train 0.1517146590907695 valid 0.1930592831812407
LOSS train 0.1517146590907695 valid 0.1929066057006518
LOSS train 0.1517146590907695 valid 0.19301412246890903
LOSS train 0.1517146590907695 valid 0.19315004333549615
LOSS train 0.1517146590907695 valid 0.19353295396072695
LOSS train 0.1517146590907695 valid 0.19383562803268434
LOSS train 0.1517146590907695 valid 0.1939568814664784
LOSS train 0.1517146590907695 valid 0.1942990521589915
LOSS train 0.1517146590907695 valid 0.1942378755622697
LOSS train 0.1517146590907695 valid 0.19430267352324265
LOSS train 0.1517146590907695 valid 0.19430064161618552
LOSS train 0.1517146590907695 valid 0.19480586051940918
LOSS train 0.1517146590907695 valid 0.19468566427163989
LOSS train 0.1517146590907695 valid 0.19501455887048333
LOSS train 0.1517146590907695 valid 0.1953785676474965
LOSS train 0.1517146590907695 valid 0.19550819965926083
LOSS train 0.1517146590907695 valid 0.19550134751710807
LOSS train 0.1517146590907695 valid 0.19514842185058764
LOSS train 0.1517146590907695 valid 0.19532816586768734
LOSS train 0.1517146590907695 valid 0.19550690538527674
LOSS train 0.1517146590907695 valid 0.19551006801750348
LOSS train 0.1517146590907695 valid 0.19594705901269255
LOSS train 0.1517146590907695 valid 0.1960983697929953
LOSS train 0.1517146590907695 valid 0.19581340998411179
LOSS train 0.1517146590907695 valid 0.1955947820879832
LOSS train 0.1517146590907695 valid 0.19539474646250407
LOSS train 0.1517146590907695 valid 0.19512490792707962
LOSS train 0.1517146590907695 valid 0.19502805002400134
LOSS train 0.1517146590907695 valid 0.19523964155980242
LOSS train 0.1517146590907695 valid 0.19538670754240406
LOSS train 0.1517146590907695 valid 0.19529345071315765
LOSS train 0.1517146590907695 valid 0.19555654447703136
LOSS train 0.1517146590907695 valid 0.1954042257990424
LOSS train 0.1517146590907695 valid 0.1954001550329849
LOSS train 0.1517146590907695 valid 0.19553855997185374
LOSS train 0.1517146590907695 valid 0.1955298444399467
LOSS train 0.1517146590907695 valid 0.19528840504984818
LOSS train 0.1517146590907695 valid 0.19501373826554327
LOSS train 0.1517146590907695 valid 0.1947991934261824
LOSS train 0.1517146590907695 valid 0.19482386123333403
LOSS train 0.1517146590907695 valid 0.19476833972666
LOSS train 0.1517146590907695 valid 0.19485004246234894
LOSS train 0.1517146590907695 valid 0.19470736569296704
LOSS train 0.1517146590907695 valid 0.19466273065494455
LOSS train 0.1517146590907695 valid 0.19441842239537685
LOSS train 0.1517146590907695 valid 0.19454590039593833
LOSS train 0.1517146590907695 valid 0.1944970154593177
LOSS train 0.1517146590907695 valid 0.19451980040946476
LOSS train 0.1517146590907695 valid 0.19447059898109703
LOSS train 0.1517146590907695 valid 0.19442743704550797
LOSS train 0.1517146590907695 valid 0.19422499259998058
LOSS train 0.1517146590907695 valid 0.19454322683892838
LOSS train 0.1517146590907695 valid 0.19432622995100865
LOSS train 0.1517146590907695 valid 0.19516890747724353
LOSS train 0.1517146590907695 valid 0.19519125325167738
LOSS train 0.1517146590907695 valid 0.19511520673831304
LOSS train 0.1517146590907695 valid 0.19524940986506986
LOSS train 0.1517146590907695 valid 0.19510838350183085
LOSS train 0.1517146590907695 valid 0.1950410176920735
LOSS train 0.1517146590907695 valid 0.19506559143592786
LOSS train 0.1517146590907695 valid 0.1950234156462454
LOSS train 0.1517146590907695 valid 0.19524957755437264
LOSS train 0.1517146590907695 valid 0.1953804057305026
LOSS train 0.1517146590907695 valid 0.19545497820724414
LOSS train 0.1517146590907695 valid 0.19547132847818938
LOSS train 0.1517146590907695 valid 0.19531799284741283
LOSS train 0.1517146590907695 valid 0.1952264441281372
LOSS train 0.1517146590907695 valid 0.19509429981311163
LOSS train 0.1517146590907695 valid 0.19486764676731788
LOSS train 0.1517146590907695 valid 0.1947677987741261
LOSS train 0.1517146590907695 valid 0.19462225437164307
LOSS train 0.1517146590907695 valid 0.19467055932225952
LOSS train 0.1517146590907695 valid 0.19494574708852938
LOSS train 0.1517146590907695 valid 0.1949900970572517
LOSS train 0.1517146590907695 valid 0.19515410541782718
LOSS train 0.1517146590907695 valid 0.19518695526263294
LOSS train 0.1517146590907695 valid 0.1951291453942918
LOSS train 0.1517146590907695 valid 0.1948987517592519
LOSS train 0.1517146590907695 valid 0.19502037617167986
LOSS train 0.1517146590907695 valid 0.1949618885229374
LOSS train 0.1517146590907695 valid 0.1948549851349422
LOSS train 0.1517146590907695 valid 0.19477076896212317
LOSS train 0.1517146590907695 valid 0.19488157904417502
LOSS train 0.1517146590907695 valid 0.1951077537757627
LOSS train 0.1517146590907695 valid 0.19502135545181828
LOSS train 0.1517146590907695 valid 0.19500823492805164
LOSS train 0.1517146590907695 valid 0.19518309189469774
LOSS train 0.1517146590907695 valid 0.19516684237744783
LOSS train 0.1517146590907695 valid 0.19511087018935408
LOSS train 0.1517146590907695 valid 0.19494481157997381
LOSS train 0.1517146590907695 valid 0.19490082127016944
LOSS train 0.1517146590907695 valid 0.19488385808403774
LOSS train 0.1517146590907695 valid 0.19467514530222685
LOSS train 0.1517146590907695 valid 0.19469184262004305
LOSS train 0.1517146590907695 valid 0.19472071489013693
LOSS train 0.1517146590907695 valid 0.19474838705439318
LOSS train 0.1517146590907695 valid 0.19469208289815493
LOSS train 0.1517146590907695 valid 0.19460779149085283
LOSS train 0.1517146590907695 valid 0.1945217364501459
LOSS train 0.1517146590907695 valid 0.1943844709660589
LOSS train 0.1517146590907695 valid 0.19413635310454247
LOSS train 0.1517146590907695 valid 0.19418055146020285
LOSS train 0.1517146590907695 valid 0.19430864984311427
LOSS train 0.1517146590907695 valid 0.19418138115092962
LOSS train 0.1517146590907695 valid 0.1942367526754063
LOSS train 0.1517146590907695 valid 0.19407407157123088
LOSS train 0.1517146590907695 valid 0.19392733112793065
LOSS train 0.1517146590907695 valid 0.19391644782949202
LOSS train 0.1517146590907695 valid 0.19396363264821434
LOSS train 0.1517146590907695 valid 0.1941173592326688
LOSS train 0.1517146590907695 valid 0.19396057019873364
LOSS train 0.1517146590907695 valid 0.19407379931037866
LOSS train 0.1517146590907695 valid 0.19395465702538328
LOSS train 0.1517146590907695 valid 0.19395419496756333
LOSS train 0.1517146590907695 valid 0.19380460301654762
LOSS train 0.1517146590907695 valid 0.1937663475672404
LOSS train 0.1517146590907695 valid 0.19366127626873306
LOSS train 0.1517146590907695 valid 0.19344780692514382
LOSS train 0.1517146590907695 valid 0.19343739824955453
LOSS train 0.1517146590907695 valid 0.19336206521664825
LOSS train 0.1517146590907695 valid 0.19328982213208842
LOSS train 0.1517146590907695 valid 0.19309321556378295
LOSS train 0.1517146590907695 valid 0.19299769944034964
LOSS train 0.1517146590907695 valid 0.19292456540492697
LOSS train 0.1517146590907695 valid 0.19305601559545352
LOSS train 0.1517146590907695 valid 0.1930266019972888
LOSS train 0.1517146590907695 valid 0.19297778545731334
LOSS train 0.1517146590907695 valid 0.19299760051407255
LOSS train 0.1517146590907695 valid 0.19310337746089884
LOSS train 0.1517146590907695 valid 0.1932287190907768
LOSS train 0.1517146590907695 valid 0.19342961927254995
LOSS train 0.1517146590907695 valid 0.19354121723269993
LOSS train 0.1517146590907695 valid 0.19375460269955286
LOSS train 0.1517146590907695 valid 0.19384254959591649
LOSS train 0.1517146590907695 valid 0.19380354796694876
LOSS train 0.1517146590907695 valid 0.19389003754957862
LOSS train 0.1517146590907695 valid 0.19405033874821354
LOSS train 0.1517146590907695 valid 0.19409128676714568
LOSS train 0.1517146590907695 valid 0.19405746568681856
LOSS train 0.1517146590907695 valid 0.19419060844896185
LOSS train 0.1517146590907695 valid 0.1942759282411413
LOSS train 0.1517146590907695 valid 0.19418657419540114
LOSS train 0.1517146590907695 valid 0.19410173519502713
LOSS train 0.1517146590907695 valid 0.19408567800742238
LOSS train 0.1517146590907695 valid 0.19389934532313166
LOSS train 0.1517146590907695 valid 0.19386871761331956
LOSS train 0.1517146590907695 valid 0.19398537562595858
LOSS train 0.1517146590907695 valid 0.1938158973189425
LOSS train 0.1517146590907695 valid 0.1940063981362331
LOSS train 0.1517146590907695 valid 0.19420726560666912
LOSS train 0.1517146590907695 valid 0.19417537219670353
LOSS train 0.1517146590907695 valid 0.19409291724848554
LOSS train 0.1517146590907695 valid 0.19420507200333753
LOSS train 0.1517146590907695 valid 0.19420611185412254
LOSS train 0.1517146590907695 valid 0.19415108895445443
LOSS train 0.1517146590907695 valid 0.19414233767986297
LOSS train 0.1517146590907695 valid 0.1940004528162489
LOSS train 0.1517146590907695 valid 0.19411563080927682
LOSS train 0.1517146590907695 valid 0.19400574408265442
LOSS train 0.1517146590907695 valid 0.1939452673037221
LOSS train 0.1517146590907695 valid 0.19398998191543654
LOSS train 0.1517146590907695 valid 0.19401920842938125
LOSS train 0.1517146590907695 valid 0.1938611117086522
LOSS train 0.1517146590907695 valid 0.19402352131383363
LOSS train 0.1517146590907695 valid 0.19400560545184897
LOSS train 0.1517146590907695 valid 0.19399166227533268
LOSS train 0.1517146590907695 valid 0.19411541544386254
LOSS train 0.1517146590907695 valid 0.19416616487366553
LOSS train 0.1517146590907695 valid 0.1941020811691937
LOSS train 0.1517146590907695 valid 0.19414324002961317
LOSS train 0.1517146590907695 valid 0.1941335195640348
LOSS train 0.1517146590907695 valid 0.19408420658201203
LOSS train 0.1517146590907695 valid 0.19417334484696835
LOSS train 0.1517146590907695 valid 0.19427322012497417
LOSS train 0.1517146590907695 valid 0.19439838228394107
LOSS train 0.1517146590907695 valid 0.19436861221437102
LOSS train 0.1517146590907695 valid 0.1944614314505095
LOSS train 0.1517146590907695 valid 0.1947361758964903
LOSS train 0.1517146590907695 valid 0.19486966587248303
LOSS train 0.1517146590907695 valid 0.19493506185329743
LOSS train 0.1517146590907695 valid 0.19487069742246108
LOSS train 0.1517146590907695 valid 0.19485830790970637
LOSS train 0.1517146590907695 valid 0.19475280297146808
LOSS train 0.1517146590907695 valid 0.1946086870144597
LOSS train 0.1517146590907695 valid 0.1945818339517894
LOSS train 0.1517146590907695 valid 0.19453755877912043
LOSS train 0.1517146590907695 valid 0.19446331692123753
LOSS train 0.1517146590907695 valid 0.19421819756005673
LOSS train 0.1517146590907695 valid 0.19428250356824153
LOSS train 0.1517146590907695 valid 0.19435872525816233
LOSS train 0.1517146590907695 valid 0.1943676127676378
LOSS train 0.1517146590907695 valid 0.19431661157966493
LOSS train 0.1517146590907695 valid 0.19423251269378727
LOSS train 0.1517146590907695 valid 0.19422937996892464
LOSS train 0.1517146590907695 valid 0.19423125771915212
LOSS train 0.1517146590907695 valid 0.19420167875700983
LOSS train 0.1517146590907695 valid 0.19404575269656493
LOSS train 0.1517146590907695 valid 0.19402591879033063
LOSS train 0.1517146590907695 valid 0.19402536373496462
LOSS train 0.1517146590907695 valid 0.1940485475217404
LOSS train 0.1517146590907695 valid 0.19417178494445347
LOSS train 0.1517146590907695 valid 0.19410518048381484
LOSS train 0.1517146590907695 valid 0.19414561715993015
LOSS train 0.1517146590907695 valid 0.19414848289233727
LOSS train 0.1517146590907695 valid 0.19419208188918125
LOSS train 0.1517146590907695 valid 0.19425659343600274
LOSS train 0.1517146590907695 valid 0.19424813822456372
LOSS train 0.1517146590907695 valid 0.19419664514577942
LOSS train 0.1517146590907695 valid 0.19432133519806877
LOSS train 0.1517146590907695 valid 0.19433016256478272
LOSS train 0.1517146590907695 valid 0.1942786327639564
LOSS train 0.1517146590907695 valid 0.19430312718830856
LOSS train 0.1517146590907695 valid 0.19428599026381776
LOSS train 0.1517146590907695 valid 0.19418874842586456
LOSS train 0.1517146590907695 valid 0.1943136155123078
LOSS train 0.1517146590907695 valid 0.19427783138328983
LOSS train 0.1517146590907695 valid 0.19419477127755952
LOSS train 0.1517146590907695 valid 0.1941899156723267
LOSS train 0.1517146590907695 valid 0.1942709444906003
LOSS train 0.1517146590907695 valid 0.19435136774732809
LOSS train 0.1517146590907695 valid 0.1942786876644407
LOSS train 0.1517146590907695 valid 0.19423775157973736
LOSS train 0.1517146590907695 valid 0.19427434329166773
LOSS train 0.1517146590907695 valid 0.19429163442655178
LOSS train 0.1517146590907695 valid 0.1943377331132799
LOSS train 0.1517146590907695 valid 0.1942252665758133
LOSS train 0.1517146590907695 valid 0.19438247015914442
LOSS train 0.1517146590907695 valid 0.1944262563534405
LOSS train 0.1517146590907695 valid 0.1943993356667067
LOSS train 0.1517146590907695 valid 0.19449107928408516
LOSS train 0.1517146590907695 valid 0.19447532965586736
LOSS train 0.1517146590907695 valid 0.19466072064967244
LOSS train 0.1517146590907695 valid 0.1948033557116803
LOSS train 0.1517146590907695 valid 0.19470756360125252
LOSS train 0.1517146590907695 valid 0.1948512187029453
LOSS train 0.1517146590907695 valid 0.19482839568094773
LOSS train 0.1517146590907695 valid 0.1947172534879961
LOSS train 0.1517146590907695 valid 0.19463486623871756
LOSS train 0.1517146590907695 valid 0.19465170899131992
LOSS train 0.1517146590907695 valid 0.19478966826628782
LOSS train 0.1517146590907695 valid 0.19481205237445548
LOSS train 0.1517146590907695 valid 0.19491602573543787
LOSS train 0.1517146590907695 valid 0.19491874985949575
LOSS train 0.1517146590907695 valid 0.1948628422952968
LOSS train 0.1517146590907695 valid 0.1949208649562172
LOSS train 0.1517146590907695 valid 0.19494385798187816
LOSS train 0.1517146590907695 valid 0.1948020683268298
LOSS train 0.1517146590907695 valid 0.19480106064624952
LOSS train 0.1517146590907695 valid 0.19486350529221683
LOSS train 0.1517146590907695 valid 0.19506605961468332
LOSS train 0.1517146590907695 valid 0.19512896317502726
LOSS train 0.1517146590907695 valid 0.19512425927240726
LOSS train 0.1517146590907695 valid 0.19498190623023667
LOSS train 0.1517146590907695 valid 0.19494513936083893
LOSS train 0.1517146590907695 valid 0.19500741742572675
LOSS train 0.1517146590907695 valid 0.19493689200707845
LOSS train 0.1517146590907695 valid 0.19482259426870915
LOSS train 0.1517146590907695 valid 0.19479443577372216
LOSS train 0.1517146590907695 valid 0.19481753370072957
LOSS train 0.1517146590907695 valid 0.19493858570938055
LOSS train 0.1517146590907695 valid 0.1950588971376419
LOSS train 0.1517146590907695 valid 0.19512205792779333
LOSS train 0.1517146590907695 valid 0.1951567353237243
LOSS train 0.1517146590907695 valid 0.1950308000824971
LOSS train 0.1517146590907695 valid 0.19506625079842996
LOSS train 0.1517146590907695 valid 0.19503207090828154
LOSS train 0.1517146590907695 valid 0.1949593498683702
LOSS train 0.1517146590907695 valid 0.1950665326243606
LOSS train 0.1517146590907695 valid 0.19502507441628406
LOSS train 0.1517146590907695 valid 0.1950352210346814
LOSS train 0.1517146590907695 valid 0.19503463016797418
LOSS train 0.1517146590907695 valid 0.19501862312600912
LOSS train 0.1517146590907695 valid 0.19493141693217877
LOSS train 0.1517146590907695 valid 0.1949249749355342
LOSS train 0.1517146590907695 valid 0.19499673163341635
EPOCH 11:
  batch 1 loss: 0.1423802673816681
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.14062318205833435
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.14822399616241455
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.15166211128234863
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.15826413929462432
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.16105163594086966
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.15226847252675466
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.15228968020528555
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.14924571497572792
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.14797586426138878
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.144494005902247
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.14263334311544895
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.14311710859720522
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.14206432391490256
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.14199159890413285
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.1439036331139505
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.1437859609723091
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.14631915216644606
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.14539764782315806
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.14404802583158016
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.1445931157185918
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.14500352232293648
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.14549104836971863
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.1458407798781991
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.14612190097570418
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.1462189397559716
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.14685095653489785
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.14686359811042035
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.14580188974224287
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.14689192101359366
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.14737053239537823
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.14791717077605426
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.14717621085318652
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.14789260945775928
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.14938073818172729
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.14852677927249008
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.1486966181043032
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.14883753363239138
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.14898603715193579
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.14916728232055904
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.1490484528788706
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.1493552555995328
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.15019174457289453
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.14932490478862415
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.14998236861493852
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.14984678898168646
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.14908091850737307
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.14909486783047518
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.14930671696760217
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.14961197048425676
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.14968028752242818
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.14995920887360206
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.15018773781803418
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.1507288827388375
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.15061713592572645
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.1505032823021923
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.15054919609898015
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.14988351793124757
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.15046338686498545
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.15017358337839445
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.15037988590412452
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.15002573593970267
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.15007623982807947
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.15028460859321058
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.15027720102897058
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.1501764120026068
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.1500152836539852
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.14998135430847898
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.1502483981674996
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.15047607166426522
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.15023692120128954
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.1503319949325588
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.15021607320602626
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.15036108465613546
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.15043394267559052
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.15060243186982056
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.15045321161870834
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.15028976649045944
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.1505827009677887
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.15085790250450373
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.15091797838240495
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.15053979916180052
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.1500896411128791
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.15031437433901287
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.1502904211773592
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.15028225093386893
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.1504377224322023
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.1506936555220322
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.1507761560464173
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.15101599742968877
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.15093083417677616
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.15086657343351323
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.15134727682477683
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.15139417318587609
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.15175583927254926
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.1518180719576776
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.15185629799193942
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.15185986428844686
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.15186478950158513
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.15182544603943826
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.1518875367865704
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.1519114572335692
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.15150567469666304
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.15154921492704979
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.15110883528277988
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.1512071043252945
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.15140058309118323
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.15119759917811113
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.15118365727980201
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.1516011583534154
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.15138642489910126
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.151056612908308
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.15066602094247278
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.15093965821883135
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.15055413032355516
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.1504452507942915
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.15076402218168616
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.1506879639701318
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.15063922226178547
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.1508259669567148
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.15063257312232797
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.15068525875933836
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.15111278633518918
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.1509435825170048
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.15093516558408737
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.1509884332735387
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.15101361444850606
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.15112132142530754
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.15113174401281415
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.15117474433321218
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.15101840790673976
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.15091645768420262
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.15138212445759236
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.1514196568794215
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.15111952081874566
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.15114006674026742
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.15121360884530702
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.15107176701227823
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.15139208423147957
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.15159395954438618
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.1516480304247944
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.15161585891750498
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.15170893635783161
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.1519641253269381
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.15184674725450317
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.15186193738489934
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.1519652442867253
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.15195567634057355
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.1521641468041695
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.15219666918118796
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.1521428469198429
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.15209332814342097
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.1521830339642132
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.15204626679807515
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.1520921761951139
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.1519498256727671
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.15188786966405857
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.15184263960470126
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.1519405500693891
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.15205458337441086
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.15201129778201536
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.1519242508543862
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.1519221877393547
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.1518628181117337
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.15198932551976407
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.15198345527232412
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.15181426315785881
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.15178677674737714
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.15151585157982697
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.15161804622587036
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.15153669243493276
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.1515907633529846
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.15142194155840516
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.1514044047344005
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.1515125398550715
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.15142355093055151
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.15136733773232852
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.15125261527601253
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.151457027807915
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.15131423444383674
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.15128145613887692
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.1511914063204121
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.15106854530794372
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.15098454070318004
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.15093498685069986
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.15096399200058752
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.15091775441392857
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.15074610432728808
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.15073416094300607
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.15083162988487042
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.15080284768062113
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.1507449692580849
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.1508096045008595
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.1505956392850458
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.15060206334560344
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.15064308073903834
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.1507834723682573
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.15097333818222536
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.15097518508608018
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.15105996925383805
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.15102027397398926
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.15103452870308762
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.15095023616340947
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.1510146774132462
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.15090337413840177
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.15079752352197193
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.15073011739962344
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.15061620769735712
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.15066634371662824
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.15081104213992755
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.1508367383296456
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.15098745119318646
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.15105542704952715
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.1508777225825274
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.15097028107144111
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.15082907297269063
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.15090545002491243
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.15093818477807788
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.15102025294957094
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.15104427737268536
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.15099089616294362
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.150973157235631
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.1508721677845369
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.15100257411333068
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.1508992838859558
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.15090333551695917
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.1509047994792199
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.15070959788404012
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.15059358158095956
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.15074314821673476
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.1508802164233092
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.1508572113475409
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.1508074217663814
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.15079188999584597
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.15081400335469144
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.15069941582821184
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.1507073974056083
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.15070250430026977
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.1507082703722072
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.15070298667997123
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.15080391940239554
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.15073025454420688
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.15069062321029084
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.15068334237229628
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.15071691724718833
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.15075689990346025
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.15062211936543346
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.15074936943429132
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.15079602000703773
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.15082427436113358
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.15099322344677382
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.1509197469623316
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.15089434438307767
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.15089495427261188
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.1509352272047716
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.15088955691317096
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.1508248370097305
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.15068862269553104
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.15073243197787223
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.15070721770708378
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.15060017414933421
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.15052563111290676
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.15054830279867004
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.15046049123912147
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.15048346075246918
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.15046283961238718
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.15044723791576056
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.15047290999053128
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.15038728448094932
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.15042055674173213
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.1503982698455508
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.15039636463146
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.15062395147569888
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.15050949362942773
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.15041433139280838
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.15061694604979045
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.1506248518340424
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.15057105501349882
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.15057922905063972
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.1506635481757777
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.15058675365940108
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.15053010821765198
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.15056188230800965
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.15057219722321336
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.15065223485754248
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.1506824460583967
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.15061629088498157
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.15062298889582357
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.15053620908705834
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.15061810885010093
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.15056741165950946
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.15052349732754983
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.1505920351467035
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.15071870949195357
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.15072079497878835
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.15076506032130202
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.15081208332219107
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.15071239084905427
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.1506021812458501
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.1505815165489912
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.1506883614136531
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.15065997545391518
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.15064984906723003
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.15063158807491786
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.1506465499762629
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.1506307568474143
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.15061382922656372
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.1505763702239696
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.15056645901554225
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.1504718548347873
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.15048482587674805
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.15044591528100845
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.15049399321262066
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.1504594235188642
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.15046506822109224
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.15059508161642884
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.15055290395717139
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.15052800873915353
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.1505234063045358
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.1505226147826761
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.1504827509416598
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.15050025316684143
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.15056742953811267
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.15049460488888952
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.150558739396242
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.15042959697590283
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.15032695760147288
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.1503811474298922
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.1503441856670162
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.15041377438288747
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.1504724073671142
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.1503821651799133
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.15026319848077074
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.15028525779614904
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.15041715736264613
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.15042864520191437
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.15035110939769433
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.15027570338732393
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.15016940418509958
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.15016320963554522
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.15006768985315502
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.1502518745150134
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.1502558822925515
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.1502454562632497
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.15024100418971933
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.15022905620967034
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.15020231973455
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.15017196765148091
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.15012247764364012
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.15005606661949838
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.15007424212334164
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.15012411889620125
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.1501783567979383
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.1502104511663402
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.15023094400973389
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.15017620944993548
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.15026196739336356
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.15036584788610816
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.15037244855112353
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.15027031424558826
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.1502264926514467
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.15024947570585415
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.150203949266393
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.15014251511912424
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.1501825049519539
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.1501321039605336
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.1501737717102594
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.1502460383243211
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.15023840521651555
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.15016050709260476
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.15016545214742985
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.15013588972950495
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.15009904658826362
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.15015761418776077
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.15017685830593108
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.15026598645651595
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.1502156148143409
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.15027017075391041
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.15018309517161513
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.15010977273708895
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.15009269755812768
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.15000760282209408
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.15010137190706735
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.15006216786180934
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.14999922529443518
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.15002188989844348
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.14999013369089564
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.1500164327240482
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.15007402631035194
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.15001400277400628
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.14996799304509711
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.1499871607793837
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.15011323506898855
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.15012064833326388
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.15012981250316282
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.1500903798097914
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.15001610026551734
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.1500756918784961
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.14999470133381082
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.14995653312653304
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.1499534634654956
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.14989558099514216
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.14981867162228815
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.1497902754362267
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.1497990948918425
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.14975213898079737
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.1497301810057216
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.149852912653895
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.14986600746881115
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.14991166122802874
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.1499045596723139
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.14993304307165656
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.1499274303419538
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.14998908618078138
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.1500238187341805
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.15004145493730903
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.1500227864769151
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.14997812146632866
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.15009982471392092
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.15004521264206797
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.15016924460681771
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.15023237343224305
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.1501571585910822
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.1501198957656633
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.15010908212731866
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.15006023790564896
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.1500486754386989
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.1500036131729868
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.14995267791000558
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.1499154852746531
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.14995469987876453
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.14990545790297566
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.1498513009579848
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.1498968557450354
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.14984464030498745
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.14994925590351635
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.1500075950673025
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.14997218921780586
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.14988046658813003
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.14984615809199484
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.1498436841125391
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.14982223182040103
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.14978946188704423
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.14978751934527815
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.14979959292693085
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.14982290941837656
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.14979529355796392
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.14988210742428368
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.1498747579338556
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.1498226509657171
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.14983601886324766
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.1498563181345178
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.1498055818755895
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.14978443192818616
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.1497575431570902
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.14980257461010887
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.14977421089843795
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.14977854921372696
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.1497989764562879
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.14987843368688356
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.1498490214961987
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.1498010040974462
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.14970395133124573
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.14972459942359348
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.1497285464438059
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.14970146033257373
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.149688812471304
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.14978047511261752
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.14983290497428065
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.14988000288288644
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.14987675393209843
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.14997672864188583
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.14997672864188583 valid 0.22327809035778046
LOSS train 0.14997672864188583 valid 0.19686996936798096
LOSS train 0.14997672864188583 valid 0.19809198876221976
LOSS train 0.14997672864188583 valid 0.18783624097704887
LOSS train 0.14997672864188583 valid 0.18214235007762908
LOSS train 0.14997672864188583 valid 0.1924809142947197
LOSS train 0.14997672864188583 valid 0.20120363788945334
LOSS train 0.14997672864188583 valid 0.20100006647408009
LOSS train 0.14997672864188583 valid 0.2009086940023634
LOSS train 0.14997672864188583 valid 0.2034038007259369
LOSS train 0.14997672864188583 valid 0.20102861659093338
LOSS train 0.14997672864188583 valid 0.20160817230741182
LOSS train 0.14997672864188583 valid 0.20130262466577384
LOSS train 0.14997672864188583 valid 0.2002347164920398
LOSS train 0.14997672864188583 valid 0.19828221400578816
LOSS train 0.14997672864188583 valid 0.19964948948472738
LOSS train 0.14997672864188583 valid 0.200766425798921
LOSS train 0.14997672864188583 valid 0.1999925962752766
LOSS train 0.14997672864188583 valid 0.20297297600068545
LOSS train 0.14997672864188583 valid 0.20302364677190782
LOSS train 0.14997672864188583 valid 0.201004296541214
LOSS train 0.14997672864188583 valid 0.19986620071259412
LOSS train 0.14997672864188583 valid 0.19922503958577695
LOSS train 0.14997672864188583 valid 0.1992123027642568
LOSS train 0.14997672864188583 valid 0.19805416345596313
LOSS train 0.14997672864188583 valid 0.19871496867675048
LOSS train 0.14997672864188583 valid 0.19861121475696564
LOSS train 0.14997672864188583 valid 0.19841519902859414
LOSS train 0.14997672864188583 valid 0.1990094755230279
LOSS train 0.14997672864188583 valid 0.20023447225491206
LOSS train 0.14997672864188583 valid 0.20102964005162638
LOSS train 0.14997672864188583 valid 0.2005855585448444
LOSS train 0.14997672864188583 valid 0.20088091989358267
LOSS train 0.14997672864188583 valid 0.20102645938887315
LOSS train 0.14997672864188583 valid 0.20293402203491756
LOSS train 0.14997672864188583 valid 0.2027436507244905
LOSS train 0.14997672864188583 valid 0.203094575856183
LOSS train 0.14997672864188583 valid 0.2040758893678063
LOSS train 0.14997672864188583 valid 0.20279555290173262
LOSS train 0.14997672864188583 valid 0.20272632427513598
LOSS train 0.14997672864188583 valid 0.2031804119668356
LOSS train 0.14997672864188583 valid 0.20363091783864157
LOSS train 0.14997672864188583 valid 0.20374986806581186
LOSS train 0.14997672864188583 valid 0.20467112213373184
LOSS train 0.14997672864188583 valid 0.20446268518765767
LOSS train 0.14997672864188583 valid 0.20517952804980072
LOSS train 0.14997672864188583 valid 0.20551484315953356
LOSS train 0.14997672864188583 valid 0.20524348070224127
LOSS train 0.14997672864188583 valid 0.20592231227427113
LOSS train 0.14997672864188583 valid 0.2052636158466339
LOSS train 0.14997672864188583 valid 0.20564354956150055
LOSS train 0.14997672864188583 valid 0.2054682818169777
LOSS train 0.14997672864188583 valid 0.20594189020822634
LOSS train 0.14997672864188583 valid 0.20567199311874532
LOSS train 0.14997672864188583 valid 0.20565132119438864
LOSS train 0.14997672864188583 valid 0.20529311256749289
LOSS train 0.14997672864188583 valid 0.20542500521007337
LOSS train 0.14997672864188583 valid 0.2051864785367045
LOSS train 0.14997672864188583 valid 0.2056642780869694
LOSS train 0.14997672864188583 valid 0.20555204004049302
LOSS train 0.14997672864188583 valid 0.20573312874700203
LOSS train 0.14997672864188583 valid 0.20644315163935384
LOSS train 0.14997672864188583 valid 0.2062137025216269
LOSS train 0.14997672864188583 valid 0.20689851581119
LOSS train 0.14997672864188583 valid 0.20736845708810367
LOSS train 0.14997672864188583 valid 0.20731752501292663
LOSS train 0.14997672864188583 valid 0.2069714147208342
LOSS train 0.14997672864188583 valid 0.20688793326125426
LOSS train 0.14997672864188583 valid 0.20617473881313766
LOSS train 0.14997672864188583 valid 0.20661729233605522
LOSS train 0.14997672864188583 valid 0.20638209014711245
LOSS train 0.14997672864188583 valid 0.20674778728021515
LOSS train 0.14997672864188583 valid 0.2070348466912361
LOSS train 0.14997672864188583 valid 0.20680052948159142
LOSS train 0.14997672864188583 valid 0.20668308397134144
LOSS train 0.14997672864188583 valid 0.2071894919009585
LOSS train 0.14997672864188583 valid 0.20708051498060104
LOSS train 0.14997672864188583 valid 0.20700695212834921
LOSS train 0.14997672864188583 valid 0.20663392770139477
LOSS train 0.14997672864188583 valid 0.20596110187470912
LOSS train 0.14997672864188583 valid 0.20543235466804033
LOSS train 0.14997672864188583 valid 0.20557696208721254
LOSS train 0.14997672864188583 valid 0.20523611716477266
LOSS train 0.14997672864188583 valid 0.2050028627827054
LOSS train 0.14997672864188583 valid 0.20445934043211095
LOSS train 0.14997672864188583 valid 0.203967890767164
LOSS train 0.14997672864188583 valid 0.20396924035987635
LOSS train 0.14997672864188583 valid 0.20369661988859827
LOSS train 0.14997672864188583 valid 0.20382738933804329
LOSS train 0.14997672864188583 valid 0.2040695200363795
LOSS train 0.14997672864188583 valid 0.20446227736525482
LOSS train 0.14997672864188583 valid 0.20440874475499857
LOSS train 0.14997672864188583 valid 0.20421509813236935
LOSS train 0.14997672864188583 valid 0.2044902540584828
LOSS train 0.14997672864188583 valid 0.20393522021017577
LOSS train 0.14997672864188583 valid 0.20378082726771632
LOSS train 0.14997672864188583 valid 0.20388979349554198
LOSS train 0.14997672864188583 valid 0.2040213027170726
LOSS train 0.14997672864188583 valid 0.2044164387866704
LOSS train 0.14997672864188583 valid 0.2047436222434044
LOSS train 0.14997672864188583 valid 0.2048950413666149
LOSS train 0.14997672864188583 valid 0.20522324361053168
LOSS train 0.14997672864188583 valid 0.20515326996451444
LOSS train 0.14997672864188583 valid 0.20520473171312076
LOSS train 0.14997672864188583 valid 0.20521980155081976
LOSS train 0.14997672864188583 valid 0.20574783463523072
LOSS train 0.14997672864188583 valid 0.2056310278511493
LOSS train 0.14997672864188583 valid 0.2059831532339255
LOSS train 0.14997672864188583 valid 0.20636554226415968
LOSS train 0.14997672864188583 valid 0.2065248813141476
LOSS train 0.14997672864188583 valid 0.20648587300433768
LOSS train 0.14997672864188583 valid 0.2061017890061651
LOSS train 0.14997672864188583 valid 0.20626196331154983
LOSS train 0.14997672864188583 valid 0.20643474958967745
LOSS train 0.14997672864188583 valid 0.20644136848657027
LOSS train 0.14997672864188583 valid 0.20692340079052696
LOSS train 0.14997672864188583 valid 0.20707154757956153
LOSS train 0.14997672864188583 valid 0.20675580708657282
LOSS train 0.14997672864188583 valid 0.2065067444027973
LOSS train 0.14997672864188583 valid 0.20629128155608972
LOSS train 0.14997672864188583 valid 0.20598855503827088
LOSS train 0.14997672864188583 valid 0.20585670884026855
LOSS train 0.14997672864188583 valid 0.20607510269657384
LOSS train 0.14997672864188583 valid 0.20626682944355473
LOSS train 0.14997672864188583 valid 0.2061830005645752
LOSS train 0.14997672864188583 valid 0.2064598961955025
LOSS train 0.14997672864188583 valid 0.2063140974739405
LOSS train 0.14997672864188583 valid 0.20631317875813693
LOSS train 0.14997672864188583 valid 0.20647116213343863
LOSS train 0.14997672864188583 valid 0.20645149533565227
LOSS train 0.14997672864188583 valid 0.20620770820679554
LOSS train 0.14997672864188583 valid 0.20591416108337315
LOSS train 0.14997672864188583 valid 0.20565723935912425
LOSS train 0.14997672864188583 valid 0.20568606057273808
LOSS train 0.14997672864188583 valid 0.2056513711258217
LOSS train 0.14997672864188583 valid 0.2057202197611332
LOSS train 0.14997672864188583 valid 0.2055644805196428
LOSS train 0.14997672864188583 valid 0.20552157751028088
LOSS train 0.14997672864188583 valid 0.20527590854133634
LOSS train 0.14997672864188583 valid 0.20541454798408917
LOSS train 0.14997672864188583 valid 0.20536177082264678
LOSS train 0.14997672864188583 valid 0.20538675113463065
LOSS train 0.14997672864188583 valid 0.205330045698406
LOSS train 0.14997672864188583 valid 0.20530481615828144
LOSS train 0.14997672864188583 valid 0.20510500720862684
LOSS train 0.14997672864188583 valid 0.20544129681505569
LOSS train 0.14997672864188583 valid 0.20519906363519683
LOSS train 0.14997672864188583 valid 0.20607285523736799
LOSS train 0.14997672864188583 valid 0.20608707082351582
LOSS train 0.14997672864188583 valid 0.20600234746932983
LOSS train 0.14997672864188583 valid 0.20616007986052937
LOSS train 0.14997672864188583 valid 0.20600705711465134
LOSS train 0.14997672864188583 valid 0.20594803268223807
LOSS train 0.14997672864188583 valid 0.20600480054106032
LOSS train 0.14997672864188583 valid 0.20596431243804195
LOSS train 0.14997672864188583 valid 0.20618853240441054
LOSS train 0.14997672864188583 valid 0.20632494957583725
LOSS train 0.14997672864188583 valid 0.20639982753539388
LOSS train 0.14997672864188583 valid 0.20640762037826035
LOSS train 0.14997672864188583 valid 0.2062423530034721
LOSS train 0.14997672864188583 valid 0.20613891848866245
LOSS train 0.14997672864188583 valid 0.20600555349647262
LOSS train 0.14997672864188583 valid 0.20577298954951984
LOSS train 0.14997672864188583 valid 0.20564394262505742
LOSS train 0.14997672864188583 valid 0.20550167885693638
LOSS train 0.14997672864188583 valid 0.2055637963026403
LOSS train 0.14997672864188583 valid 0.20584009373616316
LOSS train 0.14997672864188583 valid 0.20588517135807446
LOSS train 0.14997672864188583 valid 0.20606940358700837
LOSS train 0.14997672864188583 valid 0.20612014952827903
LOSS train 0.14997672864188583 valid 0.20606030009643375
LOSS train 0.14997672864188583 valid 0.20582614492538365
LOSS train 0.14997672864188583 valid 0.20593854113121254
LOSS train 0.14997672864188583 valid 0.2058795346268292
LOSS train 0.14997672864188583 valid 0.20575473615101406
LOSS train 0.14997672864188583 valid 0.2056775570593097
LOSS train 0.14997672864188583 valid 0.2057885537713261
LOSS train 0.14997672864188583 valid 0.20603581174705807
LOSS train 0.14997672864188583 valid 0.2059321261151543
LOSS train 0.14997672864188583 valid 0.2059205651283264
LOSS train 0.14997672864188583 valid 0.20608971197960785
LOSS train 0.14997672864188583 valid 0.2060582696736514
LOSS train 0.14997672864188583 valid 0.20600375101540258
LOSS train 0.14997672864188583 valid 0.20584089592423127
LOSS train 0.14997672864188583 valid 0.20578802743473568
LOSS train 0.14997672864188583 valid 0.2057730004351626
LOSS train 0.14997672864188583 valid 0.20556080907105126
LOSS train 0.14997672864188583 valid 0.20558153283088765
LOSS train 0.14997672864188583 valid 0.20558181326225322
LOSS train 0.14997672864188583 valid 0.20561508324585462
LOSS train 0.14997672864188583 valid 0.20557239199184
LOSS train 0.14997672864188583 valid 0.20546616171486676
LOSS train 0.14997672864188583 valid 0.20536995119381446
LOSS train 0.14997672864188583 valid 0.20521227646734297
LOSS train 0.14997672864188583 valid 0.20494748736039187
LOSS train 0.14997672864188583 valid 0.205007654139582
LOSS train 0.14997672864188583 valid 0.20515426494143335
LOSS train 0.14997672864188583 valid 0.2050259533253583
LOSS train 0.14997672864188583 valid 0.20509060932763257
LOSS train 0.14997672864188583 valid 0.20491414837539196
LOSS train 0.14997672864188583 valid 0.20476562548336105
LOSS train 0.14997672864188583 valid 0.20474969396496764
LOSS train 0.14997672864188583 valid 0.20480097962424085
LOSS train 0.14997672864188583 valid 0.20495691449910985
LOSS train 0.14997672864188583 valid 0.2047844738495059
LOSS train 0.14997672864188583 valid 0.20490197461206935
LOSS train 0.14997672864188583 valid 0.2048021080557275
LOSS train 0.14997672864188583 valid 0.20480520672236496
LOSS train 0.14997672864188583 valid 0.20464114866188268
LOSS train 0.14997672864188583 valid 0.20459583401679993
LOSS train 0.14997672864188583 valid 0.20449531219581857
LOSS train 0.14997672864188583 valid 0.2042829525077118
LOSS train 0.14997672864188583 valid 0.20428766988812477
LOSS train 0.14997672864188583 valid 0.2042212296869153
LOSS train 0.14997672864188583 valid 0.20413398375344832
LOSS train 0.14997672864188583 valid 0.20393233994642893
LOSS train 0.14997672864188583 valid 0.20383228643149276
LOSS train 0.14997672864188583 valid 0.2037430528653871
LOSS train 0.14997672864188583 valid 0.20387710331509648
LOSS train 0.14997672864188583 valid 0.20384875258261506
LOSS train 0.14997672864188583 valid 0.2038092754814959
LOSS train 0.14997672864188583 valid 0.20382960932748811
LOSS train 0.14997672864188583 valid 0.20394386290969335
LOSS train 0.14997672864188583 valid 0.2040740443127496
LOSS train 0.14997672864188583 valid 0.20429187032911514
LOSS train 0.14997672864188583 valid 0.20440972846957434
LOSS train 0.14997672864188583 valid 0.2046329672509878
LOSS train 0.14997672864188583 valid 0.20471627879561038
LOSS train 0.14997672864188583 valid 0.20467738032861568
LOSS train 0.14997672864188583 valid 0.20477135369311208
LOSS train 0.14997672864188583 valid 0.2049423358141086
LOSS train 0.14997672864188583 valid 0.2049786562164282
LOSS train 0.14997672864188583 valid 0.20494677458965727
LOSS train 0.14997672864188583 valid 0.20508152221003148
LOSS train 0.14997672864188583 valid 0.20516500010135327
LOSS train 0.14997672864188583 valid 0.20506925571520448
LOSS train 0.14997672864188583 valid 0.2049816705380814
LOSS train 0.14997672864188583 valid 0.20496802958620697
LOSS train 0.14997672864188583 valid 0.2047623416494625
LOSS train 0.14997672864188583 valid 0.20472146018097798
LOSS train 0.14997672864188583 valid 0.20485737653936092
LOSS train 0.14997672864188583 valid 0.20467973666742814
LOSS train 0.14997672864188583 valid 0.20486706433963384
LOSS train 0.14997672864188583 valid 0.20506107367453028
LOSS train 0.14997672864188583 valid 0.20502658863456882
LOSS train 0.14997672864188583 valid 0.20493809236743585
LOSS train 0.14997672864188583 valid 0.20505744659224986
LOSS train 0.14997672864188583 valid 0.20504465645119066
LOSS train 0.14997672864188583 valid 0.20499311830384664
LOSS train 0.14997672864188583 valid 0.2049826610684395
LOSS train 0.14997672864188583 valid 0.2048392023342064
LOSS train 0.14997672864188583 valid 0.2049510180358849
LOSS train 0.14997672864188583 valid 0.20484765339274652
LOSS train 0.14997672864188583 valid 0.2047879573867077
LOSS train 0.14997672864188583 valid 0.20483702806865467
LOSS train 0.14997672864188583 valid 0.20486678590532392
LOSS train 0.14997672864188583 valid 0.2046968008061791
LOSS train 0.14997672864188583 valid 0.20485948765462683
LOSS train 0.14997672864188583 valid 0.2048454007932118
LOSS train 0.14997672864188583 valid 0.20482798270308056
LOSS train 0.14997672864188583 valid 0.2049623727227536
LOSS train 0.14997672864188583 valid 0.2050239514872318
LOSS train 0.14997672864188583 valid 0.20495919696278445
LOSS train 0.14997672864188583 valid 0.20500172076351714
LOSS train 0.14997672864188583 valid 0.20498758993058835
LOSS train 0.14997672864188583 valid 0.20493705144950322
LOSS train 0.14997672864188583 valid 0.20501994383469058
LOSS train 0.14997672864188583 valid 0.20511688614514337
LOSS train 0.14997672864188583 valid 0.20524854038506193
LOSS train 0.14997672864188583 valid 0.20522098121819674
LOSS train 0.14997672864188583 valid 0.205319847512949
LOSS train 0.14997672864188583 valid 0.20561341015512452
LOSS train 0.14997672864188583 valid 0.20574957607211647
LOSS train 0.14997672864188583 valid 0.20582331798590012
LOSS train 0.14997672864188583 valid 0.20575709359212355
LOSS train 0.14997672864188583 valid 0.20574752662492835
LOSS train 0.14997672864188583 valid 0.2056415345156666
LOSS train 0.14997672864188583 valid 0.20549516889045563
LOSS train 0.14997672864188583 valid 0.20546995450518893
LOSS train 0.14997672864188583 valid 0.20542423544185504
LOSS train 0.14997672864188583 valid 0.20535080293955751
LOSS train 0.14997672864188583 valid 0.20508465635861065
LOSS train 0.14997672864188583 valid 0.20515083682200091
LOSS train 0.14997672864188583 valid 0.2052419592272228
LOSS train 0.14997672864188583 valid 0.20524268516323022
LOSS train 0.14997672864188583 valid 0.20519709342217946
LOSS train 0.14997672864188583 valid 0.20511966170126553
LOSS train 0.14997672864188583 valid 0.20510668478285274
LOSS train 0.14997672864188583 valid 0.2051131204028443
LOSS train 0.14997672864188583 valid 0.20506980650383852
LOSS train 0.14997672864188583 valid 0.2048981057102328
LOSS train 0.14997672864188583 valid 0.20488026467700526
LOSS train 0.14997672864188583 valid 0.2048801852981385
LOSS train 0.14997672864188583 valid 0.20490559651738122
LOSS train 0.14997672864188583 valid 0.20502450819742882
LOSS train 0.14997672864188583 valid 0.20495087789321267
LOSS train 0.14997672864188583 valid 0.20499182755898948
LOSS train 0.14997672864188583 valid 0.20499647818755784
LOSS train 0.14997672864188583 valid 0.20503551226395828
LOSS train 0.14997672864188583 valid 0.20509463518857957
LOSS train 0.14997672864188583 valid 0.2050800542795777
LOSS train 0.14997672864188583 valid 0.2050280034147351
LOSS train 0.14997672864188583 valid 0.2051517921115699
LOSS train 0.14997672864188583 valid 0.205166168167795
LOSS train 0.14997672864188583 valid 0.20512069986491907
LOSS train 0.14997672864188583 valid 0.20515242817939497
LOSS train 0.14997672864188583 valid 0.20513224261979715
LOSS train 0.14997672864188583 valid 0.2050360327513961
LOSS train 0.14997672864188583 valid 0.20516120480873823
LOSS train 0.14997672864188583 valid 0.2051208806614722
LOSS train 0.14997672864188583 valid 0.20503456109589702
LOSS train 0.14997672864188583 valid 0.2050340999968541
LOSS train 0.14997672864188583 valid 0.20512140106659726
LOSS train 0.14997672864188583 valid 0.20521354120058619
LOSS train 0.14997672864188583 valid 0.2051464557647705
LOSS train 0.14997672864188583 valid 0.20510679667320433
LOSS train 0.14997672864188583 valid 0.20515762786571912
LOSS train 0.14997672864188583 valid 0.20518622266234093
LOSS train 0.14997672864188583 valid 0.2052410343504251
LOSS train 0.14997672864188583 valid 0.20511954450048506
LOSS train 0.14997672864188583 valid 0.2052772442964007
LOSS train 0.14997672864188583 valid 0.20533554815912838
LOSS train 0.14997672864188583 valid 0.20529768738215184
LOSS train 0.14997672864188583 valid 0.20538842995409612
LOSS train 0.14997672864188583 valid 0.20537872621646294
LOSS train 0.14997672864188583 valid 0.20558188174582698
LOSS train 0.14997672864188583 valid 0.20572796878647
LOSS train 0.14997672864188583 valid 0.2056333627551794
LOSS train 0.14997672864188583 valid 0.20577853750494113
LOSS train 0.14997672864188583 valid 0.2057554401683085
LOSS train 0.14997672864188583 valid 0.20564771621428948
LOSS train 0.14997672864188583 valid 0.20556310227656938
LOSS train 0.14997672864188583 valid 0.20558945337931314
LOSS train 0.14997672864188583 valid 0.20572724635015704
LOSS train 0.14997672864188583 valid 0.2057475078461775
LOSS train 0.14997672864188583 valid 0.2058616615831852
LOSS train 0.14997672864188583 valid 0.20587776128191623
LOSS train 0.14997672864188583 valid 0.20582545244482142
LOSS train 0.14997672864188583 valid 0.20589697932423393
LOSS train 0.14997672864188583 valid 0.20591866255683058
LOSS train 0.14997672864188583 valid 0.2057695225798839
LOSS train 0.14997672864188583 valid 0.2057648881049881
LOSS train 0.14997672864188583 valid 0.2058333710661435
LOSS train 0.14997672864188583 valid 0.20604380319804647
LOSS train 0.14997672864188583 valid 0.20610377697840981
LOSS train 0.14997672864188583 valid 0.20609186159048468
LOSS train 0.14997672864188583 valid 0.20594785201446467
LOSS train 0.14997672864188583 valid 0.20590722141252166
LOSS train 0.14997672864188583 valid 0.205966145312547
LOSS train 0.14997672864188583 valid 0.20589025752885
LOSS train 0.14997672864188583 valid 0.20577385881517687
LOSS train 0.14997672864188583 valid 0.20574737534942952
LOSS train 0.14997672864188583 valid 0.20577430598458893
LOSS train 0.14997672864188583 valid 0.20588957088983664
LOSS train 0.14997672864188583 valid 0.20601055055436954
LOSS train 0.14997672864188583 valid 0.2060807521842169
LOSS train 0.14997672864188583 valid 0.20613007568845562
LOSS train 0.14997672864188583 valid 0.20599537759020342
LOSS train 0.14997672864188583 valid 0.20602929965699285
LOSS train 0.14997672864188583 valid 0.2059951858801974
LOSS train 0.14997672864188583 valid 0.20592286452196973
LOSS train 0.14997672864188583 valid 0.20603249090317205
LOSS train 0.14997672864188583 valid 0.2059813554375625
LOSS train 0.14997672864188583 valid 0.20598549964827495
LOSS train 0.14997672864188583 valid 0.20599229543992917
LOSS train 0.14997672864188583 valid 0.20598127937219182
LOSS train 0.14997672864188583 valid 0.20589632270933822
LOSS train 0.14997672864188583 valid 0.2058896735391539
LOSS train 0.14997672864188583 valid 0.20597034781605894
EPOCH 12:
  batch 1 loss: 0.1370209902524948
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.1409384161233902
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.14844798545042673
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.15181563049554825
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.15863154828548431
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.16108488291502
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.15169288111584528
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.15135862585157156
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.14806250648366082
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.14624348059296607
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.1433250599286773
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.14201633942623934
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.1423105030105664
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.14161907402532442
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.14157931953668595
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.14325808500871062
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.14354082722874248
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.1459975793129868
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.14483618030422613
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.14365418814122677
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.14399823936678113
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.14431722902438857
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.14460770753414734
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.14506622310727835
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.1452035489678383
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.1449275420835385
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.14558320106179626
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.14562596194446087
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.14432415623089362
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.14559606164693834
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.14578946175113802
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.14652619883418083
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.14588080527204456
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.14669578741578496
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.14803311569350106
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.1471235483056969
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.14712222282950943
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.14730263815114372
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.1473405258013652
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.14751268699765205
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.14720110522537697
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.1474970763637906
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.14826057400814321
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.14751466549932957
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.14823336253563563
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.14814886337389116
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.14745781982832767
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.14734458659465113
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.14744224247275567
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.14769355252385138
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.1477155032403329
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.14798881357105878
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.14796224700392419
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.14846235334321303
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.1485006721182303
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.14846013127160923
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.14844573236871184
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.14780039725632504
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.1482732770806652
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.14798393075664837
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.1482855289197359
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.14800267642544163
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.1479552960585034
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.148056426551193
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.14807829902722286
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.14802759501970175
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.14775104153512128
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.14776869315434904
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.1481613609669865
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.1483663952776364
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.14814854570677582
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.1481823470029566
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.14812368583189298
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.14823875213797028
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.14839532335599265
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.1485987915412376
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.1484136949111889
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.1481401494298226
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.14838214841070055
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.1485938122496009
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.14868556754088696
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.1484297572112665
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.14799866529114275
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.1482550421995776
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.14820999134989346
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.14824386582125065
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.1483531578861434
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.14860657399350946
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.14882594313514366
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.14908104009098477
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.14905298460315872
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.14898523137621258
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.1494991087144421
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.1494920142787568
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.14983208367699072
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.14985551328087846
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.14982499996411433
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.14980686516786107
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.1498793146827004
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.1498430772125721
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.14984505291622463
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.1498283917705218
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.14938649535179138
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.14944190446000832
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.1490108555271512
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.14907604315370884
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.14929168232690507
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.14905548171588667
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.14905096146218275
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.14940717443823814
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.14923941142655708
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.14890268790934766
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.14856587996525047
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.14881338976454317
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.14846362581719522
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.1483774365664556
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.14870837502754652
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.14864680189955032
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.1486086967487295
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.14882181429614624
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.14861356516268628
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.14877415918668763
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.14920765216030726
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.1491073768345579
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.1491012423634529
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.14913355021013153
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.14915962405796127
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.1491872034384869
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.14923964238675066
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.1492996318409076
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.14912477539468358
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.14900072297137795
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.14947460821472613
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.14948538173712902
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.14917292909489738
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.14915432085228317
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.14924343457839784
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.14915992065832234
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.14943177038602692
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.14967592917382716
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.14971583273182523
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.14967352902175676
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.14979920234088298
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.15007921955030826
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.14999287554929996
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.1500174820116938
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.15008070228659376
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.15005352574627143
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.15025605716361295
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.15030444905161858
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.15025766702085142
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.1501717700279857
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.15025362659708347
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.1500718750349887
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.15013236220805876
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.15001478724372694
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.14997233127712445
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.1499481170992308
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.15006957573335875
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.15022769551724197
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.15019106235563384
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.15013796229053428
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.15014810915007912
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.15007751843914752
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.15019198674144166
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.1501643814774881
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.15001582769219746
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.14997827785000914
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.14971561716682108
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.14979481087887989
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.14974216092922535
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.1497508942820998
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.14960039208906922
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.14956768460828682
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.14971004396677018
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.14961328150027178
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.14956665944076528
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.14943376589524612
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.14963995865103918
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.1495033748033974
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.14949341846928413
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.1493798163975333
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.14926868949920102
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.14917212558667298
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.14910604659769985
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.14911647080894438
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.14906189630535197
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.14893178784466804
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.14892144288335527
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.14897270924166628
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.14893192216675943
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.14886093876945475
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.14891657920390214
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.14870301906749145
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.14871651847393086
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.1487773740093927
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.14894400881177883
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.1491061198816757
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.14912111948632714
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.1492136625573039
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.1491755077717316
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.14917706362534278
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.1490924819716679
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.1491678207017043
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.1490274330828248
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.14892083200291523
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.14885873729048144
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.1487662075445629
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.1488136981021274
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.14893859961912745
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.1489297973248066
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.14906656211417801
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.1491186678619452
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.14893419003096697
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.14900394137515577
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.14886193612107523
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.14892731359752093
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.14896369397366813
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.14905715430980404
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.14909271259199489
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.14902783222327945
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.14902761544998702
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.1489393720445077
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.14904874004423618
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.14894245551692115
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.14895585621089008
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.14898557035408355
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.14880386599453918
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.14869429031183626
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.14886617618410483
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.14897051443914314
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.1489336004737636
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.14888870412315933
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.1488800766185308
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.14887077145754024
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.1487905191478588
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.14879663147126573
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.14878248407685457
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.14877308185005786
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.14877816721176107
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.1488805970030207
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.14882147493810693
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.14879981005265389
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.14878730741558505
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.14883171122293082
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.14888596465069104
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.14876253574121337
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.14886131020443094
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.14891753082414227
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.14896540227532387
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.14911937698781252
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.14906180859912957
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.1490600701318428
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.14906778770286266
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.14911426706056968
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.14911352275521494
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.14904618782176118
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.14892996374026748
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.1489582012288819
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.14894111958833842
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.14884360051817364
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.14876159772504377
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.14879448097128378
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.14868260680161643
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.1486937974140329
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.14867286683809489
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.14867161551999689
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.1487359718433512
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.14865314387145095
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.1486786023610168
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.14865924827617033
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.1486337042742354
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.14886755654554226
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.1487443328828272
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.14864113574678248
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.14886070512559102
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.14883703316161778
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.14878071592651682
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.14881251068739054
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.1489114048225539
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.14883797125247875
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.14879387807338795
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.1488143982293328
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.1488226458535228
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.14890666661555307
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.14897404736155398
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.14892308813769642
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.14893421784250271
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.1488343429297312
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.14891493084101842
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.14889571244773997
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.14885189190302808
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.14895072726869746
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.14907691602398748
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.14907283308142322
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.14914417624272205
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.1491852892870052
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.1490823651020159
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.14897232333254257
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.1489460909118255
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.14907162799886692
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.14906162514493165
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.14904576532616473
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.1490405881532321
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.14910078696051582
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.14910046758702378
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.1490896321129333
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.14907084432031428
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.14905261663075017
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.14897054195884735
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.14897518432504495
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.1489388972090987
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.14896931989600484
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.14893066035998856
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.1489286393636749
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.14903671136479588
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.1489974477736732
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.14897975285754264
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.14896900337598168
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.14898283223155886
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.14895566877824867
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.14892959708104964
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.14897985574169426
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.1488960175142612
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.14895669877529144
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.1488434392387516
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.14873249195402186
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.148777403028273
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.14875299902491293
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.14882601788549712
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.14889009846121162
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.14880067662122737
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.14868888782488332
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.14872364797991908
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.14883789331165712
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.14885645945157325
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.14876863636761814
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.14869687648919913
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.14856152359539077
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.14855786272708107
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.1484746984766026
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.14864844608812305
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.14865917899257589
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.14866271556549987
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.14864778762710268
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.14864295107350184
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.1486281356868208
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.14861393623567862
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.1485604199989136
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.14847393121038163
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.14849520816422596
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.14855728010562333
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.14861028115911457
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.14865212395029553
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.14866592934433842
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.1486151081541281
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.1487098593337863
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.1488198929742062
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.1488193978755255
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.1487502186662621
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.148696050460649
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.148716997910929
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.148682122511312
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.14862653920119936
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.14867695198483663
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.14863549559657038
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.148684836132325
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.14876328793394825
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.14874208979005735
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.1486543618948073
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.1486552902108254
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.14862135771702054
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.14859583180887131
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.14865596522701616
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.1486743704676628
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.14876954929229427
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.14872134196268152
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.14877668052675233
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.14867538675705172
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.14859260926513296
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.1485610052239238
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.14849391037412962
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.14859815120852649
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.14855928695760667
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.1484853131817533
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.14848742027974499
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.14845925958427655
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.14848828945577758
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.14853511743036824
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.14847990087209603
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.14845833060381664
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.14846957155636378
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.14862126824813338
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.14864563230935693
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.14866006306455104
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.1486158562504282
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.1485409561722945
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.14860764479068056
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.14852549766836906
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.14849064871668816
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.14850323905522686
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.14844476023864983
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.14836530877328097
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.14834157036303883
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.14836294741174322
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.14831696004776532
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.14829986315467727
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.14844382300461625
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.1484557033714572
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.1485032142117256
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.14848903240296094
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.14852513985943447
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.14851806348879748
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.1486073131110645
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.14864627954112478
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.1486716556470268
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.148638033591729
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.14860716053957573
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.14873654921683604
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.14869445659929797
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.14881060017613504
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.14887208655759057
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.14879249723245067
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.1487710896949723
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.14877514432458316
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.1487310585071783
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.14872082575841586
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.14868590744020782
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.14863970062949441
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.14858729960613473
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.14860926478617154
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.1485656368787642
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.14851769925402714
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.14855935994518518
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.1484958482028424
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.14860194694216644
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.14865298359495005
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.14862871372468395
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.14855988960127625
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.14852493254637175
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.14853153580508263
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.14849509047639317
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.14848250878920136
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.14846434583467943
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.14847799333628645
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.14850856799782658
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.14848313117253967
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.14857206734762127
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.14855895198068533
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.14849149010247653
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.14850152191394184
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.1485282669453758
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.14848751515544803
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.14846048998084363
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.14842560240528085
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.14847926708885975
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.14844602194800158
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.14844961902004664
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.14846721754159803
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.14853848207580006
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.14851386759454413
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.14847555942027083
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.14838424051425883
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.148399258417816
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.14841844923393702
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.14839295374605277
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.14838238653968183
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.14846431623157272
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.1485214368748004
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.14857651546914527
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.14856321720560645
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.14866871907675672
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.14866871907675672 valid 0.22157591581344604
LOSS train 0.14866871907675672 valid 0.1948605701327324
LOSS train 0.14866871907675672 valid 0.195478489001592
LOSS train 0.14866871907675672 valid 0.18545221537351608
LOSS train 0.14866871907675672 valid 0.1796529173851013
LOSS train 0.14866871907675672 valid 0.18998881181081137
LOSS train 0.14866871907675672 valid 0.19830010831356049
LOSS train 0.14866871907675672 valid 0.19825494661927223
LOSS train 0.14866871907675672 valid 0.19807894031206766
LOSS train 0.14866871907675672 valid 0.2001856669783592
LOSS train 0.14866871907675672 valid 0.19777110760862177
LOSS train 0.14866871907675672 valid 0.1982998363673687
LOSS train 0.14866871907675672 valid 0.19781080461465395
LOSS train 0.14866871907675672 valid 0.19677441780056273
LOSS train 0.14866871907675672 valid 0.19494509200255075
LOSS train 0.14866871907675672 valid 0.19625601079314947
LOSS train 0.14866871907675672 valid 0.19755148624672608
LOSS train 0.14866871907675672 valid 0.19661173969507217
LOSS train 0.14866871907675672 valid 0.1996517926454544
LOSS train 0.14866871907675672 valid 0.19968569576740264
LOSS train 0.14866871907675672 valid 0.19767169512453533
LOSS train 0.14866871907675672 valid 0.19661148366602985
LOSS train 0.14866871907675672 valid 0.19594856578370798
LOSS train 0.14866871907675672 valid 0.1959271306792895
LOSS train 0.14866871907675672 valid 0.19475782215595244
LOSS train 0.14866871907675672 valid 0.1953910359969506
LOSS train 0.14866871907675672 valid 0.19531565131964507
LOSS train 0.14866871907675672 valid 0.19507955546889985
LOSS train 0.14866871907675672 valid 0.1956457906755908
LOSS train 0.14866871907675672 valid 0.19682660450538
LOSS train 0.14866871907675672 valid 0.19765676750290778
LOSS train 0.14866871907675672 valid 0.1971989613957703
LOSS train 0.14866871907675672 valid 0.1974594308571382
LOSS train 0.14866871907675672 valid 0.1976187641129774
LOSS train 0.14866871907675672 valid 0.19955374002456666
LOSS train 0.14866871907675672 valid 0.19929216305414835
LOSS train 0.14866871907675672 valid 0.19968037991910367
LOSS train 0.14866871907675672 valid 0.20061167055054716
LOSS train 0.14866871907675672 valid 0.19935765786048693
LOSS train 0.14866871907675672 valid 0.19929186590015888
LOSS train 0.14866871907675672 valid 0.1996965128474119
LOSS train 0.14866871907675672 valid 0.2001157881958144
LOSS train 0.14866871907675672 valid 0.20024865969668987
LOSS train 0.14866871907675672 valid 0.20115879584442486
LOSS train 0.14866871907675672 valid 0.20096558067533704
LOSS train 0.14866871907675672 valid 0.201674306522245
LOSS train 0.14866871907675672 valid 0.20203927729992158
LOSS train 0.14866871907675672 valid 0.2017715135589242
LOSS train 0.14866871907675672 valid 0.2024291723358388
LOSS train 0.14866871907675672 valid 0.20180041283369066
LOSS train 0.14866871907675672 valid 0.20218396362136393
LOSS train 0.14866871907675672 valid 0.2020019106566906
LOSS train 0.14866871907675672 valid 0.20247219418579676
LOSS train 0.14866871907675672 valid 0.20218958622879452
LOSS train 0.14866871907675672 valid 0.20217247632416813
LOSS train 0.14866871907675672 valid 0.20185706072619983
LOSS train 0.14866871907675672 valid 0.20201917044949114
LOSS train 0.14866871907675672 valid 0.20178534244668894
LOSS train 0.14866871907675672 valid 0.2022804062245256
LOSS train 0.14866871907675672 valid 0.20213811720410982
LOSS train 0.14866871907675672 valid 0.20230264106734855
LOSS train 0.14866871907675672 valid 0.20297366044213694
LOSS train 0.14866871907675672 valid 0.20274323035800268
LOSS train 0.14866871907675672 valid 0.20341044338420033
LOSS train 0.14866871907675672 valid 0.20389483823226048
LOSS train 0.14866871907675672 valid 0.20384875549511475
LOSS train 0.14866871907675672 valid 0.20355590799851203
LOSS train 0.14866871907675672 valid 0.2034610514255131
LOSS train 0.14866871907675672 valid 0.20275492512661478
LOSS train 0.14866871907675672 valid 0.20320411239351546
LOSS train 0.14866871907675672 valid 0.2029650391827167
LOSS train 0.14866871907675672 valid 0.20334667981498772
LOSS train 0.14866871907675672 valid 0.2036128044128418
LOSS train 0.14866871907675672 valid 0.20337417560654716
LOSS train 0.14866871907675672 valid 0.20328311920166015
LOSS train 0.14866871907675672 valid 0.20376639597510038
LOSS train 0.14866871907675672 valid 0.2036408692985386
LOSS train 0.14866871907675672 valid 0.20357629542167371
LOSS train 0.14866871907675672 valid 0.2032048887089838
LOSS train 0.14866871907675672 valid 0.202566260099411
LOSS train 0.14866871907675672 valid 0.20202447805139753
LOSS train 0.14866871907675672 valid 0.20219368887383762
LOSS train 0.14866871907675672 valid 0.20184197483292546
LOSS train 0.14866871907675672 valid 0.20159234408111798
LOSS train 0.14866871907675672 valid 0.20105968889068154
LOSS train 0.14866871907675672 valid 0.20056702213924985
LOSS train 0.14866871907675672 valid 0.200557644682369
LOSS train 0.14866871907675672 valid 0.20029782165180554
LOSS train 0.14866871907675672 valid 0.20044099715318572
LOSS train 0.14866871907675672 valid 0.20068046649297078
LOSS train 0.14866871907675672 valid 0.20108671224379276
LOSS train 0.14866871907675672 valid 0.2010347264940324
LOSS train 0.14866871907675672 valid 0.20085262579302635
LOSS train 0.14866871907675672 valid 0.2011249414149751
LOSS train 0.14866871907675672 valid 0.2005656650191859
LOSS train 0.14866871907675672 valid 0.20043421598772207
LOSS train 0.14866871907675672 valid 0.2005268068043227
LOSS train 0.14866871907675672 valid 0.2006608112430086
LOSS train 0.14866871907675672 valid 0.2010411930204642
LOSS train 0.14866871907675672 valid 0.2013494937121868
LOSS train 0.14866871907675672 valid 0.20152294355453831
LOSS train 0.14866871907675672 valid 0.20184358954429626
LOSS train 0.14866871907675672 valid 0.2017716073874131
LOSS train 0.14866871907675672 valid 0.2018411659563963
LOSS train 0.14866871907675672 valid 0.2018618494272232
LOSS train 0.14866871907675672 valid 0.2023993529519945
LOSS train 0.14866871907675672 valid 0.20228101062440426
LOSS train 0.14866871907675672 valid 0.20261645689606667
LOSS train 0.14866871907675672 valid 0.20301014964186817
LOSS train 0.14866871907675672 valid 0.20317218859087338
LOSS train 0.14866871907675672 valid 0.20311298926134366
LOSS train 0.14866871907675672 valid 0.20272191055119038
LOSS train 0.14866871907675672 valid 0.20285841427018156
LOSS train 0.14866871907675672 valid 0.20304073445629656
LOSS train 0.14866871907675672 valid 0.2030402871577636
LOSS train 0.14866871907675672 valid 0.20353317479121275
LOSS train 0.14866871907675672 valid 0.20366627984067315
LOSS train 0.14866871907675672 valid 0.20332914931794344
LOSS train 0.14866871907675672 valid 0.20306239846874685
LOSS train 0.14866871907675672 valid 0.20283457251886527
LOSS train 0.14866871907675672 valid 0.20252350970240665
LOSS train 0.14866871907675672 valid 0.20238834082103166
LOSS train 0.14866871907675672 valid 0.20259823159473697
LOSS train 0.14866871907675672 valid 0.20279425995484476
LOSS train 0.14866871907675672 valid 0.20271899366378784
LOSS train 0.14866871907675672 valid 0.20299859072953935
LOSS train 0.14866871907675672 valid 0.2028591418125498
LOSS train 0.14866871907675672 valid 0.20285741507541388
LOSS train 0.14866871907675672 valid 0.20301410267057346
LOSS train 0.14866871907675672 valid 0.20298257286732013
LOSS train 0.14866871907675672 valid 0.2027376024550154
LOSS train 0.14866871907675672 valid 0.2024569267576391
LOSS train 0.14866871907675672 valid 0.20219090547328605
LOSS train 0.14866871907675672 valid 0.20223260203849025
LOSS train 0.14866871907675672 valid 0.20221124876428534
LOSS train 0.14866871907675672 valid 0.20226444369729826
LOSS train 0.14866871907675672 valid 0.20210076520477768
LOSS train 0.14866871907675672 valid 0.20205410677885663
LOSS train 0.14866871907675672 valid 0.20181918197827373
LOSS train 0.14866871907675672 valid 0.20195048398205212
LOSS train 0.14866871907675672 valid 0.20188638768720288
LOSS train 0.14866871907675672 valid 0.20189659633267093
LOSS train 0.14866871907675672 valid 0.20183279714384278
LOSS train 0.14866871907675672 valid 0.20182269201096562
LOSS train 0.14866871907675672 valid 0.2016335687760649
LOSS train 0.14866871907675672 valid 0.20195929491764877
LOSS train 0.14866871907675672 valid 0.20171543965939762
LOSS train 0.14866871907675672 valid 0.2025804943530946
LOSS train 0.14866871907675672 valid 0.20258822797128817
LOSS train 0.14866871907675672 valid 0.20250362674395242
LOSS train 0.14866871907675672 valid 0.20267414237489764
LOSS train 0.14866871907675672 valid 0.2025196145435697
LOSS train 0.14866871907675672 valid 0.20247189233116075
LOSS train 0.14866871907675672 valid 0.2025430603073789
LOSS train 0.14866871907675672 valid 0.20250702356138536
LOSS train 0.14866871907675672 valid 0.2027210821517003
LOSS train 0.14866871907675672 valid 0.20285474086643024
LOSS train 0.14866871907675672 valid 0.2029286035442654
LOSS train 0.14866871907675672 valid 0.20292904101452738
LOSS train 0.14866871907675672 valid 0.20275075295940043
LOSS train 0.14866871907675672 valid 0.20263734024874172
LOSS train 0.14866871907675672 valid 0.20250786914501662
LOSS train 0.14866871907675672 valid 0.20228901478044825
LOSS train 0.14866871907675672 valid 0.20214806070051541
LOSS train 0.14866871907675672 valid 0.2020169656385075
LOSS train 0.14866871907675672 valid 0.20209840370948057
LOSS train 0.14866871907675672 valid 0.2023831098201032
LOSS train 0.14866871907675672 valid 0.202430989948057
LOSS train 0.14866871907675672 valid 0.20261613574959117
LOSS train 0.14866871907675672 valid 0.20266815967419569
LOSS train 0.14866871907675672 valid 0.2026080346246909
LOSS train 0.14866871907675672 valid 0.20237718219327372
LOSS train 0.14866871907675672 valid 0.2024794885877929
LOSS train 0.14866871907675672 valid 0.20241877402382336
LOSS train 0.14866871907675672 valid 0.20228774564606802
LOSS train 0.14866871907675672 valid 0.20221354778517375
LOSS train 0.14866871907675672 valid 0.20232002731770446
LOSS train 0.14866871907675672 valid 0.20256802958718848
LOSS train 0.14866871907675672 valid 0.20245113169680762
LOSS train 0.14866871907675672 valid 0.20243690485755603
LOSS train 0.14866871907675672 valid 0.20259846582267824
LOSS train 0.14866871907675672 valid 0.20255351762522708
LOSS train 0.14866871907675672 valid 0.2025115161808462
LOSS train 0.14866871907675672 valid 0.20236256297515787
LOSS train 0.14866871907675672 valid 0.20230158430498998
LOSS train 0.14866871907675672 valid 0.20229753419276206
LOSS train 0.14866871907675672 valid 0.2020925028438874
LOSS train 0.14866871907675672 valid 0.2021086888902999
LOSS train 0.14866871907675672 valid 0.20208778764520371
LOSS train 0.14866871907675672 valid 0.20211654250559055
LOSS train 0.14866871907675672 valid 0.20207815738248575
LOSS train 0.14866871907675672 valid 0.20197059260681272
LOSS train 0.14866871907675672 valid 0.20187144900233017
LOSS train 0.14866871907675672 valid 0.20170791554696782
LOSS train 0.14866871907675672 valid 0.20144641063152216
LOSS train 0.14866871907675672 valid 0.20151107347741418
LOSS train 0.14866871907675672 valid 0.20165285937072056
LOSS train 0.14866871907675672 valid 0.20153529768941378
LOSS train 0.14866871907675672 valid 0.20160481131556046
LOSS train 0.14866871907675672 valid 0.20141815692186354
LOSS train 0.14866871907675672 valid 0.2012738108783219
LOSS train 0.14866871907675672 valid 0.20125904605530276
LOSS train 0.14866871907675672 valid 0.20131674218060347
LOSS train 0.14866871907675672 valid 0.20147437565759116
LOSS train 0.14866871907675672 valid 0.2012956704308347
LOSS train 0.14866871907675672 valid 0.2014107709226099
LOSS train 0.14866871907675672 valid 0.2013254038233688
LOSS train 0.14866871907675672 valid 0.20132814569828603
LOSS train 0.14866871907675672 valid 0.20116539969683833
LOSS train 0.14866871907675672 valid 0.20112251823856717
LOSS train 0.14866871907675672 valid 0.20102205095697917
LOSS train 0.14866871907675672 valid 0.2008152723593532
LOSS train 0.14866871907675672 valid 0.20082279710982326
LOSS train 0.14866871907675672 valid 0.20076421690042887
LOSS train 0.14866871907675672 valid 0.20067401916481728
LOSS train 0.14866871907675672 valid 0.20047164446225874
LOSS train 0.14866871907675672 valid 0.20037414827105088
LOSS train 0.14866871907675672 valid 0.20027992286539953
LOSS train 0.14866871907675672 valid 0.20040311626919874
LOSS train 0.14866871907675672 valid 0.20037980817935683
LOSS train 0.14866871907675672 valid 0.20034606498560753
LOSS train 0.14866871907675672 valid 0.2003686087088542
LOSS train 0.14866871907675672 valid 0.20048208026875294
LOSS train 0.14866871907675672 valid 0.20061499465789115
LOSS train 0.14866871907675672 valid 0.20084376295407613
LOSS train 0.14866871907675672 valid 0.20096111271233685
LOSS train 0.14866871907675672 valid 0.20118248055684934
LOSS train 0.14866871907675672 valid 0.20126012366330415
LOSS train 0.14866871907675672 valid 0.2012247392444111
LOSS train 0.14866871907675672 valid 0.20132680366868558
LOSS train 0.14866871907675672 valid 0.20149902541400033
LOSS train 0.14866871907675672 valid 0.20152713551089682
LOSS train 0.14866871907675672 valid 0.20149589058155665
LOSS train 0.14866871907675672 valid 0.20162503510458857
LOSS train 0.14866871907675672 valid 0.2016911266966069
LOSS train 0.14866871907675672 valid 0.20159522241960137
LOSS train 0.14866871907675672 valid 0.20151202059999296
LOSS train 0.14866871907675672 valid 0.201507193576388
LOSS train 0.14866871907675672 valid 0.20129918528401203
LOSS train 0.14866871907675672 valid 0.20125751631955305
LOSS train 0.14866871907675672 valid 0.20140319870962642
LOSS train 0.14866871907675672 valid 0.20123154799307674
LOSS train 0.14866871907675672 valid 0.2014181998408871
LOSS train 0.14866871907675672 valid 0.20160557860966588
LOSS train 0.14866871907675672 valid 0.20157041847705842
LOSS train 0.14866871907675672 valid 0.20147071990782653
LOSS train 0.14866871907675672 valid 0.20158732305412833
LOSS train 0.14866871907675672 valid 0.2015690224305276
LOSS train 0.14866871907675672 valid 0.20151788139917765
LOSS train 0.14866871907675672 valid 0.20150774139165878
LOSS train 0.14866871907675672 valid 0.20136837168518765
LOSS train 0.14866871907675672 valid 0.2014716976573543
LOSS train 0.14866871907675672 valid 0.20137625850236462
LOSS train 0.14866871907675672 valid 0.20131117005751828
LOSS train 0.14866871907675672 valid 0.20136294616203682
LOSS train 0.14866871907675672 valid 0.20139227784238756
LOSS train 0.14866871907675672 valid 0.20122152642285313
LOSS train 0.14866871907675672 valid 0.2013791195412939
LOSS train 0.14866871907675672 valid 0.2013665547011902
LOSS train 0.14866871907675672 valid 0.2013441406763517
LOSS train 0.14866871907675672 valid 0.2014793798727094
LOSS train 0.14866871907675672 valid 0.20154559464400051
LOSS train 0.14866871907675672 valid 0.20147875619931818
LOSS train 0.14866871907675672 valid 0.2015180646470099
LOSS train 0.14866871907675672 valid 0.20150111295142265
LOSS train 0.14866871907675672 valid 0.2014494466602354
LOSS train 0.14866871907675672 valid 0.20153505067700303
LOSS train 0.14866871907675672 valid 0.20163161580019923
LOSS train 0.14866871907675672 valid 0.201764690887086
LOSS train 0.14866871907675672 valid 0.20173482580317392
LOSS train 0.14866871907675672 valid 0.20182764420210214
LOSS train 0.14866871907675672 valid 0.20211856899892583
LOSS train 0.14866871907675672 valid 0.2022554234707312
LOSS train 0.14866871907675672 valid 0.20233243176319304
LOSS train 0.14866871907675672 valid 0.202260674292391
LOSS train 0.14866871907675672 valid 0.20225451195585556
LOSS train 0.14866871907675672 valid 0.20216142308195575
LOSS train 0.14866871907675672 valid 0.20202027041491843
LOSS train 0.14866871907675672 valid 0.20199521024808234
LOSS train 0.14866871907675672 valid 0.20195094650345188
LOSS train 0.14866871907675672 valid 0.2018837722488994
LOSS train 0.14866871907675672 valid 0.2016113447487777
LOSS train 0.14866871907675672 valid 0.20167635244320645
LOSS train 0.14866871907675672 valid 0.20177594446380373
LOSS train 0.14866871907675672 valid 0.2017692824204763
LOSS train 0.14866871907675672 valid 0.20172393301150182
LOSS train 0.14866871907675672 valid 0.20165189996829017
LOSS train 0.14866871907675672 valid 0.20163592390923035
LOSS train 0.14866871907675672 valid 0.20164707462886625
LOSS train 0.14866871907675672 valid 0.2015993300696899
LOSS train 0.14866871907675672 valid 0.20142318385163532
LOSS train 0.14866871907675672 valid 0.20140203527391773
LOSS train 0.14866871907675672 valid 0.20140548355546828
LOSS train 0.14866871907675672 valid 0.2014356789236166
LOSS train 0.14866871907675672 valid 0.2015442943168899
LOSS train 0.14866871907675672 valid 0.2014630812950231
LOSS train 0.14866871907675672 valid 0.2015018702958168
LOSS train 0.14866871907675672 valid 0.20149919390678406
LOSS train 0.14866871907675672 valid 0.20152766678644263
LOSS train 0.14866871907675672 valid 0.20158498694499333
LOSS train 0.14866871907675672 valid 0.20156616452722453
LOSS train 0.14866871907675672 valid 0.20151483622805172
LOSS train 0.14866871907675672 valid 0.20162826801093892
LOSS train 0.14866871907675672 valid 0.20164103487408475
LOSS train 0.14866871907675672 valid 0.20160167906127993
LOSS train 0.14866871907675672 valid 0.20163718830136693
LOSS train 0.14866871907675672 valid 0.2016161428309419
LOSS train 0.14866871907675672 valid 0.20151922605060912
LOSS train 0.14866871907675672 valid 0.20163543300528355
LOSS train 0.14866871907675672 valid 0.20159447111429707
LOSS train 0.14866871907675672 valid 0.20151091901244075
LOSS train 0.14866871907675672 valid 0.20151037675065872
LOSS train 0.14866871907675672 valid 0.20159822897598767
LOSS train 0.14866871907675672 valid 0.20168688798406323
LOSS train 0.14866871907675672 valid 0.20162574871191902
LOSS train 0.14866871907675672 valid 0.2015932691814024
LOSS train 0.14866871907675672 valid 0.2016476830849512
LOSS train 0.14866871907675672 valid 0.2016801211818959
LOSS train 0.14866871907675672 valid 0.2017438749441159
LOSS train 0.14866871907675672 valid 0.201624145032838
LOSS train 0.14866871907675672 valid 0.20177976443574436
LOSS train 0.14866871907675672 valid 0.20184410548543338
LOSS train 0.14866871907675672 valid 0.20179936469708434
LOSS train 0.14866871907675672 valid 0.2018918475067174
LOSS train 0.14866871907675672 valid 0.20188216356130748
LOSS train 0.14866871907675672 valid 0.20209127079489772
LOSS train 0.14866871907675672 valid 0.20223276344461177
LOSS train 0.14866871907675672 valid 0.20213673777151397
LOSS train 0.14866871907675672 valid 0.2022773722656592
LOSS train 0.14866871907675672 valid 0.20226257664687705
LOSS train 0.14866871907675672 valid 0.20215875432570535
LOSS train 0.14866871907675672 valid 0.20207469739827766
LOSS train 0.14866871907675672 valid 0.202104881719068
LOSS train 0.14866871907675672 valid 0.20223173508030212
LOSS train 0.14866871907675672 valid 0.20224828208560375
LOSS train 0.14866871907675672 valid 0.20236526323216303
LOSS train 0.14866871907675672 valid 0.20239047882995548
LOSS train 0.14866871907675672 valid 0.20234666839682844
LOSS train 0.14866871907675672 valid 0.20242600995706597
LOSS train 0.14866871907675672 valid 0.20244970098137854
LOSS train 0.14866871907675672 valid 0.202299268495652
LOSS train 0.14866871907675672 valid 0.20229501952553353
LOSS train 0.14866871907675672 valid 0.20237021065661928
LOSS train 0.14866871907675672 valid 0.2025806436878304
LOSS train 0.14866871907675672 valid 0.20263510182283928
LOSS train 0.14866871907675672 valid 0.20261978060868435
LOSS train 0.14866871907675672 valid 0.20248159006109157
LOSS train 0.14866871907675672 valid 0.20244270417539553
LOSS train 0.14866871907675672 valid 0.20249567477614286
LOSS train 0.14866871907675672 valid 0.2024168284876006
LOSS train 0.14866871907675672 valid 0.20230566126498742
LOSS train 0.14866871907675672 valid 0.20227967998520893
LOSS train 0.14866871907675672 valid 0.2023039603706122
LOSS train 0.14866871907675672 valid 0.202418832876588
LOSS train 0.14866871907675672 valid 0.2025312666322144
LOSS train 0.14866871907675672 valid 0.2026016390725468
LOSS train 0.14866871907675672 valid 0.2026614335989084
LOSS train 0.14866871907675672 valid 0.20252688154161974
LOSS train 0.14866871907675672 valid 0.20255913164124184
LOSS train 0.14866871907675672 valid 0.20252925472127067
LOSS train 0.14866871907675672 valid 0.20245847584798396
LOSS train 0.14866871907675672 valid 0.20256581445589908
LOSS train 0.14866871907675672 valid 0.20251207193231452
LOSS train 0.14866871907675672 valid 0.20251219993927977
LOSS train 0.14866871907675672 valid 0.20252443298901598
LOSS train 0.14866871907675672 valid 0.20251734110989858
LOSS train 0.14866871907675672 valid 0.20243947390639488
LOSS train 0.14866871907675672 valid 0.2024339174689806
LOSS train 0.14866871907675672 valid 0.2025161836205459
EPOCH 13:
  batch 1 loss: 0.13943718373775482
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.13851137459278107
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.1432874302069346
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.14706746861338615
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.15475997030735017
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.15827402224143347
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.1492284025464739
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.1491838414222002
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.14537733296553293
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.14388441294431686
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.14095223491842096
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.13940501337250075
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.1399882836983754
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.13916280972106115
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.13903967241446177
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.14055096823722124
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.14085623446632833
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.1428109887573454
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.1421679375987304
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.14088354855775834
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.14125401349294753
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.14165852422064001
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.14213324953680453
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.14277177676558495
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.14300204396247865
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.14314744908076066
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.14384810902454234
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.14416352180497988
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.14315956882361708
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.14413255006074904
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.1444519902429273
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.1452767881564796
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.14466429975899783
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.14547994776683695
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.14691055842808315
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.14603593800630835
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.14605325963851568
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.146305111874091
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.14629359600635675
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.14648640174418687
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.1463530577900933
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.1466696093834582
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.14752861944048903
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.14681838537481698
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.147535426583555
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.147422444075346
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.14660349091950883
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.14628416004901132
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.14631594063676134
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.14661874130368233
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.14655928357559092
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.1469491652857799
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.1468770274857305
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.14748876307297637
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.1473312102935531
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.14717007441712276
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.1472683518863561
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.14666639987764687
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.14716024292727647
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.14691762775182723
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.14717015868327657
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.14692544889065526
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.14681688353182779
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.1469447249546647
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.14699131433780377
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.14690277635148077
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.14656808616510078
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.14671710352687276
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.14717233116212097
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.14733937489134924
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.14708362204927794
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.14705964488287768
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.14699435540258068
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.1471029977137978
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.14719687779744467
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.1473600086020796
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.14720590799659877
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.146943670243789
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.1471775947492334
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.14742232393473387
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.1475493405704145
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.14723806119546656
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.14681268977113518
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.14712897918763615
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.14712110231904424
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.1471588235608367
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.14728107894289083
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.1475617592646317
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.14773617684841156
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.14797457920180426
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.14785920984142428
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.14783392052935518
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.14824772618150198
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.14833325876834544
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.14869380985435687
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.14877652501066527
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.14877526892214707
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.14876131166000756
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.14880237224126103
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.1487497515976429
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.14873657840313298
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.14872296227543963
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.14823136985012628
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.14823517554367965
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.14782703689166476
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.14794716632591104
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.14809618862432855
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.14792280575191533
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.1479870052239217
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.14837009378454902
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.148143033253717
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.14777458259569748
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.14740381805242692
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.14770265132711644
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.14734549885210785
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.14723364713376966
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.147556309771334
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.1475187959307331
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.14743250458180404
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.1476373174538215
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.14743498712778091
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.14759754029209496
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.14802431908807134
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.14792280253623763
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.14791386741399765
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.14796813536021444
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.14795091413841474
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.14801792829530314
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.1480219846432523
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.1480958883005839
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.14789202906020726
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.14779270321808077
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.1482313198590637
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.14826072646833177
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.14794847004943423
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.14798250399968205
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.14809046120104127
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.14798672929190207
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.14820573848786114
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.1484398060611316
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.1484797719522571
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.14845860550101375
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.14859197710777497
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.14888983757959473
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.14879371760220364
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.148827937061656
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.14889660751333042
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.14888737390975695
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.14907241927697354
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.14907900849978128
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.1490436423298539
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.14893403257194318
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.1490024715856789
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.14884449464160127
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.14889983554040231
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.1487452700161017
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.1487256393880601
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.14870180566854116
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.14885353976450627
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.1489726477302611
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.14892539604110008
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.14885357197052168
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.14885022687765717
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.14880147231061283
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.14890550049868498
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.14892271520143532
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.14879983958013043
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.14876879299325602
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.14850681570507365
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.14857509670888677
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.14849225566749685
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.14847975977978042
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.14834123634534074
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.148315220643049
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.14847119510173798
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.14835459552705288
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.14830754368992174
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.14816107726498937
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.14838368965926782
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.1482328816006581
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.1481862058708681
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.1480643395129796
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.14796696403801768
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.14789168872749028
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.1478329598500922
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.14786108122557723
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.14783379407649372
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.14768955527626454
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.14765573561033873
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.1477234749809692
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.14771944155711778
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.14765252471746257
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.1477546618932887
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.1475434287537619
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.14760382095208535
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.14770277623771405
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.14784815684339117
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.14806272627578843
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.14808991952008338
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.14818227428942918
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.14818377433279853
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.1481541194169238
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.14805793230022704
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.148128432568674
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.14797647050241144
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.1478589607167591
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.14778384505118725
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.1476422458027418
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.14771316168410925
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.14785915166139602
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.14788196988015379
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.14799959486666717
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.14805197596829822
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.14787242596394548
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.14795411859833918
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.14781532374521097
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.14786973616219887
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.1478960899584884
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.14801863802078108
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.14805092811584472
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.14799220381279338
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.14801558959591496
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.1479273245339971
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.14803503906088217
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.14793143527375327
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.1479377805593267
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.1479585388355318
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.1477986868928399
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.14767896009844983
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.1478479728102684
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.14797844870007915
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.14794282082082896
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.1478859397858509
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.14784578221221256
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.14785651849939468
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.1477674204793017
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.1478010913364998
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.14780238846770855
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.14779763989867525
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.1477902311210831
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.1478841973785543
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.14780004464151444
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.147776023411947
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.14776042592330058
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.14780890807813526
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.14785452126487483
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.1477396619102733
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.1478402784034129
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.14786016324677143
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.14789649248123168
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.14803701710416026
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.14799379693373801
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.14796764742244373
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.14797750209260174
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.14802418269363105
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.14801360393175855
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.14793350579441752
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.14782379356812136
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.14784617074783243
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.14780222478394325
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.14771514231788702
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.1476247105036528
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.1476465922288115
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.14756285311710654
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.14756576657857534
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.14751266029880458
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.1474979231029414
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.14755113425650704
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.1474526320137499
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.14749614883352208
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.14749758836308088
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.14749079038772514
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.14770690288954166
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.14757832574800853
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.14747493342919782
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.14767569379098172
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.14764624661917292
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.14760463700877677
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.14765380520547164
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.14773877103413854
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.14767401668100594
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.1475974033910332
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.14761511275919925
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.14762187812110067
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.14772224091646963
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.147795260160953
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.1477393399443776
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.14775289848653805
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.14766450254665525
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.14774139222913774
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.14771332002894574
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.14766982654492333
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.14776327817004695
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.14787989619130992
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.14786870098215038
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.14795974010249247
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.14799896426975526
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.1478851384564534
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.14777849499995893
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.14775882730881373
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.14789664755231915
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.14786657429491448
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.14785503701801742
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.1478486318435324
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.1478664205211108
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.14786012621681674
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.14784299341979942
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.14783289810860312
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.147814919985228
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.14771857626976506
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.1477222681716324
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.14768601576678264
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.14770264718860102
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.14767224137570448
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.14768005983223992
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.14779389031891582
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.14773894728922318
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.14771019568983115
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.14769104590236581
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.14770129192620515
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.14766859469755417
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.14765185872034997
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.14769180082130728
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.14760203404283082
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.14765662360649842
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.1475400634201392
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.14744004459100396
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.14747732625592772
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.14744184206956062
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.1475186059646534
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.14758670354807846
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.1475023423736713
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.14739029147513993
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.14742381243916328
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.14755244808855342
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.14756963931999745
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.1474849282810879
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.1474225233645129
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.14729520430167517
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.1472795636557481
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.14717876939567304
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.14736372304328701
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.14736426426179208
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.14738005973658588
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.14737470502006836
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.14737754799640937
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.14737757200895882
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.14735583162427635
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.14730675085140846
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.14722950731004988
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.1472405457173997
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.14729124857959422
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.14733772173988244
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.14737187144951633
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.1474056643079704
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.1473629231868165
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.14743698845390513
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.14753824235174243
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.14755819364344508
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.1474873861297965
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.14741729691088035
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.1474465587027165
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.1474158924801619
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.14735132513137963
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.14740478608706226
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.14735636143573647
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.14740318766406837
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.1474851533446623
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.14746740767303199
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.14738960846050367
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.1473885397866087
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.147354460331381
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.14731884785695626
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.14737559109926224
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.14739376882712046
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.14748261036708
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.14743042711553903
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.1474647035516759
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.14738762948789824
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.1473153260193373
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.14727676200272216
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.14720477899097648
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.14731560458791473
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.14728316278584921
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.14721098625427717
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.1472037054135083
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.14717437937201147
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.14720497255395984
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.1472491370873157
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.14718835781782103
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.1471653807803493
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.14717107167353435
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.14732337638773688
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.14735019055719908
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.14735535896277127
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.14732140874621844
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.14722898549937483
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.1472891855179964
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.14720444101140015
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.1471791970729828
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.14718368244438695
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.14712566602511787
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.14704806383505056
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.14702408530260666
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.14704924794258895
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.1469952147912803
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.1469811552023419
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.14712710603706391
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.14714363299328423
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.14717137212070022
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.14716688297018227
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.1472146855215135
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.1472041268289522
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.14728121587259757
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.14732018736112548
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.14734870308222106
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.14732968437371494
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.14729794765417084
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.1474165676566183
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.147363591460245
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.14747978557529473
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.14754977877035524
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.14747583552596136
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.1474529790498738
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.1474492931365967
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.1474004941577083
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.1473940768272592
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.1473586847222297
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.1472967303855158
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.14725033429472945
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.1472895624950148
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.147244060791477
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.14721450381686452
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.14724421508026564
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.14717940914219824
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.14729390125892577
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.14735537347995445
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.14731841068289594
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.14723598853534337
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.14719171562994068
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.14719198635713854
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.14717412296665738
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.14715367022472903
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.14714129978941903
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.14716534221105362
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.14718651649596445
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.147161035579723
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.14725657530860708
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.14723372152427788
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.1471745246152083
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.14719059582626212
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.1472137636480342
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.14717229982019786
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.14715095453522278
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.14711975817169462
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.14718098417251257
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.14715079250755664
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.14715232483749827
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.147171696595247
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.1472528934640729
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.1472201088600976
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.14716698190608582
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.14706842955322555
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.14707853063812543
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.14711039691202102
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.1470790584072023
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.14706679687254945
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.14713707594917372
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.14720257608366927
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.1472580535297698
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.14725738317723486
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.14737169955999163
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.14737169955999163 valid 0.21989783644676208
LOSS train 0.14737169955999163 valid 0.1935759112238884
LOSS train 0.14737169955999163 valid 0.1939228524764379
LOSS train 0.14737169955999163 valid 0.18407757952809334
LOSS train 0.14737169955999163 valid 0.17783620059490204
LOSS train 0.14737169955999163 valid 0.1878348415096601
LOSS train 0.14737169955999163 valid 0.1956986082451684
LOSS train 0.14737169955999163 valid 0.1957779172807932
LOSS train 0.14737169955999163 valid 0.1956113593445884
LOSS train 0.14737169955999163 valid 0.1973489210009575
LOSS train 0.14737169955999163 valid 0.1949610940434716
LOSS train 0.14737169955999163 valid 0.1953738071024418
LOSS train 0.14737169955999163 valid 0.19473551442989936
LOSS train 0.14737169955999163 valid 0.19378554501703807
LOSS train 0.14737169955999163 valid 0.1920090824365616
LOSS train 0.14737169955999163 valid 0.1932425620034337
LOSS train 0.14737169955999163 valid 0.19452247111236348
LOSS train 0.14737169955999163 valid 0.1935217703382174
LOSS train 0.14737169955999163 valid 0.19655246875788035
LOSS train 0.14737169955999163 valid 0.19656580910086632
LOSS train 0.14737169955999163 valid 0.1946034495319639
LOSS train 0.14737169955999163 valid 0.19351993094791065
LOSS train 0.14737169955999163 valid 0.19286436425602954
LOSS train 0.14737169955999163 valid 0.19284074070552984
LOSS train 0.14737169955999163 valid 0.19167765736579895
LOSS train 0.14737169955999163 valid 0.19229418497819167
LOSS train 0.14737169955999163 valid 0.1921948558754391
LOSS train 0.14737169955999163 valid 0.191917463604893
LOSS train 0.14737169955999163 valid 0.19244687053663978
LOSS train 0.14737169955999163 valid 0.1935771311322848
LOSS train 0.14737169955999163 valid 0.1944236635200439
LOSS train 0.14737169955999163 valid 0.19395899819210172
LOSS train 0.14737169955999163 valid 0.1941811555262768
LOSS train 0.14737169955999163 valid 0.19437034471946604
LOSS train 0.14737169955999163 valid 0.19632233338696617
LOSS train 0.14737169955999163 valid 0.19597283171282875
LOSS train 0.14737169955999163 valid 0.1963819724482459
LOSS train 0.14737169955999163 valid 0.19730785802790993
LOSS train 0.14737169955999163 valid 0.19609434100297782
LOSS train 0.14737169955999163 valid 0.19602480977773667
LOSS train 0.14737169955999163 valid 0.19642108787850635
LOSS train 0.14737169955999163 valid 0.19683454895303362
LOSS train 0.14737169955999163 valid 0.19699023594689924
LOSS train 0.14737169955999163 valid 0.19790858606045897
LOSS train 0.14737169955999163 valid 0.19773426155249277
LOSS train 0.14737169955999163 valid 0.19845317433709683
LOSS train 0.14737169955999163 valid 0.19884859184001355
LOSS train 0.14737169955999163 valid 0.19857195702691874
LOSS train 0.14737169955999163 valid 0.19923357908823053
LOSS train 0.14737169955999163 valid 0.19863916724920272
LOSS train 0.14737169955999163 valid 0.19903836735323363
LOSS train 0.14737169955999163 valid 0.1988419916194219
LOSS train 0.14737169955999163 valid 0.1993253011748476
LOSS train 0.14737169955999163 valid 0.19904488490687477
LOSS train 0.14737169955999163 valid 0.19906459748744965
LOSS train 0.14737169955999163 valid 0.19877733822379792
LOSS train 0.14737169955999163 valid 0.19896170915218822
LOSS train 0.14737169955999163 valid 0.19871748495718528
LOSS train 0.14737169955999163 valid 0.19923203602685766
LOSS train 0.14737169955999163 valid 0.1990568501253923
LOSS train 0.14737169955999163 valid 0.1992275206769099
LOSS train 0.14737169955999163 valid 0.1998691164678143
LOSS train 0.14737169955999163 valid 0.1996415016197023
LOSS train 0.14737169955999163 valid 0.20029480895027518
LOSS train 0.14737169955999163 valid 0.2007741996875176
LOSS train 0.14737169955999163 valid 0.20074012604626743
LOSS train 0.14737169955999163 valid 0.2004762047262334
LOSS train 0.14737169955999163 valid 0.2003879981005893
LOSS train 0.14737169955999163 valid 0.19969107642553854
LOSS train 0.14737169955999163 valid 0.20014058564390455
LOSS train 0.14737169955999163 valid 0.1999042963897678
LOSS train 0.14737169955999163 valid 0.20030053849849436
LOSS train 0.14737169955999163 valid 0.20055715964265067
LOSS train 0.14737169955999163 valid 0.20030733218064178
LOSS train 0.14737169955999163 valid 0.20023340245087942
LOSS train 0.14737169955999163 valid 0.20073073043635017
LOSS train 0.14737169955999163 valid 0.20058330061373772
LOSS train 0.14737169955999163 valid 0.2005228109848805
LOSS train 0.14737169955999163 valid 0.20014635332023042
LOSS train 0.14737169955999163 valid 0.199533280916512
LOSS train 0.14737169955999163 valid 0.19898199519993345
LOSS train 0.14737169955999163 valid 0.19918486857559622
LOSS train 0.14737169955999163 valid 0.19883204314364009
LOSS train 0.14737169955999163 valid 0.19859631022527105
LOSS train 0.14737169955999163 valid 0.19805670959108015
LOSS train 0.14737169955999163 valid 0.197566433528135
LOSS train 0.14737169955999163 valid 0.19754061633828043
LOSS train 0.14737169955999163 valid 0.1972804909402674
LOSS train 0.14737169955999163 valid 0.19742422140716168
LOSS train 0.14737169955999163 valid 0.19765907128651936
LOSS train 0.14737169955999163 valid 0.19807214969462092
LOSS train 0.14737169955999163 valid 0.1980156673361426
LOSS train 0.14737169955999163 valid 0.1978482541538054
LOSS train 0.14737169955999163 valid 0.19811573925804585
LOSS train 0.14737169955999163 valid 0.19754926013319116
LOSS train 0.14737169955999163 valid 0.19741998395572105
LOSS train 0.14737169955999163 valid 0.19751368370867267
LOSS train 0.14737169955999163 valid 0.19763158155339106
LOSS train 0.14737169955999163 valid 0.19798687461650732
LOSS train 0.14737169955999163 valid 0.19826160609722138
LOSS train 0.14737169955999163 valid 0.19845230198732697
LOSS train 0.14737169955999163 valid 0.1987573670990327
LOSS train 0.14737169955999163 valid 0.1986662655779459
LOSS train 0.14737169955999163 valid 0.19874472486285064
LOSS train 0.14737169955999163 valid 0.19876025631314231
LOSS train 0.14737169955999163 valid 0.19928807273225965
LOSS train 0.14737169955999163 valid 0.19916573097215634
LOSS train 0.14737169955999163 valid 0.1994854727828944
LOSS train 0.14737169955999163 valid 0.19987865175129077
LOSS train 0.14737169955999163 valid 0.2000678677450527
LOSS train 0.14737169955999163 valid 0.1999982532348719
LOSS train 0.14737169955999163 valid 0.19960559452218668
LOSS train 0.14737169955999163 valid 0.19971975895155847
LOSS train 0.14737169955999163 valid 0.19990151101037076
LOSS train 0.14737169955999163 valid 0.19989006843255913
LOSS train 0.14737169955999163 valid 0.2003861297050427
LOSS train 0.14737169955999163 valid 0.20049240841315344
LOSS train 0.14737169955999163 valid 0.2001533619428085
LOSS train 0.14737169955999163 valid 0.19988420117302103
LOSS train 0.14737169955999163 valid 0.19963771564265093
LOSS train 0.14737169955999163 valid 0.1993183392631121
LOSS train 0.14737169955999163 valid 0.19918221821550464
LOSS train 0.14737169955999163 valid 0.199382420598976
LOSS train 0.14737169955999163 valid 0.1995884128395588
LOSS train 0.14737169955999163 valid 0.19953297221660615
LOSS train 0.14737169955999163 valid 0.19980066269636154
LOSS train 0.14737169955999163 valid 0.19966649011833462
LOSS train 0.14737169955999163 valid 0.19967015529982746
LOSS train 0.14737169955999163 valid 0.19982124772644783
LOSS train 0.14737169955999163 valid 0.19978038863493847
LOSS train 0.14737169955999163 valid 0.19952638310785512
LOSS train 0.14737169955999163 valid 0.1992578206188751
LOSS train 0.14737169955999163 valid 0.1989701344330508
LOSS train 0.14737169955999163 valid 0.1990297325511477
LOSS train 0.14737169955999163 valid 0.19903095088623188
LOSS train 0.14737169955999163 valid 0.19906258331063917
LOSS train 0.14737169955999163 valid 0.19889139770156275
LOSS train 0.14737169955999163 valid 0.19884553713643033
LOSS train 0.14737169955999163 valid 0.1986278557091308
LOSS train 0.14737169955999163 valid 0.19876036367246083
LOSS train 0.14737169955999163 valid 0.19869796678106835
LOSS train 0.14737169955999163 valid 0.1987038191775201
LOSS train 0.14737169955999163 valid 0.19864468036831676
LOSS train 0.14737169955999163 valid 0.1986450336666571
LOSS train 0.14737169955999163 valid 0.1984622700461026
LOSS train 0.14737169955999163 valid 0.19878573154341683
LOSS train 0.14737169955999163 valid 0.19853421052296957
LOSS train 0.14737169955999163 valid 0.19940368327740077
LOSS train 0.14737169955999163 valid 0.199402065885147
LOSS train 0.14737169955999163 valid 0.19931567430496216
LOSS train 0.14737169955999163 valid 0.19948655623473868
LOSS train 0.14737169955999163 valid 0.1993444691363134
LOSS train 0.14737169955999163 valid 0.1993075998779995
LOSS train 0.14737169955999163 valid 0.19939020905014757
LOSS train 0.14737169955999163 valid 0.19936080555762015
LOSS train 0.14737169955999163 valid 0.19957789243795934
LOSS train 0.14737169955999163 valid 0.19970372471080464
LOSS train 0.14737169955999163 valid 0.1997719272216664
LOSS train 0.14737169955999163 valid 0.19976592888622163
LOSS train 0.14737169955999163 valid 0.19958054777234793
LOSS train 0.14737169955999163 valid 0.19946104334378095
LOSS train 0.14737169955999163 valid 0.1993300011496485
LOSS train 0.14737169955999163 valid 0.19912221214156942
LOSS train 0.14737169955999163 valid 0.19896386636466515
LOSS train 0.14737169955999163 valid 0.19883720567732147
LOSS train 0.14737169955999163 valid 0.19892790249313216
LOSS train 0.14737169955999163 valid 0.19921243556602272
LOSS train 0.14737169955999163 valid 0.1992501581885985
LOSS train 0.14737169955999163 valid 0.1994440970865227
LOSS train 0.14737169955999163 valid 0.19949200162116218
LOSS train 0.14737169955999163 valid 0.19943488533036752
LOSS train 0.14737169955999163 valid 0.1992096393260845
LOSS train 0.14737169955999163 valid 0.19929205975091527
LOSS train 0.14737169955999163 valid 0.1992299940394259
LOSS train 0.14737169955999163 valid 0.19909895939486366
LOSS train 0.14737169955999163 valid 0.19902227852832188
LOSS train 0.14737169955999163 valid 0.1991230331066638
LOSS train 0.14737169955999163 valid 0.1993673067079501
LOSS train 0.14737169955999163 valid 0.19924848106320345
LOSS train 0.14737169955999163 valid 0.19924033449755774
LOSS train 0.14737169955999163 valid 0.19939204293061358
LOSS train 0.14737169955999163 valid 0.19933741359592794
LOSS train 0.14737169955999163 valid 0.19929945542186986
LOSS train 0.14737169955999163 valid 0.1991608707315248
LOSS train 0.14737169955999163 valid 0.19909939290703954
LOSS train 0.14737169955999163 valid 0.19909380576623384
LOSS train 0.14737169955999163 valid 0.19889837017352568
LOSS train 0.14737169955999163 valid 0.19890760131021765
LOSS train 0.14737169955999163 valid 0.19887389100733258
LOSS train 0.14737169955999163 valid 0.1988985538482666
LOSS train 0.14737169955999163 valid 0.19885751542620633
LOSS train 0.14737169955999163 valid 0.19874372084935507
LOSS train 0.14737169955999163 valid 0.19863607707418926
LOSS train 0.14737169955999163 valid 0.198466576251787
LOSS train 0.14737169955999163 valid 0.19820866951575647
LOSS train 0.14737169955999163 valid 0.19827644602984798
LOSS train 0.14737169955999163 valid 0.19841061895571385
LOSS train 0.14737169955999163 valid 0.19830207014926757
LOSS train 0.14737169955999163 valid 0.1983689948841555
LOSS train 0.14737169955999163 valid 0.19818383023142816
LOSS train 0.14737169955999163 valid 0.19805388071050692
LOSS train 0.14737169955999163 valid 0.1980418974515235
LOSS train 0.14737169955999163 valid 0.19810444380849454
LOSS train 0.14737169955999163 valid 0.1982617039306491
LOSS train 0.14737169955999163 valid 0.1980824342588099
LOSS train 0.14737169955999163 valid 0.1981947888593072
LOSS train 0.14737169955999163 valid 0.19812091385972672
LOSS train 0.14737169955999163 valid 0.1981300711631775
LOSS train 0.14737169955999163 valid 0.19795935818453156
LOSS train 0.14737169955999163 valid 0.1979120494354339
LOSS train 0.14737169955999163 valid 0.19780919450154236
LOSS train 0.14737169955999163 valid 0.19760727896442953
LOSS train 0.14737169955999163 valid 0.1976219911670461
LOSS train 0.14737169955999163 valid 0.1975682345784713
LOSS train 0.14737169955999163 valid 0.1974664010973864
LOSS train 0.14737169955999163 valid 0.19726358112637643
LOSS train 0.14737169955999163 valid 0.1971710605143402
LOSS train 0.14737169955999163 valid 0.19707504010528598
LOSS train 0.14737169955999163 valid 0.19718354851960046
LOSS train 0.14737169955999163 valid 0.197161965072155
LOSS train 0.14737169955999163 valid 0.19712678474538467
LOSS train 0.14737169955999163 valid 0.19714751644982947
LOSS train 0.14737169955999163 valid 0.19725692532789546
LOSS train 0.14737169955999163 valid 0.19739339434142625
LOSS train 0.14737169955999163 valid 0.19761792527304756
LOSS train 0.14737169955999163 valid 0.19773173978370903
LOSS train 0.14737169955999163 valid 0.19794587311765696
LOSS train 0.14737169955999163 valid 0.1980156242324595
LOSS train 0.14737169955999163 valid 0.19798082653351745
LOSS train 0.14737169955999163 valid 0.19808781710655793
LOSS train 0.14737169955999163 valid 0.1982545732032685
LOSS train 0.14737169955999163 valid 0.19828382095900074
LOSS train 0.14737169955999163 valid 0.19824773776684707
LOSS train 0.14737169955999163 valid 0.19837162000501257
LOSS train 0.14737169955999163 valid 0.19843249580961592
LOSS train 0.14737169955999163 valid 0.1983411099951146
LOSS train 0.14737169955999163 valid 0.19826300630841073
LOSS train 0.14737169955999163 valid 0.1982647977581545
LOSS train 0.14737169955999163 valid 0.19805195335314363
LOSS train 0.14737169955999163 valid 0.1980112796028455
LOSS train 0.14737169955999163 valid 0.1981606082797545
LOSS train 0.14737169955999163 valid 0.1979958895316794
LOSS train 0.14737169955999163 valid 0.19818549955823295
LOSS train 0.14737169955999163 valid 0.1983624032286347
LOSS train 0.14737169955999163 valid 0.1983294830638535
LOSS train 0.14737169955999163 valid 0.1982254338700597
LOSS train 0.14737169955999163 valid 0.19833592805061262
LOSS train 0.14737169955999163 valid 0.1983051806568138
LOSS train 0.14737169955999163 valid 0.19825461794094867
LOSS train 0.14737169955999163 valid 0.1982410424351692
LOSS train 0.14737169955999163 valid 0.19810879485778124
LOSS train 0.14737169955999163 valid 0.19820661966999373
LOSS train 0.14737169955999163 valid 0.1981128402967227
LOSS train 0.14737169955999163 valid 0.1980472860608514
LOSS train 0.14737169955999163 valid 0.19810019188067493
LOSS train 0.14737169955999163 valid 0.1981271313270554
LOSS train 0.14737169955999163 valid 0.1979580831782827
LOSS train 0.14737169955999163 valid 0.1981097194691037
LOSS train 0.14737169955999163 valid 0.19810030905666498
LOSS train 0.14737169955999163 valid 0.19807579058867233
LOSS train 0.14737169955999163 valid 0.19820928014101197
LOSS train 0.14737169955999163 valid 0.19827545343237069
LOSS train 0.14737169955999163 valid 0.19820942930848878
LOSS train 0.14737169955999163 valid 0.19824425740675491
LOSS train 0.14737169955999163 valid 0.1982314927960342
LOSS train 0.14737169955999163 valid 0.19818385640033206
LOSS train 0.14737169955999163 valid 0.19826403516955143
LOSS train 0.14737169955999163 valid 0.19836246455783274
LOSS train 0.14737169955999163 valid 0.1984944076010729
LOSS train 0.14737169955999163 valid 0.19846290344441378
LOSS train 0.14737169955999163 valid 0.19856456609449705
LOSS train 0.14737169955999163 valid 0.19884885118945556
LOSS train 0.14737169955999163 valid 0.19899155843607236
LOSS train 0.14737169955999163 valid 0.1990767895740314
LOSS train 0.14737169955999163 valid 0.19900996858423406
LOSS train 0.14737169955999163 valid 0.19901026067310484
LOSS train 0.14737169955999163 valid 0.19891861484584394
LOSS train 0.14737169955999163 valid 0.19878529033643735
LOSS train 0.14737169955999163 valid 0.19876209211178578
LOSS train 0.14737169955999163 valid 0.19872298634478025
LOSS train 0.14737169955999163 valid 0.19866839056549548
LOSS train 0.14737169955999163 valid 0.1983933667553232
LOSS train 0.14737169955999163 valid 0.19846846737204507
LOSS train 0.14737169955999163 valid 0.19857586458535262
LOSS train 0.14737169955999163 valid 0.19856547995617516
LOSS train 0.14737169955999163 valid 0.19852226889216817
LOSS train 0.14737169955999163 valid 0.19845518931902245
LOSS train 0.14737169955999163 valid 0.19843373877099818
LOSS train 0.14737169955999163 valid 0.19844302509276512
LOSS train 0.14737169955999163 valid 0.1983961184990817
LOSS train 0.14737169955999163 valid 0.19821931790445269
LOSS train 0.14737169955999163 valid 0.19819529135137387
LOSS train 0.14737169955999163 valid 0.1981971168273952
LOSS train 0.14737169955999163 valid 0.1982335078270257
LOSS train 0.14737169955999163 valid 0.19833503905999458
LOSS train 0.14737169955999163 valid 0.1982523937382408
LOSS train 0.14737169955999163 valid 0.19829761415017574
LOSS train 0.14737169955999163 valid 0.19829287870018275
LOSS train 0.14737169955999163 valid 0.19831685273344302
LOSS train 0.14737169955999163 valid 0.1983755416671435
LOSS train 0.14737169955999163 valid 0.19835623380947748
LOSS train 0.14737169955999163 valid 0.19830697917977705
LOSS train 0.14737169955999163 valid 0.19841176969776847
LOSS train 0.14737169955999163 valid 0.1984222101439771
LOSS train 0.14737169955999163 valid 0.19838856844628444
LOSS train 0.14737169955999163 valid 0.19842809735754735
LOSS train 0.14737169955999163 valid 0.19840610740821601
LOSS train 0.14737169955999163 valid 0.1983104343828443
LOSS train 0.14737169955999163 valid 0.1984237955708334
LOSS train 0.14737169955999163 valid 0.19838087688530645
LOSS train 0.14737169955999163 valid 0.19829685962085172
LOSS train 0.14737169955999163 valid 0.19829943518225962
LOSS train 0.14737169955999163 valid 0.1983959684356714
LOSS train 0.14737169955999163 valid 0.19848507509869376
LOSS train 0.14737169955999163 valid 0.19842873631961763
LOSS train 0.14737169955999163 valid 0.19839971507840518
LOSS train 0.14737169955999163 valid 0.1984596438588404
LOSS train 0.14737169955999163 valid 0.19849763459192132
LOSS train 0.14737169955999163 valid 0.19856672899850109
LOSS train 0.14737169955999163 valid 0.19844988202676178
LOSS train 0.14737169955999163 valid 0.19860212244905787
LOSS train 0.14737169955999163 valid 0.19866669372372006
LOSS train 0.14737169955999163 valid 0.19861944601078152
LOSS train 0.14737169955999163 valid 0.19870870638592744
LOSS train 0.14737169955999163 valid 0.19869882482748766
LOSS train 0.14737169955999163 valid 0.19891063329632297
LOSS train 0.14737169955999163 valid 0.19905004014662647
LOSS train 0.14737169955999163 valid 0.19895572742310966
LOSS train 0.14737169955999163 valid 0.1990907548227571
LOSS train 0.14737169955999163 valid 0.1990744985414274
LOSS train 0.14737169955999163 valid 0.1989740443013586
LOSS train 0.14737169955999163 valid 0.19888968128396803
LOSS train 0.14737169955999163 valid 0.1989200073588002
LOSS train 0.14737169955999163 valid 0.19904452727404898
LOSS train 0.14737169955999163 valid 0.19905475364692177
LOSS train 0.14737169955999163 valid 0.19916830810585193
LOSS train 0.14737169955999163 valid 0.19919330027584506
LOSS train 0.14737169955999163 valid 0.19915726260673364
LOSS train 0.14737169955999163 valid 0.1992414690400295
LOSS train 0.14737169955999163 valid 0.19925927817821504
LOSS train 0.14737169955999163 valid 0.19911148523655106
LOSS train 0.14737169955999163 valid 0.19910380130971383
LOSS train 0.14737169955999163 valid 0.19918348898693006
LOSS train 0.14737169955999163 valid 0.1993898657518764
LOSS train 0.14737169955999163 valid 0.19943975534128106
LOSS train 0.14737169955999163 valid 0.19942604423086077
LOSS train 0.14737169955999163 valid 0.1992918361160872
LOSS train 0.14737169955999163 valid 0.19925100844481897
LOSS train 0.14737169955999163 valid 0.19929959388038831
LOSS train 0.14737169955999163 valid 0.19922130363328117
LOSS train 0.14737169955999163 valid 0.19911520076952768
LOSS train 0.14737169955999163 valid 0.19909207958897407
LOSS train 0.14737169955999163 valid 0.19911920496660995
LOSS train 0.14737169955999163 valid 0.19922922109648333
LOSS train 0.14737169955999163 valid 0.1993334627067539
LOSS train 0.14737169955999163 valid 0.1993987329639076
LOSS train 0.14737169955999163 valid 0.19946777023354165
LOSS train 0.14737169955999163 valid 0.19933697036025244
LOSS train 0.14737169955999163 valid 0.19936620263502125
LOSS train 0.14737169955999163 valid 0.19933858828412163
LOSS train 0.14737169955999163 valid 0.19927009692482672
LOSS train 0.14737169955999163 valid 0.19937641185950178
LOSS train 0.14737169955999163 valid 0.19932149855558537
LOSS train 0.14737169955999163 valid 0.19931709586264013
LOSS train 0.14737169955999163 valid 0.1993311004687662
LOSS train 0.14737169955999163 valid 0.1993229613522363
LOSS train 0.14737169955999163 valid 0.19925050997766552
LOSS train 0.14737169955999163 valid 0.1992442269364129
LOSS train 0.14737169955999163 valid 0.19932845255062187
EPOCH 14:
  batch 1 loss: 0.14231756329536438
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.13851448893547058
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.14302165309588113
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.14650213718414307
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.1551442712545395
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.15779120723406473
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.14887559413909912
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.14922884106636047
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.14544164223803413
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.1436942033469677
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.1404383419589563
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.13920848878721395
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.13973534852266312
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.1393150437091078
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.13911765962839126
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.140719014685601
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.1405932802487822
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.14265276201897198
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.14164922229553523
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.14051780477166176
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.14108227619103023
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.14136942679231818
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.14143389333849368
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.14212870225310326
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.14227865934371947
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.14269519310731155
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.14343720895272713
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.14368897091065133
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.14259695236025186
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.14358151058355967
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.14384982999294035
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.14458679920062423
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.14382855210340384
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.1446869726128438
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.14615499739136015
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.14532281996475327
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.14538980939903776
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.14560484023470627
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.1457847792368669
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.14597679078578948
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.14574424522679028
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.14616196638061887
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.1471141996078713
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.14632012678140943
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.147066317167547
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.14693079872623735
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.14613120035922272
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.1459622591113051
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.14599904995791765
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.1462954804301262
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.14630462083162046
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.1466274055150839
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.14650969910171796
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.14712878316640854
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.1471197781237689
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.14699398912489414
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.14703352827774852
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.1465171155487669
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.14694502532987272
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.14669484905898572
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.14701029157540837
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.14670233464529436
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.14656075397654186
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.14670768252108246
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.14672886396829898
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.1466677709285057
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.14640002186173823
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.14645026350284324
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.14671712195959644
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.14699013350265366
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.1467764622098963
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.14676737505942583
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.14672634056577943
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.14679321999082695
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.14685477207104364
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.14706147354292243
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.14692543069650602
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.1467362136986011
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.14699397632215597
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.14721617503091694
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.14732988316703727
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.1470058374106884
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.14658478288406349
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.14692845470493748
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.1468941819141893
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.14696697830114253
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.14710467805465063
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.14743233091113242
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.14758280614453756
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.14779186240500874
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.14767993880169733
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.14762508942057256
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.14793004648339364
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.14795751544706365
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.148297701932882
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.1483632333111018
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.14831137680208561
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.148303856426964
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.1483581176761425
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.148315292224288
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.14831677027563056
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.14824983484896959
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.14776233887498819
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.1477997305158239
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.1474108800292015
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.1475086138355282
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.14772997914909203
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.14749092132680947
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.14756546743692608
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.14791836555708537
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.14773122946152817
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.14739742016951954
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.1470231644611443
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.1472690393004501
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.14690361029427984
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.14678233624275389
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.14704165862411514
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.14699878411020262
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.1469617318831572
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.14720462256421646
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.14699874104054506
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.1471312919356784
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.1475545138847537
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.1474439823819745
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.14743434011936188
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.14743918653518434
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.14744201924387865
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.14746943418867886
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.14747971134592397
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.14755920412448736
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.14736989186010288
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.14724730203549066
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.14772881466643253
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.14767820599363812
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.1473604522921421
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.14734133283662446
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.14746496804656772
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.14733618697610454
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.147583217554384
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.1478009118033307
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.14788032471076817
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.1478867323570688
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.14800067907655157
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.1483482860753106
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.14822299105340037
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.14824287333104708
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.14830948194476212
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.14827686724429195
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.14847536580074552
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.14850853716333706
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.14845649468780353
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.14839787776336857
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.14847594290192612
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.14828816128822117
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.14834565179001902
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.14823267307992166
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.14818473838886637
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.14813665012958682
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.1482774350534445
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.14847351196222008
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.148427344118216
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.14835206828551528
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.14836823012382705
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.14832068039331495
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.14843405263893533
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.14839151757488767
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.14823194959028038
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.14820424356453477
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.1479688195963583
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.14809407872312208
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.14803957703866458
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.1480564261071904
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.14790532837024314
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.14784569695763206
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.14798625894955225
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.1478924917226488
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.14782896452704392
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.14768834851598472
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.14787436101856177
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.14773936888409986
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.14770542286871546
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.14755048918036315
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.14745105530232028
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.14736266875558574
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.14729721960989203
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.147312610099713
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.14725806444564604
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.14712423866892116
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.14711232645013345
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.1471826414920782
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.14719167683293058
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.14712211462513855
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.14721527166780413
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.14696654266456968
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.1470006172091533
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.1470606855820028
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.1471800918733408
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.1473778984838664
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.14739934849229888
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.1474767203256488
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.1474781565331108
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.14748725999552426
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.1473962071126905
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.1474244685514885
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.14728283936657557
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.14717029982689514
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.14708106982823155
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.1469620200805366
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.14699975500979492
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.14713978160704885
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.14715683725066658
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.14729135725998654
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.1473476989238475
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.1471873788145658
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.14728658771099046
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.1471534963283274
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.14721239718698687
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.14725135136908346
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.14734907135299352
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.14736815345558255
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.1473118323006781
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.14734337250660132
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.14728100992104398
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.1474149995483458
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.14732460068331824
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.14737487571165625
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.14742066136803397
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.14726080774868788
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.1471365052214356
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.14729672641209934
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.1474463842583425
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.14737569582488003
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.1473249570814325
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.1472981700466739
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.14728402898032614
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.1472055898442612
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.14722770971210697
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.14719800848294706
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.14718406641208975
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.147180260065943
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.14727588229777902
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.14720344571165803
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.1471988873655904
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.14719115365601954
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.1471969204593678
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.14724404192188892
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.14711370566475246
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.14722398695566
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.14726362006372237
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.14730342051386833
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.1474391916892918
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.14738872325018285
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.14736384268216937
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.1473515601315367
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.1474192289452927
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.14738062498508953
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.14729856548954076
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.14719188516569692
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.147208294209136
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.1471830633110725
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.14706939747164532
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.14699097213740567
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.147021971359679
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.14689926848267065
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.14690733574471385
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.14688485810407123
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.1469065656934338
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.14697246342452605
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.14690577535159527
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.14694054893873357
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.14691270348990534
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.14688829020323121
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.14710817783525137
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.14698515476210275
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.1468745785138824
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.1470874390038459
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.14706121968771146
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.14702026366437082
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.14703849649664322
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.14715051350316832
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.14708691058527956
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.14701585479556245
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.1470498898899176
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.14703100859384302
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.147141024653326
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.14719286396891088
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.14711672641362877
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.14711070322017702
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.14700831207333964
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.14709300982027218
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.14703791148166886
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.14700161510032334
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.14711388526007585
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.1472168495612485
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.1472134992480278
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.14728249012920502
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.14733763819290732
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.14723648383413385
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.14713999432165886
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.1471014550079902
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.14723362060777373
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.14719963421588703
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.14718834507485035
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.14720212950028086
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.1472212381783079
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.14723350996382875
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.14720191432812316
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.1471743021257125
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.14717285916160996
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.14710108688281429
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.14710483479633976
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.14706749347253487
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.14709627949677337
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.14706933220765392
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.1470754146812454
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.1471874217585295
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.14715659376292575
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.14712400919517632
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.14710512540090045
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.14711876499932258
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.1470779839937932
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.1470528424757978
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.1470969199165471
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.14702329734041367
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.1470643949508667
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.14695477929034848
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.1468454101763734
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.14691190934944443
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.14688023521726254
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.14694536558606408
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.14702084840604546
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.14693604986441422
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.14682498916580872
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.14685165670192885
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.14696037945017887
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.1469669499700623
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.1468758025933444
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.14682455312394532
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.14668684290134099
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.1466677294277093
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.146577717927544
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.14677229432160394
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.14677200116673295
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.1467805341925732
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.14677891398685566
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.14676575039680292
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.1467624746413327
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.14674820699568453
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.14669319897633912
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.14661239117383956
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.14662636820746963
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.1466821858828718
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.1467410098476383
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.14676829932604807
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.14678257481313087
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.14675026646490846
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.14682453916687258
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.14692793241259772
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.14695243446109688
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.14688035742276245
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.14682217833903358
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.14685093520919262
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.1468034034425562
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.14675697835755872
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.14680766246906699
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.14675816006022072
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.14680248511097413
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.14689313640575047
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.14685777738326933
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.1467741944499918
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.1467771234136386
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.14674781258868916
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.14671849804972836
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.14677898143542642
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.14681609193483988
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.14689656886014532
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.14683987350457542
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.14688253347520475
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.1467889087535146
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.1467176915979699
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.14669239343073112
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.14663429451004373
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.14676193085647438
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.14672295374718183
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.14665246671670443
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.1466380507139962
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.14660874297914578
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.1466278029778569
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.1466789340988223
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.1466161571061
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.1465811710947615
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.14657945757997887
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.14672799474789594
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.1467401156305964
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.1467520317887958
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.14670804957624037
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.14662109619424085
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.14667620157925926
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.14659576566147625
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.14654793918132783
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.14655359512998575
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.14649661673019773
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.1464153115750838
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.14638287192302765
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.14641371286577648
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.14635714771125116
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.14633827175057199
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.1464869078321784
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.14649310001242716
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.14651831595635995
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.14652031573065877
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.14654870801469655
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.1465408029192585
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.14660997454814864
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.14664478718516338
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.1466743187405742
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.1466580484982589
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.14662642288578753
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.14673893800925525
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.14668814323487736
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.14681153303795358
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.14689501954057205
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.14681777439077978
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.14678609497704595
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.14679298201028038
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.1467414304719005
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.14674227405557588
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.14671024664470525
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.14665181224510107
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.14660705620812814
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.1466307068403525
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.14660113009934625
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.14656569760542268
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.14656703199865082
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.14649564202489523
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.14659717316747806
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.1466586776333488
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.14662270346596906
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.14654447287660524
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.1464994644576853
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.14650317120984568
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.14648210564080408
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.1464612775274109
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.14644114475126738
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.14646748709544707
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.14649449482626978
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.1464764981195164
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.14657217793033592
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.1465547672557937
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.14649227407243517
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.1465052040810067
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.1465326065642644
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.1464851534129768
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.14647570075180036
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.14643734719727067
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.14650074389289347
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.14646191200490072
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.1464618170495637
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.14647790016951384
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.14656501587318338
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.14653291936293084
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.1464814298274197
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.14638862406460823
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.14640092377647243
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.14643678809365918
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.14640402039233194
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.14638471861850527
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.1464637853205204
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.146539117195713
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.1465970661094848
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.14658118607258847
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.14668870433154751
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.14668870433154751 valid 0.20998254418373108
LOSS train 0.14668870433154751 valid 0.18325618654489517
LOSS train 0.14668870433154751 valid 0.18333873649438223
LOSS train 0.14668870433154751 valid 0.17350107058882713
LOSS train 0.14668870433154751 valid 0.1670662671327591
LOSS train 0.14668870433154751 valid 0.17647256205479303
LOSS train 0.14668870433154751 valid 0.1834683439561299
LOSS train 0.14668870433154751 valid 0.18353097699582577
LOSS train 0.14668870433154751 valid 0.18342154721419016
LOSS train 0.14668870433154751 valid 0.18470315784215927
LOSS train 0.14668870433154751 valid 0.18210576339201492
LOSS train 0.14668870433154751 valid 0.18245991691946983
LOSS train 0.14668870433154751 valid 0.181761506658334
LOSS train 0.14668870433154751 valid 0.18090928558792388
LOSS train 0.14668870433154751 valid 0.17923491696516672
LOSS train 0.14668870433154751 valid 0.18038076069206
LOSS train 0.14668870433154751 valid 0.1815174747915829
LOSS train 0.14668870433154751 valid 0.18053457472059461
LOSS train 0.14668870433154751 valid 0.18359943283231636
LOSS train 0.14668870433154751 valid 0.18357121869921683
LOSS train 0.14668870433154751 valid 0.18160887701170786
LOSS train 0.14668870433154751 valid 0.18058759380470624
LOSS train 0.14668870433154751 valid 0.17981589034847592
LOSS train 0.14668870433154751 valid 0.17982023830215135
LOSS train 0.14668870433154751 valid 0.17870261132717133
LOSS train 0.14668870433154751 valid 0.1792896814071215
LOSS train 0.14668870433154751 valid 0.17924076429119817
LOSS train 0.14668870433154751 valid 0.17889359167643956
LOSS train 0.14668870433154751 valid 0.17931087571999121
LOSS train 0.14668870433154751 valid 0.18030461470286052
LOSS train 0.14668870433154751 valid 0.18114634627296078
LOSS train 0.14668870433154751 valid 0.18070073705166578
LOSS train 0.14668870433154751 valid 0.1807314125877438
LOSS train 0.14668870433154751 valid 0.18100198971874573
LOSS train 0.14668870433154751 valid 0.18287785947322846
LOSS train 0.14668870433154751 valid 0.1825071345600817
LOSS train 0.14668870433154751 valid 0.1829337888472789
LOSS train 0.14668870433154751 valid 0.1837690166736904
LOSS train 0.14668870433154751 valid 0.18268466416077736
LOSS train 0.14668870433154751 valid 0.18253690153360366
LOSS train 0.14668870433154751 valid 0.1829050848396813
LOSS train 0.14668870433154751 valid 0.18326840407791592
LOSS train 0.14668870433154751 valid 0.18344626419766005
LOSS train 0.14668870433154751 valid 0.18436333774165672
LOSS train 0.14668870433154751 valid 0.18421217501163484
LOSS train 0.14668870433154751 valid 0.18488960356816
LOSS train 0.14668870433154751 valid 0.18524086950941288
LOSS train 0.14668870433154751 valid 0.18495198246091604
LOSS train 0.14668870433154751 valid 0.18553518245414813
LOSS train 0.14668870433154751 valid 0.1849736550450325
LOSS train 0.14668870433154751 valid 0.1853681083987741
LOSS train 0.14668870433154751 valid 0.18513304052444604
LOSS train 0.14668870433154751 valid 0.1856355847052808
LOSS train 0.14668870433154751 valid 0.18538302155556502
LOSS train 0.14668870433154751 valid 0.18543862754648383
LOSS train 0.14668870433154751 valid 0.18519998395017215
LOSS train 0.14668870433154751 valid 0.18539633092127347
LOSS train 0.14668870433154751 valid 0.18512624142498807
LOSS train 0.14668870433154751 valid 0.1856136198266078
LOSS train 0.14668870433154751 valid 0.18539262066284815
LOSS train 0.14668870433154751 valid 0.18556173023630362
LOSS train 0.14668870433154751 valid 0.18616887013758382
LOSS train 0.14668870433154751 valid 0.1859387152251743
LOSS train 0.14668870433154751 valid 0.18657968938350677
LOSS train 0.14668870433154751 valid 0.18702404957551222
LOSS train 0.14668870433154751 valid 0.18697668679735877
LOSS train 0.14668870433154751 valid 0.18675714197443494
LOSS train 0.14668870433154751 valid 0.186660795527346
LOSS train 0.14668870433154751 valid 0.1860083278970442
LOSS train 0.14668870433154751 valid 0.1864610037633351
LOSS train 0.14668870433154751 valid 0.18624099732284816
LOSS train 0.14668870433154751 valid 0.1866185191190905
LOSS train 0.14668870433154751 valid 0.18684897194169972
LOSS train 0.14668870433154751 valid 0.18658638826093157
LOSS train 0.14668870433154751 valid 0.18653695404529572
LOSS train 0.14668870433154751 valid 0.18700227455088966
LOSS train 0.14668870433154751 valid 0.18687161822597703
LOSS train 0.14668870433154751 valid 0.18681719192327598
LOSS train 0.14668870433154751 valid 0.18648879494093643
LOSS train 0.14668870433154751 valid 0.18590167220681905
LOSS train 0.14668870433154751 valid 0.1853701400536078
LOSS train 0.14668870433154751 valid 0.18557794337592473
LOSS train 0.14668870433154751 valid 0.18524588949709053
LOSS train 0.14668870433154751 valid 0.1850263625383377
LOSS train 0.14668870433154751 valid 0.18450405439909767
LOSS train 0.14668870433154751 valid 0.18406451753405637
LOSS train 0.14668870433154751 valid 0.18402712132738924
LOSS train 0.14668870433154751 valid 0.18378412232480265
LOSS train 0.14668870433154751 valid 0.1839053255788396
LOSS train 0.14668870433154751 valid 0.1841106495923466
LOSS train 0.14668870433154751 valid 0.18453064348016465
LOSS train 0.14668870433154751 valid 0.18447796741257544
LOSS train 0.14668870433154751 valid 0.18433218072819454
LOSS train 0.14668870433154751 valid 0.1845759465656382
LOSS train 0.14668870433154751 valid 0.18401750326156616
LOSS train 0.14668870433154751 valid 0.18389792709300914
LOSS train 0.14668870433154751 valid 0.18397795938953912
LOSS train 0.14668870433154751 valid 0.18409615648644312
LOSS train 0.14668870433154751 valid 0.1844336163214963
LOSS train 0.14668870433154751 valid 0.18466666117310523
LOSS train 0.14668870433154751 valid 0.18486810748529905
LOSS train 0.14668870433154751 valid 0.18515784424893997
LOSS train 0.14668870433154751 valid 0.1850578586742716
LOSS train 0.14668870433154751 valid 0.18515780252905992
LOSS train 0.14668870433154751 valid 0.18519009436879838
LOSS train 0.14668870433154751 valid 0.18569709065387835
LOSS train 0.14668870433154751 valid 0.18558302591337222
LOSS train 0.14668870433154751 valid 0.18587364077016158
LOSS train 0.14668870433154751 valid 0.18626752263362253
LOSS train 0.14668870433154751 valid 0.1864550685340708
LOSS train 0.14668870433154751 valid 0.186374406288336
LOSS train 0.14668870433154751 valid 0.18598860981208937
LOSS train 0.14668870433154751 valid 0.18606642310598256
LOSS train 0.14668870433154751 valid 0.18624969937822275
LOSS train 0.14668870433154751 valid 0.1862346421117368
LOSS train 0.14668870433154751 valid 0.18673250490221485
LOSS train 0.14668870433154751 valid 0.18680145763433897
LOSS train 0.14668870433154751 valid 0.1864747722017563
LOSS train 0.14668870433154751 valid 0.18621433895676076
LOSS train 0.14668870433154751 valid 0.18595605281492075
LOSS train 0.14668870433154751 valid 0.18562567295614352
LOSS train 0.14668870433154751 valid 0.1855051734896957
LOSS train 0.14668870433154751 valid 0.1856899913248977
LOSS train 0.14668870433154751 valid 0.1858936170897176
LOSS train 0.14668870433154751 valid 0.18586884200572967
LOSS train 0.14668870433154751 valid 0.18611343973685826
LOSS train 0.14668870433154751 valid 0.18600010120962548
LOSS train 0.14668870433154751 valid 0.18599741358775645
LOSS train 0.14668870433154751 valid 0.18613224644069523
LOSS train 0.14668870433154751 valid 0.18607651740312575
LOSS train 0.14668870433154751 valid 0.18582184874374447
LOSS train 0.14668870433154751 valid 0.18557344523794722
LOSS train 0.14668870433154751 valid 0.18526387270680048
LOSS train 0.14668870433154751 valid 0.1853490489632336
LOSS train 0.14668870433154751 valid 0.1853772407328641
LOSS train 0.14668870433154751 valid 0.18537969010717728
LOSS train 0.14668870433154751 valid 0.1852053851976882
LOSS train 0.14668870433154751 valid 0.1851605642316998
LOSS train 0.14668870433154751 valid 0.1849646475032079
LOSS train 0.14668870433154751 valid 0.18508024279560362
LOSS train 0.14668870433154751 valid 0.1850059634827553
LOSS train 0.14668870433154751 valid 0.18500913364786498
LOSS train 0.14668870433154751 valid 0.18494831859231828
LOSS train 0.14668870433154751 valid 0.18495272317280373
LOSS train 0.14668870433154751 valid 0.18478501911821035
LOSS train 0.14668870433154751 valid 0.18509086047949858
LOSS train 0.14668870433154751 valid 0.18483796077115194
LOSS train 0.14668870433154751 valid 0.18569247877678355
LOSS train 0.14668870433154751 valid 0.18568696061636777
LOSS train 0.14668870433154751 valid 0.18561361153920491
LOSS train 0.14668870433154751 valid 0.185785459761588
LOSS train 0.14668870433154751 valid 0.18565427904066287
LOSS train 0.14668870433154751 valid 0.18563123933630052
LOSS train 0.14668870433154751 valid 0.1857165177534153
LOSS train 0.14668870433154751 valid 0.18569419134047724
LOSS train 0.14668870433154751 valid 0.18591385506666624
LOSS train 0.14668870433154751 valid 0.18602689047148274
LOSS train 0.14668870433154751 valid 0.18607371941774706
LOSS train 0.14668870433154751 valid 0.18605809419784905
LOSS train 0.14668870433154751 valid 0.1858794500119984
LOSS train 0.14668870433154751 valid 0.18575587618795242
LOSS train 0.14668870433154751 valid 0.18563324222226202
LOSS train 0.14668870433154751 valid 0.1854400375138031
LOSS train 0.14668870433154751 valid 0.18526773872535404
LOSS train 0.14668870433154751 valid 0.18514538913062126
LOSS train 0.14668870433154751 valid 0.18523585697613568
LOSS train 0.14668870433154751 valid 0.18550995982692628
LOSS train 0.14668870433154751 valid 0.18553163678873152
LOSS train 0.14668870433154751 valid 0.18571801404275837
LOSS train 0.14668870433154751 valid 0.18576142586329403
LOSS train 0.14668870433154751 valid 0.1857060852803682
LOSS train 0.14668870433154751 valid 0.185499950377054
LOSS train 0.14668870433154751 valid 0.18554780447069621
LOSS train 0.14668870433154751 valid 0.18547731518060312
LOSS train 0.14668870433154751 valid 0.18534205411161694
LOSS train 0.14668870433154751 valid 0.1852686484767632
LOSS train 0.14668870433154751 valid 0.18537138208831097
LOSS train 0.14668870433154751 valid 0.18560831834760944
LOSS train 0.14668870433154751 valid 0.18549852431153452
LOSS train 0.14668870433154751 valid 0.18549036780993144
LOSS train 0.14668870433154751 valid 0.18563072679780465
LOSS train 0.14668870433154751 valid 0.1855705635069491
LOSS train 0.14668870433154751 valid 0.18553890971863857
LOSS train 0.14668870433154751 valid 0.18540837056934834
LOSS train 0.14668870433154751 valid 0.18534616602433696
LOSS train 0.14668870433154751 valid 0.1853470123743498
LOSS train 0.14668870433154751 valid 0.18517813668212788
LOSS train 0.14668870433154751 valid 0.1851736971355499
LOSS train 0.14668870433154751 valid 0.18513350740627008
LOSS train 0.14668870433154751 valid 0.1851431825443318
LOSS train 0.14668870433154751 valid 0.18510293078984266
LOSS train 0.14668870433154751 valid 0.18499548290856183
LOSS train 0.14668870433154751 valid 0.18488342118077944
LOSS train 0.14668870433154751 valid 0.18471698608902312
LOSS train 0.14668870433154751 valid 0.18446429127301925
LOSS train 0.14668870433154751 valid 0.18452543476406408
LOSS train 0.14668870433154751 valid 0.1846475189712447
LOSS train 0.14668870433154751 valid 0.18455224740083773
LOSS train 0.14668870433154751 valid 0.18462471439311254
LOSS train 0.14668870433154751 valid 0.1844520527869463
LOSS train 0.14668870433154751 valid 0.1843346352156122
LOSS train 0.14668870433154751 valid 0.18433108431572962
LOSS train 0.14668870433154751 valid 0.18440361453100965
LOSS train 0.14668870433154751 valid 0.18456866928175383
LOSS train 0.14668870433154751 valid 0.1844025079796954
LOSS train 0.14668870433154751 valid 0.18450584052835853
LOSS train 0.14668870433154751 valid 0.1844365177811056
LOSS train 0.14668870433154751 valid 0.18445011887412804
LOSS train 0.14668870433154751 valid 0.18427215431010324
LOSS train 0.14668870433154751 valid 0.1842231280037335
LOSS train 0.14668870433154751 valid 0.18412425393741844
LOSS train 0.14668870433154751 valid 0.18393755844741497
LOSS train 0.14668870433154751 valid 0.18395681182543436
LOSS train 0.14668870433154751 valid 0.18390607429999056
LOSS train 0.14668870433154751 valid 0.18379931082559187
LOSS train 0.14668870433154751 valid 0.1835981150744138
LOSS train 0.14668870433154751 valid 0.18351264771777914
LOSS train 0.14668870433154751 valid 0.1834176139000359
LOSS train 0.14668870433154751 valid 0.18351859847704569
LOSS train 0.14668870433154751 valid 0.1834904626689174
LOSS train 0.14668870433154751 valid 0.18345447275703308
LOSS train 0.14668870433154751 valid 0.18347306797901788
LOSS train 0.14668870433154751 valid 0.18357882444901316
LOSS train 0.14668870433154751 valid 0.1837071284119572
LOSS train 0.14668870433154751 valid 0.18391403132014805
LOSS train 0.14668870433154751 valid 0.18402461397700606
LOSS train 0.14668870433154751 valid 0.18422415301138084
LOSS train 0.14668870433154751 valid 0.18428118108657368
LOSS train 0.14668870433154751 valid 0.18425297769656868
LOSS train 0.14668870433154751 valid 0.1843643708721451
LOSS train 0.14668870433154751 valid 0.18452591658670664
LOSS train 0.14668870433154751 valid 0.18455413247233834
LOSS train 0.14668870433154751 valid 0.18452229904002898
LOSS train 0.14668870433154751 valid 0.1846258211721722
LOSS train 0.14668870433154751 valid 0.1846621536828102
LOSS train 0.14668870433154751 valid 0.18457395248746467
LOSS train 0.14668870433154751 valid 0.1845062627822538
LOSS train 0.14668870433154751 valid 0.18451428025209604
LOSS train 0.14668870433154751 valid 0.1843062733381862
LOSS train 0.14668870433154751 valid 0.18427201167990764
LOSS train 0.14668870433154751 valid 0.18441536224976615
LOSS train 0.14668870433154751 valid 0.18426057801020046
LOSS train 0.14668870433154751 valid 0.18445264422353894
LOSS train 0.14668870433154751 valid 0.1846251313070782
LOSS train 0.14668870433154751 valid 0.1846029871580552
LOSS train 0.14668870433154751 valid 0.184488827741243
LOSS train 0.14668870433154751 valid 0.1845942904592043
LOSS train 0.14668870433154751 valid 0.18455468004028644
LOSS train 0.14668870433154751 valid 0.18450621823709173
LOSS train 0.14668870433154751 valid 0.18449200558662415
LOSS train 0.14668870433154751 valid 0.184365359969823
LOSS train 0.14668870433154751 valid 0.18445567065288151
LOSS train 0.14668870433154751 valid 0.1843594045860494
LOSS train 0.14668870433154751 valid 0.18429386545353987
LOSS train 0.14668870433154751 valid 0.18435183202519137
LOSS train 0.14668870433154751 valid 0.184365724446252
LOSS train 0.14668870433154751 valid 0.1842072099331288
LOSS train 0.14668870433154751 valid 0.18434767142053723
LOSS train 0.14668870433154751 valid 0.1843368377119418
LOSS train 0.14668870433154751 valid 0.18429857790470122
LOSS train 0.14668870433154751 valid 0.18443037147037827
LOSS train 0.14668870433154751 valid 0.18449666641140713
LOSS train 0.14668870433154751 valid 0.18442609101647206
LOSS train 0.14668870433154751 valid 0.1844476070819479
LOSS train 0.14668870433154751 valid 0.18443194245392422
LOSS train 0.14668870433154751 valid 0.1843860975109545
LOSS train 0.14668870433154751 valid 0.18447728069980493
LOSS train 0.14668870433154751 valid 0.1845679230876823
LOSS train 0.14668870433154751 valid 0.18469040976581078
LOSS train 0.14668870433154751 valid 0.18465371363692815
LOSS train 0.14668870433154751 valid 0.18476641738986618
LOSS train 0.14668870433154751 valid 0.1850444681065924
LOSS train 0.14668870433154751 valid 0.18518701725172035
LOSS train 0.14668870433154751 valid 0.18527121455782522
LOSS train 0.14668870433154751 valid 0.1852110997655175
LOSS train 0.14668870433154751 valid 0.18522980490672417
LOSS train 0.14668870433154751 valid 0.1851460879352549
LOSS train 0.14668870433154751 valid 0.1850218250811529
LOSS train 0.14668870433154751 valid 0.18499831283818863
LOSS train 0.14668870433154751 valid 0.18496605205748762
LOSS train 0.14668870433154751 valid 0.1849213140184769
LOSS train 0.14668870433154751 valid 0.18465392286261767
LOSS train 0.14668870433154751 valid 0.18473680338463597
LOSS train 0.14668870433154751 valid 0.18484775764001926
LOSS train 0.14668870433154751 valid 0.1848298971067395
LOSS train 0.14668870433154751 valid 0.1847851637985323
LOSS train 0.14668870433154751 valid 0.18472417970982993
LOSS train 0.14668870433154751 valid 0.18470107593263188
LOSS train 0.14668870433154751 valid 0.1847046232141013
LOSS train 0.14668870433154751 valid 0.18465601565509007
LOSS train 0.14668870433154751 valid 0.18448798209940856
LOSS train 0.14668870433154751 valid 0.18445144522271745
LOSS train 0.14668870433154751 valid 0.18444240418717317
LOSS train 0.14668870433154751 valid 0.18448017005409514
LOSS train 0.14668870433154751 valid 0.18455977475239058
LOSS train 0.14668870433154751 valid 0.18446760644783844
LOSS train 0.14668870433154751 valid 0.18451656456346865
LOSS train 0.14668870433154751 valid 0.18449792320896316
LOSS train 0.14668870433154751 valid 0.18451228907275757
LOSS train 0.14668870433154751 valid 0.184580607910951
LOSS train 0.14668870433154751 valid 0.18456356267002333
LOSS train 0.14668870433154751 valid 0.18451504290893378
LOSS train 0.14668870433154751 valid 0.18460465633239684
LOSS train 0.14668870433154751 valid 0.18460614670460163
LOSS train 0.14668870433154751 valid 0.18457681033454956
LOSS train 0.14668870433154751 valid 0.18461623953448403
LOSS train 0.14668870433154751 valid 0.18459684656574982
LOSS train 0.14668870433154751 valid 0.1845092066116147
LOSS train 0.14668870433154751 valid 0.184619581139975
LOSS train 0.14668870433154751 valid 0.18457448578649951
LOSS train 0.14668870433154751 valid 0.18449709744123785
LOSS train 0.14668870433154751 valid 0.18449387259972402
LOSS train 0.14668870433154751 valid 0.18459235717313358
LOSS train 0.14668870433154751 valid 0.18467761443299094
LOSS train 0.14668870433154751 valid 0.18463119476560563
LOSS train 0.14668870433154751 valid 0.18461390094289296
LOSS train 0.14668870433154751 valid 0.1846815090074148
LOSS train 0.14668870433154751 valid 0.18471948877445557
LOSS train 0.14668870433154751 valid 0.18479560880825438
LOSS train 0.14668870433154751 valid 0.18468790724873543
LOSS train 0.14668870433154751 valid 0.1848355787936772
LOSS train 0.14668870433154751 valid 0.18490379863644238
LOSS train 0.14668870433154751 valid 0.1848585050172481
LOSS train 0.14668870433154751 valid 0.1849438175559044
LOSS train 0.14668870433154751 valid 0.18493415378607236
LOSS train 0.14668870433154751 valid 0.18513836080859775
LOSS train 0.14668870433154751 valid 0.18526502044740437
LOSS train 0.14668870433154751 valid 0.18517358864589437
LOSS train 0.14668870433154751 valid 0.18529864393831386
LOSS train 0.14668870433154751 valid 0.1852875240372889
LOSS train 0.14668870433154751 valid 0.18518899399168182
LOSS train 0.14668870433154751 valid 0.1851048431841724
LOSS train 0.14668870433154751 valid 0.18513471308771196
LOSS train 0.14668870433154751 valid 0.18524651336455775
LOSS train 0.14668870433154751 valid 0.18525184851084184
LOSS train 0.14668870433154751 valid 0.18535469618758985
LOSS train 0.14668870433154751 valid 0.1853770507107149
LOSS train 0.14668870433154751 valid 0.18535275841076698
LOSS train 0.14668870433154751 valid 0.18544249324496165
LOSS train 0.14668870433154751 valid 0.18546150940306047
LOSS train 0.14668870433154751 valid 0.1853206833261898
LOSS train 0.14668870433154751 valid 0.18531402797378294
LOSS train 0.14668870433154751 valid 0.18539463752560295
LOSS train 0.14668870433154751 valid 0.18559412519599117
LOSS train 0.14668870433154751 valid 0.1856409061646116
LOSS train 0.14668870433154751 valid 0.18562670418120533
LOSS train 0.14668870433154751 valid 0.18549940247700605
LOSS train 0.14668870433154751 valid 0.18545893211474365
LOSS train 0.14668870433154751 valid 0.18549846881269383
LOSS train 0.14668870433154751 valid 0.18542106768914632
LOSS train 0.14668870433154751 valid 0.18532914632236178
LOSS train 0.14668870433154751 valid 0.18530973372980952
LOSS train 0.14668870433154751 valid 0.18533798292405881
LOSS train 0.14668870433154751 valid 0.18543339263921405
LOSS train 0.14668870433154751 valid 0.18552632613081327
LOSS train 0.14668870433154751 valid 0.18558079192645094
LOSS train 0.14668870433154751 valid 0.18565720883237213
LOSS train 0.14668870433154751 valid 0.18553643657009028
LOSS train 0.14668870433154751 valid 0.18555588397641035
LOSS train 0.14668870433154751 valid 0.18552761603560713
LOSS train 0.14668870433154751 valid 0.185459099152742
LOSS train 0.14668870433154751 valid 0.18555860563042414
LOSS train 0.14668870433154751 valid 0.18551016120707037
LOSS train 0.14668870433154751 valid 0.18550029589415906
LOSS train 0.14668870433154751 valid 0.18551237713800717
LOSS train 0.14668870433154751 valid 0.18550513745819935
LOSS train 0.14668870433154751 valid 0.1854361961989052
LOSS train 0.14668870433154751 valid 0.18543220457175505
LOSS train 0.14668870433154751 valid 0.1855088811175933
EPOCH 15:
  batch 1 loss: 0.14067624509334564
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.13911586999893188
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.14401192466417947
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.14894255250692368
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.15622446537017823
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.1581123967965444
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.14930683267968042
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.1484148083254695
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.14470373590787253
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.143513286113739
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.14057077331976456
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.13864419423043728
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.13895786668245608
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.13800920067088945
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.13778058737516402
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.139606689568609
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.13947173383306055
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.14188679845796692
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.1409129178837726
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.13963311202824116
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.14036550869544348
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.14071916687217625
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.14090195265801056
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.1415325834726294
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.14178841501474382
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.141979902696151
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.14257299706891732
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.14284730782466276
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.14188137701873121
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.14299454440673193
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.14324710926701945
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.143757957033813
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.14289741601907846
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.1438056530759615
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.14519212522677014
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.1444288461158673
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.14450762179252263
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.1447172649204731
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.14496488047715944
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.14500050749629737
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.14488056356587062
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.14529635278241976
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.14607294129077777
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.1454781498759985
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.14626619733042187
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.1460065107954585
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.14530089814612207
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.14522790567328533
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.14533244620780555
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.14567560881376265
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.1456045660318113
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.14601359573694375
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.1459146667763872
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.14648074794698646
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.14641358662735332
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.14628367205815657
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.1463720968417954
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.14581720338299356
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.14629518341715053
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.14603880681097509
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.14621404827129644
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.14596520424369844
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.14584944018768886
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.14608124212827533
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.1461988328741147
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.14614534005522728
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.14589292645009597
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.14605968938592603
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.1463496986290683
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.1465339304081031
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.14630064168866252
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.1462852606135938
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.14620370172882732
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.14626424830104853
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.1462811115384102
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.14648109174480564
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.14634239412360378
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.14604571948830897
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.14620677532651757
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.14643084248527885
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.14657633052563962
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.14622955196877804
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.14576244803078203
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.14612093142100743
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.14613623654141145
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.14620115296092145
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.1463929894326747
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.14671023494817995
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.14683879091498558
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.14709550340970357
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.14702096434084924
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.146946894571833
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.14733788355063365
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.14751102331470936
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.14781365002456465
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.14791275250415006
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.14788168561212794
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.14783035705284198
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.14788260213052384
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.14779184460639955
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.14780790616970252
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.14776678602485097
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.14730566371123768
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.14725333258796197
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.14687293775024868
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.14693385917904242
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.14719983176371762
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.14697946939203474
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.14705136924161824
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.14739900109442797
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.14720131926708394
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.1468719793483615
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.14654988373539088
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.14677236105004945
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.14642518953136777
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.1463039117897379
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.1466153390132464
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.14659486092248206
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.14648320657365463
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.14668333952625592
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.14641230633436156
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.14653977641805274
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.14695683654730882
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.14690026352482458
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.14688542330265045
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.14691116980143956
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.1469201057210682
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.14699670288246125
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.1469940849283869
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.1470405439917858
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.14684966121238607
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.14668875407766213
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.1471681150614767
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.14718078338165783
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.14686239433509332
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.14682444179540172
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.1469394650011167
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.1468463453153769
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.1470737213389479
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.14726484316800323
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.1473384493737356
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.14732451658223716
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.1474024525784946
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.14773152473693094
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.14763920692534283
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.14767705909397505
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.1477503671633954
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.1476924411851812
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.14791893464006833
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.14794874146580697
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.14786861555663167
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.1478088050590534
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.14781976676259945
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.14765711607677595
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.14769767960233074
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.1475563979206177
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.14751897244506582
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.14749332371203205
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.14766032970363988
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.14788336507044733
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.14784274472398048
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.1477771221579593
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.14785995316103193
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.1477799964841546
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.1478903836824677
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.14786071684884738
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.14772002064360829
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.14768284590293965
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.14742424414002683
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.1475186387405676
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.14745148798527077
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.14747810892240945
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.14732678221657097
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.14727389576962624
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.14742886709315436
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.14729364622722974
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.1472638972566626
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.14712012354075238
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.14729656990846443
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.14714208559857475
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.147114969419511
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.14701684097667317
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.14691091072363932
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.14682263354568378
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.14676113861638146
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.14681067678236193
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.14677017281718432
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.14663988082332813
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.14662267803830445
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.14669248626420373
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.14671351109187641
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.1466356338157008
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.1467191729057638
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.14649196130405998
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.1465333341788023
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.1466254569894197
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.14675440363169925
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.14694573278679993
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.1470034453437556
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.14708360008895396
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.14707114002597865
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.14704478296017884
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.14693180306498052
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.14700096889453776
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.1468475059765141
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.14677042564720783
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.14668388204010213
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.14655045326799154
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.14664231967983063
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.14676540522348314
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.14673891098578395
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.14689145961178923
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.14695913229190127
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.14676405611300022
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.1468667273951131
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.14674752336685304
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.14681500395596853
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.1468653353529239
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.14696477387593762
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.14700808626684275
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.14692799079472124
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.14692045479744403
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.14683280217006067
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.1469717393629253
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.14686575399504767
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.14690231109878657
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.14692992195158802
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.14675574537301272
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.14663537930062764
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.14681574504660524
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.14695162351900365
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.1469162291815055
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.14684051822055563
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.14679262752079555
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.14680001605698403
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.1466989290221768
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.14673114096439338
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.14670986002858946
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.14670346873699372
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.14668339928612112
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.14677227171006538
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.14671451330554386
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.14669937315423792
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.14668302051723003
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.14670237445709658
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.1467361821815735
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.146618821601636
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.14673169017318757
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.1467455971432498
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.1467767847776413
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.14693590369357531
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.14686197988570682
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.14684511621007806
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.14686536431077898
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.14691561700082292
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.14688757562544197
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.14679691740510992
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.146694648924262
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.14672228889575797
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.14669096275017812
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.1465649498890643
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.14648381316366085
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.14652666537367345
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.1464181517279058
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.14643465727567673
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.14640423389417784
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.14641847271709407
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.14648926094063183
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.14639275241296973
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.14644024101672348
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.14642074605195723
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.14643783352392561
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.14661971100302407
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.14651213328007365
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.14641208337111908
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.1466417432749185
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.14661261915407456
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.14656420279857066
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.1465680007705979
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.14664881785533257
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.1465769170707231
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.14651353644035386
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.14657366905005997
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.14657840193052527
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.1466889463234366
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.1467574748594861
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.14671389916944172
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.14670582713248828
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.14660161782610376
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.14669708442585222
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.146666228028712
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.14662852993056383
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.14672489487168733
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.14682935530535218
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.1468149488002567
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.1468946227703143
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.14693871920658683
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.14683041914097414
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.146742099229508
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.14670940396686394
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.14683566598896172
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.14680831738753825
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.14680627600686386
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.1468082530374982
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.1468120797980027
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.14680111108749522
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.14680851940126294
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.14679132114079865
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.14678243885244752
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.1466914363205433
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.14670943440060907
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.1466840310022235
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.1467159829629115
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.14669954821847048
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.14669430906337405
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.14681077996103825
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.1467917758724667
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.1467435619319385
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.1467271066711614
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.14672116877045482
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.14668461283512205
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.14666829334560388
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.1467362837664114
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.14664996024451138
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.1467158331320836
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.14661762484012206
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.1465055656588041
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.14654654907289802
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.1465001207943383
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.14655387299981984
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.14661322131167726
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.14652549282434474
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.1464133560210019
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.14644596426786777
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.14655492786151258
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.1465713953421939
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.14647711842190972
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.14641481953171584
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.14628661617328986
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.1462822686004288
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.14619253178146227
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.14637656335593663
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.14636636566663971
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.14639162592763125
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.14637142514836962
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.14635803821803517
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.14635053251249988
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.14632317702146783
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.14626513122658333
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.146192080697843
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.1462006500770903
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.14626810341988775
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.14633346994858626
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.14637278144558272
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.14640438756052876
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.1463658499709341
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.1464484141779547
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.1465443193537896
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.14654114383757944
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.1464624884020951
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.1463964221550157
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.1464345102810728
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.14639655161987652
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.14633939488903508
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.14637599350654915
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.1463321042679698
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.14637384541353024
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.14646051338185434
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.14643945324873214
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.14636497936538748
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.14636957926570243
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.1463473863819594
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.1463225651282727
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.14637867456929568
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.14639573653539023
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.1464994130854277
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.1464422710378227
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.14648956438851735
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.1464073221570269
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.14634524499507326
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.14631899653535502
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.14624422696950548
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.14637884561838121
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.1463513556518592
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.14628420187668367
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.14626970950880816
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.14623898978150168
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.14625313434481008
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.1463035220007357
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.14623832177275267
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.14619580080823216
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.1461909233345365
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.14633707795055162
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.14636749984119748
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.14637964962026742
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.14633552634129018
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.146254645674595
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.14631134836967266
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.14624155114841342
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.14619868310168385
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.14620889855218944
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.14614803492281567
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.14606668003291706
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.14604381722683954
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.14606191799228574
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.1460098417044567
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.14599052223511938
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.1461390241359671
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.14613154120652191
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.14615864959068414
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.14614449359857254
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.14619045554506546
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.1461808749860193
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.1462695699563061
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.1463188002806112
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.14633896733777454
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.146309978759689
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.14628081627938735
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.14640406049734653
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.14636783640654313
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.1464985351204306
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.1465546410195353
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.14648043399513755
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.14645999383603064
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.14646309047937392
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.14640457949168245
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.14640005051531335
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.14636496750411587
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.1463105351358027
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.14626185758862384
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.14629041536639847
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.14625461351264407
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.14621895589536663
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.14624762940241995
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.14618635814765404
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.14628182501967893
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.14635343368866352
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.14633024936397326
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.1462470369248021
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.14620152442631396
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.14621137013308316
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.14617586562334142
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.14615772030490784
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.1461436809753781
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.14615203954195707
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.14617181707992147
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.14614866092234383
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.14625004394578614
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.14624331315734604
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.14618172241581812
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.14620816102974166
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.14623843407077072
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.1461907804308348
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.1461678475371279
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.14612586175674921
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.1461941319958944
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.14616047551843217
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.1461507757255344
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.1461724922190824
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.14626791909660983
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.14622297212319882
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.14617004482583565
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.14607218445377268
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.14607992899957403
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.14610774757400635
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.14608116345599997
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.14607513525246044
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.1461548649220385
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.14622337588750478
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.14628290245507625
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.14628432580366763
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.14639518848793992
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.14639518848793992 valid 0.20388421416282654
LOSS train 0.14639518848793992 valid 0.1768551543354988
LOSS train 0.14639518848793992 valid 0.17661453783512115
LOSS train 0.14639518848793992 valid 0.16690735146403313
LOSS train 0.14639518848793992 valid 0.16052830517292022
LOSS train 0.14639518848793992 valid 0.1694823702176412
LOSS train 0.14639518848793992 valid 0.1760178655385971
LOSS train 0.14639518848793992 valid 0.17601737938821316
LOSS train 0.14639518848793992 valid 0.17601352433363596
LOSS train 0.14639518848793992 valid 0.17714920789003372
LOSS train 0.14639518848793992 valid 0.17454350671984933
LOSS train 0.14639518848793992 valid 0.17477362478772798
LOSS train 0.14639518848793992 valid 0.17398689687252045
LOSS train 0.14639518848793992 valid 0.17320836335420609
LOSS train 0.14639518848793992 valid 0.17163471281528472
LOSS train 0.14639518848793992 valid 0.17269548308104277
LOSS train 0.14639518848793992 valid 0.17367986633497126
LOSS train 0.14639518848793992 valid 0.1727150504787763
LOSS train 0.14639518848793992 valid 0.1757960507744237
LOSS train 0.14639518848793992 valid 0.1757824033498764
LOSS train 0.14639518848793992 valid 0.17381081339858828
LOSS train 0.14639518848793992 valid 0.1728421375155449
LOSS train 0.14639518848793992 valid 0.17205559365127399
LOSS train 0.14639518848793992 valid 0.17210575876136622
LOSS train 0.14639518848793992 valid 0.1710164874792099
LOSS train 0.14639518848793992 valid 0.17158182251911896
LOSS train 0.14639518848793992 valid 0.17152862857889245
LOSS train 0.14639518848793992 valid 0.17116500011512212
LOSS train 0.14639518848793992 valid 0.17152975294096717
LOSS train 0.14639518848793992 valid 0.17246206502119701
LOSS train 0.14639518848793992 valid 0.17330099209662406
LOSS train 0.14639518848793992 valid 0.1729030883871019
LOSS train 0.14639518848793992 valid 0.17288099139025717
LOSS train 0.14639518848793992 valid 0.17318525077665553
LOSS train 0.14639518848793992 valid 0.17501069520201
LOSS train 0.14639518848793992 valid 0.17465385960208046
LOSS train 0.14639518848793992 valid 0.1750735197518323
LOSS train 0.14639518848793992 valid 0.17589520819877325
LOSS train 0.14639518848793992 valid 0.17487835310972655
LOSS train 0.14639518848793992 valid 0.1747159030288458
LOSS train 0.14639518848793992 valid 0.1750831069742761
LOSS train 0.14639518848793992 valid 0.1754557185229801
LOSS train 0.14639518848793992 valid 0.175668922967689
LOSS train 0.14639518848793992 valid 0.1765736426142129
LOSS train 0.14639518848793992 valid 0.17642327712641823
LOSS train 0.14639518848793992 valid 0.17711909277283627
LOSS train 0.14639518848793992 valid 0.17746721873892116
LOSS train 0.14639518848793992 valid 0.17717741740246615
LOSS train 0.14639518848793992 valid 0.17773238949629724
LOSS train 0.14639518848793992 valid 0.1772046220302582
LOSS train 0.14639518848793992 valid 0.17757872009978576
LOSS train 0.14639518848793992 valid 0.1773490163569267
LOSS train 0.14639518848793992 valid 0.17785036479527094
LOSS train 0.14639518848793992 valid 0.1775958157247967
LOSS train 0.14639518848793992 valid 0.17769022258845243
LOSS train 0.14639518848793992 valid 0.1774631714714425
LOSS train 0.14639518848793992 valid 0.17766557712303965
LOSS train 0.14639518848793992 valid 0.1773835184245274
LOSS train 0.14639518848793992 valid 0.17784480548511117
LOSS train 0.14639518848793992 valid 0.17760676071047782
LOSS train 0.14639518848793992 valid 0.17778167299559858
LOSS train 0.14639518848793992 valid 0.1783797284287791
LOSS train 0.14639518848793992 valid 0.1781424160987612
LOSS train 0.14639518848793992 valid 0.1787619087845087
LOSS train 0.14639518848793992 valid 0.17920356667958773
LOSS train 0.14639518848793992 valid 0.17916362890691467
LOSS train 0.14639518848793992 valid 0.1789559208190263
LOSS train 0.14639518848793992 valid 0.17886100336909294
LOSS train 0.14639518848793992 valid 0.17823142353175342
LOSS train 0.14639518848793992 valid 0.17867275859628404
LOSS train 0.14639518848793992 valid 0.17846600186656897
LOSS train 0.14639518848793992 valid 0.1788260576625665
LOSS train 0.14639518848793992 valid 0.17904798625266716
LOSS train 0.14639518848793992 valid 0.17878480618064468
LOSS train 0.14639518848793992 valid 0.1787400201956431
LOSS train 0.14639518848793992 valid 0.17920773162653572
LOSS train 0.14639518848793992 valid 0.17908659229030857
LOSS train 0.14639518848793992 valid 0.17902848774041885
LOSS train 0.14639518848793992 valid 0.17870261605027354
LOSS train 0.14639518848793992 valid 0.17813332807272672
LOSS train 0.14639518848793992 valid 0.17762404992992495
LOSS train 0.14639518848793992 valid 0.1778163014025223
LOSS train 0.14639518848793992 valid 0.17749472303562855
LOSS train 0.14639518848793992 valid 0.17729400612768673
LOSS train 0.14639518848793992 valid 0.17677367802928476
LOSS train 0.14639518848793992 valid 0.17636859278346217
LOSS train 0.14639518848793992 valid 0.17632382816966924
LOSS train 0.14639518848793992 valid 0.1760929858481342
LOSS train 0.14639518848793992 valid 0.1761986447519131
LOSS train 0.14639518848793992 valid 0.17639419933160147
LOSS train 0.14639518848793992 valid 0.17682135039633448
LOSS train 0.14639518848793992 valid 0.17678606996069784
LOSS train 0.14639518848793992 valid 0.17665264779521572
LOSS train 0.14639518848793992 valid 0.17689122925413417
LOSS train 0.14639518848793992 valid 0.1763458838588313
LOSS train 0.14639518848793992 valid 0.17622160911560059
LOSS train 0.14639518848793992 valid 0.17629851923160947
LOSS train 0.14639518848793992 valid 0.17641250317802235
LOSS train 0.14639518848793992 valid 0.17673311495419705
LOSS train 0.14639518848793992 valid 0.17695645198225976
LOSS train 0.14639518848793992 valid 0.17715049557166523
LOSS train 0.14639518848793992 valid 0.17742538364494548
LOSS train 0.14639518848793992 valid 0.1773114395373076
LOSS train 0.14639518848793992 valid 0.17740375433976835
LOSS train 0.14639518848793992 valid 0.17743034249260312
LOSS train 0.14639518848793992 valid 0.17792659435632094
LOSS train 0.14639518848793992 valid 0.17780738177700578
LOSS train 0.14639518848793992 valid 0.1781001671872757
LOSS train 0.14639518848793992 valid 0.17849270671332648
LOSS train 0.14639518848793992 valid 0.17869077934460206
LOSS train 0.14639518848793992 valid 0.1786119274727933
LOSS train 0.14639518848793992 valid 0.17824435194156
LOSS train 0.14639518848793992 valid 0.1783259170772755
LOSS train 0.14639518848793992 valid 0.17850649239201294
LOSS train 0.14639518848793992 valid 0.17849382175051648
LOSS train 0.14639518848793992 valid 0.17899332799274345
LOSS train 0.14639518848793992 valid 0.17904413166718605
LOSS train 0.14639518848793992 valid 0.17873142950110515
LOSS train 0.14639518848793992 valid 0.1784839755346795
LOSS train 0.14639518848793992 valid 0.178214705735445
LOSS train 0.14639518848793992 valid 0.1778897127090407
LOSS train 0.14639518848793992 valid 0.17777215848203565
LOSS train 0.14639518848793992 valid 0.17794102079015436
LOSS train 0.14639518848793992 valid 0.17814101242730696
LOSS train 0.14639518848793992 valid 0.17813180255889893
LOSS train 0.14639518848793992 valid 0.17836381447693658
LOSS train 0.14639518848793992 valid 0.17825739578468594
LOSS train 0.14639518848793992 valid 0.1782498536631465
LOSS train 0.14639518848793992 valid 0.1783763507539912
LOSS train 0.14639518848793992 valid 0.17831854224205018
LOSS train 0.14639518848793992 valid 0.17806211762300883
LOSS train 0.14639518848793992 valid 0.17782080354112567
LOSS train 0.14639518848793992 valid 0.17751350214606837
LOSS train 0.14639518848793992 valid 0.17760619943711295
LOSS train 0.14639518848793992 valid 0.17763784439475447
LOSS train 0.14639518848793992 valid 0.17764219532118125
LOSS train 0.14639518848793992 valid 0.1774713537336266
LOSS train 0.14639518848793992 valid 0.1774358087475749
LOSS train 0.14639518848793992 valid 0.17724913969743167
LOSS train 0.14639518848793992 valid 0.17736584373882838
LOSS train 0.14639518848793992 valid 0.17729273840045254
LOSS train 0.14639518848793992 valid 0.17729238525662624
LOSS train 0.14639518848793992 valid 0.17723150082401462
LOSS train 0.14639518848793992 valid 0.17723479442712334
LOSS train 0.14639518848793992 valid 0.17706988885484892
LOSS train 0.14639518848793992 valid 0.17736753607041214
LOSS train 0.14639518848793992 valid 0.17711015704537736
LOSS train 0.14639518848793992 valid 0.17794756329542882
LOSS train 0.14639518848793992 valid 0.17793670486683813
LOSS train 0.14639518848793992 valid 0.17786478191614152
LOSS train 0.14639518848793992 valid 0.1780194178322293
LOSS train 0.14639518848793992 valid 0.17789711724770696
LOSS train 0.14639518848793992 valid 0.1778715665823494
LOSS train 0.14639518848793992 valid 0.17795592052983
LOSS train 0.14639518848793992 valid 0.17793895035020768
LOSS train 0.14639518848793992 valid 0.17816504425345323
LOSS train 0.14639518848793992 valid 0.17826942188344944
LOSS train 0.14639518848793992 valid 0.17830504891993124
LOSS train 0.14639518848793992 valid 0.17830725370338127
LOSS train 0.14639518848793992 valid 0.1781347039155662
LOSS train 0.14639518848793992 valid 0.1780183476320705
LOSS train 0.14639518848793992 valid 0.17789543475265857
LOSS train 0.14639518848793992 valid 0.1777079315638981
LOSS train 0.14639518848793992 valid 0.17753539815908526
LOSS train 0.14639518848793992 valid 0.17741614173759113
LOSS train 0.14639518848793992 valid 0.17750602089856043
LOSS train 0.14639518848793992 valid 0.17777364179045854
LOSS train 0.14639518848793992 valid 0.17777644674338045
LOSS train 0.14639518848793992 valid 0.17796605243485356
LOSS train 0.14639518848793992 valid 0.1779998966876198
LOSS train 0.14639518848793992 valid 0.17794887442686405
LOSS train 0.14639518848793992 valid 0.17775626965733463
LOSS train 0.14639518848793992 valid 0.17778850919593966
LOSS train 0.14639518848793992 valid 0.1777095740725254
LOSS train 0.14639518848793992 valid 0.17758024505206516
LOSS train 0.14639518848793992 valid 0.1775074049153111
LOSS train 0.14639518848793992 valid 0.17760952440022076
LOSS train 0.14639518848793992 valid 0.17784156884705082
LOSS train 0.14639518848793992 valid 0.17774034786823742
LOSS train 0.14639518848793992 valid 0.177731701069408
LOSS train 0.14639518848793992 valid 0.1778720180961967
LOSS train 0.14639518848793992 valid 0.17780786392452952
LOSS train 0.14639518848793992 valid 0.17777752086438767
LOSS train 0.14639518848793992 valid 0.17764651686277078
LOSS train 0.14639518848793992 valid 0.17758901554184992
LOSS train 0.14639518848793992 valid 0.17758940528797848
LOSS train 0.14639518848793992 valid 0.17743268274368448
LOSS train 0.14639518848793992 valid 0.17742517860011853
LOSS train 0.14639518848793992 valid 0.17738528376218504
LOSS train 0.14639518848793992 valid 0.17739489674568176
LOSS train 0.14639518848793992 valid 0.1773448647316838
LOSS train 0.14639518848793992 valid 0.177247867686674
LOSS train 0.14639518848793992 valid 0.17713200382000424
LOSS train 0.14639518848793992 valid 0.17696364922929056
LOSS train 0.14639518848793992 valid 0.17671700845926236
LOSS train 0.14639518848793992 valid 0.176770203119638
LOSS train 0.14639518848793992 valid 0.17688535653092535
LOSS train 0.14639518848793992 valid 0.17679913530145028
LOSS train 0.14639518848793992 valid 0.17687407577756661
LOSS train 0.14639518848793992 valid 0.1767100875079632
LOSS train 0.14639518848793992 valid 0.17659567125994174
LOSS train 0.14639518848793992 valid 0.1765947943866843
LOSS train 0.14639518848793992 valid 0.17667592247131422
LOSS train 0.14639518848793992 valid 0.176840395757965
LOSS train 0.14639518848793992 valid 0.17668246866726295
LOSS train 0.14639518848793992 valid 0.17678977946922617
LOSS train 0.14639518848793992 valid 0.1767246972967461
LOSS train 0.14639518848793992 valid 0.17674007861373517
LOSS train 0.14639518848793992 valid 0.17656062393667596
LOSS train 0.14639518848793992 valid 0.17650636902877262
LOSS train 0.14639518848793992 valid 0.17640471769170174
LOSS train 0.14639518848793992 valid 0.17622214991529034
LOSS train 0.14639518848793992 valid 0.17623887067669433
LOSS train 0.14639518848793992 valid 0.17618843105351814
LOSS train 0.14639518848793992 valid 0.1760783524014229
LOSS train 0.14639518848793992 valid 0.17588193066142224
LOSS train 0.14639518848793992 valid 0.17580134569225223
LOSS train 0.14639518848793992 valid 0.1757123210561385
LOSS train 0.14639518848793992 valid 0.17581390109780717
LOSS train 0.14639518848793992 valid 0.17577968591993506
LOSS train 0.14639518848793992 valid 0.1757405055054712
LOSS train 0.14639518848793992 valid 0.175757531714332
LOSS train 0.14639518848793992 valid 0.17585839424699945
LOSS train 0.14639518848793992 valid 0.17598535140444124
LOSS train 0.14639518848793992 valid 0.17618452257580228
LOSS train 0.14639518848793992 valid 0.176291046416865
LOSS train 0.14639518848793992 valid 0.17648110121882435
LOSS train 0.14639518848793992 valid 0.1765302523578468
LOSS train 0.14639518848793992 valid 0.17650109034958886
LOSS train 0.14639518848793992 valid 0.17661107493483502
LOSS train 0.14639518848793992 valid 0.1767671022451285
LOSS train 0.14639518848793992 valid 0.17679512391573396
LOSS train 0.14639518848793992 valid 0.1767657005224105
LOSS train 0.14639518848793992 valid 0.17687216815021303
LOSS train 0.14639518848793992 valid 0.17690627245192833
LOSS train 0.14639518848793992 valid 0.17682256120241294
LOSS train 0.14639518848793992 valid 0.1767600916860476
LOSS train 0.14639518848793992 valid 0.17676894704834753
LOSS train 0.14639518848793992 valid 0.17656334037561297
LOSS train 0.14639518848793992 valid 0.17653153340021768
LOSS train 0.14639518848793992 valid 0.17667551462333728
LOSS train 0.14639518848793992 valid 0.17652596539455997
LOSS train 0.14639518848793992 valid 0.17671998114988147
LOSS train 0.14639518848793992 valid 0.17688660751112173
LOSS train 0.14639518848793992 valid 0.17686874136632802
LOSS train 0.14639518848793992 valid 0.1767611335811576
LOSS train 0.14639518848793992 valid 0.176863315858339
LOSS train 0.14639518848793992 valid 0.17682477855874645
LOSS train 0.14639518848793992 valid 0.17677750668851247
LOSS train 0.14639518848793992 valid 0.17675768250226975
LOSS train 0.14639518848793992 valid 0.1766354206548744
LOSS train 0.14639518848793992 valid 0.17672669822497974
LOSS train 0.14639518848793992 valid 0.17662723574478165
LOSS train 0.14639518848793992 valid 0.17656300070248251
LOSS train 0.14639518848793992 valid 0.17662106343344147
LOSS train 0.14639518848793992 valid 0.17663245450239629
LOSS train 0.14639518848793992 valid 0.17647739056251394
LOSS train 0.14639518848793992 valid 0.1766180057627286
LOSS train 0.14639518848793992 valid 0.17661192972917814
LOSS train 0.14639518848793992 valid 0.17656843599218588
LOSS train 0.14639518848793992 valid 0.1767018316920233
LOSS train 0.14639518848793992 valid 0.17676831884238556
LOSS train 0.14639518848793992 valid 0.17669602016079108
LOSS train 0.14639518848793992 valid 0.17671056509469496
LOSS train 0.14639518848793992 valid 0.1766943944512673
LOSS train 0.14639518848793992 valid 0.17665229687341175
LOSS train 0.14639518848793992 valid 0.17674085218808178
LOSS train 0.14639518848793992 valid 0.17682540805926963
LOSS train 0.14639518848793992 valid 0.17694490555272227
LOSS train 0.14639518848793992 valid 0.17690514441993502
LOSS train 0.14639518848793992 valid 0.17702427077557328
LOSS train 0.14639518848793992 valid 0.17729444308754275
LOSS train 0.14639518848793992 valid 0.17744011112621852
LOSS train 0.14639518848793992 valid 0.17753123796551767
LOSS train 0.14639518848793992 valid 0.1774756183949384
LOSS train 0.14639518848793992 valid 0.1775016282023727
LOSS train 0.14639518848793992 valid 0.17741582862736946
LOSS train 0.14639518848793992 valid 0.17729927385024888
LOSS train 0.14639518848793992 valid 0.1772778361074386
LOSS train 0.14639518848793992 valid 0.17724716290831566
LOSS train 0.14639518848793992 valid 0.17720635023193426
LOSS train 0.14639518848793992 valid 0.17694404763532867
LOSS train 0.14639518848793992 valid 0.17703195058414878
LOSS train 0.14639518848793992 valid 0.17713366044868886
LOSS train 0.14639518848793992 valid 0.17711358880787564
LOSS train 0.14639518848793992 valid 0.17707090048523216
LOSS train 0.14639518848793992 valid 0.17701145248546002
LOSS train 0.14639518848793992 valid 0.17698849965300825
LOSS train 0.14639518848793992 valid 0.1769874711040807
LOSS train 0.14639518848793992 valid 0.17694057072031086
LOSS train 0.14639518848793992 valid 0.17677895064206467
LOSS train 0.14639518848793992 valid 0.17674323671484646
LOSS train 0.14639518848793992 valid 0.176730731699247
LOSS train 0.14639518848793992 valid 0.17677169734117937
LOSS train 0.14639518848793992 valid 0.17684815192626693
LOSS train 0.14639518848793992 valid 0.17676340859081294
LOSS train 0.14639518848793992 valid 0.17681249394159926
LOSS train 0.14639518848793992 valid 0.1767919082569596
LOSS train 0.14639518848793992 valid 0.17680484664480026
LOSS train 0.14639518848793992 valid 0.17687603031595547
LOSS train 0.14639518848793992 valid 0.17686033798412626
LOSS train 0.14639518848793992 valid 0.17681378890939106
LOSS train 0.14639518848793992 valid 0.1769025697566495
LOSS train 0.14639518848793992 valid 0.17690890708840207
LOSS train 0.14639518848793992 valid 0.17687794693180772
LOSS train 0.14639518848793992 valid 0.17691289000456628
LOSS train 0.14639518848793992 valid 0.17689082934141936
LOSS train 0.14639518848793992 valid 0.17680978523446367
LOSS train 0.14639518848793992 valid 0.17691907320400657
LOSS train 0.14639518848793992 valid 0.17686871376729782
LOSS train 0.14639518848793992 valid 0.1767913930573264
LOSS train 0.14639518848793992 valid 0.17678646671657378
LOSS train 0.14639518848793992 valid 0.17688354454672756
LOSS train 0.14639518848793992 valid 0.17696221182300786
LOSS train 0.14639518848793992 valid 0.17691403797694616
LOSS train 0.14639518848793992 valid 0.17689841735777975
LOSS train 0.14639518848793992 valid 0.17696646405883393
LOSS train 0.14639518848793992 valid 0.17700046427407354
LOSS train 0.14639518848793992 valid 0.1770774467051216
LOSS train 0.14639518848793992 valid 0.17697588098235428
LOSS train 0.14639518848793992 valid 0.17711836271382567
LOSS train 0.14639518848793992 valid 0.17718549243966986
LOSS train 0.14639518848793992 valid 0.17714392317706956
LOSS train 0.14639518848793992 valid 0.1772281709184617
LOSS train 0.14639518848793992 valid 0.17722104466878452
LOSS train 0.14639518848793992 valid 0.1774181937513176
LOSS train 0.14639518848793992 valid 0.17754038676209405
LOSS train 0.14639518848793992 valid 0.1774487451354905
LOSS train 0.14639518848793992 valid 0.17756957626451475
LOSS train 0.14639518848793992 valid 0.17755624349370147
LOSS train 0.14639518848793992 valid 0.17746047342471846
LOSS train 0.14639518848793992 valid 0.17737704578293376
LOSS train 0.14639518848793992 valid 0.17740423981849854
LOSS train 0.14639518848793992 valid 0.17751318350166617
LOSS train 0.14639518848793992 valid 0.1775186030722376
LOSS train 0.14639518848793992 valid 0.17762204929299297
LOSS train 0.14639518848793992 valid 0.17763899761951288
LOSS train 0.14639518848793992 valid 0.17761825241104384
LOSS train 0.14639518848793992 valid 0.17770437179765167
LOSS train 0.14639518848793992 valid 0.177720349501161
LOSS train 0.14639518848793992 valid 0.1775873902081744
LOSS train 0.14639518848793992 valid 0.17758052695912924
LOSS train 0.14639518848793992 valid 0.17765891777878254
LOSS train 0.14639518848793992 valid 0.17785639857310195
LOSS train 0.14639518848793992 valid 0.17790772474330405
LOSS train 0.14639518848793992 valid 0.17789572899396708
LOSS train 0.14639518848793992 valid 0.1777688737217906
LOSS train 0.14639518848793992 valid 0.1777299672108272
LOSS train 0.14639518848793992 valid 0.17776752858927053
LOSS train 0.14639518848793992 valid 0.17769220352172851
LOSS train 0.14639518848793992 valid 0.17760256017714823
LOSS train 0.14639518848793992 valid 0.17758373488587412
LOSS train 0.14639518848793992 valid 0.17761514384415603
LOSS train 0.14639518848793992 valid 0.1777031271386955
LOSS train 0.14639518848793992 valid 0.17779339745011127
LOSS train 0.14639518848793992 valid 0.1778436658924885
LOSS train 0.14639518848793992 valid 0.17792188490973132
LOSS train 0.14639518848793992 valid 0.17780493753415913
LOSS train 0.14639518848793992 valid 0.17782067052999248
LOSS train 0.14639518848793992 valid 0.1777917116466496
LOSS train 0.14639518848793992 valid 0.1777236645736853
LOSS train 0.14639518848793992 valid 0.17781625922707564
LOSS train 0.14639518848793992 valid 0.17777197662299657
LOSS train 0.14639518848793992 valid 0.17776211289750352
LOSS train 0.14639518848793992 valid 0.17777443458772685
LOSS train 0.14639518848793992 valid 0.17776616178249402
LOSS train 0.14639518848793992 valid 0.17769855313632404
LOSS train 0.14639518848793992 valid 0.1776950592984972
LOSS train 0.14639518848793992 valid 0.17776867961334342
bichrom_seq(
  (conv1d): Conv1d(4, 256, kernel_size=(24,), stride=(1,))
  (relu): ReLU()
  (batchNorm1d): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (maxPool1d): MaxPool1d(kernel_size=15, stride=15, padding=0, dilation=1, ceil_mode=True)
  (lstm): LSTM(256, 32, batch_first=True)
  (tanh): Tanh()
  (model_dense_repeat): Sequential(
    (0): Linear(in_features=32, out_features=512, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.5, inplace=False)
    (3): Linear(in_features=512, out_features=512, bias=True)
    (4): ReLU()
    (5): Dropout(p=0.5, inplace=False)
    (6): Linear(in_features=512, out_features=512, bias=True)
    (7): ReLU()
    (8): Dropout(p=0.5, inplace=False)
  )
  (linear): Linear(in_features=512, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)
bimodal_network(
  (base_model): bichrom_seq(
    (conv1d): Conv1d(4, 256, kernel_size=(24,), stride=(1,))
    (relu): ReLU()
    (batchNorm1d): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (maxPool1d): MaxPool1d(kernel_size=15, stride=15, padding=0, dilation=1, ceil_mode=True)
    (lstm): LSTM(256, 32, batch_first=True)
    (tanh): Tanh()
    (model_dense_repeat): Sequential(
      (0): Linear(in_features=32, out_features=512, bias=True)
      (1): ReLU()
      (2): Dropout(p=0.5, inplace=False)
      (3): Linear(in_features=512, out_features=512, bias=True)
      (4): ReLU()
      (5): Dropout(p=0.5, inplace=False)
      (6): Linear(in_features=512, out_features=512, bias=True)
      (7): ReLU()
      (8): Dropout(p=0.5, inplace=False)
    )
    (linear): Linear(in_features=512, out_features=1, bias=True)
    (sigmoid): Sigmoid()
  )
  (linear): Linear(in_features=512, out_features=1, bias=True)
  (tanh): Tanh()
  (model): bichrom_chrom(
    (_reshape): _reshape()
    (conv1d): Conv1d(12, 15, kernel_size=(1,), stride=(1,), padding=valid)
    (relu): ReLU()
    (lstm): LSTM(15, 5, batch_first=True)
    (relu2): ReLU()
    (linear): Linear(in_features=5, out_features=1, bias=True)
    (tanh): Tanh()
  )
  (linear2): Linear(in_features=2, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)
0.44518272425249167
0.6744186046511628
