Training seq
DEVICE = cpu
####################
Total Parameters = 605185
Total Trainable Parameters = 605185
bichrom_seq(
  (conv1d): Conv1d(4, 256, kernel_size=(24,), stride=(1,))
  (relu): ReLU()
  (batchNorm1d): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (maxPool1d): MaxPool1d(kernel_size=15, stride=15, padding=0, dilation=1, ceil_mode=True)
  (lstm): LSTM(256, 32, batch_first=True)
  (tanh): Tanh()
  (model_dense_repeat): Sequential(
    (0): Linear(in_features=32, out_features=512, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.5, inplace=False)
    (3): Linear(in_features=512, out_features=512, bias=True)
    (4): ReLU()
    (5): Dropout(p=0.5, inplace=False)
    (6): Linear(in_features=512, out_features=512, bias=True)
    (7): ReLU()
    (8): Dropout(p=0.5, inplace=False)
  )
  (linear): Linear(in_features=512, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)
####################
EPOCH 1:
  batch 1 loss: 0.6947129964828491
  batch 2 loss: 0.6935129761695862
  batch 3 loss: 0.6945641040802002
  batch 4 loss: 0.6942316144704819
  batch 5 loss: 0.6943819761276245
  batch 6 loss: 0.6943146884441376
  batch 7 loss: 0.6943596686635699
  batch 8 loss: 0.6944925859570503
  batch 9 loss: 0.6944864392280579
  batch 10 loss: 0.6946814775466919
  batch 11 loss: 0.6944611506028608
  batch 12 loss: 0.6943951994180679
  batch 13 loss: 0.6943222421866196
  batch 14 loss: 0.6940150303500039
  batch 15 loss: 0.6941065152486166
  batch 16 loss: 0.6938247382640839
  batch 17 loss: 0.6935348650988411
  batch 18 loss: 0.6935659382078383
  batch 19 loss: 0.6933580982057672
  batch 20 loss: 0.6933367073535919
  batch 21 loss: 0.693255174727667
  batch 22 loss: 0.6931716122410514
  batch 23 loss: 0.6930495111838632
  batch 24 loss: 0.6928213462233543
  batch 25 loss: 0.6929187726974487
  batch 26 loss: 0.6926819590421823
  batch 27 loss: 0.6922274585123416
  batch 28 loss: 0.6916581030402865
  batch 29 loss: 0.6918553627770523
  batch 30 loss: 0.6917612691720326
  batch 31 loss: 0.69134122133255
  batch 32 loss: 0.6909312400966883
  batch 33 loss: 0.6903068007844867
  batch 34 loss: 0.6898229911046869
  batch 35 loss: 0.68967547586986
  batch 36 loss: 0.6889202776882384
  batch 37 loss: 0.6882159194430789
  batch 38 loss: 0.6875577474895277
  batch 39 loss: 0.6868295272191366
  batch 40 loss: 0.6856679871678353
  batch 41 loss: 0.6845722300250355
  batch 42 loss: 0.6843265734967732
  batch 43 loss: 0.6843413641286451
  batch 44 loss: 0.6836429346691478
  batch 45 loss: 0.6833511339293585
  batch 46 loss: 0.6828841243101202
  batch 47 loss: 0.6832800811909615
  batch 48 loss: 0.6821597032248974
  batch 49 loss: 0.6814291793472913
  batch 50 loss: 0.6807345199584961
  batch 51 loss: 0.6797137237062642
  batch 52 loss: 0.6790956900669978
  batch 53 loss: 0.6787281531207966
  batch 54 loss: 0.6776241041995861
  batch 55 loss: 0.6769411758943038
  batch 56 loss: 0.6759636913027082
  batch 57 loss: 0.6756512846863061
  batch 58 loss: 0.675527523303854
  batch 59 loss: 0.6744511410341425
  batch 60 loss: 0.6741908232371012
  batch 61 loss: 0.6735640852177729
  batch 62 loss: 0.673349110349532
  batch 63 loss: 0.6721944894109454
  batch 64 loss: 0.6715076509863138
  batch 65 loss: 0.6707155310190641
  batch 66 loss: 0.6701662558497805
  batch 67 loss: 0.6696888484171967
  batch 68 loss: 0.6691970553468255
  batch 69 loss: 0.6686980862548386
  batch 70 loss: 0.6680379441806248
  batch 71 loss: 0.6674316004968025
  batch 72 loss: 0.6668246115247408
  batch 73 loss: 0.6664919477619536
  batch 74 loss: 0.6656585980106045
  batch 75 loss: 0.6651185735066731
  batch 76 loss: 0.6648135498950356
  batch 77 loss: 0.6642025747856537
  batch 78 loss: 0.6633046789047046
  batch 79 loss: 0.6626512151730212
  batch 80 loss: 0.6618600390851498
  batch 81 loss: 0.6611680498829594
  batch 82 loss: 0.6610309910483476
  batch 83 loss: 0.6607192252055708
  batch 84 loss: 0.6599860652571633
  batch 85 loss: 0.6589355587959289
  batch 86 loss: 0.6587656958158626
  batch 87 loss: 0.6582488684818663
  batch 88 loss: 0.6573325605555014
  batch 89 loss: 0.6564732803387588
  batch 90 loss: 0.6558926595581902
  batch 91 loss: 0.6550594823701041
  batch 92 loss: 0.6538812207139056
  batch 93 loss: 0.6533111885029782
  batch 94 loss: 0.6533607878583543
  batch 95 loss: 0.6528446210058112
  batch 96 loss: 0.6522403309742609
  batch 97 loss: 0.6516325891632395
  batch 98 loss: 0.6511108145421866
  batch 99 loss: 0.6505609106535863
  batch 100 loss: 0.6499741810560227
  batch 101 loss: 0.6489648636024777
  batch 102 loss: 0.6481778645047954
  batch 103 loss: 0.6474971487684157
  batch 104 loss: 0.6465245909415759
  batch 105 loss: 0.6455399763016474
  batch 106 loss: 0.644850844482206
  batch 107 loss: 0.6437119184253371
  batch 108 loss: 0.6429840138664952
  batch 109 loss: 0.6419006400152084
  batch 110 loss: 0.6409521861509844
  batch 111 loss: 0.6404268209998673
  batch 112 loss: 0.6399064862302372
  batch 113 loss: 0.6393694925097237
  batch 114 loss: 0.6389044057904628
  batch 115 loss: 0.6383006500161212
  batch 116 loss: 0.6372609847578509
  batch 117 loss: 0.6366433114068121
  batch 118 loss: 0.636103228997376
  batch 119 loss: 0.6354037388032224
  batch 120 loss: 0.6344600195686022
  batch 121 loss: 0.6334436846173499
  batch 122 loss: 0.6327379917512175
  batch 123 loss: 0.6319199407972941
  batch 124 loss: 0.6312971840943059
  batch 125 loss: 0.6304117369651795
  batch 126 loss: 0.629381849652245
  batch 127 loss: 0.6287800979426527
  batch 128 loss: 0.6276481931563467
  batch 129 loss: 0.6266677991826405
  batch 130 loss: 0.625862135566198
  batch 131 loss: 0.6253197331919925
  batch 132 loss: 0.6243727078491991
  batch 133 loss: 0.623909243291482
  batch 134 loss: 0.6229466360006759
  batch 135 loss: 0.6222296008357295
  batch 136 loss: 0.621192971134887
  batch 137 loss: 0.6203641234523188
  batch 138 loss: 0.6196483257024185
  batch 139 loss: 0.6188061344537804
  batch 140 loss: 0.6178835700665202
  batch 141 loss: 0.6171028341384645
  batch 142 loss: 0.6160509042756658
  batch 143 loss: 0.6146795393286885
  batch 144 loss: 0.6137261827372842
  batch 145 loss: 0.6128931956044559
  batch 146 loss: 0.6118727250866693
  batch 147 loss: 0.6111628062465564
  batch 148 loss: 0.610282955338826
  batch 149 loss: 0.6092940700934237
  batch 150 loss: 0.608317627509435
  batch 151 loss: 0.607565773441302
  batch 152 loss: 0.6068109399393985
  batch 153 loss: 0.6060096578660354
  batch 154 loss: 0.605167175268198
  batch 155 loss: 0.6042017952088387
  batch 156 loss: 0.603402935159512
  batch 157 loss: 0.6028037606530888
  batch 158 loss: 0.6019175467611868
  batch 159 loss: 0.6009328436551604
  batch 160 loss: 0.6001015197485685
  batch 161 loss: 0.5994349871733173
  batch 162 loss: 0.5985138098030914
  batch 163 loss: 0.5975972189128034
  batch 164 loss: 0.5967724310552202
  batch 165 loss: 0.5958611923636812
  batch 166 loss: 0.5949066803756967
  batch 167 loss: 0.5938985802813205
  batch 168 loss: 0.5933074928110554
  batch 169 loss: 0.5922804843391892
  batch 170 loss: 0.5914283454418182
  batch 171 loss: 0.5906120036777697
  batch 172 loss: 0.5898763825033986
  batch 173 loss: 0.5889502557026858
  batch 174 loss: 0.5880980546447052
  batch 175 loss: 0.5874047606331961
  batch 176 loss: 0.586186649616469
  batch 177 loss: 0.5852111313639388
  batch 178 loss: 0.5844722227768951
  batch 179 loss: 0.583709540480342
  batch 180 loss: 0.582738468878799
  batch 181 loss: 0.5819385076425352
  batch 182 loss: 0.5810075207398488
  batch 183 loss: 0.5801504959499901
  batch 184 loss: 0.5791308539717094
  batch 185 loss: 0.5782781818428555
  batch 186 loss: 0.5776993887078378
  batch 187 loss: 0.576934335862889
  batch 188 loss: 0.5759321497158801
  batch 189 loss: 0.5749297632426812
  batch 190 loss: 0.573972708614249
  batch 191 loss: 0.5731494629570327
  batch 192 loss: 0.5724255898967385
  batch 193 loss: 0.5716976931058063
  batch 194 loss: 0.5711866048807951
  batch 195 loss: 0.5704959485775385
  batch 196 loss: 0.5694257709748891
  batch 197 loss: 0.5685370447974519
  batch 198 loss: 0.5679976962732546
  batch 199 loss: 0.5672906485933754
  batch 200 loss: 0.5665785406529903
  batch 201 loss: 0.5659547547499338
  batch 202 loss: 0.565260506797545
  batch 203 loss: 0.5644579774934083
  batch 204 loss: 0.5637051090598106
  batch 205 loss: 0.5630402935714256
  batch 206 loss: 0.5624679831914532
  batch 207 loss: 0.5617581364037334
  batch 208 loss: 0.5608997763349459
  batch 209 loss: 0.559990813572441
  batch 210 loss: 0.559299502770106
  batch 211 loss: 0.5584975529056024
  batch 212 loss: 0.5577244942761818
  batch 213 loss: 0.5571160711033244
  batch 214 loss: 0.556409343102268
  batch 215 loss: 0.5554680965667547
  batch 216 loss: 0.5546841985649533
  batch 217 loss: 0.554040469057549
  batch 218 loss: 0.5534244926150785
  batch 219 loss: 0.5526707507159612
  batch 220 loss: 0.5518800743601538
  batch 221 loss: 0.5512937049790205
  batch 222 loss: 0.5507863122332204
  batch 223 loss: 0.550134256415303
  batch 224 loss: 0.5495715430006385
  batch 225 loss: 0.5489298437701331
  batch 226 loss: 0.5483455183231725
  batch 227 loss: 0.5475715443426292
  batch 228 loss: 0.5468760786349314
  batch 229 loss: 0.5461518811346662
  batch 230 loss: 0.5455293457145276
  batch 231 loss: 0.5448768829112445
  batch 232 loss: 0.5440829021149668
  batch 233 loss: 0.5432994342668885
  batch 234 loss: 0.5429253567997206
  batch 235 loss: 0.5424473784071334
  batch 236 loss: 0.5417854782130759
  batch 237 loss: 0.5412111292408488
  batch 238 loss: 0.5405458344631836
  batch 239 loss: 0.5399732219374829
  batch 240 loss: 0.5394418007383744
  batch 241 loss: 0.5389510008556714
  batch 242 loss: 0.5382400736335881
  batch 243 loss: 0.5377993196126365
  batch 244 loss: 0.5372447629199654
  batch 245 loss: 0.5367055956198244
  batch 246 loss: 0.5360604788714308
  batch 247 loss: 0.5356618026007525
  batch 248 loss: 0.5352101536287416
  batch 249 loss: 0.5345157693667584
  batch 250 loss: 0.5339928300380706
  batch 251 loss: 0.5335669845223902
  batch 252 loss: 0.5329638516146039
  batch 253 loss: 0.5322786231521561
  batch 254 loss: 0.531658713625172
  batch 255 loss: 0.5312618444947635
  batch 256 loss: 0.5307055381126702
  batch 257 loss: 0.5301053032336995
  batch 258 loss: 0.5295376779959183
  batch 259 loss: 0.5290238081718504
  batch 260 loss: 0.5284877301408695
  batch 261 loss: 0.5278950907946547
  batch 262 loss: 0.5271858298824034
  batch 263 loss: 0.5266379385393382
  batch 264 loss: 0.5260437947105278
  batch 265 loss: 0.5256274736152505
  batch 266 loss: 0.5250524042692399
  batch 267 loss: 0.5247726293092363
  batch 268 loss: 0.5242573933592484
  batch 269 loss: 0.5238196760083663
  batch 270 loss: 0.5233999564691826
  batch 271 loss: 0.5229299478645254
  batch 272 loss: 0.522516936830738
  batch 273 loss: 0.5219425006663843
  batch 274 loss: 0.5215388287375443
  batch 275 loss: 0.5209403230927208
  batch 276 loss: 0.5204232380441998
  batch 277 loss: 0.5198320430108356
  batch 278 loss: 0.5191688541885761
  batch 279 loss: 0.5188196276892043
  batch 280 loss: 0.5182626102651868
  batch 281 loss: 0.5176637449510582
  batch 282 loss: 0.5172236808014254
  batch 283 loss: 0.516645740915103
  batch 284 loss: 0.5163108788325753
  batch 285 loss: 0.5157963474591573
  batch 286 loss: 0.5154537779467923
  batch 287 loss: 0.515043420451028
  batch 288 loss: 0.5145142419884602
  batch 289 loss: 0.5142267271102918
  batch 290 loss: 0.5136543605861993
  batch 291 loss: 0.5132632916121139
  batch 292 loss: 0.5128200386893259
  batch 293 loss: 0.5123119924865892
  batch 294 loss: 0.5117918522585005
  batch 295 loss: 0.5113825129250349
  batch 296 loss: 0.5109354118640358
  batch 297 loss: 0.5105434364140636
  batch 298 loss: 0.5101849662777561
  batch 299 loss: 0.5096423046844062
  batch 300 loss: 0.5092089173197746
  batch 301 loss: 0.5086574283153116
  batch 302 loss: 0.5082781984908691
  batch 303 loss: 0.5078728268838951
  batch 304 loss: 0.5074011386421166
  batch 305 loss: 0.5068314033453581
  batch 306 loss: 0.5065277295759301
  batch 307 loss: 0.5061188549871165
  batch 308 loss: 0.505838309692872
  batch 309 loss: 0.5054256158932128
  batch 310 loss: 0.5050071379830761
  batch 311 loss: 0.5046651455367109
  batch 312 loss: 0.5044128588185861
  batch 313 loss: 0.5041156214075728
  batch 314 loss: 0.5037020635642823
  batch 315 loss: 0.50321757878576
  batch 316 loss: 0.5028063334241698
  batch 317 loss: 0.5024344169002978
  batch 318 loss: 0.501963105685306
  batch 319 loss: 0.5014473076524406
  batch 320 loss: 0.501024191454053
  batch 321 loss: 0.5007284084955851
  batch 322 loss: 0.5003061375077467
  batch 323 loss: 0.5000219970664742
  batch 324 loss: 0.49951455273010115
  batch 325 loss: 0.4990560049277086
  batch 326 loss: 0.49869053312605877
  batch 327 loss: 0.49830690254129767
  batch 328 loss: 0.4977472493561303
  batch 329 loss: 0.49749344025701736
  batch 330 loss: 0.49710785114403927
  batch 331 loss: 0.49674239520580027
  batch 332 loss: 0.49636472025549555
  batch 333 loss: 0.495976154063199
  batch 334 loss: 0.49559756042714603
  batch 335 loss: 0.4950616919282657
  batch 336 loss: 0.4945836766135125
  batch 337 loss: 0.49411418131975465
  batch 338 loss: 0.4937981761242511
  batch 339 loss: 0.4935119566136757
  batch 340 loss: 0.49322000952327955
  batch 341 loss: 0.49276427719250454
  batch 342 loss: 0.49238499476198566
  batch 343 loss: 0.49203227482106177
  batch 344 loss: 0.49165595184231914
  batch 345 loss: 0.49142060418059863
  batch 346 loss: 0.4909867306665189
  batch 347 loss: 0.4906402866675462
  batch 348 loss: 0.4902989562044198
  batch 349 loss: 0.4898395334239676
  batch 350 loss: 0.48953988194465636
  batch 351 loss: 0.48915747470325893
  batch 352 loss: 0.48896513862366026
  batch 353 loss: 0.4886628805577924
  batch 354 loss: 0.48850140101828815
  batch 355 loss: 0.48818449176533124
  batch 356 loss: 0.48779775185531443
  batch 357 loss: 0.48738486943792564
  batch 358 loss: 0.4871141313674064
  batch 359 loss: 0.4867811706902921
  batch 360 loss: 0.4865204498171806
  batch 361 loss: 0.4861424316824968
  batch 362 loss: 0.48573885841593556
  batch 363 loss: 0.48536505260743384
  batch 364 loss: 0.4850346386268899
  batch 365 loss: 0.4848095247190293
  batch 366 loss: 0.4844365041764056
  batch 367 loss: 0.48403309216616264
  batch 368 loss: 0.4836498795320158
  batch 369 loss: 0.4833765588801728
  batch 370 loss: 0.48310807137875944
  batch 371 loss: 0.48285465669439165
  batch 372 loss: 0.4824966191444346
  batch 373 loss: 0.48209228728156306
  batch 374 loss: 0.48163628458658003
  batch 375 loss: 0.4812576649983724
  batch 376 loss: 0.48101890483435167
  batch 377 loss: 0.4807211498999153
  batch 378 loss: 0.4802816570908935
  batch 379 loss: 0.47996081006872937
  batch 380 loss: 0.4796807609106365
  batch 381 loss: 0.4792928981968737
  batch 382 loss: 0.4789160697560036
  batch 383 loss: 0.47856441661520976
  batch 384 loss: 0.4781991722217451
  batch 385 loss: 0.478054692451056
  batch 386 loss: 0.4776751025353071
  batch 387 loss: 0.4773594914172663
  batch 388 loss: 0.4770984597427329
  batch 389 loss: 0.47678236207496233
  batch 390 loss: 0.4764686837410316
  batch 391 loss: 0.4761604308472265
  batch 392 loss: 0.47585035320751523
  batch 393 loss: 0.47553651887951914
  batch 394 loss: 0.4753067446541665
  batch 395 loss: 0.4750674279430245
  batch 396 loss: 0.47480878504839813
  batch 397 loss: 0.47464394847151614
  batch 398 loss: 0.47433893831830526
  batch 399 loss: 0.4741188851663642
  batch 400 loss: 0.473863869830966
  batch 401 loss: 0.4735068967217519
  batch 402 loss: 0.47328878813122044
  batch 403 loss: 0.4730906839997833
  batch 404 loss: 0.4728649443033898
  batch 405 loss: 0.47264358828097214
  batch 406 loss: 0.47247068385772517
  batch 407 loss: 0.4721393465117096
  batch 408 loss: 0.4719668764109705
  batch 409 loss: 0.4716774174957229
  batch 410 loss: 0.471517433771273
  batch 411 loss: 0.47127433008811187
  batch 412 loss: 0.47093628397555026
  batch 413 loss: 0.4707646843307532
  batch 414 loss: 0.4704501534573698
  batch 415 loss: 0.47012948831879947
  batch 416 loss: 0.4699035214546781
  batch 417 loss: 0.469652416751825
  batch 418 loss: 0.4693311920576689
  batch 419 loss: 0.4690991373363713
  batch 420 loss: 0.4688418515381359
  batch 421 loss: 0.4685330523306287
  batch 422 loss: 0.468456354398298
  batch 423 loss: 0.46814624884168027
  batch 424 loss: 0.4678843985048105
  batch 425 loss: 0.46759105002178863
  batch 426 loss: 0.4673582754084762
  batch 427 loss: 0.4670954819864635
  batch 428 loss: 0.46675064359034335
  batch 429 loss: 0.4665109071976099
  batch 430 loss: 0.4662317800660466
  batch 431 loss: 0.46613395608218255
  batch 432 loss: 0.4659517064552616
  batch 433 loss: 0.46569274933729105
  batch 434 loss: 0.46555630526235026
  batch 435 loss: 0.46525528506300917
  batch 436 loss: 0.4650746394598156
  batch 437 loss: 0.46488634256530953
  batch 438 loss: 0.4647432124778016
  batch 439 loss: 0.46448252854151717
  batch 440 loss: 0.46431543921882457
  batch 441 loss: 0.46402468784055473
  batch 442 loss: 0.4637570229455896
  batch 443 loss: 0.46348953691077827
  batch 444 loss: 0.46318857553037435
  batch 445 loss: 0.46302515080805573
  batch 446 loss: 0.46274993899424516
  batch 447 loss: 0.4624373408772001
  batch 448 loss: 0.4622582056160484
  batch 449 loss: 0.462052524222563
  batch 450 loss: 0.4617824035220676
  batch 451 loss: 0.4615907003636371
  batch 452 loss: 0.46144221268132724
  batch 453 loss: 0.46127149344280066
  batch 454 loss: 0.4611243979127397
  batch 455 loss: 0.4608897145632859
  batch 456 loss: 0.46073164366054953
  batch 457 loss: 0.46049690240060737
  batch 458 loss: 0.46024441478294054
  batch 459 loss: 0.4601263346755167
  batch 460 loss: 0.460002633864465
  batch 461 loss: 0.4597868093828834
  batch 462 loss: 0.4595907453076664
  batch 463 loss: 0.45929791956946864
  batch 464 loss: 0.4590942369581297
  batch 465 loss: 0.45887070125149143
  batch 466 loss: 0.45852217289255415
  batch 467 loss: 0.4584236386246038
  batch 468 loss: 0.4582095497693771
  batch 469 loss: 0.45817028694569684
  batch 470 loss: 0.4579614670352733
  batch 471 loss: 0.45776562927381753
  batch 472 loss: 0.45745583794127076
LOSS train 0.45745583794127076 valid 0.2502254843711853
LOSS train 0.45745583794127076 valid 0.24164752662181854
LOSS train 0.45745583794127076 valid 0.24449883898099264
LOSS train 0.45745583794127076 valid 0.24555246159434319
LOSS train 0.45745583794127076 valid 0.24134514033794402
LOSS train 0.45745583794127076 valid 0.2446750377615293
LOSS train 0.45745583794127076 valid 0.25476575110639843
LOSS train 0.45745583794127076 valid 0.2546264510601759
LOSS train 0.45745583794127076 valid 0.2532542629374398
LOSS train 0.45745583794127076 valid 0.25254287868738173
LOSS train 0.45745583794127076 valid 0.2506115206263282
LOSS train 0.45745583794127076 valid 0.2517305401464303
LOSS train 0.45745583794127076 valid 0.25145285633894116
LOSS train 0.45745583794127076 valid 0.25106289982795715
LOSS train 0.45745583794127076 valid 0.24758796890576681
LOSS train 0.45745583794127076 valid 0.2482154704630375
LOSS train 0.45745583794127076 valid 0.24993831269881306
LOSS train 0.45745583794127076 valid 0.2509588367409176
LOSS train 0.45745583794127076 valid 0.25261284018817703
LOSS train 0.45745583794127076 valid 0.2519842557609081
LOSS train 0.45745583794127076 valid 0.2513888492470696
LOSS train 0.45745583794127076 valid 0.25004989857023413
LOSS train 0.45745583794127076 valid 0.249137788363125
LOSS train 0.45745583794127076 valid 0.24839862994849682
LOSS train 0.45745583794127076 valid 0.24711797654628753
LOSS train 0.45745583794127076 valid 0.2467514477097071
LOSS train 0.45745583794127076 valid 0.24657659453374367
LOSS train 0.45745583794127076 valid 0.24690283781715802
LOSS train 0.45745583794127076 valid 0.24714673130676665
LOSS train 0.45745583794127076 valid 0.24781880527734756
LOSS train 0.45745583794127076 valid 0.24814081720767484
LOSS train 0.45745583794127076 valid 0.24769590329378843
LOSS train 0.45745583794127076 valid 0.24829343683791882
LOSS train 0.45745583794127076 valid 0.24810853249886455
LOSS train 0.45745583794127076 valid 0.24932631935392108
LOSS train 0.45745583794127076 valid 0.24859080215295157
LOSS train 0.45745583794127076 valid 0.24859590425684647
LOSS train 0.45745583794127076 valid 0.24966542775693693
LOSS train 0.45745583794127076 valid 0.24916585171834016
LOSS train 0.45745583794127076 valid 0.24899360202252865
LOSS train 0.45745583794127076 valid 0.2496787022526671
LOSS train 0.45745583794127076 valid 0.24949261368740172
LOSS train 0.45745583794127076 valid 0.24915752681188805
LOSS train 0.45745583794127076 valid 0.24950021403757008
LOSS train 0.45745583794127076 valid 0.24921756154961056
LOSS train 0.45745583794127076 valid 0.24955337170673453
LOSS train 0.45745583794127076 valid 0.25018244696424363
LOSS train 0.45745583794127076 valid 0.25013868790119886
LOSS train 0.45745583794127076 valid 0.25051212827770075
LOSS train 0.45745583794127076 valid 0.24973218619823456
LOSS train 0.45745583794127076 valid 0.2500622745822458
LOSS train 0.45745583794127076 valid 0.24998293043329164
LOSS train 0.45745583794127076 valid 0.2500638317949367
LOSS train 0.45745583794127076 valid 0.2501610674791866
LOSS train 0.45745583794127076 valid 0.2500691546635194
LOSS train 0.45745583794127076 valid 0.249801725681339
LOSS train 0.45745583794127076 valid 0.2495386357370176
LOSS train 0.45745583794127076 valid 0.24941294640302658
LOSS train 0.45745583794127076 valid 0.24984338551254595
LOSS train 0.45745583794127076 valid 0.24991637741525968
LOSS train 0.45745583794127076 valid 0.24956359408917975
LOSS train 0.45745583794127076 valid 0.2502009260558313
LOSS train 0.45745583794127076 valid 0.25027621383704834
LOSS train 0.45745583794127076 valid 0.25084802764467895
LOSS train 0.45745583794127076 valid 0.25089836005981153
LOSS train 0.45745583794127076 valid 0.25081256483540393
LOSS train 0.45745583794127076 valid 0.2503332815953155
LOSS train 0.45745583794127076 valid 0.2502784829805879
LOSS train 0.45745583794127076 valid 0.24976388526999432
LOSS train 0.45745583794127076 valid 0.24973678312131337
LOSS train 0.45745583794127076 valid 0.2492885751203752
LOSS train 0.45745583794127076 valid 0.24938111276262337
LOSS train 0.45745583794127076 valid 0.2494396985801932
LOSS train 0.45745583794127076 valid 0.24915836348726944
LOSS train 0.45745583794127076 valid 0.24925431768099468
LOSS train 0.45745583794127076 valid 0.24956454297429637
LOSS train 0.45745583794127076 valid 0.249500889282722
LOSS train 0.45745583794127076 valid 0.24959145600979143
LOSS train 0.45745583794127076 valid 0.2495589759908145
LOSS train 0.45745583794127076 valid 0.24911819119006395
LOSS train 0.45745583794127076 valid 0.2484990135755068
LOSS train 0.45745583794127076 valid 0.24859538510805224
LOSS train 0.45745583794127076 valid 0.24848532174007001
LOSS train 0.45745583794127076 valid 0.24847215236652465
LOSS train 0.45745583794127076 valid 0.2481906820746029
LOSS train 0.45745583794127076 valid 0.24759445166171984
LOSS train 0.45745583794127076 valid 0.24776533845512347
LOSS train 0.45745583794127076 valid 0.24724805236540057
LOSS train 0.45745583794127076 valid 0.24755614577384477
LOSS train 0.45745583794127076 valid 0.24770223448673884
LOSS train 0.45745583794127076 valid 0.24774401348370773
LOSS train 0.45745583794127076 valid 0.2478857579762521
LOSS train 0.45745583794127076 valid 0.24780037470402255
LOSS train 0.45745583794127076 valid 0.24769240633604375
LOSS train 0.45745583794127076 valid 0.24759426038516194
LOSS train 0.45745583794127076 valid 0.2478063853147129
LOSS train 0.45745583794127076 valid 0.2479751830555729
LOSS train 0.45745583794127076 valid 0.24809664168528148
LOSS train 0.45745583794127076 valid 0.24811510380470392
LOSS train 0.45745583794127076 valid 0.24845266595482826
LOSS train 0.45745583794127076 valid 0.24860277491630894
LOSS train 0.45745583794127076 valid 0.24849757698236727
LOSS train 0.45745583794127076 valid 0.24836104572976678
LOSS train 0.45745583794127076 valid 0.24826568451065284
LOSS train 0.45745583794127076 valid 0.24831634021940685
LOSS train 0.45745583794127076 valid 0.24856376900987806
LOSS train 0.45745583794127076 valid 0.24836786369854044
LOSS train 0.45745583794127076 valid 0.2484608313275708
LOSS train 0.45745583794127076 valid 0.24877556406576698
LOSS train 0.45745583794127076 valid 0.24915900460698387
LOSS train 0.45745583794127076 valid 0.2488567619710355
LOSS train 0.45745583794127076 valid 0.2487107455464346
LOSS train 0.45745583794127076 valid 0.248766428733294
LOSS train 0.45745583794127076 valid 0.24869802711825623
LOSS train 0.45745583794127076 valid 0.2487261967814487
LOSS train 0.45745583794127076 valid 0.24886191854703016
LOSS train 0.45745583794127076 valid 0.24894051457572186
LOSS train 0.45745583794127076 valid 0.24899561539039775
LOSS train 0.45745583794127076 valid 0.24898044088808427
LOSS train 0.45745583794127076 valid 0.248795481522878
LOSS train 0.45745583794127076 valid 0.24854741928991209
LOSS train 0.45745583794127076 valid 0.24840251612858694
LOSS train 0.45745583794127076 valid 0.24828016515669785
LOSS train 0.45745583794127076 valid 0.24846912391724124
LOSS train 0.45745583794127076 valid 0.24842733597755431
LOSS train 0.45745583794127076 valid 0.24861603360327464
LOSS train 0.45745583794127076 valid 0.24860576041570798
LOSS train 0.45745583794127076 valid 0.24884693243075162
LOSS train 0.45745583794127076 valid 0.24891952384811963
LOSS train 0.45745583794127076 valid 0.24891078277276113
LOSS train 0.45745583794127076 valid 0.24885798785977692
LOSS train 0.45745583794127076 valid 0.24867257712916893
LOSS train 0.45745583794127076 valid 0.2484883230207558
LOSS train 0.45745583794127076 valid 0.24858192954935246
LOSS train 0.45745583794127076 valid 0.24859744895387578
LOSS train 0.45745583794127076 valid 0.24858997937510996
LOSS train 0.45745583794127076 valid 0.24846569247489428
LOSS train 0.45745583794127076 valid 0.2483631751459578
LOSS train 0.45745583794127076 valid 0.24822088282742946
LOSS train 0.45745583794127076 valid 0.24830147070544106
LOSS train 0.45745583794127076 valid 0.24834772077858025
LOSS train 0.45745583794127076 valid 0.2486194067857635
LOSS train 0.45745583794127076 valid 0.24860185926610773
LOSS train 0.45745583794127076 valid 0.24858168760935465
LOSS train 0.45745583794127076 valid 0.24840507610090848
LOSS train 0.45745583794127076 valid 0.2485435658121762
LOSS train 0.45745583794127076 valid 0.24831243477711062
LOSS train 0.45745583794127076 valid 0.24886983794135017
LOSS train 0.45745583794127076 valid 0.2490144163970179
LOSS train 0.45745583794127076 valid 0.2489384957154592
LOSS train 0.45745583794127076 valid 0.24901260879655548
LOSS train 0.45745583794127076 valid 0.24886959124552577
LOSS train 0.45745583794127076 valid 0.248893480090534
LOSS train 0.45745583794127076 valid 0.2488241044731883
LOSS train 0.45745583794127076 valid 0.24884384024527764
LOSS train 0.45745583794127076 valid 0.24901089855493644
LOSS train 0.45745583794127076 valid 0.2489808243551072
LOSS train 0.45745583794127076 valid 0.24907715467712546
LOSS train 0.45745583794127076 valid 0.24907997325531342
LOSS train 0.45745583794127076 valid 0.24910400081425904
LOSS train 0.45745583794127076 valid 0.2490593361558381
LOSS train 0.45745583794127076 valid 0.2490287514196502
LOSS train 0.45745583794127076 valid 0.2489254870480555
LOSS train 0.45745583794127076 valid 0.24873228698242
LOSS train 0.45745583794127076 valid 0.248682414040421
LOSS train 0.45745583794127076 valid 0.24869575575891747
LOSS train 0.45745583794127076 valid 0.24884077835225774
LOSS train 0.45745583794127076 valid 0.24873213532070318
LOSS train 0.45745583794127076 valid 0.24886365993135778
LOSS train 0.45745583794127076 valid 0.2489543947226861
LOSS train 0.45745583794127076 valid 0.24888502749783253
LOSS train 0.45745583794127076 valid 0.24880328090038412
LOSS train 0.45745583794127076 valid 0.24895630518480533
LOSS train 0.45745583794127076 valid 0.24897768742394175
LOSS train 0.45745583794127076 valid 0.24890561384814125
LOSS train 0.45745583794127076 valid 0.24882770236581564
LOSS train 0.45745583794127076 valid 0.2487599918734556
LOSS train 0.45745583794127076 valid 0.24893001023303257
LOSS train 0.45745583794127076 valid 0.24885203069148784
LOSS train 0.45745583794127076 valid 0.24898815800746282
LOSS train 0.45745583794127076 valid 0.24911768347518878
LOSS train 0.45745583794127076 valid 0.24913291174631852
LOSS train 0.45745583794127076 valid 0.24905180580954733
LOSS train 0.45745583794127076 valid 0.24907968914055306
LOSS train 0.45745583794127076 valid 0.24887485214181848
LOSS train 0.45745583794127076 valid 0.2488718155410982
LOSS train 0.45745583794127076 valid 0.2488118082603669
LOSS train 0.45745583794127076 valid 0.2488937591143111
LOSS train 0.45745583794127076 valid 0.2488215976448917
LOSS train 0.45745583794127076 valid 0.24882450958615857
LOSS train 0.45745583794127076 valid 0.24882235553564203
LOSS train 0.45745583794127076 valid 0.24887897609733045
LOSS train 0.45745583794127076 valid 0.24883793510612429
LOSS train 0.45745583794127076 valid 0.2487543839769265
LOSS train 0.45745583794127076 valid 0.2486640847646273
LOSS train 0.45745583794127076 valid 0.2486996238633078
LOSS train 0.45745583794127076 valid 0.2487755600873589
LOSS train 0.45745583794127076 valid 0.24864439937201413
LOSS train 0.45745583794127076 valid 0.2486619613877493
LOSS train 0.45745583794127076 valid 0.24861115604639053
LOSS train 0.45745583794127076 valid 0.24847166744334187
LOSS train 0.45745583794127076 valid 0.2484472827008455
LOSS train 0.45745583794127076 valid 0.24844932079021567
LOSS train 0.45745583794127076 valid 0.24842418876348757
LOSS train 0.45745583794127076 valid 0.24828460652653764
LOSS train 0.45745583794127076 valid 0.24824278953584653
LOSS train 0.45745583794127076 valid 0.24815245807746758
LOSS train 0.45745583794127076 valid 0.24807313951448753
LOSS train 0.45745583794127076 valid 0.2479399473330621
LOSS train 0.45745583794127076 valid 0.2479393020981834
LOSS train 0.45745583794127076 valid 0.24797398826522285
LOSS train 0.45745583794127076 valid 0.24793958340613348
LOSS train 0.45745583794127076 valid 0.2480439220515775
LOSS train 0.45745583794127076 valid 0.24801065751882356
LOSS train 0.45745583794127076 valid 0.24795151560805564
LOSS train 0.45745583794127076 valid 0.24794590769818536
LOSS train 0.45745583794127076 valid 0.24799977229212836
LOSS train 0.45745583794127076 valid 0.24801433694745423
LOSS train 0.45745583794127076 valid 0.2481037716479062
LOSS train 0.45745583794127076 valid 0.24817663343115287
LOSS train 0.45745583794127076 valid 0.24825345896757567
LOSS train 0.45745583794127076 valid 0.2482066052454012
LOSS train 0.45745583794127076 valid 0.24834935119868393
LOSS train 0.45745583794127076 valid 0.2484833471743124
LOSS train 0.45745583794127076 valid 0.2484902991188897
LOSS train 0.45745583794127076 valid 0.24857262673631178
LOSS train 0.45745583794127076 valid 0.2488295753621845
LOSS train 0.45745583794127076 valid 0.24888622460135243
LOSS train 0.45745583794127076 valid 0.2489458037515915
LOSS train 0.45745583794127076 valid 0.24902832559917285
LOSS train 0.45745583794127076 valid 0.24909128022916388
LOSS train 0.45745583794127076 valid 0.24909958795740686
LOSS train 0.45745583794127076 valid 0.24901470938465625
LOSS train 0.45745583794127076 valid 0.2490430163522052
LOSS train 0.45745583794127076 valid 0.24909655350319881
LOSS train 0.45745583794127076 valid 0.24903756260114202
LOSS train 0.45745583794127076 valid 0.24911538250587156
LOSS train 0.45745583794127076 valid 0.24907015276556255
LOSS train 0.45745583794127076 valid 0.24896950608267446
LOSS train 0.45745583794127076 valid 0.24894739954421918
LOSS train 0.45745583794127076 valid 0.24893116876792115
LOSS train 0.45745583794127076 valid 0.24877247534507563
LOSS train 0.45745583794127076 valid 0.2488723952829102
LOSS train 0.45745583794127076 valid 0.24900432089801694
LOSS train 0.45745583794127076 valid 0.2490798351716022
LOSS train 0.45745583794127076 valid 0.24906681441679235
LOSS train 0.45745583794127076 valid 0.24911220667333256
LOSS train 0.45745583794127076 valid 0.24910451261506927
LOSS train 0.45745583794127076 valid 0.24901683006181297
LOSS train 0.45745583794127076 valid 0.24910700231790542
LOSS train 0.45745583794127076 valid 0.24903757798956686
LOSS train 0.45745583794127076 valid 0.24910209404807243
LOSS train 0.45745583794127076 valid 0.24904789136568076
LOSS train 0.45745583794127076 valid 0.24907025184453002
LOSS train 0.45745583794127076 valid 0.24911169953206005
LOSS train 0.45745583794127076 valid 0.24912663985742256
LOSS train 0.45745583794127076 valid 0.24905638756223225
LOSS train 0.45745583794127076 valid 0.24913234667953596
LOSS train 0.45745583794127076 valid 0.24906183848279784
LOSS train 0.45745583794127076 valid 0.24906101713959988
LOSS train 0.45745583794127076 valid 0.24913667564876235
LOSS train 0.45745583794127076 valid 0.24914868579793523
LOSS train 0.45745583794127076 valid 0.2491654898163937
LOSS train 0.45745583794127076 valid 0.2491722388588118
LOSS train 0.45745583794127076 valid 0.24911355269405078
LOSS train 0.45745583794127076 valid 0.2491393493194329
LOSS train 0.45745583794127076 valid 0.2492061284709066
LOSS train 0.45745583794127076 valid 0.24936463978530757
LOSS train 0.45745583794127076 valid 0.24944881551992495
LOSS train 0.45745583794127076 valid 0.24941006385617787
LOSS train 0.45745583794127076 valid 0.24947379342524328
LOSS train 0.45745583794127076 valid 0.2495541597223457
LOSS train 0.45745583794127076 valid 0.24961982722029144
LOSS train 0.45745583794127076 valid 0.2495901910384206
LOSS train 0.45745583794127076 valid 0.249559847983447
LOSS train 0.45745583794127076 valid 0.24942689961281375
LOSS train 0.45745583794127076 valid 0.2492792711253631
LOSS train 0.45745583794127076 valid 0.24916396895758539
LOSS train 0.45745583794127076 valid 0.24915629331569944
LOSS train 0.45745583794127076 valid 0.24906032186533725
LOSS train 0.45745583794127076 valid 0.24894939824355455
LOSS train 0.45745583794127076 valid 0.24887044225812804
LOSS train 0.45745583794127076 valid 0.24889693933114568
LOSS train 0.45745583794127076 valid 0.2489837384454801
LOSS train 0.45745583794127076 valid 0.24901251358944074
LOSS train 0.45745583794127076 valid 0.24889751002713517
LOSS train 0.45745583794127076 valid 0.24883656401044402
LOSS train 0.45745583794127076 valid 0.24885597426651251
LOSS train 0.45745583794127076 valid 0.24888185104903052
LOSS train 0.45745583794127076 valid 0.24884475238364318
LOSS train 0.45745583794127076 valid 0.24881596279512977
LOSS train 0.45745583794127076 valid 0.24886109603390302
LOSS train 0.45745583794127076 valid 0.24886369537371417
LOSS train 0.45745583794127076 valid 0.2489515995594109
LOSS train 0.45745583794127076 valid 0.24901307220176114
LOSS train 0.45745583794127076 valid 0.24896273434766242
LOSS train 0.45745583794127076 valid 0.24898464174985083
LOSS train 0.45745583794127076 valid 0.24896211387127037
LOSS train 0.45745583794127076 valid 0.24900211114747867
LOSS train 0.45745583794127076 valid 0.24898420676589011
LOSS train 0.45745583794127076 valid 0.24903744988861273
LOSS train 0.45745583794127076 valid 0.24906826922238268
LOSS train 0.45745583794127076 valid 0.24913201036036212
LOSS train 0.45745583794127076 valid 0.24907902525247713
LOSS train 0.45745583794127076 valid 0.2489765478450744
LOSS train 0.45745583794127076 valid 0.248938032663336
LOSS train 0.45745583794127076 valid 0.24893001843158896
LOSS train 0.45745583794127076 valid 0.24888274842849026
LOSS train 0.45745583794127076 valid 0.24893726900364588
LOSS train 0.45745583794127076 valid 0.24892476499080657
LOSS train 0.45745583794127076 valid 0.24884933995088965
LOSS train 0.45745583794127076 valid 0.24887292846464193
LOSS train 0.45745583794127076 valid 0.2488936729991017
LOSS train 0.45745583794127076 valid 0.24893400239147198
LOSS train 0.45745583794127076 valid 0.24894509490520236
LOSS train 0.45745583794127076 valid 0.24891680139529554
LOSS train 0.45745583794127076 valid 0.24889858649920213
LOSS train 0.45745583794127076 valid 0.2489612912700611
LOSS train 0.45745583794127076 valid 0.24901079734477877
LOSS train 0.45745583794127076 valid 0.24894818309694527
LOSS train 0.45745583794127076 valid 0.24896110515356806
LOSS train 0.45745583794127076 valid 0.2489005587763668
LOSS train 0.45745583794127076 valid 0.2488652300723935
LOSS train 0.45745583794127076 valid 0.24879187906597866
LOSS train 0.45745583794127076 valid 0.24884906383661123
LOSS train 0.45745583794127076 valid 0.24896216959309725
LOSS train 0.45745583794127076 valid 0.24897356828053793
LOSS train 0.45745583794127076 valid 0.24895411729812622
LOSS train 0.45745583794127076 valid 0.24901277850464124
LOSS train 0.45745583794127076 valid 0.2489846885204315
LOSS train 0.45745583794127076 valid 0.2488942206626212
LOSS train 0.45745583794127076 valid 0.24881407649940754
LOSS train 0.45745583794127076 valid 0.2488424829445086
LOSS train 0.45745583794127076 valid 0.24896638644134214
LOSS train 0.45745583794127076 valid 0.24895511830030984
LOSS train 0.45745583794127076 valid 0.24895498236375196
LOSS train 0.45745583794127076 valid 0.2489151488602692
LOSS train 0.45745583794127076 valid 0.2488936033298278
LOSS train 0.45745583794127076 valid 0.24886871487517623
LOSS train 0.45745583794127076 valid 0.2488485422642792
LOSS train 0.45745583794127076 valid 0.2487604245761972
LOSS train 0.45745583794127076 valid 0.2487718031594628
LOSS train 0.45745583794127076 valid 0.2487681508498706
LOSS train 0.45745583794127076 valid 0.24889650026899438
LOSS train 0.45745583794127076 valid 0.24892156439414923
LOSS train 0.45745583794127076 valid 0.24886796653615256
LOSS train 0.45745583794127076 valid 0.24876337479102165
LOSS train 0.45745583794127076 valid 0.24869843109928327
LOSS train 0.45745583794127076 valid 0.24875836420195835
LOSS train 0.45745583794127076 valid 0.24871905505657196
LOSS train 0.45745583794127076 valid 0.24868599556789778
LOSS train 0.45745583794127076 valid 0.24870560326698152
LOSS train 0.45745583794127076 valid 0.24877306970928614
LOSS train 0.45745583794127076 valid 0.2488348746703843
LOSS train 0.45745583794127076 valid 0.248914535952286
LOSS train 0.45745583794127076 valid 0.24899944118904263
LOSS train 0.45745583794127076 valid 0.2489438705584582
LOSS train 0.45745583794127076 valid 0.2489238735600557
LOSS train 0.45745583794127076 valid 0.24899314152995192
LOSS train 0.45745583794127076 valid 0.24898919148577583
LOSS train 0.45745583794127076 valid 0.24898051113468128
LOSS train 0.45745583794127076 valid 0.24905426763203922
LOSS train 0.45745583794127076 valid 0.24891964988603407
LOSS train 0.45745583794127076 valid 0.24890228598327427
LOSS train 0.45745583794127076 valid 0.2488670682253903
LOSS train 0.45745583794127076 valid 0.24880980716539863
LOSS train 0.45745583794127076 valid 0.248705137125153
LOSS train 0.45745583794127076 valid 0.24867198996893738
LOSS train 0.45745583794127076 valid 0.24874391392640627
EPOCH 2:
  batch 1 loss: 0.32733362913131714
  batch 2 loss: 0.33111660182476044
  batch 3 loss: 0.3442929883797963
  batch 4 loss: 0.3478066995739937
  batch 5 loss: 0.35876678228378295
  batch 6 loss: 0.3562227686246236
  batch 7 loss: 0.35743431534085957
  batch 8 loss: 0.3589475303888321
  batch 9 loss: 0.35921041170756024
  batch 10 loss: 0.35885811150074004
  batch 11 loss: 0.35858475078235974
  batch 12 loss: 0.35657470176617306
  batch 13 loss: 0.3550640367544614
  batch 14 loss: 0.35545408938612255
  batch 15 loss: 0.3568692366282145
  batch 16 loss: 0.35655631124973297
  batch 17 loss: 0.35372257232666016
  batch 18 loss: 0.3545195360978444
  batch 19 loss: 0.35423061722203303
  batch 20 loss: 0.3525020137429237
  batch 21 loss: 0.35267360579399837
  batch 22 loss: 0.35325573655692016
  batch 23 loss: 0.35403479441352514
  batch 24 loss: 0.352523036301136
  batch 25 loss: 0.3542872989177704
  batch 26 loss: 0.352764205290721
  batch 27 loss: 0.3534114360809326
  batch 28 loss: 0.35157691687345505
  batch 29 loss: 0.35279550840114726
  batch 30 loss: 0.351885586977005
  batch 31 loss: 0.3525868202409437
  batch 32 loss: 0.3522541206330061
  batch 33 loss: 0.3533307539694237
  batch 34 loss: 0.3539590160636341
  batch 35 loss: 0.35501589264188493
  batch 36 loss: 0.3549923466311561
  batch 37 loss: 0.3564067434620213
  batch 38 loss: 0.35695330014354304
  batch 39 loss: 0.3568809437446105
  batch 40 loss: 0.35668562799692155
  batch 41 loss: 0.3564268764926166
  batch 42 loss: 0.35744584671088625
  batch 43 loss: 0.35849871607713923
  batch 44 loss: 0.3585979444059459
  batch 45 loss: 0.3582980983787113
  batch 46 loss: 0.35855659453765204
  batch 47 loss: 0.35873942362501265
  batch 48 loss: 0.35779465921223164
  batch 49 loss: 0.35719045996665955
  batch 50 loss: 0.35683180212974547
  batch 51 loss: 0.3565537958752875
  batch 52 loss: 0.35752257188925374
  batch 53 loss: 0.35692512876582594
  batch 54 loss: 0.3572341959785532
  batch 55 loss: 0.3570359560576352
  batch 56 loss: 0.3567296915820667
  batch 57 loss: 0.3560582713076943
  batch 58 loss: 0.35668069175605116
  batch 59 loss: 0.356451438645185
  batch 60 loss: 0.35600804338852565
  batch 61 loss: 0.3565025080422886
  batch 62 loss: 0.35643805275040286
  batch 63 loss: 0.3562067609930795
  batch 64 loss: 0.3567650909535587
  batch 65 loss: 0.3563361929013179
  batch 66 loss: 0.3560195163343892
  batch 67 loss: 0.3568528699341105
  batch 68 loss: 0.35702027424293403
  batch 69 loss: 0.35705780767012335
  batch 70 loss: 0.35729641829218184
  batch 71 loss: 0.35662205118528556
  batch 72 loss: 0.35672009405162597
  batch 73 loss: 0.3568455291120973
  batch 74 loss: 0.35730383645843816
  batch 75 loss: 0.35691415627797446
  batch 76 loss: 0.35699902592520966
  batch 77 loss: 0.3569349038910556
  batch 78 loss: 0.35695562951075727
  batch 79 loss: 0.35706726923773563
  batch 80 loss: 0.3565471064299345
  batch 81 loss: 0.35708977190064795
  batch 82 loss: 0.3574887405081493
  batch 83 loss: 0.35761634413018284
  batch 84 loss: 0.35719725560574306
  batch 85 loss: 0.3566480966175304
  batch 86 loss: 0.3572213469549667
  batch 87 loss: 0.3571549965732399
  batch 88 loss: 0.35661225190216844
  batch 89 loss: 0.35625380813405755
  batch 90 loss: 0.3560744709438748
  batch 91 loss: 0.35647783567617225
  batch 92 loss: 0.35627599676018173
  batch 93 loss: 0.35618290305137634
  batch 94 loss: 0.3567402502323719
  batch 95 loss: 0.356554051763133
  batch 96 loss: 0.35663846290359896
  batch 97 loss: 0.3573942205955073
  batch 98 loss: 0.3576075498546873
  batch 99 loss: 0.35778189698855084
  batch 100 loss: 0.35765882939100263
  batch 101 loss: 0.35766685127031683
  batch 102 loss: 0.35769543548425037
  batch 103 loss: 0.35803084263523804
  batch 104 loss: 0.3577548827116306
  batch 105 loss: 0.35725360058602834
  batch 106 loss: 0.3575072190109289
  batch 107 loss: 0.3571101819800439
  batch 108 loss: 0.35697649722849883
  batch 109 loss: 0.35675289762129475
  batch 110 loss: 0.35707268796183844
  batch 111 loss: 0.3566268212623424
  batch 112 loss: 0.35655892826616764
  batch 113 loss: 0.3568969714430581
  batch 114 loss: 0.35714026788870495
  batch 115 loss: 0.3576461182988208
  batch 116 loss: 0.35803369021621245
  batch 117 loss: 0.35815957901824236
  batch 118 loss: 0.35791187064122343
  batch 119 loss: 0.35818402206196504
  batch 120 loss: 0.3583694212138653
  batch 121 loss: 0.35837883028117096
  batch 122 loss: 0.3582894457656829
  batch 123 loss: 0.35830937871118873
  batch 124 loss: 0.3589009731527298
  batch 125 loss: 0.3589751765727997
  batch 126 loss: 0.35884611237616765
  batch 127 loss: 0.359412059774549
  batch 128 loss: 0.3593746682163328
  batch 129 loss: 0.35949190896610883
  batch 130 loss: 0.3594740101924309
  batch 131 loss: 0.35958441113697664
  batch 132 loss: 0.3595132705840198
  batch 133 loss: 0.3596029655825823
  batch 134 loss: 0.35941861191792274
  batch 135 loss: 0.3594288603023246
  batch 136 loss: 0.3594735845485154
  batch 137 loss: 0.35936053477934676
  batch 138 loss: 0.35935106333615124
  batch 139 loss: 0.3597197438315522
  batch 140 loss: 0.35949862471648625
  batch 141 loss: 0.35963127291794367
  batch 142 loss: 0.35951609049044864
  batch 143 loss: 0.3594837042835209
  batch 144 loss: 0.35938666698833305
  batch 145 loss: 0.35927570873293385
  batch 146 loss: 0.359242440698898
  batch 147 loss: 0.35965398014808186
  batch 148 loss: 0.35950806434895544
  batch 149 loss: 0.35931439167701157
  batch 150 loss: 0.3592365954319636
  batch 151 loss: 0.3594444587530679
  batch 152 loss: 0.3595722835314901
  batch 153 loss: 0.3594703839884864
  batch 154 loss: 0.3595617100015863
  batch 155 loss: 0.35956652318277665
  batch 156 loss: 0.3594612351212746
  batch 157 loss: 0.3596366089620408
  batch 158 loss: 0.35981507542767105
  batch 159 loss: 0.3598240167839722
  batch 160 loss: 0.3595828616991639
  batch 161 loss: 0.35956706782305464
  batch 162 loss: 0.35957381202850813
  batch 163 loss: 0.35936614871025085
  batch 164 loss: 0.35939843025876256
  batch 165 loss: 0.359173015572808
  batch 166 loss: 0.3589473623468215
  batch 167 loss: 0.35880740245659193
  batch 168 loss: 0.3590227220030058
  batch 169 loss: 0.3587488584969876
  batch 170 loss: 0.35864902927595027
  batch 171 loss: 0.3586340157957802
  batch 172 loss: 0.3586485877979633
  batch 173 loss: 0.3586262833176321
  batch 174 loss: 0.358638195984665
  batch 175 loss: 0.35865907754216875
  batch 176 loss: 0.3582087892361663
  batch 177 loss: 0.35813295420280283
  batch 178 loss: 0.358141739381833
  batch 179 loss: 0.35824095287136526
  batch 180 loss: 0.35818322781059475
  batch 181 loss: 0.35818254074997663
  batch 182 loss: 0.3578830313879055
  batch 183 loss: 0.3575462840619634
  batch 184 loss: 0.35741222601221956
  batch 185 loss: 0.35740248351483733
  batch 186 loss: 0.3575841739613523
  batch 187 loss: 0.35752138909809095
  batch 188 loss: 0.35712115799492977
  batch 189 loss: 0.3568166287803145
  batch 190 loss: 0.3568102340949209
  batch 191 loss: 0.35661131892528836
  batch 192 loss: 0.3565111480032404
  batch 193 loss: 0.3564626410835148
  batch 194 loss: 0.3565913850499183
  batch 195 loss: 0.35664054888945357
  batch 196 loss: 0.356256010581036
  batch 197 loss: 0.35599903467342936
  batch 198 loss: 0.35614102777808604
  batch 199 loss: 0.35608817854119307
  batch 200 loss: 0.3560462792217731
  batch 201 loss: 0.35610883093591944
  batch 202 loss: 0.35611204640700084
  batch 203 loss: 0.3560153303181597
  batch 204 loss: 0.3558301011136934
  batch 205 loss: 0.35588000082388155
  batch 206 loss: 0.35583678828281107
  batch 207 loss: 0.3557627340733717
  batch 208 loss: 0.3555982948209231
  batch 209 loss: 0.35530505996001394
  batch 210 loss: 0.3553440708489645
  batch 211 loss: 0.35523600699777286
  batch 212 loss: 0.35515187849413676
  batch 213 loss: 0.35508796614660343
  batch 214 loss: 0.3550240980966069
  batch 215 loss: 0.35475699735242266
  batch 216 loss: 0.3544741795846709
  batch 217 loss: 0.3544652639171495
  batch 218 loss: 0.35442591492736014
  batch 219 loss: 0.3544758258889255
  batch 220 loss: 0.35440034812146964
  batch 221 loss: 0.35441402113275833
  batch 222 loss: 0.354396115417953
  batch 223 loss: 0.35432041105668105
  batch 224 loss: 0.35429537748651846
  batch 225 loss: 0.3540858977370792
  batch 226 loss: 0.3541073797814614
  batch 227 loss: 0.35385427745428377
  batch 228 loss: 0.35370989444485884
  batch 229 loss: 0.3536300482187729
  batch 230 loss: 0.3536248370357182
  batch 231 loss: 0.3534649678897032
  batch 232 loss: 0.35334224472271986
  batch 233 loss: 0.3532589468065761
  batch 234 loss: 0.3533161821273657
  batch 235 loss: 0.35330468834714684
  batch 236 loss: 0.35312712129394885
  batch 237 loss: 0.3531328542826045
  batch 238 loss: 0.35304357624855365
  batch 239 loss: 0.3529615275031852
  batch 240 loss: 0.35294997158149877
  batch 241 loss: 0.3530504035751849
  batch 242 loss: 0.35283830737279465
  batch 243 loss: 0.3529481652342243
  batch 244 loss: 0.3529390831951235
  batch 245 loss: 0.35283701273859763
  batch 246 loss: 0.3527652518293722
  batch 247 loss: 0.35289062107140234
  batch 248 loss: 0.3529908590980114
  batch 249 loss: 0.35286219543721303
  batch 250 loss: 0.3527902833223343
  batch 251 loss: 0.3528796519891199
  batch 252 loss: 0.3527817924817403
  batch 253 loss: 0.352585990084961
  batch 254 loss: 0.3524880279002227
  batch 255 loss: 0.3525354241623598
  batch 256 loss: 0.35253162460867316
  batch 257 loss: 0.35238185640902836
  batch 258 loss: 0.3523435388193574
  batch 259 loss: 0.3522805038796429
  batch 260 loss: 0.3523323721610583
  batch 261 loss: 0.35221028853193553
  batch 262 loss: 0.3519688667231844
  batch 263 loss: 0.35194663346493654
  batch 264 loss: 0.35180581440076686
  batch 265 loss: 0.3516752137328094
  batch 266 loss: 0.35151209318100063
  batch 267 loss: 0.35159132654747266
  batch 268 loss: 0.35150753817896346
  batch 269 loss: 0.35141700317868513
  batch 270 loss: 0.3514645237613607
  batch 271 loss: 0.3514043284078366
  batch 272 loss: 0.3514613172806361
  batch 273 loss: 0.35148218940029213
  batch 274 loss: 0.35151221728237875
  batch 275 loss: 0.3515045223452828
  batch 276 loss: 0.3514229105650515
  batch 277 loss: 0.3513150005349183
  batch 278 loss: 0.35119531759255224
  batch 279 loss: 0.3512391313429802
  batch 280 loss: 0.3511347655739103
  batch 281 loss: 0.35097158930904077
  batch 282 loss: 0.350930244787365
  batch 283 loss: 0.35089762787936857
  batch 284 loss: 0.35097005480611826
  batch 285 loss: 0.350912124232242
  batch 286 loss: 0.35108017410848524
  batch 287 loss: 0.3510003429464347
  batch 288 loss: 0.35082074627280235
  batch 289 loss: 0.3509929596759044
  batch 290 loss: 0.3508249041335336
  batch 291 loss: 0.3508293600016853
  batch 292 loss: 0.3507708498876389
  batch 293 loss: 0.350682476681654
  batch 294 loss: 0.35054853254435014
  batch 295 loss: 0.3504696803577876
  batch 296 loss: 0.3504432387448646
  batch 297 loss: 0.35047684233598037
  batch 298 loss: 0.35054349269242896
  batch 299 loss: 0.3503456754628631
  batch 300 loss: 0.3503758222858111
  batch 301 loss: 0.35022693565517565
  batch 302 loss: 0.3501919976725484
  batch 303 loss: 0.3501307677121052
  batch 304 loss: 0.35003723243349477
  batch 305 loss: 0.3498458345405391
  batch 306 loss: 0.3498663041326735
  batch 307 loss: 0.3498495138623427
  batch 308 loss: 0.34990844198248605
  batch 309 loss: 0.3498301516651721
  batch 310 loss: 0.3497673740310054
  batch 311 loss: 0.3497912588610143
  batch 312 loss: 0.34984792367770123
  batch 313 loss: 0.34977014815083707
  batch 314 loss: 0.34966869348553337
  batch 315 loss: 0.3495342843116276
  batch 316 loss: 0.3494401603371282
  batch 317 loss: 0.3494053601466519
  batch 318 loss: 0.34924311645375855
  batch 319 loss: 0.3491208919537105
  batch 320 loss: 0.3491034832783043
  batch 321 loss: 0.3491599658196589
  batch 322 loss: 0.3491358141536298
  batch 323 loss: 0.3490826078797272
  batch 324 loss: 0.3489216274501365
  batch 325 loss: 0.3488283262803004
  batch 326 loss: 0.34883915930072223
  batch 327 loss: 0.3488900164763133
  batch 328 loss: 0.34868542541091035
  batch 329 loss: 0.3486749343234355
  batch 330 loss: 0.34859680551471134
  batch 331 loss: 0.3485613662490672
  batch 332 loss: 0.3484728067933795
  batch 333 loss: 0.3484597386898579
  batch 334 loss: 0.34834970301853685
  batch 335 loss: 0.34813105499566493
  batch 336 loss: 0.347956099946584
  batch 337 loss: 0.34781599540031277
  batch 338 loss: 0.3477807274231544
  batch 339 loss: 0.3478404107698649
  batch 340 loss: 0.3478272993774975
  batch 341 loss: 0.3476810263049218
  batch 342 loss: 0.3475767849830159
  batch 343 loss: 0.3475716576284292
  batch 344 loss: 0.3475546760614528
  batch 345 loss: 0.34768641496050184
  batch 346 loss: 0.3475440491658415
  batch 347 loss: 0.347494729314139
  batch 348 loss: 0.34746310457415963
  batch 349 loss: 0.347326075706236
  batch 350 loss: 0.3472673276492528
  batch 351 loss: 0.3471721037160977
  batch 352 loss: 0.34726464028724213
  batch 353 loss: 0.34720730832210683
  batch 354 loss: 0.34737073785841127
  batch 355 loss: 0.34730993996203785
  batch 356 loss: 0.3472074314617039
  batch 357 loss: 0.34706127059225944
  batch 358 loss: 0.3471115388017793
  batch 359 loss: 0.347036148429249
  batch 360 loss: 0.346952958818939
  batch 361 loss: 0.3468466603359687
  batch 362 loss: 0.34671236319436555
  batch 363 loss: 0.3465517813509161
  batch 364 loss: 0.34644704041900215
  batch 365 loss: 0.34649427075908606
  batch 366 loss: 0.34642407607511094
  batch 367 loss: 0.3463123582039607
  batch 368 loss: 0.34615513058784214
  batch 369 loss: 0.34614564853955093
  batch 370 loss: 0.346087389942762
  batch 371 loss: 0.3461132788593878
  batch 372 loss: 0.34606580812764426
  batch 373 loss: 0.3459430686909775
  batch 374 loss: 0.3457761946368345
  batch 375 loss: 0.34569711327552793
  batch 376 loss: 0.345791526218044
  batch 377 loss: 0.3457281082789525
  batch 378 loss: 0.3456161899699105
  batch 379 loss: 0.3455490591815405
  batch 380 loss: 0.3455585097011767
  batch 381 loss: 0.3454564544628924
  batch 382 loss: 0.3453218589746515
  batch 383 loss: 0.3452260980562504
  batch 384 loss: 0.345083134714514
  batch 385 loss: 0.3451820878239421
  batch 386 loss: 0.3450192033935705
  batch 387 loss: 0.34500776389156507
  batch 388 loss: 0.3449732171966858
  batch 389 loss: 0.3449482953027465
  batch 390 loss: 0.3448871014974056
  batch 391 loss: 0.3448704760092908
  batch 392 loss: 0.34485192034317524
  batch 393 loss: 0.34482466325201755
  batch 394 loss: 0.3448216458262526
  batch 395 loss: 0.3448110268840307
  batch 396 loss: 0.3447678968159839
  batch 397 loss: 0.3447993511817317
  batch 398 loss: 0.3447248720044467
  batch 399 loss: 0.3447608751312533
  batch 400 loss: 0.34475787945091724
  batch 401 loss: 0.34465221759684367
  batch 402 loss: 0.344659869276469
  batch 403 loss: 0.3447612671934937
  batch 404 loss: 0.34480035784516005
  batch 405 loss: 0.34481699716897657
  batch 406 loss: 0.344927090655994
  batch 407 loss: 0.3448478920014543
  batch 408 loss: 0.3448905625617972
  batch 409 loss: 0.3448735170958969
  batch 410 loss: 0.3449310810827627
  batch 411 loss: 0.3448999275401271
  batch 412 loss: 0.3447845366249964
  batch 413 loss: 0.3448427740222894
  batch 414 loss: 0.34477157574057005
  batch 415 loss: 0.34468807922788414
  batch 416 loss: 0.34471398341254544
  batch 417 loss: 0.3446930796265316
  batch 418 loss: 0.34458813335051375
  batch 419 loss: 0.3445456494579452
  batch 420 loss: 0.34447242149284907
  batch 421 loss: 0.3443785804609222
  batch 422 loss: 0.344479712759149
  batch 423 loss: 0.34448541542317007
  batch 424 loss: 0.34443936180674806
  batch 425 loss: 0.3444331992373747
  batch 426 loss: 0.3443406926634166
  batch 427 loss: 0.3443389956510039
  batch 428 loss: 0.34422989252293223
  batch 429 loss: 0.34421108555404734
  batch 430 loss: 0.3441070108219635
  batch 431 loss: 0.344184041368989
  batch 432 loss: 0.34422314870688653
  batch 433 loss: 0.34418309485113924
  batch 434 loss: 0.3442069576357916
  batch 435 loss: 0.3441177279099651
  batch 436 loss: 0.3441104330464241
  batch 437 loss: 0.34417332023450387
  batch 438 loss: 0.34423458630635856
  batch 439 loss: 0.34419817818596044
  batch 440 loss: 0.3442125168036331
  batch 441 loss: 0.3441264964555667
  batch 442 loss: 0.3440086188224646
  batch 443 loss: 0.3439245142597646
  batch 444 loss: 0.3438451205556457
  batch 445 loss: 0.3438798449682386
  batch 446 loss: 0.3438264443601728
  batch 447 loss: 0.343680481115977
  batch 448 loss: 0.3436665083281696
  batch 449 loss: 0.3436681611633513
  batch 450 loss: 0.34361536959807076
  batch 451 loss: 0.34358178796102096
  batch 452 loss: 0.34359955682163745
  batch 453 loss: 0.3435992525508072
  batch 454 loss: 0.34363906873217764
  batch 455 loss: 0.34359625892324763
  batch 456 loss: 0.34361487698920984
  batch 457 loss: 0.34355965054530807
  batch 458 loss: 0.34349191357995745
  batch 459 loss: 0.3435691395746077
  batch 460 loss: 0.34359535451816475
  batch 461 loss: 0.343574422623744
  batch 462 loss: 0.3435662823580044
  batch 463 loss: 0.34348113310259826
  batch 464 loss: 0.343475550091986
  batch 465 loss: 0.3434023601393546
  batch 466 loss: 0.34324397236236687
  batch 467 loss: 0.3433164038418192
  batch 468 loss: 0.3433023685446152
  batch 469 loss: 0.34342320190309716
  batch 470 loss: 0.3433744594137719
  batch 471 loss: 0.3433532721171207
  batch 472 loss: 0.3431870411386934
LOSS train 0.3431870411386934 valid 0.25000646710395813
LOSS train 0.3431870411386934 valid 0.238341823220253
LOSS train 0.3431870411386934 valid 0.23930289347966513
LOSS train 0.3431870411386934 valid 0.23843294009566307
LOSS train 0.3431870411386934 valid 0.23365312516689302
LOSS train 0.3431870411386934 valid 0.23628176500399908
LOSS train 0.3431870411386934 valid 0.24620121930326735
LOSS train 0.3431870411386934 valid 0.24612274020910263
LOSS train 0.3431870411386934 valid 0.24468564987182617
LOSS train 0.3431870411386934 valid 0.24460934102535248
LOSS train 0.3431870411386934 valid 0.24228377911177548
LOSS train 0.3431870411386934 valid 0.24235512192050615
LOSS train 0.3431870411386934 valid 0.2416770549920889
LOSS train 0.3431870411386934 valid 0.24182038541351045
LOSS train 0.3431870411386934 valid 0.23823840220769246
LOSS train 0.3431870411386934 valid 0.23912279680371284
LOSS train 0.3431870411386934 valid 0.24055839286131017
LOSS train 0.3431870411386934 valid 0.24129917886522081
LOSS train 0.3431870411386934 valid 0.24301128011000783
LOSS train 0.3431870411386934 valid 0.2425155058503151
LOSS train 0.3431870411386934 valid 0.24171385381902968
LOSS train 0.3431870411386934 valid 0.24002631956880743
LOSS train 0.3431870411386934 valid 0.23948836650537408
LOSS train 0.3431870411386934 valid 0.23838171176612377
LOSS train 0.3431870411386934 valid 0.23695936441421508
LOSS train 0.3431870411386934 valid 0.23707560220590004
LOSS train 0.3431870411386934 valid 0.2371308947051013
LOSS train 0.3431870411386934 valid 0.23770279543740408
LOSS train 0.3431870411386934 valid 0.23785371513202272
LOSS train 0.3431870411386934 valid 0.23846327662467956
LOSS train 0.3431870411386934 valid 0.2390646924895625
LOSS train 0.3431870411386934 valid 0.23836585925891995
LOSS train 0.3431870411386934 valid 0.2396973240556139
LOSS train 0.3431870411386934 valid 0.23951675173114328
LOSS train 0.3431870411386934 valid 0.24092635086604527
LOSS train 0.3431870411386934 valid 0.24017908341354793
LOSS train 0.3431870411386934 valid 0.24048919291109652
LOSS train 0.3431870411386934 valid 0.2414801912872415
LOSS train 0.3431870411386934 valid 0.2410085270038018
LOSS train 0.3431870411386934 valid 0.24093768298625945
LOSS train 0.3431870411386934 valid 0.2416609830972625
LOSS train 0.3431870411386934 valid 0.24165289174942745
LOSS train 0.3431870411386934 valid 0.24140514260114626
LOSS train 0.3431870411386934 valid 0.24183791943571784
LOSS train 0.3431870411386934 valid 0.24165472818745506
LOSS train 0.3431870411386934 valid 0.2419478384696919
LOSS train 0.3431870411386934 valid 0.24266462281663367
LOSS train 0.3431870411386934 valid 0.24276605962465206
LOSS train 0.3431870411386934 valid 0.24313477746077947
LOSS train 0.3431870411386934 valid 0.24227243959903716
LOSS train 0.3431870411386934 valid 0.24255108716441134
LOSS train 0.3431870411386934 valid 0.24239613889501646
LOSS train 0.3431870411386934 valid 0.2424565466507426
LOSS train 0.3431870411386934 valid 0.24237820285337944
LOSS train 0.3431870411386934 valid 0.24230710782788017
LOSS train 0.3431870411386934 valid 0.24210344973419393
LOSS train 0.3431870411386934 valid 0.24178225560146466
LOSS train 0.3431870411386934 valid 0.24166710777529354
LOSS train 0.3431870411386934 valid 0.24192685494988653
LOSS train 0.3431870411386934 valid 0.24192964658141136
LOSS train 0.3431870411386934 valid 0.24155036250098808
LOSS train 0.3431870411386934 valid 0.24236161574240653
LOSS train 0.3431870411386934 valid 0.24236725389011324
LOSS train 0.3431870411386934 valid 0.24321724381297827
LOSS train 0.3431870411386934 valid 0.2433563204912039
LOSS train 0.3431870411386934 valid 0.2433452606201172
LOSS train 0.3431870411386934 valid 0.2425768170783769
LOSS train 0.3431870411386934 valid 0.24242979397668557
LOSS train 0.3431870411386934 valid 0.24185416663902393
LOSS train 0.3431870411386934 valid 0.24173091096537455
LOSS train 0.3431870411386934 valid 0.24140602497147842
LOSS train 0.3431870411386934 valid 0.24158809313343632
LOSS train 0.3431870411386934 valid 0.24165338461529717
LOSS train 0.3431870411386934 valid 0.24142339745083372
LOSS train 0.3431870411386934 valid 0.2414321800072988
LOSS train 0.3431870411386934 valid 0.2417016723438313
LOSS train 0.3431870411386934 valid 0.24165859257245992
LOSS train 0.3431870411386934 valid 0.2417409788721647
LOSS train 0.3431870411386934 valid 0.24169725160810013
LOSS train 0.3431870411386934 valid 0.24117608852684497
LOSS train 0.3431870411386934 valid 0.2404016528600528
LOSS train 0.3431870411386934 valid 0.24054635843125785
LOSS train 0.3431870411386934 valid 0.2404379203736064
LOSS train 0.3431870411386934 valid 0.24043726779165722
LOSS train 0.3431870411386934 valid 0.2401504776057075
LOSS train 0.3431870411386934 valid 0.23944783297389052
LOSS train 0.3431870411386934 valid 0.23945825932354764
LOSS train 0.3431870411386934 valid 0.23879029974341393
LOSS train 0.3431870411386934 valid 0.23921140961432724
LOSS train 0.3431870411386934 valid 0.239385121067365
LOSS train 0.3431870411386934 valid 0.2394857747214181
LOSS train 0.3431870411386934 valid 0.23966337027757065
LOSS train 0.3431870411386934 valid 0.2395009978484082
LOSS train 0.3431870411386934 valid 0.23941011124468864
LOSS train 0.3431870411386934 valid 0.23929027099358408
LOSS train 0.3431870411386934 valid 0.2394847348332405
LOSS train 0.3431870411386934 valid 0.23971232065220469
LOSS train 0.3431870411386934 valid 0.23984767800691176
LOSS train 0.3431870411386934 valid 0.23981600608488526
LOSS train 0.3431870411386934 valid 0.24003977209329605
LOSS train 0.3431870411386934 valid 0.24030557068267672
LOSS train 0.3431870411386934 valid 0.24029957371599533
LOSS train 0.3431870411386934 valid 0.24021502287642468
LOSS train 0.3431870411386934 valid 0.24005781329022005
LOSS train 0.3431870411386934 valid 0.24012498216969627
LOSS train 0.3431870411386934 valid 0.2404072235496539
LOSS train 0.3431870411386934 valid 0.2401792919524362
LOSS train 0.3431870411386934 valid 0.24023556654100064
LOSS train 0.3431870411386934 valid 0.24052105748325314
LOSS train 0.3431870411386934 valid 0.240859594670209
LOSS train 0.3431870411386934 valid 0.2404741764337093
LOSS train 0.3431870411386934 valid 0.24036942182907037
LOSS train 0.3431870411386934 valid 0.24033155404360948
LOSS train 0.3431870411386934 valid 0.24026453952517426
LOSS train 0.3431870411386934 valid 0.2403316299552503
LOSS train 0.3431870411386934 valid 0.240499096055483
LOSS train 0.3431870411386934 valid 0.24065211147834092
LOSS train 0.3431870411386934 valid 0.24069417949955343
LOSS train 0.3431870411386934 valid 0.2405977401913715
LOSS train 0.3431870411386934 valid 0.2403592032690843
LOSS train 0.3431870411386934 valid 0.24013728001886162
LOSS train 0.3431870411386934 valid 0.23996286020904292
LOSS train 0.3431870411386934 valid 0.23982418588991086
LOSS train 0.3431870411386934 valid 0.24011341638622746
LOSS train 0.3431870411386934 valid 0.24008037388324738
LOSS train 0.3431870411386934 valid 0.24034472413006283
LOSS train 0.3431870411386934 valid 0.2402838917933111
LOSS train 0.3431870411386934 valid 0.24054287176113576
LOSS train 0.3431870411386934 valid 0.240669712655304
LOSS train 0.3431870411386934 valid 0.24060758478366412
LOSS train 0.3431870411386934 valid 0.24055500192041615
LOSS train 0.3431870411386934 valid 0.24033518530654185
LOSS train 0.3431870411386934 valid 0.24020326081523322
LOSS train 0.3431870411386934 valid 0.24030191851640814
LOSS train 0.3431870411386934 valid 0.24025560286309983
LOSS train 0.3431870411386934 valid 0.24015424959361553
LOSS train 0.3431870411386934 valid 0.2399715998747053
LOSS train 0.3431870411386934 valid 0.23991649403520252
LOSS train 0.3431870411386934 valid 0.2397839759322379
LOSS train 0.3431870411386934 valid 0.2398408123425075
LOSS train 0.3431870411386934 valid 0.23981242188325164
LOSS train 0.3431870411386934 valid 0.24007610368057036
LOSS train 0.3431870411386934 valid 0.23998489913406906
LOSS train 0.3431870411386934 valid 0.23996037017140123
LOSS train 0.3431870411386934 valid 0.23971544093099134
LOSS train 0.3431870411386934 valid 0.23989142783700604
LOSS train 0.3431870411386934 valid 0.23961883975940496
LOSS train 0.3431870411386934 valid 0.2402051301622713
LOSS train 0.3431870411386934 valid 0.24036831353734803
LOSS train 0.3431870411386934 valid 0.24028878917296728
LOSS train 0.3431870411386934 valid 0.24040151619358568
LOSS train 0.3431870411386934 valid 0.24019495848762362
LOSS train 0.3431870411386934 valid 0.240271201320723
LOSS train 0.3431870411386934 valid 0.24014573786165808
LOSS train 0.3431870411386934 valid 0.24024397865418465
LOSS train 0.3431870411386934 valid 0.24042417338261238
LOSS train 0.3431870411386934 valid 0.24036237625939072
LOSS train 0.3431870411386934 valid 0.24057422267108025
LOSS train 0.3431870411386934 valid 0.2405777721659942
LOSS train 0.3431870411386934 valid 0.24057989539578556
LOSS train 0.3431870411386934 valid 0.2405213863583085
LOSS train 0.3431870411386934 valid 0.2404743442196905
LOSS train 0.3431870411386934 valid 0.24037148954312496
LOSS train 0.3431870411386934 valid 0.24013282022461657
LOSS train 0.3431870411386934 valid 0.2400500198205312
LOSS train 0.3431870411386934 valid 0.24011867053537483
LOSS train 0.3431870411386934 valid 0.24034592152355674
LOSS train 0.3431870411386934 valid 0.24022158607840538
LOSS train 0.3431870411386934 valid 0.24042733851269152
LOSS train 0.3431870411386934 valid 0.24057204320150263
LOSS train 0.3431870411386934 valid 0.24048794779861182
LOSS train 0.3431870411386934 valid 0.24040813744068146
LOSS train 0.3431870411386934 valid 0.24054295737619344
LOSS train 0.3431870411386934 valid 0.24054774462149062
LOSS train 0.3431870411386934 valid 0.2404778848375593
LOSS train 0.3431870411386934 valid 0.24035911456766454
LOSS train 0.3431870411386934 valid 0.24029612238124265
LOSS train 0.3431870411386934 valid 0.2404600962829054
LOSS train 0.3431870411386934 valid 0.2403668643043028
LOSS train 0.3431870411386934 valid 0.24048200332456166
LOSS train 0.3431870411386934 valid 0.2406549247918208
LOSS train 0.3431870411386934 valid 0.2406250019813632
LOSS train 0.3431870411386934 valid 0.24052211503839233
LOSS train 0.3431870411386934 valid 0.24053991334917751
LOSS train 0.3431870411386934 valid 0.24032113825952683
LOSS train 0.3431870411386934 valid 0.24026890010923468
LOSS train 0.3431870411386934 valid 0.24020853120694186
LOSS train 0.3431870411386934 valid 0.24026254635859043
LOSS train 0.3431870411386934 valid 0.24024209064781352
LOSS train 0.3431870411386934 valid 0.24030359450139496
LOSS train 0.3431870411386934 valid 0.24027929630579126
LOSS train 0.3431870411386934 valid 0.24034402379766107
LOSS train 0.3431870411386934 valid 0.24030587463181252
LOSS train 0.3431870411386934 valid 0.2402327950160528
LOSS train 0.3431870411386934 valid 0.24007822512052
LOSS train 0.3431870411386934 valid 0.24012690288375835
LOSS train 0.3431870411386934 valid 0.24025678218621288
LOSS train 0.3431870411386934 valid 0.2401046065068004
LOSS train 0.3431870411386934 valid 0.2401492645093544
LOSS train 0.3431870411386934 valid 0.24009997673332692
LOSS train 0.3431870411386934 valid 0.23996399098367832
LOSS train 0.3431870411386934 valid 0.2398884402643336
LOSS train 0.3431870411386934 valid 0.2398217142802741
LOSS train 0.3431870411386934 valid 0.23980332034475663
LOSS train 0.3431870411386934 valid 0.2396340745978239
LOSS train 0.3431870411386934 valid 0.2396281088874178
LOSS train 0.3431870411386934 valid 0.23955401490275988
LOSS train 0.3431870411386934 valid 0.23940739329331195
LOSS train 0.3431870411386934 valid 0.23923378876236637
LOSS train 0.3431870411386934 valid 0.239244568276973
LOSS train 0.3431870411386934 valid 0.2393270411056365
LOSS train 0.3431870411386934 valid 0.23933001298387097
LOSS train 0.3431870411386934 valid 0.23944372330473063
LOSS train 0.3431870411386934 valid 0.23937711467809766
LOSS train 0.3431870411386934 valid 0.23931200975595518
LOSS train 0.3431870411386934 valid 0.2393181090829549
LOSS train 0.3431870411386934 valid 0.23935530674622355
LOSS train 0.3431870411386934 valid 0.23939195671759614
LOSS train 0.3431870411386934 valid 0.23947172145865278
LOSS train 0.3431870411386934 valid 0.23955498134548014
LOSS train 0.3431870411386934 valid 0.23964931455133187
LOSS train 0.3431870411386934 valid 0.2396139169598485
LOSS train 0.3431870411386934 valid 0.23975800759589191
LOSS train 0.3431870411386934 valid 0.23991908878087997
LOSS train 0.3431870411386934 valid 0.2399471519390742
LOSS train 0.3431870411386934 valid 0.240044971548878
LOSS train 0.3431870411386934 valid 0.24030936965595784
LOSS train 0.3431870411386934 valid 0.24038505207812577
LOSS train 0.3431870411386934 valid 0.24045439012446257
LOSS train 0.3431870411386934 valid 0.24053915551175242
LOSS train 0.3431870411386934 valid 0.2405507058163226
LOSS train 0.3431870411386934 valid 0.24057192682962994
LOSS train 0.3431870411386934 valid 0.2404909358387853
LOSS train 0.3431870411386934 valid 0.24052435458980054
LOSS train 0.3431870411386934 valid 0.240572467699964
LOSS train 0.3431870411386934 valid 0.2404813812572067
LOSS train 0.3431870411386934 valid 0.2405770431716734
LOSS train 0.3431870411386934 valid 0.24055686987498226
LOSS train 0.3431870411386934 valid 0.24046741968418264
LOSS train 0.3431870411386934 valid 0.24043549957374732
LOSS train 0.3431870411386934 valid 0.2404579700152409
LOSS train 0.3431870411386934 valid 0.24024519662965427
LOSS train 0.3431870411386934 valid 0.24036474317434883
LOSS train 0.3431870411386934 valid 0.24053575886321849
LOSS train 0.3431870411386934 valid 0.24063867446111173
LOSS train 0.3431870411386934 valid 0.24061405870730315
LOSS train 0.3431870411386934 valid 0.2406209423715769
LOSS train 0.3431870411386934 valid 0.2405927500897838
LOSS train 0.3431870411386934 valid 0.24049097000355701
LOSS train 0.3431870411386934 valid 0.2406174122095108
LOSS train 0.3431870411386934 valid 0.2405681562613681
LOSS train 0.3431870411386934 valid 0.2406680407741713
LOSS train 0.3431870411386934 valid 0.2405991925316837
LOSS train 0.3431870411386934 valid 0.24055700403029526
LOSS train 0.3431870411386934 valid 0.24061773033703074
LOSS train 0.3431870411386934 valid 0.24060873384587467
LOSS train 0.3431870411386934 valid 0.24049454747702825
LOSS train 0.3431870411386934 valid 0.24055677392455035
LOSS train 0.3431870411386934 valid 0.24053769273877604
LOSS train 0.3431870411386934 valid 0.24050685952489192
LOSS train 0.3431870411386934 valid 0.24058155327236058
LOSS train 0.3431870411386934 valid 0.24059657473828047
LOSS train 0.3431870411386934 valid 0.240597402301578
LOSS train 0.3431870411386934 valid 0.24062740328637036
LOSS train 0.3431870411386934 valid 0.24055931095807057
LOSS train 0.3431870411386934 valid 0.24057941866996593
LOSS train 0.3431870411386934 valid 0.24061801415704162
LOSS train 0.3431870411386934 valid 0.24079426122245504
LOSS train 0.3431870411386934 valid 0.24089749672598998
LOSS train 0.3431870411386934 valid 0.24089217290834145
LOSS train 0.3431870411386934 valid 0.24096053294370096
LOSS train 0.3431870411386934 valid 0.24105690359411872
LOSS train 0.3431870411386934 valid 0.24110259978107482
LOSS train 0.3431870411386934 valid 0.2410610099561023
LOSS train 0.3431870411386934 valid 0.24103307534347881
LOSS train 0.3431870411386934 valid 0.2408888354573561
LOSS train 0.3431870411386934 valid 0.24071962765623087
LOSS train 0.3431870411386934 valid 0.24058358028209467
LOSS train 0.3431870411386934 valid 0.24057970166633633
LOSS train 0.3431870411386934 valid 0.24048121071287565
LOSS train 0.3431870411386934 valid 0.2403625601975519
LOSS train 0.3431870411386934 valid 0.24024086324035698
LOSS train 0.3431870411386934 valid 0.2402415712604253
LOSS train 0.3431870411386934 valid 0.24030443637723653
LOSS train 0.3431870411386934 valid 0.2403283279193075
LOSS train 0.3431870411386934 valid 0.2402431551705707
LOSS train 0.3431870411386934 valid 0.24013835845924006
LOSS train 0.3431870411386934 valid 0.24014976072228617
LOSS train 0.3431870411386934 valid 0.24015943113082833
LOSS train 0.3431870411386934 valid 0.24012475522427723
LOSS train 0.3431870411386934 valid 0.24011348044544562
LOSS train 0.3431870411386934 valid 0.2401518687503795
LOSS train 0.3431870411386934 valid 0.24015368500870649
LOSS train 0.3431870411386934 valid 0.24028806586046608
LOSS train 0.3431870411386934 valid 0.2403506497710438
LOSS train 0.3431870411386934 valid 0.24029681162052863
LOSS train 0.3431870411386934 valid 0.24031697720389575
LOSS train 0.3431870411386934 valid 0.24028857927394393
LOSS train 0.3431870411386934 valid 0.2402991788542789
LOSS train 0.3431870411386934 valid 0.2402917356789112
LOSS train 0.3431870411386934 valid 0.24034733852476772
LOSS train 0.3431870411386934 valid 0.24037465270583994
LOSS train 0.3431870411386934 valid 0.24043244216898488
LOSS train 0.3431870411386934 valid 0.24039996018339144
LOSS train 0.3431870411386934 valid 0.24030323175133253
LOSS train 0.3431870411386934 valid 0.24025122831070345
LOSS train 0.3431870411386934 valid 0.24020580195060381
LOSS train 0.3431870411386934 valid 0.2401274773110817
LOSS train 0.3431870411386934 valid 0.24016656135856912
LOSS train 0.3431870411386934 valid 0.2401698755160455
LOSS train 0.3431870411386934 valid 0.24010232685079913
LOSS train 0.3431870411386934 valid 0.24012971363770655
LOSS train 0.3431870411386934 valid 0.24015477856698508
LOSS train 0.3431870411386934 valid 0.2402129018097926
LOSS train 0.3431870411386934 valid 0.24021692999771663
LOSS train 0.3431870411386934 valid 0.2401880195152156
LOSS train 0.3431870411386934 valid 0.2402034994179518
LOSS train 0.3431870411386934 valid 0.2402689169225453
LOSS train 0.3431870411386934 valid 0.2403431707601936
LOSS train 0.3431870411386934 valid 0.24025667207315565
LOSS train 0.3431870411386934 valid 0.24026998343683106
LOSS train 0.3431870411386934 valid 0.24019700321166412
LOSS train 0.3431870411386934 valid 0.24012079862618224
LOSS train 0.3431870411386934 valid 0.24007074093377148
LOSS train 0.3431870411386934 valid 0.24012487558218149
LOSS train 0.3431870411386934 valid 0.24021396315170943
LOSS train 0.3431870411386934 valid 0.24021151775796115
LOSS train 0.3431870411386934 valid 0.24019676687695632
LOSS train 0.3431870411386934 valid 0.24028178410870688
LOSS train 0.3431870411386934 valid 0.2402393135609049
LOSS train 0.3431870411386934 valid 0.2401501816835288
LOSS train 0.3431870411386934 valid 0.2400413910039218
LOSS train 0.3431870411386934 valid 0.2400508585187408
LOSS train 0.3431870411386934 valid 0.24019637083757422
LOSS train 0.3431870411386934 valid 0.24017464029255198
LOSS train 0.3431870411386934 valid 0.24016258466456616
LOSS train 0.3431870411386934 valid 0.2401328447222356
LOSS train 0.3431870411386934 valid 0.24011027010587546
LOSS train 0.3431870411386934 valid 0.24005961325843778
LOSS train 0.3431870411386934 valid 0.24006072997170336
LOSS train 0.3431870411386934 valid 0.23995694085474936
LOSS train 0.3431870411386934 valid 0.23996987943237985
LOSS train 0.3431870411386934 valid 0.23996016387515443
LOSS train 0.3431870411386934 valid 0.2401195168062005
LOSS train 0.3431870411386934 valid 0.2401625247537226
LOSS train 0.3431870411386934 valid 0.2401240607992762
LOSS train 0.3431870411386934 valid 0.2400125659766046
LOSS train 0.3431870411386934 valid 0.23992538135284666
LOSS train 0.3431870411386934 valid 0.23999156531768406
LOSS train 0.3431870411386934 valid 0.23992883209671292
LOSS train 0.3431870411386934 valid 0.23987573151744668
LOSS train 0.3431870411386934 valid 0.2398798552934419
LOSS train 0.3431870411386934 valid 0.23996350903686653
LOSS train 0.3431870411386934 valid 0.2400361272574818
LOSS train 0.3431870411386934 valid 0.24010909569095557
LOSS train 0.3431870411386934 valid 0.24021002415860637
LOSS train 0.3431870411386934 valid 0.2401505772973977
LOSS train 0.3431870411386934 valid 0.24011991776567598
LOSS train 0.3431870411386934 valid 0.24014200203764074
LOSS train 0.3431870411386934 valid 0.240128092509177
LOSS train 0.3431870411386934 valid 0.2400963914031137
LOSS train 0.3431870411386934 valid 0.2401633848801502
LOSS train 0.3431870411386934 valid 0.24002543050723957
LOSS train 0.3431870411386934 valid 0.24003367035926043
LOSS train 0.3431870411386934 valid 0.2400156943357154
LOSS train 0.3431870411386934 valid 0.23995284516303267
LOSS train 0.3431870411386934 valid 0.2398596235045945
LOSS train 0.3431870411386934 valid 0.23981115809115378
LOSS train 0.3431870411386934 valid 0.23990477993074794
EPOCH 3:
  batch 1 loss: 0.3166023790836334
  batch 2 loss: 0.30740290880203247
  batch 3 loss: 0.315785547097524
  batch 4 loss: 0.31494706124067307
  batch 5 loss: 0.32602238059043886
  batch 6 loss: 0.3237551550070445
  batch 7 loss: 0.3287666695458548
  batch 8 loss: 0.3314811401069164
  batch 9 loss: 0.33031924896770054
  batch 10 loss: 0.3313004106283188
  batch 11 loss: 0.33208997412161395
  batch 12 loss: 0.3285013511776924
  batch 13 loss: 0.3267934712079855
  batch 14 loss: 0.32823540908949717
  batch 15 loss: 0.3298958897590637
  batch 16 loss: 0.32860488817095757
  batch 17 loss: 0.32621920810026284
  batch 18 loss: 0.3267054276333915
  batch 19 loss: 0.3253931889408513
  batch 20 loss: 0.32362392395734785
  batch 21 loss: 0.3238563026700701
  batch 22 loss: 0.3237030113285238
  batch 23 loss: 0.323387808125952
  batch 24 loss: 0.32224701965848607
  batch 25 loss: 0.3250205552577972
  batch 26 loss: 0.3232473284006119
  batch 27 loss: 0.3238365252812703
  batch 28 loss: 0.3223365715571812
  batch 29 loss: 0.3232506626638873
  batch 30 loss: 0.32258221209049226
  batch 31 loss: 0.32302588032137963
  batch 32 loss: 0.32229873444885015
  batch 33 loss: 0.3229028952844215
  batch 34 loss: 0.3237258309827131
  batch 35 loss: 0.32413167783192226
  batch 36 loss: 0.32439905736181474
  batch 37 loss: 0.3255013565759401
  batch 38 loss: 0.32627980254198374
  batch 39 loss: 0.3259204901181735
  batch 40 loss: 0.32599923610687254
  batch 41 loss: 0.32574080694012525
  batch 42 loss: 0.32647490359487985
  batch 43 loss: 0.327590742083483
  batch 44 loss: 0.32791548560966144
  batch 45 loss: 0.32773246566454567
  batch 46 loss: 0.3281259970820468
  batch 47 loss: 0.3281829832716191
  batch 48 loss: 0.3272593424965938
  batch 49 loss: 0.32696964911052157
  batch 50 loss: 0.3267262476682663
  batch 51 loss: 0.32621624481444267
  batch 52 loss: 0.32688124019366044
  batch 53 loss: 0.3262785138948908
  batch 54 loss: 0.32649117708206177
  batch 55 loss: 0.32629689411683516
  batch 56 loss: 0.3259682607437883
  batch 57 loss: 0.3253567527260697
  batch 58 loss: 0.32602431496669504
  batch 59 loss: 0.32571515339916035
  batch 60 loss: 0.325178591410319
  batch 61 loss: 0.3256533507440911
  batch 62 loss: 0.32591662964513224
  batch 63 loss: 0.32580829754708307
  batch 64 loss: 0.3265725667588413
  batch 65 loss: 0.3261490711799035
  batch 66 loss: 0.325733536120617
  batch 67 loss: 0.3262827858996035
  batch 68 loss: 0.32666946903747673
  batch 69 loss: 0.326575646797816
  batch 70 loss: 0.32672353088855743
  batch 71 loss: 0.3264229876894346
  batch 72 loss: 0.3266402735478348
  batch 73 loss: 0.3268849335304678
  batch 74 loss: 0.3272590246554968
  batch 75 loss: 0.3268852468331655
  batch 76 loss: 0.32673157986841705
  batch 77 loss: 0.32655157129485884
  batch 78 loss: 0.3263873354746745
  batch 79 loss: 0.32661687460126754
  batch 80 loss: 0.3261803973466158
  batch 81 loss: 0.32671389167691456
  batch 82 loss: 0.3272340693851797
  batch 83 loss: 0.3274764490414815
  batch 84 loss: 0.3270947723871186
  batch 85 loss: 0.32666626011624056
  batch 86 loss: 0.32754301851571993
  batch 87 loss: 0.3274728748305091
  batch 88 loss: 0.3270086534321308
  batch 89 loss: 0.32663487517431883
  batch 90 loss: 0.32640588680903115
  batch 91 loss: 0.3264152178397545
  batch 92 loss: 0.3263004007546798
  batch 93 loss: 0.32614835584035484
  batch 94 loss: 0.3265658636042412
  batch 95 loss: 0.3263534213367261
  batch 96 loss: 0.3266720122968157
  batch 97 loss: 0.32739326142773184
  batch 98 loss: 0.327585406145271
  batch 99 loss: 0.327954612296037
  batch 100 loss: 0.3280134707689285
  batch 101 loss: 0.32805903860838104
  batch 102 loss: 0.32823789353464167
  batch 103 loss: 0.32858737059009885
  batch 104 loss: 0.32858519055522406
  batch 105 loss: 0.3281035363674164
  batch 106 loss: 0.32843569455281746
  batch 107 loss: 0.32808427331603573
  batch 108 loss: 0.32789015107684666
  batch 109 loss: 0.3276613644503672
  batch 110 loss: 0.3280375884337859
  batch 111 loss: 0.3276258758059493
  batch 112 loss: 0.3274071275123528
  batch 113 loss: 0.32805057727130116
  batch 114 loss: 0.32843471697547977
  batch 115 loss: 0.3289931172909944
  batch 116 loss: 0.32942058094616594
  batch 117 loss: 0.3297167947659126
  batch 118 loss: 0.32950990154581555
  batch 119 loss: 0.32973961143934427
  batch 120 loss: 0.3297712944447994
  batch 121 loss: 0.32972413744808227
  batch 122 loss: 0.32957870305561626
  batch 123 loss: 0.3295802125116674
  batch 124 loss: 0.32995060326591613
  batch 125 loss: 0.32998682260513307
  batch 126 loss: 0.3298861687145536
  batch 127 loss: 0.33059321918825463
  batch 128 loss: 0.330451728310436
  batch 129 loss: 0.33070585870927616
  batch 130 loss: 0.33080437641877397
  batch 131 loss: 0.3308933814063327
  batch 132 loss: 0.3307432259122531
  batch 133 loss: 0.330866013926671
  batch 134 loss: 0.330759566666475
  batch 135 loss: 0.33083848864943893
  batch 136 loss: 0.33076073141659007
  batch 137 loss: 0.33058407750442953
  batch 138 loss: 0.33048448268918024
  batch 139 loss: 0.3310212507951174
  batch 140 loss: 0.33083036435501917
  batch 141 loss: 0.33101746841525354
  batch 142 loss: 0.3308755679869316
  batch 143 loss: 0.3308744659790626
  batch 144 loss: 0.3307659448020988
  batch 145 loss: 0.33076606618946996
  batch 146 loss: 0.3307322951620572
  batch 147 loss: 0.3311299426215036
  batch 148 loss: 0.3310519410548983
  batch 149 loss: 0.33088749307114007
  batch 150 loss: 0.3309195860226949
  batch 151 loss: 0.33100201495435855
  batch 152 loss: 0.33121012562983915
  batch 153 loss: 0.33112110811121326
  batch 154 loss: 0.33118350126526575
  batch 155 loss: 0.3312357006534453
  batch 156 loss: 0.33117313052599245
  batch 157 loss: 0.33132715001227747
  batch 158 loss: 0.331309891954253
  batch 159 loss: 0.3313128882609073
  batch 160 loss: 0.33107562363147736
  batch 161 loss: 0.33116735823406196
  batch 162 loss: 0.33120087524990977
  batch 163 loss: 0.3310065922195926
  batch 164 loss: 0.331045067528399
  batch 165 loss: 0.33079816717090027
  batch 166 loss: 0.33054898607443617
  batch 167 loss: 0.33043701705818407
  batch 168 loss: 0.33060457557439804
  batch 169 loss: 0.33043631505684035
  batch 170 loss: 0.33027628081686355
  batch 171 loss: 0.3302970716827794
  batch 172 loss: 0.3304375548695409
  batch 173 loss: 0.33043977377042605
  batch 174 loss: 0.3304670562689332
  batch 175 loss: 0.3305842968395778
  batch 176 loss: 0.3302190207283605
  batch 177 loss: 0.3302036527186464
  batch 178 loss: 0.33028675833444915
  batch 179 loss: 0.3304439213688813
  batch 180 loss: 0.3303407775031196
  batch 181 loss: 0.33031294770662295
  batch 182 loss: 0.3301305877310889
  batch 183 loss: 0.32980122738848616
  batch 184 loss: 0.32959478040752205
  batch 185 loss: 0.32964315527194255
  batch 186 loss: 0.3298177570104599
  batch 187 loss: 0.32971546930425305
  batch 188 loss: 0.3293138598190977
  batch 189 loss: 0.32914568617861106
  batch 190 loss: 0.3292236637128027
  batch 191 loss: 0.32918832845088697
  batch 192 loss: 0.3291577436029911
  batch 193 loss: 0.3291463674350106
  batch 194 loss: 0.3292320382963751
  batch 195 loss: 0.3293461655959105
  batch 196 loss: 0.329087959564462
  batch 197 loss: 0.3288713164135889
  batch 198 loss: 0.3291380823862673
  batch 199 loss: 0.3291043636187836
  batch 200 loss: 0.32914748713374137
  batch 201 loss: 0.3293394925285928
  batch 202 loss: 0.3294194082517435
  batch 203 loss: 0.32928813443395305
  batch 204 loss: 0.32915256216245536
  batch 205 loss: 0.32926475260315874
  batch 206 loss: 0.3292084763061653
  batch 207 loss: 0.3292174373847851
  batch 208 loss: 0.32906748368763006
  batch 209 loss: 0.328850314947977
  batch 210 loss: 0.3289666208482924
  batch 211 loss: 0.3289204941824149
  batch 212 loss: 0.3287552328042264
  batch 213 loss: 0.3286255612059938
  batch 214 loss: 0.3285602267100432
  batch 215 loss: 0.3283600869566895
  batch 216 loss: 0.32809933619918646
  batch 217 loss: 0.3280623898528139
  batch 218 loss: 0.3281343497267557
  batch 219 loss: 0.32824634879691417
  batch 220 loss: 0.32815788320519707
  batch 221 loss: 0.3281613754201259
  batch 222 loss: 0.3282312051401482
  batch 223 loss: 0.3282283071445243
  batch 224 loss: 0.3281704147479364
  batch 225 loss: 0.3280411684513092
  batch 226 loss: 0.32799028730497953
  batch 227 loss: 0.32777442359714254
  batch 228 loss: 0.3276908100958456
  batch 229 loss: 0.3275894561709275
  batch 230 loss: 0.3276789983977442
  batch 231 loss: 0.3275430083790899
  batch 232 loss: 0.3273609809834382
  batch 233 loss: 0.32730924788974386
  batch 234 loss: 0.32748935428949505
  batch 235 loss: 0.3274772820320535
  batch 236 loss: 0.32732872702812743
  batch 237 loss: 0.3273370047158833
  batch 238 loss: 0.3272737143169932
  batch 239 loss: 0.3272167081364029
  batch 240 loss: 0.32716073654592037
  batch 241 loss: 0.32730387862292565
  batch 242 loss: 0.32713549885868043
  batch 243 loss: 0.32724091974796093
  batch 244 loss: 0.32720478294325656
  batch 245 loss: 0.32719132888073826
  batch 246 loss: 0.3271401444586312
  batch 247 loss: 0.3272351119441059
  batch 248 loss: 0.3272369421057163
  batch 249 loss: 0.327156932598137
  batch 250 loss: 0.32708397710323334
  batch 251 loss: 0.32711297439863957
  batch 252 loss: 0.32695794649540433
  batch 253 loss: 0.3267930611791347
  batch 254 loss: 0.3266752668487744
  batch 255 loss: 0.32661623943085766
  batch 256 loss: 0.3266173858428374
  batch 257 loss: 0.32652776981142245
  batch 258 loss: 0.32649214022843415
  batch 259 loss: 0.32641931021995985
  batch 260 loss: 0.32643020382294285
  batch 261 loss: 0.32638592646953246
  batch 262 loss: 0.3261922031413508
  batch 263 loss: 0.3262032550324052
  batch 264 loss: 0.32609618206818897
  batch 265 loss: 0.32599283996618017
  batch 266 loss: 0.32588006522422447
  batch 267 loss: 0.3259907965356491
  batch 268 loss: 0.3259143517978156
  batch 269 loss: 0.325890011171426
  batch 270 loss: 0.3259233388635847
  batch 271 loss: 0.3259122947925131
  batch 272 loss: 0.32593879103660583
  batch 273 loss: 0.32594381507499753
  batch 274 loss: 0.3259657179787211
  batch 275 loss: 0.3259677261655981
  batch 276 loss: 0.32594627001579257
  batch 277 loss: 0.3258341370076479
  batch 278 loss: 0.3256933454343741
  batch 279 loss: 0.325733969715761
  batch 280 loss: 0.32560285617198265
  batch 281 loss: 0.32545848567290664
  batch 282 loss: 0.3254028185673639
  batch 283 loss: 0.32536545753900237
  batch 284 loss: 0.3254339252139481
  batch 285 loss: 0.325335240991492
  batch 286 loss: 0.3254240535564356
  batch 287 loss: 0.3253657737883126
  batch 288 loss: 0.32518875050461954
  batch 289 loss: 0.32540302035305324
  batch 290 loss: 0.32521144213347597
  batch 291 loss: 0.3252639997865736
  batch 292 loss: 0.325291854265618
  batch 293 loss: 0.32527444702366515
  batch 294 loss: 0.3251106774523145
  batch 295 loss: 0.32511689834675545
  batch 296 loss: 0.32513591093388766
  batch 297 loss: 0.3251079155339135
  batch 298 loss: 0.3251900891929665
  batch 299 loss: 0.32507010955475646
  batch 300 loss: 0.32510105162858965
  batch 301 loss: 0.32508222852830476
  batch 302 loss: 0.32501127319225415
  batch 303 loss: 0.3249903624207273
  batch 304 loss: 0.3249775630078818
  batch 305 loss: 0.32479878957154323
  batch 306 loss: 0.3249301849042668
  batch 307 loss: 0.3249027763981773
  batch 308 loss: 0.325047159156242
  batch 309 loss: 0.32489032305560067
  batch 310 loss: 0.324865456358079
  batch 311 loss: 0.3249348538098228
  batch 312 loss: 0.3249905784733785
  batch 313 loss: 0.32492560614793065
  batch 314 loss: 0.3247898030243102
  batch 315 loss: 0.32469207124104577
  batch 316 loss: 0.3246384633681442
  batch 317 loss: 0.3245563154528946
  batch 318 loss: 0.3244225839211506
  batch 319 loss: 0.32430149871727515
  batch 320 loss: 0.32429672237485646
  batch 321 loss: 0.3243315724940315
  batch 322 loss: 0.32432501008791953
  batch 323 loss: 0.3242753056734339
  batch 324 loss: 0.32411863185741285
  batch 325 loss: 0.32406635366953335
  batch 326 loss: 0.3240538514647747
  batch 327 loss: 0.3240807278805187
  batch 328 loss: 0.3238657644790847
  batch 329 loss: 0.32394015453869085
  batch 330 loss: 0.3239010033282367
  batch 331 loss: 0.32388860293385485
  batch 332 loss: 0.3238188489732972
  batch 333 loss: 0.3238587815303344
  batch 334 loss: 0.3237326654666912
  batch 335 loss: 0.3235417419405126
  batch 336 loss: 0.323357041038218
  batch 337 loss: 0.3232582183195505
  batch 338 loss: 0.32327774255233405
  batch 339 loss: 0.32330816280876995
  batch 340 loss: 0.32324119858882006
  batch 341 loss: 0.32309003904068573
  batch 342 loss: 0.3230060718910039
  batch 343 loss: 0.32297203883137715
  batch 344 loss: 0.3229804553611334
  batch 345 loss: 0.3230631686639095
  batch 346 loss: 0.32287837694146043
  batch 347 loss: 0.32282415650763496
  batch 348 loss: 0.3228056301501976
  batch 349 loss: 0.3226888322386154
  batch 350 loss: 0.3226426627806255
  batch 351 loss: 0.3226214706727922
  batch 352 loss: 0.32266829442232847
  batch 353 loss: 0.3226382809218874
  batch 354 loss: 0.32279191616564823
  batch 355 loss: 0.32270299145873166
  batch 356 loss: 0.3226039870903733
  batch 357 loss: 0.3224947557729833
  batch 358 loss: 0.32255150535919147
  batch 359 loss: 0.3224927153760012
  batch 360 loss: 0.32241063324941527
  batch 361 loss: 0.3223642980127784
  batch 362 loss: 0.3222780834410072
  batch 363 loss: 0.3221514367860211
  batch 364 loss: 0.3220244619872544
  batch 365 loss: 0.32212043233113746
  batch 366 loss: 0.3220500323941799
  batch 367 loss: 0.3219553526155955
  batch 368 loss: 0.3218296103827331
  batch 369 loss: 0.32186895326224124
  batch 370 loss: 0.32179632927920365
  batch 371 loss: 0.3218244208961806
  batch 372 loss: 0.32175478026751547
  batch 373 loss: 0.32164007848772863
  batch 374 loss: 0.3215131588320044
  batch 375 loss: 0.3214347817103068
  batch 376 loss: 0.32149410208171986
  batch 377 loss: 0.321464249958094
  batch 378 loss: 0.32136507198293374
  batch 379 loss: 0.32137598966231135
  batch 380 loss: 0.32139881833603506
  batch 381 loss: 0.32132144557835235
  batch 382 loss: 0.3212176689302734
  batch 383 loss: 0.3211658150346721
  batch 384 loss: 0.32106046735619503
  batch 385 loss: 0.3211252003521114
  batch 386 loss: 0.32099863508513554
  batch 387 loss: 0.32104698485798305
  batch 388 loss: 0.3210409874307741
  batch 389 loss: 0.3210723754066428
  batch 390 loss: 0.3210087536237179
  batch 391 loss: 0.32109785308618377
  batch 392 loss: 0.3211318055281834
  batch 393 loss: 0.3211520493940543
  batch 394 loss: 0.32116076645209707
  batch 395 loss: 0.32124663266954545
  batch 396 loss: 0.32132779550973817
  batch 397 loss: 0.32135257395148575
  batch 398 loss: 0.3213376152755028
  batch 399 loss: 0.32149689754746613
  batch 400 loss: 0.3215499534457922
  batch 401 loss: 0.3214593143237202
  batch 402 loss: 0.3214976129247181
  batch 403 loss: 0.321641681359956
  batch 404 loss: 0.3217507888479988
  batch 405 loss: 0.32181990551359857
  batch 406 loss: 0.3219339361625352
  batch 407 loss: 0.321866991991493
  batch 408 loss: 0.321921943887776
  batch 409 loss: 0.32188911346176724
  batch 410 loss: 0.3219467184165629
  batch 411 loss: 0.32195092668788566
  batch 412 loss: 0.32187827162950944
  batch 413 loss: 0.3219323829357618
  batch 414 loss: 0.3218781858826605
  batch 415 loss: 0.32182721380727836
  batch 416 loss: 0.3218475411144587
  batch 417 loss: 0.32184439425845796
  batch 418 loss: 0.32177896540986295
  batch 419 loss: 0.32178398696505656
  batch 420 loss: 0.3216976265112559
  batch 421 loss: 0.3216198456966962
  batch 422 loss: 0.3217804166378003
  batch 423 loss: 0.3217619793493979
  batch 424 loss: 0.3217572533999974
  batch 425 loss: 0.3217349619725171
  batch 426 loss: 0.32168455484887243
  batch 427 loss: 0.32167942029810065
  batch 428 loss: 0.32161424441315306
  batch 429 loss: 0.3216226222492876
  batch 430 loss: 0.32154219698074255
  batch 431 loss: 0.32162696925227435
  batch 432 loss: 0.3216927570325357
  batch 433 loss: 0.3216510008306481
  batch 434 loss: 0.321665763168291
  batch 435 loss: 0.3215778095283727
  batch 436 loss: 0.3215523053199873
  batch 437 loss: 0.32164028170998094
  batch 438 loss: 0.3217062462274342
  batch 439 loss: 0.3216828473743925
  batch 440 loss: 0.3217168713835153
  batch 441 loss: 0.32163582732077356
  batch 442 loss: 0.3215329570198491
  batch 443 loss: 0.32150152863821113
  batch 444 loss: 0.32142330041608297
  batch 445 loss: 0.3214529196867782
  batch 446 loss: 0.32140881785362824
  batch 447 loss: 0.3213157393788331
  batch 448 loss: 0.3212906980354871
  batch 449 loss: 0.3213077337545382
  batch 450 loss: 0.3212361964914534
  batch 451 loss: 0.3211997331774684
  batch 452 loss: 0.3212263584136963
  batch 453 loss: 0.32124497611527936
  batch 454 loss: 0.3212931242808371
  batch 455 loss: 0.32127499134985954
  batch 456 loss: 0.32129940612797153
  batch 457 loss: 0.3212450026683265
  batch 458 loss: 0.3211733977997667
  batch 459 loss: 0.32124921679496765
  batch 460 loss: 0.3212997647083324
  batch 461 loss: 0.32129726363366184
  batch 462 loss: 0.32130343431518193
  batch 463 loss: 0.32124571099909544
  batch 464 loss: 0.3212783124683232
  batch 465 loss: 0.32121316252216214
  batch 466 loss: 0.32104885763620616
  batch 467 loss: 0.32114819292082775
  batch 468 loss: 0.3211053679259414
  batch 469 loss: 0.3212811965932216
  batch 470 loss: 0.32119904325363485
  batch 471 loss: 0.3211823742986991
  batch 472 loss: 0.32105309509877433
LOSS train 0.32105309509877433 valid 0.2339247763156891
LOSS train 0.32105309509877433 valid 0.22648131847381592
LOSS train 0.32105309509877433 valid 0.22608208656311035
LOSS train 0.32105309509877433 valid 0.22500250861048698
LOSS train 0.32105309509877433 valid 0.22205840647220612
LOSS train 0.32105309509877433 valid 0.22488388667503992
LOSS train 0.32105309509877433 valid 0.23487985346998488
LOSS train 0.32105309509877433 valid 0.23463845998048782
LOSS train 0.32105309509877433 valid 0.23279956976572672
LOSS train 0.32105309509877433 valid 0.23261848986148834
LOSS train 0.32105309509877433 valid 0.23103869638659738
LOSS train 0.32105309509877433 valid 0.23150760928789774
LOSS train 0.32105309509877433 valid 0.2305263727903366
LOSS train 0.32105309509877433 valid 0.23106266132422856
LOSS train 0.32105309509877433 valid 0.2278638501962026
LOSS train 0.32105309509877433 valid 0.22844685800373554
LOSS train 0.32105309509877433 valid 0.22964777490671942
LOSS train 0.32105309509877433 valid 0.23047850281000137
LOSS train 0.32105309509877433 valid 0.2322860195448524
LOSS train 0.32105309509877433 valid 0.23161672353744506
LOSS train 0.32105309509877433 valid 0.2305804356223061
LOSS train 0.32105309509877433 valid 0.22886430472135544
LOSS train 0.32105309509877433 valid 0.22842616231545157
LOSS train 0.32105309509877433 valid 0.22753568614522615
LOSS train 0.32105309509877433 valid 0.22635920822620392
LOSS train 0.32105309509877433 valid 0.22640886501624033
LOSS train 0.32105309509877433 valid 0.22620399020336293
LOSS train 0.32105309509877433 valid 0.22673551152859414
LOSS train 0.32105309509877433 valid 0.22685187284288735
LOSS train 0.32105309509877433 valid 0.2275423804918925
LOSS train 0.32105309509877433 valid 0.22817169658599362
LOSS train 0.32105309509877433 valid 0.22766107693314552
LOSS train 0.32105309509877433 valid 0.22890901746171893
LOSS train 0.32105309509877433 valid 0.22884495293392854
LOSS train 0.32105309509877433 valid 0.23023741500718253
LOSS train 0.32105309509877433 valid 0.2295619286596775
LOSS train 0.32105309509877433 valid 0.22964572262119604
LOSS train 0.32105309509877433 valid 0.23058942509324928
LOSS train 0.32105309509877433 valid 0.23017420027500543
LOSS train 0.32105309509877433 valid 0.2302210159599781
LOSS train 0.32105309509877433 valid 0.2308832370653385
LOSS train 0.32105309509877433 valid 0.2310373917931602
LOSS train 0.32105309509877433 valid 0.23073935785958932
LOSS train 0.32105309509877433 valid 0.2310242009433833
LOSS train 0.32105309509877433 valid 0.23074293235937754
LOSS train 0.32105309509877433 valid 0.23107369373673978
LOSS train 0.32105309509877433 valid 0.2317942716973893
LOSS train 0.32105309509877433 valid 0.2321097245439887
LOSS train 0.32105309509877433 valid 0.23230741644392208
LOSS train 0.32105309509877433 valid 0.23143260151147843
LOSS train 0.32105309509877433 valid 0.23166797353940852
LOSS train 0.32105309509877433 valid 0.23143878063330284
LOSS train 0.32105309509877433 valid 0.23143110747607248
LOSS train 0.32105309509877433 valid 0.23135373686198835
LOSS train 0.32105309509877433 valid 0.23120551840825515
LOSS train 0.32105309509877433 valid 0.23091630557818071
LOSS train 0.32105309509877433 valid 0.23054881681475722
LOSS train 0.32105309509877433 valid 0.2303500884565814
LOSS train 0.32105309509877433 valid 0.23066011424792016
LOSS train 0.32105309509877433 valid 0.2306763047973315
LOSS train 0.32105309509877433 valid 0.23029504349974336
LOSS train 0.32105309509877433 valid 0.23109049470193924
LOSS train 0.32105309509877433 valid 0.23099533409353287
LOSS train 0.32105309509877433 valid 0.23184966458939016
LOSS train 0.32105309509877433 valid 0.23209183857991145
LOSS train 0.32105309509877433 valid 0.23204630974567297
LOSS train 0.32105309509877433 valid 0.2312117472513398
LOSS train 0.32105309509877433 valid 0.23112387495005832
LOSS train 0.32105309509877433 valid 0.23054612697898477
LOSS train 0.32105309509877433 valid 0.2304976565497262
LOSS train 0.32105309509877433 valid 0.23022986759602185
LOSS train 0.32105309509877433 valid 0.23031822302275234
LOSS train 0.32105309509877433 valid 0.23032112807443697
LOSS train 0.32105309509877433 valid 0.23010550056760376
LOSS train 0.32105309509877433 valid 0.2301578172047933
LOSS train 0.32105309509877433 valid 0.23032239471611224
LOSS train 0.32105309509877433 valid 0.2303652742079326
LOSS train 0.32105309509877433 valid 0.23042643929903323
LOSS train 0.32105309509877433 valid 0.23037687830532652
LOSS train 0.32105309509877433 valid 0.22985178828239441
LOSS train 0.32105309509877433 valid 0.22910677871586363
LOSS train 0.32105309509877433 valid 0.22931200556638764
LOSS train 0.32105309509877433 valid 0.22920339707150517
LOSS train 0.32105309509877433 valid 0.22924713754937762
LOSS train 0.32105309509877433 valid 0.2290082153152017
LOSS train 0.32105309509877433 valid 0.22834585589724918
LOSS train 0.32105309509877433 valid 0.22828876013043284
LOSS train 0.32105309509877433 valid 0.2276729175990278
LOSS train 0.32105309509877433 valid 0.22809992579931623
LOSS train 0.32105309509877433 valid 0.22821563316716087
LOSS train 0.32105309509877433 valid 0.22830990284353822
LOSS train 0.32105309509877433 valid 0.2285112566922022
LOSS train 0.32105309509877433 valid 0.22834039823983304
LOSS train 0.32105309509877433 valid 0.22834123544236448
LOSS train 0.32105309509877433 valid 0.22822280353621432
LOSS train 0.32105309509877433 valid 0.22838381119072437
LOSS train 0.32105309509877433 valid 0.2285741206604181
LOSS train 0.32105309509877433 valid 0.22870475646792626
LOSS train 0.32105309509877433 valid 0.2286896872701067
LOSS train 0.32105309509877433 valid 0.22889130905270577
LOSS train 0.32105309509877433 valid 0.22917496376108415
LOSS train 0.32105309509877433 valid 0.22918657753981797
LOSS train 0.32105309509877433 valid 0.22900999026391114
LOSS train 0.32105309509877433 valid 0.22898726165294647
LOSS train 0.32105309509877433 valid 0.22908137369723547
LOSS train 0.32105309509877433 valid 0.2293650596490446
LOSS train 0.32105309509877433 valid 0.229123421918566
LOSS train 0.32105309509877433 valid 0.22912489415870774
LOSS train 0.32105309509877433 valid 0.22942518463375372
LOSS train 0.32105309509877433 valid 0.22966729009693318
LOSS train 0.32105309509877433 valid 0.22924581008988457
LOSS train 0.32105309509877433 valid 0.2291410807520151
LOSS train 0.32105309509877433 valid 0.2291739244112926
LOSS train 0.32105309509877433 valid 0.22910003377157345
LOSS train 0.32105309509877433 valid 0.22916439706864564
LOSS train 0.32105309509877433 valid 0.22928089609947697
LOSS train 0.32105309509877433 valid 0.22943342699963823
LOSS train 0.32105309509877433 valid 0.22951727171065445
LOSS train 0.32105309509877433 valid 0.2294375692345515
LOSS train 0.32105309509877433 valid 0.22916077338159085
LOSS train 0.32105309509877433 valid 0.22900346783567066
LOSS train 0.32105309509877433 valid 0.22885676192455603
LOSS train 0.32105309509877433 valid 0.2287384648390902
LOSS train 0.32105309509877433 valid 0.2290117592821198
LOSS train 0.32105309509877433 valid 0.22895863354206086
LOSS train 0.32105309509877433 valid 0.22917179656880243
LOSS train 0.32105309509877433 valid 0.22904192615212418
LOSS train 0.32105309509877433 valid 0.22933446581009775
LOSS train 0.32105309509877433 valid 0.2294264423061711
LOSS train 0.32105309509877433 valid 0.22938381399099644
LOSS train 0.32105309509877433 valid 0.2293243495790103
LOSS train 0.32105309509877433 valid 0.2291415591131557
LOSS train 0.32105309509877433 valid 0.22905030420848302
LOSS train 0.32105309509877433 valid 0.22915566479092214
LOSS train 0.32105309509877433 valid 0.2290203803115421
LOSS train 0.32105309509877433 valid 0.2288729083888671
LOSS train 0.32105309509877433 valid 0.22871563382392382
LOSS train 0.32105309509877433 valid 0.2286565553234971
LOSS train 0.32105309509877433 valid 0.22854272796095704
LOSS train 0.32105309509877433 valid 0.22863049347485814
LOSS train 0.32105309509877433 valid 0.2286252073996456
LOSS train 0.32105309509877433 valid 0.2288413758219128
LOSS train 0.32105309509877433 valid 0.22876663245521225
LOSS train 0.32105309509877433 valid 0.22875400798188317
LOSS train 0.32105309509877433 valid 0.22851981582312747
LOSS train 0.32105309509877433 valid 0.22869050278239053
LOSS train 0.32105309509877433 valid 0.22844725451907333
LOSS train 0.32105309509877433 valid 0.22900741497004354
LOSS train 0.32105309509877433 valid 0.22912980996122295
LOSS train 0.32105309509877433 valid 0.22906434804201126
LOSS train 0.32105309509877433 valid 0.22917492766648728
LOSS train 0.32105309509877433 valid 0.22895432293022933
LOSS train 0.32105309509877433 valid 0.2290188097486309
LOSS train 0.32105309509877433 valid 0.2288819415035186
LOSS train 0.32105309509877433 valid 0.22896229653589187
LOSS train 0.32105309509877433 valid 0.2291332873969506
LOSS train 0.32105309509877433 valid 0.2290257997573561
LOSS train 0.32105309509877433 valid 0.2292259343062775
LOSS train 0.32105309509877433 valid 0.22921806133393222
LOSS train 0.32105309509877433 valid 0.22918960619717837
LOSS train 0.32105309509877433 valid 0.22909355533789405
LOSS train 0.32105309509877433 valid 0.22905967467361027
LOSS train 0.32105309509877433 valid 0.22894338175562992
LOSS train 0.32105309509877433 valid 0.22867462684105083
LOSS train 0.32105309509877433 valid 0.22860902900045568
LOSS train 0.32105309509877433 valid 0.22871008416615338
LOSS train 0.32105309509877433 valid 0.22893379271744255
LOSS train 0.32105309509877433 valid 0.22884338872418516
LOSS train 0.32105309509877433 valid 0.22903277508958558
LOSS train 0.32105309509877433 valid 0.22915502997005688
LOSS train 0.32105309509877433 valid 0.22908542065592538
LOSS train 0.32105309509877433 valid 0.22901915628896202
LOSS train 0.32105309509877433 valid 0.22911795396680776
LOSS train 0.32105309509877433 valid 0.22910552514695573
LOSS train 0.32105309509877433 valid 0.22899211040564946
LOSS train 0.32105309509877433 valid 0.2288776964626529
LOSS train 0.32105309509877433 valid 0.22882292171319327
LOSS train 0.32105309509877433 valid 0.22898728400468826
LOSS train 0.32105309509877433 valid 0.22889388431716898
LOSS train 0.32105309509877433 valid 0.22895347972710928
LOSS train 0.32105309509877433 valid 0.2291283582784853
LOSS train 0.32105309509877433 valid 0.22911659865588932
LOSS train 0.32105309509877433 valid 0.2290031312267637
LOSS train 0.32105309509877433 valid 0.2290264270875765
LOSS train 0.32105309509877433 valid 0.22881864430131138
LOSS train 0.32105309509877433 valid 0.2287793771554065
LOSS train 0.32105309509877433 valid 0.22876176349619493
LOSS train 0.32105309509877433 valid 0.2287642072014352
LOSS train 0.32105309509877433 valid 0.2287821885612276
LOSS train 0.32105309509877433 valid 0.22887889295816422
LOSS train 0.32105309509877433 valid 0.22885622669264907
LOSS train 0.32105309509877433 valid 0.22893186775036156
LOSS train 0.32105309509877433 valid 0.22889874010814903
LOSS train 0.32105309509877433 valid 0.22883119234411986
LOSS train 0.32105309509877433 valid 0.2287033961369441
LOSS train 0.32105309509877433 valid 0.22874103554961633
LOSS train 0.32105309509877433 valid 0.2288893871954855
LOSS train 0.32105309509877433 valid 0.22872730532679894
LOSS train 0.32105309509877433 valid 0.22882025715094714
LOSS train 0.32105309509877433 valid 0.22874874226748942
LOSS train 0.32105309509877433 valid 0.22857858796617878
LOSS train 0.32105309509877433 valid 0.22848996829868543
LOSS train 0.32105309509877433 valid 0.22839990397685855
LOSS train 0.32105309509877433 valid 0.22836867040571043
LOSS train 0.32105309509877433 valid 0.22818941154131075
LOSS train 0.32105309509877433 valid 0.2281855057599475
LOSS train 0.32105309509877433 valid 0.22809036723945453
LOSS train 0.32105309509877433 valid 0.22793782488084757
LOSS train 0.32105309509877433 valid 0.2278094227519332
LOSS train 0.32105309509877433 valid 0.2278090952407746
LOSS train 0.32105309509877433 valid 0.22790929251372533
LOSS train 0.32105309509877433 valid 0.2278933860361576
LOSS train 0.32105309509877433 valid 0.22796827239889494
LOSS train 0.32105309509877433 valid 0.22795907052877906
LOSS train 0.32105309509877433 valid 0.22787919737571893
LOSS train 0.32105309509877433 valid 0.22789103365330785
LOSS train 0.32105309509877433 valid 0.22791407748301457
LOSS train 0.32105309509877433 valid 0.22796159039396757
LOSS train 0.32105309509877433 valid 0.2280352493943689
LOSS train 0.32105309509877433 valid 0.22809928398240698
LOSS train 0.32105309509877433 valid 0.22818779722867508
LOSS train 0.32105309509877433 valid 0.22813948894943203
LOSS train 0.32105309509877433 valid 0.22827594550201177
LOSS train 0.32105309509877433 valid 0.22841652469443424
LOSS train 0.32105309509877433 valid 0.22844896846347384
LOSS train 0.32105309509877433 valid 0.22854043576305946
LOSS train 0.32105309509877433 valid 0.22877956939951438
LOSS train 0.32105309509877433 valid 0.2288947194945394
LOSS train 0.32105309509877433 valid 0.22897009310763997
LOSS train 0.32105309509877433 valid 0.22904917286789936
LOSS train 0.32105309509877433 valid 0.2290779754712984
LOSS train 0.32105309509877433 valid 0.22910957423777417
LOSS train 0.32105309509877433 valid 0.22905334177958608
LOSS train 0.32105309509877433 valid 0.22905165530168092
LOSS train 0.32105309509877433 valid 0.2291010261216062
LOSS train 0.32105309509877433 valid 0.2290116078141382
LOSS train 0.32105309509877433 valid 0.22910426255268387
LOSS train 0.32105309509877433 valid 0.22909076129939376
LOSS train 0.32105309509877433 valid 0.228983652205148
LOSS train 0.32105309509877433 valid 0.22891937860598166
LOSS train 0.32105309509877433 valid 0.22896019867099668
LOSS train 0.32105309509877433 valid 0.22874285132924388
LOSS train 0.32105309509877433 valid 0.2288802826600801
LOSS train 0.32105309509877433 valid 0.22905958724803613
LOSS train 0.32105309509877433 valid 0.22918716036543554
LOSS train 0.32105309509877433 valid 0.2291611990308374
LOSS train 0.32105309509877433 valid 0.22919162142614605
LOSS train 0.32105309509877433 valid 0.22912710814947082
LOSS train 0.32105309509877433 valid 0.2290366740231533
LOSS train 0.32105309509877433 valid 0.22918251663446426
LOSS train 0.32105309509877433 valid 0.2291467272309193
LOSS train 0.32105309509877433 valid 0.2292710000442134
LOSS train 0.32105309509877433 valid 0.22921071852384348
LOSS train 0.32105309509877433 valid 0.22915559713765393
LOSS train 0.32105309509877433 valid 0.22925054582895016
LOSS train 0.32105309509877433 valid 0.22924892662558705
LOSS train 0.32105309509877433 valid 0.22914581197940886
LOSS train 0.32105309509877433 valid 0.2291942691618158
LOSS train 0.32105309509877433 valid 0.22917064207401056
LOSS train 0.32105309509877433 valid 0.22913887449181997
LOSS train 0.32105309509877433 valid 0.22922673975599223
LOSS train 0.32105309509877433 valid 0.2292844619004781
LOSS train 0.32105309509877433 valid 0.22927385710264794
LOSS train 0.32105309509877433 valid 0.2292791989377954
LOSS train 0.32105309509877433 valid 0.2292058621375066
LOSS train 0.32105309509877433 valid 0.22922232214893615
LOSS train 0.32105309509877433 valid 0.2293007243773464
LOSS train 0.32105309509877433 valid 0.22948664699250193
LOSS train 0.32105309509877433 valid 0.22957560292628618
LOSS train 0.32105309509877433 valid 0.22956317834280154
LOSS train 0.32105309509877433 valid 0.2296143327032068
LOSS train 0.32105309509877433 valid 0.22973940839224002
LOSS train 0.32105309509877433 valid 0.22977839950676804
LOSS train 0.32105309509877433 valid 0.2297520700597415
LOSS train 0.32105309509877433 valid 0.2297071813995188
LOSS train 0.32105309509877433 valid 0.22955332383297491
LOSS train 0.32105309509877433 valid 0.2293792335780519
LOSS train 0.32105309509877433 valid 0.2292377975025623
LOSS train 0.32105309509877433 valid 0.2292251803221241
LOSS train 0.32105309509877433 valid 0.22914837497685636
LOSS train 0.32105309509877433 valid 0.22901232848175904
LOSS train 0.32105309509877433 valid 0.22890655802074053
LOSS train 0.32105309509877433 valid 0.22888498934966514
LOSS train 0.32105309509877433 valid 0.22893179575322378
LOSS train 0.32105309509877433 valid 0.22895834273413607
LOSS train 0.32105309509877433 valid 0.22890302429457646
LOSS train 0.32105309509877433 valid 0.2288083838565009
LOSS train 0.32105309509877433 valid 0.2288294946257439
LOSS train 0.32105309509877433 valid 0.2288474698895814
LOSS train 0.32105309509877433 valid 0.22882852019934818
LOSS train 0.32105309509877433 valid 0.22882415369613884
LOSS train 0.32105309509877433 valid 0.22887678786295734
LOSS train 0.32105309509877433 valid 0.2288787897434658
LOSS train 0.32105309509877433 valid 0.22899716823887664
LOSS train 0.32105309509877433 valid 0.22905628135648826
LOSS train 0.32105309509877433 valid 0.2290277543502885
LOSS train 0.32105309509877433 valid 0.22905468454063943
LOSS train 0.32105309509877433 valid 0.22903721629573195
LOSS train 0.32105309509877433 valid 0.2290684209519804
LOSS train 0.32105309509877433 valid 0.22904912268122038
LOSS train 0.32105309509877433 valid 0.22908639511792764
LOSS train 0.32105309509877433 valid 0.22911713484501997
LOSS train 0.32105309509877433 valid 0.22915103352896057
LOSS train 0.32105309509877433 valid 0.2291353627745258
LOSS train 0.32105309509877433 valid 0.2290270019750126
LOSS train 0.32105309509877433 valid 0.2289679457549176
LOSS train 0.32105309509877433 valid 0.22892785897472395
LOSS train 0.32105309509877433 valid 0.22885430991262584
LOSS train 0.32105309509877433 valid 0.2288991458786344
LOSS train 0.32105309509877433 valid 0.22891422639931402
LOSS train 0.32105309509877433 valid 0.22884020669283975
LOSS train 0.32105309509877433 valid 0.22887108193185085
LOSS train 0.32105309509877433 valid 0.2288916171454012
LOSS train 0.32105309509877433 valid 0.22896430897674744
LOSS train 0.32105309509877433 valid 0.22898829896298667
LOSS train 0.32105309509877433 valid 0.22895554113614408
LOSS train 0.32105309509877433 valid 0.22899512998313182
LOSS train 0.32105309509877433 valid 0.2290643793794344
LOSS train 0.32105309509877433 valid 0.22914497998067204
LOSS train 0.32105309509877433 valid 0.229073603823781
LOSS train 0.32105309509877433 valid 0.22909426967674326
LOSS train 0.32105309509877433 valid 0.22900571618576226
LOSS train 0.32105309509877433 valid 0.2289145826924327
LOSS train 0.32105309509877433 valid 0.22887933378418288
LOSS train 0.32105309509877433 valid 0.22892571637263665
LOSS train 0.32105309509877433 valid 0.22899380934201866
LOSS train 0.32105309509877433 valid 0.22898538747876426
LOSS train 0.32105309509877433 valid 0.22895851213394142
LOSS train 0.32105309509877433 valid 0.22903097155971006
LOSS train 0.32105309509877433 valid 0.22898623487263015
LOSS train 0.32105309509877433 valid 0.22890154634717727
LOSS train 0.32105309509877433 valid 0.22877003939216395
LOSS train 0.32105309509877433 valid 0.2287999257758573
LOSS train 0.32105309509877433 valid 0.22892612455312364
LOSS train 0.32105309509877433 valid 0.22888448073792814
LOSS train 0.32105309509877433 valid 0.22884720430842467
LOSS train 0.32105309509877433 valid 0.2288311384728825
LOSS train 0.32105309509877433 valid 0.2288151170022389
LOSS train 0.32105309509877433 valid 0.22875340335664496
LOSS train 0.32105309509877433 valid 0.2287563776706948
LOSS train 0.32105309509877433 valid 0.22864166815434733
LOSS train 0.32105309509877433 valid 0.2286484796139929
LOSS train 0.32105309509877433 valid 0.2286517037073308
LOSS train 0.32105309509877433 valid 0.22877436573075693
LOSS train 0.32105309509877433 valid 0.22882752574008444
LOSS train 0.32105309509877433 valid 0.22880783870902366
LOSS train 0.32105309509877433 valid 0.22868828741720842
LOSS train 0.32105309509877433 valid 0.22861276768233585
LOSS train 0.32105309509877433 valid 0.2286566496322353
LOSS train 0.32105309509877433 valid 0.2285741349203246
LOSS train 0.32105309509877433 valid 0.22851272139623974
LOSS train 0.32105309509877433 valid 0.22852332356639884
LOSS train 0.32105309509877433 valid 0.228602087278204
LOSS train 0.32105309509877433 valid 0.22867827605729724
LOSS train 0.32105309509877433 valid 0.22876135001719838
LOSS train 0.32105309509877433 valid 0.22885867124528028
LOSS train 0.32105309509877433 valid 0.22879756674045273
LOSS train 0.32105309509877433 valid 0.22877055181804315
LOSS train 0.32105309509877433 valid 0.2287814554065715
LOSS train 0.32105309509877433 valid 0.22875942616826958
LOSS train 0.32105309509877433 valid 0.22876042821070494
LOSS train 0.32105309509877433 valid 0.22880748050153585
LOSS train 0.32105309509877433 valid 0.22868170074030716
LOSS train 0.32105309509877433 valid 0.22869918912976653
LOSS train 0.32105309509877433 valid 0.2286792665719986
LOSS train 0.32105309509877433 valid 0.2286290700644092
LOSS train 0.32105309509877433 valid 0.22854020472930628
LOSS train 0.32105309509877433 valid 0.2285067217586481
LOSS train 0.32105309509877433 valid 0.22860889928289221
EPOCH 4:
  batch 1 loss: 0.29075443744659424
  batch 2 loss: 0.28666409850120544
  batch 3 loss: 0.2957485318183899
  batch 4 loss: 0.2981085032224655
  batch 5 loss: 0.30928668975830076
  batch 6 loss: 0.3059374888737996
  batch 7 loss: 0.31022416268076214
  batch 8 loss: 0.31136440858244896
  batch 9 loss: 0.3124990463256836
  batch 10 loss: 0.3134882926940918
  batch 11 loss: 0.3139112347906286
  batch 12 loss: 0.3113427435358365
  batch 13 loss: 0.30944453523709226
  batch 14 loss: 0.3103465586900711
  batch 15 loss: 0.3119362990061442
  batch 16 loss: 0.3127541355788708
  batch 17 loss: 0.3099468560779796
  batch 18 loss: 0.3111402773194843
  batch 19 loss: 0.310642294193569
  batch 20 loss: 0.3089106634259224
  batch 21 loss: 0.3097735018957229
  batch 22 loss: 0.31005081127990375
  batch 23 loss: 0.30974853945815045
  batch 24 loss: 0.3080086149275303
  batch 25 loss: 0.31060906767845153
  batch 26 loss: 0.3087415431554501
  batch 27 loss: 0.3094987129723584
  batch 28 loss: 0.30826092937162947
  batch 29 loss: 0.3091852171667691
  batch 30 loss: 0.30855164925257367
  batch 31 loss: 0.30881808361699503
  batch 32 loss: 0.3081582812592387
  batch 33 loss: 0.3082170477419188
  batch 34 loss: 0.3084934976171045
  batch 35 loss: 0.3091161395822253
  batch 36 loss: 0.3094981883962949
  batch 37 loss: 0.31046034919249044
  batch 38 loss: 0.3111759337939714
  batch 39 loss: 0.31106393535931903
  batch 40 loss: 0.31092014238238336
  batch 41 loss: 0.31085116252666567
  batch 42 loss: 0.3113804849840346
  batch 43 loss: 0.31223794679309047
  batch 44 loss: 0.31221732564947824
  batch 45 loss: 0.3118645542197757
  batch 46 loss: 0.31209089250668237
  batch 47 loss: 0.31184388792261164
  batch 48 loss: 0.3109033325066169
  batch 49 loss: 0.3104754911393535
  batch 50 loss: 0.3101511371135712
  batch 51 loss: 0.3099786685962303
  batch 52 loss: 0.3103663818194316
  batch 53 loss: 0.3098948378607912
  batch 54 loss: 0.31020312507947284
  batch 55 loss: 0.3100028336048126
  batch 56 loss: 0.30953478600297657
  batch 57 loss: 0.30897833851345796
  batch 58 loss: 0.3096448445114596
  batch 59 loss: 0.30938383229708266
  batch 60 loss: 0.3089651828010877
  batch 61 loss: 0.30916959291598833
  batch 62 loss: 0.3098407849188774
  batch 63 loss: 0.3095769863280039
  batch 64 loss: 0.31048482144251466
  batch 65 loss: 0.3101745380805089
  batch 66 loss: 0.30989346540335455
  batch 67 loss: 0.31041967601918463
  batch 68 loss: 0.31073695070603313
  batch 69 loss: 0.31044219715007837
  batch 70 loss: 0.310564449429512
  batch 71 loss: 0.310021612006174
  batch 72 loss: 0.3102954472932551
  batch 73 loss: 0.310634096191354
  batch 74 loss: 0.31083477308621277
  batch 75 loss: 0.3103420078754425
  batch 76 loss: 0.3104293867945671
  batch 77 loss: 0.31048087562833515
  batch 78 loss: 0.31048580889518446
  batch 79 loss: 0.31075867369205135
  batch 80 loss: 0.31057925410568715
  batch 81 loss: 0.31081129481763015
  batch 82 loss: 0.3113267294517377
  batch 83 loss: 0.31146954951516115
  batch 84 loss: 0.31097463837691713
  batch 85 loss: 0.310431350329343
  batch 86 loss: 0.3111356278491575
  batch 87 loss: 0.3110035113219557
  batch 88 loss: 0.31055745448578487
  batch 89 loss: 0.3106404666820269
  batch 90 loss: 0.3103999372985628
  batch 91 loss: 0.31038206488221554
  batch 92 loss: 0.3101753339819286
  batch 93 loss: 0.31009547524554754
  batch 94 loss: 0.3104884573753844
  batch 95 loss: 0.31043021365215906
  batch 96 loss: 0.31068273168057203
  batch 97 loss: 0.31127764361420857
  batch 98 loss: 0.31161865135844874
  batch 99 loss: 0.31190689585425635
  batch 100 loss: 0.3118214616179466
  batch 101 loss: 0.31191653337809117
  batch 102 loss: 0.3122187873312071
  batch 103 loss: 0.31250260582247985
  batch 104 loss: 0.31247200071811676
  batch 105 loss: 0.3122178247996739
  batch 106 loss: 0.3126560905069675
  batch 107 loss: 0.31211838655382673
  batch 108 loss: 0.31198891859363626
  batch 109 loss: 0.31163942212358525
  batch 110 loss: 0.3117956898429177
  batch 111 loss: 0.31139199744473706
  batch 112 loss: 0.3109419210148709
  batch 113 loss: 0.3112063215369672
  batch 114 loss: 0.3115177774115613
  batch 115 loss: 0.3119090448255124
  batch 116 loss: 0.312007790752526
  batch 117 loss: 0.3120910573718894
  batch 118 loss: 0.3118644192562265
  batch 119 loss: 0.31203519797124785
  batch 120 loss: 0.31186436836918197
  batch 121 loss: 0.3117362613027746
  batch 122 loss: 0.3118034945648225
  batch 123 loss: 0.3118455901863129
  batch 124 loss: 0.3121395632624626
  batch 125 loss: 0.3121768100261688
  batch 126 loss: 0.3120290171059351
  batch 127 loss: 0.31249151736732544
  batch 128 loss: 0.3123629312030971
  batch 129 loss: 0.31253938573275425
  batch 130 loss: 0.312534092939817
  batch 131 loss: 0.3127030830346901
  batch 132 loss: 0.3125195801258087
  batch 133 loss: 0.3126784948478068
  batch 134 loss: 0.31263454258441925
  batch 135 loss: 0.3126042483029542
  batch 136 loss: 0.31261297665974674
  batch 137 loss: 0.3125266443638906
  batch 138 loss: 0.312443095056907
  batch 139 loss: 0.3129619203454299
  batch 140 loss: 0.31281043865850994
  batch 141 loss: 0.31297037787471255
  batch 142 loss: 0.3128920703286856
  batch 143 loss: 0.3127702597554747
  batch 144 loss: 0.3127446033888393
  batch 145 loss: 0.31260833493594464
  batch 146 loss: 0.3126242254694847
  batch 147 loss: 0.313029733239388
  batch 148 loss: 0.31285506305662364
  batch 149 loss: 0.31260857126056746
  batch 150 loss: 0.3127042484283447
  batch 151 loss: 0.3127664838800367
  batch 152 loss: 0.31302770835004357
  batch 153 loss: 0.31295829875017306
  batch 154 loss: 0.31304666806351056
  batch 155 loss: 0.3129909667276567
  batch 156 loss: 0.31284752086951184
  batch 157 loss: 0.3130136232846861
  batch 158 loss: 0.31302073390423496
  batch 159 loss: 0.3129673776386669
  batch 160 loss: 0.31276169288903477
  batch 161 loss: 0.31288975958498366
  batch 162 loss: 0.3128323976272418
  batch 163 loss: 0.3126379079613949
  batch 164 loss: 0.3128860509250222
  batch 165 loss: 0.3125897407531738
  batch 166 loss: 0.3125386523554124
  batch 167 loss: 0.3125493074962479
  batch 168 loss: 0.31259010022594813
  batch 169 loss: 0.31248119173670663
  batch 170 loss: 0.31222844860133003
  batch 171 loss: 0.31222399441819443
  batch 172 loss: 0.31217672190693924
  batch 173 loss: 0.31213660960252576
  batch 174 loss: 0.312179341055881
  batch 175 loss: 0.31229213918958393
  batch 176 loss: 0.3119795010848479
  batch 177 loss: 0.31193948583414327
  batch 178 loss: 0.31203477319036976
  batch 179 loss: 0.3122289896677326
  batch 180 loss: 0.31205934501356547
  batch 181 loss: 0.31210335858619015
  batch 182 loss: 0.31195061219917547
  batch 183 loss: 0.3116661640789991
  batch 184 loss: 0.31151839783010277
  batch 185 loss: 0.3116217766259168
  batch 186 loss: 0.3117657294196467
  batch 187 loss: 0.3117086052257109
  batch 188 loss: 0.3113132052599116
  batch 189 loss: 0.31110265582957597
  batch 190 loss: 0.3111879446004566
  batch 191 loss: 0.31111308370585217
  batch 192 loss: 0.3111002550770839
  batch 193 loss: 0.31110017240973953
  batch 194 loss: 0.31129103576399614
  batch 195 loss: 0.3113110545354012
  batch 196 loss: 0.31106614427907125
  batch 197 loss: 0.31088864288959406
  batch 198 loss: 0.3111123459206687
  batch 199 loss: 0.31103213573220984
  batch 200 loss: 0.3111485634744167
  batch 201 loss: 0.3112379351065527
  batch 202 loss: 0.31126660639696785
  batch 203 loss: 0.3111154911259712
  batch 204 loss: 0.3109295917492287
  batch 205 loss: 0.31102364266790994
  batch 206 loss: 0.31094260325709594
  batch 207 loss: 0.3110296600970669
  batch 208 loss: 0.31083502204945457
  batch 209 loss: 0.3106104429258684
  batch 210 loss: 0.3107466288975307
  batch 211 loss: 0.31069417771004953
  batch 212 loss: 0.31063725110494866
  batch 213 loss: 0.31049604427086913
  batch 214 loss: 0.31045931229524526
  batch 215 loss: 0.3102427349534146
  batch 216 loss: 0.3100386130864973
  batch 217 loss: 0.3100330446447645
  batch 218 loss: 0.31008211844558015
  batch 219 loss: 0.3102101487656162
  batch 220 loss: 0.3101717153733427
  batch 221 loss: 0.31020277266588686
  batch 222 loss: 0.3103980340130694
  batch 223 loss: 0.3103846713833745
  batch 224 loss: 0.31030109910560505
  batch 225 loss: 0.3101896611849467
  batch 226 loss: 0.31023916312023603
  batch 227 loss: 0.31006394910917406
  batch 228 loss: 0.30992300207154794
  batch 229 loss: 0.3098435197594905
  batch 230 loss: 0.3098522028197413
  batch 231 loss: 0.3097596804558973
  batch 232 loss: 0.3095704985075983
  batch 233 loss: 0.30956743676775006
  batch 234 loss: 0.30973456698095697
  batch 235 loss: 0.30972682757580533
  batch 236 loss: 0.3095973366397922
  batch 237 loss: 0.3096698400591999
  batch 238 loss: 0.3096481216054003
  batch 239 loss: 0.30961890462552155
  batch 240 loss: 0.3096314616501331
  batch 241 loss: 0.30974876212875874
  batch 242 loss: 0.30959171795647994
  batch 243 loss: 0.30967636473875476
  batch 244 loss: 0.30964294906522405
  batch 245 loss: 0.3096062588448427
  batch 246 loss: 0.3096273227678082
  batch 247 loss: 0.3098199657100415
  batch 248 loss: 0.30987493573657926
  batch 249 loss: 0.30979330317083614
  batch 250 loss: 0.3096584496498108
  batch 251 loss: 0.3096856362078769
  batch 252 loss: 0.3095337375998497
  batch 253 loss: 0.309408079729721
  batch 254 loss: 0.309286367001496
  batch 255 loss: 0.309225297091054
  batch 256 loss: 0.30929488979745656
  batch 257 loss: 0.3092399506253491
  batch 258 loss: 0.3091803789138794
  batch 259 loss: 0.30911284213360674
  batch 260 loss: 0.30912526696920395
  batch 261 loss: 0.30909502357815416
  batch 262 loss: 0.3088776074520504
  batch 263 loss: 0.30881559214211235
  batch 264 loss: 0.3087019527500326
  batch 265 loss: 0.30856271712285166
  batch 266 loss: 0.3084227332943364
  batch 267 loss: 0.3085745638899142
  batch 268 loss: 0.30849027644787264
  batch 269 loss: 0.30846268126955706
  batch 270 loss: 0.3086156423445101
  batch 271 loss: 0.3086239497600006
  batch 272 loss: 0.3086997943327707
  batch 273 loss: 0.3086698929687123
  batch 274 loss: 0.3086913566302209
  batch 275 loss: 0.3086957833983681
  batch 276 loss: 0.30864311851885007
  batch 277 loss: 0.3085460366970365
  batch 278 loss: 0.3084203165640934
  batch 279 loss: 0.30843742805997104
  batch 280 loss: 0.30831536991255626
  batch 281 loss: 0.30817062765678055
  batch 282 loss: 0.30813900988998144
  batch 283 loss: 0.3080632506537353
  batch 284 loss: 0.308161107482205
  batch 285 loss: 0.308080422041709
  batch 286 loss: 0.30819027907364854
  batch 287 loss: 0.308201039709696
  batch 288 loss: 0.3080603936687112
  batch 289 loss: 0.30830164630107815
  batch 290 loss: 0.308145760976035
  batch 291 loss: 0.30819927427367244
  batch 292 loss: 0.30835266033672304
  batch 293 loss: 0.308366168175948
  batch 294 loss: 0.3082030810871903
  batch 295 loss: 0.30818122726375774
  batch 296 loss: 0.30821759737021215
  batch 297 loss: 0.3081830558351395
  batch 298 loss: 0.30824484451105133
  batch 299 loss: 0.3081663825041474
  batch 300 loss: 0.3082006120681763
  batch 301 loss: 0.3082491824793261
  batch 302 loss: 0.3081989306093052
  batch 303 loss: 0.30817153272849107
  batch 304 loss: 0.3082010987165727
  batch 305 loss: 0.30803606480848594
  batch 306 loss: 0.3081480795456693
  batch 307 loss: 0.30815666370360784
  batch 308 loss: 0.3082934139997928
  batch 309 loss: 0.3081520649221723
  batch 310 loss: 0.3081146990099261
  batch 311 loss: 0.308207110576691
  batch 312 loss: 0.3082865174764242
  batch 313 loss: 0.30824700331154725
  batch 314 loss: 0.3081075763626463
  batch 315 loss: 0.30806890470641
  batch 316 loss: 0.3080188299093065
  batch 317 loss: 0.3079388658707074
  batch 318 loss: 0.3078105086050693
  batch 319 loss: 0.3076957685633513
  batch 320 loss: 0.3076112547889352
  batch 321 loss: 0.3076471197085217
  batch 322 loss: 0.3076143822869899
  batch 323 loss: 0.30764044918142974
  batch 324 loss: 0.3074687523422418
  batch 325 loss: 0.30737980539982135
  batch 326 loss: 0.30742372106189375
  batch 327 loss: 0.3074409696487112
  batch 328 loss: 0.3072411469751742
  batch 329 loss: 0.30726343828131725
  batch 330 loss: 0.3072419562123039
  batch 331 loss: 0.30721015350336034
  batch 332 loss: 0.3071530759334564
  batch 333 loss: 0.3072164567025216
  batch 334 loss: 0.3070915323531556
  batch 335 loss: 0.3069420627693632
  batch 336 loss: 0.30679926471341223
  batch 337 loss: 0.306724815057364
  batch 338 loss: 0.3067022119400769
  batch 339 loss: 0.3066466596625899
  batch 340 loss: 0.30661145106834525
  batch 341 loss: 0.3064608650822793
  batch 342 loss: 0.3063740194366689
  batch 343 loss: 0.30635915700965305
  batch 344 loss: 0.30639073503918424
  batch 345 loss: 0.3065389488918194
  batch 346 loss: 0.30636382163260023
  batch 347 loss: 0.30632375717506627
  batch 348 loss: 0.3062923373333339
  batch 349 loss: 0.30620539017598064
  batch 350 loss: 0.30618936896324156
  batch 351 loss: 0.30616166491454144
  batch 352 loss: 0.3062197830189358
  batch 353 loss: 0.30619494215962567
  batch 354 loss: 0.30631732654436833
  batch 355 loss: 0.30626880568517767
  batch 356 loss: 0.30620170041416467
  batch 357 loss: 0.3061083086899349
  batch 358 loss: 0.30612069976063416
  batch 359 loss: 0.3060376574402068
  batch 360 loss: 0.30600803651743463
  batch 361 loss: 0.3059609650409783
  batch 362 loss: 0.30586199413017673
  batch 363 loss: 0.30575060015210764
  batch 364 loss: 0.30565413791727236
  batch 365 loss: 0.3056881476755012
  batch 366 loss: 0.30565897770266715
  batch 367 loss: 0.305592687480781
  batch 368 loss: 0.3054821967430737
  batch 369 loss: 0.30553373508660125
  batch 370 loss: 0.3055005479503322
  batch 371 loss: 0.30550887690721495
  batch 372 loss: 0.305459473882952
  batch 373 loss: 0.3054031617839598
  batch 374 loss: 0.3052437219788684
  batch 375 loss: 0.30515478551387787
  batch 376 loss: 0.3052512223812494
  batch 377 loss: 0.3051684606691887
  batch 378 loss: 0.30506682825624626
  batch 379 loss: 0.3050960875988636
  batch 380 loss: 0.3051142950199152
  batch 381 loss: 0.3050234354934667
  batch 382 loss: 0.30497096841716015
  batch 383 loss: 0.30501373462359527
  batch 384 loss: 0.3048962312362467
  batch 385 loss: 0.3049687111919576
  batch 386 loss: 0.3048700765547357
  batch 387 loss: 0.3049448118268365
  batch 388 loss: 0.304952293135149
  batch 389 loss: 0.3049656323188375
  batch 390 loss: 0.3050243929792673
  batch 391 loss: 0.3050536183673707
  batch 392 loss: 0.30510173939472557
  batch 393 loss: 0.3051303698285542
  batch 394 loss: 0.305141027165851
  batch 395 loss: 0.3052112869446791
  batch 396 loss: 0.30529702533826686
  batch 397 loss: 0.30531671692051876
  batch 398 loss: 0.305323283539046
  batch 399 loss: 0.3054768604816948
  batch 400 loss: 0.30553141865879296
  batch 401 loss: 0.3054666940931073
  batch 402 loss: 0.30551978482387554
  batch 403 loss: 0.30567896451163235
  batch 404 loss: 0.3057508471209814
  batch 405 loss: 0.3058286365535524
  batch 406 loss: 0.30593397101304803
  batch 407 loss: 0.3058479825533579
  batch 408 loss: 0.30592225071992357
  batch 409 loss: 0.30592068989731575
  batch 410 loss: 0.30601232985897764
  batch 411 loss: 0.3060352362492949
  batch 412 loss: 0.30597826878278
  batch 413 loss: 0.3060442552367365
  batch 414 loss: 0.3060027803991728
  batch 415 loss: 0.30598764257976807
  batch 416 loss: 0.30602288586445725
  batch 417 loss: 0.3060159764344172
  batch 418 loss: 0.305992581591937
  batch 419 loss: 0.3060364260522733
  batch 420 loss: 0.30598642127144904
  batch 421 loss: 0.30590561651702164
  batch 422 loss: 0.3060568962687565
  batch 423 loss: 0.30601493027897875
  batch 424 loss: 0.3059589367641031
  batch 425 loss: 0.3059367402511485
  batch 426 loss: 0.305894807435817
  batch 427 loss: 0.3059143843849035
  batch 428 loss: 0.3058257849068842
  batch 429 loss: 0.30583768058351146
  batch 430 loss: 0.30577714141718176
  batch 431 loss: 0.30585744634720125
  batch 432 loss: 0.30594611171357056
  batch 433 loss: 0.30590727983804
  batch 434 loss: 0.30593364665173167
  batch 435 loss: 0.30583054426757766
  batch 436 loss: 0.30582707488072025
  batch 437 loss: 0.30587735611868666
  batch 438 loss: 0.30594462404648465
  batch 439 loss: 0.3058986930274203
  batch 440 loss: 0.3059157421304421
  batch 441 loss: 0.3058353899355108
  batch 442 loss: 0.3057604480989918
  batch 443 loss: 0.3057423001554696
  batch 444 loss: 0.30564281166539536
  batch 445 loss: 0.30568716536077223
  batch 446 loss: 0.30564756668068366
  batch 447 loss: 0.30554923258505146
  batch 448 loss: 0.30555610568262637
  batch 449 loss: 0.3055708872648018
  batch 450 loss: 0.30548301700088715
  batch 451 loss: 0.3054547898322674
  batch 452 loss: 0.30553357556989763
  batch 453 loss: 0.30552931998344446
  batch 454 loss: 0.30556638581505957
  batch 455 loss: 0.30553498487551134
  batch 456 loss: 0.30552675695926473
  batch 457 loss: 0.3054882175486771
  batch 458 loss: 0.3054020342022571
  batch 459 loss: 0.3054446717298109
  batch 460 loss: 0.3055091552436352
  batch 461 loss: 0.30549274352651873
  batch 462 loss: 0.30548338550122783
  batch 463 loss: 0.3054425732170274
  batch 464 loss: 0.30545870025224725
  batch 465 loss: 0.30538893614404944
  batch 466 loss: 0.3052088092580885
  batch 467 loss: 0.30530264169552085
  batch 468 loss: 0.30527247201937896
  batch 469 loss: 0.3054608824029406
  batch 470 loss: 0.3054105253295695
  batch 471 loss: 0.3054113693677696
  batch 472 loss: 0.30528583822740335
LOSS train 0.30528583822740335 valid 0.2529895007610321
LOSS train 0.30528583822740335 valid 0.24007190763950348
LOSS train 0.30528583822740335 valid 0.23958352704842886
LOSS train 0.30528583822740335 valid 0.238048754632473
LOSS train 0.30528583822740335 valid 0.23485709726810455
LOSS train 0.30528583822740335 valid 0.23886336634556452
LOSS train 0.30528583822740335 valid 0.24949927202292851
LOSS train 0.30528583822740335 valid 0.24863689579069614
LOSS train 0.30528583822740335 valid 0.24675476882192823
LOSS train 0.30528583822740335 valid 0.24619702696800233
LOSS train 0.30528583822740335 valid 0.24490010738372803
LOSS train 0.30528583822740335 valid 0.24509375914931297
LOSS train 0.30528583822740335 valid 0.2434658591563885
LOSS train 0.30528583822740335 valid 0.2437596172094345
LOSS train 0.30528583822740335 valid 0.24050336480140685
LOSS train 0.30528583822740335 valid 0.24019534140825272
LOSS train 0.30528583822740335 valid 0.24115804889622858
LOSS train 0.30528583822740335 valid 0.24177643656730652
LOSS train 0.30528583822740335 valid 0.24377797465575368
LOSS train 0.30528583822740335 valid 0.24265542924404143
LOSS train 0.30528583822740335 valid 0.24147121182509831
LOSS train 0.30528583822740335 valid 0.23949001323093067
LOSS train 0.30528583822740335 valid 0.23916851243247156
LOSS train 0.30528583822740335 valid 0.2382366502036651
LOSS train 0.30528583822740335 valid 0.23700044870376588
LOSS train 0.30528583822740335 valid 0.2369544614966099
LOSS train 0.30528583822740335 valid 0.23668489467214654
LOSS train 0.30528583822740335 valid 0.23722662297742708
LOSS train 0.30528583822740335 valid 0.23753298870448408
LOSS train 0.30528583822740335 valid 0.2383042186498642
LOSS train 0.30528583822740335 valid 0.23924441395267362
LOSS train 0.30528583822740335 valid 0.23864868097007275
LOSS train 0.30528583822740335 valid 0.2400054642648408
LOSS train 0.30528583822740335 valid 0.23978953107314951
LOSS train 0.30528583822740335 valid 0.241257067663329
LOSS train 0.30528583822740335 valid 0.24054603651165962
LOSS train 0.30528583822740335 valid 0.2405670325498323
LOSS train 0.30528583822740335 valid 0.2417236244992206
LOSS train 0.30528583822740335 valid 0.2415329355459947
LOSS train 0.30528583822740335 valid 0.24162145033478738
LOSS train 0.30528583822740335 valid 0.24233772042320995
LOSS train 0.30528583822740335 valid 0.2426966435852505
LOSS train 0.30528583822740335 valid 0.24231402887854464
LOSS train 0.30528583822740335 valid 0.2427785870703784
LOSS train 0.30528583822740335 valid 0.24248505565855238
LOSS train 0.30528583822740335 valid 0.2429935115834941
LOSS train 0.30528583822740335 valid 0.24367331507358145
LOSS train 0.30528583822740335 valid 0.2442207634449005
LOSS train 0.30528583822740335 valid 0.24451191753757243
LOSS train 0.30528583822740335 valid 0.2435903963446617
LOSS train 0.30528583822740335 valid 0.24381911374774634
LOSS train 0.30528583822740335 valid 0.24339895093670258
LOSS train 0.30528583822740335 valid 0.24334637370874296
LOSS train 0.30528583822740335 valid 0.24333453923463821
LOSS train 0.30528583822740335 valid 0.2431282249363986
LOSS train 0.30528583822740335 valid 0.24275993954922473
LOSS train 0.30528583822740335 valid 0.2423943417114124
LOSS train 0.30528583822740335 valid 0.24202768684461198
LOSS train 0.30528583822740335 valid 0.24231385654312068
LOSS train 0.30528583822740335 valid 0.24228023141622543
LOSS train 0.30528583822740335 valid 0.2418833140467034
LOSS train 0.30528583822740335 valid 0.2428462149635438
LOSS train 0.30528583822740335 valid 0.2427965412064204
LOSS train 0.30528583822740335 valid 0.24372869869694114
LOSS train 0.30528583822740335 valid 0.24391644092706533
LOSS train 0.30528583822740335 valid 0.24383957638885034
LOSS train 0.30528583822740335 valid 0.24296866246123813
LOSS train 0.30528583822740335 valid 0.24285351178225348
LOSS train 0.30528583822740335 valid 0.24226256513941116
LOSS train 0.30528583822740335 valid 0.24220818600484303
LOSS train 0.30528583822740335 valid 0.24187407065445268
LOSS train 0.30528583822740335 valid 0.24211816117167473
LOSS train 0.30528583822740335 valid 0.2421056920126693
LOSS train 0.30528583822740335 valid 0.2418651619070285
LOSS train 0.30528583822740335 valid 0.241982385913531
LOSS train 0.30528583822740335 valid 0.24215680458828023
LOSS train 0.30528583822740335 valid 0.24216113365315772
LOSS train 0.30528583822740335 valid 0.24225325825122687
LOSS train 0.30528583822740335 valid 0.2422221069094501
LOSS train 0.30528583822740335 valid 0.24161502197384835
LOSS train 0.30528583822740335 valid 0.24071918299168715
LOSS train 0.30528583822740335 valid 0.2410846817784193
LOSS train 0.30528583822740335 valid 0.24090346227209253
LOSS train 0.30528583822740335 valid 0.24098445581538336
LOSS train 0.30528583822740335 valid 0.2407656382111942
LOSS train 0.30528583822740335 valid 0.24009222675894581
LOSS train 0.30528583822740335 valid 0.2399852555031064
LOSS train 0.30528583822740335 valid 0.23928880962458524
LOSS train 0.30528583822740335 valid 0.23970410763547662
LOSS train 0.30528583822740335 valid 0.23982146713468763
LOSS train 0.30528583822740335 valid 0.23994072191007845
LOSS train 0.30528583822740335 valid 0.2402265920587208
LOSS train 0.30528583822740335 valid 0.23999325498457877
LOSS train 0.30528583822740335 valid 0.24003792855333775
LOSS train 0.30528583822740335 valid 0.23988339163755115
LOSS train 0.30528583822740335 valid 0.2399953093069295
LOSS train 0.30528583822740335 valid 0.2401730057197748
LOSS train 0.30528583822740335 valid 0.24033408277497
LOSS train 0.30528583822740335 valid 0.24037834505240122
LOSS train 0.30528583822740335 valid 0.24051579400897027
LOSS train 0.30528583822740335 valid 0.24078761572294896
LOSS train 0.30528583822740335 valid 0.24081936829230366
LOSS train 0.30528583822740335 valid 0.2405577747567186
LOSS train 0.30528583822740335 valid 0.24054059013724327
LOSS train 0.30528583822740335 valid 0.24062772563525608
LOSS train 0.30528583822740335 valid 0.24094027421384487
LOSS train 0.30528583822740335 valid 0.24072260527967293
LOSS train 0.30528583822740335 valid 0.24076783256950202
LOSS train 0.30528583822740335 valid 0.24103788867456102
LOSS train 0.30528583822740335 valid 0.2412860002030026
LOSS train 0.30528583822740335 valid 0.24086974265876118
LOSS train 0.30528583822740335 valid 0.2408032181805798
LOSS train 0.30528583822740335 valid 0.2407584481798442
LOSS train 0.30528583822740335 valid 0.24073851788253114
LOSS train 0.30528583822740335 valid 0.24079169281150983
LOSS train 0.30528583822740335 valid 0.24091747904132152
LOSS train 0.30528583822740335 valid 0.2410808476882103
LOSS train 0.30528583822740335 valid 0.24109766165078697
LOSS train 0.30528583822740335 valid 0.24096139877283274
LOSS train 0.30528583822740335 valid 0.2406722314655781
LOSS train 0.30528583822740335 valid 0.24052735769059047
LOSS train 0.30528583822740335 valid 0.2403742101837377
LOSS train 0.30528583822740335 valid 0.2402558241917835
LOSS train 0.30528583822740335 valid 0.24057066753025977
LOSS train 0.30528583822740335 valid 0.240533109664917
LOSS train 0.30528583822740335 valid 0.24082382070639777
LOSS train 0.30528583822740335 valid 0.2406635300850305
LOSS train 0.30528583822740335 valid 0.2409732670057565
LOSS train 0.30528583822740335 valid 0.24108513034591378
LOSS train 0.30528583822740335 valid 0.2410060179921297
LOSS train 0.30528583822740335 valid 0.2409783894096622
LOSS train 0.30528583822740335 valid 0.24071396334153233
LOSS train 0.30528583822740335 valid 0.24062516579502508
LOSS train 0.30528583822740335 valid 0.24077222407308976
LOSS train 0.30528583822740335 valid 0.24064394312876242
LOSS train 0.30528583822740335 valid 0.2405397004502661
LOSS train 0.30528583822740335 valid 0.240389897114169
LOSS train 0.30528583822740335 valid 0.2402428239583969
LOSS train 0.30528583822740335 valid 0.24008412519804864
LOSS train 0.30528583822740335 valid 0.24016567042895726
LOSS train 0.30528583822740335 valid 0.24018217433006206
LOSS train 0.30528583822740335 valid 0.240439069627876
LOSS train 0.30528583822740335 valid 0.2403936157901804
LOSS train 0.30528583822740335 valid 0.2403842578124669
LOSS train 0.30528583822740335 valid 0.24013490872136478
LOSS train 0.30528583822740335 valid 0.2403229616888582
LOSS train 0.30528583822740335 valid 0.2400831742351558
LOSS train 0.30528583822740335 valid 0.24068829135314837
LOSS train 0.30528583822740335 valid 0.2408177088571075
LOSS train 0.30528583822740335 valid 0.24072681546211241
LOSS train 0.30528583822740335 valid 0.24081212341390698
LOSS train 0.30528583822740335 valid 0.24058575751750091
LOSS train 0.30528583822740335 valid 0.24063947894214804
LOSS train 0.30528583822740335 valid 0.24047398547847548
LOSS train 0.30528583822740335 valid 0.24052265151854485
LOSS train 0.30528583822740335 valid 0.24069411212053055
LOSS train 0.30528583822740335 valid 0.2405895655322227
LOSS train 0.30528583822740335 valid 0.2407830136863491
LOSS train 0.30528583822740335 valid 0.24080374780690894
LOSS train 0.30528583822740335 valid 0.24075783109292387
LOSS train 0.30528583822740335 valid 0.24066566921168972
LOSS train 0.30528583822740335 valid 0.24058838998094018
LOSS train 0.30528583822740335 valid 0.2404771994776521
LOSS train 0.30528583822740335 valid 0.24020370704735197
LOSS train 0.30528583822740335 valid 0.24013481284632826
LOSS train 0.30528583822740335 valid 0.24028872563896408
LOSS train 0.30528583822740335 valid 0.24054760133434913
LOSS train 0.30528583822740335 valid 0.24045212247541972
LOSS train 0.30528583822740335 valid 0.2406451716225528
LOSS train 0.30528583822740335 valid 0.24079293696319357
LOSS train 0.30528583822740335 valid 0.24074917404275192
LOSS train 0.30528583822740335 valid 0.24066516514434372
LOSS train 0.30528583822740335 valid 0.24074371908441444
LOSS train 0.30528583822740335 valid 0.24074881172728266
LOSS train 0.30528583822740335 valid 0.24060273196016038
LOSS train 0.30528583822740335 valid 0.24053797862407836
LOSS train 0.30528583822740335 valid 0.24052240914207393
LOSS train 0.30528583822740335 valid 0.24067048368494162
LOSS train 0.30528583822740335 valid 0.24061516768106536
LOSS train 0.30528583822740335 valid 0.24068295632799466
LOSS train 0.30528583822740335 valid 0.24090121982835275
LOSS train 0.30528583822740335 valid 0.24089503935077689
LOSS train 0.30528583822740335 valid 0.24082229217219223
LOSS train 0.30528583822740335 valid 0.24083315297637298
LOSS train 0.30528583822740335 valid 0.2406022707352767
LOSS train 0.30528583822740335 valid 0.2405213245781519
LOSS train 0.30528583822740335 valid 0.24051355741878244
LOSS train 0.30528583822740335 valid 0.2405136451125145
LOSS train 0.30528583822740335 valid 0.24054537279896004
LOSS train 0.30528583822740335 valid 0.24067082656057256
LOSS train 0.30528583822740335 valid 0.2406208359134135
LOSS train 0.30528583822740335 valid 0.24068317329511046
LOSS train 0.30528583822740335 valid 0.24064310667119496
LOSS train 0.30528583822740335 valid 0.24056241814930415
LOSS train 0.30528583822740335 valid 0.24042182175012736
LOSS train 0.30528583822740335 valid 0.24045873281298852
LOSS train 0.30528583822740335 valid 0.24060710281284933
LOSS train 0.30528583822740335 valid 0.2404512623795355
LOSS train 0.30528583822740335 valid 0.24051741432005436
LOSS train 0.30528583822740335 valid 0.24041161768138408
LOSS train 0.30528583822740335 valid 0.24018366901732202
LOSS train 0.30528583822740335 valid 0.2401106626828118
LOSS train 0.30528583822740335 valid 0.23999937620069006
LOSS train 0.30528583822740335 valid 0.2399423836054755
LOSS train 0.30528583822740335 valid 0.23975725181219054
LOSS train 0.30528583822740335 valid 0.2397673604557815
LOSS train 0.30528583822740335 valid 0.2396635151859643
LOSS train 0.30528583822740335 valid 0.23948053492663
LOSS train 0.30528583822740335 valid 0.239368813959035
LOSS train 0.30528583822740335 valid 0.2393785949973833
LOSS train 0.30528583822740335 valid 0.23948344906077002
LOSS train 0.30528583822740335 valid 0.23948880039014905
LOSS train 0.30528583822740335 valid 0.2395581924159762
LOSS train 0.30528583822740335 valid 0.239529767395737
LOSS train 0.30528583822740335 valid 0.2394153270610543
LOSS train 0.30528583822740335 valid 0.23942027944657537
LOSS train 0.30528583822740335 valid 0.23944807313554298
LOSS train 0.30528583822740335 valid 0.23951204585919686
LOSS train 0.30528583822740335 valid 0.23957381128720498
LOSS train 0.30528583822740335 valid 0.2396224023266272
LOSS train 0.30528583822740335 valid 0.23969590785276837
LOSS train 0.30528583822740335 valid 0.23962690459715352
LOSS train 0.30528583822740335 valid 0.2397661802480039
LOSS train 0.30528583822740335 valid 0.23991255848003284
LOSS train 0.30528583822740335 valid 0.23993440608183542
LOSS train 0.30528583822740335 valid 0.2400037008708557
LOSS train 0.30528583822740335 valid 0.2402665626241247
LOSS train 0.30528583822740335 valid 0.2404185766844373
LOSS train 0.30528583822740335 valid 0.24046805572561822
LOSS train 0.30528583822740335 valid 0.24055482000112532
LOSS train 0.30528583822740335 valid 0.2405552240290167
LOSS train 0.30528583822740335 valid 0.2405729002865224
LOSS train 0.30528583822740335 valid 0.24053276199127982
LOSS train 0.30528583822740335 valid 0.2405321084153958
LOSS train 0.30528583822740335 valid 0.2405863843699719
LOSS train 0.30528583822740335 valid 0.2404562985619246
LOSS train 0.30528583822740335 valid 0.24055766034478376
LOSS train 0.30528583822740335 valid 0.24054043470811443
LOSS train 0.30528583822740335 valid 0.24043357521919026
LOSS train 0.30528583822740335 valid 0.24037005454301835
LOSS train 0.30528583822740335 valid 0.24042034532519296
LOSS train 0.30528583822740335 valid 0.24020009056842032
LOSS train 0.30528583822740335 valid 0.24032869720409927
LOSS train 0.30528583822740335 valid 0.24056752227613185
LOSS train 0.30528583822740335 valid 0.24069282710552214
LOSS train 0.30528583822740335 valid 0.2406736068972727
LOSS train 0.30528583822740335 valid 0.2407063366430491
LOSS train 0.30528583822740335 valid 0.24062380795517274
LOSS train 0.30528583822740335 valid 0.24054234292373122
LOSS train 0.30528583822740335 valid 0.24069037920236588
LOSS train 0.30528583822740335 valid 0.24066889102003014
LOSS train 0.30528583822740335 valid 0.24085598740549313
LOSS train 0.30528583822740335 valid 0.24082843829756198
LOSS train 0.30528583822740335 valid 0.24078385619901296
LOSS train 0.30528583822740335 valid 0.24091839714377533
LOSS train 0.30528583822740335 valid 0.24094731087097898
LOSS train 0.30528583822740335 valid 0.24079592096898342
LOSS train 0.30528583822740335 valid 0.24088720290004745
LOSS train 0.30528583822740335 valid 0.24086628370993846
LOSS train 0.30528583822740335 valid 0.24084943842429382
LOSS train 0.30528583822740335 valid 0.24099632184167474
LOSS train 0.30528583822740335 valid 0.24103524973829285
LOSS train 0.30528583822740335 valid 0.2410153959073948
LOSS train 0.30528583822740335 valid 0.24102702832809
LOSS train 0.30528583822740335 valid 0.24094343950163644
LOSS train 0.30528583822740335 valid 0.2409551195744285
LOSS train 0.30528583822740335 valid 0.24104886192284272
LOSS train 0.30528583822740335 valid 0.24122406147531608
LOSS train 0.30528583822740335 valid 0.24129869509142127
LOSS train 0.30528583822740335 valid 0.2413000769637249
LOSS train 0.30528583822740335 valid 0.24135069054211197
LOSS train 0.30528583822740335 valid 0.24150488081881227
LOSS train 0.30528583822740335 valid 0.24152368072406713
LOSS train 0.30528583822740335 valid 0.24148424198157595
LOSS train 0.30528583822740335 valid 0.24145654846321452
LOSS train 0.30528583822740335 valid 0.24130657617596613
LOSS train 0.30528583822740335 valid 0.24112654867370206
LOSS train 0.30528583822740335 valid 0.2409731433760348
LOSS train 0.30528583822740335 valid 0.24092692163469118
LOSS train 0.30528583822740335 valid 0.24085764645465782
LOSS train 0.30528583822740335 valid 0.2407055827540435
LOSS train 0.30528583822740335 valid 0.24059023221967912
LOSS train 0.30528583822740335 valid 0.2405592444823403
LOSS train 0.30528583822740335 valid 0.24060584693937234
LOSS train 0.30528583822740335 valid 0.24064825998063674
LOSS train 0.30528583822740335 valid 0.24058942314419712
LOSS train 0.30528583822740335 valid 0.2404683253075603
LOSS train 0.30528583822740335 valid 0.24048183713522223
LOSS train 0.30528583822740335 valid 0.24050751869859993
LOSS train 0.30528583822740335 valid 0.2404768015803962
LOSS train 0.30528583822740335 valid 0.24045660262255325
LOSS train 0.30528583822740335 valid 0.24052362031724356
LOSS train 0.30528583822740335 valid 0.24053023470750035
LOSS train 0.30528583822740335 valid 0.2406561310295345
LOSS train 0.30528583822740335 valid 0.24072993039074592
LOSS train 0.30528583822740335 valid 0.24070476987273307
LOSS train 0.30528583822740335 valid 0.2407567547728317
LOSS train 0.30528583822740335 valid 0.24074495373756294
LOSS train 0.30528583822740335 valid 0.24079226954126837
LOSS train 0.30528583822740335 valid 0.24073780715465545
LOSS train 0.30528583822740335 valid 0.24076945104472264
LOSS train 0.30528583822740335 valid 0.24080125385562315
LOSS train 0.30528583822740335 valid 0.24084049974731092
LOSS train 0.30528583822740335 valid 0.240816085256244
LOSS train 0.30528583822740335 valid 0.24069002573607398
LOSS train 0.30528583822740335 valid 0.2406297151754105
LOSS train 0.30528583822740335 valid 0.24058129572907178
LOSS train 0.30528583822740335 valid 0.2404763362334146
LOSS train 0.30528583822740335 valid 0.24052679726799714
LOSS train 0.30528583822740335 valid 0.2405455609963786
LOSS train 0.30528583822740335 valid 0.24047550214065233
LOSS train 0.30528583822740335 valid 0.24048644275619432
LOSS train 0.30528583822740335 valid 0.24052248300074008
LOSS train 0.30528583822740335 valid 0.24060137968534118
LOSS train 0.30528583822740335 valid 0.24058406272577862
LOSS train 0.30528583822740335 valid 0.24052815285475948
LOSS train 0.30528583822740335 valid 0.24058143719325684
LOSS train 0.30528583822740335 valid 0.24066237688252012
LOSS train 0.30528583822740335 valid 0.24077420684050616
LOSS train 0.30528583822740335 valid 0.24071969208307564
LOSS train 0.30528583822740335 valid 0.24074087990592943
LOSS train 0.30528583822740335 valid 0.24065360593499605
LOSS train 0.30528583822740335 valid 0.24055857781101675
LOSS train 0.30528583822740335 valid 0.24050831537187836
LOSS train 0.30528583822740335 valid 0.24054706573486329
LOSS train 0.30528583822740335 valid 0.2406129387258752
LOSS train 0.30528583822740335 valid 0.24060004243245547
LOSS train 0.30528583822740335 valid 0.24059877076708688
LOSS train 0.30528583822740335 valid 0.24067039316729572
LOSS train 0.30528583822740335 valid 0.2406391560128241
LOSS train 0.30528583822740335 valid 0.24055833470785365
LOSS train 0.30528583822740335 valid 0.2404076581349574
LOSS train 0.30528583822740335 valid 0.24044469755154113
LOSS train 0.30528583822740335 valid 0.24059702058930596
LOSS train 0.30528583822740335 valid 0.24055223051291794
LOSS train 0.30528583822740335 valid 0.24051319679156655
LOSS train 0.30528583822740335 valid 0.24049434653906157
LOSS train 0.30528583822740335 valid 0.2404649232530735
LOSS train 0.30528583822740335 valid 0.24042326950393947
LOSS train 0.30528583822740335 valid 0.2404306057621451
LOSS train 0.30528583822740335 valid 0.2403054862690112
LOSS train 0.30528583822740335 valid 0.24030639658197325
LOSS train 0.30528583822740335 valid 0.2403073975539416
LOSS train 0.30528583822740335 valid 0.2404530040573242
LOSS train 0.30528583822740335 valid 0.24050592693729678
LOSS train 0.30528583822740335 valid 0.24047875679986325
LOSS train 0.30528583822740335 valid 0.2403239825101682
LOSS train 0.30528583822740335 valid 0.24022866687041589
LOSS train 0.30528583822740335 valid 0.24026397908143807
LOSS train 0.30528583822740335 valid 0.24018891228096825
LOSS train 0.30528583822740335 valid 0.2401135919185785
LOSS train 0.30528583822740335 valid 0.24010914847762746
LOSS train 0.30528583822740335 valid 0.24016503660435717
LOSS train 0.30528583822740335 valid 0.2402424833784669
LOSS train 0.30528583822740335 valid 0.24033727775996838
LOSS train 0.30528583822740335 valid 0.2404212320872237
LOSS train 0.30528583822740335 valid 0.24035696702010156
LOSS train 0.30528583822740335 valid 0.24033898964274528
LOSS train 0.30528583822740335 valid 0.24033619384579671
LOSS train 0.30528583822740335 valid 0.24030919865601594
LOSS train 0.30528583822740335 valid 0.24030968542739625
LOSS train 0.30528583822740335 valid 0.24036431530727206
LOSS train 0.30528583822740335 valid 0.2402299002182385
LOSS train 0.30528583822740335 valid 0.24025252752087928
LOSS train 0.30528583822740335 valid 0.24023093205608734
LOSS train 0.30528583822740335 valid 0.24016957103066106
LOSS train 0.30528583822740335 valid 0.24005271019337937
LOSS train 0.30528583822740335 valid 0.24004830990958473
LOSS train 0.30528583822740335 valid 0.2401573080115202
EPOCH 5:
  batch 1 loss: 0.2957671284675598
  batch 2 loss: 0.28262433409690857
  batch 3 loss: 0.2895738383134206
  batch 4 loss: 0.2880817949771881
  batch 5 loss: 0.2953158140182495
  batch 6 loss: 0.29223895569642383
  batch 7 loss: 0.2972014290945871
  batch 8 loss: 0.29955413565039635
  batch 9 loss: 0.3004492355717553
  batch 10 loss: 0.3010115444660187
  batch 11 loss: 0.301096341826699
  batch 12 loss: 0.29783028612534207
  batch 13 loss: 0.29601731437903184
  batch 14 loss: 0.2963566758802959
  batch 15 loss: 0.2980203131834666
  batch 16 loss: 0.29722328670322895
  batch 17 loss: 0.2947818075909334
  batch 18 loss: 0.2953454934888416
  batch 19 loss: 0.2947684651926944
  batch 20 loss: 0.29321408867835996
  batch 21 loss: 0.2943617417698815
  batch 22 loss: 0.2940365658565001
  batch 23 loss: 0.29348483552103455
  batch 24 loss: 0.29263245190183323
  batch 25 loss: 0.29522343397140505
  batch 26 loss: 0.2935061431848086
  batch 27 loss: 0.2946330518634231
  batch 28 loss: 0.2932441458106041
  batch 29 loss: 0.2938253499310592
  batch 30 loss: 0.29325162967046103
  batch 31 loss: 0.29373240182476656
  batch 32 loss: 0.2930930610746145
  batch 33 loss: 0.29331705154794635
  batch 34 loss: 0.2935869632398381
  batch 35 loss: 0.2945398560592106
  batch 36 loss: 0.29491744769944084
  batch 37 loss: 0.2953903312618668
  batch 38 loss: 0.2962698795293507
  batch 39 loss: 0.29621604543465835
  batch 40 loss: 0.2963846743106842
  batch 41 loss: 0.2962093912973637
  batch 42 loss: 0.2964826027552287
  batch 43 loss: 0.29749582118766255
  batch 44 loss: 0.2974988039244305
  batch 45 loss: 0.29725430806477865
  batch 46 loss: 0.2976817916268888
  batch 47 loss: 0.29749940240636785
  batch 48 loss: 0.29685048510630924
  batch 49 loss: 0.29668647050857544
  batch 50 loss: 0.2962500846385956
  batch 51 loss: 0.29604584796755923
  batch 52 loss: 0.29654546769765705
  batch 53 loss: 0.2959164150480954
  batch 54 loss: 0.2960513808109142
  batch 55 loss: 0.2956747244704853
  batch 56 loss: 0.29527310335210394
  batch 57 loss: 0.294919887132812
  batch 58 loss: 0.29585992108131276
  batch 59 loss: 0.2958702793565847
  batch 60 loss: 0.29536988735198977
  batch 61 loss: 0.29573629039232846
  batch 62 loss: 0.2962774571872527
  batch 63 loss: 0.2959373134469229
  batch 64 loss: 0.296760993078351
  batch 65 loss: 0.2965830825842344
  batch 66 loss: 0.2964260821992701
  batch 67 loss: 0.2970750256260829
  batch 68 loss: 0.29745184104232225
  batch 69 loss: 0.2973654036936553
  batch 70 loss: 0.29765078595706396
  batch 71 loss: 0.29735517669731465
  batch 72 loss: 0.29760702409678036
  batch 73 loss: 0.29792164041571423
  batch 74 loss: 0.29820282153181127
  batch 75 loss: 0.2977389693260193
  batch 76 loss: 0.2977215055572359
  batch 77 loss: 0.2981409767231384
  batch 78 loss: 0.2980642215563701
  batch 79 loss: 0.2988969117780275
  batch 80 loss: 0.2989616446197033
  batch 81 loss: 0.29904064536094666
  batch 82 loss: 0.2996765291545449
  batch 83 loss: 0.299956147929272
  batch 84 loss: 0.2996000881705965
  batch 85 loss: 0.29933249459547157
  batch 86 loss: 0.2999955162752506
  batch 87 loss: 0.29988458821143227
  batch 88 loss: 0.2995334802703424
  batch 89 loss: 0.29945294106944226
  batch 90 loss: 0.2991736650466919
  batch 91 loss: 0.2991731399363214
  batch 92 loss: 0.298938202145307
  batch 93 loss: 0.29876996112126175
  batch 94 loss: 0.29894895280929323
  batch 95 loss: 0.2987426911529742
  batch 96 loss: 0.2989495201036334
  batch 97 loss: 0.29940392676088
  batch 98 loss: 0.29956346385333005
  batch 99 loss: 0.29983365836769643
  batch 100 loss: 0.2997452408075333
  batch 101 loss: 0.2998078241206632
  batch 102 loss: 0.29992643290875004
  batch 103 loss: 0.3001758791867969
  batch 104 loss: 0.30016148978701007
  batch 105 loss: 0.29979934919448126
  batch 106 loss: 0.30000965718953115
  batch 107 loss: 0.29951977827281595
  batch 108 loss: 0.2994955997500155
  batch 109 loss: 0.29920966267038923
  batch 110 loss: 0.29929041225801817
  batch 111 loss: 0.29883608691864183
  batch 112 loss: 0.29865780086921795
  batch 113 loss: 0.2989391221673088
  batch 114 loss: 0.29911019783793835
  batch 115 loss: 0.2995252937078476
  batch 116 loss: 0.2997127469500591
  batch 117 loss: 0.3000243755742016
  batch 118 loss: 0.2999177950166039
  batch 119 loss: 0.30029477654885844
  batch 120 loss: 0.3001476095368465
  batch 121 loss: 0.30001690355706806
  batch 122 loss: 0.2999959349876545
  batch 123 loss: 0.29998129376066407
  batch 124 loss: 0.3002349057745549
  batch 125 loss: 0.3002901426553726
  batch 126 loss: 0.30011106022293604
  batch 127 loss: 0.3005641148550304
  batch 128 loss: 0.3004196766996756
  batch 129 loss: 0.30059301633705465
  batch 130 loss: 0.3005303390897237
  batch 131 loss: 0.30065744626158064
  batch 132 loss: 0.30044058631315373
  batch 133 loss: 0.3005713651278861
  batch 134 loss: 0.3005249997350707
  batch 135 loss: 0.30048557376420054
  batch 136 loss: 0.30047590121188583
  batch 137 loss: 0.3004472713618383
  batch 138 loss: 0.30035873869622964
  batch 139 loss: 0.3007423763009284
  batch 140 loss: 0.3005811030311244
  batch 141 loss: 0.3007576386345194
  batch 142 loss: 0.30057643175544874
  batch 143 loss: 0.30041835451876364
  batch 144 loss: 0.3003229702719384
  batch 145 loss: 0.30026360226088555
  batch 146 loss: 0.3003207205297196
  batch 147 loss: 0.30066886696280265
  batch 148 loss: 0.3005800054886857
  batch 149 loss: 0.30029502841050193
  batch 150 loss: 0.30039449443419775
  batch 151 loss: 0.30038476098846917
  batch 152 loss: 0.30062830026604626
  batch 153 loss: 0.3004594268946866
  batch 154 loss: 0.30058699062505323
  batch 155 loss: 0.30051198303699495
  batch 156 loss: 0.30043723405553746
  batch 157 loss: 0.30052115345836444
  batch 158 loss: 0.3005011640960657
  batch 159 loss: 0.3004639061554423
  batch 160 loss: 0.3002543502487242
  batch 161 loss: 0.3003340888282527
  batch 162 loss: 0.3003058487802376
  batch 163 loss: 0.3001361404277064
  batch 164 loss: 0.3000865766733158
  batch 165 loss: 0.29990344110763434
  batch 166 loss: 0.2997649623507477
  batch 167 loss: 0.29970332917695985
  batch 168 loss: 0.2997151030493634
  batch 169 loss: 0.29956122042512046
  batch 170 loss: 0.29927147004534216
  batch 171 loss: 0.2992110519904142
  batch 172 loss: 0.2991971303383971
  batch 173 loss: 0.2991616936949636
  batch 174 loss: 0.29909948585019713
  batch 175 loss: 0.2991546827554703
  batch 176 loss: 0.2989255539564924
  batch 177 loss: 0.29892053720304523
  batch 178 loss: 0.2990312600571118
  batch 179 loss: 0.2992246456485887
  batch 180 loss: 0.29909498219688735
  batch 181 loss: 0.2991794192165301
  batch 182 loss: 0.29899533559660335
  batch 183 loss: 0.29872839739088153
  batch 184 loss: 0.298600136504873
  batch 185 loss: 0.29866405910736804
  batch 186 loss: 0.298679870703528
  batch 187 loss: 0.2985657736419994
  batch 188 loss: 0.298144085055336
  batch 189 loss: 0.29791242349400093
  batch 190 loss: 0.2980742309438555
  batch 191 loss: 0.29801283417883967
  batch 192 loss: 0.2980084070780625
  batch 193 loss: 0.29807678953662436
  batch 194 loss: 0.29822305912516783
  batch 195 loss: 0.2982680009725766
  batch 196 loss: 0.29813074465004763
  batch 197 loss: 0.2981054015117248
  batch 198 loss: 0.2983056992442921
  batch 199 loss: 0.29810035595642265
  batch 200 loss: 0.29823853872716427
  batch 201 loss: 0.2983520212606411
  batch 202 loss: 0.298334954767534
  batch 203 loss: 0.29822797295201586
  batch 204 loss: 0.2980345895915639
  batch 205 loss: 0.29811658037871847
  batch 206 loss: 0.2980008053692799
  batch 207 loss: 0.2980448798712901
  batch 208 loss: 0.29785010354736674
  batch 209 loss: 0.29768381713394915
  batch 210 loss: 0.2978549245567549
  batch 211 loss: 0.2978435891075722
  batch 212 loss: 0.2978442387901387
  batch 213 loss: 0.2976707099049304
  batch 214 loss: 0.29760633652733864
  batch 215 loss: 0.2974005128755126
  batch 216 loss: 0.29725468413973294
  batch 217 loss: 0.2972811316976899
  batch 218 loss: 0.29739103972091585
  batch 219 loss: 0.297599755942005
  batch 220 loss: 0.29757510484619576
  batch 221 loss: 0.29760902978446147
  batch 222 loss: 0.297889888756447
  batch 223 loss: 0.297805107428354
  batch 224 loss: 0.2977570902689227
  batch 225 loss: 0.2976309914721383
  batch 226 loss: 0.297686693876718
  batch 227 loss: 0.29749123437026526
  batch 228 loss: 0.2973445913378607
  batch 229 loss: 0.2972776315227867
  batch 230 loss: 0.2973298423316168
  batch 231 loss: 0.29724303552340636
  batch 232 loss: 0.29707704208277425
  batch 233 loss: 0.2970474959059335
  batch 234 loss: 0.2971894953113336
  batch 235 loss: 0.2971783435725151
  batch 236 loss: 0.297084673797175
  batch 237 loss: 0.29718020174825244
  batch 238 loss: 0.29713127837211145
  batch 239 loss: 0.2970548086959448
  batch 240 loss: 0.2970708382005493
  batch 241 loss: 0.29723734312779676
  batch 242 loss: 0.29709495844180916
  batch 243 loss: 0.2971471302180624
  batch 244 loss: 0.2971283521686421
  batch 245 loss: 0.2970679226578498
  batch 246 loss: 0.297042464277124
  batch 247 loss: 0.2971309839110625
  batch 248 loss: 0.2971787450895194
  batch 249 loss: 0.2970643269130025
  batch 250 loss: 0.29692447346448897
  batch 251 loss: 0.29692354424303746
  batch 252 loss: 0.29669221135832013
  batch 253 loss: 0.29656162174794043
  batch 254 loss: 0.29641199229270454
  batch 255 loss: 0.2963432054893643
  batch 256 loss: 0.2964114078786224
  batch 257 loss: 0.2963888451515005
  batch 258 loss: 0.29626210646111834
  batch 259 loss: 0.2962189604416777
  batch 260 loss: 0.29626131034814396
  batch 261 loss: 0.29620981696008264
  batch 262 loss: 0.2960333761026841
  batch 263 loss: 0.2960333355592684
  batch 264 loss: 0.2958856216547164
  batch 265 loss: 0.2957684537712133
  batch 266 loss: 0.29562014391771835
  batch 267 loss: 0.2958178726140033
  batch 268 loss: 0.2957323276396118
  batch 269 loss: 0.295689128200804
  batch 270 loss: 0.2958654588571301
  batch 271 loss: 0.29576488345091634
  batch 272 loss: 0.2958569720058757
  batch 273 loss: 0.29584924276276825
  batch 274 loss: 0.29587361298120807
  batch 275 loss: 0.2958687300573696
  batch 276 loss: 0.2958216845881248
  batch 277 loss: 0.2957760952547569
  batch 278 loss: 0.29565806874482753
  batch 279 loss: 0.29568845466259985
  batch 280 loss: 0.29560008916471686
  batch 281 loss: 0.29546640240637007
  batch 282 loss: 0.2953985828562831
  batch 283 loss: 0.29531416428594626
  batch 284 loss: 0.2953308134956259
  batch 285 loss: 0.2952351656399275
  batch 286 loss: 0.2952990040599883
  batch 287 loss: 0.2953296698863498
  batch 288 loss: 0.29520301101729274
  batch 289 loss: 0.295453209417089
  batch 290 loss: 0.2952386383352609
  batch 291 loss: 0.295258031677954
  batch 292 loss: 0.29537327812142566
  batch 293 loss: 0.29536123904351896
  batch 294 loss: 0.29521313522543224
  batch 295 loss: 0.295204528105461
  batch 296 loss: 0.2952382665228199
  batch 297 loss: 0.2951815902986109
  batch 298 loss: 0.29524002419222123
  batch 299 loss: 0.2952248430172337
  batch 300 loss: 0.29531418850024543
  batch 301 loss: 0.2953310356385684
  batch 302 loss: 0.2953006514650307
  batch 303 loss: 0.2952958776219056
  batch 304 loss: 0.2952817935300501
  batch 305 loss: 0.29515638116930354
  batch 306 loss: 0.29527800496107615
  batch 307 loss: 0.29526678009219587
  batch 308 loss: 0.29544934223998676
  batch 309 loss: 0.2953802605662917
  batch 310 loss: 0.2953910136415112
  batch 311 loss: 0.2954776759507955
  batch 312 loss: 0.2955829258530568
  batch 313 loss: 0.2955761445216097
  batch 314 loss: 0.29545461704396897
  batch 315 loss: 0.2954413087595077
  batch 316 loss: 0.2953410109009924
  batch 317 loss: 0.29524393017735767
  batch 318 loss: 0.2951219565463516
  batch 319 loss: 0.29501510656739477
  batch 320 loss: 0.29498331984505055
  batch 321 loss: 0.29505758437783547
  batch 322 loss: 0.29510315799194836
  batch 323 loss: 0.2950728202567381
  batch 324 loss: 0.2949236750050827
  batch 325 loss: 0.2948943682817312
  batch 326 loss: 0.2948941609427973
  batch 327 loss: 0.29491570446104814
  batch 328 loss: 0.29470028914511204
  batch 329 loss: 0.29476859345805684
  batch 330 loss: 0.29474464505910875
  batch 331 loss: 0.294758505736593
  batch 332 loss: 0.2946755838053054
  batch 333 loss: 0.29469527939597406
  batch 334 loss: 0.2945544369920285
  batch 335 loss: 0.29443873006906085
  batch 336 loss: 0.2943161291380723
  batch 337 loss: 0.29425100049208464
  batch 338 loss: 0.29423322506557553
  batch 339 loss: 0.2941998598498229
  batch 340 loss: 0.2941312527831863
  batch 341 loss: 0.29403305490695136
  batch 342 loss: 0.29393686580727674
  batch 343 loss: 0.2939297533417582
  batch 344 loss: 0.29389992044415586
  batch 345 loss: 0.2940439418606136
  batch 346 loss: 0.29389062782243497
  batch 347 loss: 0.29383873887982764
  batch 348 loss: 0.29383901446715166
  batch 349 loss: 0.2937585325500685
  batch 350 loss: 0.29373890817165377
  batch 351 loss: 0.2937038111211228
  batch 352 loss: 0.2937249598855322
  batch 353 loss: 0.2937063408472045
  batch 354 loss: 0.2938699968117105
  batch 355 loss: 0.2938003066559913
  batch 356 loss: 0.29374805754155253
  batch 357 loss: 0.29365348790874
  batch 358 loss: 0.29370639893595735
  batch 359 loss: 0.29364558498175364
  batch 360 loss: 0.29365459713670944
  batch 361 loss: 0.29361961026601185
  batch 362 loss: 0.29353550932683997
  batch 363 loss: 0.29343676222257375
  batch 364 loss: 0.293333806343131
  batch 365 loss: 0.293338329089831
  batch 366 loss: 0.29332238338032707
  batch 367 loss: 0.2932582029854569
  batch 368 loss: 0.29308471334693226
  batch 369 loss: 0.29310923548248724
  batch 370 loss: 0.293078955766317
  batch 371 loss: 0.29311803850844864
  batch 372 loss: 0.29308728080603386
  batch 373 loss: 0.2930642611699194
  batch 374 loss: 0.2929127927132469
  batch 375 loss: 0.2928313813209534
  batch 376 loss: 0.29296072001786944
  batch 377 loss: 0.29289391391789565
  batch 378 loss: 0.29280547774027266
  batch 379 loss: 0.29290012484490086
  batch 380 loss: 0.292983000137304
  batch 381 loss: 0.29289410910581354
  batch 382 loss: 0.2928172276594252
  batch 383 loss: 0.29290162746651055
  batch 384 loss: 0.2927953739805768
  batch 385 loss: 0.2928567135488832
  batch 386 loss: 0.2927944906480572
  batch 387 loss: 0.29293793262745366
  batch 388 loss: 0.29293076409814284
  batch 389 loss: 0.29294121763393627
  batch 390 loss: 0.29298448723096115
  batch 391 loss: 0.29298306921558914
  batch 392 loss: 0.29299863633148526
  batch 393 loss: 0.2930316300944214
  batch 394 loss: 0.29304392060955164
  batch 395 loss: 0.2930114775518828
  batch 396 loss: 0.2931291141142749
  batch 397 loss: 0.2931644962326405
  batch 398 loss: 0.29311197981163484
  batch 399 loss: 0.2932036864877046
  batch 400 loss: 0.2932862981408835
  batch 401 loss: 0.29318486863835497
  batch 402 loss: 0.29321312600403876
  batch 403 loss: 0.29334974000530856
  batch 404 loss: 0.29342792758552155
  batch 405 loss: 0.29347421990500555
  batch 406 loss: 0.2935314408223617
  batch 407 loss: 0.2934565328116499
  batch 408 loss: 0.2935141391467814
  batch 409 loss: 0.2935246728306003
  batch 410 loss: 0.2935782168696566
  batch 411 loss: 0.2935697336991628
  batch 412 loss: 0.2935161236854433
  batch 413 loss: 0.29356926852796617
  batch 414 loss: 0.2935436544354987
  batch 415 loss: 0.29349989151380146
  batch 416 loss: 0.293586300399441
  batch 417 loss: 0.2935665968081934
  batch 418 loss: 0.293530993983506
  batch 419 loss: 0.2935440116395245
  batch 420 loss: 0.29350402780941554
  batch 421 loss: 0.2934457014830265
  batch 422 loss: 0.2935807869473905
  batch 423 loss: 0.2935211091317747
  batch 424 loss: 0.2934982586160021
  batch 425 loss: 0.2935108939339133
  batch 426 loss: 0.29347416128910764
  batch 427 loss: 0.2934924428999005
  batch 428 loss: 0.29341899157963064
  batch 429 loss: 0.293404876560598
  batch 430 loss: 0.29334124985129334
  batch 431 loss: 0.2934327306022777
  batch 432 loss: 0.2935293020887507
  batch 433 loss: 0.2934853301725542
  batch 434 loss: 0.2935261308185516
  batch 435 loss: 0.2934356577780055
  batch 436 loss: 0.2934502322876125
  batch 437 loss: 0.29352294996346706
  batch 438 loss: 0.2936134245161596
  batch 439 loss: 0.29356453589806525
  batch 440 loss: 0.2935963009568778
  batch 441 loss: 0.29351522396751545
  batch 442 loss: 0.29348938207550823
  batch 443 loss: 0.2934665404781531
  batch 444 loss: 0.29339744117077404
  batch 445 loss: 0.2934733679455318
  batch 446 loss: 0.2934278225016701
  batch 447 loss: 0.2933301384283659
  batch 448 loss: 0.29338969263647285
  batch 449 loss: 0.293409093583878
  batch 450 loss: 0.2933706398804983
  batch 451 loss: 0.2933564781481834
  batch 452 loss: 0.2934739925296961
  batch 453 loss: 0.2935155897214186
  batch 454 loss: 0.29355113779395686
  batch 455 loss: 0.29353675089039644
  batch 456 loss: 0.2935603775345443
  batch 457 loss: 0.29350170845275897
  batch 458 loss: 0.2934260987819022
  batch 459 loss: 0.293457250189937
  batch 460 loss: 0.2934993412831555
  batch 461 loss: 0.29347967493818533
  batch 462 loss: 0.293476383536409
  batch 463 loss: 0.29342525669616965
  batch 464 loss: 0.29345651510460624
  batch 465 loss: 0.2933908886166029
  batch 466 loss: 0.29322001382772506
  batch 467 loss: 0.2933013865472928
  batch 468 loss: 0.29327253895437616
  batch 469 loss: 0.29340792591892073
  batch 470 loss: 0.2933572767262763
  batch 471 loss: 0.29335771510555486
  batch 472 loss: 0.2932304906011638
LOSS train 0.2932304906011638 valid 0.28400546312332153
LOSS train 0.2932304906011638 valid 0.26774975657463074
LOSS train 0.2932304906011638 valid 0.2673279345035553
LOSS train 0.2932304906011638 valid 0.26459645479917526
LOSS train 0.2932304906011638 valid 0.2614810049533844
LOSS train 0.2932304906011638 valid 0.26610174278418225
LOSS train 0.2932304906011638 valid 0.2784230921949659
LOSS train 0.2932304906011638 valid 0.27726123481988907
LOSS train 0.2932304906011638 valid 0.27505195803112453
LOSS train 0.2932304906011638 valid 0.2755674809217453
LOSS train 0.2932304906011638 valid 0.27426559816707263
LOSS train 0.2932304906011638 valid 0.27389200280110043
LOSS train 0.2932304906011638 valid 0.27173568537602055
LOSS train 0.2932304906011638 valid 0.2728648451822145
LOSS train 0.2932304906011638 valid 0.26902427077293395
LOSS train 0.2932304906011638 valid 0.2687039468437433
LOSS train 0.2932304906011638 valid 0.26991922539823193
LOSS train 0.2932304906011638 valid 0.2713168677356508
LOSS train 0.2932304906011638 valid 0.2733513007038518
LOSS train 0.2932304906011638 valid 0.27206614464521406
LOSS train 0.2932304906011638 valid 0.2706757088502248
LOSS train 0.2932304906011638 valid 0.26851464807987213
LOSS train 0.2932304906011638 valid 0.26830194436985516
LOSS train 0.2932304906011638 valid 0.26733419485390186
LOSS train 0.2932304906011638 valid 0.26611169159412384
LOSS train 0.2932304906011638 valid 0.26609746710612225
LOSS train 0.2932304906011638 valid 0.2661991908594414
LOSS train 0.2932304906011638 valid 0.26686319016984533
LOSS train 0.2932304906011638 valid 0.26722249748377963
LOSS train 0.2932304906011638 valid 0.2680948848525683
LOSS train 0.2932304906011638 valid 0.26916797843671614
LOSS train 0.2932304906011638 valid 0.26860196935012937
LOSS train 0.2932304906011638 valid 0.269860230160482
LOSS train 0.2932304906011638 valid 0.2695305009975153
LOSS train 0.2932304906011638 valid 0.27129918549742016
LOSS train 0.2932304906011638 valid 0.27064843227465946
LOSS train 0.2932304906011638 valid 0.27051820062302256
LOSS train 0.2932304906011638 valid 0.27161867602875356
LOSS train 0.2932304906011638 valid 0.2711627055437137
LOSS train 0.2932304906011638 valid 0.2712031863629818
LOSS train 0.2932304906011638 valid 0.2721558758398382
LOSS train 0.2932304906011638 valid 0.27261464368729366
LOSS train 0.2932304906011638 valid 0.27242808840995614
LOSS train 0.2932304906011638 valid 0.2728421457789161
LOSS train 0.2932304906011638 valid 0.2722429533799489
LOSS train 0.2932304906011638 valid 0.2726796295331872
LOSS train 0.2932304906011638 valid 0.2736727988466303
LOSS train 0.2932304906011638 valid 0.2741236835718155
LOSS train 0.2932304906011638 valid 0.27448165964107124
LOSS train 0.2932304906011638 valid 0.2734898632764816
LOSS train 0.2932304906011638 valid 0.27365334478079106
LOSS train 0.2932304906011638 valid 0.27318257752519387
LOSS train 0.2932304906011638 valid 0.27338544518317814
LOSS train 0.2932304906011638 valid 0.27333275615065183
LOSS train 0.2932304906011638 valid 0.2731221998279745
LOSS train 0.2932304906011638 valid 0.27265207815383163
LOSS train 0.2932304906011638 valid 0.2722354139152326
LOSS train 0.2932304906011638 valid 0.27190964746064156
LOSS train 0.2932304906011638 valid 0.2722295694432016
LOSS train 0.2932304906011638 valid 0.2721197466055552
LOSS train 0.2932304906011638 valid 0.27170847307463164
LOSS train 0.2932304906011638 valid 0.2726369084369752
LOSS train 0.2932304906011638 valid 0.27248793012566036
LOSS train 0.2932304906011638 valid 0.27358232089318335
LOSS train 0.2932304906011638 valid 0.27374104880369626
LOSS train 0.2932304906011638 valid 0.27371021089228714
LOSS train 0.2932304906011638 valid 0.27277217427296424
LOSS train 0.2932304906011638 valid 0.2726710846318918
LOSS train 0.2932304906011638 valid 0.2720634174951609
LOSS train 0.2932304906011638 valid 0.2719207586986678
LOSS train 0.2932304906011638 valid 0.2716231805757737
LOSS train 0.2932304906011638 valid 0.27182739828195834
LOSS train 0.2932304906011638 valid 0.2717122298805681
LOSS train 0.2932304906011638 valid 0.2715285990286518
LOSS train 0.2932304906011638 valid 0.2716321227947871
LOSS train 0.2932304906011638 valid 0.27182381541321154
LOSS train 0.2932304906011638 valid 0.2719329314959514
LOSS train 0.2932304906011638 valid 0.2720218207209538
LOSS train 0.2932304906011638 valid 0.27213235124002527
LOSS train 0.2932304906011638 valid 0.2714527567848563
LOSS train 0.2932304906011638 valid 0.2705618365679258
LOSS train 0.2932304906011638 valid 0.27101338727445135
LOSS train 0.2932304906011638 valid 0.2708517788763506
LOSS train 0.2932304906011638 valid 0.27093887311362086
LOSS train 0.2932304906011638 valid 0.2708072699168149
LOSS train 0.2932304906011638 valid 0.2700279653072357
LOSS train 0.2932304906011638 valid 0.2696943056994471
LOSS train 0.2932304906011638 valid 0.2689674218947237
LOSS train 0.2932304906011638 valid 0.269586163290431
LOSS train 0.2932304906011638 valid 0.26964787443478905
LOSS train 0.2932304906011638 valid 0.26972238899587275
LOSS train 0.2932304906011638 valid 0.2700069617965947
LOSS train 0.2932304906011638 valid 0.2699334425951845
LOSS train 0.2932304906011638 valid 0.2700618568887102
LOSS train 0.2932304906011638 valid 0.2698792705410405
LOSS train 0.2932304906011638 valid 0.2701244705046217
LOSS train 0.2932304906011638 valid 0.270333829614305
LOSS train 0.2932304906011638 valid 0.2704672570131263
LOSS train 0.2932304906011638 valid 0.2706014092522438
LOSS train 0.2932304906011638 valid 0.27079066723585127
LOSS train 0.2932304906011638 valid 0.2710657748255399
LOSS train 0.2932304906011638 valid 0.2711261017065422
LOSS train 0.2932304906011638 valid 0.2708535885926589
LOSS train 0.2932304906011638 valid 0.2707856973776451
LOSS train 0.2932304906011638 valid 0.2708433435076759
LOSS train 0.2932304906011638 valid 0.27107034286238113
LOSS train 0.2932304906011638 valid 0.27091757883535367
LOSS train 0.2932304906011638 valid 0.27100273690841814
LOSS train 0.2932304906011638 valid 0.27133288788139276
LOSS train 0.2932304906011638 valid 0.27166594673286787
LOSS train 0.2932304906011638 valid 0.27121243659440464
LOSS train 0.2932304906011638 valid 0.2711078742785113
LOSS train 0.2932304906011638 valid 0.27107315807215937
LOSS train 0.2932304906011638 valid 0.2710524407917993
LOSS train 0.2932304906011638 valid 0.27111862275911414
LOSS train 0.2932304906011638 valid 0.27114322678796177
LOSS train 0.2932304906011638 valid 0.2713584466877147
LOSS train 0.2932304906011638 valid 0.2713497488175408
LOSS train 0.2932304906011638 valid 0.2712708363512985
LOSS train 0.2932304906011638 valid 0.27098191442588965
LOSS train 0.2932304906011638 valid 0.2708533491231193
LOSS train 0.2932304906011638 valid 0.27071699170304125
LOSS train 0.2932304906011638 valid 0.27051170642783
LOSS train 0.2932304906011638 valid 0.2708398644481936
LOSS train 0.2932304906011638 valid 0.2707488195896149
LOSS train 0.2932304906011638 valid 0.27102750161337474
LOSS train 0.2932304906011638 valid 0.2708716078067389
LOSS train 0.2932304906011638 valid 0.2712670748587698
LOSS train 0.2932304906011638 valid 0.2713782336822776
LOSS train 0.2932304906011638 valid 0.2713388507182781
LOSS train 0.2932304906011638 valid 0.2713238048644466
LOSS train 0.2932304906011638 valid 0.2710566448442864
LOSS train 0.2932304906011638 valid 0.2710072416111939
LOSS train 0.2932304906011638 valid 0.271129191811405
LOSS train 0.2932304906011638 valid 0.2710063600981677
LOSS train 0.2932304906011638 valid 0.2708640462335418
LOSS train 0.2932304906011638 valid 0.27066175378587126
LOSS train 0.2932304906011638 valid 0.270600370414879
LOSS train 0.2932304906011638 valid 0.2704687460506563
LOSS train 0.2932304906011638 valid 0.2705810337194375
LOSS train 0.2932304906011638 valid 0.2706227734790626
LOSS train 0.2932304906011638 valid 0.27096907930894637
LOSS train 0.2932304906011638 valid 0.27084865905604993
LOSS train 0.2932304906011638 valid 0.27085779265811044
LOSS train 0.2932304906011638 valid 0.2705815814692399
LOSS train 0.2932304906011638 valid 0.27082564936925285
LOSS train 0.2932304906011638 valid 0.2705272446278812
LOSS train 0.2932304906011638 valid 0.27114446783387985
LOSS train 0.2932304906011638 valid 0.2713218897381085
LOSS train 0.2932304906011638 valid 0.2712527447938919
LOSS train 0.2932304906011638 valid 0.271375141001695
LOSS train 0.2932304906011638 valid 0.2710935604807578
LOSS train 0.2932304906011638 valid 0.2712497621579887
LOSS train 0.2932304906011638 valid 0.27103588294673275
LOSS train 0.2932304906011638 valid 0.2710477161792017
LOSS train 0.2932304906011638 valid 0.2712082687096718
LOSS train 0.2932304906011638 valid 0.27118383376461686
LOSS train 0.2932304906011638 valid 0.271349817137175
LOSS train 0.2932304906011638 valid 0.2713813607422811
LOSS train 0.2932304906011638 valid 0.271334714256227
LOSS train 0.2932304906011638 valid 0.27121431542479474
LOSS train 0.2932304906011638 valid 0.2711170260553007
LOSS train 0.2932304906011638 valid 0.270956811546548
LOSS train 0.2932304906011638 valid 0.27068197999785587
LOSS train 0.2932304906011638 valid 0.27055550452434657
LOSS train 0.2932304906011638 valid 0.2707051279315029
LOSS train 0.2932304906011638 valid 0.2710269793065008
LOSS train 0.2932304906011638 valid 0.27086329903630985
LOSS train 0.2932304906011638 valid 0.27107014246946254
LOSS train 0.2932304906011638 valid 0.27121399430667653
LOSS train 0.2932304906011638 valid 0.2712121506532033
LOSS train 0.2932304906011638 valid 0.2710843441444774
LOSS train 0.2932304906011638 valid 0.2711788030029032
LOSS train 0.2932304906011638 valid 0.2711797024326763
LOSS train 0.2932304906011638 valid 0.27100302858012065
LOSS train 0.2932304906011638 valid 0.27095209612426435
LOSS train 0.2932304906011638 valid 0.2709516410605382
LOSS train 0.2932304906011638 valid 0.27112450149287
LOSS train 0.2932304906011638 valid 0.2710669892293781
LOSS train 0.2932304906011638 valid 0.2711326697634326
LOSS train 0.2932304906011638 valid 0.2714055000253804
LOSS train 0.2932304906011638 valid 0.27140658774546217
LOSS train 0.2932304906011638 valid 0.2713192656554811
LOSS train 0.2932304906011638 valid 0.27136064426082634
LOSS train 0.2932304906011638 valid 0.27107113366191454
LOSS train 0.2932304906011638 valid 0.27102560758270244
LOSS train 0.2932304906011638 valid 0.2709991693656075
LOSS train 0.2932304906011638 valid 0.27101911580626
LOSS train 0.2932304906011638 valid 0.2710236655498939
LOSS train 0.2932304906011638 valid 0.2711783897719885
LOSS train 0.2932304906011638 valid 0.27111275728148315
LOSS train 0.2932304906011638 valid 0.2711800789305319
LOSS train 0.2932304906011638 valid 0.2710911741991735
LOSS train 0.2932304906011638 valid 0.27098045175530244
LOSS train 0.2932304906011638 valid 0.27083307626919867
LOSS train 0.2932304906011638 valid 0.27082575340660253
LOSS train 0.2932304906011638 valid 0.27101181470198077
LOSS train 0.2932304906011638 valid 0.2708878569831752
LOSS train 0.2932304906011638 valid 0.27091466317224744
LOSS train 0.2932304906011638 valid 0.2707618590444326
LOSS train 0.2932304906011638 valid 0.2705382394849958
LOSS train 0.2932304906011638 valid 0.2704624616273559
LOSS train 0.2932304906011638 valid 0.27030398560862234
LOSS train 0.2932304906011638 valid 0.2702550053888676
LOSS train 0.2932304906011638 valid 0.27002980185718073
LOSS train 0.2932304906011638 valid 0.27004188212376196
LOSS train 0.2932304906011638 valid 0.2699204257864883
LOSS train 0.2932304906011638 valid 0.2697095869538876
LOSS train 0.2932304906011638 valid 0.26960193020019807
LOSS train 0.2932304906011638 valid 0.2696618235536984
LOSS train 0.2932304906011638 valid 0.26976448078573595
LOSS train 0.2932304906011638 valid 0.26972437656994136
LOSS train 0.2932304906011638 valid 0.26981489038523376
LOSS train 0.2932304906011638 valid 0.26981958085409946
LOSS train 0.2932304906011638 valid 0.2696693726750307
LOSS train 0.2932304906011638 valid 0.2696477064379939
LOSS train 0.2932304906011638 valid 0.2697034827025805
LOSS train 0.2932304906011638 valid 0.26979027920906695
LOSS train 0.2932304906011638 valid 0.26990012259788165
LOSS train 0.2932304906011638 valid 0.2699625444683162
LOSS train 0.2932304906011638 valid 0.27002272751536305
LOSS train 0.2932304906011638 valid 0.2699737176970319
LOSS train 0.2932304906011638 valid 0.2701527782352516
LOSS train 0.2932304906011638 valid 0.27026963300470797
LOSS train 0.2932304906011638 valid 0.2702492076820797
LOSS train 0.2932304906011638 valid 0.2703466931011824
LOSS train 0.2932304906011638 valid 0.2706041087925697
LOSS train 0.2932304906011638 valid 0.2707796703305161
LOSS train 0.2932304906011638 valid 0.2708338499069214
LOSS train 0.2932304906011638 valid 0.2709083095840786
LOSS train 0.2932304906011638 valid 0.27093942818187533
LOSS train 0.2932304906011638 valid 0.27100290807670563
LOSS train 0.2932304906011638 valid 0.2709741474732821
LOSS train 0.2932304906011638 valid 0.2709596927604105
LOSS train 0.2932304906011638 valid 0.271012467526375
LOSS train 0.2932304906011638 valid 0.2708296080009412
LOSS train 0.2932304906011638 valid 0.270940528514516
LOSS train 0.2932304906011638 valid 0.27094329881067036
LOSS train 0.2932304906011638 valid 0.2708059040572354
LOSS train 0.2932304906011638 valid 0.2707207111020883
LOSS train 0.2932304906011638 valid 0.27076347994606526
LOSS train 0.2932304906011638 valid 0.27052932903786336
LOSS train 0.2932304906011638 valid 0.27067166493262773
LOSS train 0.2932304906011638 valid 0.27096168037320745
LOSS train 0.2932304906011638 valid 0.2711243607559983
LOSS train 0.2932304906011638 valid 0.2710921329453709
LOSS train 0.2932304906011638 valid 0.2711521938503513
LOSS train 0.2932304906011638 valid 0.27111452649677953
LOSS train 0.2932304906011638 valid 0.2710386688211357
LOSS train 0.2932304906011638 valid 0.2712164491415024
LOSS train 0.2932304906011638 valid 0.27121677139840755
LOSS train 0.2932304906011638 valid 0.27143759091222097
LOSS train 0.2932304906011638 valid 0.27140112757211615
LOSS train 0.2932304906011638 valid 0.27130756380520465
LOSS train 0.2932304906011638 valid 0.271431875930113
LOSS train 0.2932304906011638 valid 0.2714968507643789
LOSS train 0.2932304906011638 valid 0.2713296976418811
LOSS train 0.2932304906011638 valid 0.27145370350096576
LOSS train 0.2932304906011638 valid 0.27139264627082926
LOSS train 0.2932304906011638 valid 0.27138409024247756
LOSS train 0.2932304906011638 valid 0.2715370247989779
LOSS train 0.2932304906011638 valid 0.2715386612383464
LOSS train 0.2932304906011638 valid 0.2715187923196604
LOSS train 0.2932304906011638 valid 0.2715416561360612
LOSS train 0.2932304906011638 valid 0.27147666502673673
LOSS train 0.2932304906011638 valid 0.27150405926587884
LOSS train 0.2932304906011638 valid 0.27159493999758016
LOSS train 0.2932304906011638 valid 0.27178502499834817
LOSS train 0.2932304906011638 valid 0.27185664439511564
LOSS train 0.2932304906011638 valid 0.2718548297330185
LOSS train 0.2932304906011638 valid 0.2719115767201814
LOSS train 0.2932304906011638 valid 0.2721213601420031
LOSS train 0.2932304906011638 valid 0.2721577194474992
LOSS train 0.2932304906011638 valid 0.27211500970769104
LOSS train 0.2932304906011638 valid 0.2720577635006471
LOSS train 0.2932304906011638 valid 0.2718878387325052
LOSS train 0.2932304906011638 valid 0.27169218912236526
LOSS train 0.2932304906011638 valid 0.2715459121324176
LOSS train 0.2932304906011638 valid 0.27150267859299976
LOSS train 0.2932304906011638 valid 0.2714543329285724
LOSS train 0.2932304906011638 valid 0.27128852050075325
LOSS train 0.2932304906011638 valid 0.2711260070931827
LOSS train 0.2932304906011638 valid 0.2710954968689187
LOSS train 0.2932304906011638 valid 0.2711261441065392
LOSS train 0.2932304906011638 valid 0.2711804229439351
LOSS train 0.2932304906011638 valid 0.27114685541474737
LOSS train 0.2932304906011638 valid 0.2710391680226509
LOSS train 0.2932304906011638 valid 0.2710421454264886
LOSS train 0.2932304906011638 valid 0.2710856422524139
LOSS train 0.2932304906011638 valid 0.2710714789813963
LOSS train 0.2932304906011638 valid 0.27100627155033585
LOSS train 0.2932304906011638 valid 0.2710554971780679
LOSS train 0.2932304906011638 valid 0.27107005665530115
LOSS train 0.2932304906011638 valid 0.27119273864695814
LOSS train 0.2932304906011638 valid 0.2712578260797565
LOSS train 0.2932304906011638 valid 0.2712508563355014
LOSS train 0.2932304906011638 valid 0.2712756246328354
LOSS train 0.2932304906011638 valid 0.2712532319378533
LOSS train 0.2932304906011638 valid 0.2712987981811415
LOSS train 0.2932304906011638 valid 0.27124673599998156
LOSS train 0.2932304906011638 valid 0.27129538941422965
LOSS train 0.2932304906011638 valid 0.27132767789213863
LOSS train 0.2932304906011638 valid 0.2714091773965571
LOSS train 0.2932304906011638 valid 0.2714016518408531
LOSS train 0.2932304906011638 valid 0.27127286513320736
LOSS train 0.2932304906011638 valid 0.2712224232507687
LOSS train 0.2932304906011638 valid 0.2711343600715022
LOSS train 0.2932304906011638 valid 0.2710202956064181
LOSS train 0.2932304906011638 valid 0.27106524714567126
LOSS train 0.2932304906011638 valid 0.2710994483482453
LOSS train 0.2932304906011638 valid 0.27101974840907805
LOSS train 0.2932304906011638 valid 0.2710301933857875
LOSS train 0.2932304906011638 valid 0.2710662195191216
LOSS train 0.2932304906011638 valid 0.2711569847669571
LOSS train 0.2932304906011638 valid 0.2711211039433404
LOSS train 0.2932304906011638 valid 0.2710564932962762
LOSS train 0.2932304906011638 valid 0.27109481554114107
LOSS train 0.2932304906011638 valid 0.2711759037660353
LOSS train 0.2932304906011638 valid 0.27130228612677054
LOSS train 0.2932304906011638 valid 0.2712370042223483
LOSS train 0.2932304906011638 valid 0.2712817119196568
LOSS train 0.2932304906011638 valid 0.2711754748077126
LOSS train 0.2932304906011638 valid 0.2710789034241124
LOSS train 0.2932304906011638 valid 0.27102727801711474
LOSS train 0.2932304906011638 valid 0.271054137303279
LOSS train 0.2932304906011638 valid 0.2711489296581116
LOSS train 0.2932304906011638 valid 0.27116262493512683
LOSS train 0.2932304906011638 valid 0.2711765722530644
LOSS train 0.2932304906011638 valid 0.2712730998145048
LOSS train 0.2932304906011638 valid 0.271259460756273
LOSS train 0.2932304906011638 valid 0.27115731990229325
LOSS train 0.2932304906011638 valid 0.2709988741630531
LOSS train 0.2932304906011638 valid 0.2710315576186767
LOSS train 0.2932304906011638 valid 0.27121463078938557
LOSS train 0.2932304906011638 valid 0.27114186602741924
LOSS train 0.2932304906011638 valid 0.2710925401410177
LOSS train 0.2932304906011638 valid 0.27104304695518505
LOSS train 0.2932304906011638 valid 0.2710015591696875
LOSS train 0.2932304906011638 valid 0.2709446068094895
LOSS train 0.2932304906011638 valid 0.27092548218720097
LOSS train 0.2932304906011638 valid 0.2708072022608648
LOSS train 0.2932304906011638 valid 0.27080011620507605
LOSS train 0.2932304906011638 valid 0.2707739896398939
LOSS train 0.2932304906011638 valid 0.27093327929114186
LOSS train 0.2932304906011638 valid 0.27098767843799315
LOSS train 0.2932304906011638 valid 0.2709581011292562
LOSS train 0.2932304906011638 valid 0.27078905821877186
LOSS train 0.2932304906011638 valid 0.27068926745105065
LOSS train 0.2932304906011638 valid 0.270734104095694
LOSS train 0.2932304906011638 valid 0.270657690678324
LOSS train 0.2932304906011638 valid 0.2705963296003831
LOSS train 0.2932304906011638 valid 0.2705741336315193
LOSS train 0.2932304906011638 valid 0.270637788321749
LOSS train 0.2932304906011638 valid 0.27072651612724963
LOSS train 0.2932304906011638 valid 0.2708484723954134
LOSS train 0.2932304906011638 valid 0.27092903044618916
LOSS train 0.2932304906011638 valid 0.27089994507176535
LOSS train 0.2932304906011638 valid 0.27085971295334105
LOSS train 0.2932304906011638 valid 0.2708867518101562
LOSS train 0.2932304906011638 valid 0.2708331115129921
LOSS train 0.2932304906011638 valid 0.27082880592577346
LOSS train 0.2932304906011638 valid 0.27088557550097037
LOSS train 0.2932304906011638 valid 0.27073863464774506
LOSS train 0.2932304906011638 valid 0.27075824661405534
LOSS train 0.2932304906011638 valid 0.27073966260642224
LOSS train 0.2932304906011638 valid 0.2706545395929305
LOSS train 0.2932304906011638 valid 0.2705365010519443
LOSS train 0.2932304906011638 valid 0.270526759043012
LOSS train 0.2932304906011638 valid 0.2706336324534765
EPOCH 6:
  batch 1 loss: 0.27453845739364624
  batch 2 loss: 0.26801761984825134
  batch 3 loss: 0.28431302309036255
  batch 4 loss: 0.28617270290851593
  batch 5 loss: 0.2935953676700592
  batch 6 loss: 0.29119070370992023
  batch 7 loss: 0.2948077235903059
  batch 8 loss: 0.2952842302620411
  batch 9 loss: 0.2961614165041182
  batch 10 loss: 0.29522954523563383
  batch 11 loss: 0.2956700921058655
  batch 12 loss: 0.29237767805655795
  batch 13 loss: 0.29213648346754223
  batch 14 loss: 0.29188385392938343
  batch 15 loss: 0.29365487893422443
  batch 16 loss: 0.2938393298536539
  batch 17 loss: 0.2910860513939577
  batch 18 loss: 0.2909485532177819
  batch 19 loss: 0.28964463190028544
  batch 20 loss: 0.28690520003437997
  batch 21 loss: 0.2879218054669244
  batch 22 loss: 0.2881324975328012
  batch 23 loss: 0.28732644280661707
  batch 24 loss: 0.28599864679078263
  batch 25 loss: 0.289791402220726
  batch 26 loss: 0.28856392491322297
  batch 27 loss: 0.28912596846068345
  batch 28 loss: 0.288378220051527
  batch 29 loss: 0.2900905778695797
  batch 30 loss: 0.28965129305919013
  batch 31 loss: 0.2897329037227938
  batch 32 loss: 0.2894369033165276
  batch 33 loss: 0.28969699279828504
  batch 34 loss: 0.2894221592475386
  batch 35 loss: 0.28936253998960765
  batch 36 loss: 0.28950610881050426
  batch 37 loss: 0.2900106765933939
  batch 38 loss: 0.2906639093631192
  batch 39 loss: 0.2905008452825057
  batch 40 loss: 0.2906292337924242
  batch 41 loss: 0.2907384528619487
  batch 42 loss: 0.29106847835438593
  batch 43 loss: 0.29229061028292014
  batch 44 loss: 0.29259645024483855
  batch 45 loss: 0.29217821260293325
  batch 46 loss: 0.29258157021325565
  batch 47 loss: 0.2925302376772495
  batch 48 loss: 0.2916229035084446
  batch 49 loss: 0.29125628453128194
  batch 50 loss: 0.29076810747385023
  batch 51 loss: 0.29036817451318103
  batch 52 loss: 0.29072898483047116
  batch 53 loss: 0.2905218626530665
  batch 54 loss: 0.2907058996734796
  batch 55 loss: 0.2902089181271466
  batch 56 loss: 0.289634231744068
  batch 57 loss: 0.28926969123514074
  batch 58 loss: 0.2903774491158025
  batch 59 loss: 0.29029119090508604
  batch 60 loss: 0.29002798919876416
  batch 61 loss: 0.2901303194096831
  batch 62 loss: 0.2908703071936484
  batch 63 loss: 0.29059331828639623
  batch 64 loss: 0.29142760834656656
  batch 65 loss: 0.29138426666076367
  batch 66 loss: 0.2912822060964324
  batch 67 loss: 0.2921403154953202
  batch 68 loss: 0.2923607740770368
  batch 69 loss: 0.2922237396672152
  batch 70 loss: 0.2924484229513577
  batch 71 loss: 0.2920717548736384
  batch 72 loss: 0.29211238047315013
  batch 73 loss: 0.2922619146435228
  batch 74 loss: 0.2924851040179665
  batch 75 loss: 0.2921006625890732
  batch 76 loss: 0.29225156593479606
  batch 77 loss: 0.2930892912985443
  batch 78 loss: 0.2928350607936199
  batch 79 loss: 0.29359915079195287
  batch 80 loss: 0.2939109357073903
  batch 81 loss: 0.29409810845498685
  batch 82 loss: 0.2945175841450691
  batch 83 loss: 0.29489561394754665
  batch 84 loss: 0.2947331399080299
  batch 85 loss: 0.2946125042789123
  batch 86 loss: 0.29520735231249834
  batch 87 loss: 0.29510642759416295
  batch 88 loss: 0.2948501795868982
  batch 89 loss: 0.2944083001171605
  batch 90 loss: 0.2940987876719899
  batch 91 loss: 0.2939674308994314
  batch 92 loss: 0.2937644378322622
  batch 93 loss: 0.2937599912446032
  batch 94 loss: 0.2943440778775418
  batch 95 loss: 0.2940361184509177
  batch 96 loss: 0.29431773085768026
  batch 97 loss: 0.29476211289155113
  batch 98 loss: 0.2947028338605044
  batch 99 loss: 0.2949323279388023
  batch 100 loss: 0.29483859285712244
  batch 101 loss: 0.2949499383126155
  batch 102 loss: 0.29519040049875483
  batch 103 loss: 0.29551627060163366
  batch 104 loss: 0.29553914886827654
  batch 105 loss: 0.29528061222462426
  batch 106 loss: 0.2955559634093968
  batch 107 loss: 0.2952560070797662
  batch 108 loss: 0.29507547051266386
  batch 109 loss: 0.2948394484749628
  batch 110 loss: 0.2948539591648362
  batch 111 loss: 0.294485528055612
  batch 112 loss: 0.29430687121514765
  batch 113 loss: 0.2948641659675446
  batch 114 loss: 0.2951125098686469
  batch 115 loss: 0.29565058078454887
  batch 116 loss: 0.2961470737796405
  batch 117 loss: 0.29643985896538466
  batch 118 loss: 0.2963158317794234
  batch 119 loss: 0.29660100633857633
  batch 120 loss: 0.2967939291149378
  batch 121 loss: 0.2969724401708477
  batch 122 loss: 0.29695271749476915
  batch 123 loss: 0.2969316218684359
  batch 124 loss: 0.2972972939812368
  batch 125 loss: 0.2973046182394028
  batch 126 loss: 0.2972686854856355
  batch 127 loss: 0.29756198565321645
  batch 128 loss: 0.29770452494267374
  batch 129 loss: 0.29796492926372115
  batch 130 loss: 0.2979056391578454
  batch 131 loss: 0.2980717867612839
  batch 132 loss: 0.2981362077548648
  batch 133 loss: 0.29835038581737
  batch 134 loss: 0.29834290326976065
  batch 135 loss: 0.2986387481292089
  batch 136 loss: 0.2988141677616274
  batch 137 loss: 0.2989557016722477
  batch 138 loss: 0.298964563595212
  batch 139 loss: 0.29930377531823493
  batch 140 loss: 0.29915007959519113
  batch 141 loss: 0.29935228708365286
  batch 142 loss: 0.2993377229907143
  batch 143 loss: 0.2993456117965125
  batch 144 loss: 0.29930534017168814
  batch 145 loss: 0.2992257322730689
  batch 146 loss: 0.29920584667627126
  batch 147 loss: 0.2995763497084987
  batch 148 loss: 0.2993773337755654
  batch 149 loss: 0.2990822193006541
  batch 150 loss: 0.2992380347847938
  batch 151 loss: 0.2993731914174478
  batch 152 loss: 0.2996103978274684
  batch 153 loss: 0.29958216709638735
  batch 154 loss: 0.299723726685171
  batch 155 loss: 0.29967122356737813
  batch 156 loss: 0.29945130636676764
  batch 157 loss: 0.2995168850490242
  batch 158 loss: 0.29954001580989814
  batch 159 loss: 0.29948259758874307
  batch 160 loss: 0.29918607948347925
  batch 161 loss: 0.29925043381148986
  batch 162 loss: 0.299226084885038
  batch 163 loss: 0.2990392125640179
  batch 164 loss: 0.29903296735592005
  batch 165 loss: 0.29889570050167313
  batch 166 loss: 0.2987820261034621
  batch 167 loss: 0.2987192579193743
  batch 168 loss: 0.2987573663925841
  batch 169 loss: 0.29851038454199685
  batch 170 loss: 0.2982274163295241
  batch 171 loss: 0.29812027845117783
  batch 172 loss: 0.2981240256407926
  batch 173 loss: 0.29801579491596003
  batch 174 loss: 0.29791084454319944
  batch 175 loss: 0.29795101821422576
  batch 176 loss: 0.2977602399716323
  batch 177 loss: 0.2977211784339894
  batch 178 loss: 0.2977544723099537
  batch 179 loss: 0.297869663248515
  batch 180 loss: 0.297689577523205
  batch 181 loss: 0.29775940227574405
  batch 182 loss: 0.2976295331513489
  batch 183 loss: 0.297299490030346
  batch 184 loss: 0.2971056792885065
  batch 185 loss: 0.2971524067021705
  batch 186 loss: 0.29723194425785415
  batch 187 loss: 0.29719588407539427
  batch 188 loss: 0.29683352356895487
  batch 189 loss: 0.2965347897754144
  batch 190 loss: 0.2966222146624013
  batch 191 loss: 0.2964952215162247
  batch 192 loss: 0.296493339817971
  batch 193 loss: 0.29653551062771694
  batch 194 loss: 0.2966669567467011
  batch 195 loss: 0.296724929412206
  batch 196 loss: 0.2964998549040483
  batch 197 loss: 0.29642880961374585
  batch 198 loss: 0.2966722466728904
  batch 199 loss: 0.2964940680630842
  batch 200 loss: 0.29654677882790564
  batch 201 loss: 0.29670950547972724
  batch 202 loss: 0.29669631486481957
  batch 203 loss: 0.296507309076234
  batch 204 loss: 0.29631537184411405
  batch 205 loss: 0.2963535157645621
  batch 206 loss: 0.29625801915682637
  batch 207 loss: 0.29630834180951693
  batch 208 loss: 0.2961290438587849
  batch 209 loss: 0.2958534382747121
  batch 210 loss: 0.2959420023929505
  batch 211 loss: 0.29585021677740375
  batch 212 loss: 0.2957353525847759
  batch 213 loss: 0.2955343261570998
  batch 214 loss: 0.29546815922884184
  batch 215 loss: 0.29519587401733843
  batch 216 loss: 0.294913561048883
  batch 217 loss: 0.29488029890620765
  batch 218 loss: 0.29485402980802256
  batch 219 loss: 0.29504258182222987
  batch 220 loss: 0.29500911513512784
  batch 221 loss: 0.2949698705613883
  batch 222 loss: 0.2950007906770921
  batch 223 loss: 0.2948761383915161
  batch 224 loss: 0.2948102388930108
  batch 225 loss: 0.2946389380428526
  batch 226 loss: 0.2946360678931253
  batch 227 loss: 0.29443469381017306
  batch 228 loss: 0.29425434781271115
  batch 229 loss: 0.2941804826259613
  batch 230 loss: 0.294255119950875
  batch 231 loss: 0.29417316609130795
  batch 232 loss: 0.2940151932938346
  batch 233 loss: 0.29391502553812937
  batch 234 loss: 0.29395858128356117
  batch 235 loss: 0.2939047413937589
  batch 236 loss: 0.2937591986383422
  batch 237 loss: 0.29380118318750886
  batch 238 loss: 0.29377094236742546
  batch 239 loss: 0.2936682022765092
  batch 240 loss: 0.2936543356627226
  batch 241 loss: 0.2937862165735965
  batch 242 loss: 0.2936175254997143
  batch 243 loss: 0.2936446035104524
  batch 244 loss: 0.29359671143723315
  batch 245 loss: 0.29350559577649954
  batch 246 loss: 0.2935000590192593
  batch 247 loss: 0.29355350293611226
  batch 248 loss: 0.2935253309626733
  batch 249 loss: 0.29338860715249454
  batch 250 loss: 0.29325948703289034
  batch 251 loss: 0.2933017656622655
  batch 252 loss: 0.2930600501833454
  batch 253 loss: 0.2929283771001303
  batch 254 loss: 0.29282528167869165
  batch 255 loss: 0.2927305053846509
  batch 256 loss: 0.29284474247833714
  batch 257 loss: 0.29276483085600785
  batch 258 loss: 0.29265188056138136
  batch 259 loss: 0.2925313366541071
  batch 260 loss: 0.29254152493981217
  batch 261 loss: 0.29246921072289406
  batch 262 loss: 0.29229116530818794
  batch 263 loss: 0.29230263806567897
  batch 264 loss: 0.29217249685616203
  batch 265 loss: 0.29199308095113286
  batch 266 loss: 0.29187744099618795
  batch 267 loss: 0.2920229722386442
  batch 268 loss: 0.29188208209712113
  batch 269 loss: 0.2917763979129189
  batch 270 loss: 0.2918857274783982
  batch 271 loss: 0.29182088314607135
  batch 272 loss: 0.2919069373541895
  batch 273 loss: 0.29183557274795713
  batch 274 loss: 0.2918162496529356
  batch 275 loss: 0.29181759146126834
  batch 276 loss: 0.29171756228459056
  batch 277 loss: 0.291702806250283
  batch 278 loss: 0.29158110632527645
  batch 279 loss: 0.2915954481423115
  batch 280 loss: 0.2915215702461345
  batch 281 loss: 0.2914219947790336
  batch 282 loss: 0.29138639532293836
  batch 283 loss: 0.29130586902791955
  batch 284 loss: 0.2913622115804276
  batch 285 loss: 0.2913043133522335
  batch 286 loss: 0.29131717203588753
  batch 287 loss: 0.2913269865284398
  batch 288 loss: 0.2911367391029166
  batch 289 loss: 0.2913947608128551
  batch 290 loss: 0.29117629322512395
  batch 291 loss: 0.2911777928522772
  batch 292 loss: 0.29129515996534533
  batch 293 loss: 0.2913327853834263
  batch 294 loss: 0.2911821556131856
  batch 295 loss: 0.29109834038605126
  batch 296 loss: 0.2911253870540374
  batch 297 loss: 0.29111229239489494
  batch 298 loss: 0.2911407682519631
  batch 299 loss: 0.29102133976974615
  batch 300 loss: 0.29110074748595555
  batch 301 loss: 0.2910277239508011
  batch 302 loss: 0.2909527878887606
  batch 303 loss: 0.29095112067638057
  batch 304 loss: 0.29095565429643583
  batch 305 loss: 0.2907747133833463
  batch 306 loss: 0.29090073821591395
  batch 307 loss: 0.29091351486572614
  batch 308 loss: 0.29107141436694506
  batch 309 loss: 0.29096887759791995
  batch 310 loss: 0.29099659092964664
  batch 311 loss: 0.2910515100626317
  batch 312 loss: 0.29110399729166275
  batch 313 loss: 0.2911015261476413
  batch 314 loss: 0.2910258724431323
  batch 315 loss: 0.29098368883132936
  batch 316 loss: 0.2909093497297432
  batch 317 loss: 0.29087843052596324
  batch 318 loss: 0.290807731402745
  batch 319 loss: 0.2906820128517091
  batch 320 loss: 0.29059814345091584
  batch 321 loss: 0.2906697288480503
  batch 322 loss: 0.2906269139569739
  batch 323 loss: 0.2905955881157158
  batch 324 loss: 0.2905302053248441
  batch 325 loss: 0.29041084977296683
  batch 326 loss: 0.29039892654843125
  batch 327 loss: 0.2904278805496496
  batch 328 loss: 0.2901969512061375
  batch 329 loss: 0.2902781496837871
  batch 330 loss: 0.29024521554961347
  batch 331 loss: 0.2902404405739372
  batch 332 loss: 0.29013210867183753
  batch 333 loss: 0.2901667577547354
  batch 334 loss: 0.2900118423614673
  batch 335 loss: 0.289880421134963
  batch 336 loss: 0.2897224190334479
  batch 337 loss: 0.28970595150743816
  batch 338 loss: 0.28965118800747325
  batch 339 loss: 0.28962341874046665
  batch 340 loss: 0.28960499710896437
  batch 341 loss: 0.28948522418125633
  batch 342 loss: 0.2893614262255312
  batch 343 loss: 0.28933705737048615
  batch 344 loss: 0.2892944561967323
  batch 345 loss: 0.2893572703219842
  batch 346 loss: 0.2892310958291065
  batch 347 loss: 0.28915025087701485
  batch 348 loss: 0.28911915588481674
  batch 349 loss: 0.2890326514370462
  batch 350 loss: 0.2890311066167695
  batch 351 loss: 0.288969874849007
  batch 352 loss: 0.2890299405817958
  batch 353 loss: 0.28901937397107186
  batch 354 loss: 0.2891359554424798
  batch 355 loss: 0.2890550136146411
  batch 356 loss: 0.2890040913408392
  batch 357 loss: 0.28888857135084833
  batch 358 loss: 0.28890585687073916
  batch 359 loss: 0.2888143303749621
  batch 360 loss: 0.2887872642527024
  batch 361 loss: 0.28872997860664146
  batch 362 loss: 0.28861688629039745
  batch 363 loss: 0.2885091230888997
  batch 364 loss: 0.2883940488918797
  batch 365 loss: 0.28834828032206183
  batch 366 loss: 0.28830189945919266
  batch 367 loss: 0.28825468913086105
  batch 368 loss: 0.28809877788728994
  batch 369 loss: 0.28805965601105676
  batch 370 loss: 0.2880532617101798
  batch 371 loss: 0.2881161493030841
  batch 372 loss: 0.2880047520081843
  batch 373 loss: 0.28798738725223744
  batch 374 loss: 0.2878266142651359
  batch 375 loss: 0.28774420674641926
  batch 376 loss: 0.2878459921067065
  batch 377 loss: 0.2878035775704472
  batch 378 loss: 0.28771828217481177
  batch 379 loss: 0.2877429248789684
  batch 380 loss: 0.2878694833893525
  batch 381 loss: 0.2877772487211102
  batch 382 loss: 0.28771494149537613
  batch 383 loss: 0.287764023396116
  batch 384 loss: 0.2877141807693988
  batch 385 loss: 0.2877690055927673
  batch 386 loss: 0.287686762414448
  batch 387 loss: 0.2877802472243937
  batch 388 loss: 0.28780729246815456
  batch 389 loss: 0.2877720772913612
  batch 390 loss: 0.2877703485580591
  batch 391 loss: 0.2877421975135803
  batch 392 loss: 0.28770642157415954
  batch 393 loss: 0.28765893490563216
  batch 394 loss: 0.2876631320279262
  batch 395 loss: 0.28756947713562203
  batch 396 loss: 0.2875453844064414
  batch 397 loss: 0.2875668186534862
  batch 398 loss: 0.287498286845696
  batch 399 loss: 0.28747588328849105
  batch 400 loss: 0.2875062330067158
  batch 401 loss: 0.2874259992579272
  batch 402 loss: 0.2874441607229745
  batch 403 loss: 0.2875280843922873
  batch 404 loss: 0.28759459605311405
  batch 405 loss: 0.2876208547456765
  batch 406 loss: 0.2876803840469257
  batch 407 loss: 0.2876183634454554
  batch 408 loss: 0.2876544170373795
  batch 409 loss: 0.287674152253601
  batch 410 loss: 0.2877335828978841
  batch 411 loss: 0.28773885617290973
  batch 412 loss: 0.2876942384879566
  batch 413 loss: 0.287739188561428
  batch 414 loss: 0.2876756459762509
  batch 415 loss: 0.2875994239226881
  batch 416 loss: 0.28764277030355656
  batch 417 loss: 0.28760931960684505
  batch 418 loss: 0.2875569106289074
  batch 419 loss: 0.2875804944510676
  batch 420 loss: 0.28753904380968637
  batch 421 loss: 0.2874468237586372
  batch 422 loss: 0.2875374889274909
  batch 423 loss: 0.2874856311167386
  batch 424 loss: 0.2874217677580298
  batch 425 loss: 0.2874046708205167
  batch 426 loss: 0.287327191184664
  batch 427 loss: 0.28733866984587364
  batch 428 loss: 0.28727414740998053
  batch 429 loss: 0.28723526240645586
  batch 430 loss: 0.2871622702063516
  batch 431 loss: 0.28724514922935285
  batch 432 loss: 0.28731045468399924
  batch 433 loss: 0.28727331058946
  batch 434 loss: 0.28730264529242494
  batch 435 loss: 0.2871994763270192
  batch 436 loss: 0.2871879089042681
  batch 437 loss: 0.28729233527619996
  batch 438 loss: 0.28737568086412946
  batch 439 loss: 0.28734711525380474
  batch 440 loss: 0.2874316103756428
  batch 441 loss: 0.28735248430245586
  batch 442 loss: 0.287354326693181
  batch 443 loss: 0.2873361397946661
  batch 444 loss: 0.28725582663271876
  batch 445 loss: 0.2873262566796849
  batch 446 loss: 0.2873051504650458
  batch 447 loss: 0.28720155961694865
  batch 448 loss: 0.2872148226347885
  batch 449 loss: 0.2872381126362922
  batch 450 loss: 0.28720921830998525
  batch 451 loss: 0.2872169367688194
  batch 452 loss: 0.28729866580229946
  batch 453 loss: 0.28728586885971213
  batch 454 loss: 0.2873337385347236
  batch 455 loss: 0.2873125267880304
  batch 456 loss: 0.28730702338119346
  batch 457 loss: 0.2872538945150584
  batch 458 loss: 0.2871848011966876
  batch 459 loss: 0.28719579865080597
  batch 460 loss: 0.2872238031224064
  batch 461 loss: 0.287166394826647
  batch 462 loss: 0.2871275232274295
  batch 463 loss: 0.28704111706103413
  batch 464 loss: 0.287088241042762
  batch 465 loss: 0.2869912483038441
  batch 466 loss: 0.28683014431480686
  batch 467 loss: 0.2869204551428237
  batch 468 loss: 0.28691679232905054
  batch 469 loss: 0.287041696721811
  batch 470 loss: 0.287018383246787
  batch 471 loss: 0.287044349875926
  batch 472 loss: 0.2869181901974193
LOSS train 0.2869181901974193 valid 0.29243695735931396
LOSS train 0.2869181901974193 valid 0.2734661102294922
LOSS train 0.2869181901974193 valid 0.27106083432833356
LOSS train 0.2869181901974193 valid 0.2692272812128067
LOSS train 0.2869181901974193 valid 0.26500712633132933
LOSS train 0.2869181901974193 valid 0.2696232795715332
LOSS train 0.2869181901974193 valid 0.2817402013710567
LOSS train 0.2869181901974193 valid 0.280494574457407
LOSS train 0.2869181901974193 valid 0.2775672674179077
LOSS train 0.2869181901974193 valid 0.27789407074451444
LOSS train 0.2869181901974193 valid 0.27699738469990814
LOSS train 0.2869181901974193 valid 0.2766294727722804
LOSS train 0.2869181901974193 valid 0.2748014239164499
LOSS train 0.2869181901974193 valid 0.27550105324813295
LOSS train 0.2869181901974193 valid 0.2712525417407354
LOSS train 0.2869181901974193 valid 0.2708381088450551
LOSS train 0.2869181901974193 valid 0.27218416771467996
LOSS train 0.2869181901974193 valid 0.27323181678851444
LOSS train 0.2869181901974193 valid 0.2751874084535398
LOSS train 0.2869181901974193 valid 0.2737171001732349
LOSS train 0.2869181901974193 valid 0.2724671030328387
LOSS train 0.2869181901974193 valid 0.27058095078576694
LOSS train 0.2869181901974193 valid 0.2706660077623699
LOSS train 0.2869181901974193 valid 0.26964958136280376
LOSS train 0.2869181901974193 valid 0.2688534677028656
LOSS train 0.2869181901974193 valid 0.2687550874856802
LOSS train 0.2869181901974193 valid 0.26869789097044205
LOSS train 0.2869181901974193 valid 0.2691445127129555
LOSS train 0.2869181901974193 valid 0.26942606527229834
LOSS train 0.2869181901974193 valid 0.27062905132770537
LOSS train 0.2869181901974193 valid 0.27182874564201603
LOSS train 0.2869181901974193 valid 0.271511004306376
LOSS train 0.2869181901974193 valid 0.27264943357669946
LOSS train 0.2869181901974193 valid 0.2723461591145572
LOSS train 0.2869181901974193 valid 0.2739458543913705
LOSS train 0.2869181901974193 valid 0.2734229192137718
LOSS train 0.2869181901974193 valid 0.2733567727578653
LOSS train 0.2869181901974193 valid 0.2745648630355534
LOSS train 0.2869181901974193 valid 0.27446815830010635
LOSS train 0.2869181901974193 valid 0.2743797741830349
LOSS train 0.2869181901974193 valid 0.27527540195279004
LOSS train 0.2869181901974193 valid 0.2755556660039084
LOSS train 0.2869181901974193 valid 0.27525907339051714
LOSS train 0.2869181901974193 valid 0.2755542959679257
LOSS train 0.2869181901974193 valid 0.27503018511666194
LOSS train 0.2869181901974193 valid 0.2754118785909984
LOSS train 0.2869181901974193 valid 0.276198647757794
LOSS train 0.2869181901974193 valid 0.2766484065602223
LOSS train 0.2869181901974193 valid 0.27690828393916694
LOSS train 0.2869181901974193 valid 0.2759466353058815
LOSS train 0.2869181901974193 valid 0.27580034060805453
LOSS train 0.2869181901974193 valid 0.2754426578489634
LOSS train 0.2869181901974193 valid 0.2756984090467669
LOSS train 0.2869181901974193 valid 0.2756798259086079
LOSS train 0.2869181901974193 valid 0.2754169038750909
LOSS train 0.2869181901974193 valid 0.2748432188693966
LOSS train 0.2869181901974193 valid 0.2743058743184073
LOSS train 0.2869181901974193 valid 0.2740187274998632
LOSS train 0.2869181901974193 valid 0.2743300856170008
LOSS train 0.2869181901974193 valid 0.2742680390675863
LOSS train 0.2869181901974193 valid 0.2739500335005463
LOSS train 0.2869181901974193 valid 0.27493039110014517
LOSS train 0.2869181901974193 valid 0.2747803850779458
LOSS train 0.2869181901974193 valid 0.27568325493484735
LOSS train 0.2869181901974193 valid 0.27580562371474043
LOSS train 0.2869181901974193 valid 0.2757636444135146
LOSS train 0.2869181901974193 valid 0.2748189250479883
LOSS train 0.2869181901974193 valid 0.27469414582147317
LOSS train 0.2869181901974193 valid 0.27405932048956555
LOSS train 0.2869181901974193 valid 0.27390897082430976
LOSS train 0.2869181901974193 valid 0.27362429403083427
LOSS train 0.2869181901974193 valid 0.27386575585438144
LOSS train 0.2869181901974193 valid 0.27369165155169084
LOSS train 0.2869181901974193 valid 0.2736379703557169
LOSS train 0.2869181901974193 valid 0.27383283714453377
LOSS train 0.2869181901974193 valid 0.27399886654395805
LOSS train 0.2869181901974193 valid 0.27408462743480483
LOSS train 0.2869181901974193 valid 0.2742979857019889
LOSS train 0.2869181901974193 valid 0.27429695691488964
LOSS train 0.2869181901974193 valid 0.2736692126840353
LOSS train 0.2869181901974193 valid 0.2727789786862738
LOSS train 0.2869181901974193 valid 0.2733470535859829
LOSS train 0.2869181901974193 valid 0.2731462842728718
LOSS train 0.2869181901974193 valid 0.2731655417453675
LOSS train 0.2869181901974193 valid 0.2731090678888209
LOSS train 0.2869181901974193 valid 0.2723213815411856
LOSS train 0.2869181901974193 valid 0.2719550660286827
LOSS train 0.2869181901974193 valid 0.2713611215691675
LOSS train 0.2869181901974193 valid 0.2718843685442142
LOSS train 0.2869181901974193 valid 0.27198264780971737
LOSS train 0.2869181901974193 valid 0.2720416504275668
LOSS train 0.2869181901974193 valid 0.2723556713241598
LOSS train 0.2869181901974193 valid 0.2722950707520208
LOSS train 0.2869181901974193 valid 0.2724518729968274
LOSS train 0.2869181901974193 valid 0.2723829609783072
LOSS train 0.2869181901974193 valid 0.2726568696089089
LOSS train 0.2869181901974193 valid 0.27273938474581416
LOSS train 0.2869181901974193 valid 0.2728201225399971
LOSS train 0.2869181901974193 valid 0.2729705949925413
LOSS train 0.2869181901974193 valid 0.27309484109282495
LOSS train 0.2869181901974193 valid 0.27338028795058184
LOSS train 0.2869181901974193 valid 0.27351391534594927
LOSS train 0.2869181901974193 valid 0.2732590358812832
LOSS train 0.2869181901974193 valid 0.2731637258369189
LOSS train 0.2869181901974193 valid 0.2732276158673423
LOSS train 0.2869181901974193 valid 0.2733957882759706
LOSS train 0.2869181901974193 valid 0.27327182833279406
LOSS train 0.2869181901974193 valid 0.2734282455510563
LOSS train 0.2869181901974193 valid 0.27375203535097453
LOSS train 0.2869181901974193 valid 0.27412298077886754
LOSS train 0.2869181901974193 valid 0.2736831320835663
LOSS train 0.2869181901974193 valid 0.2736456125442471
LOSS train 0.2869181901974193 valid 0.27362897596528046
LOSS train 0.2869181901974193 valid 0.2735607247603567
LOSS train 0.2869181901974193 valid 0.2735968302125516
LOSS train 0.2869181901974193 valid 0.2735754230926777
LOSS train 0.2869181901974193 valid 0.2738091117805905
LOSS train 0.2869181901974193 valid 0.2738178832046056
LOSS train 0.2869181901974193 valid 0.27373889113674643
LOSS train 0.2869181901974193 valid 0.27342964994410673
LOSS train 0.2869181901974193 valid 0.2732289778299568
LOSS train 0.2869181901974193 valid 0.2730772153275912
LOSS train 0.2869181901974193 valid 0.27294365395375386
LOSS train 0.2869181901974193 valid 0.2732519303118029
LOSS train 0.2869181901974193 valid 0.27319341468811037
LOSS train 0.2869181901974193 valid 0.27353611564825453
LOSS train 0.2869181901974193 valid 0.2733874217731746
LOSS train 0.2869181901974193 valid 0.2737440140917897
LOSS train 0.2869181901974193 valid 0.2738364304220954
LOSS train 0.2869181901974193 valid 0.27376700204152327
LOSS train 0.2869181901974193 valid 0.2738455044859238
LOSS train 0.2869181901974193 valid 0.2736124861421007
LOSS train 0.2869181901974193 valid 0.2736501767671198
LOSS train 0.2869181901974193 valid 0.27377661462150404
LOSS train 0.2869181901974193 valid 0.27366677279825563
LOSS train 0.2869181901974193 valid 0.2735593509586418
LOSS train 0.2869181901974193 valid 0.27334150802480045
LOSS train 0.2869181901974193 valid 0.27329081750434375
LOSS train 0.2869181901974193 valid 0.2731239261815874
LOSS train 0.2869181901974193 valid 0.27319024886403764
LOSS train 0.2869181901974193 valid 0.27319425302194367
LOSS train 0.2869181901974193 valid 0.2735457006894367
LOSS train 0.2869181901974193 valid 0.27345643552033216
LOSS train 0.2869181901974193 valid 0.2734862201743656
LOSS train 0.2869181901974193 valid 0.2731992720529951
LOSS train 0.2869181901974193 valid 0.2733999412149599
LOSS train 0.2869181901974193 valid 0.27317334052656783
LOSS train 0.2869181901974193 valid 0.2738195885274861
LOSS train 0.2869181901974193 valid 0.2739520764990941
LOSS train 0.2869181901974193 valid 0.2738929565747579
LOSS train 0.2869181901974193 valid 0.27400043705441307
LOSS train 0.2869181901974193 valid 0.27372052520513535
LOSS train 0.2869181901974193 valid 0.2738565025376339
LOSS train 0.2869181901974193 valid 0.2736483710733327
LOSS train 0.2869181901974193 valid 0.27368584554041586
LOSS train 0.2869181901974193 valid 0.27383407100270957
LOSS train 0.2869181901974193 valid 0.27372670391942283
LOSS train 0.2869181901974193 valid 0.2738801507821566
LOSS train 0.2869181901974193 valid 0.273905873392363
LOSS train 0.2869181901974193 valid 0.2738874754868448
LOSS train 0.2869181901974193 valid 0.2737873954617459
LOSS train 0.2869181901974193 valid 0.2736673571261359
LOSS train 0.2869181901974193 valid 0.27349800721633655
LOSS train 0.2869181901974193 valid 0.2732289072762175
LOSS train 0.2869181901974193 valid 0.27313789066040156
LOSS train 0.2869181901974193 valid 0.2733032740203731
LOSS train 0.2869181901974193 valid 0.2736037950851246
LOSS train 0.2869181901974193 valid 0.27346233943743364
LOSS train 0.2869181901974193 valid 0.27366878409710155
LOSS train 0.2869181901974193 valid 0.2738187734695042
LOSS train 0.2869181901974193 valid 0.2738145202049735
LOSS train 0.2869181901974193 valid 0.2736987103383208
LOSS train 0.2869181901974193 valid 0.2737534010858205
LOSS train 0.2869181901974193 valid 0.27374498143620873
LOSS train 0.2869181901974193 valid 0.2735503613097327
LOSS train 0.2869181901974193 valid 0.2735450310971249
LOSS train 0.2869181901974193 valid 0.273557887659908
LOSS train 0.2869181901974193 valid 0.27374475231666245
LOSS train 0.2869181901974193 valid 0.2736377567052841
LOSS train 0.2869181901974193 valid 0.27365767980615296
LOSS train 0.2869181901974193 valid 0.27384825067296215
LOSS train 0.2869181901974193 valid 0.2738811928001079
LOSS train 0.2869181901974193 valid 0.2738455759534419
LOSS train 0.2869181901974193 valid 0.2738821107246306
LOSS train 0.2869181901974193 valid 0.2736547567554422
LOSS train 0.2869181901974193 valid 0.27361455952288
LOSS train 0.2869181901974193 valid 0.2735908543840449
LOSS train 0.2869181901974193 valid 0.27362370990375257
LOSS train 0.2869181901974193 valid 0.2736117050603584
LOSS train 0.2869181901974193 valid 0.2737323154744349
LOSS train 0.2869181901974193 valid 0.27364853328747274
LOSS train 0.2869181901974193 valid 0.2737212211359292
LOSS train 0.2869181901974193 valid 0.2736142139515111
LOSS train 0.2869181901974193 valid 0.27348972557439016
LOSS train 0.2869181901974193 valid 0.27331415361318834
LOSS train 0.2869181901974193 valid 0.27330012353403227
LOSS train 0.2869181901974193 valid 0.27350979733285563
LOSS train 0.2869181901974193 valid 0.2733650528872856
LOSS train 0.2869181901974193 valid 0.27341277927310026
LOSS train 0.2869181901974193 valid 0.27327167071402075
LOSS train 0.2869181901974193 valid 0.2730818888292977
LOSS train 0.2869181901974193 valid 0.27300208563556766
LOSS train 0.2869181901974193 valid 0.27285744381949234
LOSS train 0.2869181901974193 valid 0.2728034176808946
LOSS train 0.2869181901974193 valid 0.2725864216321852
LOSS train 0.2869181901974193 valid 0.27258296318135217
LOSS train 0.2869181901974193 valid 0.272497046612887
LOSS train 0.2869181901974193 valid 0.27227905593239343
LOSS train 0.2869181901974193 valid 0.2721548106824382
LOSS train 0.2869181901974193 valid 0.272218415637811
LOSS train 0.2869181901974193 valid 0.27230646408282183
LOSS train 0.2869181901974193 valid 0.27226446688456357
LOSS train 0.2869181901974193 valid 0.272337159430477
LOSS train 0.2869181901974193 valid 0.27235766597718836
LOSS train 0.2869181901974193 valid 0.2722540265598962
LOSS train 0.2869181901974193 valid 0.27224423205135045
LOSS train 0.2869181901974193 valid 0.2722687658763701
LOSS train 0.2869181901974193 valid 0.2723300145990258
LOSS train 0.2869181901974193 valid 0.27247885624839835
LOSS train 0.2869181901974193 valid 0.2725658501413735
LOSS train 0.2869181901974193 valid 0.2726540536497513
LOSS train 0.2869181901974193 valid 0.27259373537323495
LOSS train 0.2869181901974193 valid 0.272759052202306
LOSS train 0.2869181901974193 valid 0.2728466232573347
LOSS train 0.2869181901974193 valid 0.2727949058347278
LOSS train 0.2869181901974193 valid 0.27290295862254843
LOSS train 0.2869181901974193 valid 0.2732113375931584
LOSS train 0.2869181901974193 valid 0.27334874038372126
LOSS train 0.2869181901974193 valid 0.2733952613246493
LOSS train 0.2869181901974193 valid 0.2734796874549078
LOSS train 0.2869181901974193 valid 0.2734850548949593
LOSS train 0.2869181901974193 valid 0.2735464292225139
LOSS train 0.2869181901974193 valid 0.2735587923183973
LOSS train 0.2869181901974193 valid 0.27353276484287703
LOSS train 0.2869181901974193 valid 0.27357577294745344
LOSS train 0.2869181901974193 valid 0.27342184266801606
LOSS train 0.2869181901974193 valid 0.2735257078323686
LOSS train 0.2869181901974193 valid 0.27354896682150226
LOSS train 0.2869181901974193 valid 0.2734055085785718
LOSS train 0.2869181901974193 valid 0.27333329102645315
LOSS train 0.2869181901974193 valid 0.27338763521172693
LOSS train 0.2869181901974193 valid 0.2731743148289436
LOSS train 0.2869181901974193 valid 0.27329980652518726
LOSS train 0.2869181901974193 valid 0.2735793095875959
LOSS train 0.2869181901974193 valid 0.27374840755851904
LOSS train 0.2869181901974193 valid 0.27372192888240504
LOSS train 0.2869181901974193 valid 0.27376069339663395
LOSS train 0.2869181901974193 valid 0.2736987246861381
LOSS train 0.2869181901974193 valid 0.27363224596862334
LOSS train 0.2869181901974193 valid 0.2737960103750229
LOSS train 0.2869181901974193 valid 0.2737960424793669
LOSS train 0.2869181901974193 valid 0.2740108397981477
LOSS train 0.2869181901974193 valid 0.2739671664275671
LOSS train 0.2869181901974193 valid 0.27388821307599076
LOSS train 0.2869181901974193 valid 0.2739857850121517
LOSS train 0.2869181901974193 valid 0.2740518676582724
LOSS train 0.2869181901974193 valid 0.2739264793433104
LOSS train 0.2869181901974193 valid 0.27405377344567644
LOSS train 0.2869181901974193 valid 0.2740220355020987
LOSS train 0.2869181901974193 valid 0.27398617817805365
LOSS train 0.2869181901974193 valid 0.2741077062727391
LOSS train 0.2869181901974193 valid 0.2741228016733213
LOSS train 0.2869181901974193 valid 0.27407764255320616
LOSS train 0.2869181901974193 valid 0.2741202068599788
LOSS train 0.2869181901974193 valid 0.27405068615697464
LOSS train 0.2869181901974193 valid 0.2740922163527711
LOSS train 0.2869181901974193 valid 0.2741599317347066
LOSS train 0.2869181901974193 valid 0.27432499870435517
LOSS train 0.2869181901974193 valid 0.2743960782940946
LOSS train 0.2869181901974193 valid 0.2743858336298554
LOSS train 0.2869181901974193 valid 0.2744524045623976
LOSS train 0.2869181901974193 valid 0.2746595652664409
LOSS train 0.2869181901974193 valid 0.27469879956472487
LOSS train 0.2869181901974193 valid 0.2746556225049235
LOSS train 0.2869181901974193 valid 0.2746157018704848
LOSS train 0.2869181901974193 valid 0.2744425406706506
LOSS train 0.2869181901974193 valid 0.27424780023872636
LOSS train 0.2869181901974193 valid 0.27410990192735796
LOSS train 0.2869181901974193 valid 0.274072577449156
LOSS train 0.2869181901974193 valid 0.2740138688257762
LOSS train 0.2869181901974193 valid 0.27383497272522
LOSS train 0.2869181901974193 valid 0.2736607847061563
LOSS train 0.2869181901974193 valid 0.27361477327009814
LOSS train 0.2869181901974193 valid 0.27367232603506303
LOSS train 0.2869181901974193 valid 0.27369546105987147
LOSS train 0.2869181901974193 valid 0.2736603570776386
LOSS train 0.2869181901974193 valid 0.27357925510988
LOSS train 0.2869181901974193 valid 0.2735692850417561
LOSS train 0.2869181901974193 valid 0.2736074068554545
LOSS train 0.2869181901974193 valid 0.273606241057659
LOSS train 0.2869181901974193 valid 0.2735345755860568
LOSS train 0.2869181901974193 valid 0.27358826688707694
LOSS train 0.2869181901974193 valid 0.2736094824893483
LOSS train 0.2869181901974193 valid 0.27372155485510014
LOSS train 0.2869181901974193 valid 0.2738049025252714
LOSS train 0.2869181901974193 valid 0.27380243880120486
LOSS train 0.2869181901974193 valid 0.27384360681479225
LOSS train 0.2869181901974193 valid 0.2738284921686121
LOSS train 0.2869181901974193 valid 0.27389567741582227
LOSS train 0.2869181901974193 valid 0.2738604275385539
LOSS train 0.2869181901974193 valid 0.2739134119000546
LOSS train 0.2869181901974193 valid 0.273955560480522
LOSS train 0.2869181901974193 valid 0.27402747749495426
LOSS train 0.2869181901974193 valid 0.2739698996669368
LOSS train 0.2869181901974193 valid 0.2738250713367931
LOSS train 0.2869181901974193 valid 0.27376746727165835
LOSS train 0.2869181901974193 valid 0.2736870364471057
LOSS train 0.2869181901974193 valid 0.27357569670715887
LOSS train 0.2869181901974193 valid 0.27360074694681324
LOSS train 0.2869181901974193 valid 0.27362824394818275
LOSS train 0.2869181901974193 valid 0.2735470173539073
LOSS train 0.2869181901974193 valid 0.27353268513121665
LOSS train 0.2869181901974193 valid 0.27358492766135034
LOSS train 0.2869181901974193 valid 0.2736490086480311
LOSS train 0.2869181901974193 valid 0.27361414758932023
LOSS train 0.2869181901974193 valid 0.2735494718993012
LOSS train 0.2869181901974193 valid 0.27360929041066756
LOSS train 0.2869181901974193 valid 0.2736711137043605
LOSS train 0.2869181901974193 valid 0.2737714908433185
LOSS train 0.2869181901974193 valid 0.2737018726300448
LOSS train 0.2869181901974193 valid 0.2737771023378194
LOSS train 0.2869181901974193 valid 0.27367044689122194
LOSS train 0.2869181901974193 valid 0.2735997085047211
LOSS train 0.2869181901974193 valid 0.27356490097296093
LOSS train 0.2869181901974193 valid 0.2735867684621077
LOSS train 0.2869181901974193 valid 0.2736880072603928
LOSS train 0.2869181901974193 valid 0.2736983188066278
LOSS train 0.2869181901974193 valid 0.27371355882141646
LOSS train 0.2869181901974193 valid 0.27379445047726386
LOSS train 0.2869181901974193 valid 0.2737931636246768
LOSS train 0.2869181901974193 valid 0.27371276554924484
LOSS train 0.2869181901974193 valid 0.2735521415450487
LOSS train 0.2869181901974193 valid 0.27357150672434327
LOSS train 0.2869181901974193 valid 0.27373510706210563
LOSS train 0.2869181901974193 valid 0.27367352976727843
LOSS train 0.2869181901974193 valid 0.27362140480961117
LOSS train 0.2869181901974193 valid 0.273555705915219
LOSS train 0.2869181901974193 valid 0.273511732030197
LOSS train 0.2869181901974193 valid 0.27344709713902093
LOSS train 0.2869181901974193 valid 0.27341718980494667
LOSS train 0.2869181901974193 valid 0.2732911115168127
LOSS train 0.2869181901974193 valid 0.27329871651024845
LOSS train 0.2869181901974193 valid 0.2732755845906783
LOSS train 0.2869181901974193 valid 0.2734282167498456
LOSS train 0.2869181901974193 valid 0.273486073811849
LOSS train 0.2869181901974193 valid 0.2734268243085442
LOSS train 0.2869181901974193 valid 0.27325666414557687
LOSS train 0.2869181901974193 valid 0.2731508480748911
LOSS train 0.2869181901974193 valid 0.2731858351852286
LOSS train 0.2869181901974193 valid 0.27311925236667906
LOSS train 0.2869181901974193 valid 0.27308463596041044
LOSS train 0.2869181901974193 valid 0.2730258177034557
LOSS train 0.2869181901974193 valid 0.2730730245623305
LOSS train 0.2869181901974193 valid 0.27314895092599134
LOSS train 0.2869181901974193 valid 0.27325393528165953
LOSS train 0.2869181901974193 valid 0.2733208522451728
LOSS train 0.2869181901974193 valid 0.27328291614683403
LOSS train 0.2869181901974193 valid 0.273254132928462
LOSS train 0.2869181901974193 valid 0.2732966466285392
LOSS train 0.2869181901974193 valid 0.2732284851372242
LOSS train 0.2869181901974193 valid 0.27321698502160174
LOSS train 0.2869181901974193 valid 0.2732844733895518
LOSS train 0.2869181901974193 valid 0.27317178076949
LOSS train 0.2869181901974193 valid 0.27318579040385865
LOSS train 0.2869181901974193 valid 0.27316275451281297
LOSS train 0.2869181901974193 valid 0.273098072067636
LOSS train 0.2869181901974193 valid 0.2729672935337072
LOSS train 0.2869181901974193 valid 0.2729547012840276
LOSS train 0.2869181901974193 valid 0.2730663844805746
EPOCH 7:
  batch 1 loss: 0.29586589336395264
  batch 2 loss: 0.27329082787036896
  batch 3 loss: 0.2762468457221985
  batch 4 loss: 0.2778725177049637
  batch 5 loss: 0.2874046564102173
  batch 6 loss: 0.2815454453229904
  batch 7 loss: 0.2862566113471985
  batch 8 loss: 0.28916288539767265
  batch 9 loss: 0.2881791392962138
  batch 10 loss: 0.28775930404663086
  batch 11 loss: 0.2869182147762992
  batch 12 loss: 0.28423428783814114
  batch 13 loss: 0.2837762970190782
  batch 14 loss: 0.28420735044138773
  batch 15 loss: 0.2863764444986979
  batch 16 loss: 0.28475665114820004
  batch 17 loss: 0.2820652258746764
  batch 18 loss: 0.28259380410114926
  batch 19 loss: 0.2801656260302192
  batch 20 loss: 0.27793719097971914
  batch 21 loss: 0.2787309338649114
  batch 22 loss: 0.27791848981922324
  batch 23 loss: 0.2769460995560107
  batch 24 loss: 0.2754391909887393
  batch 25 loss: 0.27796357452869414
  batch 26 loss: 0.27677198270192516
  batch 27 loss: 0.2772340294387605
  batch 28 loss: 0.27614050092441694
  batch 29 loss: 0.2772980337512904
  batch 30 loss: 0.27672794212897617
  batch 31 loss: 0.27715491431374706
  batch 32 loss: 0.27702005626633763
  batch 33 loss: 0.2769880344470342
  batch 34 loss: 0.27622421129661445
  batch 35 loss: 0.27637438561235156
  batch 36 loss: 0.27681174832913613
  batch 37 loss: 0.2772344983107335
  batch 38 loss: 0.27839702720704834
  batch 39 loss: 0.2783305985041154
  batch 40 loss: 0.27854530028998853
  batch 41 loss: 0.27885728456625125
  batch 42 loss: 0.27912014687345144
  batch 43 loss: 0.2801268928965857
  batch 44 loss: 0.2803665687414733
  batch 45 loss: 0.27997081445323097
  batch 46 loss: 0.28027435003415396
  batch 47 loss: 0.2802654596719336
  batch 48 loss: 0.2793386069436868
  batch 49 loss: 0.27897394311671353
  batch 50 loss: 0.27819434851408004
  batch 51 loss: 0.27789882029972823
  batch 52 loss: 0.278685027303604
  batch 53 loss: 0.2781583911405419
  batch 54 loss: 0.27849343998564613
  batch 55 loss: 0.27815834527665917
  batch 56 loss: 0.27777644912047045
  batch 57 loss: 0.27764038584734263
  batch 58 loss: 0.27866958312947177
  batch 59 loss: 0.2787341190091634
  batch 60 loss: 0.2784618583818277
  batch 61 loss: 0.2784907917995922
  batch 62 loss: 0.2791412734696942
  batch 63 loss: 0.27879803497639916
  batch 64 loss: 0.27952461200766265
  batch 65 loss: 0.27937013117166665
  batch 66 loss: 0.279277002946897
  batch 67 loss: 0.28022146291697203
  batch 68 loss: 0.2805761293891598
  batch 69 loss: 0.2804979476807774
  batch 70 loss: 0.28064346334763934
  batch 71 loss: 0.28036975209981624
  batch 72 loss: 0.2804465291814672
  batch 73 loss: 0.28070034045879155
  batch 74 loss: 0.2809065828049505
  batch 75 loss: 0.2803466049830119
  batch 76 loss: 0.28036807712755707
  batch 77 loss: 0.2804326554397484
  batch 78 loss: 0.28047425204362625
  batch 79 loss: 0.28122721555866775
  batch 80 loss: 0.2813100893050432
  batch 81 loss: 0.2816907248379272
  batch 82 loss: 0.28216209934978953
  batch 83 loss: 0.2824292753834322
  batch 84 loss: 0.28210762533403577
  batch 85 loss: 0.2818941417862387
  batch 86 loss: 0.28252681535343793
  batch 87 loss: 0.2826038174245549
  batch 88 loss: 0.28242700038985774
  batch 89 loss: 0.2819521072205533
  batch 90 loss: 0.28163440856668687
  batch 91 loss: 0.281345777459197
  batch 92 loss: 0.2811446455509766
  batch 93 loss: 0.28093135036447997
  batch 94 loss: 0.28163907153809326
  batch 95 loss: 0.2812965378949517
  batch 96 loss: 0.281667141088595
  batch 97 loss: 0.2822525253922669
  batch 98 loss: 0.2823768229204781
  batch 99 loss: 0.28260542241611863
  batch 100 loss: 0.28272666200995444
  batch 101 loss: 0.2828682954653655
  batch 102 loss: 0.28322068952462254
  batch 103 loss: 0.2836523040116412
  batch 104 loss: 0.2837660818432386
  batch 105 loss: 0.2835705934535889
  batch 106 loss: 0.2839202210307121
  batch 107 loss: 0.2836551882117708
  batch 108 loss: 0.28348104007266184
  batch 109 loss: 0.2833290530727544
  batch 110 loss: 0.2835830137133598
  batch 111 loss: 0.2834343651125023
  batch 112 loss: 0.2832303343872939
  batch 113 loss: 0.28364153311843365
  batch 114 loss: 0.28410037084106815
  batch 115 loss: 0.28439191126305124
  batch 116 loss: 0.2847649322758461
  batch 117 loss: 0.28519000011122125
  batch 118 loss: 0.2851604656395266
  batch 119 loss: 0.2854359366563188
  batch 120 loss: 0.28542618912955126
  batch 121 loss: 0.28555249194960947
  batch 122 loss: 0.2855166388583965
  batch 123 loss: 0.28541521994563623
  batch 124 loss: 0.28567749730521635
  batch 125 loss: 0.2857461203336716
  batch 126 loss: 0.2857259697620831
  batch 127 loss: 0.2859837437473883
  batch 128 loss: 0.28577364154625684
  batch 129 loss: 0.2861290224069773
  batch 130 loss: 0.28623831008489314
  batch 131 loss: 0.2863172016298498
  batch 132 loss: 0.2864730521810777
  batch 133 loss: 0.28686000093033437
  batch 134 loss: 0.2867540340147801
  batch 135 loss: 0.2869867255290349
  batch 136 loss: 0.28733408045681086
  batch 137 loss: 0.28745898919819046
  batch 138 loss: 0.28753017763728683
  batch 139 loss: 0.28801175040735616
  batch 140 loss: 0.2878093307571752
  batch 141 loss: 0.28806857067219754
  batch 142 loss: 0.2882374551304629
  batch 143 loss: 0.28820476634102266
  batch 144 loss: 0.2882204611475269
  batch 145 loss: 0.28829036673595165
  batch 146 loss: 0.28834923765022463
  batch 147 loss: 0.2887676480878778
  batch 148 loss: 0.2886934333757774
  batch 149 loss: 0.2884676221433102
  batch 150 loss: 0.28863364686568577
  batch 151 loss: 0.2889589433441099
  batch 152 loss: 0.2891381756451569
  batch 153 loss: 0.2891418108947916
  batch 154 loss: 0.28955382911802885
  batch 155 loss: 0.28966700252025357
  batch 156 loss: 0.2895723194457017
  batch 157 loss: 0.2896614024403748
  batch 158 loss: 0.2898464673493482
  batch 159 loss: 0.2901097253995871
  batch 160 loss: 0.2899433263577521
  batch 161 loss: 0.29002149384584486
  batch 162 loss: 0.2901232120247535
  batch 163 loss: 0.2902898933807034
  batch 164 loss: 0.29012047980980177
  batch 165 loss: 0.2900276900240869
  batch 166 loss: 0.29003576476530857
  batch 167 loss: 0.2900465306586134
  batch 168 loss: 0.2902041021734476
  batch 169 loss: 0.28999548736055925
  batch 170 loss: 0.2900277177200598
  batch 171 loss: 0.2900982097401256
  batch 172 loss: 0.29020710679334266
  batch 173 loss: 0.29016128828415294
  batch 174 loss: 0.29014010735969437
  batch 175 loss: 0.29021311376776016
  batch 176 loss: 0.2899688053029505
  batch 177 loss: 0.28996685340916367
  batch 178 loss: 0.29009999560841015
  batch 179 loss: 0.29017201340731297
  batch 180 loss: 0.2900520984497335
  batch 181 loss: 0.29010954276962175
  batch 182 loss: 0.28996011704861463
  batch 183 loss: 0.2897210700915811
  batch 184 loss: 0.2895360205160535
  batch 185 loss: 0.2895943955795185
  batch 186 loss: 0.2896243091872943
  batch 187 loss: 0.2895597021847485
  batch 188 loss: 0.28921949213489573
  batch 189 loss: 0.288919762328819
  batch 190 loss: 0.2890225987685354
  batch 191 loss: 0.28897578866069856
  batch 192 loss: 0.2889548510623475
  batch 193 loss: 0.28891985048901847
  batch 194 loss: 0.28910505295414285
  batch 195 loss: 0.2891566068698198
  batch 196 loss: 0.28896774503649497
  batch 197 loss: 0.28886251507071675
  batch 198 loss: 0.2891015234318646
  batch 199 loss: 0.2891430714022574
  batch 200 loss: 0.28917188629508017
  batch 201 loss: 0.2892814214846388
  batch 202 loss: 0.2892601513626552
  batch 203 loss: 0.2892116035146666
  batch 204 loss: 0.2889559035207711
  batch 205 loss: 0.2890447568602678
  batch 206 loss: 0.2890837667057815
  batch 207 loss: 0.28910293196134523
  batch 208 loss: 0.28891697296729457
  batch 209 loss: 0.2885856580648696
  batch 210 loss: 0.2886352565316927
  batch 211 loss: 0.28858473353193836
  batch 212 loss: 0.2885116527805913
  batch 213 loss: 0.28833464494613414
  batch 214 loss: 0.28831129743952616
  batch 215 loss: 0.2880960444378298
  batch 216 loss: 0.2878608592544441
  batch 217 loss: 0.28782552707305153
  batch 218 loss: 0.2878108603555128
  batch 219 loss: 0.2878864117000745
  batch 220 loss: 0.2877984154630791
  batch 221 loss: 0.28776080960332
  batch 222 loss: 0.2877963851835277
  batch 223 loss: 0.2877284742123343
  batch 224 loss: 0.2876717866664486
  batch 225 loss: 0.2875042810042699
  batch 226 loss: 0.2875634352058436
  batch 227 loss: 0.2873474635873072
  batch 228 loss: 0.2871375354497056
  batch 229 loss: 0.28701968653753857
  batch 230 loss: 0.2870202672222386
  batch 231 loss: 0.2869048634648839
  batch 232 loss: 0.2867899204379526
  batch 233 loss: 0.2867117252984272
  batch 234 loss: 0.2866790573566388
  batch 235 loss: 0.2866773261668834
  batch 236 loss: 0.2865937361020153
  batch 237 loss: 0.286658252467586
  batch 238 loss: 0.2866064715786141
  batch 239 loss: 0.28660159255670203
  batch 240 loss: 0.28664707665642103
  batch 241 loss: 0.286790379102794
  batch 242 loss: 0.28666108193968937
  batch 243 loss: 0.2867498684812475
  batch 244 loss: 0.2868293675242877
  batch 245 loss: 0.28669161772241397
  batch 246 loss: 0.2866559226096161
  batch 247 loss: 0.2868646826097357
  batch 248 loss: 0.2869789628011565
  batch 249 loss: 0.2868211308277276
  batch 250 loss: 0.28667500108480454
  batch 251 loss: 0.2869244844196327
  batch 252 loss: 0.2867435050743913
  batch 253 loss: 0.28662355715342663
  batch 254 loss: 0.2864916752290538
  batch 255 loss: 0.2865193419012369
  batch 256 loss: 0.2866648291819729
  batch 257 loss: 0.2866639333128465
  batch 258 loss: 0.28659523001243903
  batch 259 loss: 0.28661569383383717
  batch 260 loss: 0.2866688770170395
  batch 261 loss: 0.2865799870413382
  batch 262 loss: 0.28645531918483835
  batch 263 loss: 0.28647656285490825
  batch 264 loss: 0.28635056627293426
  batch 265 loss: 0.2861646687084774
  batch 266 loss: 0.2860703453757709
  batch 267 loss: 0.2862042318122664
  batch 268 loss: 0.2860274463891983
  batch 269 loss: 0.2859507598619922
  batch 270 loss: 0.2861547925957927
  batch 271 loss: 0.2860215606504701
  batch 272 loss: 0.286129671642009
  batch 273 loss: 0.28608901978849055
  batch 274 loss: 0.2860970268719388
  batch 275 loss: 0.28609174728393555
  batch 276 loss: 0.28602123595234275
  batch 277 loss: 0.2860325235967602
  batch 278 loss: 0.285930433504873
  batch 279 loss: 0.285931102050248
  batch 280 loss: 0.2858738905617169
  batch 281 loss: 0.28577765260302723
  batch 282 loss: 0.2857319675650157
  batch 283 loss: 0.28570511157858075
  batch 284 loss: 0.28577331663437294
  batch 285 loss: 0.28569062038471826
  batch 286 loss: 0.28575231760115055
  batch 287 loss: 0.2858153828760473
  batch 288 loss: 0.28561647955535185
  batch 289 loss: 0.2858159162078349
  batch 290 loss: 0.28561082076409766
  batch 291 loss: 0.28561437606197043
  batch 292 loss: 0.2856928900598663
  batch 293 loss: 0.28567557067391003
  batch 294 loss: 0.2855868462701233
  batch 295 loss: 0.28549288526429967
  batch 296 loss: 0.28549334502502066
  batch 297 loss: 0.2854350523816215
  batch 298 loss: 0.28548496646569077
  batch 299 loss: 0.28532683645004414
  batch 300 loss: 0.28530113980174066
  batch 301 loss: 0.2851660614492885
  batch 302 loss: 0.28512704091158925
  batch 303 loss: 0.2850446482204368
  batch 304 loss: 0.2849952490804227
  batch 305 loss: 0.2848225303849236
  batch 306 loss: 0.28493646055070404
  batch 307 loss: 0.2849220108811164
  batch 308 loss: 0.28505263414669346
  batch 309 loss: 0.2849944867073139
  batch 310 loss: 0.2850137065975897
  batch 311 loss: 0.28507927496141944
  batch 312 loss: 0.2851366236901436
  batch 313 loss: 0.28512234890613314
  batch 314 loss: 0.2849833582806739
  batch 315 loss: 0.2849141906178187
  batch 316 loss: 0.28489968906852264
  batch 317 loss: 0.28482110605630967
  batch 318 loss: 0.2847450802911003
  batch 319 loss: 0.28468031064843685
  batch 320 loss: 0.28458182476460936
  batch 321 loss: 0.28460929670437846
  batch 322 loss: 0.28459466956787227
  batch 323 loss: 0.2845557753700221
  batch 324 loss: 0.28443869345901923
  batch 325 loss: 0.2843147298005911
  batch 326 loss: 0.28430935740470886
  batch 327 loss: 0.2843168203801555
  batch 328 loss: 0.2840878065766358
  batch 329 loss: 0.2841813608686975
  batch 330 loss: 0.2841510421398914
  batch 331 loss: 0.28418323712766713
  batch 332 loss: 0.2840844463691654
  batch 333 loss: 0.2841067221071627
  batch 334 loss: 0.2839825518295437
  batch 335 loss: 0.28384541468833807
  batch 336 loss: 0.28370741479808376
  batch 337 loss: 0.28360983561337527
  batch 338 loss: 0.28353553014041405
  batch 339 loss: 0.2834575952505995
  batch 340 loss: 0.2834153717931579
  batch 341 loss: 0.28333757044283175
  batch 342 loss: 0.2832295139543494
  batch 343 loss: 0.2832051058117919
  batch 344 loss: 0.2831755663853052
  batch 345 loss: 0.28321983568046405
  batch 346 loss: 0.2830490621958854
  batch 347 loss: 0.2829239682350791
  batch 348 loss: 0.2828961224134626
  batch 349 loss: 0.28276930830540153
  batch 350 loss: 0.2827165380546025
  batch 351 loss: 0.28266463218591154
  batch 352 loss: 0.2826836244626479
  batch 353 loss: 0.2826619604153944
  batch 354 loss: 0.282754905479776
  batch 355 loss: 0.2826797413154387
  batch 356 loss: 0.2826122661654869
  batch 357 loss: 0.2824689962950741
  batch 358 loss: 0.2824666589998
  batch 359 loss: 0.2823653487192887
  batch 360 loss: 0.28231791311668025
  batch 361 loss: 0.2822493107117444
  batch 362 loss: 0.28209772216186996
  batch 363 loss: 0.28198583842801656
  batch 364 loss: 0.281789004024896
  batch 365 loss: 0.28172742018144425
  batch 366 loss: 0.28166221387562207
  batch 367 loss: 0.2815981804918528
  batch 368 loss: 0.2814222287953548
  batch 369 loss: 0.2813773974574355
  batch 370 loss: 0.28132357271136466
  batch 371 loss: 0.2813313000526711
  batch 372 loss: 0.28125092992058365
  batch 373 loss: 0.2811472819014465
  batch 374 loss: 0.28099296887450037
  batch 375 loss: 0.2808996963103612
  batch 376 loss: 0.28089736937049853
  batch 377 loss: 0.2808144951451679
  batch 378 loss: 0.28073151902389276
  batch 379 loss: 0.28068394668026775
  batch 380 loss: 0.2807271702117042
  batch 381 loss: 0.28063618647927063
  batch 382 loss: 0.28056640240378405
  batch 383 loss: 0.280545170601292
  batch 384 loss: 0.280412207202365
  batch 385 loss: 0.28049348585017314
  batch 386 loss: 0.28044724873619375
  batch 387 loss: 0.2805114647214727
  batch 388 loss: 0.2805195651257161
  batch 389 loss: 0.2805324445806616
  batch 390 loss: 0.28049739324129547
  batch 391 loss: 0.2804579408577336
  batch 392 loss: 0.2803891901763118
  batch 393 loss: 0.2802807225374168
  batch 394 loss: 0.28027156721516916
  batch 395 loss: 0.28018182685858084
  batch 396 loss: 0.280199567589796
  batch 397 loss: 0.28021088576587083
  batch 398 loss: 0.28013256401871917
  batch 399 loss: 0.2801745931307475
  batch 400 loss: 0.28023947328329085
  batch 401 loss: 0.28016367327988595
  batch 402 loss: 0.2802069338770648
  batch 403 loss: 0.28022691906977526
  batch 404 loss: 0.2802751056554884
  batch 405 loss: 0.2802845721259529
  batch 406 loss: 0.28031900631529943
  batch 407 loss: 0.28028445333901436
  batch 408 loss: 0.28031822721309524
  batch 409 loss: 0.28033488271143153
  batch 410 loss: 0.2803961363870923
  batch 411 loss: 0.28038850212764277
  batch 412 loss: 0.2803588736332157
  batch 413 loss: 0.2803946024496965
  batch 414 loss: 0.28036559006010275
  batch 415 loss: 0.280306481489216
  batch 416 loss: 0.2803814356358579
  batch 417 loss: 0.2803580944272254
  batch 418 loss: 0.2803343802191424
  batch 419 loss: 0.2803485688346667
  batch 420 loss: 0.2803007453325249
  batch 421 loss: 0.28020638599106934
  batch 422 loss: 0.28028440825040873
  batch 423 loss: 0.2802267566710781
  batch 424 loss: 0.2801625400711343
  batch 425 loss: 0.28015607381568236
  batch 426 loss: 0.28008482150488617
  batch 427 loss: 0.28012475172445983
  batch 428 loss: 0.28005706494517413
  batch 429 loss: 0.2800683173891548
  batch 430 loss: 0.2799706049090208
  batch 431 loss: 0.28004918494661557
  batch 432 loss: 0.2801012691041386
  batch 433 loss: 0.2800811596954667
  batch 434 loss: 0.2800693011709622
  batch 435 loss: 0.2799985697214631
  batch 436 loss: 0.2800023044468066
  batch 437 loss: 0.28003194341670457
  batch 438 loss: 0.28011539462766694
  batch 439 loss: 0.2800304369665767
  batch 440 loss: 0.2800971615720879
  batch 441 loss: 0.28000744785311
  batch 442 loss: 0.2799369800347009
  batch 443 loss: 0.27989659624901636
  batch 444 loss: 0.279809560962357
  batch 445 loss: 0.27986544702160226
  batch 446 loss: 0.27986371614072353
  batch 447 loss: 0.2797447060311934
  batch 448 loss: 0.2797489421043013
  batch 449 loss: 0.2797829188190749
  batch 450 loss: 0.27971020744906533
  batch 451 loss: 0.27969203013802846
  batch 452 loss: 0.279810427696304
  batch 453 loss: 0.2798131487227434
  batch 454 loss: 0.2798507148605086
  batch 455 loss: 0.2798513531029879
  batch 456 loss: 0.2798302821292166
  batch 457 loss: 0.2797818851679629
  batch 458 loss: 0.2797232771284195
  batch 459 loss: 0.27977023989546534
  batch 460 loss: 0.27978980618974436
  batch 461 loss: 0.279744731242124
  batch 462 loss: 0.2796923562690809
  batch 463 loss: 0.27963885774365255
  batch 464 loss: 0.2796579448442007
  batch 465 loss: 0.27957494989518195
  batch 466 loss: 0.27944196223445206
  batch 467 loss: 0.27952815257252156
  batch 468 loss: 0.27950763173847115
  batch 469 loss: 0.279644485920477
  batch 470 loss: 0.2796372611471947
  batch 471 loss: 0.2796802242343846
  batch 472 loss: 0.2795755296503588
LOSS train 0.2795755296503588 valid 0.25069379806518555
LOSS train 0.2795755296503588 valid 0.24119000136852264
LOSS train 0.2795755296503588 valid 0.24018438160419464
LOSS train 0.2795755296503588 valid 0.23593954741954803
LOSS train 0.2795755296503588 valid 0.23157936930656434
LOSS train 0.2795755296503588 valid 0.23586920897165933
LOSS train 0.2795755296503588 valid 0.24792652044977462
LOSS train 0.2795755296503588 valid 0.24546701461076736
LOSS train 0.2795755296503588 valid 0.24278036091062757
LOSS train 0.2795755296503588 valid 0.24312638789415358
LOSS train 0.2795755296503588 valid 0.2421737638386813
LOSS train 0.2795755296503588 valid 0.2423588496943315
LOSS train 0.2795755296503588 valid 0.24020530741948348
LOSS train 0.2795755296503588 valid 0.24038623699120112
LOSS train 0.2795755296503588 valid 0.23627696931362152
LOSS train 0.2795755296503588 valid 0.23629621136933565
LOSS train 0.2795755296503588 valid 0.23741740251288695
LOSS train 0.2795755296503588 valid 0.23874649322695202
LOSS train 0.2795755296503588 valid 0.24053242410484113
LOSS train 0.2795755296503588 valid 0.23899317979812623
LOSS train 0.2795755296503588 valid 0.23821912138235002
LOSS train 0.2795755296503588 valid 0.2364311617883769
LOSS train 0.2795755296503588 valid 0.2366377970446711
LOSS train 0.2795755296503588 valid 0.23551952466368675
LOSS train 0.2795755296503588 valid 0.23451322793960572
LOSS train 0.2795755296503588 valid 0.23467695942291847
LOSS train 0.2795755296503588 valid 0.23471664830490394
LOSS train 0.2795755296503588 valid 0.2349024579993316
LOSS train 0.2795755296503588 valid 0.23509148945068492
LOSS train 0.2795755296503588 valid 0.23574193169673283
LOSS train 0.2795755296503588 valid 0.23650627126616816
LOSS train 0.2795755296503588 valid 0.2361260880716145
LOSS train 0.2795755296503588 valid 0.2371170281460791
LOSS train 0.2795755296503588 valid 0.2367501421009793
LOSS train 0.2795755296503588 valid 0.2381570258310863
LOSS train 0.2795755296503588 valid 0.23773024852077165
LOSS train 0.2795755296503588 valid 0.23774374054895864
LOSS train 0.2795755296503588 valid 0.23886430381160034
LOSS train 0.2795755296503588 valid 0.23871121956751898
LOSS train 0.2795755296503588 valid 0.23857021555304528
LOSS train 0.2795755296503588 valid 0.2395080575128881
LOSS train 0.2795755296503588 valid 0.23985567405110314
LOSS train 0.2795755296503588 valid 0.2395226588082868
LOSS train 0.2795755296503588 valid 0.23962190916592424
LOSS train 0.2795755296503588 valid 0.23919891450140213
LOSS train 0.2795755296503588 valid 0.2396434519601905
LOSS train 0.2795755296503588 valid 0.24048903838117072
LOSS train 0.2795755296503588 valid 0.24084791541099548
LOSS train 0.2795755296503588 valid 0.2411598325992117
LOSS train 0.2795755296503588 valid 0.24032038569450379
LOSS train 0.2795755296503588 valid 0.24025340874989828
LOSS train 0.2795755296503588 valid 0.23975984207712686
LOSS train 0.2795755296503588 valid 0.24009233572573033
LOSS train 0.2795755296503588 valid 0.24020469823369273
LOSS train 0.2795755296503588 valid 0.24006248333237387
LOSS train 0.2795755296503588 valid 0.23949159043175833
LOSS train 0.2795755296503588 valid 0.23896921883549607
LOSS train 0.2795755296503588 valid 0.23869441218417267
LOSS train 0.2795755296503588 valid 0.23906033599780777
LOSS train 0.2795755296503588 valid 0.238946462670962
LOSS train 0.2795755296503588 valid 0.238764650753287
LOSS train 0.2795755296503588 valid 0.23968580293078576
LOSS train 0.2795755296503588 valid 0.23953585161103141
LOSS train 0.2795755296503588 valid 0.24032424064353108
LOSS train 0.2795755296503588 valid 0.24044341101096225
LOSS train 0.2795755296503588 valid 0.24040111447825577
LOSS train 0.2795755296503588 valid 0.23959669470787048
LOSS train 0.2795755296503588 valid 0.23953661300680218
LOSS train 0.2795755296503588 valid 0.23894378910030145
LOSS train 0.2795755296503588 valid 0.2388568863272667
LOSS train 0.2795755296503588 valid 0.2385757693522413
LOSS train 0.2795755296503588 valid 0.2388181353194846
LOSS train 0.2795755296503588 valid 0.2386490482173554
LOSS train 0.2795755296503588 valid 0.23862519054799466
LOSS train 0.2795755296503588 valid 0.2387571746110916
LOSS train 0.2795755296503588 valid 0.23907669378738655
LOSS train 0.2795755296503588 valid 0.2391597744706389
LOSS train 0.2795755296503588 valid 0.23939628860889337
LOSS train 0.2795755296503588 valid 0.23930924282043795
LOSS train 0.2795755296503588 valid 0.23866935074329376
LOSS train 0.2795755296503588 valid 0.2378688461986589
LOSS train 0.2795755296503588 valid 0.2383528544408519
LOSS train 0.2795755296503588 valid 0.23818157236260104
LOSS train 0.2795755296503588 valid 0.238167549350432
LOSS train 0.2795755296503588 valid 0.23808268729378196
LOSS train 0.2795755296503588 valid 0.23744556550369705
LOSS train 0.2795755296503588 valid 0.23712474210508938
LOSS train 0.2795755296503588 valid 0.2366205215115439
LOSS train 0.2795755296503588 valid 0.23709383657139338
LOSS train 0.2795755296503588 valid 0.23712016857332655
LOSS train 0.2795755296503588 valid 0.23713444566333686
LOSS train 0.2795755296503588 valid 0.23742877416636632
LOSS train 0.2795755296503588 valid 0.23741888919825194
LOSS train 0.2795755296503588 valid 0.23754914089086207
LOSS train 0.2795755296503588 valid 0.23749314389730755
LOSS train 0.2795755296503588 valid 0.2378763466452559
LOSS train 0.2795755296503588 valid 0.23803014423429353
LOSS train 0.2795755296503588 valid 0.23817968186067076
LOSS train 0.2795755296503588 valid 0.2381910403268506
LOSS train 0.2795755296503588 valid 0.23830269411206245
LOSS train 0.2795755296503588 valid 0.23869231446544723
LOSS train 0.2795755296503588 valid 0.23879838413467594
LOSS train 0.2795755296503588 valid 0.23857062664425488
LOSS train 0.2795755296503588 valid 0.23859884962439537
LOSS train 0.2795755296503588 valid 0.23868014897618975
LOSS train 0.2795755296503588 valid 0.23884291525156992
LOSS train 0.2795755296503588 valid 0.23877942673513822
LOSS train 0.2795755296503588 valid 0.2388949775033527
LOSS train 0.2795755296503588 valid 0.23925910712382115
LOSS train 0.2795755296503588 valid 0.2396261299198324
LOSS train 0.2795755296503588 valid 0.23926291822850168
LOSS train 0.2795755296503588 valid 0.23926437765892064
LOSS train 0.2795755296503588 valid 0.23924044895488605
LOSS train 0.2795755296503588 valid 0.23922601093848547
LOSS train 0.2795755296503588 valid 0.23925836280636165
LOSS train 0.2795755296503588 valid 0.23927528436841636
LOSS train 0.2795755296503588 valid 0.23948002498374027
LOSS train 0.2795755296503588 valid 0.23948245167227114
LOSS train 0.2795755296503588 valid 0.23941755382453694
LOSS train 0.2795755296503588 valid 0.23912461735308171
LOSS train 0.2795755296503588 valid 0.23897804590788754
LOSS train 0.2795755296503588 valid 0.23884610550813987
LOSS train 0.2795755296503588 valid 0.23867383538707485
LOSS train 0.2795755296503588 valid 0.23894304553827933
LOSS train 0.2795755296503588 valid 0.2388891054391861
LOSS train 0.2795755296503588 valid 0.23924369341324245
LOSS train 0.2795755296503588 valid 0.2391230312623377
LOSS train 0.2795755296503588 valid 0.23949184536468238
LOSS train 0.2795755296503588 valid 0.23960654206516208
LOSS train 0.2795755296503588 valid 0.23944498942448542
LOSS train 0.2795755296503588 valid 0.2395067311652744
LOSS train 0.2795755296503588 valid 0.23931541327725758
LOSS train 0.2795755296503588 valid 0.23930380039645316
LOSS train 0.2795755296503588 valid 0.239444703308504
LOSS train 0.2795755296503588 valid 0.23928389770013314
LOSS train 0.2795755296503588 valid 0.23916234152720256
LOSS train 0.2795755296503588 valid 0.23897427701166946
LOSS train 0.2795755296503588 valid 0.23892029288454333
LOSS train 0.2795755296503588 valid 0.2388015157884831
LOSS train 0.2795755296503588 valid 0.2388848243015153
LOSS train 0.2795755296503588 valid 0.2389018216454391
LOSS train 0.2795755296503588 valid 0.23919557160894636
LOSS train 0.2795755296503588 valid 0.2391670479641094
LOSS train 0.2795755296503588 valid 0.23917765035811397
LOSS train 0.2795755296503588 valid 0.23890193310277216
LOSS train 0.2795755296503588 valid 0.23910225437928553
LOSS train 0.2795755296503588 valid 0.23888704647012307
LOSS train 0.2795755296503588 valid 0.23950640615579244
LOSS train 0.2795755296503588 valid 0.2396296882789407
LOSS train 0.2795755296503588 valid 0.23959472755591074
LOSS train 0.2795755296503588 valid 0.23969242410943997
LOSS train 0.2795755296503588 valid 0.23939691601615204
LOSS train 0.2795755296503588 valid 0.23949993435853448
LOSS train 0.2795755296503588 valid 0.23930017601747017
LOSS train 0.2795755296503588 valid 0.23929270534746108
LOSS train 0.2795755296503588 valid 0.23943078508361793
LOSS train 0.2795755296503588 valid 0.23933256213452406
LOSS train 0.2795755296503588 valid 0.2394580566619016
LOSS train 0.2795755296503588 valid 0.239522452530621
LOSS train 0.2795755296503588 valid 0.23951634177938103
LOSS train 0.2795755296503588 valid 0.2393943200015133
LOSS train 0.2795755296503588 valid 0.23925184550476664
LOSS train 0.2795755296503588 valid 0.23910022650393972
LOSS train 0.2795755296503588 valid 0.238869614717437
LOSS train 0.2795755296503588 valid 0.23873747289180755
LOSS train 0.2795755296503588 valid 0.23889594112175055
LOSS train 0.2795755296503588 valid 0.23919285887372707
LOSS train 0.2795755296503588 valid 0.2390410645554463
LOSS train 0.2795755296503588 valid 0.23921287950326706
LOSS train 0.2795755296503588 valid 0.23933793244992985
LOSS train 0.2795755296503588 valid 0.2393641362064763
LOSS train 0.2795755296503588 valid 0.23928412517835929
LOSS train 0.2795755296503588 valid 0.23935357769789722
LOSS train 0.2795755296503588 valid 0.239368066962423
LOSS train 0.2795755296503588 valid 0.23914002997534617
LOSS train 0.2795755296503588 valid 0.2391246525062756
LOSS train 0.2795755296503588 valid 0.2391360418944709
LOSS train 0.2795755296503588 valid 0.23935185858372893
LOSS train 0.2795755296503588 valid 0.23930906583477
LOSS train 0.2795755296503588 valid 0.23929198599523968
LOSS train 0.2795755296503588 valid 0.23942526862107588
LOSS train 0.2795755296503588 valid 0.23946498252533294
LOSS train 0.2795755296503588 valid 0.23947147528330484
LOSS train 0.2795755296503588 valid 0.23948680925304475
LOSS train 0.2795755296503588 valid 0.23924880349958266
LOSS train 0.2795755296503588 valid 0.2391922812788717
LOSS train 0.2795755296503588 valid 0.23907121665337505
LOSS train 0.2795755296503588 valid 0.23910284478296626
LOSS train 0.2795755296503588 valid 0.23907258609930673
LOSS train 0.2795755296503588 valid 0.23920024013832997
LOSS train 0.2795755296503588 valid 0.2391282513503629
LOSS train 0.2795755296503588 valid 0.239222117078801
LOSS train 0.2795755296503588 valid 0.23904709263169086
LOSS train 0.2795755296503588 valid 0.23891746375671366
LOSS train 0.2795755296503588 valid 0.23875190760844794
LOSS train 0.2795755296503588 valid 0.23875075745947508
LOSS train 0.2795755296503588 valid 0.23896813982634374
LOSS train 0.2795755296503588 valid 0.23882856408152917
LOSS train 0.2795755296503588 valid 0.23887127240998063
LOSS train 0.2795755296503588 valid 0.2387650840729475
LOSS train 0.2795755296503588 valid 0.23859089902087824
LOSS train 0.2795755296503588 valid 0.23855671212814822
LOSS train 0.2795755296503588 valid 0.2384463023082376
LOSS train 0.2795755296503588 valid 0.2384214766469656
LOSS train 0.2795755296503588 valid 0.23818731845878974
LOSS train 0.2795755296503588 valid 0.2382074917115054
LOSS train 0.2795755296503588 valid 0.2381489764953005
LOSS train 0.2795755296503588 valid 0.23795106906730396
LOSS train 0.2795755296503588 valid 0.23783955812169033
LOSS train 0.2795755296503588 valid 0.23787944813569387
LOSS train 0.2795755296503588 valid 0.2379830201372716
LOSS train 0.2795755296503588 valid 0.23795954693319663
LOSS train 0.2795755296503588 valid 0.2380468727137561
LOSS train 0.2795755296503588 valid 0.23802116568957535
LOSS train 0.2795755296503588 valid 0.23792533957680992
LOSS train 0.2795755296503588 valid 0.23792034132337128
LOSS train 0.2795755296503588 valid 0.23791342726500903
LOSS train 0.2795755296503588 valid 0.23801533836837208
LOSS train 0.2795755296503588 valid 0.238150338059691
LOSS train 0.2795755296503588 valid 0.23822896507653324
LOSS train 0.2795755296503588 valid 0.23828127204832447
LOSS train 0.2795755296503588 valid 0.23821546560203707
LOSS train 0.2795755296503588 valid 0.23839481308588534
LOSS train 0.2795755296503588 valid 0.23846024015386189
LOSS train 0.2795755296503588 valid 0.23844888812965817
LOSS train 0.2795755296503588 valid 0.23854290944548834
LOSS train 0.2795755296503588 valid 0.23878555945100238
LOSS train 0.2795755296503588 valid 0.2389261821252212
LOSS train 0.2795755296503588 valid 0.23898985936391823
LOSS train 0.2795755296503588 valid 0.23906255034000978
LOSS train 0.2795755296503588 valid 0.2390744351979458
LOSS train 0.2795755296503588 valid 0.23915569317238083
LOSS train 0.2795755296503588 valid 0.2392023250524578
LOSS train 0.2795755296503588 valid 0.23917702375314173
LOSS train 0.2795755296503588 valid 0.23928381927469944
LOSS train 0.2795755296503588 valid 0.23915477892604925
LOSS train 0.2795755296503588 valid 0.2392812646642516
LOSS train 0.2795755296503588 valid 0.23931774979128556
LOSS train 0.2795755296503588 valid 0.23919434639699289
LOSS train 0.2795755296503588 valid 0.239104097088178
LOSS train 0.2795755296503588 valid 0.23915422939660638
LOSS train 0.2795755296503588 valid 0.23894472629570765
LOSS train 0.2795755296503588 valid 0.2390408824991297
LOSS train 0.2795755296503588 valid 0.23929244267647384
LOSS train 0.2795755296503588 valid 0.23946109693877551
LOSS train 0.2795755296503588 valid 0.2394370446360208
LOSS train 0.2795755296503588 valid 0.23949569439598423
LOSS train 0.2795755296503588 valid 0.23946177382622996
LOSS train 0.2795755296503588 valid 0.23942341640531778
LOSS train 0.2795755296503588 valid 0.23959577232599258
LOSS train 0.2795755296503588 valid 0.23957732676034904
LOSS train 0.2795755296503588 valid 0.2397738310789305
LOSS train 0.2795755296503588 valid 0.23970430818471042
LOSS train 0.2795755296503588 valid 0.23964523876041877
LOSS train 0.2795755296503588 valid 0.23975352522204904
LOSS train 0.2795755296503588 valid 0.23980075999861583
LOSS train 0.2795755296503588 valid 0.2396825424428116
LOSS train 0.2795755296503588 valid 0.23980135672776273
LOSS train 0.2795755296503588 valid 0.23978415649369877
LOSS train 0.2795755296503588 valid 0.2397481062091314
LOSS train 0.2795755296503588 valid 0.2398494496884474
LOSS train 0.2795755296503588 valid 0.23988658690270576
LOSS train 0.2795755296503588 valid 0.2398781076130305
LOSS train 0.2795755296503588 valid 0.23992033110875072
LOSS train 0.2795755296503588 valid 0.23983520010732254
LOSS train 0.2795755296503588 valid 0.23987387837772084
LOSS train 0.2795755296503588 valid 0.23993110355366482
LOSS train 0.2795755296503588 valid 0.24009141603957362
LOSS train 0.2795755296503588 valid 0.24016141658821957
LOSS train 0.2795755296503588 valid 0.24014132867256802
LOSS train 0.2795755296503588 valid 0.24020938069398112
LOSS train 0.2795755296503588 valid 0.24040613241274567
LOSS train 0.2795755296503588 valid 0.24044778184358018
LOSS train 0.2795755296503588 valid 0.2403872267064387
LOSS train 0.2795755296503588 valid 0.24035845642740075
LOSS train 0.2795755296503588 valid 0.24018816126213557
LOSS train 0.2795755296503588 valid 0.240014414978802
LOSS train 0.2795755296503588 valid 0.23987494736052245
LOSS train 0.2795755296503588 valid 0.23983940439412244
LOSS train 0.2795755296503588 valid 0.23979642827595984
LOSS train 0.2795755296503588 valid 0.23962423406886035
LOSS train 0.2795755296503588 valid 0.23945927699195577
LOSS train 0.2795755296503588 valid 0.23940763872630183
LOSS train 0.2795755296503588 valid 0.23943715469098428
LOSS train 0.2795755296503588 valid 0.2394759668592821
LOSS train 0.2795755296503588 valid 0.23943465234307976
LOSS train 0.2795755296503588 valid 0.23936439967529283
LOSS train 0.2795755296503588 valid 0.23933903810878596
LOSS train 0.2795755296503588 valid 0.23936316917511832
LOSS train 0.2795755296503588 valid 0.23934517615828022
LOSS train 0.2795755296503588 valid 0.23927835705354042
LOSS train 0.2795755296503588 valid 0.23930001304778334
LOSS train 0.2795755296503588 valid 0.23932550674615866
LOSS train 0.2795755296503588 valid 0.2394208409855155
LOSS train 0.2795755296503588 valid 0.2394726700196832
LOSS train 0.2795755296503588 valid 0.23944042336095023
LOSS train 0.2795755296503588 valid 0.23947636502158362
LOSS train 0.2795755296503588 valid 0.23947737030934968
LOSS train 0.2795755296503588 valid 0.2395346410896467
LOSS train 0.2795755296503588 valid 0.23952043821414312
LOSS train 0.2795755296503588 valid 0.23954817103191073
LOSS train 0.2795755296503588 valid 0.23958021861235826
LOSS train 0.2795755296503588 valid 0.23966135910832057
LOSS train 0.2795755296503588 valid 0.23961566854268312
LOSS train 0.2795755296503588 valid 0.23949676033903342
LOSS train 0.2795755296503588 valid 0.23946738179797442
LOSS train 0.2795755296503588 valid 0.23938617784930363
LOSS train 0.2795755296503588 valid 0.2393104697686511
LOSS train 0.2795755296503588 valid 0.23929883337136612
LOSS train 0.2795755296503588 valid 0.23932836397040275
LOSS train 0.2795755296503588 valid 0.23927153570284032
LOSS train 0.2795755296503588 valid 0.23927432776261598
LOSS train 0.2795755296503588 valid 0.23932218170775393
LOSS train 0.2795755296503588 valid 0.23935455373328202
LOSS train 0.2795755296503588 valid 0.23931575663506038
LOSS train 0.2795755296503588 valid 0.23924594985533365
LOSS train 0.2795755296503588 valid 0.2393003773614059
LOSS train 0.2795755296503588 valid 0.23935553768895707
LOSS train 0.2795755296503588 valid 0.23947109137209233
LOSS train 0.2795755296503588 valid 0.23940805289894343
LOSS train 0.2795755296503588 valid 0.23948125369452242
LOSS train 0.2795755296503588 valid 0.2393834523347594
LOSS train 0.2795755296503588 valid 0.23931171641261215
LOSS train 0.2795755296503588 valid 0.23928178644474643
LOSS train 0.2795755296503588 valid 0.23929878189013554
LOSS train 0.2795755296503588 valid 0.23940709297276713
LOSS train 0.2795755296503588 valid 0.23941685646681246
LOSS train 0.2795755296503588 valid 0.23944100864776752
LOSS train 0.2795755296503588 valid 0.23951254508777955
LOSS train 0.2795755296503588 valid 0.23951499742088897
LOSS train 0.2795755296503588 valid 0.23941966531139847
LOSS train 0.2795755296503588 valid 0.2392690280025982
LOSS train 0.2795755296503588 valid 0.2392886977743458
LOSS train 0.2795755296503588 valid 0.2394543034498563
LOSS train 0.2795755296503588 valid 0.23940275502738667
LOSS train 0.2795755296503588 valid 0.23934055953508332
LOSS train 0.2795755296503588 valid 0.23927422141993435
LOSS train 0.2795755296503588 valid 0.23924850892914823
LOSS train 0.2795755296503588 valid 0.23918960733575456
LOSS train 0.2795755296503588 valid 0.2391711501075941
LOSS train 0.2795755296503588 valid 0.2390495041423529
LOSS train 0.2795755296503588 valid 0.23906763721453517
LOSS train 0.2795755296503588 valid 0.23904320906098314
LOSS train 0.2795755296503588 valid 0.23920008379879387
LOSS train 0.2795755296503588 valid 0.23925866843133733
LOSS train 0.2795755296503588 valid 0.23922919128843814
LOSS train 0.2795755296503588 valid 0.23908870029518173
LOSS train 0.2795755296503588 valid 0.23899877589495702
LOSS train 0.2795755296503588 valid 0.2390437572340569
LOSS train 0.2795755296503588 valid 0.23898191217865264
LOSS train 0.2795755296503588 valid 0.23893915936138554
LOSS train 0.2795755296503588 valid 0.23890883627940307
LOSS train 0.2795755296503588 valid 0.23893760549954585
LOSS train 0.2795755296503588 valid 0.23902166138092676
LOSS train 0.2795755296503588 valid 0.2391160934323996
LOSS train 0.2795755296503588 valid 0.23917577428261885
LOSS train 0.2795755296503588 valid 0.2391618604586572
LOSS train 0.2795755296503588 valid 0.23915266037486785
LOSS train 0.2795755296503588 valid 0.23918985942778148
LOSS train 0.2795755296503588 valid 0.23914093180663057
LOSS train 0.2795755296503588 valid 0.23911610014550905
LOSS train 0.2795755296503588 valid 0.2391800719894757
LOSS train 0.2795755296503588 valid 0.2390738871389841
LOSS train 0.2795755296503588 valid 0.23908419792468733
LOSS train 0.2795755296503588 valid 0.23906824009059227
LOSS train 0.2795755296503588 valid 0.23900118926183775
LOSS train 0.2795755296503588 valid 0.2388723932754766
LOSS train 0.2795755296503588 valid 0.23886159305339275
LOSS train 0.2795755296503588 valid 0.23895876097485302
EPOCH 8:
  batch 1 loss: 0.2764724791049957
  batch 2 loss: 0.26512715220451355
  batch 3 loss: 0.27069716652234393
  batch 4 loss: 0.26896679401397705
  batch 5 loss: 0.2786174058914185
  batch 6 loss: 0.2735770295063655
  batch 7 loss: 0.2771983487265451
  batch 8 loss: 0.28186872974038124
  batch 9 loss: 0.27904665138986373
  batch 10 loss: 0.2783380687236786
  batch 11 loss: 0.2788664319298484
  batch 12 loss: 0.27603938430547714
  batch 13 loss: 0.2739478647708893
  batch 14 loss: 0.27365644701889585
  batch 15 loss: 0.2762716809908549
  batch 16 loss: 0.27543262578547
  batch 17 loss: 0.2730637630995582
  batch 18 loss: 0.27321339812543655
  batch 19 loss: 0.27053257587708923
  batch 20 loss: 0.26869463101029395
  batch 21 loss: 0.27001649638017017
  batch 22 loss: 0.26917282762852585
  batch 23 loss: 0.2681575482306273
  batch 24 loss: 0.2664706266174714
  batch 25 loss: 0.2688783448934555
  batch 26 loss: 0.2671778523004972
  batch 27 loss: 0.26819170735500475
  batch 28 loss: 0.2672262894255774
  batch 29 loss: 0.2674283806619973
  batch 30 loss: 0.2667591467499733
  batch 31 loss: 0.26695484451709256
  batch 32 loss: 0.26616840064525604
  batch 33 loss: 0.26580513697682007
  batch 34 loss: 0.2650693790877567
  batch 35 loss: 0.26515757994992395
  batch 36 loss: 0.2651546262204647
  batch 37 loss: 0.26548310026929184
  batch 38 loss: 0.2663407086541778
  batch 39 loss: 0.26613003665055984
  batch 40 loss: 0.2665171425789595
  batch 41 loss: 0.2666943658415864
  batch 42 loss: 0.26716685898247217
  batch 43 loss: 0.2677966657766076
  batch 44 loss: 0.26809105717323045
  batch 45 loss: 0.26730301678180696
  batch 46 loss: 0.2679422935066016
  batch 47 loss: 0.2679638104869964
  batch 48 loss: 0.26752026410152513
  batch 49 loss: 0.2672799153595555
  batch 50 loss: 0.2669006305932999
  batch 51 loss: 0.26680245060546726
  batch 52 loss: 0.2673771215172914
  batch 53 loss: 0.2669325113858817
  batch 54 loss: 0.2672955379993827
  batch 55 loss: 0.26711037836291573
  batch 56 loss: 0.2670004865420716
  batch 57 loss: 0.2668185649733794
  batch 58 loss: 0.2679565926564151
  batch 59 loss: 0.26792033319756137
  batch 60 loss: 0.26764988775054616
  batch 61 loss: 0.2679274084626651
  batch 62 loss: 0.2687742313550365
  batch 63 loss: 0.26813158321948277
  batch 64 loss: 0.26885579130612314
  batch 65 loss: 0.26878994909616616
  batch 66 loss: 0.26860704543915664
  batch 67 loss: 0.26937902707662154
  batch 68 loss: 0.26959618422038417
  batch 69 loss: 0.26953379377938697
  batch 70 loss: 0.26968477006469455
  batch 71 loss: 0.269344644227498
  batch 72 loss: 0.26960303137699765
  batch 73 loss: 0.26970023774120905
  batch 74 loss: 0.270051182524578
  batch 75 loss: 0.26970402638117474
  batch 76 loss: 0.2699570824441157
  batch 77 loss: 0.26981994393584013
  batch 78 loss: 0.270195803963221
  batch 79 loss: 0.27068697085863425
  batch 80 loss: 0.2704720120877028
  batch 81 loss: 0.2708814401685456
  batch 82 loss: 0.2715087576610286
  batch 83 loss: 0.2716212301369173
  batch 84 loss: 0.2713425769692376
  batch 85 loss: 0.271189784302431
  batch 86 loss: 0.27167656186015104
  batch 87 loss: 0.2717427683287653
  batch 88 loss: 0.2715465155514804
  batch 89 loss: 0.2711436031909471
  batch 90 loss: 0.27107449372609455
  batch 91 loss: 0.2709462528045361
  batch 92 loss: 0.2706547863781452
  batch 93 loss: 0.27043304087654235
  batch 94 loss: 0.2710813545483224
  batch 95 loss: 0.27073215105031667
  batch 96 loss: 0.2713089515455067
  batch 97 loss: 0.27207713044181314
  batch 98 loss: 0.27235872937100275
  batch 99 loss: 0.2725200109710597
  batch 100 loss: 0.2727542735636234
  batch 101 loss: 0.2731052148755234
  batch 102 loss: 0.27346547605360255
  batch 103 loss: 0.2739917567922074
  batch 104 loss: 0.2743358129205612
  batch 105 loss: 0.27440362530095236
  batch 106 loss: 0.2746862343178605
  batch 107 loss: 0.27457928448636953
  batch 108 loss: 0.2745261938759574
  batch 109 loss: 0.2742895120327626
  batch 110 loss: 0.2743031978607178
  batch 111 loss: 0.2741669837955956
  batch 112 loss: 0.2737535314102258
  batch 113 loss: 0.27393959805501245
  batch 114 loss: 0.274269071718057
  batch 115 loss: 0.2746625497289326
  batch 116 loss: 0.2749573736098306
  batch 117 loss: 0.27536411659839827
  batch 118 loss: 0.27520084317963
  batch 119 loss: 0.27549917895753845
  batch 120 loss: 0.27536414625744027
  batch 121 loss: 0.27532982124277383
  batch 122 loss: 0.27526939635882613
  batch 123 loss: 0.2751682377685376
  batch 124 loss: 0.27545527645176454
  batch 125 loss: 0.2754907408952713
  batch 126 loss: 0.2753378693310041
  batch 127 loss: 0.27575369202715205
  batch 128 loss: 0.27555581100750715
  batch 129 loss: 0.27580652022084523
  batch 130 loss: 0.27578641118911595
  batch 131 loss: 0.2758774860897137
  batch 132 loss: 0.27585540745746007
  batch 133 loss: 0.2761780262217486
  batch 134 loss: 0.2760484935854798
  batch 135 loss: 0.2760476546155082
  batch 136 loss: 0.2762483289355741
  batch 137 loss: 0.27637925276356023
  batch 138 loss: 0.2764256824401842
  batch 139 loss: 0.27690400183200836
  batch 140 loss: 0.2766451491841248
  batch 141 loss: 0.2769215556112587
  batch 142 loss: 0.27705666538275464
  batch 143 loss: 0.2770043754285866
  batch 144 loss: 0.27702268430342275
  batch 145 loss: 0.2769827888957385
  batch 146 loss: 0.2769565540429664
  batch 147 loss: 0.27730349162403417
  batch 148 loss: 0.2773692413560442
  batch 149 loss: 0.2771024782985649
  batch 150 loss: 0.27717023859421414
  batch 151 loss: 0.27749423386640104
  batch 152 loss: 0.2777634460086885
  batch 153 loss: 0.2778020332646526
  batch 154 loss: 0.27806260920577236
  batch 155 loss: 0.27805771721947575
  batch 156 loss: 0.27796903109321225
  batch 157 loss: 0.2780359832534365
  batch 158 loss: 0.27821615687276746
  batch 159 loss: 0.27840174565900044
  batch 160 loss: 0.27817120030522346
  batch 161 loss: 0.278219300212327
  batch 162 loss: 0.27833448074482103
  batch 163 loss: 0.2784590874712891
  batch 164 loss: 0.27833353509990183
  batch 165 loss: 0.2782300421685884
  batch 166 loss: 0.278315733534744
  batch 167 loss: 0.2782699024962808
  batch 168 loss: 0.2783270804654984
  batch 169 loss: 0.2782440716345635
  batch 170 loss: 0.27816540931954103
  batch 171 loss: 0.27812602575759443
  batch 172 loss: 0.27829460834347924
  batch 173 loss: 0.27826895603554785
  batch 174 loss: 0.27815135302899896
  batch 175 loss: 0.2782051134109497
  batch 176 loss: 0.2780083470385183
  batch 177 loss: 0.27805978581730256
  batch 178 loss: 0.2782229600327738
  batch 179 loss: 0.2783590531881961
  batch 180 loss: 0.2782556047042211
  batch 181 loss: 0.27832159070678836
  batch 182 loss: 0.27827064021603093
  batch 183 loss: 0.27798329139016364
  batch 184 loss: 0.2779515605905782
  batch 185 loss: 0.2780373272058126
  batch 186 loss: 0.2780552395889836
  batch 187 loss: 0.2779251662167636
  batch 188 loss: 0.2776325991813173
  batch 189 loss: 0.2773181756652852
  batch 190 loss: 0.27751357649502
  batch 191 loss: 0.27748663700063814
  batch 192 loss: 0.2774881587053339
  batch 193 loss: 0.2774739886194931
  batch 194 loss: 0.2776683301655288
  batch 195 loss: 0.2777016118550912
  batch 196 loss: 0.2774866809498291
  batch 197 loss: 0.27740345259910915
  batch 198 loss: 0.2777011893463857
  batch 199 loss: 0.2776570818082771
  batch 200 loss: 0.27770687274634837
  batch 201 loss: 0.2778228508744074
  batch 202 loss: 0.27785385414810465
  batch 203 loss: 0.2778191452689946
  batch 204 loss: 0.27767902598077177
  batch 205 loss: 0.27778285189372737
  batch 206 loss: 0.27776895231992293
  batch 207 loss: 0.27777780844393557
  batch 208 loss: 0.2776241499500779
  batch 209 loss: 0.27732063972493676
  batch 210 loss: 0.2774166654263224
  batch 211 loss: 0.2774011468011621
  batch 212 loss: 0.27734247559927544
  batch 213 loss: 0.27723099850992644
  batch 214 loss: 0.27723975054850086
  batch 215 loss: 0.27703657961168954
  batch 216 loss: 0.27685354712108773
  batch 217 loss: 0.27684728389236785
  batch 218 loss: 0.27678781276175735
  batch 219 loss: 0.27690296718791196
  batch 220 loss: 0.2768497917462479
  batch 221 loss: 0.27689598891799805
  batch 222 loss: 0.27695146317149066
  batch 223 loss: 0.2768645321975374
  batch 224 loss: 0.27679493497791036
  batch 225 loss: 0.2766574246353573
  batch 226 loss: 0.2767260198572041
  batch 227 loss: 0.27652706934491966
  batch 228 loss: 0.2763704085036328
  batch 229 loss: 0.27628668817370217
  batch 230 loss: 0.27629195050052974
  batch 231 loss: 0.2761687793772974
  batch 232 loss: 0.2760440929439561
  batch 233 loss: 0.2759972209582513
  batch 234 loss: 0.27595989138652116
  batch 235 loss: 0.27595562173965127
  batch 236 loss: 0.27584517235725614
  batch 237 loss: 0.27591187522632665
  batch 238 loss: 0.2757957773048337
  batch 239 loss: 0.27569025131449043
  batch 240 loss: 0.2756984946628412
  batch 241 loss: 0.27583333812808597
  batch 242 loss: 0.27571805782061964
  batch 243 loss: 0.27581495390016847
  batch 244 loss: 0.2758461400622227
  batch 245 loss: 0.2757095488358517
  batch 246 loss: 0.2757128482548202
  batch 247 loss: 0.2759306279391895
  batch 248 loss: 0.2759745751297282
  batch 249 loss: 0.2758608811710733
  batch 250 loss: 0.27580253285169604
  batch 251 loss: 0.2760141465174724
  batch 252 loss: 0.27579777457174803
  batch 253 loss: 0.27568923708478454
  batch 254 loss: 0.27563022241348356
  batch 255 loss: 0.2756637318461549
  batch 256 loss: 0.2757005166495219
  batch 257 loss: 0.2756777928729002
  batch 258 loss: 0.2755770973803461
  batch 259 loss: 0.2755686103952437
  batch 260 loss: 0.27556857655827816
  batch 261 loss: 0.2754745271584997
  batch 262 loss: 0.27536952427325356
  batch 263 loss: 0.275372328187123
  batch 264 loss: 0.2752284376571576
  batch 265 loss: 0.27509774919950736
  batch 266 loss: 0.2750092767795226
  batch 267 loss: 0.2751196314221464
  batch 268 loss: 0.2749349157423226
  batch 269 loss: 0.27481084818290513
  batch 270 loss: 0.27493497486467716
  batch 271 loss: 0.2748640685503773
  batch 272 loss: 0.2749347921241732
  batch 273 loss: 0.2748916860901829
  batch 274 loss: 0.2748900217949039
  batch 275 loss: 0.27484515937891874
  batch 276 loss: 0.2747863964110181
  batch 277 loss: 0.27470059870382507
  batch 278 loss: 0.27452065522293395
  batch 279 loss: 0.2745863204574927
  batch 280 loss: 0.2745251092527594
  batch 281 loss: 0.2743925187087144
  batch 282 loss: 0.2743272728531073
  batch 283 loss: 0.27423898122757145
  batch 284 loss: 0.2742732659402028
  batch 285 loss: 0.274176616940582
  batch 286 loss: 0.27414423800431764
  batch 287 loss: 0.27420331335233894
  batch 288 loss: 0.27399472799152136
  batch 289 loss: 0.27422909216897295
  batch 290 loss: 0.27402545927927413
  batch 291 loss: 0.27400504953877625
  batch 292 loss: 0.2741258586820674
  batch 293 loss: 0.27415220398748286
  batch 294 loss: 0.27404558344357677
  batch 295 loss: 0.2740056065179534
  batch 296 loss: 0.2740599334844061
  batch 297 loss: 0.27399874967758103
  batch 298 loss: 0.2740489012442979
  batch 299 loss: 0.27390593295312643
  batch 300 loss: 0.27391526942451794
  batch 301 loss: 0.2738089269874896
  batch 302 loss: 0.2737406777921102
  batch 303 loss: 0.2736671783841482
  batch 304 loss: 0.27364184349579246
  batch 305 loss: 0.2734675925774652
  batch 306 loss: 0.27363262052824294
  batch 307 loss: 0.2736155344628356
  batch 308 loss: 0.2737388919990558
  batch 309 loss: 0.27366659783043906
  batch 310 loss: 0.2736696001502775
  batch 311 loss: 0.27371692681427556
  batch 312 loss: 0.27376806310927254
  batch 313 loss: 0.2737933742447783
  batch 314 loss: 0.27369857550996124
  batch 315 loss: 0.2736675712324324
  batch 316 loss: 0.27365584534746185
  batch 317 loss: 0.2735629979086223
  batch 318 loss: 0.2734587347638682
  batch 319 loss: 0.2734069602336256
  batch 320 loss: 0.27333875740878283
  batch 321 loss: 0.2733818697873677
  batch 322 loss: 0.2734013732453311
  batch 323 loss: 0.2733493857431707
  batch 324 loss: 0.2731905012494988
  batch 325 loss: 0.2730972249232806
  batch 326 loss: 0.2731079508099088
  batch 327 loss: 0.2731519942527882
  batch 328 loss: 0.27294758139405306
  batch 329 loss: 0.2729821479157474
  batch 330 loss: 0.2729441556515116
  batch 331 loss: 0.27297997488175996
  batch 332 loss: 0.27285714083945894
  batch 333 loss: 0.27289142260501337
  batch 334 loss: 0.27274605109841527
  batch 335 loss: 0.2725972350408782
  batch 336 loss: 0.27245883692410733
  batch 337 loss: 0.27238143078474336
  batch 338 loss: 0.27231191676220246
  batch 339 loss: 0.2722569092624659
  batch 340 loss: 0.2722164887277519
  batch 341 loss: 0.2722102863785109
  batch 342 loss: 0.27210558946543967
  batch 343 loss: 0.2720807817256833
  batch 344 loss: 0.27201451833338236
  batch 345 loss: 0.27208407352799957
  batch 346 loss: 0.27193721017741057
  batch 347 loss: 0.27185690755115804
  batch 348 loss: 0.2717998229909217
  batch 349 loss: 0.27168334486832935
  batch 350 loss: 0.27163126570837837
  batch 351 loss: 0.2715661803371886
  batch 352 loss: 0.27163712815804913
  batch 353 loss: 0.27158399318838256
  batch 354 loss: 0.2717360665569198
  batch 355 loss: 0.27170319985335983
  batch 356 loss: 0.2716587490580055
  batch 357 loss: 0.27157135612490463
  batch 358 loss: 0.2715873804838298
  batch 359 loss: 0.2714734396645618
  batch 360 loss: 0.27144817763732537
  batch 361 loss: 0.2713960602755692
  batch 362 loss: 0.27126308839294794
  batch 363 loss: 0.271160614433039
  batch 364 loss: 0.2709580038029414
  batch 365 loss: 0.2709123576748861
  batch 366 loss: 0.2708857700717254
  batch 367 loss: 0.27082659848217094
  batch 368 loss: 0.2706559295968517
  batch 369 loss: 0.2706272715556266
  batch 370 loss: 0.270563193068311
  batch 371 loss: 0.27058252069667343
  batch 372 loss: 0.2704643225798043
  batch 373 loss: 0.2703864697476814
  batch 374 loss: 0.27023342363337144
  batch 375 loss: 0.27016369366645815
  batch 376 loss: 0.27022303902405376
  batch 377 loss: 0.27011847468364775
  batch 378 loss: 0.27006114680300314
  batch 379 loss: 0.27005139189848487
  batch 380 loss: 0.2700477872239916
  batch 381 loss: 0.27000509527098787
  batch 382 loss: 0.26992672295626546
  batch 383 loss: 0.2699196384174083
  batch 384 loss: 0.2698286570375785
  batch 385 loss: 0.26985501759238056
  batch 386 loss: 0.26978077928636973
  batch 387 loss: 0.2698634628023596
  batch 388 loss: 0.2698969634384224
  batch 389 loss: 0.26988464402355694
  batch 390 loss: 0.269829563338023
  batch 391 loss: 0.26982636917430114
  batch 392 loss: 0.2697467789510075
  batch 393 loss: 0.26965434663471677
  batch 394 loss: 0.26965629630887566
  batch 395 loss: 0.269548320468468
  batch 396 loss: 0.26954751928346327
  batch 397 loss: 0.2695415366806972
  batch 398 loss: 0.2694747976996192
  batch 399 loss: 0.2694639240515262
  batch 400 loss: 0.26952561397105457
  batch 401 loss: 0.26944070303826556
  batch 402 loss: 0.26948193909220436
  batch 403 loss: 0.26955647297000174
  batch 404 loss: 0.269630335566431
  batch 405 loss: 0.2696693435863212
  batch 406 loss: 0.26972428786343544
  batch 407 loss: 0.26975497771832513
  batch 408 loss: 0.2698277429330583
  batch 409 loss: 0.26986261444453213
  batch 410 loss: 0.2699681222438812
  batch 411 loss: 0.2699756822446837
  batch 412 loss: 0.2699575767065715
  batch 413 loss: 0.27000163701198293
  batch 414 loss: 0.26997465895857786
  batch 415 loss: 0.2699378117739436
  batch 416 loss: 0.2700340482048117
  batch 417 loss: 0.269999433621514
  batch 418 loss: 0.269952914361178
  batch 419 loss: 0.2699903930428489
  batch 420 loss: 0.26997219849200477
  batch 421 loss: 0.2698836090870538
  batch 422 loss: 0.26998272396941886
  batch 423 loss: 0.2699316497671971
  batch 424 loss: 0.26985456622293535
  batch 425 loss: 0.2698492650775348
  batch 426 loss: 0.2698113561865869
  batch 427 loss: 0.2698441038575608
  batch 428 loss: 0.269827493614404
  batch 429 loss: 0.2698510318716645
  batch 430 loss: 0.2697748775745547
  batch 431 loss: 0.26985264955015026
  batch 432 loss: 0.2698944456944311
  batch 433 loss: 0.2698789267493175
  batch 434 loss: 0.2699051230527838
  batch 435 loss: 0.2698277789628369
  batch 436 loss: 0.2698129283247191
  batch 437 loss: 0.2699031403940384
  batch 438 loss: 0.26996927579107893
  batch 439 loss: 0.2699183663841261
  batch 440 loss: 0.26998534890061077
  batch 441 loss: 0.26988336687185327
  batch 442 loss: 0.26984101142818573
  batch 443 loss: 0.2698111749395022
  batch 444 loss: 0.26972437629828583
  batch 445 loss: 0.26976328760050655
  batch 446 loss: 0.26978099292703805
  batch 447 loss: 0.2696892898488098
  batch 448 loss: 0.2697356725111604
  batch 449 loss: 0.26974917440478147
  batch 450 loss: 0.26970833049880133
  batch 451 loss: 0.2696938479686788
  batch 452 loss: 0.2698140292700413
  batch 453 loss: 0.2698391060823899
  batch 454 loss: 0.26986673316766513
  batch 455 loss: 0.2698803206066509
  batch 456 loss: 0.2698425739480738
  batch 457 loss: 0.2697758433333633
  batch 458 loss: 0.2697015438863284
  batch 459 loss: 0.2697275320234382
  batch 460 loss: 0.26975088615132414
  batch 461 loss: 0.2697002272285246
  batch 462 loss: 0.26968394832693654
  batch 463 loss: 0.2696096732987183
  batch 464 loss: 0.2695905291957074
  batch 465 loss: 0.2694695241348718
  batch 466 loss: 0.26935273421680467
  batch 467 loss: 0.269513195855204
  batch 468 loss: 0.2695063637235226
  batch 469 loss: 0.2696576381543044
  batch 470 loss: 0.2696221335137144
  batch 471 loss: 0.26967661092235784
  batch 472 loss: 0.26961476216881963
LOSS train 0.26961476216881963 valid 0.24462534487247467
LOSS train 0.26961476216881963 valid 0.2366839498281479
LOSS train 0.26961476216881963 valid 0.23220327496528625
LOSS train 0.26961476216881963 valid 0.2283806949853897
LOSS train 0.26961476216881963 valid 0.22409812211990357
LOSS train 0.26961476216881963 valid 0.2302269289890925
LOSS train 0.26961476216881963 valid 0.24198104228292192
LOSS train 0.26961476216881963 valid 0.23964531160891056
LOSS train 0.26961476216881963 valid 0.23844196067916024
LOSS train 0.26961476216881963 valid 0.23837637305259704
LOSS train 0.26961476216881963 valid 0.23733470250259747
LOSS train 0.26961476216881963 valid 0.237187838802735
LOSS train 0.26961476216881963 valid 0.2354178107701815
LOSS train 0.26961476216881963 valid 0.23560678320271627
LOSS train 0.26961476216881963 valid 0.2314355581998825
LOSS train 0.26961476216881963 valid 0.23152620065957308
LOSS train 0.26961476216881963 valid 0.23234465805923238
LOSS train 0.26961476216881963 valid 0.23341638760434258
LOSS train 0.26961476216881963 valid 0.23482234148602738
LOSS train 0.26961476216881963 valid 0.2331383615732193
LOSS train 0.26961476216881963 valid 0.23233228638058617
LOSS train 0.26961476216881963 valid 0.2307409183545546
LOSS train 0.26961476216881963 valid 0.23119070413319961
LOSS train 0.26961476216881963 valid 0.22994627617299557
LOSS train 0.26961476216881963 valid 0.2291003054380417
LOSS train 0.26961476216881963 valid 0.22906543371769097
LOSS train 0.26961476216881963 valid 0.22910115509121506
LOSS train 0.26961476216881963 valid 0.22937665560415813
LOSS train 0.26961476216881963 valid 0.2292569562278945
LOSS train 0.26961476216881963 valid 0.22998726218938828
LOSS train 0.26961476216881963 valid 0.23044021427631378
LOSS train 0.26961476216881963 valid 0.2299037454649806
LOSS train 0.26961476216881963 valid 0.23092082504070166
LOSS train 0.26961476216881963 valid 0.23057530469754162
LOSS train 0.26961476216881963 valid 0.23183965768132891
LOSS train 0.26961476216881963 valid 0.23139738332894114
LOSS train 0.26961476216881963 valid 0.23137825445548907
LOSS train 0.26961476216881963 valid 0.2323618143012649
LOSS train 0.26961476216881963 valid 0.23217522983367628
LOSS train 0.26961476216881963 valid 0.23206769488751888
LOSS train 0.26961476216881963 valid 0.2328465344702325
LOSS train 0.26961476216881963 valid 0.23303636695657456
LOSS train 0.26961476216881963 valid 0.23269300162792206
LOSS train 0.26961476216881963 valid 0.2328793318434195
LOSS train 0.26961476216881963 valid 0.23244806461864048
LOSS train 0.26961476216881963 valid 0.23274555122074875
LOSS train 0.26961476216881963 valid 0.23364655610094678
LOSS train 0.26961476216881963 valid 0.23375974937031666
LOSS train 0.26961476216881963 valid 0.2341695984407347
LOSS train 0.26961476216881963 valid 0.23331222385168077
LOSS train 0.26961476216881963 valid 0.23338017744176529
LOSS train 0.26961476216881963 valid 0.23285927136357015
LOSS train 0.26961476216881963 valid 0.23320330848109047
LOSS train 0.26961476216881963 valid 0.23332302437888253
LOSS train 0.26961476216881963 valid 0.23309423435818066
LOSS train 0.26961476216881963 valid 0.23231496528855392
LOSS train 0.26961476216881963 valid 0.23184026098042204
LOSS train 0.26961476216881963 valid 0.23139776340846357
LOSS train 0.26961476216881963 valid 0.23178950861348943
LOSS train 0.26961476216881963 valid 0.2317556786040465
LOSS train 0.26961476216881963 valid 0.23143313287711534
LOSS train 0.26961476216881963 valid 0.2321673509094023
LOSS train 0.26961476216881963 valid 0.2319386118934268
LOSS train 0.26961476216881963 valid 0.23279010131955147
LOSS train 0.26961476216881963 valid 0.23293191882280204
LOSS train 0.26961476216881963 valid 0.23278625670707587
LOSS train 0.26961476216881963 valid 0.23197937456529533
LOSS train 0.26961476216881963 valid 0.23198116220095577
LOSS train 0.26961476216881963 valid 0.23157028098037277
LOSS train 0.26961476216881963 valid 0.23153166430337088
LOSS train 0.26961476216881963 valid 0.2313055393981262
LOSS train 0.26961476216881963 valid 0.23151530221932465
LOSS train 0.26961476216881963 valid 0.23145667494159855
LOSS train 0.26961476216881963 valid 0.23144887266932307
LOSS train 0.26961476216881963 valid 0.23170244296391804
LOSS train 0.26961476216881963 valid 0.23197979009465167
LOSS train 0.26961476216881963 valid 0.23209440533990983
LOSS train 0.26961476216881963 valid 0.23237475771934557
LOSS train 0.26961476216881963 valid 0.2323651998480664
LOSS train 0.26961476216881963 valid 0.23169687036424874
LOSS train 0.26961476216881963 valid 0.23089248052349798
LOSS train 0.26961476216881963 valid 0.23142284918122175
LOSS train 0.26961476216881963 valid 0.2312762098140027
LOSS train 0.26961476216881963 valid 0.2311730216301623
LOSS train 0.26961476216881963 valid 0.23110520383890937
LOSS train 0.26961476216881963 valid 0.23060007999802745
LOSS train 0.26961476216881963 valid 0.2302461888255744
LOSS train 0.26961476216881963 valid 0.22975365902212533
LOSS train 0.26961476216881963 valid 0.23027825104386618
LOSS train 0.26961476216881963 valid 0.23023964977926678
LOSS train 0.26961476216881963 valid 0.2302133185522897
LOSS train 0.26961476216881963 valid 0.23048466962316763
LOSS train 0.26961476216881963 valid 0.23037564690395068
LOSS train 0.26961476216881963 valid 0.230481116220038
LOSS train 0.26961476216881963 valid 0.23045377041164197
LOSS train 0.26961476216881963 valid 0.23071593542893729
LOSS train 0.26961476216881963 valid 0.23075722296213366
LOSS train 0.26961476216881963 valid 0.2308961238179888
LOSS train 0.26961476216881963 valid 0.23096598970769633
LOSS train 0.26961476216881963 valid 0.2311239893734455
LOSS train 0.26961476216881963 valid 0.23144868149025605
LOSS train 0.26961476216881963 valid 0.23148713436196833
LOSS train 0.26961476216881963 valid 0.23133146704979313
LOSS train 0.26961476216881963 valid 0.23138969062039486
LOSS train 0.26961476216881963 valid 0.23148497442404428
LOSS train 0.26961476216881963 valid 0.2316342201435341
LOSS train 0.26961476216881963 valid 0.2315483867565048
LOSS train 0.26961476216881963 valid 0.23163789179590014
LOSS train 0.26961476216881963 valid 0.2319734323462215
LOSS train 0.26961476216881963 valid 0.23227906985716387
LOSS train 0.26961476216881963 valid 0.2318950680730579
LOSS train 0.26961476216881963 valid 0.2318631373345852
LOSS train 0.26961476216881963 valid 0.23181857577467388
LOSS train 0.26961476216881963 valid 0.2317711020770826
LOSS train 0.26961476216881963 valid 0.23182457348574761
LOSS train 0.26961476216881963 valid 0.23182546848367
LOSS train 0.26961476216881963 valid 0.2319459380247654
LOSS train 0.26961476216881963 valid 0.2318758703136848
LOSS train 0.26961476216881963 valid 0.23177179531389927
LOSS train 0.26961476216881963 valid 0.2315068775167068
LOSS train 0.26961476216881963 valid 0.23142086905388792
LOSS train 0.26961476216881963 valid 0.2313270838778527
LOSS train 0.26961476216881963 valid 0.23120139418094138
LOSS train 0.26961476216881963 valid 0.23152815057866036
LOSS train 0.26961476216881963 valid 0.23144670009613036
LOSS train 0.26961476216881963 valid 0.23178627779559483
LOSS train 0.26961476216881963 valid 0.23171261316678654
LOSS train 0.26961476216881963 valid 0.23212979699019343
LOSS train 0.26961476216881963 valid 0.23216160531191862
LOSS train 0.26961476216881963 valid 0.23197740110067222
LOSS train 0.26961476216881963 valid 0.23197768275974362
LOSS train 0.26961476216881963 valid 0.23182883858680725
LOSS train 0.26961476216881963 valid 0.2318476157304936
LOSS train 0.26961476216881963 valid 0.23194103621280016
LOSS train 0.26961476216881963 valid 0.23177932070361243
LOSS train 0.26961476216881963 valid 0.23174736079047709
LOSS train 0.26961476216881963 valid 0.2315593991618957
LOSS train 0.26961476216881963 valid 0.23153667745814807
LOSS train 0.26961476216881963 valid 0.2314462593133501
LOSS train 0.26961476216881963 valid 0.23158052350793565
LOSS train 0.26961476216881963 valid 0.23160290485578225
LOSS train 0.26961476216881963 valid 0.23192119703326428
LOSS train 0.26961476216881963 valid 0.23189938089230677
LOSS train 0.26961476216881963 valid 0.23188210123529038
LOSS train 0.26961476216881963 valid 0.23165056993221414
LOSS train 0.26961476216881963 valid 0.23182381138409655
LOSS train 0.26961476216881963 valid 0.2316382199931307
LOSS train 0.26961476216881963 valid 0.2322701184532127
LOSS train 0.26961476216881963 valid 0.2323446316807062
LOSS train 0.26961476216881963 valid 0.23233418037494025
LOSS train 0.26961476216881963 valid 0.23239358331983453
LOSS train 0.26961476216881963 valid 0.23208399979691757
LOSS train 0.26961476216881963 valid 0.23219440624215246
LOSS train 0.26961476216881963 valid 0.23197646381018996
LOSS train 0.26961476216881963 valid 0.23197803737655762
LOSS train 0.26961476216881963 valid 0.23214762036999068
LOSS train 0.26961476216881963 valid 0.23203329068080636
LOSS train 0.26961476216881963 valid 0.2321259238863293
LOSS train 0.26961476216881963 valid 0.23220027655175646
LOSS train 0.26961476216881963 valid 0.23217124324291943
LOSS train 0.26961476216881963 valid 0.2320827764013539
LOSS train 0.26961476216881963 valid 0.23198034090024452
LOSS train 0.26961476216881963 valid 0.2318390176340115
LOSS train 0.26961476216881963 valid 0.23160291072435496
LOSS train 0.26961476216881963 valid 0.23142509397232172
LOSS train 0.26961476216881963 valid 0.23153652421322213
LOSS train 0.26961476216881963 valid 0.2318491429804328
LOSS train 0.26961476216881963 valid 0.23169824622926258
LOSS train 0.26961476216881963 valid 0.2318847375155906
LOSS train 0.26961476216881963 valid 0.23200454203521503
LOSS train 0.26961476216881963 valid 0.23200637790543294
LOSS train 0.26961476216881963 valid 0.23189938492899717
LOSS train 0.26961476216881963 valid 0.2319528046198663
LOSS train 0.26961476216881963 valid 0.23196043450941986
LOSS train 0.26961476216881963 valid 0.23171140798500606
LOSS train 0.26961476216881963 valid 0.23169577460397373
LOSS train 0.26961476216881963 valid 0.23175412669020184
LOSS train 0.26961476216881963 valid 0.23198818591203582
LOSS train 0.26961476216881963 valid 0.23194649565819256
LOSS train 0.26961476216881963 valid 0.23190614928801853
LOSS train 0.26961476216881963 valid 0.23203399415174242
LOSS train 0.26961476216881963 valid 0.2320261361834767
LOSS train 0.26961476216881963 valid 0.23207809283433717
LOSS train 0.26961476216881963 valid 0.23207133668272392
LOSS train 0.26961476216881963 valid 0.23181056122522095
LOSS train 0.26961476216881963 valid 0.23175957742878187
LOSS train 0.26961476216881963 valid 0.2316550671416808
LOSS train 0.26961476216881963 valid 0.23166131909857404
LOSS train 0.26961476216881963 valid 0.23156475981391927
LOSS train 0.26961476216881963 valid 0.23170635990406338
LOSS train 0.26961476216881963 valid 0.23164534927662755
LOSS train 0.26961476216881963 valid 0.2317256834357977
LOSS train 0.26961476216881963 valid 0.2315635551442754
LOSS train 0.26961476216881963 valid 0.23144573042380442
LOSS train 0.26961476216881963 valid 0.23127900874003385
LOSS train 0.26961476216881963 valid 0.23128624602544065
LOSS train 0.26961476216881963 valid 0.23153207727192623
LOSS train 0.26961476216881963 valid 0.23143801190937408
LOSS train 0.26961476216881963 valid 0.2314904667624277
LOSS train 0.26961476216881963 valid 0.2313444186002016
LOSS train 0.26961476216881963 valid 0.23116705674140609
LOSS train 0.26961476216881963 valid 0.231099469679417
LOSS train 0.26961476216881963 valid 0.23099882041879474
LOSS train 0.26961476216881963 valid 0.2309584542378491
LOSS train 0.26961476216881963 valid 0.23073454426556098
LOSS train 0.26961476216881963 valid 0.23077323960447774
LOSS train 0.26961476216881963 valid 0.23071862973164822
LOSS train 0.26961476216881963 valid 0.23051382128435832
LOSS train 0.26961476216881963 valid 0.23038593155630469
LOSS train 0.26961476216881963 valid 0.23041535984902156
LOSS train 0.26961476216881963 valid 0.23054161102850856
LOSS train 0.26961476216881963 valid 0.23056250374834492
LOSS train 0.26961476216881963 valid 0.2306454259325081
LOSS train 0.26961476216881963 valid 0.23061175150013416
LOSS train 0.26961476216881963 valid 0.23047258153904315
LOSS train 0.26961476216881963 valid 0.23047845141479262
LOSS train 0.26961476216881963 valid 0.2304975071123668
LOSS train 0.26961476216881963 valid 0.23062916352934792
LOSS train 0.26961476216881963 valid 0.23073058606010594
LOSS train 0.26961476216881963 valid 0.2308234126053073
LOSS train 0.26961476216881963 valid 0.2308368705111931
LOSS train 0.26961476216881963 valid 0.2308091242034156
LOSS train 0.26961476216881963 valid 0.2309348723546272
LOSS train 0.26961476216881963 valid 0.23100295443353908
LOSS train 0.26961476216881963 valid 0.2310005909204483
LOSS train 0.26961476216881963 valid 0.23108620077899072
LOSS train 0.26961476216881963 valid 0.23132718679400793
LOSS train 0.26961476216881963 valid 0.23145986145787073
LOSS train 0.26961476216881963 valid 0.2315166776096977
LOSS train 0.26961476216881963 valid 0.23157551580149194
LOSS train 0.26961476216881963 valid 0.2315787182096795
LOSS train 0.26961476216881963 valid 0.2316496847518559
LOSS train 0.26961476216881963 valid 0.2317030463735433
LOSS train 0.26961476216881963 valid 0.23171537942611253
LOSS train 0.26961476216881963 valid 0.2317991848321671
LOSS train 0.26961476216881963 valid 0.23167789064474026
LOSS train 0.26961476216881963 valid 0.23178307448006882
LOSS train 0.26961476216881963 valid 0.23181244931301148
LOSS train 0.26961476216881963 valid 0.23168922499383343
LOSS train 0.26961476216881963 valid 0.2316139101361235
LOSS train 0.26961476216881963 valid 0.23168622909492476
LOSS train 0.26961476216881963 valid 0.23148498487127714
LOSS train 0.26961476216881963 valid 0.23157555999333967
LOSS train 0.26961476216881963 valid 0.23180272521787001
LOSS train 0.26961476216881963 valid 0.23195561687557065
LOSS train 0.26961476216881963 valid 0.23191376527150473
LOSS train 0.26961476216881963 valid 0.23201501043701944
LOSS train 0.26961476216881963 valid 0.23195547009668044
LOSS train 0.26961476216881963 valid 0.23191535796027585
LOSS train 0.26961476216881963 valid 0.2320685349702835
LOSS train 0.26961476216881963 valid 0.2320283747526754
LOSS train 0.26961476216881963 valid 0.2322272491596994
LOSS train 0.26961476216881963 valid 0.23217126544520789
LOSS train 0.26961476216881963 valid 0.23208415977598176
LOSS train 0.26961476216881963 valid 0.23215078337519776
LOSS train 0.26961476216881963 valid 0.23219734529266134
LOSS train 0.26961476216881963 valid 0.23208881102879222
LOSS train 0.26961476216881963 valid 0.23219561166772545
LOSS train 0.26961476216881963 valid 0.23217390247523553
LOSS train 0.26961476216881963 valid 0.23214955725348912
LOSS train 0.26961476216881963 valid 0.2322363744407778
LOSS train 0.26961476216881963 valid 0.23222884985099312
LOSS train 0.26961476216881963 valid 0.23222366961236235
LOSS train 0.26961476216881963 valid 0.23228716246332182
LOSS train 0.26961476216881963 valid 0.23221354703858213
LOSS train 0.26961476216881963 valid 0.2322691763590153
LOSS train 0.26961476216881963 valid 0.23231456470623446
LOSS train 0.26961476216881963 valid 0.2324620728283676
LOSS train 0.26961476216881963 valid 0.23250367223773305
LOSS train 0.26961476216881963 valid 0.2324921750360065
LOSS train 0.26961476216881963 valid 0.2325274152509401
LOSS train 0.26961476216881963 valid 0.23272464380544775
LOSS train 0.26961476216881963 valid 0.23277356227238974
LOSS train 0.26961476216881963 valid 0.23270409440037107
LOSS train 0.26961476216881963 valid 0.23267622313716194
LOSS train 0.26961476216881963 valid 0.23251731612760088
LOSS train 0.26961476216881963 valid 0.232336318611238
LOSS train 0.26961476216881963 valid 0.23220874996279642
LOSS train 0.26961476216881963 valid 0.23219959063982878
LOSS train 0.26961476216881963 valid 0.23218771362943308
LOSS train 0.26961476216881963 valid 0.232013379604791
LOSS train 0.26961476216881963 valid 0.23182938765760855
LOSS train 0.26961476216881963 valid 0.23178513158547162
LOSS train 0.26961476216881963 valid 0.23181040369918648
LOSS train 0.26961476216881963 valid 0.23184196028793067
LOSS train 0.26961476216881963 valid 0.2317893850532445
LOSS train 0.26961476216881963 valid 0.23172881031077913
LOSS train 0.26961476216881963 valid 0.2316889427602291
LOSS train 0.26961476216881963 valid 0.2317272510289321
LOSS train 0.26961476216881963 valid 0.2317035258843981
LOSS train 0.26961476216881963 valid 0.23162806771465183
LOSS train 0.26961476216881963 valid 0.23164622337646681
LOSS train 0.26961476216881963 valid 0.23165684262029954
LOSS train 0.26961476216881963 valid 0.23177982760327204
LOSS train 0.26961476216881963 valid 0.23185002546189196
LOSS train 0.26961476216881963 valid 0.23180712719221372
LOSS train 0.26961476216881963 valid 0.2318446523814089
LOSS train 0.26961476216881963 valid 0.23182994527304732
LOSS train 0.26961476216881963 valid 0.23186816479450084
LOSS train 0.26961476216881963 valid 0.23185771842797598
LOSS train 0.26961476216881963 valid 0.231884911111819
LOSS train 0.26961476216881963 valid 0.2319070522654925
LOSS train 0.26961476216881963 valid 0.23197853550462438
LOSS train 0.26961476216881963 valid 0.23193529851146436
LOSS train 0.26961476216881963 valid 0.23182382871870136
LOSS train 0.26961476216881963 valid 0.23178243189076192
LOSS train 0.26961476216881963 valid 0.23170981689656597
LOSS train 0.26961476216881963 valid 0.2316252894506052
LOSS train 0.26961476216881963 valid 0.2316178064612509
LOSS train 0.26961476216881963 valid 0.231675435794938
LOSS train 0.26961476216881963 valid 0.2316361487486738
LOSS train 0.26961476216881963 valid 0.23162545106158808
LOSS train 0.26961476216881963 valid 0.2316857010316544
LOSS train 0.26961476216881963 valid 0.23170321358806767
LOSS train 0.26961476216881963 valid 0.23163972776087505
LOSS train 0.26961476216881963 valid 0.2315866318118723
LOSS train 0.26961476216881963 valid 0.2316399609628912
LOSS train 0.26961476216881963 valid 0.23169215576453778
LOSS train 0.26961476216881963 valid 0.231809623189107
LOSS train 0.26961476216881963 valid 0.23176142903976144
LOSS train 0.26961476216881963 valid 0.23186865973509732
LOSS train 0.26961476216881963 valid 0.2317450630090992
LOSS train 0.26961476216881963 valid 0.2316727665002132
LOSS train 0.26961476216881963 valid 0.23164947588502624
LOSS train 0.26961476216881963 valid 0.23165033345039074
LOSS train 0.26961476216881963 valid 0.2317447038424527
LOSS train 0.26961476216881963 valid 0.2317673470540149
LOSS train 0.26961476216881963 valid 0.2317789520886613
LOSS train 0.26961476216881963 valid 0.23183785125296166
LOSS train 0.26961476216881963 valid 0.23183812539685855
LOSS train 0.26961476216881963 valid 0.23175361824539853
LOSS train 0.26961476216881963 valid 0.23160437180335264
LOSS train 0.26961476216881963 valid 0.2316255666561671
LOSS train 0.26961476216881963 valid 0.2317658145716804
LOSS train 0.26961476216881963 valid 0.23169826335871396
LOSS train 0.26961476216881963 valid 0.2316316689497658
LOSS train 0.26961476216881963 valid 0.23156194808928832
LOSS train 0.26961476216881963 valid 0.23153103182654408
LOSS train 0.26961476216881963 valid 0.23150160179025656
LOSS train 0.26961476216881963 valid 0.23146606241078937
LOSS train 0.26961476216881963 valid 0.23135738738995493
LOSS train 0.26961476216881963 valid 0.23136534925267013
LOSS train 0.26961476216881963 valid 0.2313591323510551
LOSS train 0.26961476216881963 valid 0.23150470904832662
LOSS train 0.26961476216881963 valid 0.23155899747558262
LOSS train 0.26961476216881963 valid 0.23153323262413114
LOSS train 0.26961476216881963 valid 0.23140044609133037
LOSS train 0.26961476216881963 valid 0.23131585669243473
LOSS train 0.26961476216881963 valid 0.23137143604393334
LOSS train 0.26961476216881963 valid 0.23132072729723793
LOSS train 0.26961476216881963 valid 0.2312680617866353
LOSS train 0.26961476216881963 valid 0.23126429916274818
LOSS train 0.26961476216881963 valid 0.23130195868589384
LOSS train 0.26961476216881963 valid 0.23134589178413995
LOSS train 0.26961476216881963 valid 0.23144609541960165
LOSS train 0.26961476216881963 valid 0.2315115032905943
LOSS train 0.26961476216881963 valid 0.2315031513780439
LOSS train 0.26961476216881963 valid 0.2314795840885386
LOSS train 0.26961476216881963 valid 0.23150923503640636
LOSS train 0.26961476216881963 valid 0.2314518360628022
LOSS train 0.26961476216881963 valid 0.23143397094140092
LOSS train 0.26961476216881963 valid 0.23149255922486112
LOSS train 0.26961476216881963 valid 0.23138579311613866
LOSS train 0.26961476216881963 valid 0.231404509722859
LOSS train 0.26961476216881963 valid 0.23142213025321698
LOSS train 0.26961476216881963 valid 0.23134845755790753
LOSS train 0.26961476216881963 valid 0.23122926401507302
LOSS train 0.26961476216881963 valid 0.2312320434771802
LOSS train 0.26961476216881963 valid 0.23130688414502595
EPOCH 9:
  batch 1 loss: 0.2662624716758728
  batch 2 loss: 0.26023155450820923
  batch 3 loss: 0.26515984535217285
  batch 4 loss: 0.2697313502430916
  batch 5 loss: 0.2775736451148987
  batch 6 loss: 0.27409052352110547
  batch 7 loss: 0.2754137558596475
  batch 8 loss: 0.27691518515348434
  batch 9 loss: 0.2742244468794929
  batch 10 loss: 0.2734419882297516
  batch 11 loss: 0.2760513831268657
  batch 12 loss: 0.2732173589368661
  batch 13 loss: 0.2716350360558583
  batch 14 loss: 0.2710824576871736
  batch 15 loss: 0.27290219565232593
  batch 16 loss: 0.2716448316350579
  batch 17 loss: 0.26933516737292795
  batch 18 loss: 0.2696983226471477
  batch 19 loss: 0.2670367760093589
  batch 20 loss: 0.26492771282792094
  batch 21 loss: 0.2662251542011897
  batch 22 loss: 0.26514398577538406
  batch 23 loss: 0.2642349186150924
  batch 24 loss: 0.2622389731307824
  batch 25 loss: 0.264367573261261
  batch 26 loss: 0.262903284568053
  batch 27 loss: 0.2635359896553887
  batch 28 loss: 0.26238061115145683
  batch 29 loss: 0.262623088627026
  batch 30 loss: 0.26227205445369084
  batch 31 loss: 0.26244967839410227
  batch 32 loss: 0.2618786692619324
  batch 33 loss: 0.26188062266869977
  batch 34 loss: 0.2610502913594246
  batch 35 loss: 0.26086459543023793
  batch 36 loss: 0.26103966558972996
  batch 37 loss: 0.2612458664823223
  batch 38 loss: 0.2617828528347768
  batch 39 loss: 0.2613717378714146
  batch 40 loss: 0.2614674672484398
  batch 41 loss: 0.2617838840659072
  batch 42 loss: 0.2625406824407123
  batch 43 loss: 0.2632296237834664
  batch 44 loss: 0.2632403705607761
  batch 45 loss: 0.2629426340262095
  batch 46 loss: 0.2635626514320788
  batch 47 loss: 0.26363952426200216
  batch 48 loss: 0.2630471295366685
  batch 49 loss: 0.2628525258327017
  batch 50 loss: 0.26245829701423645
  batch 51 loss: 0.26236990386364506
  batch 52 loss: 0.26272257016255307
  batch 53 loss: 0.26225854259616926
  batch 54 loss: 0.2626468726882228
  batch 55 loss: 0.26246486631306737
  batch 56 loss: 0.262237085295575
  batch 57 loss: 0.26187582993716524
  batch 58 loss: 0.2631076304563161
  batch 59 loss: 0.26311768338841907
  batch 60 loss: 0.2629337452352047
  batch 61 loss: 0.2631657477773604
  batch 62 loss: 0.2640285138641634
  batch 63 loss: 0.26354803665289805
  batch 64 loss: 0.26420526136644185
  batch 65 loss: 0.2642851070715831
  batch 66 loss: 0.26423249303391483
  batch 67 loss: 0.2651055942275631
  batch 68 loss: 0.26545026701162844
  batch 69 loss: 0.26533220794753754
  batch 70 loss: 0.26551053928477425
  batch 71 loss: 0.26517955371191804
  batch 72 loss: 0.2654229189372725
  batch 73 loss: 0.2652705726149964
  batch 74 loss: 0.2654275739112416
  batch 75 loss: 0.2648761703570684
  batch 76 loss: 0.2653403346867938
  batch 77 loss: 0.2651052471105154
  batch 78 loss: 0.26523878673712414
  batch 79 loss: 0.26562350045276595
  batch 80 loss: 0.2652121542021632
  batch 81 loss: 0.2656283936014882
  batch 82 loss: 0.266085878196286
  batch 83 loss: 0.26618325081934413
  batch 84 loss: 0.2657932007596606
  batch 85 loss: 0.265499847426134
  batch 86 loss: 0.265910774122837
  batch 87 loss: 0.2659836031239608
  batch 88 loss: 0.26565739207647066
  batch 89 loss: 0.2653910874315862
  batch 90 loss: 0.2651779716213544
  batch 91 loss: 0.2650132262772256
  batch 92 loss: 0.2647100212457387
  batch 93 loss: 0.26449077119750364
  batch 94 loss: 0.2652864923819582
  batch 95 loss: 0.26485002401627994
  batch 96 loss: 0.2653925454554458
  batch 97 loss: 0.26640524185195413
  batch 98 loss: 0.2667676458249287
  batch 99 loss: 0.26700506231399496
  batch 100 loss: 0.26721020087599756
  batch 101 loss: 0.26766167167979893
  batch 102 loss: 0.2682587277947688
  batch 103 loss: 0.2687502649513263
  batch 104 loss: 0.26907082604101074
  batch 105 loss: 0.2692024527561097
  batch 106 loss: 0.26967325258367464
  batch 107 loss: 0.26959722746755477
  batch 108 loss: 0.2695375734733211
  batch 109 loss: 0.2694783816370395
  batch 110 loss: 0.26952208592133087
  batch 111 loss: 0.2692295943831538
  batch 112 loss: 0.2689789973997644
  batch 113 loss: 0.2692349475305692
  batch 114 loss: 0.269399036310221
  batch 115 loss: 0.2698723143857458
  batch 116 loss: 0.2701579708734463
  batch 117 loss: 0.2704312145455271
  batch 118 loss: 0.27013714422108764
  batch 119 loss: 0.27044025977619557
  batch 120 loss: 0.2702618611355623
  batch 121 loss: 0.27018475187711477
  batch 122 loss: 0.2701803291918802
  batch 123 loss: 0.2701117398777628
  batch 124 loss: 0.270480047070211
  batch 125 loss: 0.2705422875881195
  batch 126 loss: 0.2704643245254244
  batch 127 loss: 0.2708297001095269
  batch 128 loss: 0.2706579634686932
  batch 129 loss: 0.2707854907873065
  batch 130 loss: 0.27064340171905665
  batch 131 loss: 0.2706642165666318
  batch 132 loss: 0.27061484088048793
  batch 133 loss: 0.2709628436574362
  batch 134 loss: 0.27087278408345894
  batch 135 loss: 0.27060786452558305
  batch 136 loss: 0.2706924912464969
  batch 137 loss: 0.27091547313832887
  batch 138 loss: 0.2708992041323496
  batch 139 loss: 0.2713178592405731
  batch 140 loss: 0.2711159135614123
  batch 141 loss: 0.2714140459578088
  batch 142 loss: 0.27137148883980766
  batch 143 loss: 0.271169165943886
  batch 144 loss: 0.2712346999388602
  batch 145 loss: 0.2712030615272193
  batch 146 loss: 0.27110198436126315
  batch 147 loss: 0.2714103122754973
  batch 148 loss: 0.2713901600926309
  batch 149 loss: 0.2711571650017028
  batch 150 loss: 0.2711938123901685
  batch 151 loss: 0.27122460305690765
  batch 152 loss: 0.27146177827135515
  batch 153 loss: 0.27121786247281465
  batch 154 loss: 0.27135556287966767
  batch 155 loss: 0.2713401851154143
  batch 156 loss: 0.2712862884195951
  batch 157 loss: 0.2713753868629978
  batch 158 loss: 0.27146349837885625
  batch 159 loss: 0.27163281839973524
  batch 160 loss: 0.271480882819742
  batch 161 loss: 0.2715625547289108
  batch 162 loss: 0.2715709670274346
  batch 163 loss: 0.2717252877409473
  batch 164 loss: 0.2716395367027783
  batch 165 loss: 0.271575074394544
  batch 166 loss: 0.27150806219103824
  batch 167 loss: 0.27152515929973053
  batch 168 loss: 0.27159985731400194
  batch 169 loss: 0.27143244207258055
  batch 170 loss: 0.2713882723275353
  batch 171 loss: 0.27140502518380594
  batch 172 loss: 0.271445170044899
  batch 173 loss: 0.271444549794831
  batch 174 loss: 0.2713614755320823
  batch 175 loss: 0.27140352777072363
  batch 176 loss: 0.2712399151006883
  batch 177 loss: 0.27135813042605667
  batch 178 loss: 0.27142952307221596
  batch 179 loss: 0.27154486047822
  batch 180 loss: 0.2714195886419879
  batch 181 loss: 0.27146480417712615
  batch 182 loss: 0.2714214831754401
  batch 183 loss: 0.27122121243203273
  batch 184 loss: 0.2711952894764102
  batch 185 loss: 0.2712520799926809
  batch 186 loss: 0.2713681626864659
  batch 187 loss: 0.2712386767653858
  batch 188 loss: 0.2709594352289717
  batch 189 loss: 0.27066685622015957
  batch 190 loss: 0.2707863871204226
  batch 191 loss: 0.27084359003923325
  batch 192 loss: 0.2708955949638039
  batch 193 loss: 0.27088513437639244
  batch 194 loss: 0.271195622679499
  batch 195 loss: 0.2712170433539611
  batch 196 loss: 0.27104480762262734
  batch 197 loss: 0.27097528824951445
  batch 198 loss: 0.27131259561789156
  batch 199 loss: 0.2713552720882186
  batch 200 loss: 0.2714772017300129
  batch 201 loss: 0.271669492051376
  batch 202 loss: 0.27180236858306545
  batch 203 loss: 0.27187924402687935
  batch 204 loss: 0.2717595732971734
  batch 205 loss: 0.271864104997821
  batch 206 loss: 0.271914825566764
  batch 207 loss: 0.27194442351659137
  batch 208 loss: 0.2717128429705134
  batch 209 loss: 0.2713467473761317
  batch 210 loss: 0.2714739189971061
  batch 211 loss: 0.2715640975541978
  batch 212 loss: 0.2714325412404987
  batch 213 loss: 0.27132209582787725
  batch 214 loss: 0.2713894640034604
  batch 215 loss: 0.2711771286504213
  batch 216 loss: 0.2709271947818774
  batch 217 loss: 0.2708790907936712
  batch 218 loss: 0.27081524942993024
  batch 219 loss: 0.27088285240952825
  batch 220 loss: 0.27088976719162683
  batch 221 loss: 0.2709814787450419
  batch 222 loss: 0.2710631960952604
  batch 223 loss: 0.27101071586523356
  batch 224 loss: 0.27087938665811506
  batch 225 loss: 0.27072993026839365
  batch 226 loss: 0.27085687853066265
  batch 227 loss: 0.27067118699330067
  batch 228 loss: 0.2704445691615866
  batch 229 loss: 0.2704109869018913
  batch 230 loss: 0.2704012890872748
  batch 231 loss: 0.270285248175844
  batch 232 loss: 0.27010909756966706
  batch 233 loss: 0.2700828355397278
  batch 234 loss: 0.27003645266477877
  batch 235 loss: 0.26995732917430554
  batch 236 loss: 0.26985567726068577
  batch 237 loss: 0.26994436306540975
  batch 238 loss: 0.2698303720279902
  batch 239 loss: 0.26978514596009356
  batch 240 loss: 0.26986680887639525
  batch 241 loss: 0.269972852651509
  batch 242 loss: 0.26986406798944
  batch 243 loss: 0.2699978923724021
  batch 244 loss: 0.270113187918409
  batch 245 loss: 0.27004431674675067
  batch 246 loss: 0.270019409072593
  batch 247 loss: 0.27019890159489174
  batch 248 loss: 0.2703435627082663
  batch 249 loss: 0.27020294323982486
  batch 250 loss: 0.27014755976200105
  batch 251 loss: 0.2704167704420736
  batch 252 loss: 0.2702079395216609
  batch 253 loss: 0.2700863268535599
  batch 254 loss: 0.2699855625864089
  batch 255 loss: 0.2700691774779675
  batch 256 loss: 0.2701881971443072
  batch 257 loss: 0.2702415048612231
  batch 258 loss: 0.27019090107245036
  batch 259 loss: 0.2702321611546181
  batch 260 loss: 0.27030635739748293
  batch 261 loss: 0.270293682471089
  batch 262 loss: 0.27015883230981025
  batch 263 loss: 0.2702319037325029
  batch 264 loss: 0.27010593292388047
  batch 265 loss: 0.26990829773668973
  batch 266 loss: 0.26987110369635703
  batch 267 loss: 0.27003128541989274
  batch 268 loss: 0.2698644287328222
  batch 269 loss: 0.2697999051955553
  batch 270 loss: 0.26998894104251153
  batch 271 loss: 0.26996230177333874
  batch 272 loss: 0.26999237804728393
  batch 273 loss: 0.2699827907722948
  batch 274 loss: 0.27004890230885387
  batch 275 loss: 0.269986042434519
  batch 276 loss: 0.26993109631365625
  batch 277 loss: 0.2700119675927214
  batch 278 loss: 0.2699345552342401
  batch 279 loss: 0.2700077961315818
  batch 280 loss: 0.26996204028172155
  batch 281 loss: 0.26991204864401835
  batch 282 loss: 0.2698928735463332
  batch 283 loss: 0.26979710672432455
  batch 284 loss: 0.2698452334168931
  batch 285 loss: 0.26977201041422394
  batch 286 loss: 0.26985817928831063
  batch 287 loss: 0.26992029740835316
  batch 288 loss: 0.2697481215517554
  batch 289 loss: 0.27004834880672113
  batch 290 loss: 0.26982951097447294
  batch 291 loss: 0.26981926655646454
  batch 292 loss: 0.2699586294387301
  batch 293 loss: 0.26997981528169873
  batch 294 loss: 0.2699213247416782
  batch 295 loss: 0.2698980768353252
  batch 296 loss: 0.2699642193981925
  batch 297 loss: 0.2699749841533526
  batch 298 loss: 0.26999489448014524
  batch 299 loss: 0.26987107446959185
  batch 300 loss: 0.2698414056996504
  batch 301 loss: 0.2697763720719125
  batch 302 loss: 0.2697226900039919
  batch 303 loss: 0.2696135904234235
  batch 304 loss: 0.2695644862067543
  batch 305 loss: 0.26938505079902586
  batch 306 loss: 0.26949186511094275
  batch 307 loss: 0.269459725930947
  batch 308 loss: 0.2695479826009893
  batch 309 loss: 0.2695326333967999
  batch 310 loss: 0.26952549493120564
  batch 311 loss: 0.2696122718298167
  batch 312 loss: 0.26970254567762214
  batch 313 loss: 0.2696840494109419
  batch 314 loss: 0.26957891620458313
  batch 315 loss: 0.2695396231753486
  batch 316 loss: 0.26952426621242415
  batch 317 loss: 0.26942769224147317
  batch 318 loss: 0.26932970706209447
  batch 319 loss: 0.2692910110034913
  batch 320 loss: 0.2691568530164659
  batch 321 loss: 0.2692098229473625
  batch 322 loss: 0.26918490629018466
  batch 323 loss: 0.26920971643444924
  batch 324 loss: 0.26909178417222
  batch 325 loss: 0.2689710635863818
  batch 326 loss: 0.26896899412928915
  batch 327 loss: 0.2690718476105174
  batch 328 loss: 0.26888607460551145
  batch 329 loss: 0.2689501178482021
  batch 330 loss: 0.268872799837228
  batch 331 loss: 0.26890370871005104
  batch 332 loss: 0.26878042846738576
  batch 333 loss: 0.2687837080107079
  batch 334 loss: 0.26863318289111476
  batch 335 loss: 0.2684846506189944
  batch 336 loss: 0.26837638304347083
  batch 337 loss: 0.26830674022524575
  batch 338 loss: 0.26825212753383365
  batch 339 loss: 0.26819107463929504
  batch 340 loss: 0.2681434591026867
  batch 341 loss: 0.26809944181847783
  batch 342 loss: 0.2679883675617084
  batch 343 loss: 0.26796675188895913
  batch 344 loss: 0.26793100191063657
  batch 345 loss: 0.26798870356186577
  batch 346 loss: 0.267835881840976
  batch 347 loss: 0.2677329671743624
  batch 348 loss: 0.26767438799999227
  batch 349 loss: 0.26754750068003263
  batch 350 loss: 0.2675317416872297
  batch 351 loss: 0.26748342306865247
  batch 352 loss: 0.26755361775444314
  batch 353 loss: 0.26753605931744023
  batch 354 loss: 0.2676653879678855
  batch 355 loss: 0.26764628047674477
  batch 356 loss: 0.267648689783691
  batch 357 loss: 0.26754196605929476
  batch 358 loss: 0.26756068677043116
  batch 359 loss: 0.26748673692553154
  batch 360 loss: 0.2674751297053364
  batch 361 loss: 0.2674261383980595
  batch 362 loss: 0.26727487768422176
  batch 363 loss: 0.26716297885438955
  batch 364 loss: 0.26700591807673263
  batch 365 loss: 0.26695870773433006
  batch 366 loss: 0.2669229461032836
  batch 367 loss: 0.2668373935553943
  batch 368 loss: 0.2666648665200109
  batch 369 loss: 0.2665710662034792
  batch 370 loss: 0.2665002903422794
  batch 371 loss: 0.26655874788921796
  batch 372 loss: 0.26644618584904617
  batch 373 loss: 0.26637456428269596
  batch 374 loss: 0.26623184799669897
  batch 375 loss: 0.2661483124891917
  batch 376 loss: 0.26621152088046074
  batch 377 loss: 0.2661368664680172
  batch 378 loss: 0.2661152688993348
  batch 379 loss: 0.2661018719537906
  batch 380 loss: 0.2661606704326052
  batch 381 loss: 0.2661177693154868
  batch 382 loss: 0.26606366912107815
  batch 383 loss: 0.26608225804396146
  batch 384 loss: 0.26596531621180475
  batch 385 loss: 0.26602123797713934
  batch 386 loss: 0.2659598531423455
  batch 387 loss: 0.2660363758180185
  batch 388 loss: 0.2660854062852786
  batch 389 loss: 0.2660835617075849
  batch 390 loss: 0.26604490604920267
  batch 391 loss: 0.266044785947446
  batch 392 loss: 0.2659671212337455
  batch 393 loss: 0.26584568604561515
  batch 394 loss: 0.26580871392022537
  batch 395 loss: 0.2657391416875622
  batch 396 loss: 0.2657167304194335
  batch 397 loss: 0.26571700772650597
  batch 398 loss: 0.2656276469404374
  batch 399 loss: 0.26563387399628047
  batch 400 loss: 0.2656578250974417
  batch 401 loss: 0.26555478487377454
  batch 402 loss: 0.26556097960738995
  batch 403 loss: 0.26563618899722846
  batch 404 loss: 0.2656849410643082
  batch 405 loss: 0.2657157647388953
  batch 406 loss: 0.26578831470805436
  batch 407 loss: 0.2657488080368581
  batch 408 loss: 0.26580149662114827
  batch 409 loss: 0.2658492486678009
  batch 410 loss: 0.2659616424179659
  batch 411 loss: 0.2659735790206858
  batch 412 loss: 0.26594929740700907
  batch 413 loss: 0.26601555723399284
  batch 414 loss: 0.26597484561124285
  batch 415 loss: 0.2659049148660108
  batch 416 loss: 0.2659999320211892
  batch 417 loss: 0.2659427055852305
  batch 418 loss: 0.2659029129304384
  batch 419 loss: 0.26588031962549485
  batch 420 loss: 0.26584393680095675
  batch 421 loss: 0.2657198026740919
  batch 422 loss: 0.265793806427463
  batch 423 loss: 0.2657444278751423
  batch 424 loss: 0.2656750133577383
  batch 425 loss: 0.2656649537647472
  batch 426 loss: 0.2656273750212271
  batch 427 loss: 0.2656457320867713
  batch 428 loss: 0.2656295662310636
  batch 429 loss: 0.2656216799934983
  batch 430 loss: 0.265531216631102
  batch 431 loss: 0.26557691160183217
  batch 432 loss: 0.2656020132686805
  batch 433 loss: 0.26558453584515745
  batch 434 loss: 0.2656031670177587
  batch 435 loss: 0.2655206779296371
  batch 436 loss: 0.26550345509014
  batch 437 loss: 0.2656441946392474
  batch 438 loss: 0.26574504998040527
  batch 439 loss: 0.2656979809317882
  batch 440 loss: 0.26580306467684833
  batch 441 loss: 0.2657037318225891
  batch 442 loss: 0.26562193376581056
  batch 443 loss: 0.26563626416784375
  batch 444 loss: 0.2655836190390694
  batch 445 loss: 0.26560450594746665
  batch 446 loss: 0.2656230866708563
  batch 447 loss: 0.2655681156672087
  batch 448 loss: 0.26558799439642045
  batch 449 loss: 0.2655996178797465
  batch 450 loss: 0.2655963701340887
  batch 451 loss: 0.2656105661273267
  batch 452 loss: 0.26574001734894986
  batch 453 loss: 0.2657503157722503
  batch 454 loss: 0.26582093533571594
  batch 455 loss: 0.2658817700304828
  batch 456 loss: 0.26585195027291775
  batch 457 loss: 0.26581087421536187
  batch 458 loss: 0.26575115693968976
  batch 459 loss: 0.265836753421879
  batch 460 loss: 0.2658765954815823
  batch 461 loss: 0.2658515826611095
  batch 462 loss: 0.2658122261881312
  batch 463 loss: 0.2657299594024345
  batch 464 loss: 0.26570401828864526
  batch 465 loss: 0.26563102907391006
  batch 466 loss: 0.2655033591351284
  batch 467 loss: 0.2656217060456695
  batch 468 loss: 0.2656382492974273
  batch 469 loss: 0.26577143367927974
  batch 470 loss: 0.265751827714291
  batch 471 loss: 0.26581623587132513
  batch 472 loss: 0.2657224415343697
LOSS train 0.2657224415343697 valid 0.22858373820781708
LOSS train 0.2657224415343697 valid 0.22182869911193848
LOSS train 0.2657224415343697 valid 0.21782625218232474
LOSS train 0.2657224415343697 valid 0.21528976410627365
LOSS train 0.2657224415343697 valid 0.2109277367591858
LOSS train 0.2657224415343697 valid 0.2167661190032959
LOSS train 0.2657224415343697 valid 0.22772279807499476
LOSS train 0.2657224415343697 valid 0.22589505091309547
LOSS train 0.2657224415343697 valid 0.22469440599282584
LOSS train 0.2657224415343697 valid 0.22479506880044936
LOSS train 0.2657224415343697 valid 0.22345426814122635
LOSS train 0.2657224415343697 valid 0.2240130715072155
LOSS train 0.2657224415343697 valid 0.2220134139060974
LOSS train 0.2657224415343697 valid 0.22174395620822906
LOSS train 0.2657224415343697 valid 0.21763558785120646
LOSS train 0.2657224415343697 valid 0.21788909193128347
LOSS train 0.2657224415343697 valid 0.21875127536409042
LOSS train 0.2657224415343697 valid 0.21985850317610633
LOSS train 0.2657224415343697 valid 0.22120773007995204
LOSS train 0.2657224415343697 valid 0.22001597434282302
LOSS train 0.2657224415343697 valid 0.21923694156465076
LOSS train 0.2657224415343697 valid 0.2178451886231249
LOSS train 0.2657224415343697 valid 0.21854464904121731
LOSS train 0.2657224415343697 valid 0.2170760271449884
LOSS train 0.2657224415343697 valid 0.2163059788942337
LOSS train 0.2657224415343697 valid 0.21653082049809969
LOSS train 0.2657224415343697 valid 0.21657681741096355
LOSS train 0.2657224415343697 valid 0.21710464251892908
LOSS train 0.2657224415343697 valid 0.21695628402561978
LOSS train 0.2657224415343697 valid 0.21801011115312577
LOSS train 0.2657224415343697 valid 0.21877146055621485
LOSS train 0.2657224415343697 valid 0.21845478704199195
LOSS train 0.2657224415343697 valid 0.21935947013623786
LOSS train 0.2657224415343697 valid 0.21893465781913085
LOSS train 0.2657224415343697 valid 0.22008810469082424
LOSS train 0.2657224415343697 valid 0.21975533291697502
LOSS train 0.2657224415343697 valid 0.21959602309239878
LOSS train 0.2657224415343697 valid 0.22086645074580846
LOSS train 0.2657224415343697 valid 0.2208424027149494
LOSS train 0.2657224415343697 valid 0.22073164097964765
LOSS train 0.2657224415343697 valid 0.22152285350532067
LOSS train 0.2657224415343697 valid 0.2217666833173661
LOSS train 0.2657224415343697 valid 0.22140445577543835
LOSS train 0.2657224415343697 valid 0.22156018458984114
LOSS train 0.2657224415343697 valid 0.221165026889907
LOSS train 0.2657224415343697 valid 0.22152098235876663
LOSS train 0.2657224415343697 valid 0.22239541119717537
LOSS train 0.2657224415343697 valid 0.2225537414972981
LOSS train 0.2657224415343697 valid 0.22297281482998205
LOSS train 0.2657224415343697 valid 0.22210247963666915
LOSS train 0.2657224415343697 valid 0.22216830037388147
LOSS train 0.2657224415343697 valid 0.2216882679898005
LOSS train 0.2657224415343697 valid 0.22196370897428044
LOSS train 0.2657224415343697 valid 0.22210249911855767
LOSS train 0.2657224415343697 valid 0.2219602094455199
LOSS train 0.2657224415343697 valid 0.22134092157440527
LOSS train 0.2657224415343697 valid 0.22095503749554618
LOSS train 0.2657224415343697 valid 0.22061130841230525
LOSS train 0.2657224415343697 valid 0.22097362022278672
LOSS train 0.2657224415343697 valid 0.22088808740178745
LOSS train 0.2657224415343697 valid 0.22064716918546645
LOSS train 0.2657224415343697 valid 0.22135749579437317
LOSS train 0.2657224415343697 valid 0.22113265475583455
LOSS train 0.2657224415343697 valid 0.22189211775548756
LOSS train 0.2657224415343697 valid 0.2219800694630696
LOSS train 0.2657224415343697 valid 0.22176425858880533
LOSS train 0.2657224415343697 valid 0.22103080798440905
LOSS train 0.2657224415343697 valid 0.22087950938764742
LOSS train 0.2657224415343697 valid 0.2204103966554006
LOSS train 0.2657224415343697 valid 0.220415270115648
LOSS train 0.2657224415343697 valid 0.22016390528477414
LOSS train 0.2657224415343697 valid 0.2204517995317777
LOSS train 0.2657224415343697 valid 0.22037587427113153
LOSS train 0.2657224415343697 valid 0.2205284795648343
LOSS train 0.2657224415343697 valid 0.2206454433997472
LOSS train 0.2657224415343697 valid 0.22084883267157956
LOSS train 0.2657224415343697 valid 0.22094604914838617
LOSS train 0.2657224415343697 valid 0.22133076993318704
LOSS train 0.2657224415343697 valid 0.22133910882322094
LOSS train 0.2657224415343697 valid 0.22070157527923584
LOSS train 0.2657224415343697 valid 0.2198618221798061
LOSS train 0.2657224415343697 valid 0.22029907256364822
LOSS train 0.2657224415343697 valid 0.22013062669570188
LOSS train 0.2657224415343697 valid 0.22004973604565575
LOSS train 0.2657224415343697 valid 0.21987342361141654
LOSS train 0.2657224415343697 valid 0.21942812914765158
LOSS train 0.2657224415343697 valid 0.21915700329446244
LOSS train 0.2657224415343697 valid 0.2188119791786779
LOSS train 0.2657224415343697 valid 0.21936223396424498
LOSS train 0.2657224415343697 valid 0.21938605391316943
LOSS train 0.2657224415343697 valid 0.21940660640433596
LOSS train 0.2657224415343697 valid 0.21970480032589124
LOSS train 0.2657224415343697 valid 0.21959773795579068
LOSS train 0.2657224415343697 valid 0.2196754662597433
LOSS train 0.2657224415343697 valid 0.21962721818371822
LOSS train 0.2657224415343697 valid 0.21986204540977874
LOSS train 0.2657224415343697 valid 0.2198345312445434
LOSS train 0.2657224415343697 valid 0.22003250888415746
LOSS train 0.2657224415343697 valid 0.22002773348129157
LOSS train 0.2657224415343697 valid 0.2201492840051651
LOSS train 0.2657224415343697 valid 0.2204858406935588
LOSS train 0.2657224415343697 valid 0.2205370089002684
LOSS train 0.2657224415343697 valid 0.22032520536658834
LOSS train 0.2657224415343697 valid 0.22037159479581392
LOSS train 0.2657224415343697 valid 0.2205416466508593
LOSS train 0.2657224415343697 valid 0.2206871529232781
LOSS train 0.2657224415343697 valid 0.22059338323980848
LOSS train 0.2657224415343697 valid 0.22064597932276903
LOSS train 0.2657224415343697 valid 0.22100342714458432
LOSS train 0.2657224415343697 valid 0.22120776826685126
LOSS train 0.2657224415343697 valid 0.22081845555756544
LOSS train 0.2657224415343697 valid 0.22075603955558368
LOSS train 0.2657224415343697 valid 0.22079556995788507
LOSS train 0.2657224415343697 valid 0.22072177419536992
LOSS train 0.2657224415343697 valid 0.22070227537466133
LOSS train 0.2657224415343697 valid 0.22067926168955607
LOSS train 0.2657224415343697 valid 0.22081867509927505
LOSS train 0.2657224415343697 valid 0.22072155937805013
LOSS train 0.2657224415343697 valid 0.22067903483114323
LOSS train 0.2657224415343697 valid 0.22047345514098804
LOSS train 0.2657224415343697 valid 0.22035940447129493
LOSS train 0.2657224415343697 valid 0.2202370380524729
LOSS train 0.2657224415343697 valid 0.22007661912499404
LOSS train 0.2657224415343697 valid 0.22036053696947713
LOSS train 0.2657224415343697 valid 0.22025644612312317
LOSS train 0.2657224415343697 valid 0.22059108411508893
LOSS train 0.2657224415343697 valid 0.22052508029412096
LOSS train 0.2657224415343697 valid 0.22087222803384066
LOSS train 0.2657224415343697 valid 0.22091400877449863
LOSS train 0.2657224415343697 valid 0.2207487144149267
LOSS train 0.2657224415343697 valid 0.22078788701359553
LOSS train 0.2657224415343697 valid 0.22060359551599532
LOSS train 0.2657224415343697 valid 0.2206161767244339
LOSS train 0.2657224415343697 valid 0.2207123584934135
LOSS train 0.2657224415343697 valid 0.22055509244954144
LOSS train 0.2657224415343697 valid 0.2205242737032035
LOSS train 0.2657224415343697 valid 0.22033482659472167
LOSS train 0.2657224415343697 valid 0.22033792183451031
LOSS train 0.2657224415343697 valid 0.22024187520682384
LOSS train 0.2657224415343697 valid 0.22032152020505497
LOSS train 0.2657224415343697 valid 0.2204169063492024
LOSS train 0.2657224415343697 valid 0.22068406966790347
LOSS train 0.2657224415343697 valid 0.2206570642603027
LOSS train 0.2657224415343697 valid 0.22068278087923923
LOSS train 0.2657224415343697 valid 0.22046118060062672
LOSS train 0.2657224415343697 valid 0.22064100093629263
LOSS train 0.2657224415343697 valid 0.22048350925348242
LOSS train 0.2657224415343697 valid 0.2210941540228354
LOSS train 0.2657224415343697 valid 0.22116947684112012
LOSS train 0.2657224415343697 valid 0.22122413386901219
LOSS train 0.2657224415343697 valid 0.2213067120274171
LOSS train 0.2657224415343697 valid 0.22105768156286917
LOSS train 0.2657224415343697 valid 0.2211645217308032
LOSS train 0.2657224415343697 valid 0.2209717674882381
LOSS train 0.2657224415343697 valid 0.22101136647885847
LOSS train 0.2657224415343697 valid 0.22112744043652827
LOSS train 0.2657224415343697 valid 0.22101100159298842
LOSS train 0.2657224415343697 valid 0.2210743066064919
LOSS train 0.2657224415343697 valid 0.22112582848881776
LOSS train 0.2657224415343697 valid 0.221083730366081
LOSS train 0.2657224415343697 valid 0.2210022631269064
LOSS train 0.2657224415343697 valid 0.220887808152187
LOSS train 0.2657224415343697 valid 0.2207579788254814
LOSS train 0.2657224415343697 valid 0.22057584364239763
LOSS train 0.2657224415343697 valid 0.22042462961240247
LOSS train 0.2657224415343697 valid 0.22053236245031815
LOSS train 0.2657224415343697 valid 0.22090688196128
LOSS train 0.2657224415343697 valid 0.2207416899147488
LOSS train 0.2657224415343697 valid 0.220936431510914
LOSS train 0.2657224415343697 valid 0.2210370684371275
LOSS train 0.2657224415343697 valid 0.2210071409306331
LOSS train 0.2657224415343697 valid 0.2209564629335736
LOSS train 0.2657224415343697 valid 0.22098812280018207
LOSS train 0.2657224415343697 valid 0.22099808731983447
LOSS train 0.2657224415343697 valid 0.22077935576438903
LOSS train 0.2657224415343697 valid 0.22078028228133917
LOSS train 0.2657224415343697 valid 0.22078933478411983
LOSS train 0.2657224415343697 valid 0.22099404062113065
LOSS train 0.2657224415343697 valid 0.22093913034020857
LOSS train 0.2657224415343697 valid 0.2208864313032892
LOSS train 0.2657224415343697 valid 0.22096725392736782
LOSS train 0.2657224415343697 valid 0.22097857558465267
LOSS train 0.2657224415343697 valid 0.22104851681678023
LOSS train 0.2657224415343697 valid 0.22103477212721886
LOSS train 0.2657224415343697 valid 0.22080885373257303
LOSS train 0.2657224415343697 valid 0.2208092130159819
LOSS train 0.2657224415343697 valid 0.22068229548752627
LOSS train 0.2657224415343697 valid 0.220654160101363
LOSS train 0.2657224415343697 valid 0.22059202351898113
LOSS train 0.2657224415343697 valid 0.220701181261163
LOSS train 0.2657224415343697 valid 0.22063710645855408
LOSS train 0.2657224415343697 valid 0.22071377405275902
LOSS train 0.2657224415343697 valid 0.22057937734176458
LOSS train 0.2657224415343697 valid 0.2204649952269092
LOSS train 0.2657224415343697 valid 0.22034189701080323
LOSS train 0.2657224415343697 valid 0.22032127970335436
LOSS train 0.2657224415343697 valid 0.22052551707640516
LOSS train 0.2657224415343697 valid 0.2204032161771649
LOSS train 0.2657224415343697 valid 0.22046042087689116
LOSS train 0.2657224415343697 valid 0.22029684834182262
LOSS train 0.2657224415343697 valid 0.22014760681942327
LOSS train 0.2657224415343697 valid 0.220098548448912
LOSS train 0.2657224415343697 valid 0.22000048709620396
LOSS train 0.2657224415343697 valid 0.21998062678704075
LOSS train 0.2657224415343697 valid 0.2197833646361421
LOSS train 0.2657224415343697 valid 0.21981329709580802
LOSS train 0.2657224415343697 valid 0.2197395288137998
LOSS train 0.2657224415343697 valid 0.21953968245249528
LOSS train 0.2657224415343697 valid 0.2194011485177364
LOSS train 0.2657224415343697 valid 0.2194451895498094
LOSS train 0.2657224415343697 valid 0.21961768020950787
LOSS train 0.2657224415343697 valid 0.21963423792764825
LOSS train 0.2657224415343697 valid 0.219689440769209
LOSS train 0.2657224415343697 valid 0.21965788562442654
LOSS train 0.2657224415343697 valid 0.21953037103941275
LOSS train 0.2657224415343697 valid 0.21952127051298265
LOSS train 0.2657224415343697 valid 0.21953853490989877
LOSS train 0.2657224415343697 valid 0.21964424048815298
LOSS train 0.2657224415343697 valid 0.21973631064913587
LOSS train 0.2657224415343697 valid 0.21979457356713034
LOSS train 0.2657224415343697 valid 0.2198007056885715
LOSS train 0.2657224415343697 valid 0.21975608935227264
LOSS train 0.2657224415343697 valid 0.2198372009490103
LOSS train 0.2657224415343697 valid 0.21990390188459838
LOSS train 0.2657224415343697 valid 0.21989605221483444
LOSS train 0.2657224415343697 valid 0.21998064041928908
LOSS train 0.2657224415343697 valid 0.22020893481596976
LOSS train 0.2657224415343697 valid 0.22034262022689768
LOSS train 0.2657224415343697 valid 0.22036526101645423
LOSS train 0.2657224415343697 valid 0.22043031480001365
LOSS train 0.2657224415343697 valid 0.2204302397125211
LOSS train 0.2657224415343697 valid 0.2205151909769609
LOSS train 0.2657224415343697 valid 0.22056779994473436
LOSS train 0.2657224415343697 valid 0.22057094437699032
LOSS train 0.2657224415343697 valid 0.22067176889865955
LOSS train 0.2657224415343697 valid 0.22056909707390657
LOSS train 0.2657224415343697 valid 0.22067925283677467
LOSS train 0.2657224415343697 valid 0.22070850006171636
LOSS train 0.2657224415343697 valid 0.22058858326538835
LOSS train 0.2657224415343697 valid 0.2204916736111045
LOSS train 0.2657224415343697 valid 0.2205576132441952
LOSS train 0.2657224415343697 valid 0.22035313840986284
LOSS train 0.2657224415343697 valid 0.22044115185492325
LOSS train 0.2657224415343697 valid 0.22069262731514994
LOSS train 0.2657224415343697 valid 0.22087938791635084
LOSS train 0.2657224415343697 valid 0.22085341605229106
LOSS train 0.2657224415343697 valid 0.22094527514357315
LOSS train 0.2657224415343697 valid 0.2208953972665533
LOSS train 0.2657224415343697 valid 0.2208536056031185
LOSS train 0.2657224415343697 valid 0.2210286015868187
LOSS train 0.2657224415343697 valid 0.22100271753580922
LOSS train 0.2657224415343697 valid 0.22115338806595122
LOSS train 0.2657224415343697 valid 0.2210898534818129
LOSS train 0.2657224415343697 valid 0.22101442357451898
LOSS train 0.2657224415343697 valid 0.2210689771409128
LOSS train 0.2657224415343697 valid 0.22112352668773383
LOSS train 0.2657224415343697 valid 0.22102013717365637
LOSS train 0.2657224415343697 valid 0.2210940951409266
LOSS train 0.2657224415343697 valid 0.22108712142268663
LOSS train 0.2657224415343697 valid 0.22106402052136567
LOSS train 0.2657224415343697 valid 0.2211672296583424
LOSS train 0.2657224415343697 valid 0.22118693089667169
LOSS train 0.2657224415343697 valid 0.2211719846544157
LOSS train 0.2657224415343697 valid 0.22119543633677743
LOSS train 0.2657224415343697 valid 0.22113125335495426
LOSS train 0.2657224415343697 valid 0.221232765599301
LOSS train 0.2657224415343697 valid 0.2212493284588003
LOSS train 0.2657224415343697 valid 0.22138443945059136
LOSS train 0.2657224415343697 valid 0.22145001407449574
LOSS train 0.2657224415343697 valid 0.22144096008053532
LOSS train 0.2657224415343697 valid 0.2214996506807109
LOSS train 0.2657224415343697 valid 0.22170473142143557
LOSS train 0.2657224415343697 valid 0.22174468980385706
LOSS train 0.2657224415343697 valid 0.2216702455477993
LOSS train 0.2657224415343697 valid 0.22163921881805768
LOSS train 0.2657224415343697 valid 0.22150161230693693
LOSS train 0.2657224415343697 valid 0.22135283719976886
LOSS train 0.2657224415343697 valid 0.2212304962303141
LOSS train 0.2657224415343697 valid 0.22122822173180118
LOSS train 0.2657224415343697 valid 0.22119189213429177
LOSS train 0.2657224415343697 valid 0.2210137837722208
LOSS train 0.2657224415343697 valid 0.22084493750164694
LOSS train 0.2657224415343697 valid 0.2208055234629358
LOSS train 0.2657224415343697 valid 0.22082211811777572
LOSS train 0.2657224415343697 valid 0.22084861575511464
LOSS train 0.2657224415343697 valid 0.22079250940701345
LOSS train 0.2657224415343697 valid 0.2207376909692113
LOSS train 0.2657224415343697 valid 0.22070019412785769
LOSS train 0.2657224415343697 valid 0.2207413469425122
LOSS train 0.2657224415343697 valid 0.22074513990303565
LOSS train 0.2657224415343697 valid 0.2206652630440558
LOSS train 0.2657224415343697 valid 0.22067725918676756
LOSS train 0.2657224415343697 valid 0.22068892805852988
LOSS train 0.2657224415343697 valid 0.22080903246897418
LOSS train 0.2657224415343697 valid 0.22087333030619863
LOSS train 0.2657224415343697 valid 0.22082797719820127
LOSS train 0.2657224415343697 valid 0.22087623866318853
LOSS train 0.2657224415343697 valid 0.22086326427907751
LOSS train 0.2657224415343697 valid 0.22091215809053402
LOSS train 0.2657224415343697 valid 0.22088231901327768
LOSS train 0.2657224415343697 valid 0.220900908310548
LOSS train 0.2657224415343697 valid 0.22090953326975274
LOSS train 0.2657224415343697 valid 0.22096110865621282
LOSS train 0.2657224415343697 valid 0.22093489780826003
LOSS train 0.2657224415343697 valid 0.2208085242353502
LOSS train 0.2657224415343697 valid 0.2207651205020013
LOSS train 0.2657224415343697 valid 0.22066947031487083
LOSS train 0.2657224415343697 valid 0.22059407443195195
LOSS train 0.2657224415343697 valid 0.22058978474255905
LOSS train 0.2657224415343697 valid 0.22065151502047817
LOSS train 0.2657224415343697 valid 0.22061015799689523
LOSS train 0.2657224415343697 valid 0.22059746387486273
LOSS train 0.2657224415343697 valid 0.22064538423816998
LOSS train 0.2657224415343697 valid 0.22064487935061666
LOSS train 0.2657224415343697 valid 0.22060887950753408
LOSS train 0.2657224415343697 valid 0.22054210451396206
LOSS train 0.2657224415343697 valid 0.2205713660653081
LOSS train 0.2657224415343697 valid 0.220600666677427
LOSS train 0.2657224415343697 valid 0.2207315025097898
LOSS train 0.2657224415343697 valid 0.22068306747823954
LOSS train 0.2657224415343697 valid 0.22081472884828798
LOSS train 0.2657224415343697 valid 0.2206926406447932
LOSS train 0.2657224415343697 valid 0.220624026683831
LOSS train 0.2657224415343697 valid 0.22060571178609942
LOSS train 0.2657224415343697 valid 0.22061370987158554
LOSS train 0.2657224415343697 valid 0.22070892182595891
LOSS train 0.2657224415343697 valid 0.2207134119505547
LOSS train 0.2657224415343697 valid 0.22073256138076142
LOSS train 0.2657224415343697 valid 0.22077561792631642
LOSS train 0.2657224415343697 valid 0.2207872913642363
LOSS train 0.2657224415343697 valid 0.220703203467444
LOSS train 0.2657224415343697 valid 0.22057356187198535
LOSS train 0.2657224415343697 valid 0.22059869068162935
LOSS train 0.2657224415343697 valid 0.2207089037952309
LOSS train 0.2657224415343697 valid 0.2206412876719859
LOSS train 0.2657224415343697 valid 0.22057402612907545
LOSS train 0.2657224415343697 valid 0.22051403151952195
LOSS train 0.2657224415343697 valid 0.22046106961175535
LOSS train 0.2657224415343697 valid 0.22041683211087476
LOSS train 0.2657224415343697 valid 0.22041048609158573
LOSS train 0.2657224415343697 valid 0.22029986400758067
LOSS train 0.2657224415343697 valid 0.2203145916437545
LOSS train 0.2657224415343697 valid 0.2202939954165467
LOSS train 0.2657224415343697 valid 0.22040142066950022
LOSS train 0.2657224415343697 valid 0.22042548436185588
LOSS train 0.2657224415343697 valid 0.2203949180813883
LOSS train 0.2657224415343697 valid 0.22027430779995766
LOSS train 0.2657224415343697 valid 0.2201879701052589
LOSS train 0.2657224415343697 valid 0.2202395280145301
LOSS train 0.2657224415343697 valid 0.22018318631819317
LOSS train 0.2657224415343697 valid 0.22011539666910795
LOSS train 0.2657224415343697 valid 0.22008458571508527
LOSS train 0.2657224415343697 valid 0.22011894219826705
LOSS train 0.2657224415343697 valid 0.22014912626164107
LOSS train 0.2657224415343697 valid 0.22026302000166664
LOSS train 0.2657224415343697 valid 0.22035214999753439
LOSS train 0.2657224415343697 valid 0.22034155825773874
LOSS train 0.2657224415343697 valid 0.2203051254319745
LOSS train 0.2657224415343697 valid 0.2203574432968097
LOSS train 0.2657224415343697 valid 0.22029701434075832
LOSS train 0.2657224415343697 valid 0.22029234489575647
LOSS train 0.2657224415343697 valid 0.2203612388497558
LOSS train 0.2657224415343697 valid 0.22026672175272108
LOSS train 0.2657224415343697 valid 0.220298242356096
LOSS train 0.2657224415343697 valid 0.22030969776519357
LOSS train 0.2657224415343697 valid 0.22026718595151693
LOSS train 0.2657224415343697 valid 0.2201488738732377
LOSS train 0.2657224415343697 valid 0.2201386207876646
LOSS train 0.2657224415343697 valid 0.2201977813954599
EPOCH 10:
  batch 1 loss: 0.25842714309692383
  batch 2 loss: 0.2586337924003601
  batch 3 loss: 0.26192328333854675
  batch 4 loss: 0.2691480740904808
  batch 5 loss: 0.27276533246040346
  batch 6 loss: 0.26832765837510425
  batch 7 loss: 0.2722740003040859
  batch 8 loss: 0.27194944024086
  batch 9 loss: 0.269308477640152
  batch 10 loss: 0.2668470829725266
  batch 11 loss: 0.26904441009868274
  batch 12 loss: 0.2664160691201687
  batch 13 loss: 0.2652636502797787
  batch 14 loss: 0.2657254221183913
  batch 15 loss: 0.2669332613547643
  batch 16 loss: 0.26563301403075457
  batch 17 loss: 0.26329824766691995
  batch 18 loss: 0.2640181771583027
  batch 19 loss: 0.2627508208939904
  batch 20 loss: 0.26041375771164893
  batch 21 loss: 0.26130850329285576
  batch 22 loss: 0.2603799111463807
  batch 23 loss: 0.2593953875095948
  batch 24 loss: 0.25748952229817706
  batch 25 loss: 0.2603023028373718
  batch 26 loss: 0.2585084644647745
  batch 27 loss: 0.25880327931156866
  batch 28 loss: 0.2574459862496172
  batch 29 loss: 0.2579275292569193
  batch 30 loss: 0.2572443976998329
  batch 31 loss: 0.25769408431745344
  batch 32 loss: 0.25797166815027595
  batch 33 loss: 0.2580566717819734
  batch 34 loss: 0.25716999611433816
  batch 35 loss: 0.2572093520845686
  batch 36 loss: 0.2577031801144282
  batch 37 loss: 0.2575718301373559
  batch 38 loss: 0.25807394087314606
  batch 39 loss: 0.25812050241690415
  batch 40 loss: 0.2583432525396347
  batch 41 loss: 0.258960815464578
  batch 42 loss: 0.2596809431200936
  batch 43 loss: 0.2600965118685434
  batch 44 loss: 0.26042439382184634
  batch 45 loss: 0.2601874980661604
  batch 46 loss: 0.260686250484508
  batch 47 loss: 0.26059847245825096
  batch 48 loss: 0.26006484031677246
  batch 49 loss: 0.25985997002951955
  batch 50 loss: 0.2594911199808121
  batch 51 loss: 0.2594238665758395
  batch 52 loss: 0.26002752494353515
  batch 53 loss: 0.259439801832415
  batch 54 loss: 0.2595880588999501
  batch 55 loss: 0.2596082627773285
  batch 56 loss: 0.25966982543468475
  batch 57 loss: 0.2593246060505248
  batch 58 loss: 0.2602813649794151
  batch 59 loss: 0.260739215349747
  batch 60 loss: 0.2603727412720521
  batch 61 loss: 0.26055986837285466
  batch 62 loss: 0.26136874311393304
  batch 63 loss: 0.2608377919310615
  batch 64 loss: 0.261495559476316
  batch 65 loss: 0.26144536045881417
  batch 66 loss: 0.26127825316154596
  batch 67 loss: 0.2619997894586022
  batch 68 loss: 0.26208264687482047
  batch 69 loss: 0.2618889972783517
  batch 70 loss: 0.26207698370729177
  batch 71 loss: 0.26169137236937673
  batch 72 loss: 0.2620257207502921
  batch 73 loss: 0.2618826597112499
  batch 74 loss: 0.26186064872387294
  batch 75 loss: 0.26152065833409627
  batch 76 loss: 0.26172525161191035
  batch 77 loss: 0.2612923372101474
  batch 78 loss: 0.2620915124813716
  batch 79 loss: 0.26247947729086574
  batch 80 loss: 0.26228162832558155
  batch 81 loss: 0.26256064721095707
  batch 82 loss: 0.26312549848382066
  batch 83 loss: 0.263275080416576
  batch 84 loss: 0.26274410138527554
  batch 85 loss: 0.26241771280765536
  batch 86 loss: 0.26287437195694724
  batch 87 loss: 0.2629280122874797
  batch 88 loss: 0.2626572974364866
  batch 89 loss: 0.2622819100872854
  batch 90 loss: 0.2621452775266435
  batch 91 loss: 0.26215775523866924
  batch 92 loss: 0.2618185416187929
  batch 93 loss: 0.2616850176165181
  batch 94 loss: 0.2621636324106379
  batch 95 loss: 0.2616300702095032
  batch 96 loss: 0.2619451293721795
  batch 97 loss: 0.26254775782221373
  batch 98 loss: 0.26309250933783396
  batch 99 loss: 0.26325853635566404
  batch 100 loss: 0.2631004151701927
  batch 101 loss: 0.2634427588764984
  batch 102 loss: 0.26408229417660656
  batch 103 loss: 0.2646313442767245
  batch 104 loss: 0.2648498478990335
  batch 105 loss: 0.26483604056494575
  batch 106 loss: 0.26520241881316564
  batch 107 loss: 0.2650581117823859
  batch 108 loss: 0.26499094165585657
  batch 109 loss: 0.2649467673596986
  batch 110 loss: 0.26489871049469166
  batch 111 loss: 0.2646525554560326
  batch 112 loss: 0.26422025609229294
  batch 113 loss: 0.2641516093131715
  batch 114 loss: 0.26432718310439796
  batch 115 loss: 0.2645981228869894
  batch 116 loss: 0.26487234877101307
  batch 117 loss: 0.2652867224990812
  batch 118 loss: 0.2649793527641539
  batch 119 loss: 0.265460180509992
  batch 120 loss: 0.2650745872408152
  batch 121 loss: 0.2647289628824912
  batch 122 loss: 0.2646000519639156
  batch 123 loss: 0.26440733235056807
  batch 124 loss: 0.2646583474932178
  batch 125 loss: 0.2647555487155914
  batch 126 loss: 0.2647871405832351
  batch 127 loss: 0.26528427990402764
  batch 128 loss: 0.26502221974078566
  batch 129 loss: 0.2650579118220381
  batch 130 loss: 0.26500514298677447
  batch 131 loss: 0.2650829857314816
  batch 132 loss: 0.2648690210824663
  batch 133 loss: 0.2651460604336029
  batch 134 loss: 0.2651609697226268
  batch 135 loss: 0.26473846700456405
  batch 136 loss: 0.26462853173999223
  batch 137 loss: 0.2647240612193616
  batch 138 loss: 0.2648608071216639
  batch 139 loss: 0.26528521836232793
  batch 140 loss: 0.2650530495813915
  batch 141 loss: 0.26539416727444803
  batch 142 loss: 0.26539676730901424
  batch 143 loss: 0.26518196992940835
  batch 144 loss: 0.2652020365413692
  batch 145 loss: 0.2651648755731254
  batch 146 loss: 0.26515453176139153
  batch 147 loss: 0.2654107586056197
  batch 148 loss: 0.2654428085362589
  batch 149 loss: 0.26516142737545423
  batch 150 loss: 0.2652030683557193
  batch 151 loss: 0.26517370550443004
  batch 152 loss: 0.2653452758922389
  batch 153 loss: 0.26513367881572325
  batch 154 loss: 0.26523287120190536
  batch 155 loss: 0.2651449921631044
  batch 156 loss: 0.2650342486225642
  batch 157 loss: 0.2651062702677052
  batch 158 loss: 0.26518027786212633
  batch 159 loss: 0.2651784065759407
  batch 160 loss: 0.26495647635310887
  batch 161 loss: 0.2649721271873261
  batch 162 loss: 0.2648540523685055
  batch 163 loss: 0.2649182785142419
  batch 164 loss: 0.26478036420374385
  batch 165 loss: 0.2646347594080549
  batch 166 loss: 0.2645572013704173
  batch 167 loss: 0.26456576880223737
  batch 168 loss: 0.2646194463152261
  batch 169 loss: 0.26442025443153266
  batch 170 loss: 0.2643807110541007
  batch 171 loss: 0.26437085666503124
  batch 172 loss: 0.2644342515877513
  batch 173 loss: 0.2644367733959518
  batch 174 loss: 0.2642670961460848
  batch 175 loss: 0.26429872640541624
  batch 176 loss: 0.2641253122551875
  batch 177 loss: 0.2641422411816268
  batch 178 loss: 0.2642769105313869
  batch 179 loss: 0.26439088449797815
  batch 180 loss: 0.26421433074606787
  batch 181 loss: 0.2642794154296264
  batch 182 loss: 0.2641982042363712
  batch 183 loss: 0.26397000375341195
  batch 184 loss: 0.26392792003310245
  batch 185 loss: 0.2639016132096986
  batch 186 loss: 0.26395898576705684
  batch 187 loss: 0.2638472663686875
  batch 188 loss: 0.26357400187469543
  batch 189 loss: 0.26328981860920236
  batch 190 loss: 0.2633054564657964
  batch 191 loss: 0.2632317272312354
  batch 192 loss: 0.2632885321509093
  batch 193 loss: 0.2633191676800733
  batch 194 loss: 0.26357324705603197
  batch 195 loss: 0.2636326602636239
  batch 196 loss: 0.26340319102211873
  batch 197 loss: 0.2633578098818735
  batch 198 loss: 0.26368541842458226
  batch 199 loss: 0.263698700010477
  batch 200 loss: 0.26384130112826826
  batch 201 loss: 0.264123345711338
  batch 202 loss: 0.26425497093708206
  batch 203 loss: 0.26422865556672287
  batch 204 loss: 0.26417407955901295
  batch 205 loss: 0.26429326308936607
  batch 206 loss: 0.26429742176845233
  batch 207 loss: 0.2643464448803289
  batch 208 loss: 0.2641364980775576
  batch 209 loss: 0.2638282515643316
  batch 210 loss: 0.26387531452235724
  batch 211 loss: 0.26382997271856423
  batch 212 loss: 0.2636918178144491
  batch 213 loss: 0.2635867566989621
  batch 214 loss: 0.26363931199379054
  batch 215 loss: 0.26343717443388565
  batch 216 loss: 0.2631894510791258
  batch 217 loss: 0.2631380816895841
  batch 218 loss: 0.26306905412892684
  batch 219 loss: 0.26319123294255503
  batch 220 loss: 0.2631759789856997
  batch 221 loss: 0.2632253234742454
  batch 222 loss: 0.26328702069617604
  batch 223 loss: 0.2632446497545114
  batch 224 loss: 0.2631902581612979
  batch 225 loss: 0.2630574169423845
  batch 226 loss: 0.2631106741924201
  batch 227 loss: 0.2629185454173235
  batch 228 loss: 0.2627129445604065
  batch 229 loss: 0.26267221927903106
  batch 230 loss: 0.2627190523173498
  batch 231 loss: 0.26257598058227855
  batch 232 loss: 0.26243087764957856
  batch 233 loss: 0.2623481091767422
  batch 234 loss: 0.26229021991165274
  batch 235 loss: 0.2621687610732748
  batch 236 loss: 0.26208279635441506
  batch 237 loss: 0.2622040693769978
  batch 238 loss: 0.2621537232849778
  batch 239 loss: 0.2620689896360102
  batch 240 loss: 0.26204235342641674
  batch 241 loss: 0.2621124611850596
  batch 242 loss: 0.2619790990860009
  batch 243 loss: 0.26204520286110694
  batch 244 loss: 0.26207244170249483
  batch 245 loss: 0.2619571279506294
  batch 246 loss: 0.26195980959791476
  batch 247 loss: 0.2621460108139254
  batch 248 loss: 0.26211119563348834
  batch 249 loss: 0.2619770742563837
  batch 250 loss: 0.2619226719141006
  batch 251 loss: 0.2621424859500976
  batch 252 loss: 0.26192607387663824
  batch 253 loss: 0.26184473107219214
  batch 254 loss: 0.2617221194810755
  batch 255 loss: 0.26178809199847425
  batch 256 loss: 0.2618709246744402
  batch 257 loss: 0.26191542926697414
  batch 258 loss: 0.2618158127563868
  batch 259 loss: 0.2618175058636426
  batch 260 loss: 0.26182990710322673
  batch 261 loss: 0.2617871697949267
  batch 262 loss: 0.26168382821874764
  batch 263 loss: 0.2616817993696198
  batch 264 loss: 0.26157870956442575
  batch 265 loss: 0.2614089104364503
  batch 266 loss: 0.2613571114782104
  batch 267 loss: 0.26148440909296383
  batch 268 loss: 0.2613345272283056
  batch 269 loss: 0.2613004519150603
  batch 270 loss: 0.26143054575831803
  batch 271 loss: 0.2614474336159625
  batch 272 loss: 0.26147550596472097
  batch 273 loss: 0.2614875275355119
  batch 274 loss: 0.26152824213469983
  batch 275 loss: 0.26146950402043084
  batch 276 loss: 0.26145262557311333
  batch 277 loss: 0.2615475896140729
  batch 278 loss: 0.26143428554637826
  batch 279 loss: 0.26151175535280646
  batch 280 loss: 0.26145681760140826
  batch 281 loss: 0.2614201549109191
  batch 282 loss: 0.2614098628784748
  batch 283 loss: 0.26135532057748667
  batch 284 loss: 0.26147616465746515
  batch 285 loss: 0.26139959410617225
  batch 286 loss: 0.2614197728933988
  batch 287 loss: 0.26148750661557557
  batch 288 loss: 0.2613113581513365
  batch 289 loss: 0.2615856060107274
  batch 290 loss: 0.26137847756517346
  batch 291 loss: 0.2613972572936225
  batch 292 loss: 0.2615116327388646
  batch 293 loss: 0.26151272196818537
  batch 294 loss: 0.2614010478160819
  batch 295 loss: 0.2614116836402376
  batch 296 loss: 0.2614958194663396
  batch 297 loss: 0.2614957533701502
  batch 298 loss: 0.26148457615167503
  batch 299 loss: 0.2613559482388672
  batch 300 loss: 0.2613901042441527
  batch 301 loss: 0.2613664342220439
  batch 302 loss: 0.2613099324979529
  batch 303 loss: 0.2611872764310428
  batch 304 loss: 0.26116517607710865
  batch 305 loss: 0.2609736921357327
  batch 306 loss: 0.26115679964910143
  batch 307 loss: 0.2611500587836151
  batch 308 loss: 0.2612415210573704
  batch 309 loss: 0.261219471403696
  batch 310 loss: 0.2612228094570098
  batch 311 loss: 0.2613285410059227
  batch 312 loss: 0.26146477862046313
  batch 313 loss: 0.2614891081572341
  batch 314 loss: 0.26139524094997696
  batch 315 loss: 0.26140803704186094
  batch 316 loss: 0.26142979017164136
  batch 317 loss: 0.2613274851808037
  batch 318 loss: 0.26126711051793966
  batch 319 loss: 0.2612345607303153
  batch 320 loss: 0.2611202649306506
  batch 321 loss: 0.26116331116618396
  batch 322 loss: 0.2611318503199897
  batch 323 loss: 0.26112647262335564
  batch 324 loss: 0.260992126332389
  batch 325 loss: 0.26088429340949426
  batch 326 loss: 0.2608521588923741
  batch 327 loss: 0.260937038911592
  batch 328 loss: 0.26075985891426484
  batch 329 loss: 0.2608354816320819
  batch 330 loss: 0.26077180712512044
  batch 331 loss: 0.2607701152473058
  batch 332 loss: 0.26064238769103243
  batch 333 loss: 0.26065693424270675
  batch 334 loss: 0.26051527297425414
  batch 335 loss: 0.2603155249979959
  batch 336 loss: 0.26023899395728395
  batch 337 loss: 0.2602097673745113
  batch 338 loss: 0.2601874487022676
  batch 339 loss: 0.26013270020484924
  batch 340 loss: 0.26006540520226257
  batch 341 loss: 0.26004674030014496
  batch 342 loss: 0.2599311552462522
  batch 343 loss: 0.2599046410607179
  batch 344 loss: 0.25977190683574175
  batch 345 loss: 0.2598351594762526
  batch 346 loss: 0.25974912911308984
  batch 347 loss: 0.25970296338415283
  batch 348 loss: 0.25964956405176515
  batch 349 loss: 0.25953215036146277
  batch 350 loss: 0.2595341710533415
  batch 351 loss: 0.2594988194965569
  batch 352 loss: 0.2595565200529315
  batch 353 loss: 0.25952217392306826
  batch 354 loss: 0.2596576232778824
  batch 355 loss: 0.25963714311660174
  batch 356 loss: 0.2596550242703282
  batch 357 loss: 0.2595318239860508
  batch 358 loss: 0.2595921065138039
  batch 359 loss: 0.2595235421880042
  batch 360 loss: 0.25949992748598255
  batch 361 loss: 0.259464324841539
  batch 362 loss: 0.2593444621612354
  batch 363 loss: 0.2591871277688292
  batch 364 loss: 0.259033143970665
  batch 365 loss: 0.2589930318398018
  batch 366 loss: 0.25897646768659843
  batch 367 loss: 0.25887733791277256
  batch 368 loss: 0.25870114042545145
  batch 369 loss: 0.258696873296244
  batch 370 loss: 0.2586019892547582
  batch 371 loss: 0.25868380089815096
  batch 372 loss: 0.2586111387498276
  batch 373 loss: 0.25856256696717667
  batch 374 loss: 0.2584334485989841
  batch 375 loss: 0.25838477818171185
  batch 376 loss: 0.2585125195377685
  batch 377 loss: 0.2584567226528805
  batch 378 loss: 0.25840141332496414
  batch 379 loss: 0.25845387039367
  batch 380 loss: 0.2585300077341105
  batch 381 loss: 0.25853728188148006
  batch 382 loss: 0.2584988953867508
  batch 383 loss: 0.25856720070614825
  batch 384 loss: 0.2584565004023413
  batch 385 loss: 0.2584923899793006
  batch 386 loss: 0.2584374609157211
  batch 387 loss: 0.25852344129282684
  batch 388 loss: 0.2585676729141437
  batch 389 loss: 0.2585363894471166
  batch 390 loss: 0.2584823530453902
  batch 391 loss: 0.25849686170478003
  batch 392 loss: 0.25840542450243115
  batch 393 loss: 0.2582971844964355
  batch 394 loss: 0.25831072259372867
  batch 395 loss: 0.25829604750947105
  batch 396 loss: 0.25823546607385983
  batch 397 loss: 0.2582625194971147
  batch 398 loss: 0.25814882336399664
  batch 399 loss: 0.25810715530002326
  batch 400 loss: 0.25816883344203234
  batch 401 loss: 0.25805647411102667
  batch 402 loss: 0.2580543191945968
  batch 403 loss: 0.2581533727885476
  batch 404 loss: 0.25822008958104814
  batch 405 loss: 0.25825842666773147
  batch 406 loss: 0.25833455950315365
  batch 407 loss: 0.2583339150253619
  batch 408 loss: 0.25839744789489344
  batch 409 loss: 0.25844220715076244
  batch 410 loss: 0.258561855590925
  batch 411 loss: 0.25855065816945405
  batch 412 loss: 0.2585299879095508
  batch 413 loss: 0.2585716751600293
  batch 414 loss: 0.25854524642517024
  batch 415 loss: 0.2584874861570726
  batch 416 loss: 0.2585875256918371
  batch 417 loss: 0.25853549297765005
  batch 418 loss: 0.2585366080965152
  batch 419 loss: 0.25851669509257225
  batch 420 loss: 0.25851231259959084
  batch 421 loss: 0.25841794692139164
  batch 422 loss: 0.2585058998298871
  batch 423 loss: 0.2584555680306527
  batch 424 loss: 0.2583754287575776
  batch 425 loss: 0.25836123073802275
  batch 426 loss: 0.258318755688242
  batch 427 loss: 0.25830919618349724
  batch 428 loss: 0.25827492717827594
  batch 429 loss: 0.25829367607067794
  batch 430 loss: 0.2582113870008047
  batch 431 loss: 0.25829706906304284
  batch 432 loss: 0.2583339341605703
  batch 433 loss: 0.258336616202535
  batch 434 loss: 0.2583796855422758
  batch 435 loss: 0.25834255663827915
  batch 436 loss: 0.258326107072174
  batch 437 loss: 0.25844631001387364
  batch 438 loss: 0.258547460311624
  batch 439 loss: 0.25847836738295327
  batch 440 loss: 0.25858053050257945
  batch 441 loss: 0.258486469992164
  batch 442 loss: 0.2584084487473803
  batch 443 loss: 0.2584031424054174
  batch 444 loss: 0.2583678825213028
  batch 445 loss: 0.25835728564958893
  batch 446 loss: 0.2583540489588083
  batch 447 loss: 0.2582559170872306
  batch 448 loss: 0.25830302726743476
  batch 449 loss: 0.25839033065766165
  batch 450 loss: 0.2583389339182112
  batch 451 loss: 0.25835486129752283
  batch 452 loss: 0.2584816584017424
  batch 453 loss: 0.25852009739570536
  batch 454 loss: 0.25860791098703895
  batch 455 loss: 0.258688962328565
  batch 456 loss: 0.25869303007136313
  batch 457 loss: 0.2586568092491225
  batch 458 loss: 0.2585945528927849
  batch 459 loss: 0.2586987910057724
  batch 460 loss: 0.25875189427448353
  batch 461 loss: 0.2587332119134915
  batch 462 loss: 0.2586745654930284
  batch 463 loss: 0.25858275878764125
  batch 464 loss: 0.25858357176184654
  batch 465 loss: 0.25849594627657246
  batch 466 loss: 0.2583641758740204
  batch 467 loss: 0.2584838495456177
  batch 468 loss: 0.25847665847748774
  batch 469 loss: 0.2586422643618289
  batch 470 loss: 0.2586215784258031
  batch 471 loss: 0.2586780766582793
  batch 472 loss: 0.2585936344970586
LOSS train 0.2585936344970586 valid 0.20829997956752777
LOSS train 0.2585936344970586 valid 0.2027411162853241
LOSS train 0.2585936344970586 valid 0.19948684175809225
LOSS train 0.2585936344970586 valid 0.19478698819875717
LOSS train 0.2585936344970586 valid 0.19131961464881897
LOSS train 0.2585936344970586 valid 0.1966788445909818
LOSS train 0.2585936344970586 valid 0.20690809616020747
LOSS train 0.2585936344970586 valid 0.204522754997015
LOSS train 0.2585936344970586 valid 0.20351210236549377
LOSS train 0.2585936344970586 valid 0.2034014567732811
LOSS train 0.2585936344970586 valid 0.20254511453888632
LOSS train 0.2585936344970586 valid 0.20354747275511423
LOSS train 0.2585936344970586 valid 0.20159884714163268
LOSS train 0.2585936344970586 valid 0.20184420581374848
LOSS train 0.2585936344970586 valid 0.19787512123584747
LOSS train 0.2585936344970586 valid 0.1984767895191908
LOSS train 0.2585936344970586 valid 0.19921729669851415
LOSS train 0.2585936344970586 valid 0.20064967374006906
LOSS train 0.2585936344970586 valid 0.20205226226856834
LOSS train 0.2585936344970586 valid 0.20081749111413955
LOSS train 0.2585936344970586 valid 0.20039289551121847
LOSS train 0.2585936344970586 valid 0.19922642748464237
LOSS train 0.2585936344970586 valid 0.20004207608492477
LOSS train 0.2585936344970586 valid 0.19882335451742014
LOSS train 0.2585936344970586 valid 0.19803647994995116
LOSS train 0.2585936344970586 valid 0.19830750616697165
LOSS train 0.2585936344970586 valid 0.19832160241074032
LOSS train 0.2585936344970586 valid 0.19878644336547172
LOSS train 0.2585936344970586 valid 0.19842219763788685
LOSS train 0.2585936344970586 valid 0.19925164431333542
LOSS train 0.2585936344970586 valid 0.20001709893826516
LOSS train 0.2585936344970586 valid 0.19969741767272353
LOSS train 0.2585936344970586 valid 0.20050205651557806
LOSS train 0.2585936344970586 valid 0.19991849056061575
LOSS train 0.2585936344970586 valid 0.20102752106530325
LOSS train 0.2585936344970586 valid 0.2008715056710773
LOSS train 0.2585936344970586 valid 0.20060631874445323
LOSS train 0.2585936344970586 valid 0.2020657595835234
LOSS train 0.2585936344970586 valid 0.20206534938934523
LOSS train 0.2585936344970586 valid 0.20211698301136494
LOSS train 0.2585936344970586 valid 0.20290683936782
LOSS train 0.2585936344970586 valid 0.20308968779586611
LOSS train 0.2585936344970586 valid 0.20243150896804277
LOSS train 0.2585936344970586 valid 0.20259704000570558
LOSS train 0.2585936344970586 valid 0.2022531204753452
LOSS train 0.2585936344970586 valid 0.20273338906143024
LOSS train 0.2585936344970586 valid 0.20359755037946903
LOSS train 0.2585936344970586 valid 0.20355085004121065
LOSS train 0.2585936344970586 valid 0.20394899072695752
LOSS train 0.2585936344970586 valid 0.20311278373003006
LOSS train 0.2585936344970586 valid 0.203163982314222
LOSS train 0.2585936344970586 valid 0.20280899852514267
LOSS train 0.2585936344970586 valid 0.20287250460318798
LOSS train 0.2585936344970586 valid 0.2029390141919807
LOSS train 0.2585936344970586 valid 0.20284158235246486
LOSS train 0.2585936344970586 valid 0.20223571147237504
LOSS train 0.2585936344970586 valid 0.20185901538321846
LOSS train 0.2585936344970586 valid 0.20146910460858508
LOSS train 0.2585936344970586 valid 0.20190690685126741
LOSS train 0.2585936344970586 valid 0.20191341737906138
LOSS train 0.2585936344970586 valid 0.2016835838067727
LOSS train 0.2585936344970586 valid 0.20223428236861382
LOSS train 0.2585936344970586 valid 0.20204914562285892
LOSS train 0.2585936344970586 valid 0.20287491660565138
LOSS train 0.2585936344970586 valid 0.2030254941720229
LOSS train 0.2585936344970586 valid 0.20283841861016821
LOSS train 0.2585936344970586 valid 0.20213163810879436
LOSS train 0.2585936344970586 valid 0.20211737756343448
LOSS train 0.2585936344970586 valid 0.20172836577546768
LOSS train 0.2585936344970586 valid 0.20177490775074278
LOSS train 0.2585936344970586 valid 0.20158778970510188
LOSS train 0.2585936344970586 valid 0.20178617226580778
LOSS train 0.2585936344970586 valid 0.20162077901298053
LOSS train 0.2585936344970586 valid 0.20176965340569214
LOSS train 0.2585936344970586 valid 0.2019849995772044
LOSS train 0.2585936344970586 valid 0.20214551236284406
LOSS train 0.2585936344970586 valid 0.2022234545899676
LOSS train 0.2585936344970586 valid 0.20253375592904213
LOSS train 0.2585936344970586 valid 0.20253069283841532
LOSS train 0.2585936344970586 valid 0.2019034255295992
LOSS train 0.2585936344970586 valid 0.2010526097851035
LOSS train 0.2585936344970586 valid 0.2014894476387559
LOSS train 0.2585936344970586 valid 0.20127425405634455
LOSS train 0.2585936344970586 valid 0.2012198381125927
LOSS train 0.2585936344970586 valid 0.20100998072063223
LOSS train 0.2585936344970586 valid 0.2005779391457868
LOSS train 0.2585936344970586 valid 0.20036646141403022
LOSS train 0.2585936344970586 valid 0.20010226067494263
LOSS train 0.2585936344970586 valid 0.20065776853079206
LOSS train 0.2585936344970586 valid 0.20065232283539242
LOSS train 0.2585936344970586 valid 0.20073996959151802
LOSS train 0.2585936344970586 valid 0.20102090485717938
LOSS train 0.2585936344970586 valid 0.20097618878528636
LOSS train 0.2585936344970586 valid 0.20105668426828183
LOSS train 0.2585936344970586 valid 0.20108693289129356
LOSS train 0.2585936344970586 valid 0.20136024368306002
LOSS train 0.2585936344970586 valid 0.20133899290537097
LOSS train 0.2585936344970586 valid 0.2015697194301352
LOSS train 0.2585936344970586 valid 0.20158511189499287
LOSS train 0.2585936344970586 valid 0.2017681734263897
LOSS train 0.2585936344970586 valid 0.2019967931036902
LOSS train 0.2585936344970586 valid 0.2020458386224859
LOSS train 0.2585936344970586 valid 0.20184731411123738
LOSS train 0.2585936344970586 valid 0.201899344388109
LOSS train 0.2585936344970586 valid 0.20207221068087078
LOSS train 0.2585936344970586 valid 0.20217115010293024
LOSS train 0.2585936344970586 valid 0.20209254018057174
LOSS train 0.2585936344970586 valid 0.20218322177728018
LOSS train 0.2585936344970586 valid 0.2024901644625795
LOSS train 0.2585936344970586 valid 0.2027296720580621
LOSS train 0.2585936344970586 valid 0.2023355728602624
LOSS train 0.2585936344970586 valid 0.2022886293541108
LOSS train 0.2585936344970586 valid 0.20231585784823494
LOSS train 0.2585936344970586 valid 0.20226612660968513
LOSS train 0.2585936344970586 valid 0.20226815524308578
LOSS train 0.2585936344970586 valid 0.2021681197501462
LOSS train 0.2585936344970586 valid 0.2023679695577703
LOSS train 0.2585936344970586 valid 0.202288101790315
LOSS train 0.2585936344970586 valid 0.202177559753426
LOSS train 0.2585936344970586 valid 0.20197025276720523
LOSS train 0.2585936344970586 valid 0.20193691871875574
LOSS train 0.2585936344970586 valid 0.2017920724436885
LOSS train 0.2585936344970586 valid 0.20165394198119155
LOSS train 0.2585936344970586 valid 0.20190031646240142
LOSS train 0.2585936344970586 valid 0.2018189070224762
LOSS train 0.2585936344970586 valid 0.2021060384928234
LOSS train 0.2585936344970586 valid 0.20201832513640247
LOSS train 0.2585936344970586 valid 0.20232647296506912
LOSS train 0.2585936344970586 valid 0.202416628483654
LOSS train 0.2585936344970586 valid 0.20230972583477314
LOSS train 0.2585936344970586 valid 0.20239211812274147
LOSS train 0.2585936344970586 valid 0.2022574284311497
LOSS train 0.2585936344970586 valid 0.202256891512333
LOSS train 0.2585936344970586 valid 0.20233866950469231
LOSS train 0.2585936344970586 valid 0.20227524560910684
LOSS train 0.2585936344970586 valid 0.2022892764824278
LOSS train 0.2585936344970586 valid 0.20214481031807668
LOSS train 0.2585936344970586 valid 0.20213771427887073
LOSS train 0.2585936344970586 valid 0.2020569210001033
LOSS train 0.2585936344970586 valid 0.20210438519716262
LOSS train 0.2585936344970586 valid 0.2021567908161921
LOSS train 0.2585936344970586 valid 0.20240588748538998
LOSS train 0.2585936344970586 valid 0.20238384708657964
LOSS train 0.2585936344970586 valid 0.20239347819652823
LOSS train 0.2585936344970586 valid 0.20217319521410712
LOSS train 0.2585936344970586 valid 0.20233399665927235
LOSS train 0.2585936344970586 valid 0.20221196125153781
LOSS train 0.2585936344970586 valid 0.20280550984111992
LOSS train 0.2585936344970586 valid 0.20288537172663132
LOSS train 0.2585936344970586 valid 0.20300084710121155
LOSS train 0.2585936344970586 valid 0.2030667257032647
LOSS train 0.2585936344970586 valid 0.20277421284270913
LOSS train 0.2585936344970586 valid 0.202881755002963
LOSS train 0.2585936344970586 valid 0.20269179111951358
LOSS train 0.2585936344970586 valid 0.2027351753365609
LOSS train 0.2585936344970586 valid 0.20283111175283408
LOSS train 0.2585936344970586 valid 0.20272942789041312
LOSS train 0.2585936344970586 valid 0.202818832254108
LOSS train 0.2585936344970586 valid 0.20286809114165277
LOSS train 0.2585936344970586 valid 0.20282614827156067
LOSS train 0.2585936344970586 valid 0.2027514803668727
LOSS train 0.2585936344970586 valid 0.20264775250796918
LOSS train 0.2585936344970586 valid 0.20249450608996525
LOSS train 0.2585936344970586 valid 0.20232081649506964
LOSS train 0.2585936344970586 valid 0.20212100635875355
LOSS train 0.2585936344970586 valid 0.2022237941084138
LOSS train 0.2585936344970586 valid 0.2025687241268729
LOSS train 0.2585936344970586 valid 0.20238402113318443
LOSS train 0.2585936344970586 valid 0.2025557435475863
LOSS train 0.2585936344970586 valid 0.20259243337547078
LOSS train 0.2585936344970586 valid 0.2025983676924343
LOSS train 0.2585936344970586 valid 0.20251868699872216
LOSS train 0.2585936344970586 valid 0.2024753788819892
LOSS train 0.2585936344970586 valid 0.20246360673644076
LOSS train 0.2585936344970586 valid 0.2022341457435063
LOSS train 0.2585936344970586 valid 0.2022218717770143
LOSS train 0.2585936344970586 valid 0.20221352956052555
LOSS train 0.2585936344970586 valid 0.20239842800277003
LOSS train 0.2585936344970586 valid 0.202325515906904
LOSS train 0.2585936344970586 valid 0.20227508611149259
LOSS train 0.2585936344970586 valid 0.20232987412102316
LOSS train 0.2585936344970586 valid 0.20234097212880522
LOSS train 0.2585936344970586 valid 0.20241686131784825
LOSS train 0.2585936344970586 valid 0.20242836099603903
LOSS train 0.2585936344970586 valid 0.20220493271544174
LOSS train 0.2585936344970586 valid 0.2021990198121276
LOSS train 0.2585936344970586 valid 0.20206349052209904
LOSS train 0.2585936344970586 valid 0.20203929172551377
LOSS train 0.2585936344970586 valid 0.20199509083278597
LOSS train 0.2585936344970586 valid 0.2020764169724364
LOSS train 0.2585936344970586 valid 0.20193734398375007
LOSS train 0.2585936344970586 valid 0.20198415064563355
LOSS train 0.2585936344970586 valid 0.2018327313074794
LOSS train 0.2585936344970586 valid 0.20177928035713963
LOSS train 0.2585936344970586 valid 0.20166570643583934
LOSS train 0.2585936344970586 valid 0.2016514511588885
LOSS train 0.2585936344970586 valid 0.2018441394500926
LOSS train 0.2585936344970586 valid 0.20175984185753446
LOSS train 0.2585936344970586 valid 0.2018093093106495
LOSS train 0.2585936344970586 valid 0.20169384822249412
LOSS train 0.2585936344970586 valid 0.20155819716738232
LOSS train 0.2585936344970586 valid 0.20151515659129265
LOSS train 0.2585936344970586 valid 0.20138172386902306
LOSS train 0.2585936344970586 valid 0.2013357287528468
LOSS train 0.2585936344970586 valid 0.20108897482476584
LOSS train 0.2585936344970586 valid 0.20113799605265403
LOSS train 0.2585936344970586 valid 0.2010552326694203
LOSS train 0.2585936344970586 valid 0.2008763668485559
LOSS train 0.2585936344970586 valid 0.20072329500645542
LOSS train 0.2585936344970586 valid 0.20076235014767874
LOSS train 0.2585936344970586 valid 0.20092010406238772
LOSS train 0.2585936344970586 valid 0.200930100566936
LOSS train 0.2585936344970586 valid 0.2010117492905245
LOSS train 0.2585936344970586 valid 0.20095560142648555
LOSS train 0.2585936344970586 valid 0.20079967317193054
LOSS train 0.2585936344970586 valid 0.20080048577101142
LOSS train 0.2585936344970586 valid 0.20081218857369665
LOSS train 0.2585936344970586 valid 0.20091635918398515
LOSS train 0.2585936344970586 valid 0.2009908087449531
LOSS train 0.2585936344970586 valid 0.20106107782233845
LOSS train 0.2585936344970586 valid 0.20106483463248517
LOSS train 0.2585936344970586 valid 0.20099056975261584
LOSS train 0.2585936344970586 valid 0.20108313704819958
LOSS train 0.2585936344970586 valid 0.20117308465497835
LOSS train 0.2585936344970586 valid 0.2011496917406718
LOSS train 0.2585936344970586 valid 0.20119440384143222
LOSS train 0.2585936344970586 valid 0.2014357652433118
LOSS train 0.2585936344970586 valid 0.20152789406609117
LOSS train 0.2585936344970586 valid 0.20157240428778803
LOSS train 0.2585936344970586 valid 0.20164538680211358
LOSS train 0.2585936344970586 valid 0.2016527367618693
LOSS train 0.2585936344970586 valid 0.20168219750811314
LOSS train 0.2585936344970586 valid 0.20171104422710484
LOSS train 0.2585936344970586 valid 0.20174599929243073
LOSS train 0.2585936344970586 valid 0.2018604981138351
LOSS train 0.2585936344970586 valid 0.20177451546414424
LOSS train 0.2585936344970586 valid 0.20187931793902997
LOSS train 0.2585936344970586 valid 0.20188839789949545
LOSS train 0.2585936344970586 valid 0.20177701165486578
LOSS train 0.2585936344970586 valid 0.20169636184970538
LOSS train 0.2585936344970586 valid 0.20174684731050152
LOSS train 0.2585936344970586 valid 0.20155261900306734
LOSS train 0.2585936344970586 valid 0.20165937041304238
LOSS train 0.2585936344970586 valid 0.20186011230603593
LOSS train 0.2585936344970586 valid 0.20200654566287995
LOSS train 0.2585936344970586 valid 0.20196811243043683
LOSS train 0.2585936344970586 valid 0.20203752634737657
LOSS train 0.2585936344970586 valid 0.20201410635584785
LOSS train 0.2585936344970586 valid 0.20200018135898085
LOSS train 0.2585936344970586 valid 0.20217124885320664
LOSS train 0.2585936344970586 valid 0.2021332655651161
LOSS train 0.2585936344970586 valid 0.20227574154971137
LOSS train 0.2585936344970586 valid 0.20219788296891766
LOSS train 0.2585936344970586 valid 0.20215814103057064
LOSS train 0.2585936344970586 valid 0.2022037967747333
LOSS train 0.2585936344970586 valid 0.20223320473451167
LOSS train 0.2585936344970586 valid 0.2021478244070877
LOSS train 0.2585936344970586 valid 0.20218336362709372
LOSS train 0.2585936344970586 valid 0.2021736270787633
LOSS train 0.2585936344970586 valid 0.2021328156384138
LOSS train 0.2585936344970586 valid 0.20223695427978633
LOSS train 0.2585936344970586 valid 0.2022500151327548
LOSS train 0.2585936344970586 valid 0.20226117435970234
LOSS train 0.2585936344970586 valid 0.2022783203106938
LOSS train 0.2585936344970586 valid 0.20223573378796847
LOSS train 0.2585936344970586 valid 0.20232100549497103
LOSS train 0.2585936344970586 valid 0.20235710119486747
LOSS train 0.2585936344970586 valid 0.20246905705599644
LOSS train 0.2585936344970586 valid 0.2025213851472259
LOSS train 0.2585936344970586 valid 0.20251364492707782
LOSS train 0.2585936344970586 valid 0.20258647247434103
LOSS train 0.2585936344970586 valid 0.20278772908975096
LOSS train 0.2585936344970586 valid 0.20281356474855444
LOSS train 0.2585936344970586 valid 0.20274659833551323
LOSS train 0.2585936344970586 valid 0.20272641149434176
LOSS train 0.2585936344970586 valid 0.20259320633350938
LOSS train 0.2585936344970586 valid 0.20245749350058903
LOSS train 0.2585936344970586 valid 0.20235785922343782
LOSS train 0.2585936344970586 valid 0.2023623500246301
LOSS train 0.2585936344970586 valid 0.2023308799202953
LOSS train 0.2585936344970586 valid 0.20216224334630253
LOSS train 0.2585936344970586 valid 0.20200593146026558
LOSS train 0.2585936344970586 valid 0.2019449710424713
LOSS train 0.2585936344970586 valid 0.20195511214330164
LOSS train 0.2585936344970586 valid 0.20201186339060465
LOSS train 0.2585936344970586 valid 0.20197267710537345
LOSS train 0.2585936344970586 valid 0.20191759291633912
LOSS train 0.2585936344970586 valid 0.20188726065680385
LOSS train 0.2585936344970586 valid 0.20191247802498433
LOSS train 0.2585936344970586 valid 0.20194229509296088
LOSS train 0.2585936344970586 valid 0.20188029299896607
LOSS train 0.2585936344970586 valid 0.20190150392790362
LOSS train 0.2585936344970586 valid 0.20190634509809188
LOSS train 0.2585936344970586 valid 0.20200487282000432
LOSS train 0.2585936344970586 valid 0.20206019464185682
LOSS train 0.2585936344970586 valid 0.2020119771965452
LOSS train 0.2585936344970586 valid 0.2020730533282765
LOSS train 0.2585936344970586 valid 0.2020812191619169
LOSS train 0.2585936344970586 valid 0.2021038814332573
LOSS train 0.2585936344970586 valid 0.2020676442484061
LOSS train 0.2585936344970586 valid 0.2020786668474096
LOSS train 0.2585936344970586 valid 0.2020892332622547
LOSS train 0.2585936344970586 valid 0.20213165475983824
LOSS train 0.2585936344970586 valid 0.2020976174818842
LOSS train 0.2585936344970586 valid 0.20197492593624553
LOSS train 0.2585936344970586 valid 0.20191971495065814
LOSS train 0.2585936344970586 valid 0.20183072916265418
LOSS train 0.2585936344970586 valid 0.20178293625449206
LOSS train 0.2585936344970586 valid 0.20177851935613503
LOSS train 0.2585936344970586 valid 0.20184703868243
LOSS train 0.2585936344970586 valid 0.2017892298207789
LOSS train 0.2585936344970586 valid 0.20175708897220782
LOSS train 0.2585936344970586 valid 0.20180596796849284
LOSS train 0.2585936344970586 valid 0.20179441030238085
LOSS train 0.2585936344970586 valid 0.20177213738835048
LOSS train 0.2585936344970586 valid 0.20172830204231829
LOSS train 0.2585936344970586 valid 0.20174479385843788
LOSS train 0.2585936344970586 valid 0.20176805569878165
LOSS train 0.2585936344970586 valid 0.2018825285673889
LOSS train 0.2585936344970586 valid 0.20183578720316292
LOSS train 0.2585936344970586 valid 0.20194331340700666
LOSS train 0.2585936344970586 valid 0.20185508768751015
LOSS train 0.2585936344970586 valid 0.20177438393834943
LOSS train 0.2585936344970586 valid 0.20176527948107248
LOSS train 0.2585936344970586 valid 0.2017961930311643
LOSS train 0.2585936344970586 valid 0.201868522706573
LOSS train 0.2585936344970586 valid 0.20186159329130016
LOSS train 0.2585936344970586 valid 0.20187188130689832
LOSS train 0.2585936344970586 valid 0.20190166702386456
LOSS train 0.2585936344970586 valid 0.201927705876755
LOSS train 0.2585936344970586 valid 0.20183579448126593
LOSS train 0.2585936344970586 valid 0.20173011419464307
LOSS train 0.2585936344970586 valid 0.20176752582863644
LOSS train 0.2585936344970586 valid 0.2018475909076051
LOSS train 0.2585936344970586 valid 0.2017947588838748
LOSS train 0.2585936344970586 valid 0.20172520753528392
LOSS train 0.2585936344970586 valid 0.2016779448759662
LOSS train 0.2585936344970586 valid 0.20163229786785397
LOSS train 0.2585936344970586 valid 0.2015872999454318
LOSS train 0.2585936344970586 valid 0.2016011213993325
LOSS train 0.2585936344970586 valid 0.2014948279539511
LOSS train 0.2585936344970586 valid 0.20151111270077746
LOSS train 0.2585936344970586 valid 0.201504419783114
LOSS train 0.2585936344970586 valid 0.20161154831564704
LOSS train 0.2585936344970586 valid 0.2016364150289176
LOSS train 0.2585936344970586 valid 0.2016204909093118
LOSS train 0.2585936344970586 valid 0.2015095655873461
LOSS train 0.2585936344970586 valid 0.20141951577074227
LOSS train 0.2585936344970586 valid 0.20146898417213244
LOSS train 0.2585936344970586 valid 0.20142292984894344
LOSS train 0.2585936344970586 valid 0.2013805026766921
LOSS train 0.2585936344970586 valid 0.2013510226441378
LOSS train 0.2585936344970586 valid 0.20137375339907898
LOSS train 0.2585936344970586 valid 0.20138929413873596
LOSS train 0.2585936344970586 valid 0.20148889619699667
LOSS train 0.2585936344970586 valid 0.2015702287049106
LOSS train 0.2585936344970586 valid 0.20151779685868604
LOSS train 0.2585936344970586 valid 0.20147652724268716
LOSS train 0.2585936344970586 valid 0.2015326166717455
LOSS train 0.2585936344970586 valid 0.2014806800832351
LOSS train 0.2585936344970586 valid 0.2014706736621434
LOSS train 0.2585936344970586 valid 0.20153206035247823
LOSS train 0.2585936344970586 valid 0.2014209166702817
LOSS train 0.2585936344970586 valid 0.20145767882138818
LOSS train 0.2585936344970586 valid 0.201464512740096
LOSS train 0.2585936344970586 valid 0.20141448512103388
LOSS train 0.2585936344970586 valid 0.2013044080795969
LOSS train 0.2585936344970586 valid 0.20130481153888546
LOSS train 0.2585936344970586 valid 0.20138484788780936
EPOCH 11:
  batch 1 loss: 0.26330041885375977
  batch 2 loss: 0.25373266637325287
  batch 3 loss: 0.2590396801630656
  batch 4 loss: 0.26645660400390625
  batch 5 loss: 0.2690932810306549
  batch 6 loss: 0.26360564678907394
  batch 7 loss: 0.26632278519017355
  batch 8 loss: 0.2678889390081167
  batch 9 loss: 0.2651868628131019
  batch 10 loss: 0.26108904778957365
  batch 11 loss: 0.26262824914672156
  batch 12 loss: 0.25918202350536984
  batch 13 loss: 0.2583343592973856
  batch 14 loss: 0.2599997158561434
  batch 15 loss: 0.26073250770568845
  batch 16 loss: 0.2588699394837022
  batch 17 loss: 0.2566571297014461
  batch 18 loss: 0.2583754070931011
  batch 19 loss: 0.2562519970693086
  batch 20 loss: 0.25424987971782687
  batch 21 loss: 0.2556160730975015
  batch 22 loss: 0.25456295704299753
  batch 23 loss: 0.25366923536943353
  batch 24 loss: 0.2514564761271079
  batch 25 loss: 0.25346002995967865
  batch 26 loss: 0.25144279576264894
  batch 27 loss: 0.2517624221466206
  batch 28 loss: 0.25074909308126997
  batch 29 loss: 0.25149273050242454
  batch 30 loss: 0.25110988368590675
  batch 31 loss: 0.25127255580117625
  batch 32 loss: 0.2511138687841594
  batch 33 loss: 0.2511745144923528
  batch 34 loss: 0.2501685676329276
  batch 35 loss: 0.24998733869620732
  batch 36 loss: 0.25051826362808544
  batch 37 loss: 0.25036868211385366
  batch 38 loss: 0.25111989912233856
  batch 39 loss: 0.2507233459215898
  batch 40 loss: 0.2509672209620476
  batch 41 loss: 0.25120948073340627
  batch 42 loss: 0.25172819410051617
  batch 43 loss: 0.252529779145884
  batch 44 loss: 0.25282022018324246
  batch 45 loss: 0.25218028922875724
  batch 46 loss: 0.25302649224581925
  batch 47 loss: 0.253186193869469
  batch 48 loss: 0.25273075451453525
  batch 49 loss: 0.25252464900211413
  batch 50 loss: 0.25229972183704374
  batch 51 loss: 0.25220133685598184
  batch 52 loss: 0.2529356823517726
  batch 53 loss: 0.25211279684642574
  batch 54 loss: 0.2524224392793797
  batch 55 loss: 0.2524323495951566
  batch 56 loss: 0.2523930961532252
  batch 57 loss: 0.2521394744776843
  batch 58 loss: 0.25316597135930224
  batch 59 loss: 0.25362114810337455
  batch 60 loss: 0.2529177727798621
  batch 61 loss: 0.25328137127102396
  batch 62 loss: 0.2542679927522136
  batch 63 loss: 0.25365360554248567
  batch 64 loss: 0.25437766243703663
  batch 65 loss: 0.25439503903572375
  batch 66 loss: 0.25429560244083405
  batch 67 loss: 0.2551039132609296
  batch 68 loss: 0.25522543533759956
  batch 69 loss: 0.2552245000134344
  batch 70 loss: 0.25549217121941703
  batch 71 loss: 0.2549420171220538
  batch 72 loss: 0.25517895652188194
  batch 73 loss: 0.25506002262030564
  batch 74 loss: 0.25492377015384465
  batch 75 loss: 0.2544289853175481
  batch 76 loss: 0.25507897746406105
  batch 77 loss: 0.2545941462377449
  batch 78 loss: 0.2550075755287439
  batch 79 loss: 0.2553206748223003
  batch 80 loss: 0.25495633017271757
  batch 81 loss: 0.25517528127005074
  batch 82 loss: 0.2557193736841039
  batch 83 loss: 0.25594504393008816
  batch 84 loss: 0.25537585981544997
  batch 85 loss: 0.25502522745553186
  batch 86 loss: 0.25539111935122066
  batch 87 loss: 0.2554147454171345
  batch 88 loss: 0.2550434966317632
  batch 89 loss: 0.2546584862336684
  batch 90 loss: 0.2545383956697252
  batch 91 loss: 0.2546755756650652
  batch 92 loss: 0.2544421984449677
  batch 93 loss: 0.2541002825062762
  batch 94 loss: 0.25459084057427467
  batch 95 loss: 0.2539892060192008
  batch 96 loss: 0.254170595202595
  batch 97 loss: 0.25477388086392705
  batch 98 loss: 0.25528137918029514
  batch 99 loss: 0.2555078715085983
  batch 100 loss: 0.2554827739298344
  batch 101 loss: 0.255664786932492
  batch 102 loss: 0.25627545208907593
  batch 103 loss: 0.2567814974819572
  batch 104 loss: 0.2570086375165444
  batch 105 loss: 0.257099452047121
  batch 106 loss: 0.2574369304303853
  batch 107 loss: 0.2574045258426221
  batch 108 loss: 0.2573791244239719
  batch 109 loss: 0.25717279520056663
  batch 110 loss: 0.2572061702609062
  batch 111 loss: 0.25681364388616235
  batch 112 loss: 0.2564628262604986
  batch 113 loss: 0.25642787487105984
  batch 114 loss: 0.2566717996409065
  batch 115 loss: 0.25694363428198774
  batch 116 loss: 0.2572031306295559
  batch 117 loss: 0.2575573893184336
  batch 118 loss: 0.25731375093682335
  batch 119 loss: 0.25788815970681295
  batch 120 loss: 0.257526154195269
  batch 121 loss: 0.2571436367497956
  batch 122 loss: 0.25711650401353836
  batch 123 loss: 0.25701266587749727
  batch 124 loss: 0.2572391101669881
  batch 125 loss: 0.25738035881519317
  batch 126 loss: 0.25736070329707766
  batch 127 loss: 0.2578167435456449
  batch 128 loss: 0.25752992974594235
  batch 129 loss: 0.2576739658680997
  batch 130 loss: 0.25758292376995084
  batch 131 loss: 0.25780193337047375
  batch 132 loss: 0.25766252071568463
  batch 133 loss: 0.2580927431135249
  batch 134 loss: 0.25820886421559464
  batch 135 loss: 0.25791030890411804
  batch 136 loss: 0.25786531048224254
  batch 137 loss: 0.25795860849592805
  batch 138 loss: 0.2580308323537094
  batch 139 loss: 0.25835330677118234
  batch 140 loss: 0.2580528962825026
  batch 141 loss: 0.2583295553922653
  batch 142 loss: 0.25835857099630466
  batch 143 loss: 0.25818590236293687
  batch 144 loss: 0.25825243826127714
  batch 145 loss: 0.25821799214543967
  batch 146 loss: 0.25816383518993036
  batch 147 loss: 0.2584379180556252
  batch 148 loss: 0.25849341510518176
  batch 149 loss: 0.2583114019776351
  batch 150 loss: 0.2583574981490771
  batch 151 loss: 0.2584484779084755
  batch 152 loss: 0.2586732195984376
  batch 153 loss: 0.2585669933191312
  batch 154 loss: 0.25858949550560545
  batch 155 loss: 0.25855123650643136
  batch 156 loss: 0.2584823711942404
  batch 157 loss: 0.25862354856387826
  batch 158 loss: 0.2587222632350801
  batch 159 loss: 0.25874304940115733
  batch 160 loss: 0.258625528216362
  batch 161 loss: 0.25871438713547606
  batch 162 loss: 0.2586474694587566
  batch 163 loss: 0.2586555821032612
  batch 164 loss: 0.2585229542924137
  batch 165 loss: 0.25846831599871317
  batch 166 loss: 0.2582800558891641
  batch 167 loss: 0.258293726308617
  batch 168 loss: 0.2583340961663496
  batch 169 loss: 0.25814673692517026
  batch 170 loss: 0.25801934503457125
  batch 171 loss: 0.25799636176803653
  batch 172 loss: 0.25807296025545096
  batch 173 loss: 0.25809451515619464
  batch 174 loss: 0.25797294054565756
  batch 175 loss: 0.25808551609516145
  batch 176 loss: 0.2579784851351922
  batch 177 loss: 0.2581033924710279
  batch 178 loss: 0.25819975916254384
  batch 179 loss: 0.25829542573936826
  batch 180 loss: 0.2581052552494738
  batch 181 loss: 0.2582179190210216
  batch 182 loss: 0.2581362810272437
  batch 183 loss: 0.2578993034981639
  batch 184 loss: 0.25784100203410437
  batch 185 loss: 0.25784290078523997
  batch 186 loss: 0.25792554838042103
  batch 187 loss: 0.25777894481618135
  batch 188 loss: 0.25747773075040353
  batch 189 loss: 0.25712835788726807
  batch 190 loss: 0.2572090744972229
  batch 191 loss: 0.25707448137368205
  batch 192 loss: 0.25703346030786633
  batch 193 loss: 0.25698655797409886
  batch 194 loss: 0.25723354272621196
  batch 195 loss: 0.2573696517027341
  batch 196 loss: 0.25721332537276403
  batch 197 loss: 0.25716967479831676
  batch 198 loss: 0.2574511233604316
  batch 199 loss: 0.2574543789703043
  batch 200 loss: 0.25761493235826494
  batch 201 loss: 0.257757832903174
  batch 202 loss: 0.2578630596399307
  batch 203 loss: 0.2578540671928763
  batch 204 loss: 0.25771187800987094
  batch 205 loss: 0.2578600767182141
  batch 206 loss: 0.2579635519136503
  batch 207 loss: 0.25797834315737667
  batch 208 loss: 0.25777760319984877
  batch 209 loss: 0.25743001285922584
  batch 210 loss: 0.2574667622645696
  batch 211 loss: 0.25744007817376846
  batch 212 loss: 0.25733444129802147
  batch 213 loss: 0.2572423465934718
  batch 214 loss: 0.25724084508196216
  batch 215 loss: 0.2570395631845607
  batch 216 loss: 0.2567502843147075
  batch 217 loss: 0.2566762710771253
  batch 218 loss: 0.2565944529044519
  batch 219 loss: 0.25669722231827913
  batch 220 loss: 0.25666489187966696
  batch 221 loss: 0.2567512192742317
  batch 222 loss: 0.25677881832863836
  batch 223 loss: 0.2567424315107243
  batch 224 loss: 0.25669383270932095
  batch 225 loss: 0.25657266159852343
  batch 226 loss: 0.25662456525374305
  batch 227 loss: 0.2564110409977152
  batch 228 loss: 0.25625402042478845
  batch 229 loss: 0.2562438748679307
  batch 230 loss: 0.25625979647688246
  batch 231 loss: 0.2561326702951869
  batch 232 loss: 0.25597475109429196
  batch 233 loss: 0.2559524626179315
  batch 234 loss: 0.25590697160133946
  batch 235 loss: 0.2558266566154805
  batch 236 loss: 0.2557336291519262
  batch 237 loss: 0.25579892708782404
  batch 238 loss: 0.2557495195074242
  batch 239 loss: 0.25580762220725856
  batch 240 loss: 0.255802141999205
  batch 241 loss: 0.25586808865495736
  batch 242 loss: 0.25586206262761896
  batch 243 loss: 0.25595618533008874
  batch 244 loss: 0.25598144787745397
  batch 245 loss: 0.25590643663795626
  batch 246 loss: 0.25588923741162306
  batch 247 loss: 0.25602542895537156
  batch 248 loss: 0.2560765246950811
  batch 249 loss: 0.25595004288068257
  batch 250 loss: 0.2558872455358505
  batch 251 loss: 0.25605830407712565
  batch 252 loss: 0.25583599838945603
  batch 253 loss: 0.2557772233669937
  batch 254 loss: 0.2557022249487441
  batch 255 loss: 0.2557239462931951
  batch 256 loss: 0.2558289844891988
  batch 257 loss: 0.2558341370134502
  batch 258 loss: 0.2557423493081285
  batch 259 loss: 0.255787940961974
  batch 260 loss: 0.25585712612821504
  batch 261 loss: 0.2558360638175431
  batch 262 loss: 0.2557433658201276
  batch 263 loss: 0.25568788609123955
  batch 264 loss: 0.2555889693286383
  batch 265 loss: 0.2554173579193511
  batch 266 loss: 0.2553803648491551
  batch 267 loss: 0.25547524058863463
  batch 268 loss: 0.25536549069098574
  batch 269 loss: 0.2553119261136285
  batch 270 loss: 0.2554292861510206
  batch 271 loss: 0.25544565492670473
  batch 272 loss: 0.2554716083823758
  batch 273 loss: 0.2554378934177287
  batch 274 loss: 0.2554969942047648
  batch 275 loss: 0.25545713451775637
  batch 276 loss: 0.2553671976902347
  batch 277 loss: 0.2553924470064012
  batch 278 loss: 0.2552522462692192
  batch 279 loss: 0.25529915831422295
  batch 280 loss: 0.2552716102451086
  batch 281 loss: 0.25521845776203267
  batch 282 loss: 0.2551948867262678
  batch 283 loss: 0.25517070446123924
  batch 284 loss: 0.2551961720094714
  batch 285 loss: 0.2551067087733955
  batch 286 loss: 0.2551404804616541
  batch 287 loss: 0.2551861802044646
  batch 288 loss: 0.25502544538014466
  batch 289 loss: 0.25529424552274
  batch 290 loss: 0.2551027113507534
  batch 291 loss: 0.255091166117347
  batch 292 loss: 0.25522052297649317
  batch 293 loss: 0.2552164550425656
  batch 294 loss: 0.25514636999693047
  batch 295 loss: 0.2551603798644017
  batch 296 loss: 0.25524427797141913
  batch 297 loss: 0.25525243821168186
  batch 298 loss: 0.2552549381484121
  batch 299 loss: 0.25514316015618305
  batch 300 loss: 0.255173835704724
  batch 301 loss: 0.2551512044925626
  batch 302 loss: 0.2550780311325528
  batch 303 loss: 0.25498687434117784
  batch 304 loss: 0.2549987303975381
  batch 305 loss: 0.25481710018681697
  batch 306 loss: 0.2549578135991408
  batch 307 loss: 0.2549231297030123
  batch 308 loss: 0.25499568686082763
  batch 309 loss: 0.2549971448179202
  batch 310 loss: 0.25503575244257526
  batch 311 loss: 0.25515014364405075
  batch 312 loss: 0.2553303812940915
  batch 313 loss: 0.25532864676877715
  batch 314 loss: 0.25526321162084104
  batch 315 loss: 0.25526346384532866
  batch 316 loss: 0.255228782992197
  batch 317 loss: 0.2551362316597147
  batch 318 loss: 0.25507510022359825
  batch 319 loss: 0.25500064158701224
  batch 320 loss: 0.2549411228392273
  batch 321 loss: 0.2550058201930233
  batch 322 loss: 0.25494207483456
  batch 323 loss: 0.2549536949248506
  batch 324 loss: 0.25484041239560384
  batch 325 loss: 0.254776521050013
  batch 326 loss: 0.2547653709178322
  batch 327 loss: 0.2547825551634535
  batch 328 loss: 0.254594543630757
  batch 329 loss: 0.254649820027018
  batch 330 loss: 0.2545838499159524
  batch 331 loss: 0.2545973990348891
  batch 332 loss: 0.2544878575816212
  batch 333 loss: 0.25448930191922114
  batch 334 loss: 0.25438532844453515
  batch 335 loss: 0.2542312132333642
  batch 336 loss: 0.2541749046317169
  batch 337 loss: 0.25415113218990915
  batch 338 loss: 0.25415467479701576
  batch 339 loss: 0.2541178671044234
  batch 340 loss: 0.2540511910091428
  batch 341 loss: 0.25404627822186593
  batch 342 loss: 0.25395152042483726
  batch 343 loss: 0.2539711641500712
  batch 344 loss: 0.2538689662550771
  batch 345 loss: 0.253982893280361
  batch 346 loss: 0.2539092641689874
  batch 347 loss: 0.25386817199352496
  batch 348 loss: 0.2538272515795697
  batch 349 loss: 0.2537220681465116
  batch 350 loss: 0.25373323057379044
  batch 351 loss: 0.25372730717699754
  batch 352 loss: 0.25375482541593636
  batch 353 loss: 0.2537586296415194
  batch 354 loss: 0.2539250696614637
  batch 355 loss: 0.253900758691237
  batch 356 loss: 0.2539364413850093
  batch 357 loss: 0.2538394086918577
  batch 358 loss: 0.2539170887633409
  batch 359 loss: 0.2538427700986437
  batch 360 loss: 0.25380088918738897
  batch 361 loss: 0.25377887693798773
  batch 362 loss: 0.2536898693327087
  batch 363 loss: 0.25352739557090215
  batch 364 loss: 0.25337515967887836
  batch 365 loss: 0.25333153299272876
  batch 366 loss: 0.2533204942697384
  batch 367 loss: 0.2532390448231788
  batch 368 loss: 0.2530861319569142
  batch 369 loss: 0.2530451869253867
  batch 370 loss: 0.25293939025015444
  batch 371 loss: 0.25303239401460015
  batch 372 loss: 0.2529743461599273
  batch 373 loss: 0.2529237261965831
  batch 374 loss: 0.25282052478050804
  batch 375 loss: 0.2527663329044978
  batch 376 loss: 0.2528820354491472
  batch 377 loss: 0.25285885038363204
  batch 378 loss: 0.2528237813600787
  batch 379 loss: 0.2528222840584048
  batch 380 loss: 0.2529465962005289
  batch 381 loss: 0.2529572695493698
  batch 382 loss: 0.25292383434728805
  batch 383 loss: 0.25295743178916663
  batch 384 loss: 0.25291641456230235
  batch 385 loss: 0.2529633906367537
  batch 386 loss: 0.2529077486284656
  batch 387 loss: 0.25298306118764313
  batch 388 loss: 0.25304993222822847
  batch 389 loss: 0.25304968701076996
  batch 390 loss: 0.25296576424286915
  batch 391 loss: 0.252998614288352
  batch 392 loss: 0.2529361972927439
  batch 393 loss: 0.2528597404635286
  batch 394 loss: 0.25283942863269504
  batch 395 loss: 0.25282558847831776
  batch 396 loss: 0.25281373287240666
  batch 397 loss: 0.2528000862832634
  batch 398 loss: 0.2526986342878198
  batch 399 loss: 0.2527330862848382
  batch 400 loss: 0.25279090493917467
  batch 401 loss: 0.2526791118401244
  batch 402 loss: 0.25269085876828995
  batch 403 loss: 0.25275108303236904
  batch 404 loss: 0.2528135982701684
  batch 405 loss: 0.25280712927565163
  batch 406 loss: 0.25289556674035313
  batch 407 loss: 0.25289089889871985
  batch 408 loss: 0.2529898104349188
  batch 409 loss: 0.2530120884803805
  batch 410 loss: 0.25315572099714745
  batch 411 loss: 0.2531635995111326
  batch 412 loss: 0.25316470908453165
  batch 413 loss: 0.25325308494793014
  batch 414 loss: 0.25324481794079723
  batch 415 loss: 0.2531786449343325
  batch 416 loss: 0.25327077604686987
  batch 417 loss: 0.2532739504564294
  batch 418 loss: 0.2532730286129924
  batch 419 loss: 0.2532683547257239
  batch 420 loss: 0.2532862690587839
  batch 421 loss: 0.25319831257336495
  batch 422 loss: 0.2532805105168108
  batch 423 loss: 0.253267019317787
  batch 424 loss: 0.25318495718375694
  batch 425 loss: 0.25316863410613116
  batch 426 loss: 0.2531406739857835
  batch 427 loss: 0.2531460336904615
  batch 428 loss: 0.2531099555160955
  batch 429 loss: 0.2531383694399209
  batch 430 loss: 0.2530385209030883
  batch 431 loss: 0.25310249598977735
  batch 432 loss: 0.2531125745218661
  batch 433 loss: 0.25309915018136725
  batch 434 loss: 0.25314661139442074
  batch 435 loss: 0.25310884943638723
  batch 436 loss: 0.2530953005776493
  batch 437 loss: 0.2531770213381377
  batch 438 loss: 0.2533017456803692
  batch 439 loss: 0.2532581543732341
  batch 440 loss: 0.2533356839960272
  batch 441 loss: 0.25324931223781744
  batch 442 loss: 0.2531526646328188
  batch 443 loss: 0.25312955132591
  batch 444 loss: 0.2530752991972206
  batch 445 loss: 0.25307650127437675
  batch 446 loss: 0.253070587634773
  batch 447 loss: 0.25298123805048217
  batch 448 loss: 0.25302160072273444
  batch 449 loss: 0.2531028760302571
  batch 450 loss: 0.25306706236468424
  batch 451 loss: 0.2530596174316237
  batch 452 loss: 0.253195354105097
  batch 453 loss: 0.25326767017772917
  batch 454 loss: 0.2533269077539444
  batch 455 loss: 0.2534130348609044
  batch 456 loss: 0.2534157733122508
  batch 457 loss: 0.2533848881395432
  batch 458 loss: 0.25335309566628983
  batch 459 loss: 0.2534425225522783
  batch 460 loss: 0.25348017176856164
  batch 461 loss: 0.2534669014805051
  batch 462 loss: 0.25340966148164884
  batch 463 loss: 0.2533246238532901
  batch 464 loss: 0.25331920838176175
  batch 465 loss: 0.25318234405850853
  batch 466 loss: 0.25303384888709357
  batch 467 loss: 0.253199338689606
  batch 468 loss: 0.25321016979650557
  batch 469 loss: 0.2534050942420451
  batch 470 loss: 0.253382290075434
  batch 471 loss: 0.2534403866453535
  batch 472 loss: 0.25335199726840196
LOSS train 0.25335199726840196 valid 0.1996220350265503
LOSS train 0.25335199726840196 valid 0.1897493600845337
LOSS train 0.25335199726840196 valid 0.18371803561846414
LOSS train 0.25335199726840196 valid 0.17905499041080475
LOSS train 0.25335199726840196 valid 0.17587512731552124
LOSS train 0.25335199726840196 valid 0.18130497634410858
LOSS train 0.25335199726840196 valid 0.19211331009864807
LOSS train 0.25335199726840196 valid 0.1909662690013647
LOSS train 0.25335199726840196 valid 0.18984981543487972
LOSS train 0.25335199726840196 valid 0.18953343480825424
LOSS train 0.25335199726840196 valid 0.18881679664958606
LOSS train 0.25335199726840196 valid 0.18981767197450003
LOSS train 0.25335199726840196 valid 0.1876154221021212
LOSS train 0.25335199726840196 valid 0.18776715014662063
LOSS train 0.25335199726840196 valid 0.1842344284057617
LOSS train 0.25335199726840196 valid 0.18541822768747807
LOSS train 0.25335199726840196 valid 0.18585172383224263
LOSS train 0.25335199726840196 valid 0.18712935017214882
LOSS train 0.25335199726840196 valid 0.18835692735094772
LOSS train 0.25335199726840196 valid 0.18697127997875213
LOSS train 0.25335199726840196 valid 0.18668766248793828
LOSS train 0.25335199726840196 valid 0.18559409813447433
LOSS train 0.25335199726840196 valid 0.18609411560970804
LOSS train 0.25335199726840196 valid 0.1847105249762535
LOSS train 0.25335199726840196 valid 0.1840383803844452
LOSS train 0.25335199726840196 valid 0.18400488449976996
LOSS train 0.25335199726840196 valid 0.18424908651245964
LOSS train 0.25335199726840196 valid 0.18439893584166253
LOSS train 0.25335199726840196 valid 0.1840475196468419
LOSS train 0.25335199726840196 valid 0.1847117066383362
LOSS train 0.25335199726840196 valid 0.1855795806454074
LOSS train 0.25335199726840196 valid 0.18523445632308722
LOSS train 0.25335199726840196 valid 0.18586885026006988
LOSS train 0.25335199726840196 valid 0.18534687073791728
LOSS train 0.25335199726840196 valid 0.18646836110523768
LOSS train 0.25335199726840196 valid 0.18641877836651272
LOSS train 0.25335199726840196 valid 0.18624246200999697
LOSS train 0.25335199726840196 valid 0.18762292908994774
LOSS train 0.25335199726840196 valid 0.1876403353153131
LOSS train 0.25335199726840196 valid 0.18764989487826825
LOSS train 0.25335199726840196 valid 0.18831175783785378
LOSS train 0.25335199726840196 valid 0.18840291138206208
LOSS train 0.25335199726840196 valid 0.187697529446247
LOSS train 0.25335199726840196 valid 0.18769303235140714
LOSS train 0.25335199726840196 valid 0.18750341501500872
LOSS train 0.25335199726840196 valid 0.18797800949086313
LOSS train 0.25335199726840196 valid 0.18882459560607343
LOSS train 0.25335199726840196 valid 0.1886431397870183
LOSS train 0.25335199726840196 valid 0.18885793552106742
LOSS train 0.25335199726840196 valid 0.1880633959174156
LOSS train 0.25335199726840196 valid 0.18817874497058346
LOSS train 0.25335199726840196 valid 0.18778614012094644
LOSS train 0.25335199726840196 valid 0.18804281978112347
LOSS train 0.25335199726840196 valid 0.1880863709030328
LOSS train 0.25335199726840196 valid 0.18796815086494792
LOSS train 0.25335199726840196 valid 0.18742756332669938
LOSS train 0.25335199726840196 valid 0.18700385276685683
LOSS train 0.25335199726840196 valid 0.18659473287648168
LOSS train 0.25335199726840196 valid 0.18701937617891926
LOSS train 0.25335199726840196 valid 0.18719814295570056
LOSS train 0.25335199726840196 valid 0.1869213314818554
LOSS train 0.25335199726840196 valid 0.18745626340950688
LOSS train 0.25335199726840196 valid 0.18715946305365788
LOSS train 0.25335199726840196 valid 0.18806802900508046
LOSS train 0.25335199726840196 valid 0.18822234891928158
LOSS train 0.25335199726840196 valid 0.18787296071197046
LOSS train 0.25335199726840196 valid 0.187134585496205
LOSS train 0.25335199726840196 valid 0.18716779833330827
LOSS train 0.25335199726840196 valid 0.18683650053065756
LOSS train 0.25335199726840196 valid 0.1868377170392445
LOSS train 0.25335199726840196 valid 0.18665751625954266
LOSS train 0.25335199726840196 valid 0.18698126781317922
LOSS train 0.25335199726840196 valid 0.1867163942693031
LOSS train 0.25335199726840196 valid 0.18695739112995766
LOSS train 0.25335199726840196 valid 0.1870512549082438
LOSS train 0.25335199726840196 valid 0.18720826310546776
LOSS train 0.25335199726840196 valid 0.1872943430751949
LOSS train 0.25335199726840196 valid 0.18759324115056258
LOSS train 0.25335199726840196 valid 0.18763965839826607
LOSS train 0.25335199726840196 valid 0.18706533070653678
LOSS train 0.25335199726840196 valid 0.18629984907162042
LOSS train 0.25335199726840196 valid 0.1867551565533731
LOSS train 0.25335199726840196 valid 0.18660653912159333
LOSS train 0.25335199726840196 valid 0.18645254611259415
LOSS train 0.25335199726840196 valid 0.18623744635020986
LOSS train 0.25335199726840196 valid 0.18585682885591373
LOSS train 0.25335199726840196 valid 0.1856634853215053
LOSS train 0.25335199726840196 valid 0.185409451411529
LOSS train 0.25335199726840196 valid 0.18602283851484233
LOSS train 0.25335199726840196 valid 0.18608641823132832
LOSS train 0.25335199726840196 valid 0.18605453267202274
LOSS train 0.25335199726840196 valid 0.18623697984477747
LOSS train 0.25335199726840196 valid 0.18620032777068435
LOSS train 0.25335199726840196 valid 0.18631543314203303
LOSS train 0.25335199726840196 valid 0.18639165962997237
LOSS train 0.25335199726840196 valid 0.18657196747759977
LOSS train 0.25335199726840196 valid 0.18654701282683106
LOSS train 0.25335199726840196 valid 0.18676359282464397
LOSS train 0.25335199726840196 valid 0.1868230700492859
LOSS train 0.25335199726840196 valid 0.1869893702864647
LOSS train 0.25335199726840196 valid 0.1872304105522609
LOSS train 0.25335199726840196 valid 0.18722834511130465
LOSS train 0.25335199726840196 valid 0.18703872023276913
LOSS train 0.25335199726840196 valid 0.18711737657968813
LOSS train 0.25335199726840196 valid 0.18722312024661472
LOSS train 0.25335199726840196 valid 0.18735965959868342
LOSS train 0.25335199726840196 valid 0.18728054280035963
LOSS train 0.25335199726840196 valid 0.18730244109476055
LOSS train 0.25335199726840196 valid 0.18756970038654608
LOSS train 0.25335199726840196 valid 0.18780896839770403
LOSS train 0.25335199726840196 valid 0.18738826075652698
LOSS train 0.25335199726840196 valid 0.18732282718909637
LOSS train 0.25335199726840196 valid 0.18735251925160398
LOSS train 0.25335199726840196 valid 0.18731655677159628
LOSS train 0.25335199726840196 valid 0.18740922186685646
LOSS train 0.25335199726840196 valid 0.18733022644602018
LOSS train 0.25335199726840196 valid 0.18744584153860044
LOSS train 0.25335199726840196 valid 0.1873445618203131
LOSS train 0.25335199726840196 valid 0.18730596947569808
LOSS train 0.25335199726840196 valid 0.187125318000714
LOSS train 0.25335199726840196 valid 0.18710579433717017
LOSS train 0.25335199726840196 valid 0.18700257688760757
LOSS train 0.25335199726840196 valid 0.18694814506585036
LOSS train 0.25335199726840196 valid 0.18712968595566287
LOSS train 0.25335199726840196 valid 0.18709783935546875
LOSS train 0.25335199726840196 valid 0.18742516814243226
LOSS train 0.25335199726840196 valid 0.18735009420105792
LOSS train 0.25335199726840196 valid 0.18765902903396636
LOSS train 0.25335199726840196 valid 0.18773618987364363
LOSS train 0.25335199726840196 valid 0.18764833773557957
LOSS train 0.25335199726840196 valid 0.18767243430359673
LOSS train 0.25335199726840196 valid 0.1875303059139035
LOSS train 0.25335199726840196 valid 0.1874644479581288
LOSS train 0.25335199726840196 valid 0.18756489055370218
LOSS train 0.25335199726840196 valid 0.18748632316236144
LOSS train 0.25335199726840196 valid 0.1875061554943814
LOSS train 0.25335199726840196 valid 0.18739553383232033
LOSS train 0.25335199726840196 valid 0.18741368891104407
LOSS train 0.25335199726840196 valid 0.18733513623261625
LOSS train 0.25335199726840196 valid 0.18741851832185472
LOSS train 0.25335199726840196 valid 0.18744758003992393
LOSS train 0.25335199726840196 valid 0.18768690830804932
LOSS train 0.25335199726840196 valid 0.18765046442305292
LOSS train 0.25335199726840196 valid 0.18765500312050185
LOSS train 0.25335199726840196 valid 0.18741985456696872
LOSS train 0.25335199726840196 valid 0.18750392125077445
LOSS train 0.25335199726840196 valid 0.1874105526881964
LOSS train 0.25335199726840196 valid 0.18806388732549306
LOSS train 0.25335199726840196 valid 0.18821500621786053
LOSS train 0.25335199726840196 valid 0.1883011328180631
LOSS train 0.25335199726840196 valid 0.1883466491438695
LOSS train 0.25335199726840196 valid 0.18806362691286363
LOSS train 0.25335199726840196 valid 0.18822086451489942
LOSS train 0.25335199726840196 valid 0.18801540407267484
LOSS train 0.25335199726840196 valid 0.18801795715285885
LOSS train 0.25335199726840196 valid 0.188139716306558
LOSS train 0.25335199726840196 valid 0.18805019785264496
LOSS train 0.25335199726840196 valid 0.18814699927085562
LOSS train 0.25335199726840196 valid 0.1881927195584999
LOSS train 0.25335199726840196 valid 0.18818638259544967
LOSS train 0.25335199726840196 valid 0.18812559841212279
LOSS train 0.25335199726840196 valid 0.18807362268368402
LOSS train 0.25335199726840196 valid 0.1879675999009536
LOSS train 0.25335199726840196 valid 0.18775402109433964
LOSS train 0.25335199726840196 valid 0.18756911528832984
LOSS train 0.25335199726840196 valid 0.18764235653791084
LOSS train 0.25335199726840196 valid 0.18797424863912388
LOSS train 0.25335199726840196 valid 0.18781910286772818
LOSS train 0.25335199726840196 valid 0.18805230739554005
LOSS train 0.25335199726840196 valid 0.18814283872351928
LOSS train 0.25335199726840196 valid 0.1881555376345651
LOSS train 0.25335199726840196 valid 0.1880925840936428
LOSS train 0.25335199726840196 valid 0.1880318041826259
LOSS train 0.25335199726840196 valid 0.18804937506886735
LOSS train 0.25335199726840196 valid 0.18784193328448703
LOSS train 0.25335199726840196 valid 0.18784798275340686
LOSS train 0.25335199726840196 valid 0.1878027517748418
LOSS train 0.25335199726840196 valid 0.18801641656776494
LOSS train 0.25335199726840196 valid 0.18789782079571452
LOSS train 0.25335199726840196 valid 0.18784094634983275
LOSS train 0.25335199726840196 valid 0.1878578870335995
LOSS train 0.25335199726840196 valid 0.18786540037983065
LOSS train 0.25335199726840196 valid 0.18793768773639136
LOSS train 0.25335199726840196 valid 0.18791969679296017
LOSS train 0.25335199726840196 valid 0.18773365906766942
LOSS train 0.25335199726840196 valid 0.18770871816142912
LOSS train 0.25335199726840196 valid 0.1875782534081668
LOSS train 0.25335199726840196 valid 0.1875451554326301
LOSS train 0.25335199726840196 valid 0.18748839160121938
LOSS train 0.25335199726840196 valid 0.18758948547275442
LOSS train 0.25335199726840196 valid 0.18743594809976546
LOSS train 0.25335199726840196 valid 0.18750527873635292
LOSS train 0.25335199726840196 valid 0.18737029743626946
LOSS train 0.25335199726840196 valid 0.18735869981578945
LOSS train 0.25335199726840196 valid 0.18724829860222644
LOSS train 0.25335199726840196 valid 0.18723905831575394
LOSS train 0.25335199726840196 valid 0.1873942050080614
LOSS train 0.25335199726840196 valid 0.18731172014065464
LOSS train 0.25335199726840196 valid 0.18732909109424706
LOSS train 0.25335199726840196 valid 0.18720280848443507
LOSS train 0.25335199726840196 valid 0.18709308628122606
LOSS train 0.25335199726840196 valid 0.18704493572511297
LOSS train 0.25335199726840196 valid 0.18691293777797022
LOSS train 0.25335199726840196 valid 0.18685411019068138
LOSS train 0.25335199726840196 valid 0.18662045779751568
LOSS train 0.25335199726840196 valid 0.18665933370300866
LOSS train 0.25335199726840196 valid 0.18656724353055448
LOSS train 0.25335199726840196 valid 0.1864376965766916
LOSS train 0.25335199726840196 valid 0.18629024530711927
LOSS train 0.25335199726840196 valid 0.18634457141160965
LOSS train 0.25335199726840196 valid 0.186506818256107
LOSS train 0.25335199726840196 valid 0.1865178885746677
LOSS train 0.25335199726840196 valid 0.18660835879789273
LOSS train 0.25335199726840196 valid 0.18657891118080816
LOSS train 0.25335199726840196 valid 0.18645303353320722
LOSS train 0.25335199726840196 valid 0.18646227292440556
LOSS train 0.25335199726840196 valid 0.1864761167682261
LOSS train 0.25335199726840196 valid 0.18657081369139733
LOSS train 0.25335199726840196 valid 0.18669760662671095
LOSS train 0.25335199726840196 valid 0.1867640540681102
LOSS train 0.25335199726840196 valid 0.1867656318715255
LOSS train 0.25335199726840196 valid 0.18671135584244858
LOSS train 0.25335199726840196 valid 0.18677066467000764
LOSS train 0.25335199726840196 valid 0.18680462021646754
LOSS train 0.25335199726840196 valid 0.18681301467948488
LOSS train 0.25335199726840196 valid 0.18686432688109642
LOSS train 0.25335199726840196 valid 0.18710754732991106
LOSS train 0.25335199726840196 valid 0.18716564970581154
LOSS train 0.25335199726840196 valid 0.18721040718940668
LOSS train 0.25335199726840196 valid 0.1872347599786261
LOSS train 0.25335199726840196 valid 0.18724149181729272
LOSS train 0.25335199726840196 valid 0.1872781702670558
LOSS train 0.25335199726840196 valid 0.18727661515765948
LOSS train 0.25335199726840196 valid 0.18730038092431858
LOSS train 0.25335199726840196 valid 0.18741923057018442
LOSS train 0.25335199726840196 valid 0.1872922491471646
LOSS train 0.25335199726840196 valid 0.18737027099363915
LOSS train 0.25335199726840196 valid 0.18740110863156678
LOSS train 0.25335199726840196 valid 0.18727668072389259
LOSS train 0.25335199726840196 valid 0.18720944623152416
LOSS train 0.25335199726840196 valid 0.1872570565998307
LOSS train 0.25335199726840196 valid 0.18709260748683915
LOSS train 0.25335199726840196 valid 0.18719716842282455
LOSS train 0.25335199726840196 valid 0.1874451325198666
LOSS train 0.25335199726840196 valid 0.187582585641316
LOSS train 0.25335199726840196 valid 0.187549655515004
LOSS train 0.25335199726840196 valid 0.187632060364673
LOSS train 0.25335199726840196 valid 0.18760931443783543
LOSS train 0.25335199726840196 valid 0.18757320982385353
LOSS train 0.25335199726840196 valid 0.18773439645767212
LOSS train 0.25335199726840196 valid 0.1877002171073777
LOSS train 0.25335199726840196 valid 0.18783069254150467
LOSS train 0.25335199726840196 valid 0.18774363333764282
LOSS train 0.25335199726840196 valid 0.1876883046833549
LOSS train 0.25335199726840196 valid 0.18772429841406205
LOSS train 0.25335199726840196 valid 0.1877406758721918
LOSS train 0.25335199726840196 valid 0.18766608068915192
LOSS train 0.25335199726840196 valid 0.18768669139276178
LOSS train 0.25335199726840196 valid 0.18769275399938973
LOSS train 0.25335199726840196 valid 0.1876597214776736
LOSS train 0.25335199726840196 valid 0.18778454149819881
LOSS train 0.25335199726840196 valid 0.1877969487477805
LOSS train 0.25335199726840196 valid 0.1878269128359769
LOSS train 0.25335199726840196 valid 0.18785786916586486
LOSS train 0.25335199726840196 valid 0.18780477508059087
LOSS train 0.25335199726840196 valid 0.18789942310492797
LOSS train 0.25335199726840196 valid 0.18794309852944777
LOSS train 0.25335199726840196 valid 0.18808321660356736
LOSS train 0.25335199726840196 valid 0.1880917150845758
LOSS train 0.25335199726840196 valid 0.1880679761921918
LOSS train 0.25335199726840196 valid 0.18816291131216661
LOSS train 0.25335199726840196 valid 0.1883509983034695
LOSS train 0.25335199726840196 valid 0.18838704964180134
LOSS train 0.25335199726840196 valid 0.188335101643618
LOSS train 0.25335199726840196 valid 0.1882956110347401
LOSS train 0.25335199726840196 valid 0.1881839233464089
LOSS train 0.25335199726840196 valid 0.18805271500069312
LOSS train 0.25335199726840196 valid 0.18796940480204796
LOSS train 0.25335199726840196 valid 0.18793985728294618
LOSS train 0.25335199726840196 valid 0.18790671798799718
LOSS train 0.25335199726840196 valid 0.18776288983872777
LOSS train 0.25335199726840196 valid 0.18761595682049473
LOSS train 0.25335199726840196 valid 0.18754875817905467
LOSS train 0.25335199726840196 valid 0.18757491968047452
LOSS train 0.25335199726840196 valid 0.18764154105855707
LOSS train 0.25335199726840196 valid 0.18761567312937516
LOSS train 0.25335199726840196 valid 0.1875261503956459
LOSS train 0.25335199726840196 valid 0.18751678589938414
LOSS train 0.25335199726840196 valid 0.18753726218398467
LOSS train 0.25335199726840196 valid 0.18756474164025536
LOSS train 0.25335199726840196 valid 0.18751015619109176
LOSS train 0.25335199726840196 valid 0.18753993480581127
LOSS train 0.25335199726840196 valid 0.18753402509786976
LOSS train 0.25335199726840196 valid 0.18760996144645067
LOSS train 0.25335199726840196 valid 0.18765421928995746
LOSS train 0.25335199726840196 valid 0.18758012631253615
LOSS train 0.25335199726840196 valid 0.18763908633479365
LOSS train 0.25335199726840196 valid 0.18764215517764124
LOSS train 0.25335199726840196 valid 0.18764905322554917
LOSS train 0.25335199726840196 valid 0.18761838724215826
LOSS train 0.25335199726840196 valid 0.18764026873927575
LOSS train 0.25335199726840196 valid 0.18763920793075436
LOSS train 0.25335199726840196 valid 0.187697499252782
LOSS train 0.25335199726840196 valid 0.1876661219192963
LOSS train 0.25335199726840196 valid 0.18753019243967337
LOSS train 0.25335199726840196 valid 0.18747023707316593
LOSS train 0.25335199726840196 valid 0.18739002821693979
LOSS train 0.25335199726840196 valid 0.18733699948756727
LOSS train 0.25335199726840196 valid 0.1873196091466737
LOSS train 0.25335199726840196 valid 0.1873839155800881
LOSS train 0.25335199726840196 valid 0.18730452081781493
LOSS train 0.25335199726840196 valid 0.18726525474817324
LOSS train 0.25335199726840196 valid 0.1873027071499596
LOSS train 0.25335199726840196 valid 0.1872607872459539
LOSS train 0.25335199726840196 valid 0.1872401809881604
LOSS train 0.25335199726840196 valid 0.18719545016183128
LOSS train 0.25335199726840196 valid 0.18721903254181052
LOSS train 0.25335199726840196 valid 0.18721705801644414
LOSS train 0.25335199726840196 valid 0.1873436747206416
LOSS train 0.25335199726840196 valid 0.18728924314491452
LOSS train 0.25335199726840196 valid 0.1873949430534773
LOSS train 0.25335199726840196 valid 0.1873235201798611
LOSS train 0.25335199726840196 valid 0.1872208672964905
LOSS train 0.25335199726840196 valid 0.18720401722339935
LOSS train 0.25335199726840196 valid 0.18722533707435315
LOSS train 0.25335199726840196 valid 0.18728957608616426
LOSS train 0.25335199726840196 valid 0.18731033906841862
LOSS train 0.25335199726840196 valid 0.1873055740554885
LOSS train 0.25335199726840196 valid 0.18733261942319956
LOSS train 0.25335199726840196 valid 0.18734243259285435
LOSS train 0.25335199726840196 valid 0.18727255943498583
LOSS train 0.25335199726840196 valid 0.18718167915042624
LOSS train 0.25335199726840196 valid 0.18722029983460367
LOSS train 0.25335199726840196 valid 0.18726735579931808
LOSS train 0.25335199726840196 valid 0.1872333309098856
LOSS train 0.25335199726840196 valid 0.18715670988673255
LOSS train 0.25335199726840196 valid 0.18710994999026687
LOSS train 0.25335199726840196 valid 0.18706874593475162
LOSS train 0.25335199726840196 valid 0.18702578509451717
LOSS train 0.25335199726840196 valid 0.18703287340262356
LOSS train 0.25335199726840196 valid 0.18695063714232962
LOSS train 0.25335199726840196 valid 0.1869918642598286
LOSS train 0.25335199726840196 valid 0.18697704965623405
LOSS train 0.25335199726840196 valid 0.18710618864658268
LOSS train 0.25335199726840196 valid 0.18712980509668156
LOSS train 0.25335199726840196 valid 0.1871337856171448
LOSS train 0.25335199726840196 valid 0.1870387729890752
LOSS train 0.25335199726840196 valid 0.18692519327347307
LOSS train 0.25335199726840196 valid 0.1869629363617446
LOSS train 0.25335199726840196 valid 0.18690780196871076
LOSS train 0.25335199726840196 valid 0.18685923605902582
LOSS train 0.25335199726840196 valid 0.18682917685840617
LOSS train 0.25335199726840196 valid 0.18685967215566393
LOSS train 0.25335199726840196 valid 0.1868350265894906
LOSS train 0.25335199726840196 valid 0.1869310308510149
LOSS train 0.25335199726840196 valid 0.1869922016192688
LOSS train 0.25335199726840196 valid 0.1869499261806659
LOSS train 0.25335199726840196 valid 0.1869082744311354
LOSS train 0.25335199726840196 valid 0.18695330603209045
LOSS train 0.25335199726840196 valid 0.18689621976680226
LOSS train 0.25335199726840196 valid 0.18688808781948776
LOSS train 0.25335199726840196 valid 0.186967760983093
LOSS train 0.25335199726840196 valid 0.18685209841439218
LOSS train 0.25335199726840196 valid 0.18689022626686883
LOSS train 0.25335199726840196 valid 0.1869055449554365
LOSS train 0.25335199726840196 valid 0.18687004270286509
LOSS train 0.25335199726840196 valid 0.18675664997068347
LOSS train 0.25335199726840196 valid 0.18675632544023835
LOSS train 0.25335199726840196 valid 0.18685258331337595
EPOCH 12:
  batch 1 loss: 0.2615455090999603
  batch 2 loss: 0.25219375640153885
  batch 3 loss: 0.26314610739549
  batch 4 loss: 0.27216487005352974
  batch 5 loss: 0.27342699468135834
  batch 6 loss: 0.267986536026001
  batch 7 loss: 0.2684506731373923
  batch 8 loss: 0.26529197208583355
  batch 9 loss: 0.2645116034481261
  batch 10 loss: 0.2607152223587036
  batch 11 loss: 0.2622033709829504
  batch 12 loss: 0.26079510400692624
  batch 13 loss: 0.26135783241345334
  batch 14 loss: 0.26023428780691965
  batch 15 loss: 0.2619679272174835
  batch 16 loss: 0.26254186034202576
  batch 17 loss: 0.25997156255385456
  batch 18 loss: 0.26071493658754563
  batch 19 loss: 0.2605547497147008
  batch 20 loss: 0.2588066481053829
  batch 21 loss: 0.2597147247620991
  batch 22 loss: 0.258594925430688
  batch 23 loss: 0.2576310628134271
  batch 24 loss: 0.2557456338157256
  batch 25 loss: 0.2570263296365738
  batch 26 loss: 0.2550300428500542
  batch 27 loss: 0.2553920293295825
  batch 28 loss: 0.25415187062961714
  batch 29 loss: 0.25475486342249243
  batch 30 loss: 0.25423480570316315
  batch 31 loss: 0.25456707996706807
  batch 32 loss: 0.25417419662699103
  batch 33 loss: 0.254051433819713
  batch 34 loss: 0.2527836372747141
  batch 35 loss: 0.25289008574826377
  batch 36 loss: 0.2536649749510818
  batch 37 loss: 0.2535723701522157
  batch 38 loss: 0.2541801196179892
  batch 39 loss: 0.2536210860961523
  batch 40 loss: 0.25379709154367447
  batch 41 loss: 0.2534878595573146
  batch 42 loss: 0.25380346746671767
  batch 43 loss: 0.25409200926159703
  batch 44 loss: 0.2541310909119519
  batch 45 loss: 0.25354720817671883
  batch 46 loss: 0.2543189421943996
  batch 47 loss: 0.25412133653113184
  batch 48 loss: 0.2538586302349965
  batch 49 loss: 0.25375991694781247
  batch 50 loss: 0.25347783386707307
  batch 51 loss: 0.25320373183371975
  batch 52 loss: 0.25353854808669823
  batch 53 loss: 0.25280560890458664
  batch 54 loss: 0.2533042190803422
  batch 55 loss: 0.25296808129007164
  batch 56 loss: 0.2531020643987826
  batch 57 loss: 0.2530332121409868
  batch 58 loss: 0.2540064821469373
  batch 59 loss: 0.25426137826200257
  batch 60 loss: 0.25363222459952034
  batch 61 loss: 0.2544910902859735
  batch 62 loss: 0.25503222548192545
  batch 63 loss: 0.25436214747883024
  batch 64 loss: 0.25544075993821025
  batch 65 loss: 0.25539819919146023
  batch 66 loss: 0.2553003730647492
  batch 67 loss: 0.256386665488357
  batch 68 loss: 0.25671858669203873
  batch 69 loss: 0.25681379545426025
  batch 70 loss: 0.257613430917263
  batch 71 loss: 0.25721202035185314
  batch 72 loss: 0.25721233317421544
  batch 73 loss: 0.2573004632371746
  batch 74 loss: 0.2570047346321312
  batch 75 loss: 0.25652733504772185
  batch 76 loss: 0.2569538058811112
  batch 77 loss: 0.2563943551345305
  batch 78 loss: 0.2563967290215003
  batch 79 loss: 0.2565655521576918
  batch 80 loss: 0.25618514493107797
  batch 81 loss: 0.2562482390138838
  batch 82 loss: 0.2569995537036803
  batch 83 loss: 0.2572015272565635
  batch 84 loss: 0.2568533413466953
  batch 85 loss: 0.2563291972174364
  batch 86 loss: 0.2568005969704584
  batch 87 loss: 0.25695173271086025
  batch 88 loss: 0.25680962137200614
  batch 89 loss: 0.2564818052093634
  batch 90 loss: 0.25642171965705024
  batch 91 loss: 0.25683985568664885
  batch 92 loss: 0.25657323569707247
  batch 93 loss: 0.25622015101935275
  batch 94 loss: 0.2564574619556995
  batch 95 loss: 0.25630367162980533
  batch 96 loss: 0.2565288625967999
  batch 97 loss: 0.25685933094049235
  batch 98 loss: 0.2575475867001378
  batch 99 loss: 0.25796286189796946
  batch 100 loss: 0.25775965228676795
  batch 101 loss: 0.2580145410086849
  batch 102 loss: 0.25846511753750784
  batch 103 loss: 0.2589629256320231
  batch 104 loss: 0.25921106725358045
  batch 105 loss: 0.25911197747503006
  batch 106 loss: 0.25937276966166944
  batch 107 loss: 0.2591787047475298
  batch 108 loss: 0.2590662413449199
  batch 109 loss: 0.2588592286503643
  batch 110 loss: 0.25887313376773485
  batch 111 loss: 0.2584552324569977
  batch 112 loss: 0.25831535804484573
  batch 113 loss: 0.2583253805616261
  batch 114 loss: 0.25842876104932083
  batch 115 loss: 0.2587147305841031
  batch 116 loss: 0.25892328034187184
  batch 117 loss: 0.25912767903417605
  batch 118 loss: 0.25871746242046356
  batch 119 loss: 0.25906633328990775
  batch 120 loss: 0.2587383156021436
  batch 121 loss: 0.25842344797839806
  batch 122 loss: 0.2584391388981069
  batch 123 loss: 0.2583713023885479
  batch 124 loss: 0.2586312165423747
  batch 125 loss: 0.25859129774570466
  batch 126 loss: 0.2584713845262452
  batch 127 loss: 0.2590102993362532
  batch 128 loss: 0.2588336810003966
  batch 129 loss: 0.25892641747644707
  batch 130 loss: 0.25868765952495426
  batch 131 loss: 0.258609497820148
  batch 132 loss: 0.2584355282738353
  batch 133 loss: 0.25872070571981876
  batch 134 loss: 0.25875157418090905
  batch 135 loss: 0.2582938234011332
  batch 136 loss: 0.25830145574667873
  batch 137 loss: 0.25831171739710507
  batch 138 loss: 0.2582389305251232
  batch 139 loss: 0.25843183117375956
  batch 140 loss: 0.25815232619643214
  batch 141 loss: 0.2585572931149327
  batch 142 loss: 0.25855229217821446
  batch 143 loss: 0.2583185030030204
  batch 144 loss: 0.2583894706848595
  batch 145 loss: 0.2582667081520475
  batch 146 loss: 0.2582435234360499
  batch 147 loss: 0.25850964525118975
  batch 148 loss: 0.2586227629233051
  batch 149 loss: 0.25848919243220514
  batch 150 loss: 0.25849002609650296
  batch 151 loss: 0.25849797226340565
  batch 152 loss: 0.2585954280864251
  batch 153 loss: 0.2585223306433048
  batch 154 loss: 0.2585926577448845
  batch 155 loss: 0.2585168801007732
  batch 156 loss: 0.2584677937512214
  batch 157 loss: 0.25864917979498575
  batch 158 loss: 0.25868769716235657
  batch 159 loss: 0.25868636660230987
  batch 160 loss: 0.258519680891186
  batch 161 loss: 0.25861772922625453
  batch 162 loss: 0.25854971416202593
  batch 163 loss: 0.258654761168123
  batch 164 loss: 0.25834330171346664
  batch 165 loss: 0.2582772827509678
  batch 166 loss: 0.25814479086772507
  batch 167 loss: 0.25807351418241053
  batch 168 loss: 0.2579973686841272
  batch 169 loss: 0.2577772613989531
  batch 170 loss: 0.257768155809711
  batch 171 loss: 0.2578641257265158
  batch 172 loss: 0.25802640850807346
  batch 173 loss: 0.25806176378203266
  batch 174 loss: 0.2578815382787551
  batch 175 loss: 0.2579612783023289
  batch 176 loss: 0.25776159670203924
  batch 177 loss: 0.25787167705721775
  batch 178 loss: 0.2580011742336027
  batch 179 loss: 0.25809817781994465
  batch 180 loss: 0.25800082956751186
  batch 181 loss: 0.2580760492639647
  batch 182 loss: 0.25805417595656366
  batch 183 loss: 0.25785701155988244
  batch 184 loss: 0.25774150155484676
  batch 185 loss: 0.25769281653133597
  batch 186 loss: 0.2577891414684634
  batch 187 loss: 0.25768967817492666
  batch 188 loss: 0.2573763738445779
  batch 189 loss: 0.25710180756591616
  batch 190 loss: 0.257182215154171
  batch 191 loss: 0.2570681201381833
  batch 192 loss: 0.25703372131101787
  batch 193 loss: 0.25693551161437456
  batch 194 loss: 0.2571481351846272
  batch 195 loss: 0.2572567697518911
  batch 196 loss: 0.25705508530444027
  batch 197 loss: 0.2569739075784151
  batch 198 loss: 0.2572997198863463
  batch 199 loss: 0.2572389194863525
  batch 200 loss: 0.2573234757035971
  batch 201 loss: 0.2574824405398535
  batch 202 loss: 0.2576102325969403
  batch 203 loss: 0.2576260013973772
  batch 204 loss: 0.2574971143667604
  batch 205 loss: 0.25751443788772677
  batch 206 loss: 0.25746128643022
  batch 207 loss: 0.2575086625877786
  batch 208 loss: 0.2572927806669703
  batch 209 loss: 0.2569382003192126
  batch 210 loss: 0.25700239276602155
  batch 211 loss: 0.25687049527021383
  batch 212 loss: 0.2567300369857617
  batch 213 loss: 0.25664406769992043
  batch 214 loss: 0.25664684963282025
  batch 215 loss: 0.2564619685328284
  batch 216 loss: 0.2561443735741907
  batch 217 loss: 0.2561047227563946
  batch 218 loss: 0.2560709490688569
  batch 219 loss: 0.25613394107448456
  batch 220 loss: 0.2561195190657269
  batch 221 loss: 0.2561766188338871
  batch 222 loss: 0.25623870513460656
  batch 223 loss: 0.25622928743939766
  batch 224 loss: 0.2562513656115958
  batch 225 loss: 0.256093761589792
  batch 226 loss: 0.2560924536227125
  batch 227 loss: 0.2559576540660228
  batch 228 loss: 0.25576298213318777
  batch 229 loss: 0.25569969363608214
  batch 230 loss: 0.25569826209026836
  batch 231 loss: 0.25558798189287063
  batch 232 loss: 0.2554231855119097
  batch 233 loss: 0.2553728351393483
  batch 234 loss: 0.2552965123556618
  batch 235 loss: 0.2552048837884944
  batch 236 loss: 0.25518285318956535
  batch 237 loss: 0.25529979326553986
  batch 238 loss: 0.2552140000738016
  batch 239 loss: 0.25514484867141835
  batch 240 loss: 0.2552091106151541
  batch 241 loss: 0.2554230867456104
  batch 242 loss: 0.2552923336625099
  batch 243 loss: 0.2554110400838616
  batch 244 loss: 0.2555387825628773
  batch 245 loss: 0.25550262149499386
  batch 246 loss: 0.25545348081646896
  batch 247 loss: 0.2555721466116577
  batch 248 loss: 0.2557504227565181
  batch 249 loss: 0.25561493180363054
  batch 250 loss: 0.25548998296260833
  batch 251 loss: 0.25568348822365716
  batch 252 loss: 0.2555553578431644
  batch 253 loss: 0.25547350100848987
  batch 254 loss: 0.2553186788568346
  batch 255 loss: 0.2554243466433357
  batch 256 loss: 0.2556507339468226
  batch 257 loss: 0.2557199085964767
  batch 258 loss: 0.25565685170103414
  batch 259 loss: 0.2558114457314539
  batch 260 loss: 0.2560594659585219
  batch 261 loss: 0.2560980166968715
  batch 262 loss: 0.2560525657111452
  batch 263 loss: 0.2560411408373611
  batch 264 loss: 0.25604397464882245
  batch 265 loss: 0.2560708370973479
  batch 266 loss: 0.25606223795198857
  batch 267 loss: 0.25609497541791937
  batch 268 loss: 0.25602667383960825
  batch 269 loss: 0.25606383316800496
  batch 270 loss: 0.2562453728583124
  batch 271 loss: 0.2563716312286159
  batch 272 loss: 0.2564053740900229
  batch 273 loss: 0.256261612087379
  batch 274 loss: 0.2562883782887111
  batch 275 loss: 0.2563422411680222
  batch 276 loss: 0.256290420403947
  batch 277 loss: 0.2562457418291147
  batch 278 loss: 0.2562171660310073
  batch 279 loss: 0.2562921428552238
  batch 280 loss: 0.2562564239970275
  batch 281 loss: 0.2562109038392844
  batch 282 loss: 0.25617483998021334
  batch 283 loss: 0.2562277182252163
  batch 284 loss: 0.2562636745647645
  batch 285 loss: 0.2562115231626912
  batch 286 loss: 0.25626596209260966
  batch 287 loss: 0.25635959862208946
  batch 288 loss: 0.2562649694478346
  batch 289 loss: 0.25647943196205947
  batch 290 loss: 0.2563064181599124
  batch 291 loss: 0.25635801086720733
  batch 292 loss: 0.2564520761370659
  batch 293 loss: 0.2564313983347636
  batch 294 loss: 0.2563679361221742
  batch 295 loss: 0.2563615764601756
  batch 296 loss: 0.2564424847227496
  batch 297 loss: 0.25640813113262356
  batch 298 loss: 0.2563906546507106
  batch 299 loss: 0.2562683239629037
  batch 300 loss: 0.2562469916045666
  batch 301 loss: 0.25628792690082247
  batch 302 loss: 0.2562133243048428
  batch 303 loss: 0.25610542479324655
  batch 304 loss: 0.25606761446320697
  batch 305 loss: 0.2559016683551132
  batch 306 loss: 0.256030177027961
  batch 307 loss: 0.2560114622213165
  batch 308 loss: 0.25606274803156975
  batch 309 loss: 0.25604252213413276
  batch 310 loss: 0.2560642728882451
  batch 311 loss: 0.25613519702693655
  batch 312 loss: 0.2563227061659862
  batch 313 loss: 0.2563336985751082
  batch 314 loss: 0.25625461226056334
  batch 315 loss: 0.25622863419472225
  batch 316 loss: 0.25620184244611593
  batch 317 loss: 0.25610204158521227
  batch 318 loss: 0.2560016536768877
  batch 319 loss: 0.2559033474858652
  batch 320 loss: 0.255833679670468
  batch 321 loss: 0.25589379245804106
  batch 322 loss: 0.2558462057979951
  batch 323 loss: 0.25576166688657764
  batch 324 loss: 0.2556320831271601
  batch 325 loss: 0.25553820339533
  batch 326 loss: 0.25551335125064556
  batch 327 loss: 0.255532304673749
  batch 328 loss: 0.2553830693316896
  batch 329 loss: 0.2554277266321936
  batch 330 loss: 0.2553526890548793
  batch 331 loss: 0.2553463244636253
  batch 332 loss: 0.2552326367831374
  batch 333 loss: 0.2552726270110757
  batch 334 loss: 0.2551383090144146
  batch 335 loss: 0.2549467766018056
  batch 336 loss: 0.2548607265399325
  batch 337 loss: 0.2547863745724766
  batch 338 loss: 0.2547810604939094
  batch 339 loss: 0.2547756969752917
  batch 340 loss: 0.2547039740225848
  batch 341 loss: 0.2547082132782754
  batch 342 loss: 0.2545776292681694
  batch 343 loss: 0.25453993208901876
  batch 344 loss: 0.25445023711857406
  batch 345 loss: 0.2545146288647168
  batch 346 loss: 0.25437820737244765
  batch 347 loss: 0.2543067333935317
  batch 348 loss: 0.2542532527840686
  batch 349 loss: 0.25414516091517525
  batch 350 loss: 0.254136155630861
  batch 351 loss: 0.25410393555449623
  batch 352 loss: 0.25405727504667913
  batch 353 loss: 0.2539969373694898
  batch 354 loss: 0.25409397322916044
  batch 355 loss: 0.2540298607147915
  batch 356 loss: 0.2539942480121436
  batch 357 loss: 0.25389112356831045
  batch 358 loss: 0.2538979548399009
  batch 359 loss: 0.25381901569685233
  batch 360 loss: 0.2537419287694825
  batch 361 loss: 0.2537050905980562
  batch 362 loss: 0.2535730334441306
  batch 363 loss: 0.25341026948831624
  batch 364 loss: 0.2532315661946496
  batch 365 loss: 0.2531723148610494
  batch 366 loss: 0.25309928942247817
  batch 367 loss: 0.2529983968760727
  batch 368 loss: 0.252844141755739
  batch 369 loss: 0.2528039611128934
  batch 370 loss: 0.25271751691360733
  batch 371 loss: 0.25277440892557573
  batch 372 loss: 0.2527058404539862
  batch 373 loss: 0.2526553001544431
  batch 374 loss: 0.25257646989694893
  batch 375 loss: 0.252553196310997
  batch 376 loss: 0.25263059143214783
  batch 377 loss: 0.2525995846452384
  batch 378 loss: 0.25263860120029047
  batch 379 loss: 0.25258913895700097
  batch 380 loss: 0.25264520629456166
  batch 381 loss: 0.25264821340405724
  batch 382 loss: 0.25271753576725564
  batch 383 loss: 0.2526912109114171
  batch 384 loss: 0.25262773025315255
  batch 385 loss: 0.2527222578014646
  batch 386 loss: 0.25277915183898697
  batch 387 loss: 0.2528695323593549
  batch 388 loss: 0.2529049485384189
  batch 389 loss: 0.25291512554133155
  batch 390 loss: 0.252857557282998
  batch 391 loss: 0.2529347303806973
  batch 392 loss: 0.2528375596644319
  batch 393 loss: 0.25273841858365154
  batch 394 loss: 0.25270164739056894
  batch 395 loss: 0.2526627346684661
  batch 396 loss: 0.25261383311766566
  batch 397 loss: 0.2527100749745477
  batch 398 loss: 0.25266344871503027
  batch 399 loss: 0.252749258974441
  batch 400 loss: 0.2527623376622796
  batch 401 loss: 0.25274185977523167
  batch 402 loss: 0.2527115303114872
  batch 403 loss: 0.25276952105627465
  batch 404 loss: 0.2528416038223422
  batch 405 loss: 0.2528860149928081
  batch 406 loss: 0.2530041008527056
  batch 407 loss: 0.25295369803026796
  batch 408 loss: 0.2530768945009685
  batch 409 loss: 0.2531002315813288
  batch 410 loss: 0.25325240401960003
  batch 411 loss: 0.2532471318677104
  batch 412 loss: 0.25322930279721334
  batch 413 loss: 0.2533392629453114
  batch 414 loss: 0.2533697985868523
  batch 415 loss: 0.25335571331432066
  batch 416 loss: 0.2534586329801151
  batch 417 loss: 0.25347146231064693
  batch 418 loss: 0.25348355015071383
  batch 419 loss: 0.25348382907295
  batch 420 loss: 0.2534595790718283
  batch 421 loss: 0.25334916148786024
  batch 422 loss: 0.25343261489653474
  batch 423 loss: 0.25338132161620663
  batch 424 loss: 0.253324668995052
  batch 425 loss: 0.25327372154768774
  batch 426 loss: 0.25323845925325517
  batch 427 loss: 0.2532085764812921
  batch 428 loss: 0.25315960416587713
  batch 429 loss: 0.2531941461882669
  batch 430 loss: 0.25309243254190267
  batch 431 loss: 0.2531188567041521
  batch 432 loss: 0.2531427455790065
  batch 433 loss: 0.2531106373592007
  batch 434 loss: 0.25316183612368626
  batch 435 loss: 0.2531404245858905
  batch 436 loss: 0.25312322805370757
  batch 437 loss: 0.25315818118967237
  batch 438 loss: 0.2532382624130271
  batch 439 loss: 0.25317788782586814
  batch 440 loss: 0.2532355936413461
  batch 441 loss: 0.2531128884240343
  batch 442 loss: 0.253010793407848
  batch 443 loss: 0.25300792037902514
  batch 444 loss: 0.25293207819665875
  batch 445 loss: 0.25291088263640243
  batch 446 loss: 0.2529088040905683
  batch 447 loss: 0.25281224614821823
  batch 448 loss: 0.25282652157225777
  batch 449 loss: 0.25287278524220386
  batch 450 loss: 0.2528260287311342
  batch 451 loss: 0.2527790193150683
  batch 452 loss: 0.2528700339978775
  batch 453 loss: 0.25296314610024423
  batch 454 loss: 0.25301186635368195
  batch 455 loss: 0.2530855453931368
  batch 456 loss: 0.25310794584322394
  batch 457 loss: 0.2530644249081351
  batch 458 loss: 0.2530089450194846
  batch 459 loss: 0.253053622373049
  batch 460 loss: 0.25310323400341944
  batch 461 loss: 0.2530716413272436
  batch 462 loss: 0.2530294544833563
  batch 463 loss: 0.25290686297622667
  batch 464 loss: 0.2528855612758419
  batch 465 loss: 0.2527141802413489
  batch 466 loss: 0.2525714482360643
  batch 467 loss: 0.2527377676912753
  batch 468 loss: 0.2527392488768977
  batch 469 loss: 0.2528988063843774
  batch 470 loss: 0.25286012139726194
  batch 471 loss: 0.2529623788126968
  batch 472 loss: 0.25283208258955153
LOSS train 0.25283208258955153 valid 0.21572338044643402
LOSS train 0.25283208258955153 valid 0.208718441426754
LOSS train 0.25283208258955153 valid 0.2033932457367579
LOSS train 0.25283208258955153 valid 0.201721403747797
LOSS train 0.25283208258955153 valid 0.19884397983551025
LOSS train 0.25283208258955153 valid 0.2033243477344513
LOSS train 0.25283208258955153 valid 0.21523868611880712
LOSS train 0.25283208258955153 valid 0.21357494592666626
LOSS train 0.25283208258955153 valid 0.21187500324514177
LOSS train 0.25283208258955153 valid 0.2121073856949806
LOSS train 0.25283208258955153 valid 0.2110093032771891
LOSS train 0.25283208258955153 valid 0.21152983357508978
LOSS train 0.25283208258955153 valid 0.20949914019841415
LOSS train 0.25283208258955153 valid 0.2100842765399388
LOSS train 0.25283208258955153 valid 0.20587484737237294
LOSS train 0.25283208258955153 valid 0.20671625807881355
LOSS train 0.25283208258955153 valid 0.2077083219500149
LOSS train 0.25283208258955153 valid 0.20898030781083637
LOSS train 0.25283208258955153 valid 0.2104045795766931
LOSS train 0.25283208258955153 valid 0.2091163918375969
LOSS train 0.25283208258955153 valid 0.2085613360007604
LOSS train 0.25283208258955153 valid 0.2070254304192283
LOSS train 0.25283208258955153 valid 0.20758133673149606
LOSS train 0.25283208258955153 valid 0.20635841228067875
LOSS train 0.25283208258955153 valid 0.20577029943466185
LOSS train 0.25283208258955153 valid 0.2057398185133934
LOSS train 0.25283208258955153 valid 0.20630207933761455
LOSS train 0.25283208258955153 valid 0.20636373971189773
LOSS train 0.25283208258955153 valid 0.2059007867656905
LOSS train 0.25283208258955153 valid 0.20675159245729446
LOSS train 0.25283208258955153 valid 0.2075459106314567
LOSS train 0.25283208258955153 valid 0.20717583782970905
LOSS train 0.25283208258955153 valid 0.2079501450061798
LOSS train 0.25283208258955153 valid 0.20755967892268123
LOSS train 0.25283208258955153 valid 0.20874955185822078
LOSS train 0.25283208258955153 valid 0.20855334773659706
LOSS train 0.25283208258955153 valid 0.20869459131279508
LOSS train 0.25283208258955153 valid 0.21022449943580127
LOSS train 0.25283208258955153 valid 0.21056643204811293
LOSS train 0.25283208258955153 valid 0.21052752435207367
LOSS train 0.25283208258955153 valid 0.21135051584825282
LOSS train 0.25283208258955153 valid 0.21153151988983154
LOSS train 0.25283208258955153 valid 0.21092598798663117
LOSS train 0.25283208258955153 valid 0.2112015953118151
LOSS train 0.25283208258955153 valid 0.21089308195643955
LOSS train 0.25283208258955153 valid 0.21127637119396872
LOSS train 0.25283208258955153 valid 0.21217658862154534
LOSS train 0.25283208258955153 valid 0.21198799399038157
LOSS train 0.25283208258955153 valid 0.2123367956706456
LOSS train 0.25283208258955153 valid 0.21142832159996033
LOSS train 0.25283208258955153 valid 0.21149907567921808
LOSS train 0.25283208258955153 valid 0.2111475940507192
LOSS train 0.25283208258955153 valid 0.21151029613782774
LOSS train 0.25283208258955153 valid 0.21153822955158022
LOSS train 0.25283208258955153 valid 0.21120992980220102
LOSS train 0.25283208258955153 valid 0.21046840851860388
LOSS train 0.25283208258955153 valid 0.20996821841649843
LOSS train 0.25283208258955153 valid 0.20950097359459977
LOSS train 0.25283208258955153 valid 0.2100362424123085
LOSS train 0.25283208258955153 valid 0.21022413472334545
LOSS train 0.25283208258955153 valid 0.20988059898868935
LOSS train 0.25283208258955153 valid 0.2104933966071375
LOSS train 0.25283208258955153 valid 0.2103478858868281
LOSS train 0.25283208258955153 valid 0.21151624456979334
LOSS train 0.25283208258955153 valid 0.21178664954809043
LOSS train 0.25283208258955153 valid 0.21162555050669293
LOSS train 0.25283208258955153 valid 0.2109399259980045
LOSS train 0.25283208258955153 valid 0.21096810479374492
LOSS train 0.25283208258955153 valid 0.21054178821867792
LOSS train 0.25283208258955153 valid 0.21061476128441947
LOSS train 0.25283208258955153 valid 0.21039002609085028
LOSS train 0.25283208258955153 valid 0.21069214223987526
LOSS train 0.25283208258955153 valid 0.2104868766379683
LOSS train 0.25283208258955153 valid 0.21068410736483498
LOSS train 0.25283208258955153 valid 0.21085300743579866
LOSS train 0.25283208258955153 valid 0.21117379026193367
LOSS train 0.25283208258955153 valid 0.21115623356459975
LOSS train 0.25283208258955153 valid 0.2114083428795521
LOSS train 0.25283208258955153 valid 0.21159817146349558
LOSS train 0.25283208258955153 valid 0.21091207824647426
LOSS train 0.25283208258955153 valid 0.2100891062506923
LOSS train 0.25283208258955153 valid 0.21057782522061977
LOSS train 0.25283208258955153 valid 0.21044146027191576
LOSS train 0.25283208258955153 valid 0.21033090406230517
LOSS train 0.25283208258955153 valid 0.2101373718065374
LOSS train 0.25283208258955153 valid 0.20971277321493903
LOSS train 0.25283208258955153 valid 0.20952458508398342
LOSS train 0.25283208258955153 valid 0.20925276811149987
LOSS train 0.25283208258955153 valid 0.20981482692648856
LOSS train 0.25283208258955153 valid 0.20990124858087963
LOSS train 0.25283208258955153 valid 0.20987552109655444
LOSS train 0.25283208258955153 valid 0.21004282715527908
LOSS train 0.25283208258955153 valid 0.20996178814800837
LOSS train 0.25283208258955153 valid 0.21004105042269888
LOSS train 0.25283208258955153 valid 0.21011889184776106
LOSS train 0.25283208258955153 valid 0.21026694029569626
LOSS train 0.25283208258955153 valid 0.21020893307076288
LOSS train 0.25283208258955153 valid 0.2104450775956621
LOSS train 0.25283208258955153 valid 0.21054108741909566
LOSS train 0.25283208258955153 valid 0.2106623648107052
LOSS train 0.25283208258955153 valid 0.2108674896235513
LOSS train 0.25283208258955153 valid 0.21089417575036778
LOSS train 0.25283208258955153 valid 0.21075945571788307
LOSS train 0.25283208258955153 valid 0.2107586763226069
LOSS train 0.25283208258955153 valid 0.21087048678171066
LOSS train 0.25283208258955153 valid 0.2109717161587949
LOSS train 0.25283208258955153 valid 0.21080736551329354
LOSS train 0.25283208258955153 valid 0.21088051451025186
LOSS train 0.25283208258955153 valid 0.21128715602082943
LOSS train 0.25283208258955153 valid 0.2116502354090864
LOSS train 0.25283208258955153 valid 0.2112138155881349
LOSS train 0.25283208258955153 valid 0.21124185089554107
LOSS train 0.25283208258955153 valid 0.21125935848835295
LOSS train 0.25283208258955153 valid 0.2110546330610911
LOSS train 0.25283208258955153 valid 0.21112833722777988
LOSS train 0.25283208258955153 valid 0.21098806709051132
LOSS train 0.25283208258955153 valid 0.21114036071504283
LOSS train 0.25283208258955153 valid 0.21104707947726978
LOSS train 0.25283208258955153 valid 0.21097883845076842
LOSS train 0.25283208258955153 valid 0.21076560181876022
LOSS train 0.25283208258955153 valid 0.21074469720036532
LOSS train 0.25283208258955153 valid 0.2105761028704096
LOSS train 0.25283208258955153 valid 0.21051339114584575
LOSS train 0.25283208258955153 valid 0.2108237000723039
LOSS train 0.25283208258955153 valid 0.21082527065277099
LOSS train 0.25283208258955153 valid 0.21107270521303964
LOSS train 0.25283208258955153 valid 0.2110109656579851
LOSS train 0.25283208258955153 valid 0.2113617033464834
LOSS train 0.25283208258955153 valid 0.21142915584320246
LOSS train 0.25283208258955153 valid 0.21123278049322275
LOSS train 0.25283208258955153 valid 0.21128663363802525
LOSS train 0.25283208258955153 valid 0.21116497932058392
LOSS train 0.25283208258955153 valid 0.21125356169571555
LOSS train 0.25283208258955153 valid 0.21140257080099475
LOSS train 0.25283208258955153 valid 0.21134608520401849
LOSS train 0.25283208258955153 valid 0.21129959668306744
LOSS train 0.25283208258955153 valid 0.21115680136819825
LOSS train 0.25283208258955153 valid 0.2111124505383381
LOSS train 0.25283208258955153 valid 0.2110367583499538
LOSS train 0.25283208258955153 valid 0.21115938659225192
LOSS train 0.25283208258955153 valid 0.21128176860775508
LOSS train 0.25283208258955153 valid 0.2115685484358962
LOSS train 0.25283208258955153 valid 0.21154370049496632
LOSS train 0.25283208258955153 valid 0.21154538811080986
LOSS train 0.25283208258955153 valid 0.21134367246052316
LOSS train 0.25283208258955153 valid 0.2114529661732177
LOSS train 0.25283208258955153 valid 0.21131713029478683
LOSS train 0.25283208258955153 valid 0.21208152859597593
LOSS train 0.25283208258955153 valid 0.21225409879780455
LOSS train 0.25283208258955153 valid 0.21233661532402037
LOSS train 0.25283208258955153 valid 0.21236652826631305
LOSS train 0.25283208258955153 valid 0.212097389133353
LOSS train 0.25283208258955153 valid 0.2122312343003703
LOSS train 0.25283208258955153 valid 0.21201540674869115
LOSS train 0.25283208258955153 valid 0.2120368210538741
LOSS train 0.25283208258955153 valid 0.21214005064505798
LOSS train 0.25283208258955153 valid 0.21202540672888423
LOSS train 0.25283208258955153 valid 0.2121335568873188
LOSS train 0.25283208258955153 valid 0.21214276936444096
LOSS train 0.25283208258955153 valid 0.2121312548406422
LOSS train 0.25283208258955153 valid 0.21205321278261102
LOSS train 0.25283208258955153 valid 0.21197167489631677
LOSS train 0.25283208258955153 valid 0.21187376482355083
LOSS train 0.25283208258955153 valid 0.21165576086538593
LOSS train 0.25283208258955153 valid 0.21146145002408462
LOSS train 0.25283208258955153 valid 0.21146068542477597
LOSS train 0.25283208258955153 valid 0.211805720261471
LOSS train 0.25283208258955153 valid 0.21162689423986844
LOSS train 0.25283208258955153 valid 0.21182568221402592
LOSS train 0.25283208258955153 valid 0.21194307918057723
LOSS train 0.25283208258955153 valid 0.21191850086750344
LOSS train 0.25283208258955153 valid 0.21180033519171004
LOSS train 0.25283208258955153 valid 0.21178007737405039
LOSS train 0.25283208258955153 valid 0.21177881870461607
LOSS train 0.25283208258955153 valid 0.21152324140071868
LOSS train 0.25283208258955153 valid 0.21157916525209491
LOSS train 0.25283208258955153 valid 0.21157871531901387
LOSS train 0.25283208258955153 valid 0.21178656364424844
LOSS train 0.25283208258955153 valid 0.21164678028841924
LOSS train 0.25283208258955153 valid 0.21156798725326856
LOSS train 0.25283208258955153 valid 0.21155957177857668
LOSS train 0.25283208258955153 valid 0.21158128821260327
LOSS train 0.25283208258955153 valid 0.211687013995452
LOSS train 0.25283208258955153 valid 0.2116943792966397
LOSS train 0.25283208258955153 valid 0.21148331326407355
LOSS train 0.25283208258955153 valid 0.21144122245811647
LOSS train 0.25283208258955153 valid 0.21129473796821532
LOSS train 0.25283208258955153 valid 0.2112667444062994
LOSS train 0.25283208258955153 valid 0.21121688124048646
LOSS train 0.25283208258955153 valid 0.21129939375739348
LOSS train 0.25283208258955153 valid 0.21112458501498738
LOSS train 0.25283208258955153 valid 0.21116731866883734
LOSS train 0.25283208258955153 valid 0.21104466505927744
LOSS train 0.25283208258955153 valid 0.21100172300621406
LOSS train 0.25283208258955153 valid 0.21087861206286992
LOSS train 0.25283208258955153 valid 0.21087395551861549
LOSS train 0.25283208258955153 valid 0.2110885995898755
LOSS train 0.25283208258955153 valid 0.21094768200859879
LOSS train 0.25283208258955153 valid 0.2109983876422422
LOSS train 0.25283208258955153 valid 0.21089571863412857
LOSS train 0.25283208258955153 valid 0.21078364914329492
LOSS train 0.25283208258955153 valid 0.21073628180097825
LOSS train 0.25283208258955153 valid 0.21056014409499801
LOSS train 0.25283208258955153 valid 0.2105023630839937
LOSS train 0.25283208258955153 valid 0.21024729478649976
LOSS train 0.25283208258955153 valid 0.21029052268532875
LOSS train 0.25283208258955153 valid 0.21020561989378814
LOSS train 0.25283208258955153 valid 0.21006258018314838
LOSS train 0.25283208258955153 valid 0.20992029882504037
LOSS train 0.25283208258955153 valid 0.20995588231654394
LOSS train 0.25283208258955153 valid 0.2101390330006161
LOSS train 0.25283208258955153 valid 0.21015647489507244
LOSS train 0.25283208258955153 valid 0.21022486252963823
LOSS train 0.25283208258955153 valid 0.21016773247273168
LOSS train 0.25283208258955153 valid 0.20998976958352467
LOSS train 0.25283208258955153 valid 0.2099962540798717
LOSS train 0.25283208258955153 valid 0.21001238008523318
LOSS train 0.25283208258955153 valid 0.21008023619651794
LOSS train 0.25283208258955153 valid 0.21018338516422602
LOSS train 0.25283208258955153 valid 0.21023916168646378
LOSS train 0.25283208258955153 valid 0.21026327147473037
LOSS train 0.25283208258955153 valid 0.2102115315359992
LOSS train 0.25283208258955153 valid 0.21029526716925104
LOSS train 0.25283208258955153 valid 0.2103676083497703
LOSS train 0.25283208258955153 valid 0.21036445995171865
LOSS train 0.25283208258955153 valid 0.2104273425266806
LOSS train 0.25283208258955153 valid 0.21069638503280505
LOSS train 0.25283208258955153 valid 0.21078057384543253
LOSS train 0.25283208258955153 valid 0.2107778485034751
LOSS train 0.25283208258955153 valid 0.21077604533537575
LOSS train 0.25283208258955153 valid 0.21082247167954712
LOSS train 0.25283208258955153 valid 0.2108760804323287
LOSS train 0.25283208258955153 valid 0.21085865322356573
LOSS train 0.25283208258955153 valid 0.21084367719470945
LOSS train 0.25283208258955153 valid 0.210972346270338
LOSS train 0.25283208258955153 valid 0.21082720842401861
LOSS train 0.25283208258955153 valid 0.21094372987998689
LOSS train 0.25283208258955153 valid 0.21095908671116628
LOSS train 0.25283208258955153 valid 0.21080811608535974
LOSS train 0.25283208258955153 valid 0.21075969506055117
LOSS train 0.25283208258955153 valid 0.21082721885556502
LOSS train 0.25283208258955153 valid 0.21062638893846639
LOSS train 0.25283208258955153 valid 0.21076776626914617
LOSS train 0.25283208258955153 valid 0.21106826520112695
LOSS train 0.25283208258955153 valid 0.21125137130824886
LOSS train 0.25283208258955153 valid 0.2112152216638007
LOSS train 0.25283208258955153 valid 0.21125977326501236
LOSS train 0.25283208258955153 valid 0.2112333034315417
LOSS train 0.25283208258955153 valid 0.21124524087551608
LOSS train 0.25283208258955153 valid 0.21144720250368118
LOSS train 0.25283208258955153 valid 0.2114666640995983
LOSS train 0.25283208258955153 valid 0.21162410707227766
LOSS train 0.25283208258955153 valid 0.2115563809753878
LOSS train 0.25283208258955153 valid 0.2114807193086842
LOSS train 0.25283208258955153 valid 0.2115208756689932
LOSS train 0.25283208258955153 valid 0.21158988995011896
LOSS train 0.25283208258955153 valid 0.21153410979282067
LOSS train 0.25283208258955153 valid 0.21158559839854868
LOSS train 0.25283208258955153 valid 0.21156527591027807
LOSS train 0.25283208258955153 valid 0.21152355917371235
LOSS train 0.25283208258955153 valid 0.21165947713157682
LOSS train 0.25283208258955153 valid 0.21167324784830327
LOSS train 0.25283208258955153 valid 0.2116939156341009
LOSS train 0.25283208258955153 valid 0.2117476953249989
LOSS train 0.25283208258955153 valid 0.2117123336724515
LOSS train 0.25283208258955153 valid 0.21180826957736695
LOSS train 0.25283208258955153 valid 0.2118228396003166
LOSS train 0.25283208258955153 valid 0.21197766799535325
LOSS train 0.25283208258955153 valid 0.21199865168355211
LOSS train 0.25283208258955153 valid 0.21198102197161428
LOSS train 0.25283208258955153 valid 0.21208452980456757
LOSS train 0.25283208258955153 valid 0.21226799345630057
LOSS train 0.25283208258955153 valid 0.2123165216310557
LOSS train 0.25283208258955153 valid 0.21226879100512414
LOSS train 0.25283208258955153 valid 0.21223876470869238
LOSS train 0.25283208258955153 valid 0.21212491798012154
LOSS train 0.25283208258955153 valid 0.21200366366641185
LOSS train 0.25283208258955153 valid 0.2119017604122059
LOSS train 0.25283208258955153 valid 0.21189515095030534
LOSS train 0.25283208258955153 valid 0.21185448435800416
LOSS train 0.25283208258955153 valid 0.21169321456413676
LOSS train 0.25283208258955153 valid 0.21152207730932437
LOSS train 0.25283208258955153 valid 0.21144136433677202
LOSS train 0.25283208258955153 valid 0.21146617856034092
LOSS train 0.25283208258955153 valid 0.21155712693406825
LOSS train 0.25283208258955153 valid 0.21150058721537357
LOSS train 0.25283208258955153 valid 0.21143307196761674
LOSS train 0.25283208258955153 valid 0.21140560150767365
LOSS train 0.25283208258955153 valid 0.21145654003100411
LOSS train 0.25283208258955153 valid 0.21149251075654193
LOSS train 0.25283208258955153 valid 0.21143994068157224
LOSS train 0.25283208258955153 valid 0.211473376222261
LOSS train 0.25283208258955153 valid 0.21147112441551155
LOSS train 0.25283208258955153 valid 0.21157100629441591
LOSS train 0.25283208258955153 valid 0.2116027360750457
LOSS train 0.25283208258955153 valid 0.21152678237774888
LOSS train 0.25283208258955153 valid 0.21156224874334303
LOSS train 0.25283208258955153 valid 0.2115499264741904
LOSS train 0.25283208258955153 valid 0.2115546753972669
LOSS train 0.25283208258955153 valid 0.2115079723795255
LOSS train 0.25283208258955153 valid 0.21152515234147196
LOSS train 0.25283208258955153 valid 0.21150117964539306
LOSS train 0.25283208258955153 valid 0.2115611516603149
LOSS train 0.25283208258955153 valid 0.2115350851396981
LOSS train 0.25283208258955153 valid 0.21140202176375467
LOSS train 0.25283208258955153 valid 0.211375542472001
LOSS train 0.25283208258955153 valid 0.21129216501301196
LOSS train 0.25283208258955153 valid 0.21120561949618452
LOSS train 0.25283208258955153 valid 0.2111895771088338
LOSS train 0.25283208258955153 valid 0.2112738672283388
LOSS train 0.25283208258955153 valid 0.21117879704263817
LOSS train 0.25283208258955153 valid 0.21114175289105147
LOSS train 0.25283208258955153 valid 0.21118237954168656
LOSS train 0.25283208258955153 valid 0.21116037486465114
LOSS train 0.25283208258955153 valid 0.21111597276869273
LOSS train 0.25283208258955153 valid 0.21109875253861463
LOSS train 0.25283208258955153 valid 0.21114301869545826
LOSS train 0.25283208258955153 valid 0.21118288773035854
LOSS train 0.25283208258955153 valid 0.21134125747277072
LOSS train 0.25283208258955153 valid 0.21129113337956368
LOSS train 0.25283208258955153 valid 0.21144489962549595
LOSS train 0.25283208258955153 valid 0.21137311989846436
LOSS train 0.25283208258955153 valid 0.21125532432975413
LOSS train 0.25283208258955153 valid 0.21124037957669775
LOSS train 0.25283208258955153 valid 0.21126124106920682
LOSS train 0.25283208258955153 valid 0.2113450132538936
LOSS train 0.25283208258955153 valid 0.2113661429295117
LOSS train 0.25283208258955153 valid 0.2113643183668212
LOSS train 0.25283208258955153 valid 0.2114011823315751
LOSS train 0.25283208258955153 valid 0.21144679590608134
LOSS train 0.25283208258955153 valid 0.21137496401356065
LOSS train 0.25283208258955153 valid 0.21129583836679
LOSS train 0.25283208258955153 valid 0.21133603925282532
LOSS train 0.25283208258955153 valid 0.21138011097551107
LOSS train 0.25283208258955153 valid 0.21133343252673079
LOSS train 0.25283208258955153 valid 0.21123800714988084
LOSS train 0.25283208258955153 valid 0.21120071853303768
LOSS train 0.25283208258955153 valid 0.21117390604061487
LOSS train 0.25283208258955153 valid 0.21113123329339828
LOSS train 0.25283208258955153 valid 0.2111648889587206
LOSS train 0.25283208258955153 valid 0.2110717917554889
LOSS train 0.25283208258955153 valid 0.21110621316913972
LOSS train 0.25283208258955153 valid 0.21106187191197198
LOSS train 0.25283208258955153 valid 0.2111984369626572
LOSS train 0.25283208258955153 valid 0.21123825229596402
LOSS train 0.25283208258955153 valid 0.2112405242000012
LOSS train 0.25283208258955153 valid 0.21111637289482854
LOSS train 0.25283208258955153 valid 0.21099755296419406
LOSS train 0.25283208258955153 valid 0.2110401484207984
LOSS train 0.25283208258955153 valid 0.21098198243549893
LOSS train 0.25283208258955153 valid 0.21096699903493593
LOSS train 0.25283208258955153 valid 0.21091063595800238
LOSS train 0.25283208258955153 valid 0.2109216486681622
LOSS train 0.25283208258955153 valid 0.21092677419468508
LOSS train 0.25283208258955153 valid 0.21101406145263726
LOSS train 0.25283208258955153 valid 0.2110835521408681
LOSS train 0.25283208258955153 valid 0.21104535744303748
LOSS train 0.25283208258955153 valid 0.2109897725242476
LOSS train 0.25283208258955153 valid 0.21106610150390348
LOSS train 0.25283208258955153 valid 0.2110115707334545
LOSS train 0.25283208258955153 valid 0.21101174563417144
LOSS train 0.25283208258955153 valid 0.2111035189411258
LOSS train 0.25283208258955153 valid 0.21098831497112253
LOSS train 0.25283208258955153 valid 0.21101663036497084
LOSS train 0.25283208258955153 valid 0.21102351068633876
LOSS train 0.25283208258955153 valid 0.21097981982706673
LOSS train 0.25283208258955153 valid 0.2108596052964312
LOSS train 0.25283208258955153 valid 0.21085575514513513
LOSS train 0.25283208258955153 valid 0.2109557250085562
EPOCH 13:
  batch 1 loss: 0.26097366213798523
  batch 2 loss: 0.2534487918019295
  batch 3 loss: 0.26399844388167065
  batch 4 loss: 0.27048007771372795
  batch 5 loss: 0.2714972227811813
  batch 6 loss: 0.26945284754037857
  batch 7 loss: 0.26803246779101236
  batch 8 loss: 0.26749775372445583
  batch 9 loss: 0.2681841966178682
  batch 10 loss: 0.2647571787238121
  batch 11 loss: 0.26396397433497687
  batch 12 loss: 0.26026445503036183
  batch 13 loss: 0.2629822435287329
  batch 14 loss: 0.26304578461817335
  batch 15 loss: 0.2636075804630915
  batch 16 loss: 0.26379936281591654
  batch 17 loss: 0.261226574287695
  batch 18 loss: 0.2638016814986865
  batch 19 loss: 0.26260080541435044
  batch 20 loss: 0.26148687303066254
  batch 21 loss: 0.2636277647245498
  batch 22 loss: 0.2635384445840662
  batch 23 loss: 0.2630847122358239
  batch 24 loss: 0.2607290502637625
  batch 25 loss: 0.26142538487911227
  batch 26 loss: 0.2591481048327226
  batch 27 loss: 0.25907474756240845
  batch 28 loss: 0.2571632159607751
  batch 29 loss: 0.25758125864226245
  batch 30 loss: 0.25658966501553854
  batch 31 loss: 0.2567880307474444
  batch 32 loss: 0.25674820970743895
  batch 33 loss: 0.2567909892761346
  batch 34 loss: 0.25564728677272797
  batch 35 loss: 0.2558476311819894
  batch 36 loss: 0.2567334994673729
  batch 37 loss: 0.25633523431984157
  batch 38 loss: 0.2564698912595448
  batch 39 loss: 0.25553932862404066
  batch 40 loss: 0.25529156886041166
  batch 41 loss: 0.25494064208937856
  batch 42 loss: 0.25518027870427995
  batch 43 loss: 0.2552448712116064
  batch 44 loss: 0.2549244612455368
  batch 45 loss: 0.25392237140072715
  batch 46 loss: 0.2552426933594372
  batch 47 loss: 0.2553932822131096
  batch 48 loss: 0.2546393973752856
  batch 49 loss: 0.2545453604994988
  batch 50 loss: 0.25422418892383575
  batch 51 loss: 0.25413598207866445
  batch 52 loss: 0.2540250185590524
  batch 53 loss: 0.2531502952553191
  batch 54 loss: 0.25390989223012217
  batch 55 loss: 0.2536980225281282
  batch 56 loss: 0.2534028094794069
  batch 57 loss: 0.25310555858570233
  batch 58 loss: 0.254796516535611
  batch 59 loss: 0.25486293510865354
  batch 60 loss: 0.2542714777092139
  batch 61 loss: 0.2548818219392026
  batch 62 loss: 0.2562366601440214
  batch 63 loss: 0.2558236914494681
  batch 64 loss: 0.2566849153954536
  batch 65 loss: 0.2569081918551372
  batch 66 loss: 0.2572955640427994
  batch 67 loss: 0.258131176455697
  batch 68 loss: 0.25849879467312026
  batch 69 loss: 0.2589273446280023
  batch 70 loss: 0.2593893691897392
  batch 71 loss: 0.259209897106802
  batch 72 loss: 0.2593584753986862
  batch 73 loss: 0.2595089700532286
  batch 74 loss: 0.25939650048275253
  batch 75 loss: 0.2590851354598999
  batch 76 loss: 0.2591877791442369
  batch 77 loss: 0.258837556684172
  batch 78 loss: 0.2591547897228828
  batch 79 loss: 0.2591074403328232
  batch 80 loss: 0.2585298717021942
  batch 81 loss: 0.258864927806972
  batch 82 loss: 0.2593832263132421
  batch 83 loss: 0.25925024194889756
  batch 84 loss: 0.2584682082136472
  batch 85 loss: 0.257984357195742
  batch 86 loss: 0.2583361436114755
  batch 87 loss: 0.25824723726716536
  batch 88 loss: 0.25809501704167237
  batch 89 loss: 0.25767451675420394
  batch 90 loss: 0.2575616737206777
  batch 91 loss: 0.25751931948976203
  batch 92 loss: 0.25714151431684906
  batch 93 loss: 0.2567684083215652
  batch 94 loss: 0.25696444543118174
  batch 95 loss: 0.256964708943116
  batch 96 loss: 0.2569729338089625
  batch 97 loss: 0.25708908917977635
  batch 98 loss: 0.25747436741176916
  batch 99 loss: 0.2578837489238893
  batch 100 loss: 0.25769562780857086
  batch 101 loss: 0.2575213268841847
  batch 102 loss: 0.2578372128453909
  batch 103 loss: 0.25813996184219434
  batch 104 loss: 0.25845432396118456
  batch 105 loss: 0.25829078881513506
  batch 106 loss: 0.2584000287472077
  batch 107 loss: 0.25806697437139314
  batch 108 loss: 0.2580048746808811
  batch 109 loss: 0.2580478780586785
  batch 110 loss: 0.2582812968980182
  batch 111 loss: 0.2578443323706721
  batch 112 loss: 0.2576201612661992
  batch 113 loss: 0.25796999195508197
  batch 114 loss: 0.2581806754072507
  batch 115 loss: 0.2583523875993231
  batch 116 loss: 0.25853185060209244
  batch 117 loss: 0.25901972343269575
  batch 118 loss: 0.25875761157880395
  batch 119 loss: 0.25899945249577533
  batch 120 loss: 0.2586165741086006
  batch 121 loss: 0.258538396397898
  batch 122 loss: 0.2584714728300689
  batch 123 loss: 0.25825157325442244
  batch 124 loss: 0.25833010841761866
  batch 125 loss: 0.25833233523368837
  batch 126 loss: 0.258358014954461
  batch 127 loss: 0.2586952065388987
  batch 128 loss: 0.25841120001859963
  batch 129 loss: 0.25855571123980736
  batch 130 loss: 0.25859844982624053
  batch 131 loss: 0.25875418127038097
  batch 132 loss: 0.258580341596495
  batch 133 loss: 0.2589467099510637
  batch 134 loss: 0.2590985970933046
  batch 135 loss: 0.25875743583396627
  batch 136 loss: 0.25884901578812036
  batch 137 loss: 0.2588053756821765
  batch 138 loss: 0.2589194567307182
  batch 139 loss: 0.2589692306175506
  batch 140 loss: 0.25875124005334715
  batch 141 loss: 0.2590899536161558
  batch 142 loss: 0.25904083325409555
  batch 143 loss: 0.2588187373809881
  batch 144 loss: 0.2589616029419833
  batch 145 loss: 0.2587642344935187
  batch 146 loss: 0.25865857017366856
  batch 147 loss: 0.25896598327727544
  batch 148 loss: 0.25907221617730886
  batch 149 loss: 0.2588335086635295
  batch 150 loss: 0.2587789093454679
  batch 151 loss: 0.25868775384710324
  batch 152 loss: 0.2586849321072039
  batch 153 loss: 0.25850721638576657
  batch 154 loss: 0.2585717456100823
  batch 155 loss: 0.25849964551387294
  batch 156 loss: 0.2584969111933158
  batch 157 loss: 0.25867199850310185
  batch 158 loss: 0.2585890690171266
  batch 159 loss: 0.2584939851138577
  batch 160 loss: 0.25816749725490806
  batch 161 loss: 0.25832501646154415
  batch 162 loss: 0.2581831124829657
  batch 163 loss: 0.25824455468932545
  batch 164 loss: 0.2579370496294847
  batch 165 loss: 0.2579326074231755
  batch 166 loss: 0.2578697514282652
  batch 167 loss: 0.2578445930859286
  batch 168 loss: 0.2578527472381081
  batch 169 loss: 0.25764500662772616
  batch 170 loss: 0.2577499206451809
  batch 171 loss: 0.2578318930334515
  batch 172 loss: 0.2580077182934728
  batch 173 loss: 0.2580373935789042
  batch 174 loss: 0.2580049737602815
  batch 175 loss: 0.2581225743464061
  batch 176 loss: 0.2579404716803269
  batch 177 loss: 0.2580852154958046
  batch 178 loss: 0.2582710509219866
  batch 179 loss: 0.2583685995147215
  batch 180 loss: 0.2582513943314552
  batch 181 loss: 0.2583133663920408
  batch 182 loss: 0.2583882237201209
  batch 183 loss: 0.2582074480304301
  batch 184 loss: 0.2580857301211875
  batch 185 loss: 0.2582027453023034
  batch 186 loss: 0.25828763153604284
  batch 187 loss: 0.2581252677236649
  batch 188 loss: 0.2577261560140772
  batch 189 loss: 0.25731016545699387
  batch 190 loss: 0.2573210929569445
  batch 191 loss: 0.25731166214218937
  batch 192 loss: 0.2573586168388526
  batch 193 loss: 0.2573301789056452
  batch 194 loss: 0.25763188379327046
  batch 195 loss: 0.2577918914648203
  batch 196 loss: 0.25756059161254335
  batch 197 loss: 0.2575252579553478
  batch 198 loss: 0.257850501573447
  batch 199 loss: 0.2579913459830548
  batch 200 loss: 0.2580597442388535
  batch 201 loss: 0.25817506660276385
  batch 202 loss: 0.2582965459268872
  batch 203 loss: 0.2584571196821523
  batch 204 loss: 0.2584040372395048
  batch 205 loss: 0.2585557800967519
  batch 206 loss: 0.2585357035537368
  batch 207 loss: 0.2586474997409876
  batch 208 loss: 0.25863738572941375
  batch 209 loss: 0.2583447738982844
  batch 210 loss: 0.25836725121452697
  batch 211 loss: 0.25838316715724097
  batch 212 loss: 0.2583748076884252
  batch 213 loss: 0.25825281979892173
  batch 214 loss: 0.2583767551685048
  batch 215 loss: 0.25825707954029703
  batch 216 loss: 0.2580725919731237
  batch 217 loss: 0.25794562388400327
  batch 218 loss: 0.25782055701684514
  batch 219 loss: 0.257911840530291
  batch 220 loss: 0.25788062783804805
  batch 221 loss: 0.25800507748288803
  batch 222 loss: 0.258135440650287
  batch 223 loss: 0.2581727111820683
  batch 224 loss: 0.25810146684359225
  batch 225 loss: 0.2579760475291146
  batch 226 loss: 0.2579435421961599
  batch 227 loss: 0.2577413411786378
  batch 228 loss: 0.25749266742352855
  batch 229 loss: 0.2574520582845638
  batch 230 loss: 0.257454530765181
  batch 231 loss: 0.2573041298559734
  batch 232 loss: 0.25720900862381374
  batch 233 loss: 0.25709607966979686
  batch 234 loss: 0.2569777345937541
  batch 235 loss: 0.25685604366850345
  batch 236 loss: 0.25674351355281927
  batch 237 loss: 0.256809994892732
  batch 238 loss: 0.25670846501568784
  batch 239 loss: 0.2566433295171131
  batch 240 loss: 0.256648452890416
  batch 241 loss: 0.25676432641462665
  batch 242 loss: 0.2566470297530663
  batch 243 loss: 0.25665462967544916
  batch 244 loss: 0.2567102796108019
  batch 245 loss: 0.256650360445587
  batch 246 loss: 0.2566473164815244
  batch 247 loss: 0.2566733684255044
  batch 248 loss: 0.25672602383119447
  batch 249 loss: 0.25663315860861274
  batch 250 loss: 0.2565573373436928
  batch 251 loss: 0.25652212569438126
  batch 252 loss: 0.256262877335151
  batch 253 loss: 0.25625685181306757
  batch 254 loss: 0.2562739435495354
  batch 255 loss: 0.2562983306599598
  batch 256 loss: 0.25646587420487776
  batch 257 loss: 0.2566980957868962
  batch 258 loss: 0.25659760827010913
  batch 259 loss: 0.2566308691579863
  batch 260 loss: 0.2567517160796202
  batch 261 loss: 0.25680029546392374
  batch 262 loss: 0.2568716510902834
  batch 263 loss: 0.25698399062165744
  batch 264 loss: 0.2569808219982819
  batch 265 loss: 0.25692256346063796
  batch 266 loss: 0.25696961273600283
  batch 267 loss: 0.2570998202660557
  batch 268 loss: 0.2569004703146308
  batch 269 loss: 0.2568085105556538
  batch 270 loss: 0.256979902530158
  batch 271 loss: 0.2569559934064471
  batch 272 loss: 0.25702136179760976
  batch 273 loss: 0.25693478481673493
  batch 274 loss: 0.25708447059575656
  batch 275 loss: 0.2570221105488864
  batch 276 loss: 0.25689696547561797
  batch 277 loss: 0.2568131116107913
  batch 278 loss: 0.25664891324026123
  batch 279 loss: 0.2567351673025384
  batch 280 loss: 0.25665971892220635
  batch 281 loss: 0.2565883489796275
  batch 282 loss: 0.2565474840977513
  batch 283 loss: 0.2565132916605515
  batch 284 loss: 0.25657270628381784
  batch 285 loss: 0.25649465353865375
  batch 286 loss: 0.2564481744816253
  batch 287 loss: 0.2564701365261543
  batch 288 loss: 0.25635504194845754
  batch 289 loss: 0.25646184163110064
  batch 290 loss: 0.2562390314607785
  batch 291 loss: 0.2563457592963353
  batch 292 loss: 0.2564442003120298
  batch 293 loss: 0.25640629781187596
  batch 294 loss: 0.2563212392788355
  batch 295 loss: 0.25627552609322435
  batch 296 loss: 0.25635909163267223
  batch 297 loss: 0.2563222347686588
  batch 298 loss: 0.25626542559206084
  batch 299 loss: 0.2561539708072924
  batch 300 loss: 0.2560876441001892
  batch 301 loss: 0.25616838656786667
  batch 302 loss: 0.25609005780409505
  batch 303 loss: 0.25594469152464727
  batch 304 loss: 0.2558603911709629
  batch 305 loss: 0.25565404960366545
  batch 306 loss: 0.2558385493124233
  batch 307 loss: 0.2558515913517545
  batch 308 loss: 0.25588678887912203
  batch 309 loss: 0.25581331517318306
  batch 310 loss: 0.25582214103591056
  batch 311 loss: 0.2559377347924702
  batch 312 loss: 0.2560911898811658
  batch 313 loss: 0.2561474373927132
  batch 314 loss: 0.2560954371550281
  batch 315 loss: 0.2561429273987573
  batch 316 loss: 0.2560820206999779
  batch 317 loss: 0.2560611421753555
  batch 318 loss: 0.25598700100895744
  batch 319 loss: 0.25588067744779736
  batch 320 loss: 0.2557616025675088
  batch 321 loss: 0.2557907136522721
  batch 322 loss: 0.25572098203882665
  batch 323 loss: 0.25558900846969973
  batch 324 loss: 0.2554535082092992
  batch 325 loss: 0.2554186446391619
  batch 326 loss: 0.25544155873586794
  batch 327 loss: 0.2554890368054037
  batch 328 loss: 0.25528827704852675
  batch 329 loss: 0.2553722115182587
  batch 330 loss: 0.25532972848776614
  batch 331 loss: 0.25537312156844355
  batch 332 loss: 0.2552953194153596
  batch 333 loss: 0.25529805134545575
  batch 334 loss: 0.25514352165474863
  batch 335 loss: 0.2549755783668205
  batch 336 loss: 0.25489087118988946
  batch 337 loss: 0.2547888083698488
  batch 338 loss: 0.2548202464919119
  batch 339 loss: 0.25482442825593077
  batch 340 loss: 0.2547357726184761
  batch 341 loss: 0.25469280080187007
  batch 342 loss: 0.2546041850545253
  batch 343 loss: 0.25460965262383833
  batch 344 loss: 0.25452598734477233
  batch 345 loss: 0.25455775705800543
  batch 346 loss: 0.2544354999685563
  batch 347 loss: 0.2543790082766618
  batch 348 loss: 0.2542952082879927
  batch 349 loss: 0.2541772144230184
  batch 350 loss: 0.2541668692656926
  batch 351 loss: 0.2541267927046175
  batch 352 loss: 0.2541188108311458
  batch 353 loss: 0.25403451640950386
  batch 354 loss: 0.2540617186976018
  batch 355 loss: 0.25399380792194687
  batch 356 loss: 0.2539783348277044
  batch 357 loss: 0.2538477637771131
  batch 358 loss: 0.2538514949339728
  batch 359 loss: 0.2538056457026091
  batch 360 loss: 0.25376466458870306
  batch 361 loss: 0.253700630321397
  batch 362 loss: 0.2535630400338884
  batch 363 loss: 0.2533918396254216
  batch 364 loss: 0.25321011523624043
  batch 365 loss: 0.2531527044430171
  batch 366 loss: 0.25302524522679753
  batch 367 loss: 0.25296108380000665
  batch 368 loss: 0.252804610149368
  batch 369 loss: 0.2527286843232669
  batch 370 loss: 0.2526231420603958
  batch 371 loss: 0.25262379320965944
  batch 372 loss: 0.25258999794561376
  batch 373 loss: 0.2524857664475812
  batch 374 loss: 0.25237091957407204
  batch 375 loss: 0.25238112994035083
  batch 376 loss: 0.2524305515070545
  batch 377 loss: 0.2523856475672608
  batch 378 loss: 0.25242829184841226
  batch 379 loss: 0.25241817258750543
  batch 380 loss: 0.2524403995981342
  batch 381 loss: 0.25240954439940416
  batch 382 loss: 0.25250364313418955
  batch 383 loss: 0.2525401553114774
  batch 384 loss: 0.252533839433454
  batch 385 loss: 0.25256024912580266
  batch 386 loss: 0.2525692817229063
  batch 387 loss: 0.25269772589822764
  batch 388 loss: 0.25274933494397045
  batch 389 loss: 0.2527463959008072
  batch 390 loss: 0.2526783929803433
  batch 391 loss: 0.25286569006150333
  batch 392 loss: 0.25290022528141126
  batch 393 loss: 0.2528686319067885
  batch 394 loss: 0.25284681779327733
  batch 395 loss: 0.2528884184888647
  batch 396 loss: 0.2528932887421112
  batch 397 loss: 0.252855899502108
  batch 398 loss: 0.25278266695276574
  batch 399 loss: 0.2528952660417198
  batch 400 loss: 0.25298293106257913
  batch 401 loss: 0.25289930514712583
  batch 402 loss: 0.2528782599230311
  batch 403 loss: 0.25295819475248493
  batch 404 loss: 0.2530169170165416
  batch 405 loss: 0.2530652666165505
  batch 406 loss: 0.25311145535096746
  batch 407 loss: 0.253019938012013
  batch 408 loss: 0.2530951106226912
  batch 409 loss: 0.2530863091604634
  batch 410 loss: 0.2531887605786324
  batch 411 loss: 0.2531685863605671
  batch 412 loss: 0.2531310582117548
  batch 413 loss: 0.2531729890056153
  batch 414 loss: 0.25316649542194636
  batch 415 loss: 0.25312857757131735
  batch 416 loss: 0.25316826411737847
  batch 417 loss: 0.2531530408621978
  batch 418 loss: 0.25314741816959885
  batch 419 loss: 0.2531313560458527
  batch 420 loss: 0.25307929945133983
  batch 421 loss: 0.25298645133218967
  batch 422 loss: 0.25306816995850107
  batch 423 loss: 0.25301824925780014
  batch 424 loss: 0.2529308551676431
  batch 425 loss: 0.2528581537218655
  batch 426 loss: 0.25283044615122074
  batch 427 loss: 0.25281835237487416
  batch 428 loss: 0.2528013880843314
  batch 429 loss: 0.25282641501971337
  batch 430 loss: 0.2527572756936384
  batch 431 loss: 0.252818929035813
  batch 432 loss: 0.252823700679949
  batch 433 loss: 0.2528344609239085
  batch 434 loss: 0.2528581857200592
  batch 435 loss: 0.2527741406498284
  batch 436 loss: 0.2527589703986951
  batch 437 loss: 0.2528282887807427
  batch 438 loss: 0.25290076608118944
  batch 439 loss: 0.25279984163260405
  batch 440 loss: 0.2528351892124523
  batch 441 loss: 0.25270274328807046
  batch 442 loss: 0.25259750953357146
  batch 443 loss: 0.2525792860890619
  batch 444 loss: 0.25250291945161046
  batch 445 loss: 0.2524807060032748
  batch 446 loss: 0.25243582272476145
  batch 447 loss: 0.25235469752643463
  batch 448 loss: 0.25235724346047
  batch 449 loss: 0.252405922868204
  batch 450 loss: 0.25232970234420565
  batch 451 loss: 0.2522837591343074
  batch 452 loss: 0.25235332472793826
  batch 453 loss: 0.25238344800261736
  batch 454 loss: 0.25238775461912155
  batch 455 loss: 0.25244903973825683
  batch 456 loss: 0.2524535795510338
  batch 457 loss: 0.25239559059200456
  batch 458 loss: 0.25232178333935257
  batch 459 loss: 0.2523930653572602
  batch 460 loss: 0.2524164086126763
  batch 461 loss: 0.2523876503918021
  batch 462 loss: 0.2523724902824406
  batch 463 loss: 0.25222719526702847
  batch 464 loss: 0.25219969656960717
  batch 465 loss: 0.25206887132378036
  batch 466 loss: 0.251957222979212
  batch 467 loss: 0.2520404083761975
  batch 468 loss: 0.25198630631988883
  batch 469 loss: 0.25203505984501545
  batch 470 loss: 0.2519784213380611
  batch 471 loss: 0.25208503279969446
  batch 472 loss: 0.2519500751237748
LOSS train 0.2519500751237748 valid 0.286471962928772
LOSS train 0.2519500751237748 valid 0.2831035554409027
LOSS train 0.2519500751237748 valid 0.28009029229482013
LOSS train 0.2519500751237748 valid 0.2773357406258583
LOSS train 0.2519500751237748 valid 0.2742501974105835
LOSS train 0.2519500751237748 valid 0.28065043687820435
LOSS train 0.2519500751237748 valid 0.2927185424736568
LOSS train 0.2519500751237748 valid 0.2917049638926983
LOSS train 0.2519500751237748 valid 0.2922775944073995
LOSS train 0.2519500751237748 valid 0.2925293743610382
LOSS train 0.2519500751237748 valid 0.2904720035466281
LOSS train 0.2519500751237748 valid 0.2918705567717552
LOSS train 0.2519500751237748 valid 0.2892941144796518
LOSS train 0.2519500751237748 valid 0.28919886904103415
LOSS train 0.2519500751237748 valid 0.28441392779350283
LOSS train 0.2519500751237748 valid 0.28556581772863865
LOSS train 0.2519500751237748 valid 0.28750666800667257
LOSS train 0.2519500751237748 valid 0.2898809595240487
LOSS train 0.2519500751237748 valid 0.2909186011866519
LOSS train 0.2519500751237748 valid 0.2896115377545357
LOSS train 0.2519500751237748 valid 0.28807085894403006
LOSS train 0.2519500751237748 valid 0.2860159013759006
LOSS train 0.2519500751237748 valid 0.2868328761795293
LOSS train 0.2519500751237748 valid 0.2843656061838071
LOSS train 0.2519500751237748 valid 0.28353713572025296
LOSS train 0.2519500751237748 valid 0.28325453114051086
LOSS train 0.2519500751237748 valid 0.28389007680945927
LOSS train 0.2519500751237748 valid 0.28397185089332716
LOSS train 0.2519500751237748 valid 0.28348949397432394
LOSS train 0.2519500751237748 valid 0.2847037081917127
LOSS train 0.2519500751237748 valid 0.28581812256766903
LOSS train 0.2519500751237748 valid 0.2857884126715362
LOSS train 0.2519500751237748 valid 0.28671385528463306
LOSS train 0.2519500751237748 valid 0.2862180318025982
LOSS train 0.2519500751237748 valid 0.28784105564866747
LOSS train 0.2519500751237748 valid 0.28770659160282874
LOSS train 0.2519500751237748 valid 0.28798204459048604
LOSS train 0.2519500751237748 valid 0.2900149441863361
LOSS train 0.2519500751237748 valid 0.29017479488482845
LOSS train 0.2519500751237748 valid 0.2903943780809641
LOSS train 0.2519500751237748 valid 0.29185199701204534
LOSS train 0.2519500751237748 valid 0.29207054738487515
LOSS train 0.2519500751237748 valid 0.2915903721437898
LOSS train 0.2519500751237748 valid 0.29179179634560237
LOSS train 0.2519500751237748 valid 0.29106332957744596
LOSS train 0.2519500751237748 valid 0.29177822658549185
LOSS train 0.2519500751237748 valid 0.29294082173641695
LOSS train 0.2519500751237748 valid 0.2932557969664534
LOSS train 0.2519500751237748 valid 0.2936962329003276
LOSS train 0.2519500751237748 valid 0.29247194617986677
LOSS train 0.2519500751237748 valid 0.29225121029451784
LOSS train 0.2519500751237748 valid 0.2919551773140064
LOSS train 0.2519500751237748 valid 0.29225513873235237
LOSS train 0.2519500751237748 valid 0.2920660936722049
LOSS train 0.2519500751237748 valid 0.2915807125243274
LOSS train 0.2519500751237748 valid 0.29060173513633863
LOSS train 0.2519500751237748 valid 0.29010389196245295
LOSS train 0.2519500751237748 valid 0.2892570449360486
LOSS train 0.2519500751237748 valid 0.2897172686407122
LOSS train 0.2519500751237748 valid 0.2899111911654472
LOSS train 0.2519500751237748 valid 0.2893393220471554
LOSS train 0.2519500751237748 valid 0.2903781153502003
LOSS train 0.2519500751237748 valid 0.2904992037349277
LOSS train 0.2519500751237748 valid 0.2917855754494667
LOSS train 0.2519500751237748 valid 0.2921873051386613
LOSS train 0.2519500751237748 valid 0.2920625589110635
LOSS train 0.2519500751237748 valid 0.29139650090416863
LOSS train 0.2519500751237748 valid 0.2913002463824609
LOSS train 0.2519500751237748 valid 0.29058348916578985
LOSS train 0.2519500751237748 valid 0.29060591501849037
LOSS train 0.2519500751237748 valid 0.2903193642555828
LOSS train 0.2519500751237748 valid 0.29079406956831616
LOSS train 0.2519500751237748 valid 0.2906505575735275
LOSS train 0.2519500751237748 valid 0.2908268486325805
LOSS train 0.2519500751237748 valid 0.2907721292972565
LOSS train 0.2519500751237748 valid 0.29127750114390727
LOSS train 0.2519500751237748 valid 0.29134635762734845
LOSS train 0.2519500751237748 valid 0.29173745138522905
LOSS train 0.2519500751237748 valid 0.29203074038783206
LOSS train 0.2519500751237748 valid 0.2911164877936244
LOSS train 0.2519500751237748 valid 0.2901461884195422
LOSS train 0.2519500751237748 valid 0.29080700529057807
LOSS train 0.2519500751237748 valid 0.29067347943782806
LOSS train 0.2519500751237748 valid 0.2904717303102925
LOSS train 0.2519500751237748 valid 0.2904146511765087
LOSS train 0.2519500751237748 valid 0.28982097447611566
LOSS train 0.2519500751237748 valid 0.289418185408088
LOSS train 0.2519500751237748 valid 0.2891171579672532
LOSS train 0.2519500751237748 valid 0.28960266860013595
LOSS train 0.2519500751237748 valid 0.2898844783504804
LOSS train 0.2519500751237748 valid 0.2898425307575163
LOSS train 0.2519500751237748 valid 0.29013551237142604
LOSS train 0.2519500751237748 valid 0.29012674654042847
LOSS train 0.2519500751237748 valid 0.29027163395856287
LOSS train 0.2519500751237748 valid 0.29016263657494595
LOSS train 0.2519500751237748 valid 0.2905277086732288
LOSS train 0.2519500751237748 valid 0.2905213290575853
LOSS train 0.2519500751237748 valid 0.2908629238301394
LOSS train 0.2519500751237748 valid 0.2911086788382193
LOSS train 0.2519500751237748 valid 0.291299149543047
LOSS train 0.2519500751237748 valid 0.29160125583115193
LOSS train 0.2519500751237748 valid 0.29160844330109803
LOSS train 0.2519500751237748 valid 0.2914505877251764
LOSS train 0.2519500751237748 valid 0.2913965733292011
LOSS train 0.2519500751237748 valid 0.2915158253340494
LOSS train 0.2519500751237748 valid 0.2917058491763079
LOSS train 0.2519500751237748 valid 0.29154535529212416
LOSS train 0.2519500751237748 valid 0.2916845705498148
LOSS train 0.2519500751237748 valid 0.2921278040616884
LOSS train 0.2519500751237748 valid 0.2924950993873856
LOSS train 0.2519500751237748 valid 0.29192724649433616
LOSS train 0.2519500751237748 valid 0.2918818652895944
LOSS train 0.2519500751237748 valid 0.2918074651893261
LOSS train 0.2519500751237748 valid 0.2915464353404547
LOSS train 0.2519500751237748 valid 0.2915839732989021
LOSS train 0.2519500751237748 valid 0.29140571106610624
LOSS train 0.2519500751237748 valid 0.29149008905276275
LOSS train 0.2519500751237748 valid 0.2914231175840911
LOSS train 0.2519500751237748 valid 0.2912820724128675
LOSS train 0.2519500751237748 valid 0.29101577065885065
LOSS train 0.2519500751237748 valid 0.29096409395213957
LOSS train 0.2519500751237748 valid 0.2906175795637193
LOSS train 0.2519500751237748 valid 0.29051266307753276
LOSS train 0.2519500751237748 valid 0.29097676781877396
LOSS train 0.2519500751237748 valid 0.29092618680000304
LOSS train 0.2519500751237748 valid 0.29122516229039147
LOSS train 0.2519500751237748 valid 0.2911624403919761
LOSS train 0.2519500751237748 valid 0.2916340453084558
LOSS train 0.2519500751237748 valid 0.29175131427225215
LOSS train 0.2519500751237748 valid 0.29161664362137135
LOSS train 0.2519500751237748 valid 0.29159412038235266
LOSS train 0.2519500751237748 valid 0.2913548124558998
LOSS train 0.2519500751237748 valid 0.2913639832260017
LOSS train 0.2519500751237748 valid 0.2915328814467387
LOSS train 0.2519500751237748 valid 0.2914117579106931
LOSS train 0.2519500751237748 valid 0.29138322568991604
LOSS train 0.2519500751237748 valid 0.2912679560428118
LOSS train 0.2519500751237748 valid 0.2912005628796591
LOSS train 0.2519500751237748 valid 0.29110436988391464
LOSS train 0.2519500751237748 valid 0.29128133612019674
LOSS train 0.2519500751237748 valid 0.2914029566954214
LOSS train 0.2519500751237748 valid 0.2918539126993905
LOSS train 0.2519500751237748 valid 0.29183983906999333
LOSS train 0.2519500751237748 valid 0.2917474613835414
LOSS train 0.2519500751237748 valid 0.29150764716082606
LOSS train 0.2519500751237748 valid 0.29168518765331947
LOSS train 0.2519500751237748 valid 0.2915168826271888
LOSS train 0.2519500751237748 valid 0.2924255309878169
LOSS train 0.2519500751237748 valid 0.2925368831461708
LOSS train 0.2519500751237748 valid 0.2927545642852783
LOSS train 0.2519500751237748 valid 0.29286517745611684
LOSS train 0.2519500751237748 valid 0.2926188472070192
LOSS train 0.2519500751237748 valid 0.29280705448069605
LOSS train 0.2519500751237748 valid 0.29268980626161994
LOSS train 0.2519500751237748 valid 0.2928005862620569
LOSS train 0.2519500751237748 valid 0.2929555256015215
LOSS train 0.2519500751237748 valid 0.29294410177097197
LOSS train 0.2519500751237748 valid 0.29309879130200495
LOSS train 0.2519500751237748 valid 0.2930554204017111
LOSS train 0.2519500751237748 valid 0.29307444058358667
LOSS train 0.2519500751237748 valid 0.29297444168825326
LOSS train 0.2519500751237748 valid 0.29284344374397653
LOSS train 0.2519500751237748 valid 0.2927609453537713
LOSS train 0.2519500751237748 valid 0.29243827702068703
LOSS train 0.2519500751237748 valid 0.29221528967221577
LOSS train 0.2519500751237748 valid 0.2922817193959133
LOSS train 0.2519500751237748 valid 0.2926314441029897
LOSS train 0.2519500751237748 valid 0.29237071903688566
LOSS train 0.2519500751237748 valid 0.29257272192712364
LOSS train 0.2519500751237748 valid 0.29274829731268043
LOSS train 0.2519500751237748 valid 0.29273396556140385
LOSS train 0.2519500751237748 valid 0.29253518009601637
LOSS train 0.2519500751237748 valid 0.29254293648493773
LOSS train 0.2519500751237748 valid 0.2925737229571945
LOSS train 0.2519500751237748 valid 0.29229685544967654
LOSS train 0.2519500751237748 valid 0.29225582849573006
LOSS train 0.2519500751237748 valid 0.29223906842328734
LOSS train 0.2519500751237748 valid 0.2924918326434125
LOSS train 0.2519500751237748 valid 0.29239277133728536
LOSS train 0.2519500751237748 valid 0.29225823481877644
LOSS train 0.2519500751237748 valid 0.29229546284807323
LOSS train 0.2519500751237748 valid 0.2923224817265521
LOSS train 0.2519500751237748 valid 0.29242501802783194
LOSS train 0.2519500751237748 valid 0.29250340034132416
LOSS train 0.2519500751237748 valid 0.2922012659343513
LOSS train 0.2519500751237748 valid 0.29214666687673135
LOSS train 0.2519500751237748 valid 0.2919860388187164
LOSS train 0.2519500751237748 valid 0.2919821434832634
LOSS train 0.2519500751237748 valid 0.29194633039847884
LOSS train 0.2519500751237748 valid 0.29205773410044217
LOSS train 0.2519500751237748 valid 0.2918807470985732
LOSS train 0.2519500751237748 valid 0.29190168560793
LOSS train 0.2519500751237748 valid 0.2917596296325249
LOSS train 0.2519500751237748 valid 0.2916299018663229
LOSS train 0.2519500751237748 valid 0.2914202596896734
LOSS train 0.2519500751237748 valid 0.29137048049240694
LOSS train 0.2519500751237748 valid 0.2915865444592413
LOSS train 0.2519500751237748 valid 0.2913947252914159
LOSS train 0.2519500751237748 valid 0.2913924268442183
LOSS train 0.2519500751237748 valid 0.2913022218644619
LOSS train 0.2519500751237748 valid 0.29116824121024476
LOSS train 0.2519500751237748 valid 0.291177699914073
LOSS train 0.2519500751237748 valid 0.29091939770529424
LOSS train 0.2519500751237748 valid 0.2908952697819355
LOSS train 0.2519500751237748 valid 0.29060622053902324
LOSS train 0.2519500751237748 valid 0.29066406848650533
LOSS train 0.2519500751237748 valid 0.29053221700560067
LOSS train 0.2519500751237748 valid 0.29030082740176183
LOSS train 0.2519500751237748 valid 0.29012309086094634
LOSS train 0.2519500751237748 valid 0.29019293096803483
LOSS train 0.2519500751237748 valid 0.2904083579778671
LOSS train 0.2519500751237748 valid 0.2904021695677964
LOSS train 0.2519500751237748 valid 0.29059574448726544
LOSS train 0.2519500751237748 valid 0.29055030483787303
LOSS train 0.2519500751237748 valid 0.29034329680509346
LOSS train 0.2519500751237748 valid 0.29037397479017574
LOSS train 0.2519500751237748 valid 0.29033896189680847
LOSS train 0.2519500751237748 valid 0.2904370130475508
LOSS train 0.2519500751237748 valid 0.29051120681305453
LOSS train 0.2519500751237748 valid 0.2906066794287075
LOSS train 0.2519500751237748 valid 0.29068570358181434
LOSS train 0.2519500751237748 valid 0.29062659810255237
LOSS train 0.2519500751237748 valid 0.29075845942368955
LOSS train 0.2519500751237748 valid 0.2908150704045381
LOSS train 0.2519500751237748 valid 0.29073375158839754
LOSS train 0.2519500751237748 valid 0.29085283891289637
LOSS train 0.2519500751237748 valid 0.2912030326374827
LOSS train 0.2519500751237748 valid 0.2913004165155846
LOSS train 0.2519500751237748 valid 0.29136092863228646
LOSS train 0.2519500751237748 valid 0.2913950362931127
LOSS train 0.2519500751237748 valid 0.2914024810770373
LOSS train 0.2519500751237748 valid 0.2914959528836711
LOSS train 0.2519500751237748 valid 0.2914767697645359
LOSS train 0.2519500751237748 valid 0.29143400706796563
LOSS train 0.2519500751237748 valid 0.29163960357929797
LOSS train 0.2519500751237748 valid 0.2914224129359601
LOSS train 0.2519500751237748 valid 0.2915073341458156
LOSS train 0.2519500751237748 valid 0.2915186834435503
LOSS train 0.2519500751237748 valid 0.29139171908590084
LOSS train 0.2519500751237748 valid 0.29130538056294125
LOSS train 0.2519500751237748 valid 0.29137242359739124
LOSS train 0.2519500751237748 valid 0.29115849751825174
LOSS train 0.2519500751237748 valid 0.29131964112750786
LOSS train 0.2519500751237748 valid 0.2917153114056001
LOSS train 0.2519500751237748 valid 0.2919114514881251
LOSS train 0.2519500751237748 valid 0.2918927140715646
LOSS train 0.2519500751237748 valid 0.2919266188313604
LOSS train 0.2519500751237748 valid 0.2918761387587555
LOSS train 0.2519500751237748 valid 0.29190806302440214
LOSS train 0.2519500751237748 valid 0.2921157310605049
LOSS train 0.2519500751237748 valid 0.2921782604370459
LOSS train 0.2519500751237748 valid 0.2924056647434121
LOSS train 0.2519500751237748 valid 0.2923825277759152
LOSS train 0.2519500751237748 valid 0.29226069263820575
LOSS train 0.2519500751237748 valid 0.2922970037249958
LOSS train 0.2519500751237748 valid 0.29239050432806835
LOSS train 0.2519500751237748 valid 0.2922941319325555
LOSS train 0.2519500751237748 valid 0.29239149154849753
LOSS train 0.2519500751237748 valid 0.2923924898203736
LOSS train 0.2519500751237748 valid 0.292364521840444
LOSS train 0.2519500751237748 valid 0.2924966663122177
LOSS train 0.2519500751237748 valid 0.2925122617423989
LOSS train 0.2519500751237748 valid 0.29252988717628525
LOSS train 0.2519500751237748 valid 0.29255843134314724
LOSS train 0.2519500751237748 valid 0.2924688127243294
LOSS train 0.2519500751237748 valid 0.29264133445974577
LOSS train 0.2519500751237748 valid 0.29265009681607007
LOSS train 0.2519500751237748 valid 0.29275611059656786
LOSS train 0.2519500751237748 valid 0.29275998013391813
LOSS train 0.2519500751237748 valid 0.2926976724355309
LOSS train 0.2519500751237748 valid 0.2928309718291258
LOSS train 0.2519500751237748 valid 0.29305569778251295
LOSS train 0.2519500751237748 valid 0.2931064736821276
LOSS train 0.2519500751237748 valid 0.29299247335561
LOSS train 0.2519500751237748 valid 0.29291840699586
LOSS train 0.2519500751237748 valid 0.2927975331203661
LOSS train 0.2519500751237748 valid 0.29265264510462863
LOSS train 0.2519500751237748 valid 0.2925200285993034
LOSS train 0.2519500751237748 valid 0.2925141867664125
LOSS train 0.2519500751237748 valid 0.2924931383026498
LOSS train 0.2519500751237748 valid 0.29230266406442773
LOSS train 0.2519500751237748 valid 0.2920858474805
LOSS train 0.2519500751237748 valid 0.2919741695740198
LOSS train 0.2519500751237748 valid 0.2919890848578702
LOSS train 0.2519500751237748 valid 0.29212278969455185
LOSS train 0.2519500751237748 valid 0.29210172265351236
LOSS train 0.2519500751237748 valid 0.29201930932674675
LOSS train 0.2519500751237748 valid 0.29197212655304206
LOSS train 0.2519500751237748 valid 0.29210420726286085
LOSS train 0.2519500751237748 valid 0.2921192076185654
LOSS train 0.2519500751237748 valid 0.29205638267535117
LOSS train 0.2519500751237748 valid 0.29206889056383745
LOSS train 0.2519500751237748 valid 0.2921191449356567
LOSS train 0.2519500751237748 valid 0.2922459320247579
LOSS train 0.2519500751237748 valid 0.2923287234569
LOSS train 0.2519500751237748 valid 0.2922671572682825
LOSS train 0.2519500751237748 valid 0.2923055162233134
LOSS train 0.2519500751237748 valid 0.29224629745187375
LOSS train 0.2519500751237748 valid 0.2922685049150301
LOSS train 0.2519500751237748 valid 0.2922239655752977
LOSS train 0.2519500751237748 valid 0.29224045990511427
LOSS train 0.2519500751237748 valid 0.2922081835321243
LOSS train 0.2519500751237748 valid 0.29231836245988463
LOSS train 0.2519500751237748 valid 0.2922487196659571
LOSS train 0.2519500751237748 valid 0.29214801402365576
LOSS train 0.2519500751237748 valid 0.29212117326610226
LOSS train 0.2519500751237748 valid 0.2919900692538252
LOSS train 0.2519500751237748 valid 0.2918616894680958
LOSS train 0.2519500751237748 valid 0.29182002985168815
LOSS train 0.2519500751237748 valid 0.29192222034738907
LOSS train 0.2519500751237748 valid 0.2918394877308818
LOSS train 0.2519500751237748 valid 0.2917988372441286
LOSS train 0.2519500751237748 valid 0.29183953786239075
LOSS train 0.2519500751237748 valid 0.29184136254962084
LOSS train 0.2519500751237748 valid 0.291795195993923
LOSS train 0.2519500751237748 valid 0.2917041576267043
LOSS train 0.2519500751237748 valid 0.2918025263194406
LOSS train 0.2519500751237748 valid 0.29189204612055664
LOSS train 0.2519500751237748 valid 0.29207510526837976
LOSS train 0.2519500751237748 valid 0.292013045726344
LOSS train 0.2519500751237748 valid 0.29220836830102026
LOSS train 0.2519500751237748 valid 0.2920710040915827
LOSS train 0.2519500751237748 valid 0.29192945684072774
LOSS train 0.2519500751237748 valid 0.2919099059553794
LOSS train 0.2519500751237748 valid 0.2919609311910776
LOSS train 0.2519500751237748 valid 0.29202243530311467
LOSS train 0.2519500751237748 valid 0.2920488089049628
LOSS train 0.2519500751237748 valid 0.2920911107666609
LOSS train 0.2519500751237748 valid 0.29213521292144407
LOSS train 0.2519500751237748 valid 0.29215212012782243
LOSS train 0.2519500751237748 valid 0.29208619140786346
LOSS train 0.2519500751237748 valid 0.29200386875365153
LOSS train 0.2519500751237748 valid 0.29200996319810907
LOSS train 0.2519500751237748 valid 0.2921275546450815
LOSS train 0.2519500751237748 valid 0.2920672940674113
LOSS train 0.2519500751237748 valid 0.29193342233165387
LOSS train 0.2519500751237748 valid 0.2918601358414404
LOSS train 0.2519500751237748 valid 0.29181751983581916
LOSS train 0.2519500751237748 valid 0.2917601900905986
LOSS train 0.2519500751237748 valid 0.291803884199437
LOSS train 0.2519500751237748 valid 0.29167538789535197
LOSS train 0.2519500751237748 valid 0.29169999267797025
LOSS train 0.2519500751237748 valid 0.2916687266906566
LOSS train 0.2519500751237748 valid 0.2918419401746157
LOSS train 0.2519500751237748 valid 0.29186299413010697
LOSS train 0.2519500751237748 valid 0.2918559621610393
LOSS train 0.2519500751237748 valid 0.29167144408487106
LOSS train 0.2519500751237748 valid 0.2915463238611989
LOSS train 0.2519500751237748 valid 0.29160283527264963
LOSS train 0.2519500751237748 valid 0.29150288564818244
LOSS train 0.2519500751237748 valid 0.29145449612215374
LOSS train 0.2519500751237748 valid 0.2913587987422943
LOSS train 0.2519500751237748 valid 0.29133728931713376
LOSS train 0.2519500751237748 valid 0.2913894029492039
LOSS train 0.2519500751237748 valid 0.29151619899440817
LOSS train 0.2519500751237748 valid 0.29164955413408494
LOSS train 0.2519500751237748 valid 0.29159953539111033
LOSS train 0.2519500751237748 valid 0.29155033760230636
LOSS train 0.2519500751237748 valid 0.29164722437314006
LOSS train 0.2519500751237748 valid 0.29160589087340566
LOSS train 0.2519500751237748 valid 0.291645741033422
LOSS train 0.2519500751237748 valid 0.2917705484516713
LOSS train 0.2519500751237748 valid 0.29163386022092225
LOSS train 0.2519500751237748 valid 0.2916434570983216
LOSS train 0.2519500751237748 valid 0.29168453232882774
LOSS train 0.2519500751237748 valid 0.29162320494651794
LOSS train 0.2519500751237748 valid 0.2914809698304948
LOSS train 0.2519500751237748 valid 0.2914724683632021
LOSS train 0.2519500751237748 valid 0.29160196485558176
EPOCH 14:
  batch 1 loss: 0.2592805027961731
  batch 2 loss: 0.245952770113945
  batch 3 loss: 0.2577782968680064
  batch 4 loss: 0.2608957812190056
  batch 5 loss: 0.25924192667007445
  batch 6 loss: 0.2596835196018219
  batch 7 loss: 0.2609060449259622
  batch 8 loss: 0.2645414397120476
  batch 9 loss: 0.26185692846775055
  batch 10 loss: 0.25886599868536
  batch 11 loss: 0.2581680701537566
  batch 12 loss: 0.25302471841375035
  batch 13 loss: 0.25228038544838244
  batch 14 loss: 0.2527747058442661
  batch 15 loss: 0.25663502315680187
  batch 16 loss: 0.2555776657536626
  batch 17 loss: 0.25162919040988474
  batch 18 loss: 0.2549770681394471
  batch 19 loss: 0.25322886125037547
  batch 20 loss: 0.252099248021841
  batch 21 loss: 0.25281994896275656
  batch 22 loss: 0.25212674655697564
  batch 23 loss: 0.2526776712873708
  batch 24 loss: 0.2512133829295635
  batch 25 loss: 0.2524771213531494
  batch 26 loss: 0.2501336680008815
  batch 27 loss: 0.2503169432834343
  batch 28 loss: 0.24936856968062265
  batch 29 loss: 0.2500539391205229
  batch 30 loss: 0.24935533354679743
  batch 31 loss: 0.24967313726102153
  batch 32 loss: 0.2495445804670453
  batch 33 loss: 0.24979351144848447
  batch 34 loss: 0.24857782441027024
  batch 35 loss: 0.24856051674910953
  batch 36 loss: 0.24910837122135693
  batch 37 loss: 0.2484658308125831
  batch 38 loss: 0.2485126856910555
  batch 39 loss: 0.2475165170736802
  batch 40 loss: 0.2477510143071413
  batch 41 loss: 0.2477076355277038
  batch 42 loss: 0.24808984320788158
  batch 43 loss: 0.2483313204937203
  batch 44 loss: 0.24804455583745783
  batch 45 loss: 0.2473713566859563
  batch 46 loss: 0.24819187204474988
  batch 47 loss: 0.2478547936424296
  batch 48 loss: 0.2469154273470243
  batch 49 loss: 0.24644436641615264
  batch 50 loss: 0.24579902827739716
  batch 51 loss: 0.2457853029755985
  batch 52 loss: 0.24589636406073204
  batch 53 loss: 0.24519072388702967
  batch 54 loss: 0.2455931212063189
  batch 55 loss: 0.24513284157622944
  batch 56 loss: 0.24496770570320742
  batch 57 loss: 0.24484150791377352
  batch 58 loss: 0.24617810367510237
  batch 59 loss: 0.2464538648471994
  batch 60 loss: 0.24635083129008611
  batch 61 loss: 0.24665328469432768
  batch 62 loss: 0.24771118019857713
  batch 63 loss: 0.2478097691422417
  batch 64 loss: 0.24869840079918504
  batch 65 loss: 0.24871511207177088
  batch 66 loss: 0.24892571049206186
  batch 67 loss: 0.24979841153123486
  batch 68 loss: 0.24980158117764137
  batch 69 loss: 0.2499418243549872
  batch 70 loss: 0.25029671554054533
  batch 71 loss: 0.2503424032892979
  batch 72 loss: 0.2506071211149295
  batch 73 loss: 0.2507754827607168
  batch 74 loss: 0.2505975879124693
  batch 75 loss: 0.25018946568171185
  batch 76 loss: 0.25046702866491516
  batch 77 loss: 0.2501148453780583
  batch 78 loss: 0.25030098129541445
  batch 79 loss: 0.2503962927981268
  batch 80 loss: 0.24982715155929328
  batch 81 loss: 0.2501572732940132
  batch 82 loss: 0.2507938275613436
  batch 83 loss: 0.2506272740751864
  batch 84 loss: 0.2500285745731422
  batch 85 loss: 0.24969527949305143
  batch 86 loss: 0.25001653699680815
  batch 87 loss: 0.24991382761248226
  batch 88 loss: 0.24964368343353271
  batch 89 loss: 0.2493137667018376
  batch 90 loss: 0.24927752978271908
  batch 91 loss: 0.24948275515011378
  batch 92 loss: 0.24934364498957343
  batch 93 loss: 0.24908777382425082
  batch 94 loss: 0.2492267408586563
  batch 95 loss: 0.24941313282439584
  batch 96 loss: 0.24943001366530856
  batch 97 loss: 0.2494117603781297
  batch 98 loss: 0.2497770328606878
  batch 99 loss: 0.2503306563153411
  batch 100 loss: 0.25030073180794715
  batch 101 loss: 0.2500438541173935
  batch 102 loss: 0.25018973896900815
  batch 103 loss: 0.25062380817908686
  batch 104 loss: 0.2509146546228574
  batch 105 loss: 0.25077229766618636
  batch 106 loss: 0.2510611589224833
  batch 107 loss: 0.2506817629404157
  batch 108 loss: 0.25059721977622423
  batch 109 loss: 0.2507920801092725
  batch 110 loss: 0.2513141610405662
  batch 111 loss: 0.25107617192977183
  batch 112 loss: 0.2507670074701309
  batch 113 loss: 0.2509876734915033
  batch 114 loss: 0.2513302426066315
  batch 115 loss: 0.25150914969651594
  batch 116 loss: 0.25168986541443855
  batch 117 loss: 0.25198299788002276
  batch 118 loss: 0.25200177905923227
  batch 119 loss: 0.2522048834993058
  batch 120 loss: 0.25172884451846284
  batch 121 loss: 0.25145845780195286
  batch 122 loss: 0.2514160368286195
  batch 123 loss: 0.2514865916918933
  batch 124 loss: 0.25161595498361894
  batch 125 loss: 0.2515723307132721
  batch 126 loss: 0.2515715892825808
  batch 127 loss: 0.2520801162156533
  batch 128 loss: 0.2518628975376487
  batch 129 loss: 0.25195314371308614
  batch 130 loss: 0.2518842943585836
  batch 131 loss: 0.25221014466449504
  batch 132 loss: 0.25191635909405624
  batch 133 loss: 0.25206868675418365
  batch 134 loss: 0.2521492676503623
  batch 135 loss: 0.251891565212497
  batch 136 loss: 0.2521160055390176
  batch 137 loss: 0.25203492758917984
  batch 138 loss: 0.25197110033553577
  batch 139 loss: 0.2522589802313194
  batch 140 loss: 0.25238250472715923
  batch 141 loss: 0.2529144268086616
  batch 142 loss: 0.2529515044789919
  batch 143 loss: 0.25275452737208015
  batch 144 loss: 0.2530269090914064
  batch 145 loss: 0.25307230517782014
  batch 146 loss: 0.2531254575268863
  batch 147 loss: 0.2534827871792981
  batch 148 loss: 0.2536626474277393
  batch 149 loss: 0.25362244678423707
  batch 150 loss: 0.2537282380461693
  batch 151 loss: 0.25370135636913854
  batch 152 loss: 0.2539555056902923
  batch 153 loss: 0.25378389315667493
  batch 154 loss: 0.25392825475760866
  batch 155 loss: 0.253760408874481
  batch 156 loss: 0.2537272297419034
  batch 157 loss: 0.2539871688101702
  batch 158 loss: 0.2540002516176127
  batch 159 loss: 0.25387368721407166
  batch 160 loss: 0.253563566878438
  batch 161 loss: 0.2536208342691386
  batch 162 loss: 0.25359686923983654
  batch 163 loss: 0.2536298348311266
  batch 164 loss: 0.2533387509969676
  batch 165 loss: 0.25329690223390405
  batch 166 loss: 0.2532323666186218
  batch 167 loss: 0.2532001649905108
  batch 168 loss: 0.25313736170175527
  batch 169 loss: 0.25288138860428827
  batch 170 loss: 0.25280808639876984
  batch 171 loss: 0.25288429812729707
  batch 172 loss: 0.2529253520418045
  batch 173 loss: 0.25298653542995453
  batch 174 loss: 0.2528540589686098
  batch 175 loss: 0.2529130971431732
  batch 176 loss: 0.2528003691272302
  batch 177 loss: 0.2528482518963895
  batch 178 loss: 0.25302629591373915
  batch 179 loss: 0.25311994469365595
  batch 180 loss: 0.25300006369749706
  batch 181 loss: 0.2530927773338655
  batch 182 loss: 0.25315056577488615
  batch 183 loss: 0.2530875629414626
  batch 184 loss: 0.2530069208663443
  batch 185 loss: 0.2530450216821722
  batch 186 loss: 0.25315990771657676
  batch 187 loss: 0.25306684615459035
  batch 188 loss: 0.25264317050893254
  batch 189 loss: 0.2522898879316118
  batch 190 loss: 0.2522548198699951
  batch 191 loss: 0.2521909558804247
  batch 192 loss: 0.252280476425464
  batch 193 loss: 0.25217060420488446
  batch 194 loss: 0.2525863092123848
  batch 195 loss: 0.25283186810138897
  batch 196 loss: 0.2527707863827141
  batch 197 loss: 0.25274689684664536
  batch 198 loss: 0.2530730806215845
  batch 199 loss: 0.253329319271011
  batch 200 loss: 0.2535888837277889
  batch 201 loss: 0.25378206298125916
  batch 202 loss: 0.2538342899320149
  batch 203 loss: 0.25397381215847187
  batch 204 loss: 0.2539597122984774
  batch 205 loss: 0.2543244619195054
  batch 206 loss: 0.2542009368104842
  batch 207 loss: 0.25420064252355823
  batch 208 loss: 0.25431021489202976
  batch 209 loss: 0.2543149660363722
  batch 210 loss: 0.2543033086118244
  batch 211 loss: 0.2543221986124301
  batch 212 loss: 0.25446865862270573
  batch 213 loss: 0.254500107865938
  batch 214 loss: 0.2545911125212072
  batch 215 loss: 0.2545093518356944
  batch 216 loss: 0.2545589272070814
  batch 217 loss: 0.25464309662717827
  batch 218 loss: 0.2545683589687041
  batch 219 loss: 0.2546883492845379
  batch 220 loss: 0.2546810427172618
  batch 221 loss: 0.25492316288915695
  batch 222 loss: 0.25505653876173606
  batch 223 loss: 0.25519571744005776
  batch 224 loss: 0.2552242301005338
  batch 225 loss: 0.2552298485570484
  batch 226 loss: 0.2553057017183937
  batch 227 loss: 0.2553153855291232
  batch 228 loss: 0.2552125954575706
  batch 229 loss: 0.2552354070296975
  batch 230 loss: 0.25534110846726793
  batch 231 loss: 0.25522528330743055
  batch 232 loss: 0.2551073422976609
  batch 233 loss: 0.25509714133749706
  batch 234 loss: 0.2550774109669221
  batch 235 loss: 0.2550233707148978
  batch 236 loss: 0.2549406979548729
  batch 237 loss: 0.2550470934638494
  batch 238 loss: 0.2551228092247699
  batch 239 loss: 0.2549890142604397
  batch 240 loss: 0.2549719072878361
  batch 241 loss: 0.25529541600789274
  batch 242 loss: 0.25531736841379116
  batch 243 loss: 0.2552601706098627
  batch 244 loss: 0.2552632558785501
  batch 245 loss: 0.255281504684565
  batch 246 loss: 0.2554669191197651
  batch 247 loss: 0.2555867302031652
  batch 248 loss: 0.2555727259285988
  batch 249 loss: 0.255531852623545
  batch 250 loss: 0.25557202970981596
  batch 251 loss: 0.2556190656950749
  batch 252 loss: 0.2554591954464004
  batch 253 loss: 0.25537135189936566
  batch 254 loss: 0.2552577575126032
  batch 255 loss: 0.25536637504895526
  batch 256 loss: 0.2554881185060367
  batch 257 loss: 0.25552356475058235
  batch 258 loss: 0.25551561791767446
  batch 259 loss: 0.2555941253555327
  batch 260 loss: 0.25560400703778635
  batch 261 loss: 0.25555397678609093
  batch 262 loss: 0.25555808421309667
  batch 263 loss: 0.2556124624190675
  batch 264 loss: 0.2556245880145015
  batch 265 loss: 0.2554910033378961
  batch 266 loss: 0.25542422944217696
  batch 267 loss: 0.2555750400274434
  batch 268 loss: 0.25544215227240946
  batch 269 loss: 0.2554063512268563
  batch 270 loss: 0.2555397459754237
  batch 271 loss: 0.25543646168004985
  batch 272 loss: 0.255414259072174
  batch 273 loss: 0.25530208008629934
  batch 274 loss: 0.2553909058770994
  batch 275 loss: 0.25534380945292384
  batch 276 loss: 0.25520634435225226
  batch 277 loss: 0.25513977490177225
  batch 278 loss: 0.2549746183397101
  batch 279 loss: 0.2550145544672525
  batch 280 loss: 0.25492040514945985
  batch 281 loss: 0.2548346645679338
  batch 282 loss: 0.25484124407277886
  batch 283 loss: 0.25482571156623085
  batch 284 loss: 0.25488545986968025
  batch 285 loss: 0.25477875178320364
  batch 286 loss: 0.2547866137294503
  batch 287 loss: 0.2548069529209403
  batch 288 loss: 0.25473371039455134
  batch 289 loss: 0.2548365479006487
  batch 290 loss: 0.25463334131857446
  batch 291 loss: 0.2546409290168703
  batch 292 loss: 0.2547083401618755
  batch 293 loss: 0.2546744267789984
  batch 294 loss: 0.2545548907550825
  batch 295 loss: 0.25456222386683447
  batch 296 loss: 0.2546021979604218
  batch 297 loss: 0.25455816927983704
  batch 298 loss: 0.2545110150271614
  batch 299 loss: 0.25436098320428346
  batch 300 loss: 0.2542606641848882
  batch 301 loss: 0.25430211177299983
  batch 302 loss: 0.2541731839247097
  batch 303 loss: 0.2540759272602919
  batch 304 loss: 0.25399515145507295
  batch 305 loss: 0.25380883124031006
  batch 306 loss: 0.253904338609549
  batch 307 loss: 0.25388734901766824
  batch 308 loss: 0.25396481224081735
  batch 309 loss: 0.2538596907672759
  batch 310 loss: 0.2538033890628046
  batch 311 loss: 0.25384962822271695
  batch 312 loss: 0.25395358817126507
  batch 313 loss: 0.2540418027212825
  batch 314 loss: 0.2539607632881517
  batch 315 loss: 0.2539423847482318
  batch 316 loss: 0.25382647899132743
  batch 317 loss: 0.25374180036200333
  batch 318 loss: 0.25370274109285584
  batch 319 loss: 0.2536166472401365
  batch 320 loss: 0.2535094219259918
  batch 321 loss: 0.25354113413537405
  batch 322 loss: 0.2534422722199689
  batch 323 loss: 0.2532941031474447
  batch 324 loss: 0.2531211533020308
  batch 325 loss: 0.253028436761636
  batch 326 loss: 0.25296298167639714
  batch 327 loss: 0.25291558275346726
  batch 328 loss: 0.25270944661120087
  batch 329 loss: 0.252818055881193
  batch 330 loss: 0.2526806609648647
  batch 331 loss: 0.25269099057080885
  batch 332 loss: 0.25264314472316257
  batch 333 loss: 0.25265017169731874
  batch 334 loss: 0.25248311611706625
  batch 335 loss: 0.25231955923251254
  batch 336 loss: 0.2522443754687196
  batch 337 loss: 0.2521886371894123
  batch 338 loss: 0.2521988339501725
  batch 339 loss: 0.25216348135190025
  batch 340 loss: 0.25205380745670375
  batch 341 loss: 0.2519812383347592
  batch 342 loss: 0.2519171513312044
  batch 343 loss: 0.25188297936937204
  batch 344 loss: 0.25183419467404833
  batch 345 loss: 0.25197483672611953
  batch 346 loss: 0.25183018611345676
  batch 347 loss: 0.25175061010318117
  batch 348 loss: 0.2517017236129306
  batch 349 loss: 0.2516249675719991
  batch 350 loss: 0.2516119114841734
  batch 351 loss: 0.25160868988077867
  batch 352 loss: 0.25159679586067796
  batch 353 loss: 0.2515868983592933
  batch 354 loss: 0.25166269353890824
  batch 355 loss: 0.25158535527511383
  batch 356 loss: 0.2515485110493858
  batch 357 loss: 0.25147063078499643
  batch 358 loss: 0.2514757027422916
  batch 359 loss: 0.2514056004687604
  batch 360 loss: 0.25139296510153347
  batch 361 loss: 0.2513264588124204
  batch 362 loss: 0.2511669515493166
  batch 363 loss: 0.2510292376122199
  batch 364 loss: 0.25088271523242467
  batch 365 loss: 0.2508267502670419
  batch 366 loss: 0.2507143119156686
  batch 367 loss: 0.2506492379494519
  batch 368 loss: 0.2505223731631818
  batch 369 loss: 0.25041653143196574
  batch 370 loss: 0.2502802555222769
  batch 371 loss: 0.2502819827865719
  batch 372 loss: 0.2502785721212946
  batch 373 loss: 0.2501142665384282
  batch 374 loss: 0.24999322642617047
  batch 375 loss: 0.24990272410710654
  batch 376 loss: 0.24995943634433948
  batch 377 loss: 0.2499446203284934
  batch 378 loss: 0.24985869512671516
  batch 379 loss: 0.2498080189435652
  batch 380 loss: 0.24986761604484758
  batch 381 loss: 0.24981540746576203
  batch 382 loss: 0.24982406706086005
  batch 383 loss: 0.24982383205280603
  batch 384 loss: 0.24982907541561872
  batch 385 loss: 0.24988221313272205
  batch 386 loss: 0.24981660770319905
  batch 387 loss: 0.24986526208330495
  batch 388 loss: 0.24987634448046536
  batch 389 loss: 0.2498848236005533
  batch 390 loss: 0.24983880737653144
  batch 391 loss: 0.24993088121151985
  batch 392 loss: 0.24992134798394175
  batch 393 loss: 0.24989884996990513
  batch 394 loss: 0.24990276822130086
  batch 395 loss: 0.24981135609029215
  batch 396 loss: 0.24989305071607984
  batch 397 loss: 0.24991868015951113
  batch 398 loss: 0.2498345436837206
  batch 399 loss: 0.24993542184058884
  batch 400 loss: 0.2500364676490426
  batch 401 loss: 0.24995167085208797
  batch 402 loss: 0.2499451561279558
  batch 403 loss: 0.2500230700665017
  batch 404 loss: 0.2500450008060082
  batch 405 loss: 0.2501051185675609
  batch 406 loss: 0.2501704908283473
  batch 407 loss: 0.25013036894124613
  batch 408 loss: 0.25020149156597316
  batch 409 loss: 0.25020020752052224
  batch 410 loss: 0.2503452027352845
  batch 411 loss: 0.25036243202477476
  batch 412 loss: 0.25031198557574774
  batch 413 loss: 0.250358882560857
  batch 414 loss: 0.25036759440593676
  batch 415 loss: 0.2503248092280813
  batch 416 loss: 0.2503983278472263
  batch 417 loss: 0.2503722377961202
  batch 418 loss: 0.2503843246179334
  batch 419 loss: 0.25037426332585283
  batch 420 loss: 0.25034508619989665
  batch 421 loss: 0.25024353247357095
  batch 422 loss: 0.25031062579267965
  batch 423 loss: 0.25028121432654965
  batch 424 loss: 0.250166599761765
  batch 425 loss: 0.2501033156759599
  batch 426 loss: 0.25002173666663013
  batch 427 loss: 0.24996885762002485
  batch 428 loss: 0.2499571960663127
  batch 429 loss: 0.2499849633042351
  batch 430 loss: 0.24993705881196399
  batch 431 loss: 0.24995773173263622
  batch 432 loss: 0.2499455467418388
  batch 433 loss: 0.24996212007817714
  batch 434 loss: 0.2499731763304653
  batch 435 loss: 0.2499367807445855
  batch 436 loss: 0.2499151663564214
  batch 437 loss: 0.2499625750687357
  batch 438 loss: 0.25006368256186784
  batch 439 loss: 0.2499918352498552
  batch 440 loss: 0.2500103437765078
  batch 441 loss: 0.2498975206259427
  batch 442 loss: 0.24978292332245752
  batch 443 loss: 0.24974736783224627
  batch 444 loss: 0.24963842053805385
  batch 445 loss: 0.24961870670988318
  batch 446 loss: 0.24957472453470186
  batch 447 loss: 0.2494726907753571
  batch 448 loss: 0.24948588426091842
  batch 449 loss: 0.24953825821589787
  batch 450 loss: 0.24948092374536726
  batch 451 loss: 0.24946033326855255
  batch 452 loss: 0.2495678661275754
  batch 453 loss: 0.24955675334856736
  batch 454 loss: 0.2495799001618104
  batch 455 loss: 0.24962842713345537
  batch 456 loss: 0.24962587478129486
  batch 457 loss: 0.24956707444441398
  batch 458 loss: 0.24951132396683423
  batch 459 loss: 0.24957843199534613
  batch 460 loss: 0.24959554743507634
  batch 461 loss: 0.24955130302207848
  batch 462 loss: 0.24955698080135114
  batch 463 loss: 0.24944070509391522
  batch 464 loss: 0.24941564717426382
  batch 465 loss: 0.249270456420478
  batch 466 loss: 0.24914781905934535
  batch 467 loss: 0.24922550767979224
  batch 468 loss: 0.24915270450023505
  batch 469 loss: 0.24921510144591585
  batch 470 loss: 0.2491948777056755
  batch 471 loss: 0.24926108779927475
  batch 472 loss: 0.24909612975256928
LOSS train 0.24909612975256928 valid 0.350885808467865
LOSS train 0.24909612975256928 valid 0.3440351337194443
LOSS train 0.24909612975256928 valid 0.3373406032721202
LOSS train 0.24909612975256928 valid 0.33808886259794235
LOSS train 0.24909612975256928 valid 0.33551124334335325
LOSS train 0.24909612975256928 valid 0.34510695437590283
LOSS train 0.24909612975256928 valid 0.35770867126328604
LOSS train 0.24909612975256928 valid 0.35755885019898415
LOSS train 0.24909612975256928 valid 0.3588383098443349
LOSS train 0.24909612975256928 valid 0.3590979605913162
LOSS train 0.24909612975256928 valid 0.35680136626416986
LOSS train 0.24909612975256928 valid 0.35785744587580365
LOSS train 0.24909612975256928 valid 0.3554797860292288
LOSS train 0.24909612975256928 valid 0.3557781002351216
LOSS train 0.24909612975256928 valid 0.34986815253893533
LOSS train 0.24909612975256928 valid 0.35025267861783504
LOSS train 0.24909612975256928 valid 0.3530353255131665
LOSS train 0.24909612975256928 valid 0.35622216429975295
LOSS train 0.24909612975256928 valid 0.35703770581044647
LOSS train 0.24909612975256928 valid 0.35604549795389173
LOSS train 0.24909612975256928 valid 0.3543629901749747
LOSS train 0.24909612975256928 valid 0.3518776974894784
LOSS train 0.24909612975256928 valid 0.3532450471235358
LOSS train 0.24909612975256928 valid 0.35016435384750366
LOSS train 0.24909612975256928 valid 0.3493601667881012
LOSS train 0.24909612975256928 valid 0.3487571684213785
LOSS train 0.24909612975256928 valid 0.34892095459832084
LOSS train 0.24909612975256928 valid 0.34923271409102846
LOSS train 0.24909612975256928 valid 0.34852937274965745
LOSS train 0.24909612975256928 valid 0.34951751033465067
LOSS train 0.24909612975256928 valid 0.35109703386983565
LOSS train 0.24909612975256928 valid 0.3509662030264735
LOSS train 0.24909612975256928 valid 0.3522125979264577
LOSS train 0.24909612975256928 valid 0.3518103141995037
LOSS train 0.24909612975256928 valid 0.3533047871930259
LOSS train 0.24909612975256928 valid 0.3532974364029037
LOSS train 0.24909612975256928 valid 0.3535923240958033
LOSS train 0.24909612975256928 valid 0.3556014777798402
LOSS train 0.24909612975256928 valid 0.3555889267187852
LOSS train 0.24909612975256928 valid 0.35598100498318674
LOSS train 0.24909612975256928 valid 0.3578676552307315
LOSS train 0.24909612975256928 valid 0.35819579660892487
LOSS train 0.24909612975256928 valid 0.3575556846552117
LOSS train 0.24909612975256928 valid 0.3577736738053235
LOSS train 0.24909612975256928 valid 0.35699549780951606
LOSS train 0.24909612975256928 valid 0.35783083801684173
LOSS train 0.24909612975256928 valid 0.35915117821794873
LOSS train 0.24909612975256928 valid 0.35949160531163216
LOSS train 0.24909612975256928 valid 0.35995895096233915
LOSS train 0.24909612975256928 valid 0.3585627919435501
LOSS train 0.24909612975256928 valid 0.35849128809629704
LOSS train 0.24909612975256928 valid 0.35787496314598966
LOSS train 0.24909612975256928 valid 0.357957124710083
LOSS train 0.24909612975256928 valid 0.35768454715057657
LOSS train 0.24909612975256928 valid 0.35718179128386757
LOSS train 0.24909612975256928 valid 0.3562873713672161
LOSS train 0.24909612975256928 valid 0.3555244155097426
LOSS train 0.24909612975256928 valid 0.3547221314290474
LOSS train 0.24909612975256928 valid 0.3552399085739912
LOSS train 0.24909612975256928 valid 0.3553294489781062
LOSS train 0.24909612975256928 valid 0.3547203237893152
LOSS train 0.24909612975256928 valid 0.3557929387015681
LOSS train 0.24909612975256928 valid 0.35591133151735577
LOSS train 0.24909612975256928 valid 0.3572361245751381
LOSS train 0.24909612975256928 valid 0.3576597259594844
LOSS train 0.24909612975256928 valid 0.35743519844430865
LOSS train 0.24909612975256928 valid 0.35654786851868703
LOSS train 0.24909612975256928 valid 0.3565811611273709
LOSS train 0.24909612975256928 valid 0.3558059822821963
LOSS train 0.24909612975256928 valid 0.3557501205376216
LOSS train 0.24909612975256928 valid 0.355389417477057
LOSS train 0.24909612975256928 valid 0.35584988279475105
LOSS train 0.24909612975256928 valid 0.35590163240694017
LOSS train 0.24909612975256928 valid 0.3559834175818675
LOSS train 0.24909612975256928 valid 0.35597586115201313
LOSS train 0.24909612975256928 valid 0.35661422422057704
LOSS train 0.24909612975256928 valid 0.3567387941595796
LOSS train 0.24909612975256928 valid 0.3571075281271568
LOSS train 0.24909612975256928 valid 0.35742581984664823
LOSS train 0.24909612975256928 valid 0.3563491769134998
LOSS train 0.24909612975256928 valid 0.35537402938913415
LOSS train 0.24909612975256928 valid 0.35602955774563116
LOSS train 0.24909612975256928 valid 0.35584445890173855
LOSS train 0.24909612975256928 valid 0.35571412600222085
LOSS train 0.24909612975256928 valid 0.3556678624714122
LOSS train 0.24909612975256928 valid 0.3549149628988532
LOSS train 0.24909612975256928 valid 0.3545193850308999
LOSS train 0.24909612975256928 valid 0.3541003313254226
LOSS train 0.24909612975256928 valid 0.35464617676949234
LOSS train 0.24909612975256928 valid 0.3549171494113074
LOSS train 0.24909612975256928 valid 0.3548185910497393
LOSS train 0.24909612975256928 valid 0.3552451959770659
LOSS train 0.24909612975256928 valid 0.35533198457892223
LOSS train 0.24909612975256928 valid 0.35549669157951436
LOSS train 0.24909612975256928 valid 0.35529977083206177
LOSS train 0.24909612975256928 valid 0.3558156468595068
LOSS train 0.24909612975256928 valid 0.3557552295861785
LOSS train 0.24909612975256928 valid 0.3559860097510474
LOSS train 0.24909612975256928 valid 0.3561732928560238
LOSS train 0.24909612975256928 valid 0.3563243964314461
LOSS train 0.24909612975256928 valid 0.3567410123230207
LOSS train 0.24909612975256928 valid 0.3568299859762192
LOSS train 0.24909612975256928 valid 0.356762540282555
LOSS train 0.24909612975256928 valid 0.3566959494581589
LOSS train 0.24909612975256928 valid 0.35664657184055876
LOSS train 0.24909612975256928 valid 0.35695037836173793
LOSS train 0.24909612975256928 valid 0.35670793585688154
LOSS train 0.24909612975256928 valid 0.3568776821096738
LOSS train 0.24909612975256928 valid 0.35747664811414315
LOSS train 0.24909612975256928 valid 0.3578943379900672
LOSS train 0.24909612975256928 valid 0.35726519103522775
LOSS train 0.24909612975256928 valid 0.3571302816271782
LOSS train 0.24909612975256928 valid 0.35699655471649844
LOSS train 0.24909612975256928 valid 0.3566846654080508
LOSS train 0.24909612975256928 valid 0.35680391710737475
LOSS train 0.24909612975256928 valid 0.35649122811596967
LOSS train 0.24909612975256928 valid 0.35668238004048664
LOSS train 0.24909612975256928 valid 0.356650252968578
LOSS train 0.24909612975256928 valid 0.3565291850005879
LOSS train 0.24909612975256928 valid 0.356185719370842
LOSS train 0.24909612975256928 valid 0.35613874930980777
LOSS train 0.24909612975256928 valid 0.3557413948363945
LOSS train 0.24909612975256928 valid 0.3556532862225199
LOSS train 0.24909612975256928 valid 0.3562038737920023
LOSS train 0.24909612975256928 valid 0.3562655539512634
LOSS train 0.24909612975256928 valid 0.35660252069670056
LOSS train 0.24909612975256928 valid 0.356510059336039
LOSS train 0.24909612975256928 valid 0.35707607702352107
LOSS train 0.24909612975256928 valid 0.3572415670221166
LOSS train 0.24909612975256928 valid 0.3570492902627358
LOSS train 0.24909612975256928 valid 0.35704209695335565
LOSS train 0.24909612975256928 valid 0.3567884557626464
LOSS train 0.24909612975256928 valid 0.35683918917985785
LOSS train 0.24909612975256928 valid 0.35703189137266644
LOSS train 0.24909612975256928 valid 0.3569754315747155
LOSS train 0.24909612975256928 valid 0.35692842041744904
LOSS train 0.24909612975256928 valid 0.3567633909465623
LOSS train 0.24909612975256928 valid 0.3566868590271991
LOSS train 0.24909612975256928 valid 0.3565916551960458
LOSS train 0.24909612975256928 valid 0.3568127887589591
LOSS train 0.24909612975256928 valid 0.35692718984387445
LOSS train 0.24909612975256928 valid 0.3573388469890809
LOSS train 0.24909612975256928 valid 0.3573030627274013
LOSS train 0.24909612975256928 valid 0.3571919697440333
LOSS train 0.24909612975256928 valid 0.35695614670885023
LOSS train 0.24909612975256928 valid 0.357176249770269
LOSS train 0.24909612975256928 valid 0.35695086693277167
LOSS train 0.24909612975256928 valid 0.3579512766084155
LOSS train 0.24909612975256928 valid 0.35808555691834265
LOSS train 0.24909612975256928 valid 0.35835955063501995
LOSS train 0.24909612975256928 valid 0.35842485222595416
LOSS train 0.24909612975256928 valid 0.35810959182287516
LOSS train 0.24909612975256928 valid 0.35833829660820804
LOSS train 0.24909612975256928 valid 0.3582439807715354
LOSS train 0.24909612975256928 valid 0.35835786731012403
LOSS train 0.24909612975256928 valid 0.35849671638928926
LOSS train 0.24909612975256928 valid 0.3584965146650934
LOSS train 0.24909612975256928 valid 0.35860083978387375
LOSS train 0.24909612975256928 valid 0.3586309910195429
LOSS train 0.24909612975256928 valid 0.3585753520950675
LOSS train 0.24909612975256928 valid 0.35847152297541224
LOSS train 0.24909612975256928 valid 0.35833989286128387
LOSS train 0.24909612975256928 valid 0.3582330059054439
LOSS train 0.24909612975256928 valid 0.3578708538921868
LOSS train 0.24909612975256928 valid 0.3576090624838164
LOSS train 0.24909612975256928 valid 0.3577113068965544
LOSS train 0.24909612975256928 valid 0.3580365327303995
LOSS train 0.24909612975256928 valid 0.35775606405167354
LOSS train 0.24909612975256928 valid 0.357974836988562
LOSS train 0.24909612975256928 valid 0.3581243271336836
LOSS train 0.24909612975256928 valid 0.3581169167457268
LOSS train 0.24909612975256928 valid 0.35793390488901805
LOSS train 0.24909612975256928 valid 0.3579214196329172
LOSS train 0.24909612975256928 valid 0.3579959321296078
LOSS train 0.24909612975256928 valid 0.3576865913186755
LOSS train 0.24909612975256928 valid 0.3575759836557237
LOSS train 0.24909612975256928 valid 0.3576985132896294
LOSS train 0.24909612975256928 valid 0.3580130604880579
LOSS train 0.24909612975256928 valid 0.35789479023917425
LOSS train 0.24909612975256928 valid 0.35786157879564495
LOSS train 0.24909612975256928 valid 0.3579326779144245
LOSS train 0.24909612975256928 valid 0.3579808165738871
LOSS train 0.24909612975256928 valid 0.3580317878332294
LOSS train 0.24909612975256928 valid 0.3581223345321158
LOSS train 0.24909612975256928 valid 0.35777068089794467
LOSS train 0.24909612975256928 valid 0.35775194950001216
LOSS train 0.24909612975256928 valid 0.3576060884139117
LOSS train 0.24909612975256928 valid 0.3576124326345768
LOSS train 0.24909612975256928 valid 0.3575385225512994
LOSS train 0.24909612975256928 valid 0.3576352734314768
LOSS train 0.24909612975256928 valid 0.35753142833709717
LOSS train 0.24909612975256928 valid 0.3575622836748759
LOSS train 0.24909612975256928 valid 0.3573917437711528
LOSS train 0.24909612975256928 valid 0.35716972646025036
LOSS train 0.24909612975256928 valid 0.35694963718071965
LOSS train 0.24909612975256928 valid 0.35687951439497423
LOSS train 0.24909612975256928 valid 0.35713811816297814
LOSS train 0.24909612975256928 valid 0.3569033991808843
LOSS train 0.24909612975256928 valid 0.3569018595182716
LOSS train 0.24909612975256928 valid 0.3567624869942665
LOSS train 0.24909612975256928 valid 0.3566618185138228
LOSS train 0.24909612975256928 valid 0.35666574493493186
LOSS train 0.24909612975256928 valid 0.35639955492442466
LOSS train 0.24909612975256928 valid 0.35637439100765717
LOSS train 0.24909612975256928 valid 0.3560477810661967
LOSS train 0.24909612975256928 valid 0.35609287095880043
LOSS train 0.24909612975256928 valid 0.3559826880261518
LOSS train 0.24909612975256928 valid 0.35573789993157756
LOSS train 0.24909612975256928 valid 0.35561764710827876
LOSS train 0.24909612975256928 valid 0.3556892383666266
LOSS train 0.24909612975256928 valid 0.3558961689754685
LOSS train 0.24909612975256928 valid 0.35588583167431487
LOSS train 0.24909612975256928 valid 0.3560932418550124
LOSS train 0.24909612975256928 valid 0.35608205770220713
LOSS train 0.24909612975256928 valid 0.3558750770812811
LOSS train 0.24909612975256928 valid 0.35586202006649087
LOSS train 0.24909612975256928 valid 0.35583448258962497
LOSS train 0.24909612975256928 valid 0.3559070482439951
LOSS train 0.24909612975256928 valid 0.35600855347772714
LOSS train 0.24909612975256928 valid 0.3561566589908166
LOSS train 0.24909612975256928 valid 0.35619128208893996
LOSS train 0.24909612975256928 valid 0.3561110442823118
LOSS train 0.24909612975256928 valid 0.35630627638021395
LOSS train 0.24909612975256928 valid 0.3563684143924287
LOSS train 0.24909612975256928 valid 0.35629269666141933
LOSS train 0.24909612975256928 valid 0.3563798556549359
LOSS train 0.24909612975256928 valid 0.3568459606118139
LOSS train 0.24909612975256928 valid 0.3568736692531067
LOSS train 0.24909612975256928 valid 0.35697599555727694
LOSS train 0.24909612975256928 valid 0.35698455766491266
LOSS train 0.24909612975256928 valid 0.35697009411209074
LOSS train 0.24909612975256928 valid 0.3571029270774332
LOSS train 0.24909612975256928 valid 0.357110754358922
LOSS train 0.24909612975256928 valid 0.35706595949128145
LOSS train 0.24909612975256928 valid 0.35733348968181206
LOSS train 0.24909612975256928 valid 0.3570915295158402
LOSS train 0.24909612975256928 valid 0.3571906795230093
LOSS train 0.24909612975256928 valid 0.35726583229393516
LOSS train 0.24909612975256928 valid 0.35710322819494306
LOSS train 0.24909612975256928 valid 0.35699516733487446
LOSS train 0.24909612975256928 valid 0.3569816697187938
LOSS train 0.24909612975256928 valid 0.35671261716480096
LOSS train 0.24909612975256928 valid 0.3569264208338388
LOSS train 0.24909612975256928 valid 0.3573216926611838
LOSS train 0.24909612975256928 valid 0.357603378441869
LOSS train 0.24909612975256928 valid 0.3575723498332791
LOSS train 0.24909612975256928 valid 0.35761716491297674
LOSS train 0.24909612975256928 valid 0.3575514967162763
LOSS train 0.24909612975256928 valid 0.35759833921869116
LOSS train 0.24909612975256928 valid 0.3578444241285324
LOSS train 0.24909612975256928 valid 0.3579072112818638
LOSS train 0.24909612975256928 valid 0.3581785608142141
LOSS train 0.24909612975256928 valid 0.3581901368180754
LOSS train 0.24909612975256928 valid 0.3580272966247844
LOSS train 0.24909612975256928 valid 0.35807032351400336
LOSS train 0.24909612975256928 valid 0.35818984278012067
LOSS train 0.24909612975256928 valid 0.35804337089163785
LOSS train 0.24909612975256928 valid 0.35816643843355106
LOSS train 0.24909612975256928 valid 0.35810916073994287
LOSS train 0.24909612975256928 valid 0.3580714488258729
LOSS train 0.24909612975256928 valid 0.3582301948262357
LOSS train 0.24909612975256928 valid 0.3582971383824603
LOSS train 0.24909612975256928 valid 0.35832487686958603
LOSS train 0.24909612975256928 valid 0.3583393827306502
LOSS train 0.24909612975256928 valid 0.35825233830595915
LOSS train 0.24909612975256928 valid 0.3584625752348649
LOSS train 0.24909612975256928 valid 0.3584847430164894
LOSS train 0.24909612975256928 valid 0.35865876990467754
LOSS train 0.24909612975256928 valid 0.35869450551426546
LOSS train 0.24909612975256928 valid 0.3586083722335321
LOSS train 0.24909612975256928 valid 0.3587907582411467
LOSS train 0.24909612975256928 valid 0.3590993686195682
LOSS train 0.24909612975256928 valid 0.3591571630357386
LOSS train 0.24909612975256928 valid 0.35906005221126724
LOSS train 0.24909612975256928 valid 0.3589525970545682
LOSS train 0.24909612975256928 valid 0.3588287217029627
LOSS train 0.24909612975256928 valid 0.3586050081554303
LOSS train 0.24909612975256928 valid 0.35844546063340826
LOSS train 0.24909612975256928 valid 0.35846056733080134
LOSS train 0.24909612975256928 valid 0.3584022111126355
LOSS train 0.24909612975256928 valid 0.35824824376462616
LOSS train 0.24909612975256928 valid 0.3579883412689182
LOSS train 0.24909612975256928 valid 0.3578636142895837
LOSS train 0.24909612975256928 valid 0.35791690028469325
LOSS train 0.24909612975256928 valid 0.3580493746096628
LOSS train 0.24909612975256928 valid 0.3580467644688133
LOSS train 0.24909612975256928 valid 0.35798429622467387
LOSS train 0.24909612975256928 valid 0.35792203268243206
LOSS train 0.24909612975256928 valid 0.3580537450767306
LOSS train 0.24909612975256928 valid 0.358064552833294
LOSS train 0.24909612975256928 valid 0.3579717501741914
LOSS train 0.24909612975256928 valid 0.3580244527491805
LOSS train 0.24909612975256928 valid 0.3580674349041115
LOSS train 0.24909612975256928 valid 0.3582632210181684
LOSS train 0.24909612975256928 valid 0.35832082881765853
LOSS train 0.24909612975256928 valid 0.35827345280228434
LOSS train 0.24909612975256928 valid 0.35830323744301845
LOSS train 0.24909612975256928 valid 0.35823779448166787
LOSS train 0.24909612975256928 valid 0.35823172300954326
LOSS train 0.24909612975256928 valid 0.35814028481642407
LOSS train 0.24909612975256928 valid 0.3581843707846645
LOSS train 0.24909612975256928 valid 0.3581167562118429
LOSS train 0.24909612975256928 valid 0.3582717355328425
LOSS train 0.24909612975256928 valid 0.35819695223318904
LOSS train 0.24909612975256928 valid 0.3580872865973926
LOSS train 0.24909612975256928 valid 0.3580576818363339
LOSS train 0.24909612975256928 valid 0.35789926403508515
LOSS train 0.24909612975256928 valid 0.3577169040580849
LOSS train 0.24909612975256928 valid 0.35768803896255863
LOSS train 0.24909612975256928 valid 0.3577969596270592
LOSS train 0.24909612975256928 valid 0.35768782354627776
LOSS train 0.24909612975256928 valid 0.3576464862204515
LOSS train 0.24909612975256928 valid 0.35766671802670047
LOSS train 0.24909612975256928 valid 0.3577189431258827
LOSS train 0.24909612975256928 valid 0.3576375054934668
LOSS train 0.24909612975256928 valid 0.3575310648619374
LOSS train 0.24909612975256928 valid 0.3576437073175087
LOSS train 0.24909612975256928 valid 0.35776994166509163
LOSS train 0.24909612975256928 valid 0.3579811996613924
LOSS train 0.24909612975256928 valid 0.3579202476888895
LOSS train 0.24909612975256928 valid 0.35818649470991804
LOSS train 0.24909612975256928 valid 0.35806599592570193
LOSS train 0.24909612975256928 valid 0.35788782536060815
LOSS train 0.24909612975256928 valid 0.35787703391210535
LOSS train 0.24909612975256928 valid 0.3579094912455632
LOSS train 0.24909612975256928 valid 0.3579880926140978
LOSS train 0.24909612975256928 valid 0.3580004042441692
LOSS train 0.24909612975256928 valid 0.3580601517020202
LOSS train 0.24909612975256928 valid 0.35812850382552686
LOSS train 0.24909612975256928 valid 0.35815127889315285
LOSS train 0.24909612975256928 valid 0.3580683621935254
LOSS train 0.24909612975256928 valid 0.35797813283391744
LOSS train 0.24909612975256928 valid 0.3579956861408623
LOSS train 0.24909612975256928 valid 0.3581163609277702
LOSS train 0.24909612975256928 valid 0.35805318168739775
LOSS train 0.24909612975256928 valid 0.35792426898011137
LOSS train 0.24909612975256928 valid 0.35789236435196875
LOSS train 0.24909612975256928 valid 0.3578539490699768
LOSS train 0.24909612975256928 valid 0.3577744298804123
LOSS train 0.24909612975256928 valid 0.3578124915852266
LOSS train 0.24909612975256928 valid 0.3576534753146409
LOSS train 0.24909612975256928 valid 0.35766069826326874
LOSS train 0.24909612975256928 valid 0.3576375233014888
LOSS train 0.24909612975256928 valid 0.35782378773356593
LOSS train 0.24909612975256928 valid 0.3579039077827896
LOSS train 0.24909612975256928 valid 0.3578646692065145
LOSS train 0.24909612975256928 valid 0.3576895127378898
LOSS train 0.24909612975256928 valid 0.35757845160604895
LOSS train 0.24909612975256928 valid 0.35766293462846205
LOSS train 0.24909612975256928 valid 0.3575552576780319
LOSS train 0.24909612975256928 valid 0.3574824015639107
LOSS train 0.24909612975256928 valid 0.3573538309640505
LOSS train 0.24909612975256928 valid 0.35729667730101783
LOSS train 0.24909612975256928 valid 0.3573828771457834
LOSS train 0.24909612975256928 valid 0.3574990011436838
LOSS train 0.24909612975256928 valid 0.3576397141546346
LOSS train 0.24909612975256928 valid 0.3576006468604593
LOSS train 0.24909612975256928 valid 0.3575262781961004
LOSS train 0.24909612975256928 valid 0.35763550153349766
LOSS train 0.24909612975256928 valid 0.35759313934379156
LOSS train 0.24909612975256928 valid 0.35762119928885694
LOSS train 0.24909612975256928 valid 0.35775710354193796
LOSS train 0.24909612975256928 valid 0.35755720209155856
LOSS train 0.24909612975256928 valid 0.3575740941278227
LOSS train 0.24909612975256928 valid 0.3576320377114701
LOSS train 0.24909612975256928 valid 0.35757417284725795
LOSS train 0.24909612975256928 valid 0.3574020155281722
LOSS train 0.24909612975256928 valid 0.35738951162151666
LOSS train 0.24909612975256928 valid 0.35751750661428705
EPOCH 15:
  batch 1 loss: 0.2963758707046509
  batch 2 loss: 0.2599303498864174
  batch 3 loss: 0.2563054859638214
  batch 4 loss: 0.2586987838149071
  batch 5 loss: 0.2648752748966217
  batch 6 loss: 0.26191216210524243
  batch 7 loss: 0.262860792023795
  batch 8 loss: 0.266817819327116
  batch 9 loss: 0.26430466771125793
  batch 10 loss: 0.2594323858618736
  batch 11 loss: 0.2571807977828113
  batch 12 loss: 0.2548226701716582
  batch 13 loss: 0.25222120720606583
  batch 14 loss: 0.252503151340144
  batch 15 loss: 0.25319799880186716
  batch 16 loss: 0.25277199503034353
  batch 17 loss: 0.2495779789545957
  batch 18 loss: 0.24896610031525293
  batch 19 loss: 0.24611876120692805
  batch 20 loss: 0.24580615982413292
  batch 21 loss: 0.24743857908816563
  batch 22 loss: 0.2467170703140172
  batch 23 loss: 0.24640705015348352
  batch 24 loss: 0.24470435145000616
  batch 25 loss: 0.24646452367305755
  batch 26 loss: 0.24457574979617044
  batch 27 loss: 0.24496695895989737
  batch 28 loss: 0.24372695279972895
  batch 29 loss: 0.24432089102679286
  batch 30 loss: 0.24389534195264181
  batch 31 loss: 0.24412477977814212
  batch 32 loss: 0.24377847090363503
  batch 33 loss: 0.24458153410391373
  batch 34 loss: 0.24352231095818913
  batch 35 loss: 0.24346661737986974
  batch 36 loss: 0.24362400670846304
  batch 37 loss: 0.2449347296276608
  batch 38 loss: 0.24566875711867683
  batch 39 loss: 0.24515280165733436
  batch 40 loss: 0.24609595574438572
  batch 41 loss: 0.2470100438449441
  batch 42 loss: 0.24729506990739278
  batch 43 loss: 0.24752195110154707
  batch 44 loss: 0.24769817258824
  batch 45 loss: 0.2477196193403668
  batch 46 loss: 0.24851810446251993
  batch 47 loss: 0.24814695024743993
  batch 48 loss: 0.2476330284650127
  batch 49 loss: 0.24738043820371433
  batch 50 loss: 0.2470869192481041
  batch 51 loss: 0.2472203674854017
  batch 52 loss: 0.24733860246264017
  batch 53 loss: 0.24686687847353378
  batch 54 loss: 0.24767063778859597
  batch 55 loss: 0.247120600938797
  batch 56 loss: 0.2467865166919572
  batch 57 loss: 0.24675302076758
  batch 58 loss: 0.24803368850000973
  batch 59 loss: 0.24826925134254715
  batch 60 loss: 0.24832541247208914
  batch 61 loss: 0.24846343573976737
  batch 62 loss: 0.24938909469112272
  batch 63 loss: 0.2491673164897495
  batch 64 loss: 0.24992129858583212
  batch 65 loss: 0.2498967326604403
  batch 66 loss: 0.24984842854918857
  batch 67 loss: 0.2505142114945312
  batch 68 loss: 0.2505910409724011
  batch 69 loss: 0.2507843038310175
  batch 70 loss: 0.25105501753943305
  batch 71 loss: 0.2508569974714602
  batch 72 loss: 0.2510356880310509
  batch 73 loss: 0.25091927651673146
  batch 74 loss: 0.25067746377474553
  batch 75 loss: 0.25041520774364473
  batch 76 loss: 0.2508833598541586
  batch 77 loss: 0.2501257150978237
  batch 78 loss: 0.2502677998481653
  batch 79 loss: 0.25053938468800313
  batch 80 loss: 0.24987318441271783
  batch 81 loss: 0.2499438060654534
  batch 82 loss: 0.2503769921093452
  batch 83 loss: 0.2501703612057559
  batch 84 loss: 0.24949825732480913
  batch 85 loss: 0.24909717423074385
  batch 86 loss: 0.24928677307311878
  batch 87 loss: 0.24934424596956406
  batch 88 loss: 0.24909991770982742
  batch 89 loss: 0.24870062644562024
  batch 90 loss: 0.24853460954295264
  batch 91 loss: 0.24869116649522885
  batch 92 loss: 0.24870542760776437
  batch 93 loss: 0.24875614848188174
  batch 94 loss: 0.24879356172490627
  batch 95 loss: 0.24891027494480736
  batch 96 loss: 0.24909061410774788
  batch 97 loss: 0.2490660864667794
  batch 98 loss: 0.24916545134417864
  batch 99 loss: 0.24948905423434095
  batch 100 loss: 0.24953239321708678
  batch 101 loss: 0.2495255237168605
  batch 102 loss: 0.24993675362830067
  batch 103 loss: 0.25007066477849643
  batch 104 loss: 0.2503407703569302
  batch 105 loss: 0.2504465648106166
  batch 106 loss: 0.25094236742775394
  batch 107 loss: 0.25072245397300363
  batch 108 loss: 0.2506390616849617
  batch 109 loss: 0.2508342799243577
  batch 110 loss: 0.25193863077597184
  batch 111 loss: 0.2523494588362204
  batch 112 loss: 0.2521376899842705
  batch 113 loss: 0.2520053241632681
  batch 114 loss: 0.25230274963797183
  batch 115 loss: 0.2526582093342491
  batch 116 loss: 0.25315511766178855
  batch 117 loss: 0.25338273807468575
  batch 118 loss: 0.25321701724650497
  batch 119 loss: 0.25362422536401186
  batch 120 loss: 0.2534666359424591
  batch 121 loss: 0.2532461938286616
  batch 122 loss: 0.2531546240703004
  batch 123 loss: 0.25302744989957265
  batch 124 loss: 0.25331785854312683
  batch 125 loss: 0.2534175623655319
  batch 126 loss: 0.25334132214387256
  batch 127 loss: 0.2536901822240334
  batch 128 loss: 0.2534295256482437
  batch 129 loss: 0.25370018807954564
  batch 130 loss: 0.25356705268988244
  batch 131 loss: 0.2534829114229625
  batch 132 loss: 0.2531399851044019
  batch 133 loss: 0.25327942245884943
  batch 134 loss: 0.25323124051983675
  batch 135 loss: 0.2528737133299863
  batch 136 loss: 0.2528007086366415
  batch 137 loss: 0.25270553368286497
  batch 138 loss: 0.25272046234728635
  batch 139 loss: 0.2528715915173935
  batch 140 loss: 0.2526493418429579
  batch 141 loss: 0.25282074341959987
  batch 142 loss: 0.25294739627082585
  batch 143 loss: 0.2527403778337932
  batch 144 loss: 0.2527869214407272
  batch 145 loss: 0.25266247669170644
  batch 146 loss: 0.2526007199736491
  batch 147 loss: 0.2528701198547065
  batch 148 loss: 0.25298162841716326
  batch 149 loss: 0.25279006681986305
  batch 150 loss: 0.252771306236585
  batch 151 loss: 0.2528166577516013
  batch 152 loss: 0.2529853351022068
  batch 153 loss: 0.2527042383851569
  batch 154 loss: 0.25279867300739534
  batch 155 loss: 0.2527495649553114
  batch 156 loss: 0.2527928782197145
  batch 157 loss: 0.25289799548258446
  batch 158 loss: 0.2529374847683725
  batch 159 loss: 0.25308656973658866
  batch 160 loss: 0.25281510269269347
  batch 161 loss: 0.25281149583943885
  batch 162 loss: 0.25273711013573186
  batch 163 loss: 0.25289395424120265
  batch 164 loss: 0.2526718693353781
  batch 165 loss: 0.25277430857672833
  batch 166 loss: 0.2526694755238223
  batch 167 loss: 0.25252195817981654
  batch 168 loss: 0.25251696524875505
  batch 169 loss: 0.25245423200567796
  batch 170 loss: 0.2523338052279809
  batch 171 loss: 0.2523186950132861
  batch 172 loss: 0.2523940608425196
  batch 173 loss: 0.2524290166666053
  batch 174 loss: 0.25229352070339794
  batch 175 loss: 0.25226798815386636
  batch 176 loss: 0.25215983102944767
  batch 177 loss: 0.2521684161350552
  batch 178 loss: 0.25231545477100975
  batch 179 loss: 0.25222787660593426
  batch 180 loss: 0.2521890295048555
  batch 181 loss: 0.2522815481569227
  batch 182 loss: 0.2522721916928396
  batch 183 loss: 0.2520825276609327
  batch 184 loss: 0.2520038405352313
  batch 185 loss: 0.2521135678967914
  batch 186 loss: 0.2523018165781934
  batch 187 loss: 0.2521117124806113
  batch 188 loss: 0.25179990793162205
  batch 189 loss: 0.25164240787899683
  batch 190 loss: 0.2514499780378844
  batch 191 loss: 0.2513387108816526
  batch 192 loss: 0.25149573812571663
  batch 193 loss: 0.2515048766846484
  batch 194 loss: 0.2518592655812342
  batch 195 loss: 0.2521354257296293
  batch 196 loss: 0.2524062897629884
  batch 197 loss: 0.2526824729242906
  batch 198 loss: 0.253106617942603
  batch 199 loss: 0.2531549790546523
  batch 200 loss: 0.25353243567049505
  batch 201 loss: 0.2540374271460433
  batch 202 loss: 0.25437990723565074
  batch 203 loss: 0.2544113882803565
  batch 204 loss: 0.25432749348236067
  batch 205 loss: 0.2547228082651045
  batch 206 loss: 0.25485616278590506
  batch 207 loss: 0.25508766581758785
  batch 208 loss: 0.2550235836981581
  batch 209 loss: 0.254874912816942
  batch 210 loss: 0.25510439425706866
  batch 211 loss: 0.25526128172591966
  batch 212 loss: 0.25513377258518954
  batch 213 loss: 0.2550178486416597
  batch 214 loss: 0.25522511320136415
  batch 215 loss: 0.2552283786063971
  batch 216 loss: 0.255056227384894
  batch 217 loss: 0.2550296524565341
  batch 218 loss: 0.25496945981312236
  batch 219 loss: 0.2550451869153541
  batch 220 loss: 0.2550822036510164
  batch 221 loss: 0.25519866971678323
  batch 222 loss: 0.25524237315665493
  batch 223 loss: 0.2552320626418152
  batch 224 loss: 0.2552401823257761
  batch 225 loss: 0.2551926103565428
  batch 226 loss: 0.2552666510496519
  batch 227 loss: 0.2552401077511027
  batch 228 loss: 0.2551488961959094
  batch 229 loss: 0.25513377987401453
  batch 230 loss: 0.2552423752520395
  batch 231 loss: 0.2551239332208386
  batch 232 loss: 0.25497056408945856
  batch 233 loss: 0.2549140826826955
  batch 234 loss: 0.2547742269742183
  batch 235 loss: 0.2546444367855153
  batch 236 loss: 0.2545702147408057
  batch 237 loss: 0.25456646053348414
  batch 238 loss: 0.25449653558370444
  batch 239 loss: 0.25438463862471
  batch 240 loss: 0.25437188930809496
  batch 241 loss: 0.2544551379205775
  batch 242 loss: 0.2542783071186917
  batch 243 loss: 0.2542504486110475
  batch 244 loss: 0.2542680958133252
  batch 245 loss: 0.25418278124867655
  batch 246 loss: 0.25429268435734076
  batch 247 loss: 0.2544472491451603
  batch 248 loss: 0.2544233715462108
  batch 249 loss: 0.2543690278228507
  batch 250 loss: 0.25434367924928664
  batch 251 loss: 0.25437435644318857
  batch 252 loss: 0.25415981513640235
  batch 253 loss: 0.2540218809494388
  batch 254 loss: 0.25380725049831737
  batch 255 loss: 0.253794749753148
  batch 256 loss: 0.2539356508641504
  batch 257 loss: 0.2539296023228753
  batch 258 loss: 0.2538331228979798
  batch 259 loss: 0.25380504309670804
  batch 260 loss: 0.25373645889071317
  batch 261 loss: 0.2536638123322264
  batch 262 loss: 0.25355786449126616
  batch 263 loss: 0.2535594895538722
  batch 264 loss: 0.2534843468191949
  batch 265 loss: 0.2532990393211257
  batch 266 loss: 0.253243474751935
  batch 267 loss: 0.2532692264193453
  batch 268 loss: 0.25309902134893547
  batch 269 loss: 0.25298553389465944
  batch 270 loss: 0.2531336678950875
  batch 271 loss: 0.2530521069725501
  batch 272 loss: 0.2529673673869932
  batch 273 loss: 0.25285190927894996
  batch 274 loss: 0.25292778531782817
  batch 275 loss: 0.252863569801504
  batch 276 loss: 0.25272900675949844
  batch 277 loss: 0.2526420879880444
  batch 278 loss: 0.25244364464025704
  batch 279 loss: 0.2524476590763283
  batch 280 loss: 0.2523792976779597
  batch 281 loss: 0.25222799712228605
  batch 282 loss: 0.25217701515830154
  batch 283 loss: 0.2520857276427872
  batch 284 loss: 0.2521623908214166
  batch 285 loss: 0.2520883772456855
  batch 286 loss: 0.2520526334955976
  batch 287 loss: 0.2521398571102461
  batch 288 loss: 0.2520952796460026
  batch 289 loss: 0.25220014897391047
  batch 290 loss: 0.2520050775388191
  batch 291 loss: 0.25198478919943584
  batch 292 loss: 0.25205396003510855
  batch 293 loss: 0.25203064129824526
  batch 294 loss: 0.25192758577818775
  batch 295 loss: 0.25187086735741565
  batch 296 loss: 0.2519009961067019
  batch 297 loss: 0.25181774589349126
  batch 298 loss: 0.25176477732274355
  batch 299 loss: 0.2516649349477379
  batch 300 loss: 0.25154523586233457
  batch 301 loss: 0.2515754179404027
  batch 302 loss: 0.25148828317787475
  batch 303 loss: 0.2513987170194242
  batch 304 loss: 0.2512611962952896
  batch 305 loss: 0.2510464695144872
  batch 306 loss: 0.2511010402265717
  batch 307 loss: 0.25099889079794435
  batch 308 loss: 0.2510618987892355
  batch 309 loss: 0.250947320345536
  batch 310 loss: 0.25087787715658066
  batch 311 loss: 0.25099022993129166
  batch 312 loss: 0.2511382618298133
  batch 313 loss: 0.25113310800573696
  batch 314 loss: 0.25102279321023613
  batch 315 loss: 0.25098039697087
  batch 316 loss: 0.2509520380651649
  batch 317 loss: 0.2508830594997677
  batch 318 loss: 0.2507942082278384
  batch 319 loss: 0.2507178740908733
  batch 320 loss: 0.25063570966012777
  batch 321 loss: 0.2506924433593067
  batch 322 loss: 0.2505613962963501
  batch 323 loss: 0.25041170796558215
  batch 324 loss: 0.2502415324158875
  batch 325 loss: 0.25019896282599524
  batch 326 loss: 0.25010631717970033
  batch 327 loss: 0.2500733263813392
  batch 328 loss: 0.24987149592943309
  batch 329 loss: 0.2500027111054916
  batch 330 loss: 0.24986813727653387
  batch 331 loss: 0.24979459077930163
  batch 332 loss: 0.2497399837586535
  batch 333 loss: 0.24976017377577028
  batch 334 loss: 0.2495782553346571
  batch 335 loss: 0.2494054799204442
  batch 336 loss: 0.2493249720317267
  batch 337 loss: 0.24933842226907124
  batch 338 loss: 0.2493651166734611
  batch 339 loss: 0.24930025465720523
  batch 340 loss: 0.24927770398995455
  batch 341 loss: 0.24926041875067345
  batch 342 loss: 0.24921291155947578
  batch 343 loss: 0.2491623518157631
  batch 344 loss: 0.2490844452363807
  batch 345 loss: 0.24918062112469605
  batch 346 loss: 0.2490702483350831
  batch 347 loss: 0.2490147389578888
  batch 348 loss: 0.24890292136148476
  batch 349 loss: 0.2487917311895201
  batch 350 loss: 0.24882085970469883
  batch 351 loss: 0.24880338903845545
  batch 352 loss: 0.24878977505828848
  batch 353 loss: 0.2487618005056219
  batch 354 loss: 0.24882459526850007
  batch 355 loss: 0.2487264073650602
  batch 356 loss: 0.24864652932862216
  batch 357 loss: 0.2485961844356788
  batch 358 loss: 0.24860799666390074
  batch 359 loss: 0.24849642414734557
  batch 360 loss: 0.24843804807298714
  batch 361 loss: 0.24838262635419903
  batch 362 loss: 0.24824984437523626
  batch 363 loss: 0.24805660890646217
  batch 364 loss: 0.24782700484598075
  batch 365 loss: 0.24777999193701025
  batch 366 loss: 0.24769632070442366
  batch 367 loss: 0.2476037863327307
  batch 368 loss: 0.2474747761355146
  batch 369 loss: 0.2474065217464597
  batch 370 loss: 0.24726977247644116
  batch 371 loss: 0.24725179060932118
  batch 372 loss: 0.2472126851238871
  batch 373 loss: 0.2470673162879637
  batch 374 loss: 0.2469837924136835
  batch 375 loss: 0.24683299708366394
  batch 376 loss: 0.2468234908152768
  batch 377 loss: 0.24682215721758988
  batch 378 loss: 0.24673088994764147
  batch 379 loss: 0.2466230232042499
  batch 380 loss: 0.24664002241272676
  batch 381 loss: 0.24662558222067324
  batch 382 loss: 0.2465802791152949
  batch 383 loss: 0.24658833241805084
  batch 384 loss: 0.24653380666859448
  batch 385 loss: 0.24660476167480666
  batch 386 loss: 0.24652962691580077
  batch 387 loss: 0.24657252688894593
  batch 388 loss: 0.24660362721872084
  batch 389 loss: 0.24656047237256498
  batch 390 loss: 0.24656427930562924
  batch 391 loss: 0.24664216319008556
  batch 392 loss: 0.2465649270357526
  batch 393 loss: 0.24657499437568753
  batch 394 loss: 0.24663447331504773
  batch 395 loss: 0.246535154158556
  batch 396 loss: 0.24646783672799968
  batch 397 loss: 0.24657831384192785
  batch 398 loss: 0.24659572002576224
  batch 399 loss: 0.2466142315762981
  batch 400 loss: 0.24666255094110967
  batch 401 loss: 0.24657465000996864
  batch 402 loss: 0.2466006703785996
  batch 403 loss: 0.24665099670810084
  batch 404 loss: 0.24664788678435995
  batch 405 loss: 0.2466761553729022
  batch 406 loss: 0.24677891581516548
  batch 407 loss: 0.2468040745879274
  batch 408 loss: 0.24691064692303247
  batch 409 loss: 0.24692984912681112
  batch 410 loss: 0.24702101015463107
  batch 411 loss: 0.24706225876680546
  batch 412 loss: 0.24701859111201416
  batch 413 loss: 0.24712096861863542
  batch 414 loss: 0.24714912880877943
  batch 415 loss: 0.24712828634733178
  batch 416 loss: 0.24721395410597324
  batch 417 loss: 0.2471981285859069
  batch 418 loss: 0.24718304402662805
  batch 419 loss: 0.24720566074290537
  batch 420 loss: 0.24720265996598062
  batch 421 loss: 0.24713425670978292
  batch 422 loss: 0.24721363621159187
  batch 423 loss: 0.24715172335611169
  batch 424 loss: 0.24705510213971138
  batch 425 loss: 0.2469955130885629
  batch 426 loss: 0.24692003841691174
  batch 427 loss: 0.2469019919494276
  batch 428 loss: 0.24686251998504746
  batch 429 loss: 0.24691860749449207
  batch 430 loss: 0.24686223427916681
  batch 431 loss: 0.24686945255698847
  batch 432 loss: 0.24685985167269353
  batch 433 loss: 0.24688691010926667
  batch 434 loss: 0.24691107676875207
  batch 435 loss: 0.24685878582384393
  batch 436 loss: 0.2468749128213716
  batch 437 loss: 0.24690769097897772
  batch 438 loss: 0.24697439780790512
  batch 439 loss: 0.24691444007437974
  batch 440 loss: 0.2469498584554954
  batch 441 loss: 0.24682277630245875
  batch 442 loss: 0.24672820975338172
  batch 443 loss: 0.24666187227714143
  batch 444 loss: 0.24653951199473562
  batch 445 loss: 0.24650892702381264
  batch 446 loss: 0.2464449599117976
  batch 447 loss: 0.24635313764647881
  batch 448 loss: 0.2464018840276237
  batch 449 loss: 0.24642350237592556
  batch 450 loss: 0.2463686302304268
  batch 451 loss: 0.24633587629197706
  batch 452 loss: 0.2464436886716733
  batch 453 loss: 0.246422116593283
  batch 454 loss: 0.24643456069383327
  batch 455 loss: 0.2464945812146742
  batch 456 loss: 0.24652967111844765
  batch 457 loss: 0.24648768746618108
  batch 458 loss: 0.24642203598563847
  batch 459 loss: 0.24649676803929613
  batch 460 loss: 0.24652933219204778
  batch 461 loss: 0.2464783443481441
  batch 462 loss: 0.2464674941840626
  batch 463 loss: 0.2463473347749875
  batch 464 loss: 0.24632561897280916
  batch 465 loss: 0.24616882666464776
  batch 466 loss: 0.24600805860220618
  batch 467 loss: 0.24613139416216784
  batch 468 loss: 0.24607552049888504
  batch 469 loss: 0.24616445963189545
  batch 470 loss: 0.24608145524212655
  batch 471 loss: 0.24616858663705757
  batch 472 loss: 0.24597283648484844
LOSS train 0.24597283648484844 valid 0.3010809123516083
LOSS train 0.24597283648484844 valid 0.30166392028331757
LOSS train 0.24597283648484844 valid 0.2949821750322978
LOSS train 0.24597283648484844 valid 0.29348713904619217
LOSS train 0.24597283648484844 valid 0.29090232253074644
LOSS train 0.24597283648484844 valid 0.2990001787741979
LOSS train 0.24597283648484844 valid 0.3128554778439658
LOSS train 0.24597283648484844 valid 0.31093088909983635
LOSS train 0.24597283648484844 valid 0.3141048087014092
LOSS train 0.24597283648484844 valid 0.31395876705646514
LOSS train 0.24597283648484844 valid 0.3124511214819821
LOSS train 0.24597283648484844 valid 0.31461024036010105
LOSS train 0.24597283648484844 valid 0.31194629118992734
LOSS train 0.24597283648484844 valid 0.31176104715892244
LOSS train 0.24597283648484844 valid 0.305663792292277
LOSS train 0.24597283648484844 valid 0.3060915656387806
LOSS train 0.24597283648484844 valid 0.3085179486695458
LOSS train 0.24597283648484844 valid 0.3111802107757992
LOSS train 0.24597283648484844 valid 0.3123351115929453
LOSS train 0.24597283648484844 valid 0.3110509216785431
LOSS train 0.24597283648484844 valid 0.3099527827330998
LOSS train 0.24597283648484844 valid 0.3076214668425647
LOSS train 0.24597283648484844 valid 0.3088108508483223
LOSS train 0.24597283648484844 valid 0.30593492773671943
LOSS train 0.24597283648484844 valid 0.3048775535821915
LOSS train 0.24597283648484844 valid 0.30448284573279893
LOSS train 0.24597283648484844 valid 0.30455297728379566
LOSS train 0.24597283648484844 valid 0.3049349013183798
LOSS train 0.24597283648484844 valid 0.30427314649368153
LOSS train 0.24597283648484844 valid 0.30536151081323626
LOSS train 0.24597283648484844 valid 0.3069761534852366
LOSS train 0.24597283648484844 valid 0.30650448659434915
LOSS train 0.24597283648484844 valid 0.3078631961887533
LOSS train 0.24597283648484844 valid 0.30765505967771306
LOSS train 0.24597283648484844 valid 0.3093064022915704
LOSS train 0.24597283648484844 valid 0.30954426651199657
LOSS train 0.24597283648484844 valid 0.3100296987875088
LOSS train 0.24597283648484844 valid 0.3118871807267791
LOSS train 0.24597283648484844 valid 0.3120307184946843
LOSS train 0.24597283648484844 valid 0.31214992366731165
LOSS train 0.24597283648484844 valid 0.31391229026201295
LOSS train 0.24597283648484844 valid 0.3142628850681441
LOSS train 0.24597283648484844 valid 0.31357596260170606
LOSS train 0.24597283648484844 valid 0.3138627630065788
LOSS train 0.24597283648484844 valid 0.31303848723570504
LOSS train 0.24597283648484844 valid 0.31383884374214255
LOSS train 0.24597283648484844 valid 0.3153821030195723
LOSS train 0.24597283648484844 valid 0.3155858302488923
LOSS train 0.24597283648484844 valid 0.3159329152228881
LOSS train 0.24597283648484844 valid 0.3144694370031357
LOSS train 0.24597283648484844 valid 0.31418758280137005
LOSS train 0.24597283648484844 valid 0.31355504118479216
LOSS train 0.24597283648484844 valid 0.3138010771769398
LOSS train 0.24597283648484844 valid 0.3134528790359144
LOSS train 0.24597283648484844 valid 0.3130033704367551
LOSS train 0.24597283648484844 valid 0.3123610056936741
LOSS train 0.24597283648484844 valid 0.311698164333377
LOSS train 0.24597283648484844 valid 0.310863662382652
LOSS train 0.24597283648484844 valid 0.3113677526934672
LOSS train 0.24597283648484844 valid 0.311524240175883
LOSS train 0.24597283648484844 valid 0.31107022039225846
LOSS train 0.24597283648484844 valid 0.3120567375613797
LOSS train 0.24597283648484844 valid 0.3121473883825635
LOSS train 0.24597283648484844 valid 0.31349647603929043
LOSS train 0.24597283648484844 valid 0.31405003254230207
LOSS train 0.24597283648484844 valid 0.3137411407449029
LOSS train 0.24597283648484844 valid 0.31279669323963905
LOSS train 0.24597283648484844 valid 0.31298642561716195
LOSS train 0.24597283648484844 valid 0.31227997809216596
LOSS train 0.24597283648484844 valid 0.31238572682653154
LOSS train 0.24597283648484844 valid 0.31193672057608485
LOSS train 0.24597283648484844 valid 0.31237773845593136
LOSS train 0.24597283648484844 valid 0.3120427421511036
LOSS train 0.24597283648484844 valid 0.31202427680427963
LOSS train 0.24597283648484844 valid 0.312071205774943
LOSS train 0.24597283648484844 valid 0.3128365211580929
LOSS train 0.24597283648484844 valid 0.3128761544630125
LOSS train 0.24597283648484844 valid 0.31335449333374316
LOSS train 0.24597283648484844 valid 0.3137747464300711
LOSS train 0.24597283648484844 valid 0.312775987572968
LOSS train 0.24597283648484844 valid 0.3117475441576522
LOSS train 0.24597283648484844 valid 0.3123654710446916
LOSS train 0.24597283648484844 valid 0.3122059596949313
LOSS train 0.24597283648484844 valid 0.3120079815742515
LOSS train 0.24597283648484844 valid 0.31178146618254043
LOSS train 0.24597283648484844 valid 0.31107068252424863
LOSS train 0.24597283648484844 valid 0.31060903802000245
LOSS train 0.24597283648484844 valid 0.3102110876617106
LOSS train 0.24597283648484844 valid 0.3108552277423023
LOSS train 0.24597283648484844 valid 0.3112425997853279
LOSS train 0.24597283648484844 valid 0.31112917651841926
LOSS train 0.24597283648484844 valid 0.31162788605560426
LOSS train 0.24597283648484844 valid 0.31159994022179677
LOSS train 0.24597283648484844 valid 0.3117187741271993
LOSS train 0.24597283648484844 valid 0.31153166717604586
LOSS train 0.24597283648484844 valid 0.3121502233358721
LOSS train 0.24597283648484844 valid 0.3120426337436302
LOSS train 0.24597283648484844 valid 0.3123729972510922
LOSS train 0.24597283648484844 valid 0.3126033419611478
LOSS train 0.24597283648484844 valid 0.31263749197125434
LOSS train 0.24597283648484844 valid 0.31299103914511084
LOSS train 0.24597283648484844 valid 0.3130766698250584
LOSS train 0.24597283648484844 valid 0.3129334121363834
LOSS train 0.24597283648484844 valid 0.31287172938195557
LOSS train 0.24597283648484844 valid 0.3128517055795306
LOSS train 0.24597283648484844 valid 0.31308052210875276
LOSS train 0.24597283648484844 valid 0.31298341692608095
LOSS train 0.24597283648484844 valid 0.31320822583856406
LOSS train 0.24597283648484844 valid 0.31372397495519133
LOSS train 0.24597283648484844 valid 0.31414750069379804
LOSS train 0.24597283648484844 valid 0.3135908781676679
LOSS train 0.24597283648484844 valid 0.3135368506024991
LOSS train 0.24597283648484844 valid 0.3133885558463831
LOSS train 0.24597283648484844 valid 0.3131051752389523
LOSS train 0.24597283648484844 valid 0.31323434619799906
LOSS train 0.24597283648484844 valid 0.31303085528057195
LOSS train 0.24597283648484844 valid 0.31319356308533597
LOSS train 0.24597283648484844 valid 0.31313510883157536
LOSS train 0.24597283648484844 valid 0.3129243140711504
LOSS train 0.24597283648484844 valid 0.31257994038363296
LOSS train 0.24597283648484844 valid 0.31261793727224524
LOSS train 0.24597283648484844 valid 0.3121404585535409
LOSS train 0.24597283648484844 valid 0.3120084859249068
LOSS train 0.24597283648484844 valid 0.3125118859592945
LOSS train 0.24597283648484844 valid 0.31245863282680514
LOSS train 0.24597283648484844 valid 0.31273842390094486
LOSS train 0.24597283648484844 valid 0.31270659615205026
LOSS train 0.24597283648484844 valid 0.31321037805173546
LOSS train 0.24597283648484844 valid 0.3134137527194134
LOSS train 0.24597283648484844 valid 0.31325999234731383
LOSS train 0.24597283648484844 valid 0.31320494265501736
LOSS train 0.24597283648484844 valid 0.3129183928849119
LOSS train 0.24597283648484844 valid 0.3129364681199081
LOSS train 0.24597283648484844 valid 0.3131519510452427
LOSS train 0.24597283648484844 valid 0.3130239792444088
LOSS train 0.24597283648484844 valid 0.3129330916220651
LOSS train 0.24597283648484844 valid 0.31279821258826845
LOSS train 0.24597283648484844 valid 0.31274273937595065
LOSS train 0.24597283648484844 valid 0.3126953577609371
LOSS train 0.24597283648484844 valid 0.31289977484515735
LOSS train 0.24597283648484844 valid 0.3129862385227325
LOSS train 0.24597283648484844 valid 0.31336755158615787
LOSS train 0.24597283648484844 valid 0.3133835205873409
LOSS train 0.24597283648484844 valid 0.3131702404676212
LOSS train 0.24597283648484844 valid 0.31296993565970455
LOSS train 0.24597283648484844 valid 0.3130397305709042
LOSS train 0.24597283648484844 valid 0.31290009587394946
LOSS train 0.24597283648484844 valid 0.31388735519470395
LOSS train 0.24597283648484844 valid 0.31401254476716856
LOSS train 0.24597283648484844 valid 0.31434225469827654
LOSS train 0.24597283648484844 valid 0.3143642595074824
LOSS train 0.24597283648484844 valid 0.3140934061651167
LOSS train 0.24597283648484844 valid 0.31435448525388254
LOSS train 0.24597283648484844 valid 0.3142077103257179
LOSS train 0.24597283648484844 valid 0.31429684537072333
LOSS train 0.24597283648484844 valid 0.3144265878467988
LOSS train 0.24597283648484844 valid 0.31433289531309894
LOSS train 0.24597283648484844 valid 0.31435141901049435
LOSS train 0.24597283648484844 valid 0.3143787345608825
LOSS train 0.24597283648484844 valid 0.31437890222296117
LOSS train 0.24597283648484844 valid 0.3142463976737135
LOSS train 0.24597283648484844 valid 0.31408779111541346
LOSS train 0.24597283648484844 valid 0.31392254556980603
LOSS train 0.24597283648484844 valid 0.3135862440415999
LOSS train 0.24597283648484844 valid 0.31337735969008823
LOSS train 0.24597283648484844 valid 0.3134770668953298
LOSS train 0.24597283648484844 valid 0.31381740657512297
LOSS train 0.24597283648484844 valid 0.3135448970078003
LOSS train 0.24597283648484844 valid 0.3137592534870791
LOSS train 0.24597283648484844 valid 0.3139721552238745
LOSS train 0.24597283648484844 valid 0.31392694730856263
LOSS train 0.24597283648484844 valid 0.31373483072533165
LOSS train 0.24597283648484844 valid 0.31375864863051156
LOSS train 0.24597283648484844 valid 0.313820475905106
LOSS train 0.24597283648484844 valid 0.3135156397308622
LOSS train 0.24597283648484844 valid 0.31344191704622726
LOSS train 0.24597283648484844 valid 0.3135104722390741
LOSS train 0.24597283648484844 valid 0.3138231694530905
LOSS train 0.24597283648484844 valid 0.31369912815826567
LOSS train 0.24597283648484844 valid 0.3135939411818981
LOSS train 0.24597283648484844 valid 0.313678746147709
LOSS train 0.24597283648484844 valid 0.3138219194261582
LOSS train 0.24597283648484844 valid 0.31396200397952656
LOSS train 0.24597283648484844 valid 0.3140614271649848
LOSS train 0.24597283648484844 valid 0.3136762776084848
LOSS train 0.24597283648484844 valid 0.3135972876221903
LOSS train 0.24597283648484844 valid 0.31344465305779706
LOSS train 0.24597283648484844 valid 0.3134865290782553
LOSS train 0.24597283648484844 valid 0.3133790449648307
LOSS train 0.24597283648484844 valid 0.3134740072645639
LOSS train 0.24597283648484844 valid 0.31335577766620676
LOSS train 0.24597283648484844 valid 0.3133418379196276
LOSS train 0.24597283648484844 valid 0.31318177375459916
LOSS train 0.24597283648484844 valid 0.31299341516089196
LOSS train 0.24597283648484844 valid 0.31274650853413805
LOSS train 0.24597283648484844 valid 0.3126466945573992
LOSS train 0.24597283648484844 valid 0.3128769071725419
LOSS train 0.24597283648484844 valid 0.31264424602491686
LOSS train 0.24597283648484844 valid 0.31264120707260307
LOSS train 0.24597283648484844 valid 0.3125703439861536
LOSS train 0.24597283648484844 valid 0.3124767925164
LOSS train 0.24597283648484844 valid 0.3124230428054781
LOSS train 0.24597283648484844 valid 0.312197607212466
LOSS train 0.24597283648484844 valid 0.312214709160959
LOSS train 0.24597283648484844 valid 0.3118581014435466
LOSS train 0.24597283648484844 valid 0.31189999432818405
LOSS train 0.24597283648484844 valid 0.31181091545284656
LOSS train 0.24597283648484844 valid 0.31153996360416597
LOSS train 0.24597283648484844 valid 0.31143151544497916
LOSS train 0.24597283648484844 valid 0.3114978749127615
LOSS train 0.24597283648484844 valid 0.31170282940163996
LOSS train 0.24597283648484844 valid 0.31168802827596664
LOSS train 0.24597283648484844 valid 0.3119184735795142
LOSS train 0.24597283648484844 valid 0.31190422219093716
LOSS train 0.24597283648484844 valid 0.311778495893922
LOSS train 0.24597283648484844 valid 0.31170684195778986
LOSS train 0.24597283648484844 valid 0.3116840228507046
LOSS train 0.24597283648484844 valid 0.31175116470100683
LOSS train 0.24597283648484844 valid 0.31189396582782
LOSS train 0.24597283648484844 valid 0.3120342047377066
LOSS train 0.24597283648484844 valid 0.3121036670056943
LOSS train 0.24597283648484844 valid 0.311940710018347
LOSS train 0.24597283648484844 valid 0.3121634978349968
LOSS train 0.24597283648484844 valid 0.3121704614854285
LOSS train 0.24597283648484844 valid 0.3121097164683872
LOSS train 0.24597283648484844 valid 0.3122147406097007
LOSS train 0.24597283648484844 valid 0.3126243565576192
LOSS train 0.24597283648484844 valid 0.31265630756031004
LOSS train 0.24597283648484844 valid 0.31276963085066284
LOSS train 0.24597283648484844 valid 0.3128027042616969
LOSS train 0.24597283648484844 valid 0.3128239526635125
LOSS train 0.24597283648484844 valid 0.31290716870591556
LOSS train 0.24597283648484844 valid 0.3129180269435752
LOSS train 0.24597283648484844 valid 0.31290185247731006
LOSS train 0.24597283648484844 valid 0.3132024366804894
LOSS train 0.24597283648484844 valid 0.31296701388338866
LOSS train 0.24597283648484844 valid 0.3130707571023627
LOSS train 0.24597283648484844 valid 0.3131276931331939
LOSS train 0.24597283648484844 valid 0.31297274950159143
LOSS train 0.24597283648484844 valid 0.31286964813868207
LOSS train 0.24597283648484844 valid 0.3128547333335481
LOSS train 0.24597283648484844 valid 0.312607057084722
LOSS train 0.24597283648484844 valid 0.3127989531053928
LOSS train 0.24597283648484844 valid 0.31322253507668857
LOSS train 0.24597283648484844 valid 0.31354906948245304
LOSS train 0.24597283648484844 valid 0.3134761411484664
LOSS train 0.24597283648484844 valid 0.3134965218513118
LOSS train 0.24597283648484844 valid 0.3134232033644953
LOSS train 0.24597283648484844 valid 0.31340309989500237
LOSS train 0.24597283648484844 valid 0.31364205050468447
LOSS train 0.24597283648484844 valid 0.3137511665602604
LOSS train 0.24597283648484844 valid 0.3139673601540308
LOSS train 0.24597283648484844 valid 0.3139823758790615
LOSS train 0.24597283648484844 valid 0.31380384749784246
LOSS train 0.24597283648484844 valid 0.3138873815536499
LOSS train 0.24597283648484844 valid 0.3140094543341547
LOSS train 0.24597283648484844 valid 0.313847910337411
LOSS train 0.24597283648484844 valid 0.3140327977810719
LOSS train 0.24597283648484844 valid 0.3139923591181118
LOSS train 0.24597283648484844 valid 0.31396309572916764
LOSS train 0.24597283648484844 valid 0.3141430413129229
LOSS train 0.24597283648484844 valid 0.3141885320193895
LOSS train 0.24597283648484844 valid 0.31425740091066395
LOSS train 0.24597283648484844 valid 0.314307308219599
LOSS train 0.24597283648484844 valid 0.3141941393321415
LOSS train 0.24597283648484844 valid 0.31431230745817484
LOSS train 0.24597283648484844 valid 0.3143360335728649
LOSS train 0.24597283648484844 valid 0.31453083744689597
LOSS train 0.24597283648484844 valid 0.3145517230477032
LOSS train 0.24597283648484844 valid 0.3144831880375191
LOSS train 0.24597283648484844 valid 0.3147056936337939
LOSS train 0.24597283648484844 valid 0.3150015440495575
LOSS train 0.24597283648484844 valid 0.3150729936557812
LOSS train 0.24597283648484844 valid 0.3149663925605969
LOSS train 0.24597283648484844 valid 0.3148822288079695
LOSS train 0.24597283648484844 valid 0.3147400485864584
LOSS train 0.24597283648484844 valid 0.3145567400360796
LOSS train 0.24597283648484844 valid 0.31439345935694607
LOSS train 0.24597283648484844 valid 0.3144012541112934
LOSS train 0.24597283648484844 valid 0.31436599397233556
LOSS train 0.24597283648484844 valid 0.3141793196099509
LOSS train 0.24597283648484844 valid 0.313937671905291
LOSS train 0.24597283648484844 valid 0.313798039572399
LOSS train 0.24597283648484844 valid 0.31385456702448955
LOSS train 0.24597283648484844 valid 0.3139768890644375
LOSS train 0.24597283648484844 valid 0.31398987056283684
LOSS train 0.24597283648484844 valid 0.3139498176163497
LOSS train 0.24597283648484844 valid 0.3139140562982195
LOSS train 0.24597283648484844 valid 0.3140261452193904
LOSS train 0.24597283648484844 valid 0.31400866883582085
LOSS train 0.24597283648484844 valid 0.3139412032881963
LOSS train 0.24597283648484844 valid 0.3139968300620987
LOSS train 0.24597283648484844 valid 0.3140434517490172
LOSS train 0.24597283648484844 valid 0.3142496083362573
LOSS train 0.24597283648484844 valid 0.3143969971749742
LOSS train 0.24597283648484844 valid 0.31432531406549186
LOSS train 0.24597283648484844 valid 0.31432558174687203
LOSS train 0.24597283648484844 valid 0.31426156482240497
LOSS train 0.24597283648484844 valid 0.31426980727691717
LOSS train 0.24597283648484844 valid 0.3141679691771666
LOSS train 0.24597283648484844 valid 0.31420238108135934
LOSS train 0.24597283648484844 valid 0.3141926079495064
LOSS train 0.24597283648484844 valid 0.3143182008868397
LOSS train 0.24597283648484844 valid 0.3142929570749402
LOSS train 0.24597283648484844 valid 0.31418488079407175
LOSS train 0.24597283648484844 valid 0.3141722203372351
LOSS train 0.24597283648484844 valid 0.3140055010586686
LOSS train 0.24597283648484844 valid 0.3138647430225626
LOSS train 0.24597283648484844 valid 0.3138296581491298
LOSS train 0.24597283648484844 valid 0.31390895963676513
LOSS train 0.24597283648484844 valid 0.31379797944493615
LOSS train 0.24597283648484844 valid 0.3137623767535656
LOSS train 0.24597283648484844 valid 0.31378119579328895
LOSS train 0.24597283648484844 valid 0.3138257193432492
LOSS train 0.24597283648484844 valid 0.313755859410952
LOSS train 0.24597283648484844 valid 0.31365602279577076
LOSS train 0.24597283648484844 valid 0.31372988369750676
LOSS train 0.24597283648484844 valid 0.3138619897969114
LOSS train 0.24597283648484844 valid 0.31405273476925016
LOSS train 0.24597283648484844 valid 0.31401345250196755
LOSS train 0.24597283648484844 valid 0.3142498071041434
LOSS train 0.24597283648484844 valid 0.3141197035123843
LOSS train 0.24597283648484844 valid 0.31397568243022306
LOSS train 0.24597283648484844 valid 0.31398014443708055
LOSS train 0.24597283648484844 valid 0.3140531279032047
LOSS train 0.24597283648484844 valid 0.31413218283031613
LOSS train 0.24597283648484844 valid 0.314128499557848
LOSS train 0.24597283648484844 valid 0.31418824645622473
LOSS train 0.24597283648484844 valid 0.31425594137613533
LOSS train 0.24597283648484844 valid 0.3142814911224625
LOSS train 0.24597283648484844 valid 0.31417031000928214
LOSS train 0.24597283648484844 valid 0.3140762255762715
LOSS train 0.24597283648484844 valid 0.3141020661270296
LOSS train 0.24597283648484844 valid 0.3142271116375923
LOSS train 0.24597283648484844 valid 0.3141636012650248
LOSS train 0.24597283648484844 valid 0.31404612368593615
LOSS train 0.24597283648484844 valid 0.31399678940765935
LOSS train 0.24597283648484844 valid 0.31395992958510416
LOSS train 0.24597283648484844 valid 0.3138561825924567
LOSS train 0.24597283648484844 valid 0.31391684504992823
LOSS train 0.24597283648484844 valid 0.31373811122084644
LOSS train 0.24597283648484844 valid 0.31372827409129395
LOSS train 0.24597283648484844 valid 0.3136866214758453
LOSS train 0.24597283648484844 valid 0.31386107115378215
LOSS train 0.24597283648484844 valid 0.31390075523784194
LOSS train 0.24597283648484844 valid 0.3138729996677768
LOSS train 0.24597283648484844 valid 0.3136797202509487
LOSS train 0.24597283648484844 valid 0.31358324492285994
LOSS train 0.24597283648484844 valid 0.3136567981929697
LOSS train 0.24597283648484844 valid 0.3135601011770112
LOSS train 0.24597283648484844 valid 0.3135094913067641
LOSS train 0.24597283648484844 valid 0.31339773345230654
LOSS train 0.24597283648484844 valid 0.31335756229104805
LOSS train 0.24597283648484844 valid 0.3134310874814368
LOSS train 0.24597283648484844 valid 0.31358168532311076
LOSS train 0.24597283648484844 valid 0.3137393920692835
LOSS train 0.24597283648484844 valid 0.31368898882084534
LOSS train 0.24597283648484844 valid 0.313593405793166
LOSS train 0.24597283648484844 valid 0.3136789554257911
LOSS train 0.24597283648484844 valid 0.31362320677273803
LOSS train 0.24597283648484844 valid 0.3136755268081734
LOSS train 0.24597283648484844 valid 0.3137967572110134
LOSS train 0.24597283648484844 valid 0.3135776908519183
LOSS train 0.24597283648484844 valid 0.3135714492873176
LOSS train 0.24597283648484844 valid 0.3136060504472419
LOSS train 0.24597283648484844 valid 0.3135664453679095
LOSS train 0.24597283648484844 valid 0.3134230866987634
LOSS train 0.24597283648484844 valid 0.3134090467558607
LOSS train 0.24597283648484844 valid 0.31352029240066764
EPOCH 16:
  batch 1 loss: 0.26865434646606445
  batch 2 loss: 0.24782859534025192
  batch 3 loss: 0.2449931651353836
  batch 4 loss: 0.24882223084568977
  batch 5 loss: 0.25391130745410917
  batch 6 loss: 0.2506044680873553
  batch 7 loss: 0.2517828962632588
  batch 8 loss: 0.2568737808614969
  batch 9 loss: 0.2547808686892192
  batch 10 loss: 0.2508647233247757
  batch 11 loss: 0.24862130257216367
  batch 12 loss: 0.2435085934897264
  batch 13 loss: 0.24128029208916885
  batch 14 loss: 0.2417833081313542
  batch 15 loss: 0.24333007335662843
  batch 16 loss: 0.24246842227876186
  batch 17 loss: 0.2391838177162058
  batch 18 loss: 0.23946640806065667
  batch 19 loss: 0.23742762600120745
  batch 20 loss: 0.23654333278536796
  batch 21 loss: 0.23841227023374467
  batch 22 loss: 0.23847596821459857
  batch 23 loss: 0.23859362045060034
  batch 24 loss: 0.23663043044507504
  batch 25 loss: 0.2390515214204788
  batch 26 loss: 0.23788206279277802
  batch 27 loss: 0.2388724199047795
  batch 28 loss: 0.23851783307535307
  batch 29 loss: 0.23925286788364936
  batch 30 loss: 0.23917474100987116
  batch 31 loss: 0.24014248242301325
  batch 32 loss: 0.2403873880393803
  batch 33 loss: 0.24069282096443753
  batch 34 loss: 0.2396513867904158
  batch 35 loss: 0.24073580886636461
  batch 36 loss: 0.24118792969319555
  batch 37 loss: 0.24071937195352605
  batch 38 loss: 0.2411829974306257
  batch 39 loss: 0.24041013916333517
  batch 40 loss: 0.24053728878498076
  batch 41 loss: 0.2407968771166918
  batch 42 loss: 0.2413018892208735
  batch 43 loss: 0.2419985907022343
  batch 44 loss: 0.24169061536138708
  batch 45 loss: 0.24144309461116792
  batch 46 loss: 0.24276399385669958
  batch 47 loss: 0.24301859577919574
  batch 48 loss: 0.24234057186792293
  batch 49 loss: 0.24197994111752025
  batch 50 loss: 0.24182351112365721
  batch 51 loss: 0.24219033881729723
  batch 52 loss: 0.24264039786962363
  batch 53 loss: 0.24239838404475517
  batch 54 loss: 0.24292937417825064
  batch 55 loss: 0.24239290410822087
  batch 56 loss: 0.24230622021215303
  batch 57 loss: 0.24208452042780423
  batch 58 loss: 0.24316141728697152
  batch 59 loss: 0.24345922065993486
  batch 60 loss: 0.24370100249846777
  batch 61 loss: 0.24410835836754471
  batch 62 loss: 0.24525604901775236
  batch 63 loss: 0.24537137008848645
  batch 64 loss: 0.2461613160558045
  batch 65 loss: 0.24607241543439717
  batch 66 loss: 0.24618089041023544
  batch 67 loss: 0.24724314083803944
  batch 68 loss: 0.24729389543919003
  batch 69 loss: 0.24739297382209613
  batch 70 loss: 0.24762612985713142
  batch 71 loss: 0.24781134157953127
  batch 72 loss: 0.2479858048674133
  batch 73 loss: 0.24793422201724902
  batch 74 loss: 0.24778667052049894
  batch 75 loss: 0.2479767103989919
  batch 76 loss: 0.2488437071442604
  batch 77 loss: 0.2482381362419624
  batch 78 loss: 0.24827295694595727
  batch 79 loss: 0.2486281987232498
  batch 80 loss: 0.24850842989981176
  batch 81 loss: 0.2484306934071176
  batch 82 loss: 0.24872207369019345
  batch 83 loss: 0.24876596069479562
  batch 84 loss: 0.24870810267471133
  batch 85 loss: 0.24837941965636084
  batch 86 loss: 0.24865308975757555
  batch 87 loss: 0.24869393154807473
  batch 88 loss: 0.24882710809734734
  batch 89 loss: 0.24892112830381716
  batch 90 loss: 0.24847276674376595
  batch 91 loss: 0.248470527948914
  batch 92 loss: 0.2486410301340663
  batch 93 loss: 0.24897006682811246
  batch 94 loss: 0.24912385468153245
  batch 95 loss: 0.24865117684790963
  batch 96 loss: 0.2489068938108782
  batch 97 loss: 0.24897612477700734
  batch 98 loss: 0.24901413659051974
  batch 99 loss: 0.24928092971594648
  batch 100 loss: 0.24918841898441316
  batch 101 loss: 0.24908953108409843
  batch 102 loss: 0.2493438869714737
  batch 103 loss: 0.24961621032177822
  batch 104 loss: 0.24962486068789774
  batch 105 loss: 0.24950788531984602
  batch 106 loss: 0.2499932570839828
  batch 107 loss: 0.25001919798761885
  batch 108 loss: 0.24994436706657763
  batch 109 loss: 0.24973134190664378
  batch 110 loss: 0.25069399936632675
  batch 111 loss: 0.2512142464921281
  batch 112 loss: 0.25111721575792345
  batch 113 loss: 0.25097495251524765
  batch 114 loss: 0.25107913751874056
  batch 115 loss: 0.25112993781981263
  batch 116 loss: 0.2515955499277033
  batch 117 loss: 0.2520493909589246
  batch 118 loss: 0.25189730626041607
  batch 119 loss: 0.2519238358285247
  batch 120 loss: 0.2516234854857127
  batch 121 loss: 0.25150844233095154
  batch 122 loss: 0.2518829869442299
  batch 123 loss: 0.2517778178056081
  batch 124 loss: 0.25187052425838286
  batch 125 loss: 0.2517792139053345
  batch 126 loss: 0.25170696893381694
  batch 127 loss: 0.25218467637309877
  batch 128 loss: 0.25186005560681224
  batch 129 loss: 0.2519390881061554
  batch 130 loss: 0.2517781546482673
  batch 131 loss: 0.25168783564603964
  batch 132 loss: 0.2513856141630447
  batch 133 loss: 0.25150681023759053
  batch 134 loss: 0.25152336739337267
  batch 135 loss: 0.2510960003844014
  batch 136 loss: 0.2510097061886507
  batch 137 loss: 0.250987775460647
  batch 138 loss: 0.250866938220418
  batch 139 loss: 0.2509092743233811
  batch 140 loss: 0.25069730484059877
  batch 141 loss: 0.2509142725814319
  batch 142 loss: 0.25089728181630794
  batch 143 loss: 0.25057679404328753
  batch 144 loss: 0.2506366114442547
  batch 145 loss: 0.25052755372277624
  batch 146 loss: 0.2503588135315947
  batch 147 loss: 0.25054122649488
  batch 148 loss: 0.25062542861780607
  batch 149 loss: 0.25040962712076686
  batch 150 loss: 0.25029720654090243
  batch 151 loss: 0.2502972786592332
  batch 152 loss: 0.25048811841560037
  batch 153 loss: 0.25025715420838274
  batch 154 loss: 0.25027158053277376
  batch 155 loss: 0.2501602481449804
  batch 156 loss: 0.25022476921096826
  batch 157 loss: 0.2504754655869903
  batch 158 loss: 0.25026499272524555
  batch 159 loss: 0.25029394077429984
  batch 160 loss: 0.25003294963389633
  batch 161 loss: 0.25020721739863755
  batch 162 loss: 0.25012619885397547
  batch 163 loss: 0.2502175849273892
  batch 164 loss: 0.2500038483157391
  batch 165 loss: 0.2500975276484634
  batch 166 loss: 0.2500512791146715
  batch 167 loss: 0.2499065584765223
  batch 168 loss: 0.2498671157019479
  batch 169 loss: 0.2499913596189939
  batch 170 loss: 0.24985120576970718
  batch 171 loss: 0.24977722398021765
  batch 172 loss: 0.2498294314326242
  batch 173 loss: 0.24977432622041315
  batch 174 loss: 0.2497589773145215
  batch 175 loss: 0.2497122266462871
  batch 176 loss: 0.24946862569248135
  batch 177 loss: 0.24944832929446872
  batch 178 loss: 0.24967739212044168
  batch 179 loss: 0.24972679137184634
  batch 180 loss: 0.24955072179436683
  batch 181 loss: 0.2496465512896111
  batch 182 loss: 0.24972815394074052
  batch 183 loss: 0.24955386126953397
  batch 184 loss: 0.24945119598313517
  batch 185 loss: 0.24953372744289604
  batch 186 loss: 0.24969709552423927
  batch 187 loss: 0.24954662778798273
  batch 188 loss: 0.24922207964861648
  batch 189 loss: 0.2489890654093374
  batch 190 loss: 0.24885991027480678
  batch 191 loss: 0.24866583264623013
  batch 192 loss: 0.24852916869955757
  batch 193 loss: 0.24843521369862434
  batch 194 loss: 0.24877542218903906
  batch 195 loss: 0.24897705553433833
  batch 196 loss: 0.24897047611219542
  batch 197 loss: 0.24910927446663078
  batch 198 loss: 0.24963546664726854
  batch 199 loss: 0.24951021867481307
  batch 200 loss: 0.2496733368188143
  batch 201 loss: 0.24989401263680625
  batch 202 loss: 0.2501658836361205
  batch 203 loss: 0.2502270238886913
  batch 204 loss: 0.25016241440293835
  batch 205 loss: 0.25035030834558536
  batch 206 loss: 0.25046015689963275
  batch 207 loss: 0.2506941294180598
  batch 208 loss: 0.2506610620050476
  batch 209 loss: 0.25034927209598595
  batch 210 loss: 0.2504015319404148
  batch 211 loss: 0.2502749015652173
  batch 212 loss: 0.2501446886585568
  batch 213 loss: 0.2500014151905624
  batch 214 loss: 0.250013095265794
  batch 215 loss: 0.24982886924300082
  batch 216 loss: 0.24953161745711608
  batch 217 loss: 0.24940633608998242
  batch 218 loss: 0.24935105360976054
  batch 219 loss: 0.24952073089063984
  batch 220 loss: 0.2495121113278649
  batch 221 loss: 0.24961182434634385
  batch 222 loss: 0.24965396966483142
  batch 223 loss: 0.24960422636147572
  batch 224 loss: 0.24955868727660604
  batch 225 loss: 0.24947148044904074
  batch 226 loss: 0.24952049210535743
  batch 227 loss: 0.24942608354900378
  batch 228 loss: 0.2492959234013892
  batch 229 loss: 0.2492156760140798
  batch 230 loss: 0.24921827730925186
  batch 231 loss: 0.24906616745057045
  batch 232 loss: 0.24892365161714883
  batch 233 loss: 0.24883663372932074
  batch 234 loss: 0.24879638697856513
  batch 235 loss: 0.2486583224636443
  batch 236 loss: 0.24846056166846872
  batch 237 loss: 0.2484904632035187
  batch 238 loss: 0.24833343522388393
  batch 239 loss: 0.24819536874982603
  batch 240 loss: 0.24812791782120863
  batch 241 loss: 0.24822609751056338
  batch 242 loss: 0.2480120784487606
  batch 243 loss: 0.24800717499521044
  batch 244 loss: 0.24797436277397344
  batch 245 loss: 0.24785870398793902
  batch 246 loss: 0.24789307078694908
  batch 247 loss: 0.2479985408213457
  batch 248 loss: 0.24797310157408636
  batch 249 loss: 0.24786428676312228
  batch 250 loss: 0.24781336241960525
  batch 251 loss: 0.24781335308494795
  batch 252 loss: 0.24754991571581553
  batch 253 loss: 0.2474464051454906
  batch 254 loss: 0.24722431805424802
  batch 255 loss: 0.24723902853096233
  batch 256 loss: 0.2474032407044433
  batch 257 loss: 0.2474291662645711
  batch 258 loss: 0.24737087614083475
  batch 259 loss: 0.2473397978714534
  batch 260 loss: 0.24725884485703248
  batch 261 loss: 0.24718758794996473
  batch 262 loss: 0.2470935613826941
  batch 263 loss: 0.2470882236390966
  batch 264 loss: 0.24703588150441647
  batch 265 loss: 0.24687841304068295
  batch 266 loss: 0.24683670828441032
  batch 267 loss: 0.24682531145851264
  batch 268 loss: 0.24666918686299183
  batch 269 loss: 0.24658384104865191
  batch 270 loss: 0.24673536083212605
  batch 271 loss: 0.2466845937980497
  batch 272 loss: 0.24659321977592566
  batch 273 loss: 0.24646629966222322
  batch 274 loss: 0.2465374783007768
  batch 275 loss: 0.24649382721294055
  batch 276 loss: 0.24639564245075418
  batch 277 loss: 0.24633411708076078
  batch 278 loss: 0.24611506557507482
  batch 279 loss: 0.24610016946296964
  batch 280 loss: 0.24597891675574438
  batch 281 loss: 0.24581779491858974
  batch 282 loss: 0.24585646972165887
  batch 283 loss: 0.24576849587814548
  batch 284 loss: 0.24582363140415137
  batch 285 loss: 0.2457375692170963
  batch 286 loss: 0.24573467494724513
  batch 287 loss: 0.24581566317986944
  batch 288 loss: 0.2457374413497746
  batch 289 loss: 0.24585874667423407
  batch 290 loss: 0.24565480716269592
  batch 291 loss: 0.2456049010208792
  batch 292 loss: 0.24563857816057663
  batch 293 loss: 0.2456241344123977
  batch 294 loss: 0.2455410282222592
  batch 295 loss: 0.2454862216771659
  batch 296 loss: 0.24550494893982605
  batch 297 loss: 0.2453884496431961
  batch 298 loss: 0.24532216687330463
  batch 299 loss: 0.2452221212279438
  batch 300 loss: 0.2451100866496563
  batch 301 loss: 0.2451097079586745
  batch 302 loss: 0.24499739553557326
  batch 303 loss: 0.24493132728357914
  batch 304 loss: 0.24486955255270004
  batch 305 loss: 0.2447292022040633
  batch 306 loss: 0.244779061355622
  batch 307 loss: 0.24470444324544666
  batch 308 loss: 0.24479343766322384
  batch 309 loss: 0.24472615243354662
  batch 310 loss: 0.24464039836199053
  batch 311 loss: 0.24473020487084648
  batch 312 loss: 0.24483127285463688
  batch 313 loss: 0.24486955281454154
  batch 314 loss: 0.24473673779114036
  batch 315 loss: 0.24469791291252016
  batch 316 loss: 0.24466222312442865
  batch 317 loss: 0.24457348431886558
  batch 318 loss: 0.24451576074901618
  batch 319 loss: 0.24442592683630676
  batch 320 loss: 0.2443394958972931
  batch 321 loss: 0.24442115927411018
  batch 322 loss: 0.24430948042351266
  batch 323 loss: 0.24414789898107664
  batch 324 loss: 0.24394696771546645
  batch 325 loss: 0.2439025126512234
  batch 326 loss: 0.24384536214767058
  batch 327 loss: 0.24382720281589287
  batch 328 loss: 0.24362960162504418
  batch 329 loss: 0.24374553982428868
  batch 330 loss: 0.2437007614608967
  batch 331 loss: 0.24365574044943575
  batch 332 loss: 0.2435291083192969
  batch 333 loss: 0.2436167081823578
  batch 334 loss: 0.24350729620385314
  batch 335 loss: 0.24336933898392007
  batch 336 loss: 0.2432545834176597
  batch 337 loss: 0.24326043799298452
  batch 338 loss: 0.24336177295834355
  batch 339 loss: 0.2433181803750429
  batch 340 loss: 0.24326682594769142
  batch 341 loss: 0.2432610419226532
  batch 342 loss: 0.2432346526990857
  batch 343 loss: 0.24326141817229135
  batch 344 loss: 0.24320060482551886
  batch 345 loss: 0.24325490265652752
  batch 346 loss: 0.24311490326775292
  batch 347 loss: 0.24307215308249855
  batch 348 loss: 0.2430145762775136
  batch 349 loss: 0.2428810239094376
  batch 350 loss: 0.24292852550745012
  batch 351 loss: 0.24293213857714607
  batch 352 loss: 0.2430165093815462
  batch 353 loss: 0.2429686620535999
  batch 354 loss: 0.24308532379609718
  batch 355 loss: 0.24301609040146144
  batch 356 loss: 0.24296897109807208
  batch 357 loss: 0.24292050844647972
  batch 358 loss: 0.2429347672192744
  batch 359 loss: 0.24287376592252247
  batch 360 loss: 0.24278973411354754
  batch 361 loss: 0.24272823754770273
  batch 362 loss: 0.24254951383198164
  batch 363 loss: 0.2423607715644127
  batch 364 loss: 0.24214487960869138
  batch 365 loss: 0.24209872688332648
  batch 366 loss: 0.24196826275743422
  batch 367 loss: 0.24191489153889284
  batch 368 loss: 0.24177715890919385
  batch 369 loss: 0.24172883809905066
  batch 370 loss: 0.2416197722828066
  batch 371 loss: 0.24156445610234037
  batch 372 loss: 0.24152256059710697
  batch 373 loss: 0.24140633235188655
  batch 374 loss: 0.24127637810088734
  batch 375 loss: 0.24116618319352467
  batch 376 loss: 0.2411508227282382
  batch 377 loss: 0.24110432360469505
  batch 378 loss: 0.24103673401648407
  batch 379 loss: 0.24094296240869487
  batch 380 loss: 0.2409692338422725
  batch 381 loss: 0.2409127845069555
  batch 382 loss: 0.24090180019433585
  batch 383 loss: 0.24091719666753675
  batch 384 loss: 0.2408483118051663
  batch 385 loss: 0.24088344291433111
  batch 386 loss: 0.24081640547730143
  batch 387 loss: 0.24088629186923496
  batch 388 loss: 0.2409082018607056
  batch 389 loss: 0.24083170417341904
  batch 390 loss: 0.24080554774174323
  batch 391 loss: 0.24089797492832174
  batch 392 loss: 0.24081909964431306
  batch 393 loss: 0.24075659521815126
  batch 394 loss: 0.24076906920689617
  batch 395 loss: 0.24067586437810826
  batch 396 loss: 0.2405909641147262
  batch 397 loss: 0.24058492200200443
  batch 398 loss: 0.24055571161472616
  batch 399 loss: 0.24060431574669697
  batch 400 loss: 0.24066798944026233
  batch 401 loss: 0.24059282254399803
  batch 402 loss: 0.24062012113741973
  batch 403 loss: 0.24066882068406856
  batch 404 loss: 0.24067115706234876
  batch 405 loss: 0.24069126259397577
  batch 406 loss: 0.24076464379509094
  batch 407 loss: 0.2407848491569116
  batch 408 loss: 0.24089908789770276
  batch 409 loss: 0.24090021280321341
  batch 410 loss: 0.24101208259419696
  batch 411 loss: 0.24102840527275762
  batch 412 loss: 0.2409869049069951
  batch 413 loss: 0.24109311412668114
  batch 414 loss: 0.24111545417043898
  batch 415 loss: 0.24109958618520255
  batch 416 loss: 0.24116827533222163
  batch 417 loss: 0.2411956354725561
  batch 418 loss: 0.24119582822163138
  batch 419 loss: 0.24120607292879737
  batch 420 loss: 0.24121165676485926
  batch 421 loss: 0.24113091242426649
  batch 422 loss: 0.2412309806564408
  batch 423 loss: 0.24116245070670514
  batch 424 loss: 0.24107592581015713
  batch 425 loss: 0.2409878218875212
  batch 426 loss: 0.24092528927074353
  batch 427 loss: 0.24089807851253126
  batch 428 loss: 0.2408546902866007
  batch 429 loss: 0.24092108629522346
  batch 430 loss: 0.2408866234296976
  batch 431 loss: 0.24091236172308778
  batch 432 loss: 0.2408886127932756
  batch 433 loss: 0.2408908654443111
  batch 434 loss: 0.2409165423998635
  batch 435 loss: 0.24083839696029136
  batch 436 loss: 0.24081417788332754
  batch 437 loss: 0.24087305247647267
  batch 438 loss: 0.24095518492400375
  batch 439 loss: 0.24087571133785204
  batch 440 loss: 0.24090894169428131
  batch 441 loss: 0.24075622636985347
  batch 442 loss: 0.24066913906670265
  batch 443 loss: 0.24063756617950802
  batch 444 loss: 0.24053091163168083
  batch 445 loss: 0.2405296397008253
  batch 446 loss: 0.24045890629826105
  batch 447 loss: 0.24037861414003692
  batch 448 loss: 0.2403982281019645
  batch 449 loss: 0.24042753885211818
  batch 450 loss: 0.2403749406668875
  batch 451 loss: 0.2403127811105711
  batch 452 loss: 0.24035907091688266
  batch 453 loss: 0.24041625571171968
  batch 454 loss: 0.24043187367758562
  batch 455 loss: 0.2404902416926164
  batch 456 loss: 0.24052524488223226
  batch 457 loss: 0.24052157107928127
  batch 458 loss: 0.24050109390999033
  batch 459 loss: 0.2405683816712926
  batch 460 loss: 0.2406034320268942
  batch 461 loss: 0.24056017515199044
  batch 462 loss: 0.24058171126233552
  batch 463 loss: 0.24046089994212205
  batch 464 loss: 0.24046581607825798
  batch 465 loss: 0.24031050673095128
  batch 466 loss: 0.2401669261089722
  batch 467 loss: 0.24024350698678346
  batch 468 loss: 0.24016505241011962
  batch 469 loss: 0.24022053169416213
  batch 470 loss: 0.2401719273721918
  batch 471 loss: 0.24026562494963344
  batch 472 loss: 0.24011128679928134
LOSS train 0.24011128679928134 valid 0.32404038310050964
LOSS train 0.24011128679928134 valid 0.3246593475341797
LOSS train 0.24011128679928134 valid 0.3183052639166514
LOSS train 0.24011128679928134 valid 0.31397391855716705
LOSS train 0.24011128679928134 valid 0.311054527759552
LOSS train 0.24011128679928134 valid 0.32042526205380756
LOSS train 0.24011128679928134 valid 0.3330509236880711
LOSS train 0.24011128679928134 valid 0.33077697083353996
LOSS train 0.24011128679928134 valid 0.33375996682378983
LOSS train 0.24011128679928134 valid 0.3339413434267044
LOSS train 0.24011128679928134 valid 0.3324801894751462
LOSS train 0.24011128679928134 valid 0.3341489906112353
LOSS train 0.24011128679928134 valid 0.331344159749838
LOSS train 0.24011128679928134 valid 0.3312036395072937
LOSS train 0.24011128679928134 valid 0.32461938858032224
LOSS train 0.24011128679928134 valid 0.32497991621494293
LOSS train 0.24011128679928134 valid 0.3269992663579829
LOSS train 0.24011128679928134 valid 0.3290332737896178
LOSS train 0.24011128679928134 valid 0.33079972235780014
LOSS train 0.24011128679928134 valid 0.3292464196681976
LOSS train 0.24011128679928134 valid 0.3277072466555096
LOSS train 0.24011128679928134 valid 0.3249365131963383
LOSS train 0.24011128679928134 valid 0.32606522674145905
LOSS train 0.24011128679928134 valid 0.3232153095304966
LOSS train 0.24011128679928134 valid 0.3225150072574616
LOSS train 0.24011128679928134 valid 0.3220959959121851
LOSS train 0.24011128679928134 valid 0.3221385423783903
LOSS train 0.24011128679928134 valid 0.3224211015871593
LOSS train 0.24011128679928134 valid 0.32179215036589526
LOSS train 0.24011128679928134 valid 0.3229944914579391
LOSS train 0.24011128679928134 valid 0.3248300600436426
LOSS train 0.24011128679928134 valid 0.3244622955098748
LOSS train 0.24011128679928134 valid 0.3260067412347505
LOSS train 0.24011128679928134 valid 0.32597940020701466
LOSS train 0.24011128679928134 valid 0.32762893608638216
LOSS train 0.24011128679928134 valid 0.3279395153125127
LOSS train 0.24011128679928134 valid 0.32808927507013885
LOSS train 0.24011128679928134 valid 0.3300029062911084
LOSS train 0.24011128679928134 valid 0.3300273349651924
LOSS train 0.24011128679928134 valid 0.3301205947995186
LOSS train 0.24011128679928134 valid 0.33208213710203405
LOSS train 0.24011128679928134 valid 0.33273602170603617
LOSS train 0.24011128679928134 valid 0.3320082550825075
LOSS train 0.24011128679928134 valid 0.3321535749868913
LOSS train 0.24011128679928134 valid 0.3311153014500936
LOSS train 0.24011128679928134 valid 0.3321912107260331
LOSS train 0.24011128679928134 valid 0.33365601745057616
LOSS train 0.24011128679928134 valid 0.3339376027385394
LOSS train 0.24011128679928134 valid 0.3342028369708937
LOSS train 0.24011128679928134 valid 0.3326581352949142
LOSS train 0.24011128679928134 valid 0.33229875038651857
LOSS train 0.24011128679928134 valid 0.3318835605795567
LOSS train 0.24011128679928134 valid 0.3320711479996735
LOSS train 0.24011128679928134 valid 0.331819333963924
LOSS train 0.24011128679928134 valid 0.3313835382461548
LOSS train 0.24011128679928134 valid 0.33062865478651865
LOSS train 0.24011128679928134 valid 0.3299899927356787
LOSS train 0.24011128679928134 valid 0.32918183967984954
LOSS train 0.24011128679928134 valid 0.3296495468939765
LOSS train 0.24011128679928134 valid 0.32981125513712567
LOSS train 0.24011128679928134 valid 0.3293328930119999
LOSS train 0.24011128679928134 valid 0.3303534960554492
LOSS train 0.24011128679928134 valid 0.33048885067303974
LOSS train 0.24011128679928134 valid 0.3318322845734656
LOSS train 0.24011128679928134 valid 0.33245696654686563
LOSS train 0.24011128679928134 valid 0.3322500902594942
LOSS train 0.24011128679928134 valid 0.3313887853231003
LOSS train 0.24011128679928134 valid 0.33145462590105396
LOSS train 0.24011128679928134 valid 0.33055227994918823
LOSS train 0.24011128679928134 valid 0.33072765256677356
LOSS train 0.24011128679928134 valid 0.3302552020046073
LOSS train 0.24011128679928134 valid 0.33080635592341423
LOSS train 0.24011128679928134 valid 0.33058904131797895
LOSS train 0.24011128679928134 valid 0.33046180936130315
LOSS train 0.24011128679928134 valid 0.3304566419124603
LOSS train 0.24011128679928134 valid 0.3311917723009461
LOSS train 0.24011128679928134 valid 0.3311301779437375
LOSS train 0.24011128679928134 valid 0.33172029715317947
LOSS train 0.24011128679928134 valid 0.33213263674627375
LOSS train 0.24011128679928134 valid 0.33117575757205486
LOSS train 0.24011128679928134 valid 0.3300878315428157
LOSS train 0.24011128679928134 valid 0.3308325251791535
LOSS train 0.24011128679928134 valid 0.3306497089475034
LOSS train 0.24011128679928134 valid 0.3304149109338011
LOSS train 0.24011128679928134 valid 0.33018435713122873
LOSS train 0.24011128679928134 valid 0.32949355885733006
LOSS train 0.24011128679928134 valid 0.32895289704032327
LOSS train 0.24011128679928134 valid 0.3285199767825278
LOSS train 0.24011128679928134 valid 0.32906030553780247
LOSS train 0.24011128679928134 valid 0.3295199766755104
LOSS train 0.24011128679928134 valid 0.3295097978232981
LOSS train 0.24011128679928134 valid 0.3299150883179644
LOSS train 0.24011128679928134 valid 0.3300347400288428
LOSS train 0.24011128679928134 valid 0.3300815112096198
LOSS train 0.24011128679928134 valid 0.3298726334383613
LOSS train 0.24011128679928134 valid 0.3304684200945
LOSS train 0.24011128679928134 valid 0.330491108224564
LOSS train 0.24011128679928134 valid 0.3308081391210459
LOSS train 0.24011128679928134 valid 0.33110632273283874
LOSS train 0.24011128679928134 valid 0.33108684584498405
LOSS train 0.24011128679928134 valid 0.33130996843965926
LOSS train 0.24011128679928134 valid 0.33137788068430096
LOSS train 0.24011128679928134 valid 0.33127272056723106
LOSS train 0.24011128679928134 valid 0.3311822828478538
LOSS train 0.24011128679928134 valid 0.33116933405399324
LOSS train 0.24011128679928134 valid 0.3313957707218404
LOSS train 0.24011128679928134 valid 0.3313851341187397
LOSS train 0.24011128679928134 valid 0.3315320673088233
LOSS train 0.24011128679928134 valid 0.33200665392460077
LOSS train 0.24011128679928134 valid 0.33238479969176377
LOSS train 0.24011128679928134 valid 0.33187640183143785
LOSS train 0.24011128679928134 valid 0.3317849605477282
LOSS train 0.24011128679928134 valid 0.33168780052028923
LOSS train 0.24011128679928134 valid 0.33134623041801287
LOSS train 0.24011128679928134 valid 0.33155858581480774
LOSS train 0.24011128679928134 valid 0.3313740207974253
LOSS train 0.24011128679928134 valid 0.3315257818525673
LOSS train 0.24011128679928134 valid 0.33135703243946624
LOSS train 0.24011128679928134 valid 0.33129486980057565
LOSS train 0.24011128679928134 valid 0.33092622173329195
LOSS train 0.24011128679928134 valid 0.330888127369329
LOSS train 0.24011128679928134 valid 0.3304426038607222
LOSS train 0.24011128679928134 valid 0.33036407477002805
LOSS train 0.24011128679928134 valid 0.3309524791134942
LOSS train 0.24011128679928134 valid 0.3308631623983383
LOSS train 0.24011128679928134 valid 0.33120717640433994
LOSS train 0.24011128679928134 valid 0.3311978382623102
LOSS train 0.24011128679928134 valid 0.33172001608181745
LOSS train 0.24011128679928134 valid 0.3319951029017914
LOSS train 0.24011128679928134 valid 0.3317987718261205
LOSS train 0.24011128679928134 valid 0.33169548531980003
LOSS train 0.24011128679928134 valid 0.33132597905668343
LOSS train 0.24011128679928134 valid 0.33132009589134304
LOSS train 0.24011128679928134 valid 0.33152923088020353
LOSS train 0.24011128679928134 valid 0.33140851336496846
LOSS train 0.24011128679928134 valid 0.33131384005879655
LOSS train 0.24011128679928134 valid 0.33117907684649867
LOSS train 0.24011128679928134 valid 0.3311795884932297
LOSS train 0.24011128679928134 valid 0.33110645252594845
LOSS train 0.24011128679928134 valid 0.3313221387565136
LOSS train 0.24011128679928134 valid 0.33138752036483576
LOSS train 0.24011128679928134 valid 0.3317985938678325
LOSS train 0.24011128679928134 valid 0.3316872458566319
LOSS train 0.24011128679928134 valid 0.33146475721150637
LOSS train 0.24011128679928134 valid 0.33132169852996696
LOSS train 0.24011128679928134 valid 0.3315249668204621
LOSS train 0.24011128679928134 valid 0.33137958380235294
LOSS train 0.24011128679928134 valid 0.33240492511037234
LOSS train 0.24011128679928134 valid 0.3325602293214542
LOSS train 0.24011128679928134 valid 0.33294046491384505
LOSS train 0.24011128679928134 valid 0.33294836150494633
LOSS train 0.24011128679928134 valid 0.3326890336251573
LOSS train 0.24011128679928134 valid 0.3328572125021928
LOSS train 0.24011128679928134 valid 0.33278141267500916
LOSS train 0.24011128679928134 valid 0.33275336959669666
LOSS train 0.24011128679928134 valid 0.3329185647651171
LOSS train 0.24011128679928134 valid 0.33282454065076866
LOSS train 0.24011128679928134 valid 0.33282969637384896
LOSS train 0.24011128679928134 valid 0.33288645809926326
LOSS train 0.24011128679928134 valid 0.3329400076530874
LOSS train 0.24011128679928134 valid 0.33277790951802866
LOSS train 0.24011128679928134 valid 0.33266237240146707
LOSS train 0.24011128679928134 valid 0.332478493360654
LOSS train 0.24011128679928134 valid 0.33207094024230793
LOSS train 0.24011128679928134 valid 0.3318465291550665
LOSS train 0.24011128679928134 valid 0.3319120790405446
LOSS train 0.24011128679928134 valid 0.33229648058642886
LOSS train 0.24011128679928134 valid 0.33202750617194743
LOSS train 0.24011128679928134 valid 0.3322530932856735
LOSS train 0.24011128679928134 valid 0.33245829545399724
LOSS train 0.24011128679928134 valid 0.33237959827944547
LOSS train 0.24011128679928134 valid 0.33219384774565697
LOSS train 0.24011128679928134 valid 0.3321798975240288
LOSS train 0.24011128679928134 valid 0.3323015943169594
LOSS train 0.24011128679928134 valid 0.3320727892432894
LOSS train 0.24011128679928134 valid 0.33196736609732563
LOSS train 0.24011128679928134 valid 0.3320773002286415
LOSS train 0.24011128679928134 valid 0.3323873543504919
LOSS train 0.24011128679928134 valid 0.33219844394222986
LOSS train 0.24011128679928134 valid 0.3321060040758716
LOSS train 0.24011128679928134 valid 0.33220263253588705
LOSS train 0.24011128679928134 valid 0.3323951182620866
LOSS train 0.24011128679928134 valid 0.33255635609066553
LOSS train 0.24011128679928134 valid 0.3326862306212601
LOSS train 0.24011128679928134 valid 0.33224647359268084
LOSS train 0.24011128679928134 valid 0.3321867677313025
LOSS train 0.24011128679928134 valid 0.3321260313458621
LOSS train 0.24011128679928134 valid 0.33220152945277537
LOSS train 0.24011128679928134 valid 0.33207418087613644
LOSS train 0.24011128679928134 valid 0.3321829730742856
LOSS train 0.24011128679928134 valid 0.33208057167330335
LOSS train 0.24011128679928134 valid 0.3320790072126935
LOSS train 0.24011128679928134 valid 0.33192204633833833
LOSS train 0.24011128679928134 valid 0.3316974204379259
LOSS train 0.24011128679928134 valid 0.3314518600702286
LOSS train 0.24011128679928134 valid 0.331406330043564
LOSS train 0.24011128679928134 valid 0.33165848156824934
LOSS train 0.24011128679928134 valid 0.33140808366464847
LOSS train 0.24011128679928134 valid 0.331402384231438
LOSS train 0.24011128679928134 valid 0.3313359319418669
LOSS train 0.24011128679928134 valid 0.33122293458352636
LOSS train 0.24011128679928134 valid 0.33119132340249446
LOSS train 0.24011128679928134 valid 0.33095371172639537
LOSS train 0.24011128679928134 valid 0.33096616050484134
LOSS train 0.24011128679928134 valid 0.33062707600070207
LOSS train 0.24011128679928134 valid 0.3307401686183457
LOSS train 0.24011128679928134 valid 0.33063116100963186
LOSS train 0.24011128679928134 valid 0.33034240189366615
LOSS train 0.24011128679928134 valid 0.33028107632005044
LOSS train 0.24011128679928134 valid 0.330404892492862
LOSS train 0.24011128679928134 valid 0.3306093434163179
LOSS train 0.24011128679928134 valid 0.33055218695750777
LOSS train 0.24011128679928134 valid 0.3308023626395794
LOSS train 0.24011128679928134 valid 0.3307146994727794
LOSS train 0.24011128679928134 valid 0.3305051852797353
LOSS train 0.24011128679928134 valid 0.33045211330884033
LOSS train 0.24011128679928134 valid 0.33040821710215185
LOSS train 0.24011128679928134 valid 0.33051613772954413
LOSS train 0.24011128679928134 valid 0.3306428055921102
LOSS train 0.24011128679928134 valid 0.33078301081603223
LOSS train 0.24011128679928134 valid 0.3308531734333858
LOSS train 0.24011128679928134 valid 0.3307209980783162
LOSS train 0.24011128679928134 valid 0.33094931470706324
LOSS train 0.24011128679928134 valid 0.3310021589776235
LOSS train 0.24011128679928134 valid 0.3309472185373306
LOSS train 0.24011128679928134 valid 0.3310166798466075
LOSS train 0.24011128679928134 valid 0.3314353443583728
LOSS train 0.24011128679928134 valid 0.331461399597557
LOSS train 0.24011128679928134 valid 0.3315542516619878
LOSS train 0.24011128679928134 valid 0.33161345765642497
LOSS train 0.24011128679928134 valid 0.33167028265856047
LOSS train 0.24011128679928134 valid 0.3317623959918474
LOSS train 0.24011128679928134 valid 0.3317833063812215
LOSS train 0.24011128679928134 valid 0.3317678531902468
LOSS train 0.24011128679928134 valid 0.3320925755069611
LOSS train 0.24011128679928134 valid 0.3318535346474688
LOSS train 0.24011128679928134 valid 0.33194531538063965
LOSS train 0.24011128679928134 valid 0.3320375123069066
LOSS train 0.24011128679928134 valid 0.3318986294783309
LOSS train 0.24011128679928134 valid 0.33183680605143306
LOSS train 0.24011128679928134 valid 0.3318448872363419
LOSS train 0.24011128679928134 valid 0.33158205541943714
LOSS train 0.24011128679928134 valid 0.33179268933863304
LOSS train 0.24011128679928134 valid 0.33222207848410135
LOSS train 0.24011128679928134 valid 0.33252592153695165
LOSS train 0.24011128679928134 valid 0.3324685589205928
LOSS train 0.24011128679928134 valid 0.33248431715164106
LOSS train 0.24011128679928134 valid 0.332423142067367
LOSS train 0.24011128679928134 valid 0.3324138559849866
LOSS train 0.24011128679928134 valid 0.3326565825343132
LOSS train 0.24011128679928134 valid 0.33276747511440063
LOSS train 0.24011128679928134 valid 0.3330223120985523
LOSS train 0.24011128679928134 valid 0.3330522717341133
LOSS train 0.24011128679928134 valid 0.3328792885180533
LOSS train 0.24011128679928134 valid 0.3329357339470994
LOSS train 0.24011128679928134 valid 0.33307102910475805
LOSS train 0.24011128679928134 valid 0.3329225121653034
LOSS train 0.24011128679928134 valid 0.3331158214876818
LOSS train 0.24011128679928134 valid 0.3330958117964645
LOSS train 0.24011128679928134 valid 0.33301647173670623
LOSS train 0.24011128679928134 valid 0.33322675069401547
LOSS train 0.24011128679928134 valid 0.3332583925309982
LOSS train 0.24011128679928134 valid 0.3333366486622807
LOSS train 0.24011128679928134 valid 0.333367562824578
LOSS train 0.24011128679928134 valid 0.33327127048429456
LOSS train 0.24011128679928134 valid 0.3334180833141607
LOSS train 0.24011128679928134 valid 0.3334569822201568
LOSS train 0.24011128679928134 valid 0.33363400994619324
LOSS train 0.24011128679928134 valid 0.3335707641334782
LOSS train 0.24011128679928134 valid 0.33348620263514694
LOSS train 0.24011128679928134 valid 0.3336626894469631
LOSS train 0.24011128679928134 valid 0.3339739865349496
LOSS train 0.24011128679928134 valid 0.33404643255057354
LOSS train 0.24011128679928134 valid 0.3339574908579353
LOSS train 0.24011128679928134 valid 0.3338783368739215
LOSS train 0.24011128679928134 valid 0.33373358926695323
LOSS train 0.24011128679928134 valid 0.3335585977734211
LOSS train 0.24011128679928134 valid 0.3334153031380914
LOSS train 0.24011128679928134 valid 0.33345352540520357
LOSS train 0.24011128679928134 valid 0.33340557446437225
LOSS train 0.24011128679928134 valid 0.3332296284810504
LOSS train 0.24011128679928134 valid 0.332986999223841
LOSS train 0.24011128679928134 valid 0.3328884090741195
LOSS train 0.24011128679928134 valid 0.3329436903373456
LOSS train 0.24011128679928134 valid 0.333083961773337
LOSS train 0.24011128679928134 valid 0.33306648102882025
LOSS train 0.24011128679928134 valid 0.3330350917494671
LOSS train 0.24011128679928134 valid 0.3329466031346884
LOSS train 0.24011128679928134 valid 0.3330501030782515
LOSS train 0.24011128679928134 valid 0.3330743521965783
LOSS train 0.24011128679928134 valid 0.3330001218110016
LOSS train 0.24011128679928134 valid 0.333054641083087
LOSS train 0.24011128679928134 valid 0.33308411351446404
LOSS train 0.24011128679928134 valid 0.3332891507314987
LOSS train 0.24011128679928134 valid 0.3333839425091016
LOSS train 0.24011128679928134 valid 0.3333172509698449
LOSS train 0.24011128679928134 valid 0.3333208844416872
LOSS train 0.24011128679928134 valid 0.3332709222531959
LOSS train 0.24011128679928134 valid 0.3332590204118486
LOSS train 0.24011128679928134 valid 0.333120341549317
LOSS train 0.24011128679928134 valid 0.33312974276138696
LOSS train 0.24011128679928134 valid 0.3330962745263087
LOSS train 0.24011128679928134 valid 0.33321339782118403
LOSS train 0.24011128679928134 valid 0.33317546796445785
LOSS train 0.24011128679928134 valid 0.33306602684200787
LOSS train 0.24011128679928134 valid 0.33305173269868676
LOSS train 0.24011128679928134 valid 0.332889717011576
LOSS train 0.24011128679928134 valid 0.33277330726578636
LOSS train 0.24011128679928134 valid 0.33273726098938666
LOSS train 0.24011128679928134 valid 0.33284861921302733
LOSS train 0.24011128679928134 valid 0.33271942898583184
LOSS train 0.24011128679928134 valid 0.33271257106501323
LOSS train 0.24011128679928134 valid 0.33271209552836495
LOSS train 0.24011128679928134 valid 0.33279564516369703
LOSS train 0.24011128679928134 valid 0.33269479544389813
LOSS train 0.24011128679928134 valid 0.3325976862843278
LOSS train 0.24011128679928134 valid 0.33268594633705606
LOSS train 0.24011128679928134 valid 0.33281715706271947
LOSS train 0.24011128679928134 valid 0.3330516649357578
LOSS train 0.24011128679928134 valid 0.332999239070341
LOSS train 0.24011128679928134 valid 0.3332650052888371
LOSS train 0.24011128679928134 valid 0.3331135088890236
LOSS train 0.24011128679928134 valid 0.332963339361613
LOSS train 0.24011128679928134 valid 0.3329748537529398
LOSS train 0.24011128679928134 valid 0.3330207179601376
LOSS train 0.24011128679928134 valid 0.3331137168352589
LOSS train 0.24011128679928134 valid 0.33313637137959856
LOSS train 0.24011128679928134 valid 0.3331967421875494
LOSS train 0.24011128679928134 valid 0.33326272068596174
LOSS train 0.24011128679928134 valid 0.3332932620789065
LOSS train 0.24011128679928134 valid 0.3331931768677386
LOSS train 0.24011128679928134 valid 0.33311589282140674
LOSS train 0.24011128679928134 valid 0.33314886720509856
LOSS train 0.24011128679928134 valid 0.3332780542041727
LOSS train 0.24011128679928134 valid 0.33321873674641794
LOSS train 0.24011128679928134 valid 0.33307135561924606
LOSS train 0.24011128679928134 valid 0.33307134640853553
LOSS train 0.24011128679928134 valid 0.3330331609182104
LOSS train 0.24011128679928134 valid 0.3328783001783675
LOSS train 0.24011128679928134 valid 0.3329197226640056
LOSS train 0.24011128679928134 valid 0.33273586442918024
LOSS train 0.24011128679928134 valid 0.33272705314277906
LOSS train 0.24011128679928134 valid 0.3326878715795261
LOSS train 0.24011128679928134 valid 0.33286940665958925
LOSS train 0.24011128679928134 valid 0.3329200969658036
LOSS train 0.24011128679928134 valid 0.3328878766281067
LOSS train 0.24011128679928134 valid 0.33268757439312396
LOSS train 0.24011128679928134 valid 0.3325978175405113
LOSS train 0.24011128679928134 valid 0.3326634560153956
LOSS train 0.24011128679928134 valid 0.3325941466859409
LOSS train 0.24011128679928134 valid 0.33252525571574515
LOSS train 0.24011128679928134 valid 0.3324045344594527
LOSS train 0.24011128679928134 valid 0.3323608784179174
LOSS train 0.24011128679928134 valid 0.33243588821194264
LOSS train 0.24011128679928134 valid 0.33260515202099167
LOSS train 0.24011128679928134 valid 0.3327699114013924
LOSS train 0.24011128679928134 valid 0.3327383032950367
LOSS train 0.24011128679928134 valid 0.3326509399703761
LOSS train 0.24011128679928134 valid 0.3327143005092828
LOSS train 0.24011128679928134 valid 0.3326916349016958
LOSS train 0.24011128679928134 valid 0.33273709782107713
LOSS train 0.24011128679928134 valid 0.3328830964904464
LOSS train 0.24011128679928134 valid 0.33264843039932984
LOSS train 0.24011128679928134 valid 0.3326180269429972
LOSS train 0.24011128679928134 valid 0.3326521159851388
LOSS train 0.24011128679928134 valid 0.33259436927857944
LOSS train 0.24011128679928134 valid 0.332426371220347
LOSS train 0.24011128679928134 valid 0.3323903703333243
LOSS train 0.24011128679928134 valid 0.33252099503669635
EPOCH 17:
  batch 1 loss: 0.27133679389953613
  batch 2 loss: 0.24407747387886047
  batch 3 loss: 0.24026298026243845
  batch 4 loss: 0.24404209479689598
  batch 5 loss: 0.24978059232234956
  batch 6 loss: 0.2462668940424919
  batch 7 loss: 0.2477540224790573
  batch 8 loss: 0.2534364927560091
  batch 9 loss: 0.25126276082462734
  batch 10 loss: 0.2474124550819397
  batch 11 loss: 0.24509115517139435
  batch 12 loss: 0.24216622859239578
  batch 13 loss: 0.2400768124140226
  batch 14 loss: 0.23954404145479202
  batch 15 loss: 0.23990460435549418
  batch 16 loss: 0.23948356695473194
  batch 17 loss: 0.23676388579256394
  batch 18 loss: 0.23632030768526924
  batch 19 loss: 0.23374927906613602
  batch 20 loss: 0.232623827457428
  batch 21 loss: 0.23388719132968358
  batch 22 loss: 0.233406509865414
  batch 23 loss: 0.2335736881131711
  batch 24 loss: 0.23162719483176866
  batch 25 loss: 0.23381995558738708
  batch 26 loss: 0.23261295488247505
  batch 27 loss: 0.23340887824694315
  batch 28 loss: 0.23255981824227742
  batch 29 loss: 0.2334289458291284
  batch 30 loss: 0.23349814713001252
  batch 31 loss: 0.2343228622790306
  batch 32 loss: 0.23399107158184052
  batch 33 loss: 0.2344686948891842
  batch 34 loss: 0.2332864044343724
  batch 35 loss: 0.23360410034656526
  batch 36 loss: 0.23436769470572472
  batch 37 loss: 0.23420430800399264
  batch 38 loss: 0.2345592410940873
  batch 39 loss: 0.23413515434815332
  batch 40 loss: 0.23451193310320378
  batch 41 loss: 0.23441331779084554
  batch 42 loss: 0.2345892588297526
  batch 43 loss: 0.23478470500125442
  batch 44 loss: 0.23435092615810307
  batch 45 loss: 0.23362004690700108
  batch 46 loss: 0.234621269547421
  batch 47 loss: 0.2345497576480216
  batch 48 loss: 0.2337936796247959
  batch 49 loss: 0.23362782567131277
  batch 50 loss: 0.23351931363344192
  batch 51 loss: 0.23368599806346146
  batch 52 loss: 0.23428576812148094
  batch 53 loss: 0.23405216915427512
  batch 54 loss: 0.23439937471239655
  batch 55 loss: 0.23399182774803856
  batch 56 loss: 0.23391903617552348
  batch 57 loss: 0.23381432995461582
  batch 58 loss: 0.2349789260790266
  batch 59 loss: 0.23514712917602668
  batch 60 loss: 0.23517333169778187
  batch 61 loss: 0.2356073700013708
  batch 62 loss: 0.2365484449171251
  batch 63 loss: 0.2363158601617056
  batch 64 loss: 0.23723116470500827
  batch 65 loss: 0.23731726522629076
  batch 66 loss: 0.2374341031818679
  batch 67 loss: 0.23831436749714524
  batch 68 loss: 0.23838325765203028
  batch 69 loss: 0.23852601064288098
  batch 70 loss: 0.23880196405308587
  batch 71 loss: 0.23856786711954733
  batch 72 loss: 0.23886056761774752
  batch 73 loss: 0.23889275344267283
  batch 74 loss: 0.23880930869160472
  batch 75 loss: 0.2387118246157964
  batch 76 loss: 0.23928161337971687
  batch 77 loss: 0.2387669210310106
  batch 78 loss: 0.23871235186472917
  batch 79 loss: 0.2388747438222547
  batch 80 loss: 0.2383555954322219
  batch 81 loss: 0.2384150986686165
  batch 82 loss: 0.23891922886051783
  batch 83 loss: 0.2388887303062232
  batch 84 loss: 0.23893185687207041
  batch 85 loss: 0.23872869558194104
  batch 86 loss: 0.23899604362803836
  batch 87 loss: 0.2390325691165595
  batch 88 loss: 0.2391631818291816
  batch 89 loss: 0.23920347549942103
  batch 90 loss: 0.2388716792066892
  batch 91 loss: 0.23882189131045078
  batch 92 loss: 0.23877318932310396
  batch 93 loss: 0.2391479987931508
  batch 94 loss: 0.23932120314937957
  batch 95 loss: 0.23891354501247405
  batch 96 loss: 0.23895874759182334
  batch 97 loss: 0.23892102152416386
  batch 98 loss: 0.23900512818779265
  batch 99 loss: 0.23953719587639125
  batch 100 loss: 0.23946693584322928
  batch 101 loss: 0.2393019003148126
  batch 102 loss: 0.23968703913338044
  batch 103 loss: 0.24004346349285643
  batch 104 loss: 0.24024567967997149
  batch 105 loss: 0.2401512384414673
  batch 106 loss: 0.24052908201262635
  batch 107 loss: 0.24052674622736245
  batch 108 loss: 0.2405260570899204
  batch 109 loss: 0.24038346357848667
  batch 110 loss: 0.24117730137976734
  batch 111 loss: 0.24165113968355162
  batch 112 loss: 0.24148841348609754
  batch 113 loss: 0.2413205619143174
  batch 114 loss: 0.24169683469492093
  batch 115 loss: 0.24177897948285806
  batch 116 loss: 0.24212168632396336
  batch 117 loss: 0.24224761166633704
  batch 118 loss: 0.24205648128764104
  batch 119 loss: 0.24217029016057984
  batch 120 loss: 0.24182593586544196
  batch 121 loss: 0.24159222755057752
  batch 122 loss: 0.24166507567050027
  batch 123 loss: 0.24152246735444882
  batch 124 loss: 0.2416999422975125
  batch 125 loss: 0.24165495574474335
  batch 126 loss: 0.24152364818349717
  batch 127 loss: 0.24176232681030363
  batch 128 loss: 0.24145861831493676
  batch 129 loss: 0.24161948922068574
  batch 130 loss: 0.24151881681038784
  batch 131 loss: 0.2414889576780887
  batch 132 loss: 0.2411891706287861
  batch 133 loss: 0.24134638087641924
  batch 134 loss: 0.24137797362324018
  batch 135 loss: 0.24093878578256678
  batch 136 loss: 0.24088545822921922
  batch 137 loss: 0.24070854223992702
  batch 138 loss: 0.24050166825021524
  batch 139 loss: 0.24062346736733004
  batch 140 loss: 0.2405019859118121
  batch 141 loss: 0.2408982607278418
  batch 142 loss: 0.24097305987502488
  batch 143 loss: 0.24064387047624256
  batch 144 loss: 0.24064607618169653
  batch 145 loss: 0.24059513509273528
  batch 146 loss: 0.24054902312282014
  batch 147 loss: 0.2408558390781182
  batch 148 loss: 0.24092387418086464
  batch 149 loss: 0.24066653597674914
  batch 150 loss: 0.24057327995697658
  batch 151 loss: 0.2405855269818906
  batch 152 loss: 0.2407872303339996
  batch 153 loss: 0.24058245278261844
  batch 154 loss: 0.2406521759830512
  batch 155 loss: 0.24059630709309732
  batch 156 loss: 0.24061058996579585
  batch 157 loss: 0.2408429697440688
  batch 158 loss: 0.24068263437174545
  batch 159 loss: 0.24074899756683493
  batch 160 loss: 0.24051613481715323
  batch 161 loss: 0.24059083234079137
  batch 162 loss: 0.24050055370654588
  batch 163 loss: 0.24062245371151555
  batch 164 loss: 0.2402795266087462
  batch 165 loss: 0.24030595846248395
  batch 166 loss: 0.24028728327837334
  batch 167 loss: 0.24015330090494214
  batch 168 loss: 0.2400602355067219
  batch 169 loss: 0.23999830904091604
  batch 170 loss: 0.2397872195524328
  batch 171 loss: 0.23976098375710828
  batch 172 loss: 0.23973791630462157
  batch 173 loss: 0.23966717702804963
  batch 174 loss: 0.23966020791009926
  batch 175 loss: 0.23964792617729733
  batch 176 loss: 0.23951301689852367
  batch 177 loss: 0.23953186447000774
  batch 178 loss: 0.2397200208199158
  batch 179 loss: 0.23973935742617986
  batch 180 loss: 0.2395766297976176
  batch 181 loss: 0.23974140029585822
  batch 182 loss: 0.23976009863091038
  batch 183 loss: 0.23957576785908372
  batch 184 loss: 0.23949422695390557
  batch 185 loss: 0.2394815057516098
  batch 186 loss: 0.23963653280209468
  batch 187 loss: 0.23944185578568097
  batch 188 loss: 0.23914688809755
  batch 189 loss: 0.2388680746788701
  batch 190 loss: 0.23878462612628937
  batch 191 loss: 0.23865079942173983
  batch 192 loss: 0.2386685472447425
  batch 193 loss: 0.238478803495669
  batch 194 loss: 0.23865477364395082
  batch 195 loss: 0.2387925686744543
  batch 196 loss: 0.23871335181958822
  batch 197 loss: 0.23876628869681188
  batch 198 loss: 0.23906957937611473
  batch 199 loss: 0.23896130732255966
  batch 200 loss: 0.2391632155328989
  batch 201 loss: 0.2393820250953608
  batch 202 loss: 0.23953441899306704
  batch 203 loss: 0.23953426383399024
  batch 204 loss: 0.23939981377300093
  batch 205 loss: 0.23974375121477173
  batch 206 loss: 0.23985814983115614
  batch 207 loss: 0.24002165818847895
  batch 208 loss: 0.2399523902254609
  batch 209 loss: 0.23979027013174084
  batch 210 loss: 0.23995639703103475
  batch 211 loss: 0.2398526000468087
  batch 212 loss: 0.23974480472926823
  batch 213 loss: 0.23959934011871267
  batch 214 loss: 0.2396929875434002
  batch 215 loss: 0.23953139068082321
  batch 216 loss: 0.23931705075557586
  batch 217 loss: 0.23918989815744937
  batch 218 loss: 0.2390649667983755
  batch 219 loss: 0.23913404270666375
  batch 220 loss: 0.2390636639838869
  batch 221 loss: 0.23913482242849618
  batch 222 loss: 0.23919834740258553
  batch 223 loss: 0.23918551567424037
  batch 224 loss: 0.23919197171926498
  batch 225 loss: 0.23912651194466486
  batch 226 loss: 0.2391774692630346
  batch 227 loss: 0.2390864570235366
  batch 228 loss: 0.2389415090152046
  batch 229 loss: 0.2389620197001503
  batch 230 loss: 0.2389464901841205
  batch 231 loss: 0.23880595175218788
  batch 232 loss: 0.23864738129336258
  batch 233 loss: 0.23862243952157672
  batch 234 loss: 0.2385517630057457
  batch 235 loss: 0.23841042252297098
  batch 236 loss: 0.23827285925715658
  batch 237 loss: 0.2382858635755531
  batch 238 loss: 0.23813423965157582
  batch 239 loss: 0.2380412182299163
  batch 240 loss: 0.2379917540277044
  batch 241 loss: 0.23815243749944995
  batch 242 loss: 0.23803544826497716
  batch 243 loss: 0.23800776028093487
  batch 244 loss: 0.23796653668167161
  batch 245 loss: 0.237892106479528
  batch 246 loss: 0.23798077341502274
  batch 247 loss: 0.23807402021489163
  batch 248 loss: 0.23802842650442355
  batch 249 loss: 0.23795968407847318
  batch 250 loss: 0.23791856646537782
  batch 251 loss: 0.23786958452477397
  batch 252 loss: 0.23764224084360258
  batch 253 loss: 0.2375643395859262
  batch 254 loss: 0.23737353907795403
  batch 255 loss: 0.23738880268498963
  batch 256 loss: 0.23752339946804568
  batch 257 loss: 0.23763614817584072
  batch 258 loss: 0.23759488085674685
  batch 259 loss: 0.23762393296915593
  batch 260 loss: 0.23757924225467902
  batch 261 loss: 0.23755327557923694
  batch 262 loss: 0.2374925412635767
  batch 263 loss: 0.23752321869474854
  batch 264 loss: 0.2374620469795032
  batch 265 loss: 0.2373163488675963
  batch 266 loss: 0.237277063996272
  batch 267 loss: 0.23728319416555127
  batch 268 loss: 0.23713552089991854
  batch 269 loss: 0.23709229150004546
  batch 270 loss: 0.23727859849179234
  batch 271 loss: 0.2372234140924862
  batch 272 loss: 0.2371187086271889
  batch 273 loss: 0.23702003340144734
  batch 274 loss: 0.23709216735658856
  batch 275 loss: 0.23706888924945485
  batch 276 loss: 0.23697341929959215
  batch 277 loss: 0.23689380293503565
  batch 278 loss: 0.2366849748457936
  batch 279 loss: 0.23669395315390762
  batch 280 loss: 0.23661942875811032
  batch 281 loss: 0.23651890774850742
  batch 282 loss: 0.23654628571466352
  batch 283 loss: 0.23643610102549037
  batch 284 loss: 0.23645395580941522
  batch 285 loss: 0.23643497469132407
  batch 286 loss: 0.23645590162360586
  batch 287 loss: 0.2365029591301177
  batch 288 loss: 0.23640914442431596
  batch 289 loss: 0.23655724984345552
  batch 290 loss: 0.2363765363549364
  batch 291 loss: 0.23635723910380885
  batch 292 loss: 0.23644336323215537
  batch 293 loss: 0.2364242187742487
  batch 294 loss: 0.23638300952457247
  batch 295 loss: 0.23637998508194746
  batch 296 loss: 0.23643530542786056
  batch 297 loss: 0.23640008075068694
  batch 298 loss: 0.23631919065377857
  batch 299 loss: 0.23625078149463818
  batch 300 loss: 0.23615229825178782
  batch 301 loss: 0.23620362832300687
  batch 302 loss: 0.23609068545679382
  batch 303 loss: 0.23601726843382265
  batch 304 loss: 0.23599388724879214
  batch 305 loss: 0.23583719505638373
  batch 306 loss: 0.23588218700652028
  batch 307 loss: 0.23589194534461738
  batch 308 loss: 0.23600430897884556
  batch 309 loss: 0.23595911346400053
  batch 310 loss: 0.2358991467183636
  batch 311 loss: 0.23601907683338766
  batch 312 loss: 0.2362024347560528
  batch 313 loss: 0.2362619220448759
  batch 314 loss: 0.23619030700747373
  batch 315 loss: 0.2361517249591767
  batch 316 loss: 0.23609404355476174
  batch 317 loss: 0.2360297454644453
  batch 318 loss: 0.23600536092834654
  batch 319 loss: 0.23593513667583466
  batch 320 loss: 0.23592121326364576
  batch 321 loss: 0.2359894589750195
  batch 322 loss: 0.23589921372462502
  batch 323 loss: 0.2357668285214864
  batch 324 loss: 0.23558502975437376
  batch 325 loss: 0.23556109414650844
  batch 326 loss: 0.2355340373205261
  batch 327 loss: 0.23554798359170967
  batch 328 loss: 0.2353425309425447
  batch 329 loss: 0.23538782187145893
  batch 330 loss: 0.2353398357376908
  batch 331 loss: 0.2352815643178012
  batch 332 loss: 0.2351890149335545
  batch 333 loss: 0.23528290354274772
  batch 334 loss: 0.23511557905616876
  batch 335 loss: 0.23503461297768266
  batch 336 loss: 0.23491983332981667
  batch 337 loss: 0.23491068838788776
  batch 338 loss: 0.2349664782719499
  batch 339 loss: 0.23494793487861093
  batch 340 loss: 0.2349222734132234
  batch 341 loss: 0.23496719063027513
  batch 342 loss: 0.23490154337987565
  batch 343 loss: 0.23488363271725768
  batch 344 loss: 0.2348700821572958
  batch 345 loss: 0.23495656122332034
  batch 346 loss: 0.2348213639390262
  batch 347 loss: 0.23481156773629036
  batch 348 loss: 0.23473468478554968
  batch 349 loss: 0.23464466396580452
  batch 350 loss: 0.23468545470918928
  batch 351 loss: 0.23470380116901507
  batch 352 loss: 0.234741892834956
  batch 353 loss: 0.2346758145110803
  batch 354 loss: 0.2348024536997585
  batch 355 loss: 0.23473175973959373
  batch 356 loss: 0.23467520858799473
  batch 357 loss: 0.234594532421657
  batch 358 loss: 0.2346273358057997
  batch 359 loss: 0.2345581605905942
  batch 360 loss: 0.23447198180688753
  batch 361 loss: 0.23444601319668365
  batch 362 loss: 0.2342754384266079
  batch 363 loss: 0.2341242135392076
  batch 364 loss: 0.23393930466129229
  batch 365 loss: 0.23387361813897956
  batch 366 loss: 0.23376354876437475
  batch 367 loss: 0.2337162997725874
  batch 368 loss: 0.23356336310667836
  batch 369 loss: 0.23350175609433554
  batch 370 loss: 0.23339968806988484
  batch 371 loss: 0.23338231661249043
  batch 372 loss: 0.2333278042334382
  batch 373 loss: 0.23320971924721715
  batch 374 loss: 0.23312253636472366
  batch 375 loss: 0.23305423529942831
  batch 376 loss: 0.23305240923419912
  batch 377 loss: 0.23303919366563347
  batch 378 loss: 0.23302017002509384
  batch 379 loss: 0.2329521588882859
  batch 380 loss: 0.23300311369331259
  batch 381 loss: 0.23298186896823522
  batch 382 loss: 0.23293807232723185
  batch 383 loss: 0.23298289908908365
  batch 384 loss: 0.2329387885207931
  batch 385 loss: 0.23301654574158903
  batch 386 loss: 0.23294711499016518
  batch 387 loss: 0.2330236411063862
  batch 388 loss: 0.2330402337475536
  batch 389 loss: 0.23302518137776146
  batch 390 loss: 0.23294584525701328
  batch 391 loss: 0.2330174200293963
  batch 392 loss: 0.23295609839260578
  batch 393 loss: 0.23292579549262846
  batch 394 loss: 0.23297027555213967
  batch 395 loss: 0.23289209347736986
  batch 396 loss: 0.232842040934948
  batch 397 loss: 0.23285197243282116
  batch 398 loss: 0.2327750316966119
  batch 399 loss: 0.2328518980129022
  batch 400 loss: 0.2329022578895092
  batch 401 loss: 0.23282796872821532
  batch 402 loss: 0.23283550917953993
  batch 403 loss: 0.23287276572388396
  batch 404 loss: 0.2329022544666682
  batch 405 loss: 0.23291734008141507
  batch 406 loss: 0.23298137469831945
  batch 407 loss: 0.23295707023114479
  batch 408 loss: 0.23302055522799492
  batch 409 loss: 0.23304721840582152
  batch 410 loss: 0.2331716157677697
  batch 411 loss: 0.23319356005702285
  batch 412 loss: 0.23313500167964732
  batch 413 loss: 0.2332506692871343
  batch 414 loss: 0.23327469714165885
  batch 415 loss: 0.2332431621939303
  batch 416 loss: 0.23335830842216426
  batch 417 loss: 0.23338782758735638
  batch 418 loss: 0.23341773265001306
  batch 419 loss: 0.23347815899689614
  batch 420 loss: 0.2334686130285263
  batch 421 loss: 0.2333875443618258
  batch 422 loss: 0.23351989806545853
  batch 423 loss: 0.23347522371204188
  batch 424 loss: 0.23338252699600076
  batch 425 loss: 0.2333155361694448
  batch 426 loss: 0.2332755456997755
  batch 427 loss: 0.233254752756561
  batch 428 loss: 0.2332088806640322
  batch 429 loss: 0.23328303193156813
  batch 430 loss: 0.2332569772074389
  batch 431 loss: 0.233332848777074
  batch 432 loss: 0.23333077974341535
  batch 433 loss: 0.23332384696733593
  batch 434 loss: 0.23336849729036954
  batch 435 loss: 0.23330080406419162
  batch 436 loss: 0.2332909355333092
  batch 437 loss: 0.23336948887434378
  batch 438 loss: 0.23345369052941398
  batch 439 loss: 0.23341663208936506
  batch 440 loss: 0.23348355364393103
  batch 441 loss: 0.23336723903004003
  batch 442 loss: 0.23327236150589464
  batch 443 loss: 0.23322821941655592
  batch 444 loss: 0.23311741287643845
  batch 445 loss: 0.23310284299796888
  batch 446 loss: 0.23304294590992777
  batch 447 loss: 0.23297580203220614
  batch 448 loss: 0.23298599554358848
  batch 449 loss: 0.2330208526621417
  batch 450 loss: 0.23299707813395396
  batch 451 loss: 0.23298650345622568
  batch 452 loss: 0.2330597716227042
  batch 453 loss: 0.23309940106295066
  batch 454 loss: 0.23310106428184174
  batch 455 loss: 0.2331471432696332
  batch 456 loss: 0.2331778715250262
  batch 457 loss: 0.23318393283912858
  batch 458 loss: 0.233179408552605
  batch 459 loss: 0.23329415511591503
  batch 460 loss: 0.2333220482196497
  batch 461 loss: 0.2332601386916353
  batch 462 loss: 0.23325471663887884
  batch 463 loss: 0.2331608605951264
  batch 464 loss: 0.23312987637673988
  batch 465 loss: 0.23297296025419748
  batch 466 loss: 0.23282297313724976
  batch 467 loss: 0.23290336955282878
  batch 468 loss: 0.23283826601174143
  batch 469 loss: 0.23290886747430384
  batch 470 loss: 0.23283314029587077
  batch 471 loss: 0.23286901268862867
  batch 472 loss: 0.23269546679142167
LOSS train 0.23269546679142167 valid 0.27465179562568665
LOSS train 0.23269546679142167 valid 0.27623821794986725
LOSS train 0.23269546679142167 valid 0.2752148111661275
LOSS train 0.23269546679142167 valid 0.269330769777298
LOSS train 0.23269546679142167 valid 0.2659446239471436
LOSS train 0.23269546679142167 valid 0.2737463215986888
LOSS train 0.23269546679142167 valid 0.2860478971685682
LOSS train 0.23269546679142167 valid 0.2840435393154621
LOSS train 0.23269546679142167 valid 0.28672248787350124
LOSS train 0.23269546679142167 valid 0.2883933186531067
LOSS train 0.23269546679142167 valid 0.28574035384438257
LOSS train 0.23269546679142167 valid 0.28734364608923596
LOSS train 0.23269546679142167 valid 0.284135653422429
LOSS train 0.23269546679142167 valid 0.2839557932955878
LOSS train 0.23269546679142167 valid 0.27755445142587026
LOSS train 0.23269546679142167 valid 0.27862571831792593
LOSS train 0.23269546679142167 valid 0.28081400166539583
LOSS train 0.23269546679142167 valid 0.2824718712104691
LOSS train 0.23269546679142167 valid 0.2841843672488865
LOSS train 0.23269546679142167 valid 0.28332082703709605
LOSS train 0.23269546679142167 valid 0.2816606341373353
LOSS train 0.23269546679142167 valid 0.27951163934035733
LOSS train 0.23269546679142167 valid 0.28071942471939587
LOSS train 0.23269546679142167 valid 0.27810112697382766
LOSS train 0.23269546679142167 valid 0.2769080847501755
LOSS train 0.23269546679142167 valid 0.27657250314950943
LOSS train 0.23269546679142167 valid 0.2768721497721142
LOSS train 0.23269546679142167 valid 0.2771500577884061
LOSS train 0.23269546679142167 valid 0.2765276724922246
LOSS train 0.23269546679142167 valid 0.27762791365385053
LOSS train 0.23269546679142167 valid 0.2795744908432807
LOSS train 0.23269546679142167 valid 0.27937348326668143
LOSS train 0.23269546679142167 valid 0.28048856104865216
LOSS train 0.23269546679142167 valid 0.2803791055784506
LOSS train 0.23269546679142167 valid 0.2820040443113872
LOSS train 0.23269546679142167 valid 0.2822003766066498
LOSS train 0.23269546679142167 valid 0.2823924965955116
LOSS train 0.23269546679142167 valid 0.28395941500601013
LOSS train 0.23269546679142167 valid 0.28387231207810915
LOSS train 0.23269546679142167 valid 0.283658654615283
LOSS train 0.23269546679142167 valid 0.28559207443783924
LOSS train 0.23269546679142167 valid 0.2863904752192043
LOSS train 0.23269546679142167 valid 0.28556255441765455
LOSS train 0.23269546679142167 valid 0.2858127975328402
LOSS train 0.23269546679142167 valid 0.28495366871356964
LOSS train 0.23269546679142167 valid 0.2858034544017004
LOSS train 0.23269546679142167 valid 0.2871378111712476
LOSS train 0.23269546679142167 valid 0.2874443142985304
LOSS train 0.23269546679142167 valid 0.28765713651569524
LOSS train 0.23269546679142167 valid 0.2861720523238182
LOSS train 0.23269546679142167 valid 0.2856869782302894
LOSS train 0.23269546679142167 valid 0.28528041994342435
LOSS train 0.23269546679142167 valid 0.28557911430889704
LOSS train 0.23269546679142167 valid 0.28541632355363283
LOSS train 0.23269546679142167 valid 0.28506503349000756
LOSS train 0.23269546679142167 valid 0.2845240067690611
LOSS train 0.23269546679142167 valid 0.28388172048225735
LOSS train 0.23269546679142167 valid 0.28305618208030175
LOSS train 0.23269546679142167 valid 0.2834367297463498
LOSS train 0.23269546679142167 valid 0.28347435196240744
LOSS train 0.23269546679142167 valid 0.2828966249696544
LOSS train 0.23269546679142167 valid 0.2838094813689109
LOSS train 0.23269546679142167 valid 0.284063764271282
LOSS train 0.23269546679142167 valid 0.2853564287070185
LOSS train 0.23269546679142167 valid 0.28594660140000855
LOSS train 0.23269546679142167 valid 0.2855926718224179
LOSS train 0.23269546679142167 valid 0.28473451622386475
LOSS train 0.23269546679142167 valid 0.2847168105928337
LOSS train 0.23269546679142167 valid 0.28390069219513214
LOSS train 0.23269546679142167 valid 0.2842202725155013
LOSS train 0.23269546679142167 valid 0.28370658381724023
LOSS train 0.23269546679142167 valid 0.2840977379431327
LOSS train 0.23269546679142167 valid 0.2838679603109621
LOSS train 0.23269546679142167 valid 0.2837280101470045
LOSS train 0.23269546679142167 valid 0.28383227566878
LOSS train 0.23269546679142167 valid 0.28412453104790886
LOSS train 0.23269546679142167 valid 0.284100907383027
LOSS train 0.23269546679142167 valid 0.284678547619245
LOSS train 0.23269546679142167 valid 0.28495551343960096
LOSS train 0.23269546679142167 valid 0.2840989727526903
LOSS train 0.23269546679142167 valid 0.28300347169976175
LOSS train 0.23269546679142167 valid 0.28360463188188834
LOSS train 0.23269546679142167 valid 0.28344268342816686
LOSS train 0.23269546679142167 valid 0.28344339859627543
LOSS train 0.23269546679142167 valid 0.2832422517678317
LOSS train 0.23269546679142167 valid 0.2826439022671345
LOSS train 0.23269546679142167 valid 0.2821490538531336
LOSS train 0.23269546679142167 valid 0.28161991319873114
LOSS train 0.23269546679142167 valid 0.2821226056372182
LOSS train 0.23269546679142167 valid 0.28258628514077927
LOSS train 0.23269546679142167 valid 0.2827045360764304
LOSS train 0.23269546679142167 valid 0.28303805835869
LOSS train 0.23269546679142167 valid 0.28298462142226516
LOSS train 0.23269546679142167 valid 0.28305757996883796
LOSS train 0.23269546679142167 valid 0.2829250953699413
LOSS train 0.23269546679142167 valid 0.2834306076789896
LOSS train 0.23269546679142167 valid 0.28341248016996484
LOSS train 0.23269546679142167 valid 0.28382631193618385
LOSS train 0.23269546679142167 valid 0.2840114955348198
LOSS train 0.23269546679142167 valid 0.28398909538984296
LOSS train 0.23269546679142167 valid 0.2841770657808474
LOSS train 0.23269546679142167 valid 0.28423361012748644
LOSS train 0.23269546679142167 valid 0.28412627739813723
LOSS train 0.23269546679142167 valid 0.2841429581435827
LOSS train 0.23269546679142167 valid 0.284115028097516
LOSS train 0.23269546679142167 valid 0.2842826840450179
LOSS train 0.23269546679142167 valid 0.2843169341577548
LOSS train 0.23269546679142167 valid 0.2843577905937477
LOSS train 0.23269546679142167 valid 0.2847171893360418
LOSS train 0.23269546679142167 valid 0.2851122880523855
LOSS train 0.23269546679142167 valid 0.2846030959406415
LOSS train 0.23269546679142167 valid 0.28440975091819254
LOSS train 0.23269546679142167 valid 0.28430047908187966
LOSS train 0.23269546679142167 valid 0.2840055814176275
LOSS train 0.23269546679142167 valid 0.28415942619676177
LOSS train 0.23269546679142167 valid 0.2839923420085989
LOSS train 0.23269546679142167 valid 0.2841189667964593
LOSS train 0.23269546679142167 valid 0.28390386283902797
LOSS train 0.23269546679142167 valid 0.28379057698390064
LOSS train 0.23269546679142167 valid 0.2834866914898157
LOSS train 0.23269546679142167 valid 0.2834413446917022
LOSS train 0.23269546679142167 valid 0.2830708846449852
LOSS train 0.23269546679142167 valid 0.2830314840970001
LOSS train 0.23269546679142167 valid 0.2835680156225158
LOSS train 0.23269546679142167 valid 0.2835064431428909
LOSS train 0.23269546679142167 valid 0.28373907707513324
LOSS train 0.23269546679142167 valid 0.2837393066779835
LOSS train 0.23269546679142167 valid 0.28424303990323097
LOSS train 0.23269546679142167 valid 0.2844179756650629
LOSS train 0.23269546679142167 valid 0.2841797119149795
LOSS train 0.23269546679142167 valid 0.28416761744568364
LOSS train 0.23269546679142167 valid 0.2838903364132751
LOSS train 0.23269546679142167 valid 0.2838702276909262
LOSS train 0.23269546679142167 valid 0.2840057496926678
LOSS train 0.23269546679142167 valid 0.28382142660794435
LOSS train 0.23269546679142167 valid 0.2836677735561834
LOSS train 0.23269546679142167 valid 0.283507899755109
LOSS train 0.23269546679142167 valid 0.28353055847295816
LOSS train 0.23269546679142167 valid 0.2835251492776459
LOSS train 0.23269546679142167 valid 0.283620121968644
LOSS train 0.23269546679142167 valid 0.28370396010841886
LOSS train 0.23269546679142167 valid 0.2840530667716349
LOSS train 0.23269546679142167 valid 0.2839961013385466
LOSS train 0.23269546679142167 valid 0.28383915467808646
LOSS train 0.23269546679142167 valid 0.2836618260062974
LOSS train 0.23269546679142167 valid 0.28386691059559993
LOSS train 0.23269546679142167 valid 0.28371985864882565
LOSS train 0.23269546679142167 valid 0.2846693526450041
LOSS train 0.23269546679142167 valid 0.2848291686117249
LOSS train 0.23269546679142167 valid 0.28520514915386835
LOSS train 0.23269546679142167 valid 0.2851753902948455
LOSS train 0.23269546679142167 valid 0.28487485186441946
LOSS train 0.23269546679142167 valid 0.28506097109878764
LOSS train 0.23269546679142167 valid 0.28485866868263715
LOSS train 0.23269546679142167 valid 0.2848424591364399
LOSS train 0.23269546679142167 valid 0.2849244413276513
LOSS train 0.23269546679142167 valid 0.2847801197296495
LOSS train 0.23269546679142167 valid 0.28484188557802875
LOSS train 0.23269546679142167 valid 0.284870334859914
LOSS train 0.23269546679142167 valid 0.28488909313455224
LOSS train 0.23269546679142167 valid 0.2846556872314548
LOSS train 0.23269546679142167 valid 0.28455039932404036
LOSS train 0.23269546679142167 valid 0.2844002895925674
LOSS train 0.23269546679142167 valid 0.2839901520893341
LOSS train 0.23269546679142167 valid 0.2836923770832293
LOSS train 0.23269546679142167 valid 0.28369857173368157
LOSS train 0.23269546679142167 valid 0.28400749557032556
LOSS train 0.23269546679142167 valid 0.28374955288711046
LOSS train 0.23269546679142167 valid 0.2839499403386426
LOSS train 0.23269546679142167 valid 0.2841042567701901
LOSS train 0.23269546679142167 valid 0.2840696114894242
LOSS train 0.23269546679142167 valid 0.28386927474030227
LOSS train 0.23269546679142167 valid 0.28388567286075195
LOSS train 0.23269546679142167 valid 0.2839491027353824
LOSS train 0.23269546679142167 valid 0.28365384221076967
LOSS train 0.23269546679142167 valid 0.28364483351734554
LOSS train 0.23269546679142167 valid 0.2837223461118795
LOSS train 0.23269546679142167 valid 0.28400253144542825
LOSS train 0.23269546679142167 valid 0.2837976436208746
LOSS train 0.23269546679142167 valid 0.283664491524299
LOSS train 0.23269546679142167 valid 0.28374218026906745
LOSS train 0.23269546679142167 valid 0.2839134130012858
LOSS train 0.23269546679142167 valid 0.2840490631881307
LOSS train 0.23269546679142167 valid 0.28411722288507485
LOSS train 0.23269546679142167 valid 0.28373880934070894
LOSS train 0.23269546679142167 valid 0.28366709684812896
LOSS train 0.23269546679142167 valid 0.28356981914948654
LOSS train 0.23269546679142167 valid 0.28365343999355397
LOSS train 0.23269546679142167 valid 0.2835476529345941
LOSS train 0.23269546679142167 valid 0.2837219465720026
LOSS train 0.23269546679142167 valid 0.28361168753414256
LOSS train 0.23269546679142167 valid 0.28362629438440007
LOSS train 0.23269546679142167 valid 0.28349434155874303
LOSS train 0.23269546679142167 valid 0.28328722623205677
LOSS train 0.23269546679142167 valid 0.283098672139339
LOSS train 0.23269546679142167 valid 0.2830220541479636
LOSS train 0.23269546679142167 valid 0.2832227704791248
LOSS train 0.23269546679142167 valid 0.2829599052366584
LOSS train 0.23269546679142167 valid 0.28296877361422207
LOSS train 0.23269546679142167 valid 0.28292340591549875
LOSS train 0.23269546679142167 valid 0.28274285778477415
LOSS train 0.23269546679142167 valid 0.2826841647672181
LOSS train 0.23269546679142167 valid 0.2824905649781814
LOSS train 0.23269546679142167 valid 0.2824432884945589
LOSS train 0.23269546679142167 valid 0.2821269228691008
LOSS train 0.23269546679142167 valid 0.28212890766778037
LOSS train 0.23269546679142167 valid 0.2820333099883536
LOSS train 0.23269546679142167 valid 0.28181813256098676
LOSS train 0.23269546679142167 valid 0.2816874547438188
LOSS train 0.23269546679142167 valid 0.28177304892312915
LOSS train 0.23269546679142167 valid 0.282032324239541
LOSS train 0.23269546679142167 valid 0.28203817569422274
LOSS train 0.23269546679142167 valid 0.28225493305166005
LOSS train 0.23269546679142167 valid 0.2821766051733605
LOSS train 0.23269546679142167 valid 0.2820390967435615
LOSS train 0.23269546679142167 valid 0.28194574183887905
LOSS train 0.23269546679142167 valid 0.28191956836507065
LOSS train 0.23269546679142167 valid 0.2820307522191914
LOSS train 0.23269546679142167 valid 0.2821338843537248
LOSS train 0.23269546679142167 valid 0.2821991339325905
LOSS train 0.23269546679142167 valid 0.2822731495982382
LOSS train 0.23269546679142167 valid 0.28209938860691347
LOSS train 0.23269546679142167 valid 0.2822747626112181
LOSS train 0.23269546679142167 valid 0.2823218225634524
LOSS train 0.23269546679142167 valid 0.2822995346122318
LOSS train 0.23269546679142167 valid 0.2823647533634068
LOSS train 0.23269546679142167 valid 0.28272605900722453
LOSS train 0.23269546679142167 valid 0.2827326014899371
LOSS train 0.23269546679142167 valid 0.2828197159100828
LOSS train 0.23269546679142167 valid 0.2828973132631053
LOSS train 0.23269546679142167 valid 0.28293094438907906
LOSS train 0.23269546679142167 valid 0.28298269392087544
LOSS train 0.23269546679142167 valid 0.2829751698500097
LOSS train 0.23269546679142167 valid 0.28297295338577694
LOSS train 0.23269546679142167 valid 0.283306958954385
LOSS train 0.23269546679142167 valid 0.28309437979833557
LOSS train 0.23269546679142167 valid 0.28321505441695827
LOSS train 0.23269546679142167 valid 0.2832847025339343
LOSS train 0.23269546679142167 valid 0.2831442819354923
LOSS train 0.23269546679142167 valid 0.28305999568353096
LOSS train 0.23269546679142167 valid 0.28305226199607136
LOSS train 0.23269546679142167 valid 0.2827852941618478
LOSS train 0.23269546679142167 valid 0.282976599325859
LOSS train 0.23269546679142167 valid 0.2833906650909635
LOSS train 0.23269546679142167 valid 0.2837069423223028
LOSS train 0.23269546679142167 valid 0.28366977333780224
LOSS train 0.23269546679142167 valid 0.2836066757739797
LOSS train 0.23269546679142167 valid 0.28354494075380987
LOSS train 0.23269546679142167 valid 0.28357380419131745
LOSS train 0.23269546679142167 valid 0.28380104929208755
LOSS train 0.23269546679142167 valid 0.2839413256165516
LOSS train 0.23269546679142167 valid 0.2841615876153348
LOSS train 0.23269546679142167 valid 0.2841872604114736
LOSS train 0.23269546679142167 valid 0.2840228502557972
LOSS train 0.23269546679142167 valid 0.28407246511356504
LOSS train 0.23269546679142167 valid 0.2841915358440019
LOSS train 0.23269546679142167 valid 0.284064362079253
LOSS train 0.23269546679142167 valid 0.28420084582049715
LOSS train 0.23269546679142167 valid 0.2842061418939281
LOSS train 0.23269546679142167 valid 0.28418914245871396
LOSS train 0.23269546679142167 valid 0.28434234931779545
LOSS train 0.23269546679142167 valid 0.2843455813882005
LOSS train 0.23269546679142167 valid 0.28442525608684627
LOSS train 0.23269546679142167 valid 0.2844206891163732
LOSS train 0.23269546679142167 valid 0.28431942625990453
LOSS train 0.23269546679142167 valid 0.2844359787895267
LOSS train 0.23269546679142167 valid 0.2844975230734
LOSS train 0.23269546679142167 valid 0.28463768675478535
LOSS train 0.23269546679142167 valid 0.28461681051546756
LOSS train 0.23269546679142167 valid 0.2845229246550136
LOSS train 0.23269546679142167 valid 0.28466592685102976
LOSS train 0.23269546679142167 valid 0.28488187037189217
LOSS train 0.23269546679142167 valid 0.2849621670695888
LOSS train 0.23269546679142167 valid 0.28490684855810916
LOSS train 0.23269546679142167 valid 0.28483333538879047
LOSS train 0.23269546679142167 valid 0.28469138759849727
LOSS train 0.23269546679142167 valid 0.2845098126128262
LOSS train 0.23269546679142167 valid 0.284365641770603
LOSS train 0.23269546679142167 valid 0.2843690267814103
LOSS train 0.23269546679142167 valid 0.28431120876755034
LOSS train 0.23269546679142167 valid 0.2841462697634918
LOSS train 0.23269546679142167 valid 0.28391623718941467
LOSS train 0.23269546679142167 valid 0.2837657651505285
LOSS train 0.23269546679142167 valid 0.2838059741426522
LOSS train 0.23269546679142167 valid 0.2839346454854597
LOSS train 0.23269546679142167 valid 0.2838783090139602
LOSS train 0.23269546679142167 valid 0.2838866690933081
LOSS train 0.23269546679142167 valid 0.2837906204577949
LOSS train 0.23269546679142167 valid 0.28386342494545513
LOSS train 0.23269546679142167 valid 0.2839218048186138
LOSS train 0.23269546679142167 valid 0.28386832451082994
LOSS train 0.23269546679142167 valid 0.2839453899084705
LOSS train 0.23269546679142167 valid 0.28391176685945163
LOSS train 0.23269546679142167 valid 0.28409125671094776
LOSS train 0.23269546679142167 valid 0.2841640223891048
LOSS train 0.23269546679142167 valid 0.28406698417824666
LOSS train 0.23269546679142167 valid 0.2841068985887649
LOSS train 0.23269546679142167 valid 0.2840793518812064
LOSS train 0.23269546679142167 valid 0.28407211217991885
LOSS train 0.23269546679142167 valid 0.28396230429410935
LOSS train 0.23269546679142167 valid 0.28399871581812636
LOSS train 0.23269546679142167 valid 0.2839994210478486
LOSS train 0.23269546679142167 valid 0.2840896812405917
LOSS train 0.23269546679142167 valid 0.28405511045926496
LOSS train 0.23269546679142167 valid 0.2839624041416606
LOSS train 0.23269546679142167 valid 0.2839541743978176
LOSS train 0.23269546679142167 valid 0.2838221891306899
LOSS train 0.23269546679142167 valid 0.28368322957645764
LOSS train 0.23269546679142167 valid 0.28366682502444124
LOSS train 0.23269546679142167 valid 0.28375119422712636
LOSS train 0.23269546679142167 valid 0.28361801079615134
LOSS train 0.23269546679142167 valid 0.2835905013176111
LOSS train 0.23269546679142167 valid 0.28355585176723835
LOSS train 0.23269546679142167 valid 0.283617990127035
LOSS train 0.23269546679142167 valid 0.2835446899845487
LOSS train 0.23269546679142167 valid 0.2834375377131414
LOSS train 0.23269546679142167 valid 0.28351478518371703
LOSS train 0.23269546679142167 valid 0.28365209333176883
LOSS train 0.23269546679142167 valid 0.28386075528437815
LOSS train 0.23269546679142167 valid 0.28380268728360536
LOSS train 0.23269546679142167 valid 0.2840493949959954
LOSS train 0.23269546679142167 valid 0.2838947563345388
LOSS train 0.23269546679142167 valid 0.283767468235441
LOSS train 0.23269546679142167 valid 0.2837573272394545
LOSS train 0.23269546679142167 valid 0.28386478286523087
LOSS train 0.23269546679142167 valid 0.2839756413288643
LOSS train 0.23269546679142167 valid 0.2840185394163161
LOSS train 0.23269546679142167 valid 0.2840744474130433
LOSS train 0.23269546679142167 valid 0.2841556707232919
LOSS train 0.23269546679142167 valid 0.28421838825399226
LOSS train 0.23269546679142167 valid 0.28411402956236526
LOSS train 0.23269546679142167 valid 0.28402965596641405
LOSS train 0.23269546679142167 valid 0.28408566564769
LOSS train 0.23269546679142167 valid 0.2842403563732159
LOSS train 0.23269546679142167 valid 0.2841793372559903
LOSS train 0.23269546679142167 valid 0.28401598383096005
LOSS train 0.23269546679142167 valid 0.2840119981005213
LOSS train 0.23269546679142167 valid 0.2839827363395832
LOSS train 0.23269546679142167 valid 0.283846110193427
LOSS train 0.23269546679142167 valid 0.2838932323981734
LOSS train 0.23269546679142167 valid 0.28371499456967775
LOSS train 0.23269546679142167 valid 0.28373803180909296
LOSS train 0.23269546679142167 valid 0.283700334000518
LOSS train 0.23269546679142167 valid 0.2838800512254238
LOSS train 0.23269546679142167 valid 0.2839375652264858
LOSS train 0.23269546679142167 valid 0.28390592000732534
LOSS train 0.23269546679142167 valid 0.2837028325205234
LOSS train 0.23269546679142167 valid 0.2836058419840089
LOSS train 0.23269546679142167 valid 0.283664119960927
LOSS train 0.23269546679142167 valid 0.2835974900211607
LOSS train 0.23269546679142167 valid 0.28355419669735465
LOSS train 0.23269546679142167 valid 0.2834534734826196
LOSS train 0.23269546679142167 valid 0.2834211880863557
LOSS train 0.23269546679142167 valid 0.28347970158030084
LOSS train 0.23269546679142167 valid 0.2836447343020372
LOSS train 0.23269546679142167 valid 0.28380358771661696
LOSS train 0.23269546679142167 valid 0.2837765185105033
LOSS train 0.23269546679142167 valid 0.28364810081167596
LOSS train 0.23269546679142167 valid 0.2836656811177565
LOSS train 0.23269546679142167 valid 0.28365359430511794
LOSS train 0.23269546679142167 valid 0.2836934442004999
LOSS train 0.23269546679142167 valid 0.2838350676041282
LOSS train 0.23269546679142167 valid 0.28364453926559324
LOSS train 0.23269546679142167 valid 0.28363000347719086
LOSS train 0.23269546679142167 valid 0.2836567515379762
LOSS train 0.23269546679142167 valid 0.2836058827860108
LOSS train 0.23269546679142167 valid 0.28344367091272443
LOSS train 0.23269546679142167 valid 0.28341694046621735
LOSS train 0.23269546679142167 valid 0.28353941553653417
EPOCH 18:
  batch 1 loss: 0.2656062841415405
  batch 2 loss: 0.243075393140316
  batch 3 loss: 0.2402158627907435
  batch 4 loss: 0.24252214655280113
  batch 5 loss: 0.24632577002048492
  batch 6 loss: 0.24181903650363287
  batch 7 loss: 0.24368028342723846
  batch 8 loss: 0.2484514582902193
  batch 9 loss: 0.24797191388077205
  batch 10 loss: 0.24466377645730972
  batch 11 loss: 0.24228611317547885
  batch 12 loss: 0.23869413261612257
  batch 13 loss: 0.23532325373246119
  batch 14 loss: 0.23632752895355225
  batch 15 loss: 0.2369245002667109
  batch 16 loss: 0.23780873324722052
  batch 17 loss: 0.2345064045751796
  batch 18 loss: 0.234036756058534
  batch 19 loss: 0.23143689412819712
  batch 20 loss: 0.2298103876411915
  batch 21 loss: 0.23033883174260458
  batch 22 loss: 0.2301079664718021
  batch 23 loss: 0.229981515718543
  batch 24 loss: 0.22807379687825838
  batch 25 loss: 0.2303600013256073
  batch 26 loss: 0.22917577509696668
  batch 27 loss: 0.23001028542165403
  batch 28 loss: 0.22936090773769788
  batch 29 loss: 0.22967228036502313
  batch 30 loss: 0.22995637357234955
  batch 31 loss: 0.23094715322217635
  batch 32 loss: 0.23111564759165049
  batch 33 loss: 0.2316825155055884
  batch 34 loss: 0.23080304966253393
  batch 35 loss: 0.23129345646926336
  batch 36 loss: 0.23207132145762444
  batch 37 loss: 0.2315247707270287
  batch 38 loss: 0.2318371887269773
  batch 39 loss: 0.23115561596858195
  batch 40 loss: 0.23125507384538652
  batch 41 loss: 0.23118148053564677
  batch 42 loss: 0.23136468018804277
  batch 43 loss: 0.2319299544012824
  batch 44 loss: 0.23149714855985207
  batch 45 loss: 0.23109152714411418
  batch 46 loss: 0.2319547009208928
  batch 47 loss: 0.23230664241821208
  batch 48 loss: 0.23156460343549648
  batch 49 loss: 0.2313653306693447
  batch 50 loss: 0.23155076652765275
  batch 51 loss: 0.23186837954848422
  batch 52 loss: 0.2321950847712847
  batch 53 loss: 0.23200879372515767
  batch 54 loss: 0.23242660446299446
  batch 55 loss: 0.23168016428297217
  batch 56 loss: 0.2314118559339217
  batch 57 loss: 0.23099614235392787
  batch 58 loss: 0.23193187703346385
  batch 59 loss: 0.2318739699105085
  batch 60 loss: 0.23211088081200917
  batch 61 loss: 0.23254159874603397
  batch 62 loss: 0.23336426433055632
  batch 63 loss: 0.2331072643162712
  batch 64 loss: 0.23406635434366763
  batch 65 loss: 0.23398151374780216
  batch 66 loss: 0.23436361077156934
  batch 67 loss: 0.23526062160285552
  batch 68 loss: 0.2355918259743382
  batch 69 loss: 0.23573038176349972
  batch 70 loss: 0.23602179906197956
  batch 71 loss: 0.2360791845220915
  batch 72 loss: 0.23648689976996845
  batch 73 loss: 0.23660819314114034
  batch 74 loss: 0.23633484602779956
  batch 75 loss: 0.23640374998251598
  batch 76 loss: 0.23706879525592453
  batch 77 loss: 0.23670019809301798
  batch 78 loss: 0.23704536908712143
  batch 79 loss: 0.23722105312951003
  batch 80 loss: 0.2368063719943166
  batch 81 loss: 0.23656117732142223
  batch 82 loss: 0.23704594301014412
  batch 83 loss: 0.23699797025646072
  batch 84 loss: 0.23716290862787337
  batch 85 loss: 0.23708226014586056
  batch 86 loss: 0.2373488112244495
  batch 87 loss: 0.2374389300058628
  batch 88 loss: 0.237299421294169
  batch 89 loss: 0.23713055336743258
  batch 90 loss: 0.2368272766470909
  batch 91 loss: 0.23674195824743627
  batch 92 loss: 0.2367917109766732
  batch 93 loss: 0.23711523237407847
  batch 94 loss: 0.23729537031117906
  batch 95 loss: 0.2369520156007064
  batch 96 loss: 0.23689203849062324
  batch 97 loss: 0.2369801611015477
  batch 98 loss: 0.23705897115323007
  batch 99 loss: 0.23730305002795327
  batch 100 loss: 0.23719394996762275
  batch 101 loss: 0.23703186331999185
  batch 102 loss: 0.23749333313282797
  batch 103 loss: 0.2377052195731876
  batch 104 loss: 0.23786979584166637
  batch 105 loss: 0.23773784183320545
  batch 106 loss: 0.2382798006512084
  batch 107 loss: 0.23830941277686682
  batch 108 loss: 0.23837379035022524
  batch 109 loss: 0.23842181197000206
  batch 110 loss: 0.23935697159983896
  batch 111 loss: 0.23984237780442108
  batch 112 loss: 0.23965321507837092
  batch 113 loss: 0.23956432912202008
  batch 114 loss: 0.2399166725706636
  batch 115 loss: 0.24001376758451048
  batch 116 loss: 0.24045122825893864
  batch 117 loss: 0.2406740448413751
  batch 118 loss: 0.24048249297222848
  batch 119 loss: 0.24059794079355834
  batch 120 loss: 0.24033234951396784
  batch 121 loss: 0.24020362503765044
  batch 122 loss: 0.24030552694543464
  batch 123 loss: 0.24013489945148064
  batch 124 loss: 0.24037082709612384
  batch 125 loss: 0.2405352191925049
  batch 126 loss: 0.24044109265955668
  batch 127 loss: 0.24068011707208287
  batch 128 loss: 0.24043788411654532
  batch 129 loss: 0.24069867369740508
  batch 130 loss: 0.2405266644862982
  batch 131 loss: 0.24053197962637166
  batch 132 loss: 0.24027819220315327
  batch 133 loss: 0.24042669465219169
  batch 134 loss: 0.24040337284999108
  batch 135 loss: 0.24003371673601645
  batch 136 loss: 0.24009962062187054
  batch 137 loss: 0.24002012556051686
  batch 138 loss: 0.23989935186894043
  batch 139 loss: 0.24002345935475056
  batch 140 loss: 0.2399028768496854
  batch 141 loss: 0.240298885297268
  batch 142 loss: 0.24026614731886017
  batch 143 loss: 0.23994046439240863
  batch 144 loss: 0.23995068369226324
  batch 145 loss: 0.2399012668379422
  batch 146 loss: 0.23975601894398257
  batch 147 loss: 0.2399132983619664
  batch 148 loss: 0.23998410456083916
  batch 149 loss: 0.23984055291086234
  batch 150 loss: 0.23964693486690522
  batch 151 loss: 0.2397458961862602
  batch 152 loss: 0.2400331763844741
  batch 153 loss: 0.23984977778266459
  batch 154 loss: 0.23990294792048342
  batch 155 loss: 0.2397767460153949
  batch 156 loss: 0.2398321853043177
  batch 157 loss: 0.24007536812572722
  batch 158 loss: 0.23994548788553552
  batch 159 loss: 0.2398636462928364
  batch 160 loss: 0.23961761901155115
  batch 161 loss: 0.2397240180961834
  batch 162 loss: 0.2396263227234652
  batch 163 loss: 0.23968163160092992
  batch 164 loss: 0.23933541529425761
  batch 165 loss: 0.23944310175650046
  batch 166 loss: 0.23933392843927245
  batch 167 loss: 0.23917392012244926
  batch 168 loss: 0.23908503637427375
  batch 169 loss: 0.2390434843372311
  batch 170 loss: 0.23887412556830576
  batch 171 loss: 0.23882367100283416
  batch 172 loss: 0.23882980116231498
  batch 173 loss: 0.23881014948979967
  batch 174 loss: 0.23886090919546699
  batch 175 loss: 0.2388734258072717
  batch 176 loss: 0.23865786728195168
  batch 177 loss: 0.2386506476813117
  batch 178 loss: 0.23884713340006516
  batch 179 loss: 0.23889143981414135
  batch 180 loss: 0.23871111497282982
  batch 181 loss: 0.23891814859861826
  batch 182 loss: 0.23900909435290557
  batch 183 loss: 0.23895811586757826
  batch 184 loss: 0.23887047973339973
  batch 185 loss: 0.2389177531003952
  batch 186 loss: 0.23904723754172685
  batch 187 loss: 0.2388999281241932
  batch 188 loss: 0.23860079327479322
  batch 189 loss: 0.23833065657388597
  batch 190 loss: 0.23825138846510335
  batch 191 loss: 0.23817602400692345
  batch 192 loss: 0.23805538712379834
  batch 193 loss: 0.23783695697784424
  batch 194 loss: 0.2380664075158306
  batch 195 loss: 0.23821265789178703
  batch 196 loss: 0.2381668185092965
  batch 197 loss: 0.2382907495280813
  batch 198 loss: 0.23866049478752444
  batch 199 loss: 0.23847627826971024
  batch 200 loss: 0.2387218850106001
  batch 201 loss: 0.23887458065552497
  batch 202 loss: 0.2390129404640434
  batch 203 loss: 0.23906464059951857
  batch 204 loss: 0.23903578892350197
  batch 205 loss: 0.23929378005062663
  batch 206 loss: 0.23939542291523183
  batch 207 loss: 0.2395944436247222
  batch 208 loss: 0.23959121459092086
  batch 209 loss: 0.2393642208365162
  batch 210 loss: 0.23945789542936144
  batch 211 loss: 0.23939766773680374
  batch 212 loss: 0.2392345128475495
  batch 213 loss: 0.23911535355126914
  batch 214 loss: 0.23920191260420273
  batch 215 loss: 0.2390774112108142
  batch 216 loss: 0.23881102167069912
  batch 217 loss: 0.23871865600759531
  batch 218 loss: 0.23862368052979127
  batch 219 loss: 0.23872492037135173
  batch 220 loss: 0.23871473067186094
  batch 221 loss: 0.23880997657506176
  batch 222 loss: 0.23885511056528436
  batch 223 loss: 0.2387949496240359
  batch 224 loss: 0.23882910815466726
  batch 225 loss: 0.23876686996883817
  batch 226 loss: 0.23886074125766754
  batch 227 loss: 0.23879871691376103
  batch 228 loss: 0.23868733271956444
  batch 229 loss: 0.23864599697975092
  batch 230 loss: 0.238698255367901
  batch 231 loss: 0.2385407748160424
  batch 232 loss: 0.23836943783379835
  batch 233 loss: 0.23828683301102963
  batch 234 loss: 0.23823535034799168
  batch 235 loss: 0.23806840726669798
  batch 236 loss: 0.23795012820322634
  batch 237 loss: 0.23790959473149184
  batch 238 loss: 0.23777215821402414
  batch 239 loss: 0.2376453814770886
  batch 240 loss: 0.23757704397042592
  batch 241 loss: 0.23766984175349667
  batch 242 loss: 0.23747940121357106
  batch 243 loss: 0.23739279990578877
  batch 244 loss: 0.23737034268799376
  batch 245 loss: 0.23731144745739138
  batch 246 loss: 0.23736493525708593
  batch 247 loss: 0.23747117624350406
  batch 248 loss: 0.2374327489204945
  batch 249 loss: 0.23731263969317976
  batch 250 loss: 0.23725415182113646
  batch 251 loss: 0.2372706883338343
  batch 252 loss: 0.23707704630399506
  batch 253 loss: 0.23701562399685147
  batch 254 loss: 0.2368506520517229
  batch 255 loss: 0.2368149731089087
  batch 256 loss: 0.23702923284145072
  batch 257 loss: 0.23709219131720205
  batch 258 loss: 0.23708091959241748
  batch 259 loss: 0.2371032296115367
  batch 260 loss: 0.23705079928040504
  batch 261 loss: 0.2370356373289079
  batch 262 loss: 0.23698735350870903
  batch 263 loss: 0.23706154055015216
  batch 264 loss: 0.23699202840075348
  batch 265 loss: 0.23680076497905658
  batch 266 loss: 0.23677463487799005
  batch 267 loss: 0.2368694997570488
  batch 268 loss: 0.23674058130205566
  batch 269 loss: 0.23669656130682581
  batch 270 loss: 0.23687733560800553
  batch 271 loss: 0.23682796938612893
  batch 272 loss: 0.2367416426868123
  batch 273 loss: 0.23661249422983371
  batch 274 loss: 0.2366637605493956
  batch 275 loss: 0.23657294300469486
  batch 276 loss: 0.23643134334597035
  batch 277 loss: 0.2363477554239521
  batch 278 loss: 0.2361842389051005
  batch 279 loss: 0.23622524861152883
  batch 280 loss: 0.2360949497137751
  batch 281 loss: 0.23598505742185055
  batch 282 loss: 0.23599777502793792
  batch 283 loss: 0.23588957240008634
  batch 284 loss: 0.23590891147163554
  batch 285 loss: 0.23584525067555276
  batch 286 loss: 0.2358305769471022
  batch 287 loss: 0.23586059826384023
  batch 288 loss: 0.2357463726463417
  batch 289 loss: 0.23582548423828137
  batch 290 loss: 0.23565650212353673
  batch 291 loss: 0.23565047610666334
  batch 292 loss: 0.23575310674432207
  batch 293 loss: 0.23571733693010571
  batch 294 loss: 0.23566451131486568
  batch 295 loss: 0.23565875079672216
  batch 296 loss: 0.23570681242523966
  batch 297 loss: 0.2356223499132728
  batch 298 loss: 0.23556982080808422
  batch 299 loss: 0.23550324615427484
  batch 300 loss: 0.23548299173514048
  batch 301 loss: 0.23550049708135104
  batch 302 loss: 0.2354010460984628
  batch 303 loss: 0.2353027879995088
  batch 304 loss: 0.23523278416771637
  batch 305 loss: 0.23506782616748184
  batch 306 loss: 0.2351591303165442
  batch 307 loss: 0.23515784221091565
  batch 308 loss: 0.23524561006720965
  batch 309 loss: 0.23519954360225825
  batch 310 loss: 0.23512461560387765
  batch 311 loss: 0.2351981054548282
  batch 312 loss: 0.2353970126654857
  batch 313 loss: 0.2354712311071329
  batch 314 loss: 0.23538770873075837
  batch 315 loss: 0.2353366115736583
  batch 316 loss: 0.23531967990949185
  batch 317 loss: 0.23526301176202033
  batch 318 loss: 0.235234593960849
  batch 319 loss: 0.23515995804418965
  batch 320 loss: 0.2351034359075129
  batch 321 loss: 0.23516417914461868
  batch 322 loss: 0.23506598035741297
  batch 323 loss: 0.2349504347279345
  batch 324 loss: 0.23479291948455353
  batch 325 loss: 0.2348578887260877
  batch 326 loss: 0.2348474078108928
  batch 327 loss: 0.23482007945714012
  batch 328 loss: 0.23462603436555804
  batch 329 loss: 0.23466226444961816
  batch 330 loss: 0.23455665043809198
  batch 331 loss: 0.23450534396841446
  batch 332 loss: 0.2343508510524968
  batch 333 loss: 0.23447273872994087
  batch 334 loss: 0.2343492613670355
  batch 335 loss: 0.2343195062519899
  batch 336 loss: 0.2342241026372427
  batch 337 loss: 0.2342044849070314
  batch 338 loss: 0.23429597579163208
  batch 339 loss: 0.2342518669138264
  batch 340 loss: 0.23417913576259333
  batch 341 loss: 0.23419799347602027
  batch 342 loss: 0.23412955778906916
  batch 343 loss: 0.23409958361884248
  batch 344 loss: 0.23408714395969413
  batch 345 loss: 0.23419112416281215
  batch 346 loss: 0.2340662313162247
  batch 347 loss: 0.23408293595231575
  batch 348 loss: 0.2340501394422575
  batch 349 loss: 0.23394204710308666
  batch 350 loss: 0.23396382510662078
  batch 351 loss: 0.23396223834437183
  batch 352 loss: 0.23403209981254555
  batch 353 loss: 0.23402001668643682
  batch 354 loss: 0.23415891523078336
  batch 355 loss: 0.23407880680661805
  batch 356 loss: 0.23407639052425877
  batch 357 loss: 0.23401109744854667
  batch 358 loss: 0.23404324391867196
  batch 359 loss: 0.2339720390633288
  batch 360 loss: 0.23391254337297546
  batch 361 loss: 0.23388782820543094
  batch 362 loss: 0.23374125847171023
  batch 363 loss: 0.2335950443238923
  batch 364 loss: 0.23340969422197605
  batch 365 loss: 0.23336487906436398
  batch 366 loss: 0.23325840815331766
  batch 367 loss: 0.23322355186906757
  batch 368 loss: 0.2330847368778094
  batch 369 loss: 0.2329968702663897
  batch 370 loss: 0.23287376044569788
  batch 371 loss: 0.23283974187714712
  batch 372 loss: 0.23279061048261582
  batch 373 loss: 0.23268230327332626
  batch 374 loss: 0.23253260744128
  batch 375 loss: 0.23242329676946005
  batch 376 loss: 0.23241058669667294
  batch 377 loss: 0.2324046520639794
  batch 378 loss: 0.23237799450991645
  batch 379 loss: 0.23230034266266786
  batch 380 loss: 0.23233094744776425
  batch 381 loss: 0.23228943418330095
  batch 382 loss: 0.23223072021263433
  batch 383 loss: 0.23227552384838424
  batch 384 loss: 0.23220529058016837
  batch 385 loss: 0.23227654122687005
  batch 386 loss: 0.23223492402795684
  batch 387 loss: 0.2322740068885399
  batch 388 loss: 0.23225251508435024
  batch 389 loss: 0.23216263099470605
  batch 390 loss: 0.2321056821407416
  batch 391 loss: 0.23217678405439762
  batch 392 loss: 0.23210967871911672
  batch 393 loss: 0.2320187020544484
  batch 394 loss: 0.23206575797293996
  batch 395 loss: 0.2319807726748382
  batch 396 loss: 0.23194691516233212
  batch 397 loss: 0.23193947520454225
  batch 398 loss: 0.2318557198323197
  batch 399 loss: 0.23193813147102682
  batch 400 loss: 0.23200337439775467
  batch 401 loss: 0.23190396132314592
  batch 402 loss: 0.23191978600784321
  batch 403 loss: 0.23195012509083335
  batch 404 loss: 0.23196570191643026
  batch 405 loss: 0.23200219689327994
  batch 406 loss: 0.23205330978913846
  batch 407 loss: 0.23204234608561167
  batch 408 loss: 0.2320979410672889
  batch 409 loss: 0.23213103930932677
  batch 410 loss: 0.23225671256460795
  batch 411 loss: 0.23226515876260698
  batch 412 loss: 0.23224618307595113
  batch 413 loss: 0.2323514679153664
  batch 414 loss: 0.2323842690402759
  batch 415 loss: 0.23234721045178103
  batch 416 loss: 0.2324396542703303
  batch 417 loss: 0.23248742320697657
  batch 418 loss: 0.23252792510261946
  batch 419 loss: 0.23258065853739127
  batch 420 loss: 0.2325593731942631
  batch 421 loss: 0.23249515508812565
  batch 422 loss: 0.23266433984464943
  batch 423 loss: 0.23263434963587046
  batch 424 loss: 0.2325181282072697
  batch 425 loss: 0.23244486128582673
  batch 426 loss: 0.23238044316080256
  batch 427 loss: 0.23235163190325753
  batch 428 loss: 0.2323044041328341
  batch 429 loss: 0.23238199504661117
  batch 430 loss: 0.23239054218974226
  batch 431 loss: 0.23246227280861265
  batch 432 loss: 0.23244329710939415
  batch 433 loss: 0.23244321717951644
  batch 434 loss: 0.23251582620330669
  batch 435 loss: 0.23244268469426824
  batch 436 loss: 0.23243608445339248
  batch 437 loss: 0.2324971020971883
  batch 438 loss: 0.23256574343192524
  batch 439 loss: 0.23252289771083276
  batch 440 loss: 0.23257081512023103
  batch 441 loss: 0.23247205106174054
  batch 442 loss: 0.23237165896331563
  batch 443 loss: 0.23236243786714955
  batch 444 loss: 0.23221596907656472
  batch 445 loss: 0.23217376470565795
  batch 446 loss: 0.23210401492268515
  batch 447 loss: 0.23201314118217836
  batch 448 loss: 0.23205916433861212
  batch 449 loss: 0.23210422409271078
  batch 450 loss: 0.2320826976497968
  batch 451 loss: 0.232077289182701
  batch 452 loss: 0.23212442652577847
  batch 453 loss: 0.2321866142828733
  batch 454 loss: 0.23222356837751582
  batch 455 loss: 0.23224455697850865
  batch 456 loss: 0.23229057501936168
  batch 457 loss: 0.23230398408824035
  batch 458 loss: 0.2323147902777622
  batch 459 loss: 0.2324060459736905
  batch 460 loss: 0.23245108169705972
  batch 461 loss: 0.23238580866258213
  batch 462 loss: 0.2324020123972005
  batch 463 loss: 0.23230226202737178
  batch 464 loss: 0.23228169486312003
  batch 465 loss: 0.23212446195463982
  batch 466 loss: 0.23200611108234512
  batch 467 loss: 0.23209345816161955
  batch 468 loss: 0.2320633175599779
  batch 469 loss: 0.23213831006463911
  batch 470 loss: 0.23210552027250858
  batch 471 loss: 0.23211835419676105
  batch 472 loss: 0.23198023747842192
LOSS train 0.23198023747842192 valid 0.33820128440856934
LOSS train 0.23198023747842192 valid 0.3407577872276306
LOSS train 0.23198023747842192 valid 0.335190753142039
LOSS train 0.23198023747842192 valid 0.33211778849363327
LOSS train 0.23198023747842192 valid 0.32946821451187136
LOSS train 0.23198023747842192 valid 0.33724501729011536
LOSS train 0.23198023747842192 valid 0.34921061992645264
LOSS train 0.23198023747842192 valid 0.34759991616010666
LOSS train 0.23198023747842192 valid 0.3499108586046431
LOSS train 0.23198023747842192 valid 0.35082263946533204
LOSS train 0.23198023747842192 valid 0.348740737546574
LOSS train 0.23198023747842192 valid 0.34933268278837204
LOSS train 0.23198023747842192 valid 0.346678960781831
LOSS train 0.23198023747842192 valid 0.3459836521318981
LOSS train 0.23198023747842192 valid 0.3391365687052409
LOSS train 0.23198023747842192 valid 0.3402519319206476
LOSS train 0.23198023747842192 valid 0.34264077684458566
LOSS train 0.23198023747842192 valid 0.34436650408638847
LOSS train 0.23198023747842192 valid 0.34593518627317327
LOSS train 0.23198023747842192 valid 0.3451068431138992
LOSS train 0.23198023747842192 valid 0.3429905048438481
LOSS train 0.23198023747842192 valid 0.34018332443454047
LOSS train 0.23198023747842192 valid 0.3416189758673958
LOSS train 0.23198023747842192 valid 0.339411170532306
LOSS train 0.23198023747842192 valid 0.3382663607597351
LOSS train 0.23198023747842192 valid 0.3376421859631172
LOSS train 0.23198023747842192 valid 0.3380853942147008
LOSS train 0.23198023747842192 valid 0.33827406061547144
LOSS train 0.23198023747842192 valid 0.33784203282718
LOSS train 0.23198023747842192 valid 0.33947073221206664
LOSS train 0.23198023747842192 valid 0.34167712157772434
LOSS train 0.23198023747842192 valid 0.34150370862334967
LOSS train 0.23198023747842192 valid 0.3429361672112436
LOSS train 0.23198023747842192 valid 0.3431669219451792
LOSS train 0.23198023747842192 valid 0.34540813820702687
LOSS train 0.23198023747842192 valid 0.3461286226908366
LOSS train 0.23198023747842192 valid 0.346268691726633
LOSS train 0.23198023747842192 valid 0.3479021536676507
LOSS train 0.23198023747842192 valid 0.3474765786757836
LOSS train 0.23198023747842192 valid 0.347327970713377
LOSS train 0.23198023747842192 valid 0.3492358188803603
LOSS train 0.23198023747842192 valid 0.35024064779281616
LOSS train 0.23198023747842192 valid 0.3491137783194697
LOSS train 0.23198023747842192 valid 0.3492043119939891
LOSS train 0.23198023747842192 valid 0.34833987222777474
LOSS train 0.23198023747842192 valid 0.34965473737405695
LOSS train 0.23198023747842192 valid 0.35112158985848124
LOSS train 0.23198023747842192 valid 0.35135980943838757
LOSS train 0.23198023747842192 valid 0.35132718207884805
LOSS train 0.23198023747842192 valid 0.3498494005203247
LOSS train 0.23198023747842192 valid 0.34966976151746865
LOSS train 0.23198023747842192 valid 0.34921584679530215
LOSS train 0.23198023747842192 valid 0.3494985390384242
LOSS train 0.23198023747842192 valid 0.34942245648966896
LOSS train 0.23198023747842192 valid 0.3488935329697349
LOSS train 0.23198023747842192 valid 0.3480796015688351
LOSS train 0.23198023747842192 valid 0.3475385010242462
LOSS train 0.23198023747842192 valid 0.3466121081648202
LOSS train 0.23198023747842192 valid 0.34709410293627596
LOSS train 0.23198023747842192 valid 0.3472809508442879
LOSS train 0.23198023747842192 valid 0.34671520014278223
LOSS train 0.23198023747842192 valid 0.3476432869511266
LOSS train 0.23198023747842192 valid 0.34792613841238473
LOSS train 0.23198023747842192 valid 0.3495191065594554
LOSS train 0.23198023747842192 valid 0.3499955342366145
LOSS train 0.23198023747842192 valid 0.3498180874369361
LOSS train 0.23198023747842192 valid 0.3488324008770843
LOSS train 0.23198023747842192 valid 0.3488780207493726
LOSS train 0.23198023747842192 valid 0.34816376929697784
LOSS train 0.23198023747842192 valid 0.34839951736586433
LOSS train 0.23198023747842192 valid 0.3479065236071466
LOSS train 0.23198023747842192 valid 0.34840798584951294
LOSS train 0.23198023747842192 valid 0.34827657145996616
LOSS train 0.23198023747842192 valid 0.3480692183649218
LOSS train 0.23198023747842192 valid 0.3481673220793406
LOSS train 0.23198023747842192 valid 0.34870760495725434
LOSS train 0.23198023747842192 valid 0.3488853864081494
LOSS train 0.23198023747842192 valid 0.3494418951181265
LOSS train 0.23198023747842192 valid 0.3498044391221638
LOSS train 0.23198023747842192 valid 0.3488913860172033
LOSS train 0.23198023747842192 valid 0.3478056177680875
LOSS train 0.23198023747842192 valid 0.3483639072354247
LOSS train 0.23198023747842192 valid 0.34813468894326544
LOSS train 0.23198023747842192 valid 0.3480628959479786
LOSS train 0.23198023747842192 valid 0.3477916380938362
LOSS train 0.23198023747842192 valid 0.34700876612995946
LOSS train 0.23198023747842192 valid 0.34659335359759713
LOSS train 0.23198023747842192 valid 0.34603774920105934
LOSS train 0.23198023747842192 valid 0.3465890639953399
LOSS train 0.23198023747842192 valid 0.3470047858026293
LOSS train 0.23198023747842192 valid 0.34696863902794134
LOSS train 0.23198023747842192 valid 0.34722030292386596
LOSS train 0.23198023747842192 valid 0.3471947229036721
LOSS train 0.23198023747842192 valid 0.34722804040350813
LOSS train 0.23198023747842192 valid 0.34707914120272587
LOSS train 0.23198023747842192 valid 0.3475867335995038
LOSS train 0.23198023747842192 valid 0.34762030102542996
LOSS train 0.23198023747842192 valid 0.3480513536808442
LOSS train 0.23198023747842192 valid 0.3482453937482352
LOSS train 0.23198023747842192 valid 0.34838089168071745
LOSS train 0.23198023747842192 valid 0.3484809888471471
LOSS train 0.23198023747842192 valid 0.3486352428501728
LOSS train 0.23198023747842192 valid 0.3486700694537857
LOSS train 0.23198023747842192 valid 0.3485541739142858
LOSS train 0.23198023747842192 valid 0.3485667841775077
LOSS train 0.23198023747842192 valid 0.34875436773839985
LOSS train 0.23198023747842192 valid 0.34871203125080213
LOSS train 0.23198023747842192 valid 0.34878568903163626
LOSS train 0.23198023747842192 valid 0.34919054819903245
LOSS train 0.23198023747842192 valid 0.34957160732962866
LOSS train 0.23198023747842192 valid 0.34890321783117345
LOSS train 0.23198023747842192 valid 0.34879588468798567
LOSS train 0.23198023747842192 valid 0.34866713290720913
LOSS train 0.23198023747842192 valid 0.34834628539127216
LOSS train 0.23198023747842192 valid 0.3486451926438705
LOSS train 0.23198023747842192 valid 0.3484287202872079
LOSS train 0.23198023747842192 valid 0.34852359182814246
LOSS train 0.23198023747842192 valid 0.3482979255712638
LOSS train 0.23198023747842192 valid 0.3481159380504063
LOSS train 0.23198023747842192 valid 0.347742123901844
LOSS train 0.23198023747842192 valid 0.347725474637402
LOSS train 0.23198023747842192 valid 0.3474025423409509
LOSS train 0.23198023747842192 valid 0.3474301623135078
LOSS train 0.23198023747842192 valid 0.3480691962665127
LOSS train 0.23198023747842192 valid 0.34798258590698244
LOSS train 0.23198023747842192 valid 0.34809835870114586
LOSS train 0.23198023747842192 valid 0.34815744034887297
LOSS train 0.23198023747842192 valid 0.3487673792988062
LOSS train 0.23198023747842192 valid 0.34908303549123365
LOSS train 0.23198023747842192 valid 0.3488074758878121
LOSS train 0.23198023747842192 valid 0.34875164982926754
LOSS train 0.23198023747842192 valid 0.3483457924290137
LOSS train 0.23198023747842192 valid 0.34820988908746187
LOSS train 0.23198023747842192 valid 0.34844136082414373
LOSS train 0.23198023747842192 valid 0.34829088096265437
LOSS train 0.23198023747842192 valid 0.34810203664443073
LOSS train 0.23198023747842192 valid 0.3479996358391142
LOSS train 0.23198023747842192 valid 0.3479562593974929
LOSS train 0.23198023747842192 valid 0.34805297444192623
LOSS train 0.23198023747842192 valid 0.34824227797133583
LOSS train 0.23198023747842192 valid 0.34837055121753235
LOSS train 0.23198023747842192 valid 0.34882537747772646
LOSS train 0.23198023747842192 valid 0.3486614940049765
LOSS train 0.23198023747842192 valid 0.3485116954478953
LOSS train 0.23198023747842192 valid 0.34829080618661024
LOSS train 0.23198023747842192 valid 0.3484783105246008
LOSS train 0.23198023747842192 valid 0.34828616263104134
LOSS train 0.23198023747842192 valid 0.34930365492363236
LOSS train 0.23198023747842192 valid 0.34951393576276385
LOSS train 0.23198023747842192 valid 0.3497348177433014
LOSS train 0.23198023747842192 valid 0.34963460355404985
LOSS train 0.23198023747842192 valid 0.3492889574875957
LOSS train 0.23198023747842192 valid 0.34941135259235606
LOSS train 0.23198023747842192 valid 0.3492827920557617
LOSS train 0.23198023747842192 valid 0.3492429167993607
LOSS train 0.23198023747842192 valid 0.3493484273934976
LOSS train 0.23198023747842192 valid 0.3492796935473278
LOSS train 0.23198023747842192 valid 0.3493648539238338
LOSS train 0.23198023747842192 valid 0.3494994057424413
LOSS train 0.23198023747842192 valid 0.3495201973244548
LOSS train 0.23198023747842192 valid 0.3494528008173712
LOSS train 0.23198023747842192 valid 0.34931366035231837
LOSS train 0.23198023747842192 valid 0.3491220009838877
LOSS train 0.23198023747842192 valid 0.34869229702687843
LOSS train 0.23198023747842192 valid 0.34841967965617326
LOSS train 0.23198023747842192 valid 0.34847120420042293
LOSS train 0.23198023747842192 valid 0.3488939654327438
LOSS train 0.23198023747842192 valid 0.34857549607044175
LOSS train 0.23198023747842192 valid 0.3488297280827923
LOSS train 0.23198023747842192 valid 0.3490352413233589
LOSS train 0.23198023747842192 valid 0.3489973545074463
LOSS train 0.23198023747842192 valid 0.34877444630445437
LOSS train 0.23198023747842192 valid 0.3488474581282952
LOSS train 0.23198023747842192 valid 0.3489404221375783
LOSS train 0.23198023747842192 valid 0.34863342114857265
LOSS train 0.23198023747842192 valid 0.34853180836547504
LOSS train 0.23198023747842192 valid 0.34864478714048525
LOSS train 0.23198023747842192 valid 0.34893040091134186
LOSS train 0.23198023747842192 valid 0.34873495937725685
LOSS train 0.23198023747842192 valid 0.3486227088504367
LOSS train 0.23198023747842192 valid 0.3486950185088163
LOSS train 0.23198023747842192 valid 0.3489057167873278
LOSS train 0.23198023747842192 valid 0.34903278295459644
LOSS train 0.23198023747842192 valid 0.34909200684531877
LOSS train 0.23198023747842192 valid 0.34863957827155656
LOSS train 0.23198023747842192 valid 0.3485537645637348
LOSS train 0.23198023747842192 valid 0.3484351888378674
LOSS train 0.23198023747842192 valid 0.3484444312275724
LOSS train 0.23198023747842192 valid 0.34828773030528315
LOSS train 0.23198023747842192 valid 0.3484449780301044
LOSS train 0.23198023747842192 valid 0.3483486866763749
LOSS train 0.23198023747842192 valid 0.3483317921248575
LOSS train 0.23198023747842192 valid 0.3481780642672524
LOSS train 0.23198023747842192 valid 0.3479210580747152
LOSS train 0.23198023747842192 valid 0.34762940009435017
LOSS train 0.23198023747842192 valid 0.3475911843837524
LOSS train 0.23198023747842192 valid 0.34789055812782443
LOSS train 0.23198023747842192 valid 0.347631319454222
LOSS train 0.23198023747842192 valid 0.34763011066757854
LOSS train 0.23198023747842192 valid 0.34752735391259193
LOSS train 0.23198023747842192 valid 0.3473633913258415
LOSS train 0.23198023747842192 valid 0.34731634359548585
LOSS train 0.23198023747842192 valid 0.34711355616893674
LOSS train 0.23198023747842192 valid 0.3470687046647072
LOSS train 0.23198023747842192 valid 0.34673035159343624
LOSS train 0.23198023747842192 valid 0.3467763464138346
LOSS train 0.23198023747842192 valid 0.34673834077402016
LOSS train 0.23198023747842192 valid 0.3464768013128868
LOSS train 0.23198023747842192 valid 0.34641934567661375
LOSS train 0.23198023747842192 valid 0.3465493823800768
LOSS train 0.23198023747842192 valid 0.3467577279179017
LOSS train 0.23198023747842192 valid 0.34677305100661404
LOSS train 0.23198023747842192 valid 0.34705776479882255
LOSS train 0.23198023747842192 valid 0.34705820289727685
LOSS train 0.23198023747842192 valid 0.34684671496236047
LOSS train 0.23198023747842192 valid 0.3467669234507614
LOSS train 0.23198023747842192 valid 0.3467717698092834
LOSS train 0.23198023747842192 valid 0.34682396026926304
LOSS train 0.23198023747842192 valid 0.3469448591748329
LOSS train 0.23198023747842192 valid 0.34702833565798674
LOSS train 0.23198023747842192 valid 0.3470626999350155
LOSS train 0.23198023747842192 valid 0.3469712869809555
LOSS train 0.23198023747842192 valid 0.34721524271729814
LOSS train 0.23198023747842192 valid 0.34726935092891964
LOSS train 0.23198023747842192 valid 0.34720730119281346
LOSS train 0.23198023747842192 valid 0.347262052176273
LOSS train 0.23198023747842192 valid 0.3476393965897581
LOSS train 0.23198023747842192 valid 0.34761970542501985
LOSS train 0.23198023747842192 valid 0.3477276356719987
LOSS train 0.23198023747842192 valid 0.34780768013518787
LOSS train 0.23198023747842192 valid 0.3478537622468296
LOSS train 0.23198023747842192 valid 0.34792818206137627
LOSS train 0.23198023747842192 valid 0.34790946792635286
LOSS train 0.23198023747842192 valid 0.3478957838737048
LOSS train 0.23198023747842192 valid 0.3482341448043255
LOSS train 0.23198023747842192 valid 0.3480238276770559
LOSS train 0.23198023747842192 valid 0.34813952521432806
LOSS train 0.23198023747842192 valid 0.3481990190363732
LOSS train 0.23198023747842192 valid 0.3480463340940835
LOSS train 0.23198023747842192 valid 0.3480372636268536
LOSS train 0.23198023747842192 valid 0.3480882581586165
LOSS train 0.23198023747842192 valid 0.34777608821707323
LOSS train 0.23198023747842192 valid 0.34801000368938523
LOSS train 0.23198023747842192 valid 0.348407793240469
LOSS train 0.23198023747842192 valid 0.34876723617923505
LOSS train 0.23198023747842192 valid 0.3487318941975028
LOSS train 0.23198023747842192 valid 0.3487403110453957
LOSS train 0.23198023747842192 valid 0.34863114477165286
LOSS train 0.23198023747842192 valid 0.3485736886420882
LOSS train 0.23198023747842192 valid 0.34881898736953737
LOSS train 0.23198023747842192 valid 0.3488991559739132
LOSS train 0.23198023747842192 valid 0.34917381074693465
LOSS train 0.23198023747842192 valid 0.34921072688498517
LOSS train 0.23198023747842192 valid 0.34901543923719663
LOSS train 0.23198023747842192 valid 0.34902334505436466
LOSS train 0.23198023747842192 valid 0.3490889467066154
LOSS train 0.23198023747842192 valid 0.34891375910911115
LOSS train 0.23198023747842192 valid 0.3491035143072291
LOSS train 0.23198023747842192 valid 0.3490988785925979
LOSS train 0.23198023747842192 valid 0.34902143249144923
LOSS train 0.23198023747842192 valid 0.34916765788049076
LOSS train 0.23198023747842192 valid 0.34919855942707934
LOSS train 0.23198023747842192 valid 0.34930177605197454
LOSS train 0.23198023747842192 valid 0.3493036479886734
LOSS train 0.23198023747842192 valid 0.34915718143841007
LOSS train 0.23198023747842192 valid 0.34931042980878874
LOSS train 0.23198023747842192 valid 0.34932636344031004
LOSS train 0.23198023747842192 valid 0.34947467884465827
LOSS train 0.23198023747842192 valid 0.34944812538012254
LOSS train 0.23198023747842192 valid 0.34933581153551735
LOSS train 0.23198023747842192 valid 0.3495292615186684
LOSS train 0.23198023747842192 valid 0.34985065241070357
LOSS train 0.23198023747842192 valid 0.3499666786455846
LOSS train 0.23198023747842192 valid 0.3499312896798127
LOSS train 0.23198023747842192 valid 0.34986013174057007
LOSS train 0.23198023747842192 valid 0.3497262950176778
LOSS train 0.23198023747842192 valid 0.3495236787985378
LOSS train 0.23198023747842192 valid 0.34935546382296856
LOSS train 0.23198023747842192 valid 0.3494157696069355
LOSS train 0.23198023747842192 valid 0.34935064635106494
LOSS train 0.23198023747842192 valid 0.34918292041775173
LOSS train 0.23198023747842192 valid 0.3489658348526515
LOSS train 0.23198023747842192 valid 0.34887259514087504
LOSS train 0.23198023747842192 valid 0.34888838871683875
LOSS train 0.23198023747842192 valid 0.34906385969697384
LOSS train 0.23198023747842192 valid 0.34904222523832656
LOSS train 0.23198023747842192 valid 0.34903010829815884
LOSS train 0.23198023747842192 valid 0.3489177685437931
LOSS train 0.23198023747842192 valid 0.3489728007968322
LOSS train 0.23198023747842192 valid 0.34902056630315453
LOSS train 0.23198023747842192 valid 0.348915993552847
LOSS train 0.23198023747842192 valid 0.34901040761846386
LOSS train 0.23198023747842192 valid 0.3489813031596942
LOSS train 0.23198023747842192 valid 0.3491633185724012
LOSS train 0.23198023747842192 valid 0.34923743252026834
LOSS train 0.23198023747842192 valid 0.34910595507637876
LOSS train 0.23198023747842192 valid 0.3491102390819126
LOSS train 0.23198023747842192 valid 0.3490807157034842
LOSS train 0.23198023747842192 valid 0.34903958340153646
LOSS train 0.23198023747842192 valid 0.3488846058646838
LOSS train 0.23198023747842192 valid 0.3489232004876945
LOSS train 0.23198023747842192 valid 0.3488644068406907
LOSS train 0.23198023747842192 valid 0.34903552587276243
LOSS train 0.23198023747842192 valid 0.3489565153263117
LOSS train 0.23198023747842192 valid 0.3488880778922409
LOSS train 0.23198023747842192 valid 0.34886101686876586
LOSS train 0.23198023747842192 valid 0.3486948890872421
LOSS train 0.23198023747842192 valid 0.3485727198905759
LOSS train 0.23198023747842192 valid 0.34857916484758694
LOSS train 0.23198023747842192 valid 0.348723050086729
LOSS train 0.23198023747842192 valid 0.34855120070877565
LOSS train 0.23198023747842192 valid 0.34850538865878034
LOSS train 0.23198023747842192 valid 0.3484814806868093
LOSS train 0.23198023747842192 valid 0.34857296326737497
LOSS train 0.23198023747842192 valid 0.34850561931019736
LOSS train 0.23198023747842192 valid 0.3483865335583687
LOSS train 0.23198023747842192 valid 0.3484809959912526
LOSS train 0.23198023747842192 valid 0.34862606124308126
LOSS train 0.23198023747842192 valid 0.348843045369211
LOSS train 0.23198023747842192 valid 0.3487862108275294
LOSS train 0.23198023747842192 valid 0.34904916375596945
LOSS train 0.23198023747842192 valid 0.3488834641920113
LOSS train 0.23198023747842192 valid 0.34873239691412483
LOSS train 0.23198023747842192 valid 0.34874661331191475
LOSS train 0.23198023747842192 valid 0.34880133060308605
LOSS train 0.23198023747842192 valid 0.3488884293959916
LOSS train 0.23198023747842192 valid 0.3489211832346902
LOSS train 0.23198023747842192 valid 0.3489717922014434
LOSS train 0.23198023747842192 valid 0.34905510624491337
LOSS train 0.23198023747842192 valid 0.3490943450819362
LOSS train 0.23198023747842192 valid 0.34899125160407446
LOSS train 0.23198023747842192 valid 0.34894502872085
LOSS train 0.23198023747842192 valid 0.3489580323567262
LOSS train 0.23198023747842192 valid 0.34912194222390297
LOSS train 0.23198023747842192 valid 0.34905692392320775
LOSS train 0.23198023747842192 valid 0.3488726267325027
LOSS train 0.23198023747842192 valid 0.34888787289047807
LOSS train 0.23198023747842192 valid 0.348823972002289
LOSS train 0.23198023747842192 valid 0.3486930451913569
LOSS train 0.23198023747842192 valid 0.3486683637780302
LOSS train 0.23198023747842192 valid 0.34848924815829546
LOSS train 0.23198023747842192 valid 0.34848585454692615
LOSS train 0.23198023747842192 valid 0.3484384950152639
LOSS train 0.23198023747842192 valid 0.3486684974023076
LOSS train 0.23198023747842192 valid 0.34872449619182644
LOSS train 0.23198023747842192 valid 0.3486528495660407
LOSS train 0.23198023747842192 valid 0.34844129152531583
LOSS train 0.23198023747842192 valid 0.3483865292764258
LOSS train 0.23198023747842192 valid 0.3484448542055223
LOSS train 0.23198023747842192 valid 0.3483273204735347
LOSS train 0.23198023747842192 valid 0.34830095947977485
LOSS train 0.23198023747842192 valid 0.348203818991103
LOSS train 0.23198023747842192 valid 0.3481076744730682
LOSS train 0.23198023747842192 valid 0.34816122156078533
LOSS train 0.23198023747842192 valid 0.3483399028509436
LOSS train 0.23198023747842192 valid 0.3485105216000857
LOSS train 0.23198023747842192 valid 0.3484912860293348
LOSS train 0.23198023747842192 valid 0.3483695464427245
LOSS train 0.23198023747842192 valid 0.34840372478729503
LOSS train 0.23198023747842192 valid 0.3483731793032752
LOSS train 0.23198023747842192 valid 0.3484708438949902
LOSS train 0.23198023747842192 valid 0.34858889935424975
LOSS train 0.23198023747842192 valid 0.34838411154497423
LOSS train 0.23198023747842192 valid 0.34835218875617774
LOSS train 0.23198023747842192 valid 0.3483917326143343
LOSS train 0.23198023747842192 valid 0.3483092944478728
LOSS train 0.23198023747842192 valid 0.34816244447588596
LOSS train 0.23198023747842192 valid 0.34809723812276905
LOSS train 0.23198023747842192 valid 0.3482569330106906
EPOCH 19:
  batch 1 loss: 0.2797408103942871
  batch 2 loss: 0.25598712265491486
  batch 3 loss: 0.252723827958107
  batch 4 loss: 0.2529772184789181
  batch 5 loss: 0.25661895573139193
  batch 6 loss: 0.2518160914381345
  batch 7 loss: 0.25210694542952944
  batch 8 loss: 0.2562876548618078
  batch 9 loss: 0.2552879783842299
  batch 10 loss: 0.2519616886973381
  batch 11 loss: 0.2507750229402022
  batch 12 loss: 0.24787943810224533
  batch 13 loss: 0.24348024221567008
  batch 14 loss: 0.24220832863024302
  batch 15 loss: 0.24307837386926015
  batch 16 loss: 0.2435940345749259
  batch 17 loss: 0.2403507741058574
  batch 18 loss: 0.240269901851813
  batch 19 loss: 0.23801006690451973
  batch 20 loss: 0.23618320152163505
  batch 21 loss: 0.23563673382713682
  batch 22 loss: 0.23487714474851434
  batch 23 loss: 0.23435778138430222
  batch 24 loss: 0.232228958979249
  batch 25 loss: 0.2338779503107071
  batch 26 loss: 0.2320364834024356
  batch 27 loss: 0.23260508201740407
  batch 28 loss: 0.2320161142519542
  batch 29 loss: 0.23203623603130208
  batch 30 loss: 0.23172547320524853
  batch 31 loss: 0.2318340943705651
  batch 32 loss: 0.23192276898771524
  batch 33 loss: 0.23223010382869028
  batch 34 loss: 0.2309779770234052
  batch 35 loss: 0.23172208666801453
  batch 36 loss: 0.23273804949389565
  batch 37 loss: 0.23242979758494609
  batch 38 loss: 0.23259302973747253
  batch 39 loss: 0.23249266927058881
  batch 40 loss: 0.23234745673835278
  batch 41 loss: 0.23224413685682344
  batch 42 loss: 0.23231820691199528
  batch 43 loss: 0.23282310228015102
  batch 44 loss: 0.2325781560079618
  batch 45 loss: 0.2321620735857222
  batch 46 loss: 0.2330241144999214
  batch 47 loss: 0.2336835892910653
  batch 48 loss: 0.2327527552843094
  batch 49 loss: 0.23234629448579283
  batch 50 loss: 0.23203000009059907
  batch 51 loss: 0.23232961226912105
  batch 52 loss: 0.23267084188186204
  batch 53 loss: 0.2324742371181272
  batch 54 loss: 0.2328927875668914
  batch 55 loss: 0.23247205073183233
  batch 56 loss: 0.23251742630132607
  batch 57 loss: 0.23217348885117917
  batch 58 loss: 0.23276849759036097
  batch 59 loss: 0.23297761607978304
  batch 60 loss: 0.23305167009433111
  batch 61 loss: 0.23323652954375157
  batch 62 loss: 0.23440956612748484
  batch 63 loss: 0.2342697157273217
  batch 64 loss: 0.23511822312138975
  batch 65 loss: 0.23506000362909757
  batch 66 loss: 0.23531535538760098
  batch 67 loss: 0.23619882117456464
  batch 68 loss: 0.23604037831811345
  batch 69 loss: 0.23618385260519775
  batch 70 loss: 0.23639261488403593
  batch 71 loss: 0.23626559867825306
  batch 72 loss: 0.23648842113713422
  batch 73 loss: 0.23657085062706307
  batch 74 loss: 0.23622235514827677
  batch 75 loss: 0.23591058333714804
  batch 76 loss: 0.23665297149043335
  batch 77 loss: 0.2362264520549155
  batch 78 loss: 0.23650436389904755
  batch 79 loss: 0.23662037789067136
  batch 80 loss: 0.23606145065277814
  batch 81 loss: 0.23596914995599677
  batch 82 loss: 0.23637952546520932
  batch 83 loss: 0.23627091746732412
  batch 84 loss: 0.23631647122757776
  batch 85 loss: 0.23635606625500846
  batch 86 loss: 0.23670811154121577
  batch 87 loss: 0.23674966377773504
  batch 88 loss: 0.23662445745007557
  batch 89 loss: 0.236500383093116
  batch 90 loss: 0.23614763269821804
  batch 91 loss: 0.2360364067685473
  batch 92 loss: 0.23596077671517496
  batch 93 loss: 0.2360872148826558
  batch 94 loss: 0.2364320919868794
  batch 95 loss: 0.2362145295268611
  batch 96 loss: 0.23607824773838124
  batch 97 loss: 0.2359420108733718
  batch 98 loss: 0.236094703935847
  batch 99 loss: 0.23641735631408115
  batch 100 loss: 0.23619256168603897
  batch 101 loss: 0.23583826216140596
  batch 102 loss: 0.23617595609496622
  batch 103 loss: 0.23641848795622297
  batch 104 loss: 0.23649096460296556
  batch 105 loss: 0.23627893243517195
  batch 106 loss: 0.2365951268178112
  batch 107 loss: 0.23651501661706192
  batch 108 loss: 0.2365294136106968
  batch 109 loss: 0.23638359928896668
  batch 110 loss: 0.23702502020380714
  batch 111 loss: 0.2373203027087289
  batch 112 loss: 0.2371126330856766
  batch 113 loss: 0.23686880901851484
  batch 114 loss: 0.23706052099403582
  batch 115 loss: 0.23716892716677293
  batch 116 loss: 0.2375685697228744
  batch 117 loss: 0.23776189626282096
  batch 118 loss: 0.23757198415065217
  batch 119 loss: 0.23765253857905125
  batch 120 loss: 0.23731780759990215
  batch 121 loss: 0.23706529886761973
  batch 122 loss: 0.23713619701686453
  batch 123 loss: 0.23704266427009085
  batch 124 loss: 0.23715250650721212
  batch 125 loss: 0.2370466728210449
  batch 126 loss: 0.23693969940382337
  batch 127 loss: 0.23719095597116965
  batch 128 loss: 0.23683211614843458
  batch 129 loss: 0.2370334462833035
  batch 130 loss: 0.2369335712148593
  batch 131 loss: 0.23684712220694273
  batch 132 loss: 0.2366421420240041
  batch 133 loss: 0.236821019895991
  batch 134 loss: 0.23683000144673816
  batch 135 loss: 0.23635029406459243
  batch 136 loss: 0.2363679765121025
  batch 137 loss: 0.2362850573811218
  batch 138 loss: 0.2362300455570221
  batch 139 loss: 0.23639538626876666
  batch 140 loss: 0.23618176813636507
  batch 141 loss: 0.23635827184568906
  batch 142 loss: 0.23628418682746485
  batch 143 loss: 0.23588655680626422
  batch 144 loss: 0.23588625507222283
  batch 145 loss: 0.23584355198103807
  batch 146 loss: 0.23578468250901732
  batch 147 loss: 0.23601209813234758
  batch 148 loss: 0.23612522676184372
  batch 149 loss: 0.23591562935569943
  batch 150 loss: 0.23588586797316868
  batch 151 loss: 0.235938445424402
  batch 152 loss: 0.23620504983945897
  batch 153 loss: 0.23606917788000667
  batch 154 loss: 0.23611284763395013
  batch 155 loss: 0.23600264316604982
  batch 156 loss: 0.2359928479179358
  batch 157 loss: 0.23622562722036033
  batch 158 loss: 0.23608707627163658
  batch 159 loss: 0.23607771904588495
  batch 160 loss: 0.2357436559163034
  batch 161 loss: 0.2358558201086447
  batch 162 loss: 0.23571992297599345
  batch 163 loss: 0.23577216011614888
  batch 164 loss: 0.2354258853124409
  batch 165 loss: 0.23544147384889197
  batch 166 loss: 0.23542033575744514
  batch 167 loss: 0.23531306700078314
  batch 168 loss: 0.23514835997706368
  batch 169 loss: 0.23505582250434265
  batch 170 loss: 0.23490702737780178
  batch 171 loss: 0.2349049464652413
  batch 172 loss: 0.2348421076529248
  batch 173 loss: 0.234795144355366
  batch 174 loss: 0.2346671305853745
  batch 175 loss: 0.23473586942468372
  batch 176 loss: 0.23452861988070337
  batch 177 loss: 0.23448101982558514
  batch 178 loss: 0.23451519020822612
  batch 179 loss: 0.23443454620558457
  batch 180 loss: 0.2343281012442377
  batch 181 loss: 0.23448950447430267
  batch 182 loss: 0.23443954568970335
  batch 183 loss: 0.23424223855219253
  batch 184 loss: 0.2341768174715664
  batch 185 loss: 0.23418659406739312
  batch 186 loss: 0.2343588202230392
  batch 187 loss: 0.2342386467252823
  batch 188 loss: 0.2339679589296909
  batch 189 loss: 0.23365510061935144
  batch 190 loss: 0.23358262510676134
  batch 191 loss: 0.23347643688711198
  batch 192 loss: 0.23335592549604675
  batch 193 loss: 0.23313001647514384
  batch 194 loss: 0.2333649691847182
  batch 195 loss: 0.23353624450854765
  batch 196 loss: 0.23340243101119995
  batch 197 loss: 0.23348212499303866
  batch 198 loss: 0.23377354397918237
  batch 199 loss: 0.2336257348707573
  batch 200 loss: 0.23386308878660203
  batch 201 loss: 0.23393619371883906
  batch 202 loss: 0.2340128693250146
  batch 203 loss: 0.23407404256865308
  batch 204 loss: 0.23397344212029494
  batch 205 loss: 0.23413567463072335
  batch 206 loss: 0.23418453473199918
  batch 207 loss: 0.2343284364627755
  batch 208 loss: 0.23429256297934514
  batch 209 loss: 0.2340850379478418
  batch 210 loss: 0.23416899939378102
  batch 211 loss: 0.23402538641369172
  batch 212 loss: 0.2339250810163201
  batch 213 loss: 0.23379918988881537
  batch 214 loss: 0.23382291316150505
  batch 215 loss: 0.2336681787357774
  batch 216 loss: 0.23344141713998937
  batch 217 loss: 0.23329045146291708
  batch 218 loss: 0.2331786688861497
  batch 219 loss: 0.2332421464326719
  batch 220 loss: 0.2332065305926583
  batch 221 loss: 0.2332988737395446
  batch 222 loss: 0.23335732989483052
  batch 223 loss: 0.23334231384666512
  batch 224 loss: 0.23330732847430877
  batch 225 loss: 0.23324593974484337
  batch 226 loss: 0.23326970195084545
  batch 227 loss: 0.23319072200863372
  batch 228 loss: 0.23309562564419026
  batch 229 loss: 0.23307198923748132
  batch 230 loss: 0.23312401201414026
  batch 231 loss: 0.23299903154889226
  batch 232 loss: 0.23284013235363468
  batch 233 loss: 0.232774588491272
  batch 234 loss: 0.23262975313979337
  batch 235 loss: 0.23249265545226158
  batch 236 loss: 0.23231568326384333
  batch 237 loss: 0.23224535911395078
  batch 238 loss: 0.23219355784293985
  batch 239 loss: 0.23212594485931318
  batch 240 loss: 0.23208771565308173
  batch 241 loss: 0.2321524305956987
  batch 242 loss: 0.2320080608868402
  batch 243 loss: 0.2320038324275625
  batch 244 loss: 0.2319960495243307
  batch 245 loss: 0.2318900941586008
  batch 246 loss: 0.23187025079155357
  batch 247 loss: 0.2319536938479072
  batch 248 loss: 0.23195271644621127
  batch 249 loss: 0.23180882650685597
  batch 250 loss: 0.2316693445444107
  batch 251 loss: 0.2316728694980363
  batch 252 loss: 0.2314713764640074
  batch 253 loss: 0.23141746275980954
  batch 254 loss: 0.23117745051703115
  batch 255 loss: 0.2311913325506098
  batch 256 loss: 0.23134731350000948
  batch 257 loss: 0.23136149300443523
  batch 258 loss: 0.23124731072159702
  batch 259 loss: 0.23125937805442737
  batch 260 loss: 0.23121862033238777
  batch 261 loss: 0.2311868041867954
  batch 262 loss: 0.23108780782186347
  batch 263 loss: 0.23114470519947009
  batch 264 loss: 0.23102606048412394
  batch 265 loss: 0.23085900404543247
  batch 266 loss: 0.2308393097237537
  batch 267 loss: 0.23088616534565273
  batch 268 loss: 0.23077225390432485
  batch 269 loss: 0.23073300824511006
  batch 270 loss: 0.23087139477332433
  batch 271 loss: 0.23080876524597957
  batch 272 loss: 0.23068605193539576
  batch 273 loss: 0.23059275364264464
  batch 274 loss: 0.2306566880675998
  batch 275 loss: 0.23056972574103962
  batch 276 loss: 0.230418482174476
  batch 277 loss: 0.23037579897723903
  batch 278 loss: 0.23028419327607258
  batch 279 loss: 0.23031695037546124
  batch 280 loss: 0.2301748713744538
  batch 281 loss: 0.23008118071377914
  batch 282 loss: 0.2300589027556967
  batch 283 loss: 0.2299346621066016
  batch 284 loss: 0.22992817695501824
  batch 285 loss: 0.2298481651565485
  batch 286 loss: 0.22984940959857061
  batch 287 loss: 0.22989014549122455
  batch 288 loss: 0.22982890350330207
  batch 289 loss: 0.22998876457189607
  batch 290 loss: 0.22981982580546675
  batch 291 loss: 0.2298165286324688
  batch 292 loss: 0.22987781434434734
  batch 293 loss: 0.2298170236597289
  batch 294 loss: 0.22974891666652394
  batch 295 loss: 0.22973913085662712
  batch 296 loss: 0.2298031029467647
  batch 297 loss: 0.22972940600881672
  batch 298 loss: 0.22964040374995878
  batch 299 loss: 0.229489050979997
  batch 300 loss: 0.22946286876996358
  batch 301 loss: 0.22950615171974284
  batch 302 loss: 0.22942940116126015
  batch 303 loss: 0.22930642711644125
  batch 304 loss: 0.22928832089038273
  batch 305 loss: 0.2291593622477328
  batch 306 loss: 0.22931130770959107
  batch 307 loss: 0.22936198236693778
  batch 308 loss: 0.22947863649044717
  batch 309 loss: 0.22947926708409702
  batch 310 loss: 0.22946088102556045
  batch 311 loss: 0.22956580268608412
  batch 312 loss: 0.22975503958952734
  batch 313 loss: 0.22983339276557532
  batch 314 loss: 0.22976707605419644
  batch 315 loss: 0.22972793607484726
  batch 316 loss: 0.22973508878221996
  batch 317 loss: 0.2297128087338589
  batch 318 loss: 0.22969282821476836
  batch 319 loss: 0.2296261523491163
  batch 320 loss: 0.22958432682789862
  batch 321 loss: 0.22962457651846876
  batch 322 loss: 0.22959397228793327
  batch 323 loss: 0.22944719581965692
  batch 324 loss: 0.22928485619249167
  batch 325 loss: 0.2293257359816478
  batch 326 loss: 0.22933968567957908
  batch 327 loss: 0.22934765352750772
  batch 328 loss: 0.22915136550621287
  batch 329 loss: 0.22926538883733893
  batch 330 loss: 0.22921519062735818
  batch 331 loss: 0.22917144482229412
  batch 332 loss: 0.22903088394956417
  batch 333 loss: 0.22913356125354767
  batch 334 loss: 0.22900626099038268
  batch 335 loss: 0.22892293471898606
  batch 336 loss: 0.22883360141089984
  batch 337 loss: 0.22882075895187762
  batch 338 loss: 0.2289160636402446
  batch 339 loss: 0.22884440118760135
  batch 340 loss: 0.22881549640613444
  batch 341 loss: 0.2288169637977902
  batch 342 loss: 0.22879603004072144
  batch 343 loss: 0.2287760577639755
  batch 344 loss: 0.2287669848910598
  batch 345 loss: 0.22885874095170394
  batch 346 loss: 0.22872917741709362
  batch 347 loss: 0.22872819182508616
  batch 348 loss: 0.22865579195920077
  batch 349 loss: 0.2285386336737854
  batch 350 loss: 0.2285712199125971
  batch 351 loss: 0.228548388577934
  batch 352 loss: 0.22859368968585675
  batch 353 loss: 0.22855873177983604
  batch 354 loss: 0.22873892795063008
  batch 355 loss: 0.22867971411053564
  batch 356 loss: 0.22865679671757677
  batch 357 loss: 0.22861733694537348
  batch 358 loss: 0.22863699907863605
  batch 359 loss: 0.22853902277010066
  batch 360 loss: 0.22848584581580428
  batch 361 loss: 0.2284987191992123
  batch 362 loss: 0.2283814810422244
  batch 363 loss: 0.22823983923462796
  batch 364 loss: 0.22809198085259605
  batch 365 loss: 0.22802695644228427
  batch 366 loss: 0.22795724807704082
  batch 367 loss: 0.2278935802486352
  batch 368 loss: 0.2278116066818652
  batch 369 loss: 0.22774443850123138
  batch 370 loss: 0.22761289312227354
  batch 371 loss: 0.22760174658420593
  batch 372 loss: 0.2275334939921415
  batch 373 loss: 0.227405629792418
  batch 374 loss: 0.227333414004767
  batch 375 loss: 0.22720932018756868
  batch 376 loss: 0.2271699385440096
  batch 377 loss: 0.22714905310372774
  batch 378 loss: 0.22711281841078762
  batch 379 loss: 0.22705969764878065
  batch 380 loss: 0.2270990399153609
  batch 381 loss: 0.2270997759551201
  batch 382 loss: 0.22705292147803682
  batch 383 loss: 0.22704890603807515
  batch 384 loss: 0.22699752112384886
  batch 385 loss: 0.22710298636516968
  batch 386 loss: 0.22706350954394267
  batch 387 loss: 0.22711164225133507
  batch 388 loss: 0.2271131877739405
  batch 389 loss: 0.22708137716456367
  batch 390 loss: 0.22702033240825703
  batch 391 loss: 0.2271066566791071
  batch 392 loss: 0.227070134931377
  batch 393 loss: 0.2269906653778547
  batch 394 loss: 0.22700487315503473
  batch 395 loss: 0.22693053216119355
  batch 396 loss: 0.22685725746130703
  batch 397 loss: 0.2268805127750416
  batch 398 loss: 0.22676721829265806
  batch 399 loss: 0.22680976045759102
  batch 400 loss: 0.22685141310095788
  batch 401 loss: 0.22676716648283743
  batch 402 loss: 0.2267762916672289
  batch 403 loss: 0.2268561504540905
  batch 404 loss: 0.226882141710508
  batch 405 loss: 0.22690056091473426
  batch 406 loss: 0.22695054319398156
  batch 407 loss: 0.2269784487436093
  batch 408 loss: 0.22704092289010683
  batch 409 loss: 0.2270378612145818
  batch 410 loss: 0.22713636948568064
  batch 411 loss: 0.22716531280763538
  batch 412 loss: 0.2271717698539345
  batch 413 loss: 0.22728248268870985
  batch 414 loss: 0.22733047280622565
  batch 415 loss: 0.2272736115268914
  batch 416 loss: 0.22738271568400356
  batch 417 loss: 0.22742947460793192
  batch 418 loss: 0.22745925501773231
  batch 419 loss: 0.22752187225016318
  batch 420 loss: 0.2274860693585305
  batch 421 loss: 0.2274646624179464
  batch 422 loss: 0.22767495745307462
  batch 423 loss: 0.22760981261448376
  batch 424 loss: 0.2275567725300789
  batch 425 loss: 0.2275235625224955
  batch 426 loss: 0.22750190939282028
  batch 427 loss: 0.22746016591159185
  batch 428 loss: 0.22745697019256164
  batch 429 loss: 0.22761419832289637
  batch 430 loss: 0.22768750530342724
  batch 431 loss: 0.2278065518270789
  batch 432 loss: 0.22780979993856615
  batch 433 loss: 0.2278367868403089
  batch 434 loss: 0.2278755670456293
  batch 435 loss: 0.22784880067425212
  batch 436 loss: 0.22786603389530008
  batch 437 loss: 0.2279406031700239
  batch 438 loss: 0.22803084293729095
  batch 439 loss: 0.22801076938188158
  batch 440 loss: 0.22810493409633636
  batch 441 loss: 0.22801548398000582
  batch 442 loss: 0.22790085480493658
  batch 443 loss: 0.2278800249839744
  batch 444 loss: 0.22776039787106686
  batch 445 loss: 0.22772702377163961
  batch 446 loss: 0.22763631776843904
  batch 447 loss: 0.22756287752248563
  batch 448 loss: 0.22761155708160782
  batch 449 loss: 0.22767901078762615
  batch 450 loss: 0.22765150159597397
  batch 451 loss: 0.227636931707748
  batch 452 loss: 0.22773266639725298
  batch 453 loss: 0.22779498257515973
  batch 454 loss: 0.2278246383638109
  batch 455 loss: 0.22785256793210795
  batch 456 loss: 0.22790592372940296
  batch 457 loss: 0.22792588034135247
  batch 458 loss: 0.22791414171066868
  batch 459 loss: 0.2280107556085441
  batch 460 loss: 0.2280848990963853
  batch 461 loss: 0.22805603204088976
  batch 462 loss: 0.22806065094161343
  batch 463 loss: 0.22797794810639088
  batch 464 loss: 0.22796859897287755
  batch 465 loss: 0.22782355345064595
  batch 466 loss: 0.22768409137690016
  batch 467 loss: 0.2277795217723867
  batch 468 loss: 0.2277522884054571
  batch 469 loss: 0.22784749477275654
  batch 470 loss: 0.22777065545954603
  batch 471 loss: 0.22779993377546848
  batch 472 loss: 0.22762979535480676
LOSS train 0.22762979535480676 valid 0.307797372341156
LOSS train 0.22762979535480676 valid 0.30493511259555817
LOSS train 0.22762979535480676 valid 0.3013509114583333
LOSS train 0.22762979535480676 valid 0.2983710691332817
LOSS train 0.22762979535480676 valid 0.2956728398799896
LOSS train 0.22762979535480676 valid 0.3008803923924764
LOSS train 0.22762979535480676 valid 0.31306928396224976
LOSS train 0.22762979535480676 valid 0.310882143676281
LOSS train 0.22762979535480676 valid 0.3139973282814026
LOSS train 0.22762979535480676 valid 0.31451165974140166
LOSS train 0.22762979535480676 valid 0.3110425689003684
LOSS train 0.22762979535480676 valid 0.31135594844818115
LOSS train 0.22762979535480676 valid 0.30844592589598435
LOSS train 0.22762979535480676 valid 0.30828459773744854
LOSS train 0.22762979535480676 valid 0.3018760114908218
LOSS train 0.22762979535480676 valid 0.30191771034151316
LOSS train 0.22762979535480676 valid 0.303178253419259
LOSS train 0.22762979535480676 valid 0.30538888027270633
LOSS train 0.22762979535480676 valid 0.3077501207590103
LOSS train 0.22762979535480676 valid 0.3060481958091259
LOSS train 0.22762979535480676 valid 0.30438271945431117
LOSS train 0.22762979535480676 valid 0.30225972492586484
LOSS train 0.22762979535480676 valid 0.3035367876291275
LOSS train 0.22762979535480676 valid 0.3013718295842409
LOSS train 0.22762979535480676 valid 0.300708686709404
LOSS train 0.22762979535480676 valid 0.30000278640251893
LOSS train 0.22762979535480676 valid 0.3003793861027117
LOSS train 0.22762979535480676 valid 0.3006802781351975
LOSS train 0.22762979535480676 valid 0.30034616743696146
LOSS train 0.22762979535480676 valid 0.30186121513446174
LOSS train 0.22762979535480676 valid 0.30422232468282023
LOSS train 0.22762979535480676 valid 0.3039625217206776
LOSS train 0.22762979535480676 valid 0.30485865067351947
LOSS train 0.22762979535480676 valid 0.3046160625184284
LOSS train 0.22762979535480676 valid 0.3066626314605985
LOSS train 0.22762979535480676 valid 0.3071511756214831
LOSS train 0.22762979535480676 valid 0.3070193122367601
LOSS train 0.22762979535480676 valid 0.30845799453948675
LOSS train 0.22762979535480676 valid 0.30841810619219756
LOSS train 0.22762979535480676 valid 0.3077693093568087
LOSS train 0.22762979535480676 valid 0.3093516474089971
LOSS train 0.22762979535480676 valid 0.3103434596033323
LOSS train 0.22762979535480676 valid 0.3094453600256942
LOSS train 0.22762979535480676 valid 0.30968569456176326
LOSS train 0.22762979535480676 valid 0.30888358453909553
LOSS train 0.22762979535480676 valid 0.31017007937897806
LOSS train 0.22762979535480676 valid 0.3117538509850806
LOSS train 0.22762979535480676 valid 0.3117495281621814
LOSS train 0.22762979535480676 valid 0.3119434364596192
LOSS train 0.22762979535480676 valid 0.3108283683657646
LOSS train 0.22762979535480676 valid 0.31051140176314934
LOSS train 0.22762979535480676 valid 0.3100106925345384
LOSS train 0.22762979535480676 valid 0.31000886463894034
LOSS train 0.22762979535480676 valid 0.3100436233260013
LOSS train 0.22762979535480676 valid 0.3099097043275833
LOSS train 0.22762979535480676 valid 0.30928967068237917
LOSS train 0.22762979535480676 valid 0.3086471863483128
LOSS train 0.22762979535480676 valid 0.30787984281778336
LOSS train 0.22762979535480676 valid 0.30830441263772673
LOSS train 0.22762979535480676 valid 0.3084786094725132
LOSS train 0.22762979535480676 valid 0.30782644890370914
LOSS train 0.22762979535480676 valid 0.3088772270467974
LOSS train 0.22762979535480676 valid 0.30912300305707113
LOSS train 0.22762979535480676 valid 0.3106132640969008
LOSS train 0.22762979535480676 valid 0.31116794966734374
LOSS train 0.22762979535480676 valid 0.3109811343478434
LOSS train 0.22762979535480676 valid 0.3100093807747115
LOSS train 0.22762979535480676 valid 0.3101915576878716
LOSS train 0.22762979535480676 valid 0.30928736536399176
LOSS train 0.22762979535480676 valid 0.30959964181695665
LOSS train 0.22762979535480676 valid 0.3090911402668751
LOSS train 0.22762979535480676 valid 0.30967109236452317
LOSS train 0.22762979535480676 valid 0.30955667120136626
LOSS train 0.22762979535480676 valid 0.3094224140450761
LOSS train 0.22762979535480676 valid 0.3094386637210846
LOSS train 0.22762979535480676 valid 0.3097841543586631
LOSS train 0.22762979535480676 valid 0.30999741267848324
LOSS train 0.22762979535480676 valid 0.3108881494173637
LOSS train 0.22762979535480676 valid 0.3114555670490748
LOSS train 0.22762979535480676 valid 0.31061794273555277
LOSS train 0.22762979535480676 valid 0.30963200238751776
LOSS train 0.22762979535480676 valid 0.3102981569926913
LOSS train 0.22762979535480676 valid 0.3102441737091685
LOSS train 0.22762979535480676 valid 0.31021293128530186
LOSS train 0.22762979535480676 valid 0.3098849382470636
LOSS train 0.22762979535480676 valid 0.30920511640088505
LOSS train 0.22762979535480676 valid 0.30893458254720974
LOSS train 0.22762979535480676 valid 0.30830754966221074
LOSS train 0.22762979535480676 valid 0.30881908921043527
LOSS train 0.22762979535480676 valid 0.30941992617315717
LOSS train 0.22762979535480676 valid 0.3095555238343857
LOSS train 0.22762979535480676 valid 0.30984675025810365
LOSS train 0.22762979535480676 valid 0.30969174238302377
LOSS train 0.22762979535480676 valid 0.3096991144913308
LOSS train 0.22762979535480676 valid 0.3095511811344247
LOSS train 0.22762979535480676 valid 0.3102342546917498
LOSS train 0.22762979535480676 valid 0.31026332418328706
LOSS train 0.22762979535480676 valid 0.31064722382900667
LOSS train 0.22762979535480676 valid 0.3109211622163503
LOSS train 0.22762979535480676 valid 0.3110397644340992
LOSS train 0.22762979535480676 valid 0.31128635633699964
LOSS train 0.22762979535480676 valid 0.3113921627986665
LOSS train 0.22762979535480676 valid 0.31146771685012337
LOSS train 0.22762979535480676 valid 0.3113745882247503
LOSS train 0.22762979535480676 valid 0.3113613235098975
LOSS train 0.22762979535480676 valid 0.31156140523699094
LOSS train 0.22762979535480676 valid 0.3114254517811481
LOSS train 0.22762979535480676 valid 0.31143433283324595
LOSS train 0.22762979535480676 valid 0.3119404202480929
LOSS train 0.22762979535480676 valid 0.31233031221411445
LOSS train 0.22762979535480676 valid 0.3117316044397182
LOSS train 0.22762979535480676 valid 0.3115054721544896
LOSS train 0.22762979535480676 valid 0.311385140614172
LOSS train 0.22762979535480676 valid 0.31102109909580467
LOSS train 0.22762979535480676 valid 0.31126375185406724
LOSS train 0.22762979535480676 valid 0.3108945718870081
LOSS train 0.22762979535480676 valid 0.3110549787425587
LOSS train 0.22762979535480676 valid 0.31091063621185594
LOSS train 0.22762979535480676 valid 0.3107626569621703
LOSS train 0.22762979535480676 valid 0.3105494286864996
LOSS train 0.22762979535480676 valid 0.310472593204049
LOSS train 0.22762979535480676 valid 0.31015036743683894
LOSS train 0.22762979535480676 valid 0.31011622720133
LOSS train 0.22762979535480676 valid 0.31066289580156725
LOSS train 0.22762979535480676 valid 0.3106313441991806
LOSS train 0.22762979535480676 valid 0.310786787597906
LOSS train 0.22762979535480676 valid 0.3108880664185276
LOSS train 0.22762979535480676 valid 0.3114397617755458
LOSS train 0.22762979535480676 valid 0.31152201703814575
LOSS train 0.22762979535480676 valid 0.311286191871533
LOSS train 0.22762979535480676 valid 0.3112670227543998
LOSS train 0.22762979535480676 valid 0.3110219385813583
LOSS train 0.22762979535480676 valid 0.31107120126261745
LOSS train 0.22762979535480676 valid 0.3112625923396936
LOSS train 0.22762979535480676 valid 0.31111101033510985
LOSS train 0.22762979535480676 valid 0.31086387189433856
LOSS train 0.22762979535480676 valid 0.3106475388699204
LOSS train 0.22762979535480676 valid 0.31053152129701944
LOSS train 0.22762979535480676 valid 0.31047256569639387
LOSS train 0.22762979535480676 valid 0.31056766478078707
LOSS train 0.22762979535480676 valid 0.3107041087767757
LOSS train 0.22762979535480676 valid 0.3110945452476891
LOSS train 0.22762979535480676 valid 0.3109761929803795
LOSS train 0.22762979535480676 valid 0.31080372393545175
LOSS train 0.22762979535480676 valid 0.31057221057086154
LOSS train 0.22762979535480676 valid 0.31078465415598594
LOSS train 0.22762979535480676 valid 0.3105546198531884
LOSS train 0.22762979535480676 valid 0.31174924738101056
LOSS train 0.22762979535480676 valid 0.31190716230229243
LOSS train 0.22762979535480676 valid 0.312236093779405
LOSS train 0.22762979535480676 valid 0.3121271687035529
LOSS train 0.22762979535480676 valid 0.31168476345115587
LOSS train 0.22762979535480676 valid 0.31178852828109965
LOSS train 0.22762979535480676 valid 0.3116517778147351
LOSS train 0.22762979535480676 valid 0.3116517479381254
LOSS train 0.22762979535480676 valid 0.3117766520724847
LOSS train 0.22762979535480676 valid 0.31164901603938666
LOSS train 0.22762979535480676 valid 0.3117353045676328
LOSS train 0.22762979535480676 valid 0.31177915257852784
LOSS train 0.22762979535480676 valid 0.3118094147183001
LOSS train 0.22762979535480676 valid 0.31165270026055925
LOSS train 0.22762979535480676 valid 0.3116100701836892
LOSS train 0.22762979535480676 valid 0.3113697559738452
LOSS train 0.22762979535480676 valid 0.3108780647559864
LOSS train 0.22762979535480676 valid 0.31049804615251947
LOSS train 0.22762979535480676 valid 0.31053147635546074
LOSS train 0.22762979535480676 valid 0.3109158953863704
LOSS train 0.22762979535480676 valid 0.31070490837806747
LOSS train 0.22762979535480676 valid 0.3109127168471997
LOSS train 0.22762979535480676 valid 0.3111224824891371
LOSS train 0.22762979535480676 valid 0.3112138345924734
LOSS train 0.22762979535480676 valid 0.31098188130661497
LOSS train 0.22762979535480676 valid 0.3109705685880143
LOSS train 0.22762979535480676 valid 0.3110763865640794
LOSS train 0.22762979535480676 valid 0.31086845551218306
LOSS train 0.22762979535480676 valid 0.3107878281311555
LOSS train 0.22762979535480676 valid 0.3107489421542755
LOSS train 0.22762979535480676 valid 0.3109865729393584
LOSS train 0.22762979535480676 valid 0.31076639441138537
LOSS train 0.22762979535480676 valid 0.3105606514546606
LOSS train 0.22762979535480676 valid 0.310691317142044
LOSS train 0.22762979535480676 valid 0.31087986961170866
LOSS train 0.22762979535480676 valid 0.31104288547416853
LOSS train 0.22762979535480676 valid 0.3110286225767239
LOSS train 0.22762979535480676 valid 0.31058186340976407
LOSS train 0.22762979535480676 valid 0.31056646570082636
LOSS train 0.22762979535480676 valid 0.3104119275342972
LOSS train 0.22762979535480676 valid 0.3104575897784943
LOSS train 0.22762979535480676 valid 0.31032114196075966
LOSS train 0.22762979535480676 valid 0.31050771129758736
LOSS train 0.22762979535480676 valid 0.3103543537137396
LOSS train 0.22762979535480676 valid 0.310386229151239
LOSS train 0.22762979535480676 valid 0.3102019341189627
LOSS train 0.22762979535480676 valid 0.3100281159595116
LOSS train 0.22762979535480676 valid 0.3097598519080724
LOSS train 0.22762979535480676 valid 0.30968380673807494
LOSS train 0.22762979535480676 valid 0.309874975136694
LOSS train 0.22762979535480676 valid 0.30959285374241646
LOSS train 0.22762979535480676 valid 0.30962976109442397
LOSS train 0.22762979535480676 valid 0.3095540854334831
LOSS train 0.22762979535480676 valid 0.30938043938347354
LOSS train 0.22762979535480676 valid 0.309246622837416
LOSS train 0.22762979535480676 valid 0.3090621322540227
LOSS train 0.22762979535480676 valid 0.30904365316325544
LOSS train 0.22762979535480676 valid 0.3087226211297803
LOSS train 0.22762979535480676 valid 0.3087157089878054
LOSS train 0.22762979535480676 valid 0.30861688012950084
LOSS train 0.22762979535480676 valid 0.30843289418575853
LOSS train 0.22762979535480676 valid 0.30833182801185044
LOSS train 0.22762979535480676 valid 0.30846721693163826
LOSS train 0.22762979535480676 valid 0.30870023405099933
LOSS train 0.22762979535480676 valid 0.30865029660317134
LOSS train 0.22762979535480676 valid 0.3089066219861519
LOSS train 0.22762979535480676 valid 0.30889932177612717
LOSS train 0.22762979535480676 valid 0.30872695342052814
LOSS train 0.22762979535480676 valid 0.3086754622420779
LOSS train 0.22762979535480676 valid 0.3087266802375767
LOSS train 0.22762979535480676 valid 0.30878819354356973
LOSS train 0.22762979535480676 valid 0.308929143362938
LOSS train 0.22762979535480676 valid 0.3090044123882597
LOSS train 0.22762979535480676 valid 0.30901503529214214
LOSS train 0.22762979535480676 valid 0.3089439618560645
LOSS train 0.22762979535480676 valid 0.30909879184891825
LOSS train 0.22762979535480676 valid 0.3091189088965101
LOSS train 0.22762979535480676 valid 0.3090681976079941
LOSS train 0.22762979535480676 valid 0.30913227354794476
LOSS train 0.22762979535480676 valid 0.30947788716150276
LOSS train 0.22762979535480676 valid 0.3094868711604361
LOSS train 0.22762979535480676 valid 0.30959958482257144
LOSS train 0.22762979535480676 valid 0.30975091269482735
LOSS train 0.22762979535480676 valid 0.3097578461939122
LOSS train 0.22762979535480676 valid 0.309781186860697
LOSS train 0.22762979535480676 valid 0.3097908933658968
LOSS train 0.22762979535480676 valid 0.3097253324002282
LOSS train 0.22762979535480676 valid 0.31004448133580226
LOSS train 0.22762979535480676 valid 0.30977545697557723
LOSS train 0.22762979535480676 valid 0.30992157468061404
LOSS train 0.22762979535480676 valid 0.31000049794171036
LOSS train 0.22762979535480676 valid 0.30980664597644963
LOSS train 0.22762979535480676 valid 0.3097998177632689
LOSS train 0.22762979535480676 valid 0.3098515145387887
LOSS train 0.22762979535480676 valid 0.3095555662000475
LOSS train 0.22762979535480676 valid 0.3097986129453643
LOSS train 0.22762979535480676 valid 0.31022250047716937
LOSS train 0.22762979535480676 valid 0.31057602635451725
LOSS train 0.22762979535480676 valid 0.31056262619369396
LOSS train 0.22762979535480676 valid 0.31044957059838996
LOSS train 0.22762979535480676 valid 0.3103176211758006
LOSS train 0.22762979535480676 valid 0.3102400164647275
LOSS train 0.22762979535480676 valid 0.31049384325742724
LOSS train 0.22762979535480676 valid 0.3106077487368983
LOSS train 0.22762979535480676 valid 0.3108336151474052
LOSS train 0.22762979535480676 valid 0.3108559197234542
LOSS train 0.22762979535480676 valid 0.31064673219844113
LOSS train 0.22762979535480676 valid 0.3106912791144614
LOSS train 0.22762979535480676 valid 0.31078553741099313
LOSS train 0.22762979535480676 valid 0.3106091123958506
LOSS train 0.22762979535480676 valid 0.3107954142976177
LOSS train 0.22762979535480676 valid 0.3108246539895599
LOSS train 0.22762979535480676 valid 0.31077393287649524
LOSS train 0.22762979535480676 valid 0.31094382828907946
LOSS train 0.22762979535480676 valid 0.31096792818253277
LOSS train 0.22762979535480676 valid 0.3110438358081158
LOSS train 0.22762979535480676 valid 0.3110744380251025
LOSS train 0.22762979535480676 valid 0.3109989747123898
LOSS train 0.22762979535480676 valid 0.3111760139577371
LOSS train 0.22762979535480676 valid 0.3111592929126618
LOSS train 0.22762979535480676 valid 0.31132386430208364
LOSS train 0.22762979535480676 valid 0.3112615079352404
LOSS train 0.22762979535480676 valid 0.31112904929452473
LOSS train 0.22762979535480676 valid 0.31128035782667984
LOSS train 0.22762979535480676 valid 0.3115391357089667
LOSS train 0.22762979535480676 valid 0.31163513316557956
LOSS train 0.22762979535480676 valid 0.3116047751620738
LOSS train 0.22762979535480676 valid 0.3114908524534919
LOSS train 0.22762979535480676 valid 0.31133708276826405
LOSS train 0.22762979535480676 valid 0.3111549546464686
LOSS train 0.22762979535480676 valid 0.31095539596226573
LOSS train 0.22762979535480676 valid 0.31091372722152316
LOSS train 0.22762979535480676 valid 0.3108583021908998
LOSS train 0.22762979535480676 valid 0.3106828717468472
LOSS train 0.22762979535480676 valid 0.3104395700473312
LOSS train 0.22762979535480676 valid 0.3103066492207059
LOSS train 0.22762979535480676 valid 0.3103329078622267
LOSS train 0.22762979535480676 valid 0.31048062882925337
LOSS train 0.22762979535480676 valid 0.31044854484237994
LOSS train 0.22762979535480676 valid 0.3103909956661251
LOSS train 0.22762979535480676 valid 0.3103300623802675
LOSS train 0.22762979535480676 valid 0.3104345676808209
LOSS train 0.22762979535480676 valid 0.3104408021630912
LOSS train 0.22762979535480676 valid 0.3103710906612095
LOSS train 0.22762979535480676 valid 0.31044382908164636
LOSS train 0.22762979535480676 valid 0.3104006680612271
LOSS train 0.22762979535480676 valid 0.310579292425493
LOSS train 0.22762979535480676 valid 0.3106609397015329
LOSS train 0.22762979535480676 valid 0.3105439699783519
LOSS train 0.22762979535480676 valid 0.3105535166030781
LOSS train 0.22762979535480676 valid 0.31052311894877643
LOSS train 0.22762979535480676 valid 0.31049082321068117
LOSS train 0.22762979535480676 valid 0.31038677295049033
LOSS train 0.22762979535480676 valid 0.31046426019003226
LOSS train 0.22762979535480676 valid 0.3104806127927161
LOSS train 0.22762979535480676 valid 0.31062865188412936
LOSS train 0.22762979535480676 valid 0.31060578203515005
LOSS train 0.22762979535480676 valid 0.31056863440841925
LOSS train 0.22762979535480676 valid 0.31055403379053853
LOSS train 0.22762979535480676 valid 0.3104189578019058
LOSS train 0.22762979535480676 valid 0.31027364150270237
LOSS train 0.22762979535480676 valid 0.3102388829475082
LOSS train 0.22762979535480676 valid 0.3103516496958271
LOSS train 0.22762979535480676 valid 0.3102392575365171
LOSS train 0.22762979535480676 valid 0.3101940274429627
LOSS train 0.22762979535480676 valid 0.310134253848475
LOSS train 0.22762979535480676 valid 0.3102178728314722
LOSS train 0.22762979535480676 valid 0.3101537660946922
LOSS train 0.22762979535480676 valid 0.31007366900957084
LOSS train 0.22762979535480676 valid 0.3101395313108005
LOSS train 0.22762979535480676 valid 0.3103148719799594
LOSS train 0.22762979535480676 valid 0.31053115432165257
LOSS train 0.22762979535480676 valid 0.3104577673599124
LOSS train 0.22762979535480676 valid 0.3107328273796961
LOSS train 0.22762979535480676 valid 0.31052248476084715
LOSS train 0.22762979535480676 valid 0.3103564725756276
LOSS train 0.22762979535480676 valid 0.3103963280165637
LOSS train 0.22762979535480676 valid 0.31046208638411305
LOSS train 0.22762979535480676 valid 0.31056097286976186
LOSS train 0.22762979535480676 valid 0.3105670447925544
LOSS train 0.22762979535480676 valid 0.3105848867718766
LOSS train 0.22762979535480676 valid 0.31060991207517025
LOSS train 0.22762979535480676 valid 0.31064740076209557
LOSS train 0.22762979535480676 valid 0.310549063084709
LOSS train 0.22762979535480676 valid 0.3104634776890996
LOSS train 0.22762979535480676 valid 0.3104910619624026
LOSS train 0.22762979535480676 valid 0.31064363600251205
LOSS train 0.22762979535480676 valid 0.31055497856282477
LOSS train 0.22762979535480676 valid 0.3103161591681696
LOSS train 0.22762979535480676 valid 0.310323413914672
LOSS train 0.22762979535480676 valid 0.31025881024860064
LOSS train 0.22762979535480676 valid 0.31013233990444194
LOSS train 0.22762979535480676 valid 0.3101207632352324
LOSS train 0.22762979535480676 valid 0.30995521377608226
LOSS train 0.22762979535480676 valid 0.3099995119529858
LOSS train 0.22762979535480676 valid 0.309937942740521
LOSS train 0.22762979535480676 valid 0.3101190190848916
LOSS train 0.22762979535480676 valid 0.3101554750532344
LOSS train 0.22762979535480676 valid 0.31007807180716124
LOSS train 0.22762979535480676 valid 0.3098547689251666
LOSS train 0.22762979535480676 valid 0.30973547230335485
LOSS train 0.22762979535480676 valid 0.3097786199312155
LOSS train 0.22762979535480676 valid 0.30969558583838597
LOSS train 0.22762979535480676 valid 0.3096389636779443
LOSS train 0.22762979535480676 valid 0.30955331861464813
LOSS train 0.22762979535480676 valid 0.3094849657826653
LOSS train 0.22762979535480676 valid 0.3095379066416773
LOSS train 0.22762979535480676 valid 0.30974707238271204
LOSS train 0.22762979535480676 valid 0.3099001751103428
LOSS train 0.22762979535480676 valid 0.30986141519887106
LOSS train 0.22762979535480676 valid 0.30975567049986824
LOSS train 0.22762979535480676 valid 0.30975066540467044
LOSS train 0.22762979535480676 valid 0.30974560268223283
LOSS train 0.22762979535480676 valid 0.30979454273827517
LOSS train 0.22762979535480676 valid 0.30993389889844875
LOSS train 0.22762979535480676 valid 0.30976009036391233
LOSS train 0.22762979535480676 valid 0.3097104532132437
LOSS train 0.22762979535480676 valid 0.30975805459773703
LOSS train 0.22762979535480676 valid 0.30970883992363196
LOSS train 0.22762979535480676 valid 0.3095377100179891
LOSS train 0.22762979535480676 valid 0.3094766847384365
LOSS train 0.22762979535480676 valid 0.3096241298002925
EPOCH 20:
  batch 1 loss: 0.2559947967529297
  batch 2 loss: 0.23776792734861374
  batch 3 loss: 0.23562920093536377
  batch 4 loss: 0.23760071396827698
  batch 5 loss: 0.24275461435317994
  batch 6 loss: 0.2403192420800527
  batch 7 loss: 0.24269249183791025
  batch 8 loss: 0.24778691306710243
  batch 9 loss: 0.24644716249571907
  batch 10 loss: 0.24341585338115693
  batch 11 loss: 0.24174801057035272
  batch 12 loss: 0.23947112883130708
  batch 13 loss: 0.23752878606319427
  batch 14 loss: 0.23746263555118016
  batch 15 loss: 0.23737605412801108
  batch 16 loss: 0.23626843467354774
  batch 17 loss: 0.2334716986207401
  batch 18 loss: 0.233628844221433
  batch 19 loss: 0.2301705887443141
  batch 20 loss: 0.22947107553482055
  batch 21 loss: 0.22957879021054223
  batch 22 loss: 0.23000528595664285
  batch 23 loss: 0.22899496684903684
  batch 24 loss: 0.22667123998204866
  batch 25 loss: 0.22830003380775452
  batch 26 loss: 0.22657500264736322
  batch 27 loss: 0.22684438195493487
  batch 28 loss: 0.22610793582030705
  batch 29 loss: 0.22647795029755297
  batch 30 loss: 0.22629863917827606
  batch 31 loss: 0.22688720976152726
  batch 32 loss: 0.22689956100657582
  batch 33 loss: 0.22707165235822852
  batch 34 loss: 0.2262797574786579
  batch 35 loss: 0.22673820768083844
  batch 36 loss: 0.2274280000064108
  batch 37 loss: 0.22682879945716342
  batch 38 loss: 0.227346166184074
  batch 39 loss: 0.22677257313178137
  batch 40 loss: 0.2271203063428402
  batch 41 loss: 0.22688844291175284
  batch 42 loss: 0.2269445593867983
  batch 43 loss: 0.22742784439131272
  batch 44 loss: 0.2274735847657377
  batch 45 loss: 0.227034396926562
  batch 46 loss: 0.22743819362443427
  batch 47 loss: 0.2279495340078435
  batch 48 loss: 0.22708077697704235
  batch 49 loss: 0.22675961408079887
  batch 50 loss: 0.22683616548776628
  batch 51 loss: 0.22728223923374624
  batch 52 loss: 0.2274408515256185
  batch 53 loss: 0.22734846624563326
  batch 54 loss: 0.22752786538115255
  batch 55 loss: 0.2272329945455898
  batch 56 loss: 0.22732760570943356
  batch 57 loss: 0.22710112599950089
  batch 58 loss: 0.22776628028729867
  batch 59 loss: 0.22809147556959572
  batch 60 loss: 0.2285083514948686
  batch 61 loss: 0.22858786045527849
  batch 62 loss: 0.2298316248962956
  batch 63 loss: 0.2298327275211849
  batch 64 loss: 0.23081053118221462
  batch 65 loss: 0.23077019980320565
  batch 66 loss: 0.23157352863839178
  batch 67 loss: 0.23275301994672462
  batch 68 loss: 0.23280472286483822
  batch 69 loss: 0.23287645435851553
  batch 70 loss: 0.23317400408642633
  batch 71 loss: 0.23320329399176046
  batch 72 loss: 0.23346950734655061
  batch 73 loss: 0.233562220041066
  batch 74 loss: 0.23341652670422117
  batch 75 loss: 0.2332384145259857
  batch 76 loss: 0.23403789848089218
  batch 77 loss: 0.23366857058816143
  batch 78 loss: 0.2338668874058968
  batch 79 loss: 0.23400793347177626
  batch 80 loss: 0.23347720317542553
  batch 81 loss: 0.23332865738574368
  batch 82 loss: 0.23373310086203786
  batch 83 loss: 0.233596123306148
  batch 84 loss: 0.23361491819932348
  batch 85 loss: 0.23344769144759459
  batch 86 loss: 0.23372721273538677
  batch 87 loss: 0.23402754996699848
  batch 88 loss: 0.2340819232843139
  batch 89 loss: 0.23397246070122452
  batch 90 loss: 0.23373946514394547
  batch 91 loss: 0.23368271448455014
  batch 92 loss: 0.23373143131966176
  batch 93 loss: 0.23401889413274746
  batch 94 loss: 0.23407863476809035
  batch 95 loss: 0.23378281389412128
  batch 96 loss: 0.23396065660441914
  batch 97 loss: 0.2338715894627817
  batch 98 loss: 0.2340268188593339
  batch 99 loss: 0.23453312781121996
  batch 100 loss: 0.23438837140798569
  batch 101 loss: 0.23424914051400553
  batch 102 loss: 0.23468383810683793
  batch 103 loss: 0.2350557826965758
  batch 104 loss: 0.2352317194812573
  batch 105 loss: 0.23515271487690154
  batch 106 loss: 0.2356638880270832
  batch 107 loss: 0.23540303194634268
  batch 108 loss: 0.235409379557327
  batch 109 loss: 0.2355966357462997
  batch 110 loss: 0.23631216043775732
  batch 111 loss: 0.23659654055629764
  batch 112 loss: 0.2365525093461786
  batch 113 loss: 0.2363649394396132
  batch 114 loss: 0.23658661348255058
  batch 115 loss: 0.23667759066042693
  batch 116 loss: 0.23700603508743748
  batch 117 loss: 0.23700928114927733
  batch 118 loss: 0.236737861092818
  batch 119 loss: 0.2367265241987565
  batch 120 loss: 0.23625307939946652
  batch 121 loss: 0.236019244494517
  batch 122 loss: 0.23587847110189375
  batch 123 loss: 0.23566831321251103
  batch 124 loss: 0.23589435388003627
  batch 125 loss: 0.23581767892837524
  batch 126 loss: 0.23576489067266856
  batch 127 loss: 0.2359087575139023
  batch 128 loss: 0.23554628645069897
  batch 129 loss: 0.23580151103263677
  batch 130 loss: 0.2357020743764364
  batch 131 loss: 0.23564815532615166
  batch 132 loss: 0.2353788274481441
  batch 133 loss: 0.23552774923636502
  batch 134 loss: 0.23552595379192437
  batch 135 loss: 0.23514593177371554
  batch 136 loss: 0.23512916387442281
  batch 137 loss: 0.23505212279566884
  batch 138 loss: 0.23506366515505142
  batch 139 loss: 0.23520440656504185
  batch 140 loss: 0.2350878751703671
  batch 141 loss: 0.23531048898155807
  batch 142 loss: 0.23519969395768475
  batch 143 loss: 0.23485471151925466
  batch 144 loss: 0.23480973785950077
  batch 145 loss: 0.23473608134121732
  batch 146 loss: 0.23452751797764268
  batch 147 loss: 0.23471850633215743
  batch 148 loss: 0.23470957154357755
  batch 149 loss: 0.23445246173631426
  batch 150 loss: 0.23428280850251515
  batch 151 loss: 0.23422081798117683
  batch 152 loss: 0.23441941879297556
  batch 153 loss: 0.23420510913421905
  batch 154 loss: 0.23424715955149045
  batch 155 loss: 0.23405131678427418
  batch 156 loss: 0.23393806366202158
  batch 157 loss: 0.2340947557596644
  batch 158 loss: 0.23397023862675775
  batch 159 loss: 0.23389753155738302
  batch 160 loss: 0.23365435302257537
  batch 161 loss: 0.23374083304997556
  batch 162 loss: 0.23361376525811206
  batch 163 loss: 0.2335832538597423
  batch 164 loss: 0.23329640497885099
  batch 165 loss: 0.2332571520949855
  batch 166 loss: 0.23316068767783155
  batch 167 loss: 0.23300878962356888
  batch 168 loss: 0.23281345888972282
  batch 169 loss: 0.2326900317118718
  batch 170 loss: 0.23245741865214178
  batch 171 loss: 0.23240171625600223
  batch 172 loss: 0.23231845401054205
  batch 173 loss: 0.23225223208438453
  batch 174 loss: 0.23229639883013978
  batch 175 loss: 0.2322809110369001
  batch 176 loss: 0.23205991995266892
  batch 177 loss: 0.23197435246685805
  batch 178 loss: 0.2321396293432525
  batch 179 loss: 0.23223545955879063
  batch 180 loss: 0.23210226918260257
  batch 181 loss: 0.2322286030699535
  batch 182 loss: 0.23219235118601347
  batch 183 loss: 0.23208628285452315
  batch 184 loss: 0.23200761972238187
  batch 185 loss: 0.23206292261948458
  batch 186 loss: 0.23219876840550413
  batch 187 loss: 0.23207499206384874
  batch 188 loss: 0.2316876361344723
  batch 189 loss: 0.23134919811808874
  batch 190 loss: 0.2312668767414595
  batch 191 loss: 0.23118734781030584
  batch 192 loss: 0.23106147962001464
  batch 193 loss: 0.23078932562949125
  batch 194 loss: 0.23095089358460044
  batch 195 loss: 0.2311415582895279
  batch 196 loss: 0.23100320690748644
  batch 197 loss: 0.23098909945657412
  batch 198 loss: 0.2312127983931339
  batch 199 loss: 0.2309809719497834
  batch 200 loss: 0.23109826296567917
  batch 201 loss: 0.23114660604676204
  batch 202 loss: 0.23120937902148408
  batch 203 loss: 0.23123412061794638
  batch 204 loss: 0.2311336195059851
  batch 205 loss: 0.2313261017566774
  batch 206 loss: 0.23139482483412455
  batch 207 loss: 0.23153242015320322
  batch 208 loss: 0.23158735373558906
  batch 209 loss: 0.23137179594576074
  batch 210 loss: 0.23140704213153748
  batch 211 loss: 0.23132003773162715
  batch 212 loss: 0.23116757059996984
  batch 213 loss: 0.23102761791065826
  batch 214 loss: 0.23110436216414532
  batch 215 loss: 0.23092448746049127
  batch 216 loss: 0.23077693864427232
  batch 217 loss: 0.23059396226010564
  batch 218 loss: 0.23053190235151064
  batch 219 loss: 0.2306441347620803
  batch 220 loss: 0.23061510947617617
  batch 221 loss: 0.23072495031680457
  batch 222 loss: 0.23070501972426166
  batch 223 loss: 0.23058612277154966
  batch 224 loss: 0.23055904132447072
  batch 225 loss: 0.23047377275096045
  batch 226 loss: 0.23051959737739733
  batch 227 loss: 0.23045728910336935
  batch 228 loss: 0.2303550334876044
  batch 229 loss: 0.23034654772437815
  batch 230 loss: 0.23033634981383447
  batch 231 loss: 0.23020022249583041
  batch 232 loss: 0.23007878102362156
  batch 233 loss: 0.23001151388039404
  batch 234 loss: 0.2299415032323609
  batch 235 loss: 0.22983514837762142
  batch 236 loss: 0.22965342178940773
  batch 237 loss: 0.22960423970524269
  batch 238 loss: 0.22956061776445694
  batch 239 loss: 0.22940192081688837
  batch 240 loss: 0.22935311601807673
  batch 241 loss: 0.2294565510329369
  batch 242 loss: 0.22933386333964087
  batch 243 loss: 0.22937292620968916
  batch 244 loss: 0.22936205144544117
  batch 245 loss: 0.22928695830763604
  batch 246 loss: 0.22930657469887075
  batch 247 loss: 0.22946475530684235
  batch 248 loss: 0.22949548107722112
  batch 249 loss: 0.22939596896669473
  batch 250 loss: 0.2292833204269409
  batch 251 loss: 0.22930163964332337
  batch 252 loss: 0.2291680688541087
  batch 253 loss: 0.22914460065807749
  batch 254 loss: 0.22889570223064873
  batch 255 loss: 0.22892286900211783
  batch 256 loss: 0.2291109201614745
  batch 257 loss: 0.22912452210480136
  batch 258 loss: 0.2290565777541131
  batch 259 loss: 0.22900069956613783
  batch 260 loss: 0.22898999062868264
  batch 261 loss: 0.22896949912624798
  batch 262 loss: 0.22885014236200857
  batch 263 loss: 0.22892552053293802
  batch 264 loss: 0.22882011323941476
  batch 265 loss: 0.2286845473748333
  batch 266 loss: 0.2286612414439818
  batch 267 loss: 0.22870006191819794
  batch 268 loss: 0.22856117367966852
  batch 269 loss: 0.2285202117325205
  batch 270 loss: 0.2287174211056144
  batch 271 loss: 0.22859001594056064
  batch 272 loss: 0.22857112508705435
  batch 273 loss: 0.22854848642707307
  batch 274 loss: 0.2286168812726536
  batch 275 loss: 0.22849472138014706
  batch 276 loss: 0.2283491763839687
  batch 277 loss: 0.2283060688942348
  batch 278 loss: 0.2281819273778003
  batch 279 loss: 0.2282201448542243
  batch 280 loss: 0.22808114056076323
  batch 281 loss: 0.22800821765886084
  batch 282 loss: 0.22797758623640588
  batch 283 loss: 0.22786331482152636
  batch 284 loss: 0.22787484091142535
  batch 285 loss: 0.22774626675405
  batch 286 loss: 0.22773110361157597
  batch 287 loss: 0.22775120747629357
  batch 288 loss: 0.22762996139418748
  batch 289 loss: 0.2277493867185289
  batch 290 loss: 0.22758949758677646
  batch 291 loss: 0.2275942233215083
  batch 292 loss: 0.22763921943021148
  batch 293 loss: 0.22759001628327288
  batch 294 loss: 0.22755126421954355
  batch 295 loss: 0.22752894941022841
  batch 296 loss: 0.2276239163569502
  batch 297 loss: 0.2276041511734728
  batch 298 loss: 0.2275136956232506
  batch 299 loss: 0.22742145147211976
  batch 300 loss: 0.22739794572194416
  batch 301 loss: 0.22745243972322077
  batch 302 loss: 0.22733022978171608
  batch 303 loss: 0.22721220115230423
  batch 304 loss: 0.22716345206687324
  batch 305 loss: 0.22699680152486582
  batch 306 loss: 0.22709293731676986
  batch 307 loss: 0.22710325260698214
  batch 308 loss: 0.2271853613292242
  batch 309 loss: 0.22714925173995565
  batch 310 loss: 0.2270766057795094
  batch 311 loss: 0.2272171196542752
  batch 312 loss: 0.22733033873522893
  batch 313 loss: 0.22738317869151362
  batch 314 loss: 0.22729058986067013
  batch 315 loss: 0.22729563504930528
  batch 316 loss: 0.22725368423175207
  batch 317 loss: 0.2271840893413742
  batch 318 loss: 0.22718803697037246
  batch 319 loss: 0.22712008248675952
  batch 320 loss: 0.22708469359204173
  batch 321 loss: 0.22710709765878423
  batch 322 loss: 0.2270827567355233
  batch 323 loss: 0.22698714627939112
  batch 324 loss: 0.22682330761978656
  batch 325 loss: 0.22679535508155824
  batch 326 loss: 0.22679103051592236
  batch 327 loss: 0.2267721370090403
  batch 328 loss: 0.22654004217829646
  batch 329 loss: 0.22661971977960013
  batch 330 loss: 0.22659167489319137
  batch 331 loss: 0.22655523233003128
  batch 332 loss: 0.22641088346759958
  batch 333 loss: 0.22649796508453987
  batch 334 loss: 0.22638749616767118
  batch 335 loss: 0.22638107560463805
  batch 336 loss: 0.22632728991586537
  batch 337 loss: 0.22633743237494008
  batch 338 loss: 0.22647444449761917
  batch 339 loss: 0.2264711658599454
  batch 340 loss: 0.2265123077613466
  batch 341 loss: 0.2265666449699234
  batch 342 loss: 0.2264808000702607
  batch 343 loss: 0.2265114782937414
  batch 344 loss: 0.22655380330979824
  batch 345 loss: 0.22662819213625313
  batch 346 loss: 0.22652413687437256
  batch 347 loss: 0.22654668754906063
  batch 348 loss: 0.22657586248784228
  batch 349 loss: 0.2265132578532812
  batch 350 loss: 0.22658295835767472
  batch 351 loss: 0.22660229824207448
  batch 352 loss: 0.22677069470625033
  batch 353 loss: 0.22675608415103837
  batch 354 loss: 0.22693148373210498
  batch 355 loss: 0.22689941189658475
  batch 356 loss: 0.22689323286327084
  batch 357 loss: 0.22684348330778234
  batch 358 loss: 0.2268804309254918
  batch 359 loss: 0.22681352126731183
  batch 360 loss: 0.22676257023380864
  batch 361 loss: 0.22680338651684842
  batch 362 loss: 0.22667336447462852
  batch 363 loss: 0.22649040945469512
  batch 364 loss: 0.22630587461721766
  batch 365 loss: 0.22627519442610544
  batch 366 loss: 0.22620205251408404
  batch 367 loss: 0.22615142934004032
  batch 368 loss: 0.22608476213139037
  batch 369 loss: 0.2259902977523442
  batch 370 loss: 0.22583674127991135
  batch 371 loss: 0.22583890603880355
  batch 372 loss: 0.2257525338440813
  batch 373 loss: 0.22563637525402508
  batch 374 loss: 0.22558749933293795
  batch 375 loss: 0.22544172036647797
  batch 376 loss: 0.2254505260906955
  batch 377 loss: 0.2254369173900518
  batch 378 loss: 0.22540443292055182
  batch 379 loss: 0.22534243864717457
  batch 380 loss: 0.22540783376285906
  batch 381 loss: 0.2253682990790665
  batch 382 loss: 0.22531336600555799
  batch 383 loss: 0.22532692707238558
  batch 384 loss: 0.22527838378058127
  batch 385 loss: 0.22535030365764322
  batch 386 loss: 0.22530287307315539
  batch 387 loss: 0.2253716150226519
  batch 388 loss: 0.22537409494986238
  batch 389 loss: 0.2253020297745506
  batch 390 loss: 0.22520575439318632
  batch 391 loss: 0.2253014292863324
  batch 392 loss: 0.22521631275208628
  batch 393 loss: 0.22515184957410847
  batch 394 loss: 0.2251249313883975
  batch 395 loss: 0.22506418164017833
  batch 396 loss: 0.22495838789024739
  batch 397 loss: 0.2249547893964974
  batch 398 loss: 0.22485659949144526
  batch 399 loss: 0.22487890922037282
  batch 400 loss: 0.22493096642196178
  batch 401 loss: 0.22486917265781442
  batch 402 loss: 0.22487427527779963
  batch 403 loss: 0.2249184476382679
  batch 404 loss: 0.2249968576401767
  batch 405 loss: 0.22502634587847156
  batch 406 loss: 0.2250997873729673
  batch 407 loss: 0.22510307430488766
  batch 408 loss: 0.2251604303863703
  batch 409 loss: 0.22516847265233036
  batch 410 loss: 0.22529019193678368
  batch 411 loss: 0.22525138315493173
  batch 412 loss: 0.22523797827872258
  batch 413 loss: 0.22537268582758546
  batch 414 loss: 0.2254058311238957
  batch 415 loss: 0.22536110648189683
  batch 416 loss: 0.2254168474330352
  batch 417 loss: 0.22544945208288783
  batch 418 loss: 0.2254666529560203
  batch 419 loss: 0.22551265469172121
  batch 420 loss: 0.2255023102320376
  batch 421 loss: 0.2254658189918536
  batch 422 loss: 0.22571248550550632
  batch 423 loss: 0.22569006329168947
  batch 424 loss: 0.22560290995774404
  batch 425 loss: 0.22551571116727942
  batch 426 loss: 0.2254940880716127
  batch 427 loss: 0.22548765997975995
  batch 428 loss: 0.22547872138218344
  batch 429 loss: 0.22557389218629378
  batch 430 loss: 0.2256271215718846
  batch 431 loss: 0.22575407608340897
  batch 432 loss: 0.22575549257022362
  batch 433 loss: 0.2258125657159792
  batch 434 loss: 0.22586989217364842
  batch 435 loss: 0.22581868466289565
  batch 436 loss: 0.22581544409104443
  batch 437 loss: 0.22595238153667144
  batch 438 loss: 0.22610226647766757
  batch 439 loss: 0.22608724606471617
  batch 440 loss: 0.22621658617122606
  batch 441 loss: 0.2261965493766629
  batch 442 loss: 0.22614511790183875
  batch 443 loss: 0.22612982453530314
  batch 444 loss: 0.22600475091252242
  batch 445 loss: 0.22604119864742409
  batch 446 loss: 0.22603554933461373
  batch 447 loss: 0.2259720632560568
  batch 448 loss: 0.2260233934835664
  batch 449 loss: 0.2261640519766606
  batch 450 loss: 0.22625588224993812
  batch 451 loss: 0.22626432003308822
  batch 452 loss: 0.22634798459774624
  batch 453 loss: 0.22641082387623146
  batch 454 loss: 0.22643607106502886
  batch 455 loss: 0.226513777918868
  batch 456 loss: 0.2266226629154724
  batch 457 loss: 0.22662039179379362
  batch 458 loss: 0.2266177666824978
  batch 459 loss: 0.2267472806644336
  batch 460 loss: 0.2268952907751436
  batch 461 loss: 0.2268847991871989
  batch 462 loss: 0.22688901617929533
  batch 463 loss: 0.22686758386393602
  batch 464 loss: 0.2269854961798109
  batch 465 loss: 0.22687204329557317
  batch 466 loss: 0.22672769715090166
  batch 467 loss: 0.22677054229078764
  batch 468 loss: 0.226763327470702
  batch 469 loss: 0.22686243826138186
  batch 470 loss: 0.22682935665262507
  batch 471 loss: 0.22688338571799535
  batch 472 loss: 0.22678912854043104
LOSS train 0.22678912854043104 valid 0.29208052158355713
LOSS train 0.22678912854043104 valid 0.2910146415233612
LOSS train 0.22678912854043104 valid 0.29251041014989215
LOSS train 0.22678912854043104 valid 0.28778062015771866
LOSS train 0.22678912854043104 valid 0.28144686818122866
LOSS train 0.22678912854043104 valid 0.2855462779601415
LOSS train 0.22678912854043104 valid 0.2960230920995985
LOSS train 0.22678912854043104 valid 0.29552721977233887
LOSS train 0.22678912854043104 valid 0.29789696799384224
LOSS train 0.22678912854043104 valid 0.29893126487731936
LOSS train 0.22678912854043104 valid 0.2962548516013406
LOSS train 0.22678912854043104 valid 0.29758323977390927
LOSS train 0.22678912854043104 valid 0.29522242683630723
LOSS train 0.22678912854043104 valid 0.2942426289830889
LOSS train 0.22678912854043104 valid 0.28797777195771534
LOSS train 0.22678912854043104 valid 0.2889478737488389
LOSS train 0.22678912854043104 valid 0.29041610482861013
LOSS train 0.22678912854043104 valid 0.29181352340512806
LOSS train 0.22678912854043104 valid 0.2937192799229371
LOSS train 0.22678912854043104 valid 0.2921549417078495
LOSS train 0.22678912854043104 valid 0.29114661685058046
LOSS train 0.22678912854043104 valid 0.2891143336892128
LOSS train 0.22678912854043104 valid 0.2901945366807606
LOSS train 0.22678912854043104 valid 0.28830822246770066
LOSS train 0.22678912854043104 valid 0.28701935827732084
LOSS train 0.22678912854043104 valid 0.2864665589653529
LOSS train 0.22678912854043104 valid 0.28667558950406535
LOSS train 0.22678912854043104 valid 0.2867763005197048
LOSS train 0.22678912854043104 valid 0.28624807092650184
LOSS train 0.22678912854043104 valid 0.28778210331996285
LOSS train 0.22678912854043104 valid 0.2901356734575764
LOSS train 0.22678912854043104 valid 0.2898137136362493
LOSS train 0.22678912854043104 valid 0.29063685057741223
LOSS train 0.22678912854043104 valid 0.2909988441011485
LOSS train 0.22678912854043104 valid 0.29295305056231363
LOSS train 0.22678912854043104 valid 0.29304857759012115
LOSS train 0.22678912854043104 valid 0.29337924275849314
LOSS train 0.22678912854043104 valid 0.2947849697972599
LOSS train 0.22678912854043104 valid 0.2946362415185341
LOSS train 0.22678912854043104 valid 0.2944835390895605
LOSS train 0.22678912854043104 valid 0.2957254164829487
LOSS train 0.22678912854043104 valid 0.29617576478492647
LOSS train 0.22678912854043104 valid 0.29524325596731765
LOSS train 0.22678912854043104 valid 0.2953241369263692
LOSS train 0.22678912854043104 valid 0.29476194348600177
LOSS train 0.22678912854043104 valid 0.295813155239043
LOSS train 0.22678912854043104 valid 0.29698726693366434
LOSS train 0.22678912854043104 valid 0.2970098378136754
LOSS train 0.22678912854043104 valid 0.29709159750111247
LOSS train 0.22678912854043104 valid 0.2960124522447586
LOSS train 0.22678912854043104 valid 0.2958468975974064
LOSS train 0.22678912854043104 valid 0.29570702004891175
LOSS train 0.22678912854043104 valid 0.2959742248058319
LOSS train 0.22678912854043104 valid 0.2961357865068648
LOSS train 0.22678912854043104 valid 0.2962086688388478
LOSS train 0.22678912854043104 valid 0.2954312488436699
LOSS train 0.22678912854043104 valid 0.2948341035006339
LOSS train 0.22678912854043104 valid 0.2941733881317336
LOSS train 0.22678912854043104 valid 0.29483623221769173
LOSS train 0.22678912854043104 valid 0.2948462352156639
LOSS train 0.22678912854043104 valid 0.2942762619159261
LOSS train 0.22678912854043104 valid 0.29511362170019456
LOSS train 0.22678912854043104 valid 0.29534865513680475
LOSS train 0.22678912854043104 valid 0.29672905895859003
LOSS train 0.22678912854043104 valid 0.2972839309619023
LOSS train 0.22678912854043104 valid 0.296989639600118
LOSS train 0.22678912854043104 valid 0.29610523559264285
LOSS train 0.22678912854043104 valid 0.2963961023179924
LOSS train 0.22678912854043104 valid 0.295583911779998
LOSS train 0.22678912854043104 valid 0.29609371657882416
LOSS train 0.22678912854043104 valid 0.2957056225605414
LOSS train 0.22678912854043104 valid 0.29613972558743423
LOSS train 0.22678912854043104 valid 0.29585455194728016
LOSS train 0.22678912854043104 valid 0.2955972203934515
LOSS train 0.22678912854043104 valid 0.2956555638710658
LOSS train 0.22678912854043104 valid 0.2958510637675461
LOSS train 0.22678912854043104 valid 0.2958670771354205
LOSS train 0.22678912854043104 valid 0.29664625743260753
LOSS train 0.22678912854043104 valid 0.2970767935997323
LOSS train 0.22678912854043104 valid 0.29622655510902407
LOSS train 0.22678912854043104 valid 0.29510210233706013
LOSS train 0.22678912854043104 valid 0.2956766251747201
LOSS train 0.22678912854043104 valid 0.2955967171723584
LOSS train 0.22678912854043104 valid 0.29563212306016967
LOSS train 0.22678912854043104 valid 0.29526436206172496
LOSS train 0.22678912854043104 valid 0.29461206963589026
LOSS train 0.22678912854043104 valid 0.29433394728720874
LOSS train 0.22678912854043104 valid 0.29367484541779215
LOSS train 0.22678912854043104 valid 0.294235200527009
LOSS train 0.22678912854043104 valid 0.2946458261873987
LOSS train 0.22678912854043104 valid 0.2946834552746553
LOSS train 0.22678912854043104 valid 0.2950044013559818
LOSS train 0.22678912854043104 valid 0.2948394887229448
LOSS train 0.22678912854043104 valid 0.2951000160042276
LOSS train 0.22678912854043104 valid 0.29489738862765463
LOSS train 0.22678912854043104 valid 0.29552310130869347
LOSS train 0.22678912854043104 valid 0.2955792222133617
LOSS train 0.22678912854043104 valid 0.2957680716502423
LOSS train 0.22678912854043104 valid 0.2960998357245416
LOSS train 0.22678912854043104 valid 0.29621348187327384
LOSS train 0.22678912854043104 valid 0.2965269854458252
LOSS train 0.22678912854043104 valid 0.2966434404838319
LOSS train 0.22678912854043104 valid 0.2967137904132454
LOSS train 0.22678912854043104 valid 0.29681351967155933
LOSS train 0.22678912854043104 valid 0.29670257525784627
LOSS train 0.22678912854043104 valid 0.2970463938589366
LOSS train 0.22678912854043104 valid 0.29694286699049943
LOSS train 0.22678912854043104 valid 0.2970083845710313
LOSS train 0.22678912854043104 valid 0.29738830084647605
LOSS train 0.22678912854043104 valid 0.2977013884620233
LOSS train 0.22678912854043104 valid 0.2971973549406808
LOSS train 0.22678912854043104 valid 0.2970846743722047
LOSS train 0.22678912854043104 valid 0.29689922203532365
LOSS train 0.22678912854043104 valid 0.2966971113754992
LOSS train 0.22678912854043104 valid 0.29684979824916174
LOSS train 0.22678912854043104 valid 0.29668567419565955
LOSS train 0.22678912854043104 valid 0.2967037422280026
LOSS train 0.22678912854043104 valid 0.2965153782802113
LOSS train 0.22678912854043104 valid 0.2963064679829012
LOSS train 0.22678912854043104 valid 0.2960017843792836
LOSS train 0.22678912854043104 valid 0.2959839749681063
LOSS train 0.22678912854043104 valid 0.29565881546892103
LOSS train 0.22678912854043104 valid 0.2955622109698086
LOSS train 0.22678912854043104 valid 0.29611681053234684
LOSS train 0.22678912854043104 valid 0.2961248162984848
LOSS train 0.22678912854043104 valid 0.2962874884879778
LOSS train 0.22678912854043104 valid 0.2963469874436461
LOSS train 0.22678912854043104 valid 0.29681255703326315
LOSS train 0.22678912854043104 valid 0.2969321251608605
LOSS train 0.22678912854043104 valid 0.2967834249138832
LOSS train 0.22678912854043104 valid 0.296789824621368
LOSS train 0.22678912854043104 valid 0.2965691996117433
LOSS train 0.22678912854043104 valid 0.2966457297255222
LOSS train 0.22678912854043104 valid 0.2967983939087213
LOSS train 0.22678912854043104 valid 0.2966004112252483
LOSS train 0.22678912854043104 valid 0.29637868472320195
LOSS train 0.22678912854043104 valid 0.2961551267517744
LOSS train 0.22678912854043104 valid 0.2961574398737023
LOSS train 0.22678912854043104 valid 0.29609966031510193
LOSS train 0.22678912854043104 valid 0.29617584656391827
LOSS train 0.22678912854043104 valid 0.29624477694643303
LOSS train 0.22678912854043104 valid 0.2966028324944872
LOSS train 0.22678912854043104 valid 0.2964976887394498
LOSS train 0.22678912854043104 valid 0.2963081466861897
LOSS train 0.22678912854043104 valid 0.29609433073422003
LOSS train 0.22678912854043104 valid 0.29624569487490066
LOSS train 0.22678912854043104 valid 0.2961237942483149
LOSS train 0.22678912854043104 valid 0.29720652717593554
LOSS train 0.22678912854043104 valid 0.29736003729721044
LOSS train 0.22678912854043104 valid 0.29763738761345543
LOSS train 0.22678912854043104 valid 0.2975602877258465
LOSS train 0.22678912854043104 valid 0.29703951901511144
LOSS train 0.22678912854043104 valid 0.29719064165564146
LOSS train 0.22678912854043104 valid 0.29709937278326454
LOSS train 0.22678912854043104 valid 0.2970962818591825
LOSS train 0.22678912854043104 valid 0.29724398943094105
LOSS train 0.22678912854043104 valid 0.29708300768190127
LOSS train 0.22678912854043104 valid 0.2971361815174924
LOSS train 0.22678912854043104 valid 0.2971241226736105
LOSS train 0.22678912854043104 valid 0.29711561743170023
LOSS train 0.22678912854043104 valid 0.2970298445372848
LOSS train 0.22678912854043104 valid 0.29700424880893145
LOSS train 0.22678912854043104 valid 0.2966915030786596
LOSS train 0.22678912854043104 valid 0.29620444865488427
LOSS train 0.22678912854043104 valid 0.2958361012466026
LOSS train 0.22678912854043104 valid 0.2958954465856035
LOSS train 0.22678912854043104 valid 0.29625763802114363
LOSS train 0.22678912854043104 valid 0.2959967801081283
LOSS train 0.22678912854043104 valid 0.2962041602155866
LOSS train 0.22678912854043104 valid 0.29633358287460665
LOSS train 0.22678912854043104 valid 0.29633631422157175
LOSS train 0.22678912854043104 valid 0.2960140439140242
LOSS train 0.22678912854043104 valid 0.2960185945206295
LOSS train 0.22678912854043104 valid 0.2960391244155237
LOSS train 0.22678912854043104 valid 0.29579457019056593
LOSS train 0.22678912854043104 valid 0.29578740716996516
LOSS train 0.22678912854043104 valid 0.29569004627607637
LOSS train 0.22678912854043104 valid 0.2959249919552482
LOSS train 0.22678912854043104 valid 0.2956786064961769
LOSS train 0.22678912854043104 valid 0.29549303725361825
LOSS train 0.22678912854043104 valid 0.29561035236271704
LOSS train 0.22678912854043104 valid 0.29581709345290946
LOSS train 0.22678912854043104 valid 0.29590459558807436
LOSS train 0.22678912854043104 valid 0.29591383728320186
LOSS train 0.22678912854043104 valid 0.2955977804757453
LOSS train 0.22678912854043104 valid 0.2956171360227369
LOSS train 0.22678912854043104 valid 0.2954265238607631
LOSS train 0.22678912854043104 valid 0.2955535372045446
LOSS train 0.22678912854043104 valid 0.29542870583042263
LOSS train 0.22678912854043104 valid 0.2955178332171942
LOSS train 0.22678912854043104 valid 0.2953145131858856
LOSS train 0.22678912854043104 valid 0.29535497065323096
LOSS train 0.22678912854043104 valid 0.2952408442379897
LOSS train 0.22678912854043104 valid 0.2950671835197616
LOSS train 0.22678912854043104 valid 0.2947884691067231
LOSS train 0.22678912854043104 valid 0.2947246704478653
LOSS train 0.22678912854043104 valid 0.29491096023980734
LOSS train 0.22678912854043104 valid 0.2945946643147806
LOSS train 0.22678912854043104 valid 0.2946629417901063
LOSS train 0.22678912854043104 valid 0.29459006696939466
LOSS train 0.22678912854043104 valid 0.2943735164196337
LOSS train 0.22678912854043104 valid 0.29418998573086047
LOSS train 0.22678912854043104 valid 0.2940131856009291
LOSS train 0.22678912854043104 valid 0.29396555923363743
LOSS train 0.22678912854043104 valid 0.29360685094100675
LOSS train 0.22678912854043104 valid 0.29360545498942864
LOSS train 0.22678912854043104 valid 0.2935550998230487
LOSS train 0.22678912854043104 valid 0.2932938077272131
LOSS train 0.22678912854043104 valid 0.29316254787182694
LOSS train 0.22678912854043104 valid 0.2932132561291967
LOSS train 0.22678912854043104 valid 0.29341770680312296
LOSS train 0.22678912854043104 valid 0.29344793050637785
LOSS train 0.22678912854043104 valid 0.29369280127012676
LOSS train 0.22678912854043104 valid 0.2936807513654789
LOSS train 0.22678912854043104 valid 0.29347258189389874
LOSS train 0.22678912854043104 valid 0.2934490107689743
LOSS train 0.22678912854043104 valid 0.29347487865230454
LOSS train 0.22678912854043104 valid 0.29352218001534086
LOSS train 0.22678912854043104 valid 0.29366993951743053
LOSS train 0.22678912854043104 valid 0.29380511797287245
LOSS train 0.22678912854043104 valid 0.2937923572047264
LOSS train 0.22678912854043104 valid 0.2937324210195928
LOSS train 0.22678912854043104 valid 0.2938846792741741
LOSS train 0.22678912854043104 valid 0.2939574874804488
LOSS train 0.22678912854043104 valid 0.2938866302039888
LOSS train 0.22678912854043104 valid 0.2939341435522105
LOSS train 0.22678912854043104 valid 0.2942226671568623
LOSS train 0.22678912854043104 valid 0.2942031418675916
LOSS train 0.22678912854043104 valid 0.29425412331866385
LOSS train 0.22678912854043104 valid 0.2943680185986602
LOSS train 0.22678912854043104 valid 0.2944155299947375
LOSS train 0.22678912854043104 valid 0.29443601229838257
LOSS train 0.22678912854043104 valid 0.2943891077722091
LOSS train 0.22678912854043104 valid 0.2943515872471353
LOSS train 0.22678912854043104 valid 0.29469583091583657
LOSS train 0.22678912854043104 valid 0.29445250768025044
LOSS train 0.22678912854043104 valid 0.29460265207642744
LOSS train 0.22678912854043104 valid 0.2946883069993067
LOSS train 0.22678912854043104 valid 0.2945572159654426
LOSS train 0.22678912854043104 valid 0.29451325417806706
LOSS train 0.22678912854043104 valid 0.2945590395774089
LOSS train 0.22678912854043104 valid 0.2942504419768152
LOSS train 0.22678912854043104 valid 0.2944777486500917
LOSS train 0.22678912854043104 valid 0.2949021106860677
LOSS train 0.22678912854043104 valid 0.2952033916298224
LOSS train 0.22678912854043104 valid 0.2952027970213231
LOSS train 0.22678912854043104 valid 0.2950925740153201
LOSS train 0.22678912854043104 valid 0.295026573682985
LOSS train 0.22678912854043104 valid 0.29490913935454494
LOSS train 0.22678912854043104 valid 0.29517320621013643
LOSS train 0.22678912854043104 valid 0.29529631838380577
LOSS train 0.22678912854043104 valid 0.29546729416128187
LOSS train 0.22678912854043104 valid 0.2954703689563887
LOSS train 0.22678912854043104 valid 0.2953340226975013
LOSS train 0.22678912854043104 valid 0.29533759075052596
LOSS train 0.22678912854043104 valid 0.2954240085091442
LOSS train 0.22678912854043104 valid 0.29527956154559837
LOSS train 0.22678912854043104 valid 0.29535076888494716
LOSS train 0.22678912854043104 valid 0.2953682602832676
LOSS train 0.22678912854043104 valid 0.2953200461772772
LOSS train 0.22678912854043104 valid 0.2954803506533305
LOSS train 0.22678912854043104 valid 0.2955636311578387
LOSS train 0.22678912854043104 valid 0.29561031272656113
LOSS train 0.22678912854043104 valid 0.2956015732929562
LOSS train 0.22678912854043104 valid 0.29551793887930095
LOSS train 0.22678912854043104 valid 0.29563249546782416
LOSS train 0.22678912854043104 valid 0.29570796996466675
LOSS train 0.22678912854043104 valid 0.2958778068423271
LOSS train 0.22678912854043104 valid 0.29584656129539233
LOSS train 0.22678912854043104 valid 0.2957694817472387
LOSS train 0.22678912854043104 valid 0.29589520367309174
LOSS train 0.22678912854043104 valid 0.29613625554039197
LOSS train 0.22678912854043104 valid 0.2962041421905979
LOSS train 0.22678912854043104 valid 0.29615657249071303
LOSS train 0.22678912854043104 valid 0.2960153555870056
LOSS train 0.22678912854043104 valid 0.29583633928627207
LOSS train 0.22678912854043104 valid 0.2957040925748942
LOSS train 0.22678912854043104 valid 0.29554874734055225
LOSS train 0.22678912854043104 valid 0.29554628941320604
LOSS train 0.22678912854043104 valid 0.29549721351691655
LOSS train 0.22678912854043104 valid 0.2953357597899182
LOSS train 0.22678912854043104 valid 0.29511607174120896
LOSS train 0.22678912854043104 valid 0.29497721280520883
LOSS train 0.22678912854043104 valid 0.29503394364261293
LOSS train 0.22678912854043104 valid 0.2951665185522615
LOSS train 0.22678912854043104 valid 0.29514607886125993
LOSS train 0.22678912854043104 valid 0.295088331510381
LOSS train 0.22678912854043104 valid 0.295061023440212
LOSS train 0.22678912854043104 valid 0.29514707408355595
LOSS train 0.22678912854043104 valid 0.2951376878495874
LOSS train 0.22678912854043104 valid 0.2950661334487581
LOSS train 0.22678912854043104 valid 0.29518067954133637
LOSS train 0.22678912854043104 valid 0.29515759158663374
LOSS train 0.22678912854043104 valid 0.2953225652478179
LOSS train 0.22678912854043104 valid 0.2953847154217251
LOSS train 0.22678912854043104 valid 0.29526412612884434
LOSS train 0.22678912854043104 valid 0.2952988349929803
LOSS train 0.22678912854043104 valid 0.2952526320247042
LOSS train 0.22678912854043104 valid 0.2952205091714859
LOSS train 0.22678912854043104 valid 0.2951349021494389
LOSS train 0.22678912854043104 valid 0.2951787502761695
LOSS train 0.22678912854043104 valid 0.2952030886778768
LOSS train 0.22678912854043104 valid 0.2953274718704003
LOSS train 0.22678912854043104 valid 0.2952942294803889
LOSS train 0.22678912854043104 valid 0.2952199395074219
LOSS train 0.22678912854043104 valid 0.2951758621272698
LOSS train 0.22678912854043104 valid 0.29500690427781706
LOSS train 0.22678912854043104 valid 0.2949201326768894
LOSS train 0.22678912854043104 valid 0.29490605937045755
LOSS train 0.22678912854043104 valid 0.294988113112988
LOSS train 0.22678912854043104 valid 0.294898475625124
LOSS train 0.22678912854043104 valid 0.2948677980651458
LOSS train 0.22678912854043104 valid 0.2947979739394051
LOSS train 0.22678912854043104 valid 0.29487592556104536
LOSS train 0.22678912854043104 valid 0.2948115792066332
LOSS train 0.22678912854043104 valid 0.2946979519970055
LOSS train 0.22678912854043104 valid 0.2947719468961379
LOSS train 0.22678912854043104 valid 0.2948807235690033
LOSS train 0.22678912854043104 valid 0.295037216106926
LOSS train 0.22678912854043104 valid 0.2949896364007145
LOSS train 0.22678912854043104 valid 0.29526440251467756
LOSS train 0.22678912854043104 valid 0.29513504633807247
LOSS train 0.22678912854043104 valid 0.2950162451610477
LOSS train 0.22678912854043104 valid 0.29506568771637515
LOSS train 0.22678912854043104 valid 0.29517102090212016
LOSS train 0.22678912854043104 valid 0.2952568214272429
LOSS train 0.22678912854043104 valid 0.2952875135415191
LOSS train 0.22678912854043104 valid 0.2952944651336932
LOSS train 0.22678912854043104 valid 0.29534104407558326
LOSS train 0.22678912854043104 valid 0.29535202406572575
LOSS train 0.22678912854043104 valid 0.29523903226024073
LOSS train 0.22678912854043104 valid 0.2951450105980936
LOSS train 0.22678912854043104 valid 0.2951775274208716
LOSS train 0.22678912854043104 valid 0.29532426457383676
LOSS train 0.22678912854043104 valid 0.2953076097947448
LOSS train 0.22678912854043104 valid 0.29511214495592175
LOSS train 0.22678912854043104 valid 0.29510394609823426
LOSS train 0.22678912854043104 valid 0.2950385074908211
LOSS train 0.22678912854043104 valid 0.2949332998829254
LOSS train 0.22678912854043104 valid 0.29494498069672026
LOSS train 0.22678912854043104 valid 0.2948102590100856
LOSS train 0.22678912854043104 valid 0.2948275013276708
LOSS train 0.22678912854043104 valid 0.29476191689947256
LOSS train 0.22678912854043104 valid 0.29493097614410313
LOSS train 0.22678912854043104 valid 0.29496974755024563
LOSS train 0.22678912854043104 valid 0.29488772179694533
LOSS train 0.22678912854043104 valid 0.2946817736632542
LOSS train 0.22678912854043104 valid 0.2945806559646267
LOSS train 0.22678912854043104 valid 0.29463817320102265
LOSS train 0.22678912854043104 valid 0.2945518403393882
LOSS train 0.22678912854043104 valid 0.2945056280009767
LOSS train 0.22678912854043104 valid 0.29443736492910166
LOSS train 0.22678912854043104 valid 0.2943979755846029
LOSS train 0.22678912854043104 valid 0.2944389633034582
LOSS train 0.22678912854043104 valid 0.29462001651105746
LOSS train 0.22678912854043104 valid 0.29475419365623023
LOSS train 0.22678912854043104 valid 0.2947700879320043
LOSS train 0.22678912854043104 valid 0.2947074630240488
LOSS train 0.22678912854043104 valid 0.2947017860113744
LOSS train 0.22678912854043104 valid 0.294695419487026
LOSS train 0.22678912854043104 valid 0.294729359757537
LOSS train 0.22678912854043104 valid 0.29487060669048054
LOSS train 0.22678912854043104 valid 0.2947139988342921
LOSS train 0.22678912854043104 valid 0.29467276299556533
LOSS train 0.22678912854043104 valid 0.29472393675209724
LOSS train 0.22678912854043104 valid 0.29469333756002575
LOSS train 0.22678912854043104 valid 0.29455000779284446
LOSS train 0.22678912854043104 valid 0.29451412707567215
LOSS train 0.22678912854043104 valid 0.2946143713262346
EPOCH 21:
  batch 1 loss: 0.2344275414943695
  batch 2 loss: 0.22049884498119354
  batch 3 loss: 0.22478696207205454
  batch 4 loss: 0.22919289395213127
  batch 5 loss: 0.23526957333087922
  batch 6 loss: 0.23650877674420676
  batch 7 loss: 0.2406578745160784
  batch 8 loss: 0.2472686842083931
  batch 9 loss: 0.24639339910613167
  batch 10 loss: 0.2455984264612198
  batch 11 loss: 0.24504400925202804
  batch 12 loss: 0.24275368576248488
  batch 13 loss: 0.2392801310007389
  batch 14 loss: 0.23854354981865203
  batch 15 loss: 0.23883750637372334
  batch 16 loss: 0.2382234837859869
  batch 17 loss: 0.2345503691364737
  batch 18 loss: 0.23544901940557691
  batch 19 loss: 0.23265572049115835
  batch 20 loss: 0.23137178346514703
  batch 21 loss: 0.23126574357350668
  batch 22 loss: 0.23093818873167038
  batch 23 loss: 0.23037773241167483
  batch 24 loss: 0.2278070996205012
  batch 25 loss: 0.23013513326644897
  batch 26 loss: 0.22800011015855348
  batch 27 loss: 0.22813609959902587
  batch 28 loss: 0.22716518225414412
  batch 29 loss: 0.22770577103927217
  batch 30 loss: 0.2269299730658531
  batch 31 loss: 0.22711333920878748
  batch 32 loss: 0.22691051103174686
  batch 33 loss: 0.22703553826519937
  batch 34 loss: 0.22616650383262074
  batch 35 loss: 0.2264189200741904
  batch 36 loss: 0.22697855449385113
  batch 37 loss: 0.22641286253929138
  batch 38 loss: 0.22652065910791097
  batch 39 loss: 0.2263380155349389
  batch 40 loss: 0.22651702873408794
  batch 41 loss: 0.22674122525424492
  batch 42 loss: 0.22651970989647366
  batch 43 loss: 0.22698142812695615
  batch 44 loss: 0.22689574110237035
  batch 45 loss: 0.22653730180528428
  batch 46 loss: 0.22662536475969397
  batch 47 loss: 0.2271459343585562
  batch 48 loss: 0.2263072927792867
  batch 49 loss: 0.22613334899045984
  batch 50 loss: 0.22594107508659364
  batch 51 loss: 0.22625450117915286
  batch 52 loss: 0.22651865247350472
  batch 53 loss: 0.2264932191596841
  batch 54 loss: 0.22686466381505685
  batch 55 loss: 0.22660568817095322
  batch 56 loss: 0.2264820643301521
  batch 57 loss: 0.2262720442131946
  batch 58 loss: 0.22693059079605957
  batch 59 loss: 0.2270995873515889
  batch 60 loss: 0.2272223855058352
  batch 61 loss: 0.22750394876863136
  batch 62 loss: 0.22867560410691845
  batch 63 loss: 0.22859131556654733
  batch 64 loss: 0.22932757367379963
  batch 65 loss: 0.22935039767852197
  batch 66 loss: 0.2296980655554569
  batch 67 loss: 0.23114233986655278
  batch 68 loss: 0.23129810984520352
  batch 69 loss: 0.231551805700081
  batch 70 loss: 0.23195014085088458
  batch 71 loss: 0.23203101754188538
  batch 72 loss: 0.23248265518082512
  batch 73 loss: 0.23282200179687917
  batch 74 loss: 0.23252544612497897
  batch 75 loss: 0.2322463208436966
  batch 76 loss: 0.2330636019377332
  batch 77 loss: 0.23279661617495798
  batch 78 loss: 0.23296377540398866
  batch 79 loss: 0.23327848296376724
  batch 80 loss: 0.2331947697326541
  batch 81 loss: 0.2329836815227697
  batch 82 loss: 0.2335564566821587
  batch 83 loss: 0.23338436021144132
  batch 84 loss: 0.23364524206235296
  batch 85 loss: 0.23366975696647868
  batch 86 loss: 0.23420056475456372
  batch 87 loss: 0.2345833715008593
  batch 88 loss: 0.2347273708067157
  batch 89 loss: 0.23468461565757065
  batch 90 loss: 0.23462117513020833
  batch 91 loss: 0.2344329751782365
  batch 92 loss: 0.23451018284844316
  batch 93 loss: 0.23482777018059967
  batch 94 loss: 0.2349235898953803
  batch 95 loss: 0.2346166624834663
  batch 96 loss: 0.23485965048894286
  batch 97 loss: 0.23491862861766027
  batch 98 loss: 0.23502632838730908
  batch 99 loss: 0.2352604762171254
  batch 100 loss: 0.23500096470117568
  batch 101 loss: 0.23477777528880847
  batch 102 loss: 0.23512737000105427
  batch 103 loss: 0.2353440932567837
  batch 104 loss: 0.23535236816566724
  batch 105 loss: 0.23521573188759032
  batch 106 loss: 0.23580304954974157
  batch 107 loss: 0.23563529314281784
  batch 108 loss: 0.23556863019863764
  batch 109 loss: 0.23557725849501585
  batch 110 loss: 0.2362149485132911
  batch 111 loss: 0.2364560983202479
  batch 112 loss: 0.23637487765933787
  batch 113 loss: 0.23619850204054232
  batch 114 loss: 0.23667351561680175
  batch 115 loss: 0.23691510972769364
  batch 116 loss: 0.23721390731375794
  batch 117 loss: 0.23717405220382234
  batch 118 loss: 0.23695714948541027
  batch 119 loss: 0.23700173061434962
  batch 120 loss: 0.2366540273030599
  batch 121 loss: 0.2363413168379098
  batch 122 loss: 0.23618824755559203
  batch 123 loss: 0.2359474197151215
  batch 124 loss: 0.23614769933685179
  batch 125 loss: 0.23624033713340759
  batch 126 loss: 0.23631560222970116
  batch 127 loss: 0.2364236040143516
  batch 128 loss: 0.23610995814669877
  batch 129 loss: 0.23628388969011085
  batch 130 loss: 0.23618058390342273
  batch 131 loss: 0.2361783495613637
  batch 132 loss: 0.23595178578839157
  batch 133 loss: 0.23627054108712905
  batch 134 loss: 0.23628085518061226
  batch 135 loss: 0.2358754731990673
  batch 136 loss: 0.23588894537704833
  batch 137 loss: 0.2358369115930404
  batch 138 loss: 0.23579992235138797
  batch 139 loss: 0.23595912325725282
  batch 140 loss: 0.23587642801659447
  batch 141 loss: 0.2361437350722915
  batch 142 loss: 0.23597097103024872
  batch 143 loss: 0.23559381609613245
  batch 144 loss: 0.23560499389552408
  batch 145 loss: 0.23549628432454733
  batch 146 loss: 0.2353227896845504
  batch 147 loss: 0.23555480997984102
  batch 148 loss: 0.23558366469837524
  batch 149 loss: 0.23525915609910183
  batch 150 loss: 0.235111802816391
  batch 151 loss: 0.2350401616846489
  batch 152 loss: 0.23517102914813318
  batch 153 loss: 0.23500767437850728
  batch 154 loss: 0.2350283491534072
  batch 155 loss: 0.23492537525392349
  batch 156 loss: 0.23482311918185308
  batch 157 loss: 0.2349360839576478
  batch 158 loss: 0.23474561177854297
  batch 159 loss: 0.23468004204567122
  batch 160 loss: 0.23457217076793313
  batch 161 loss: 0.23459782159846762
  batch 162 loss: 0.23445858852362927
  batch 163 loss: 0.23447518352350574
  batch 164 loss: 0.23411567291108573
  batch 165 loss: 0.2340899116162098
  batch 166 loss: 0.23409363573574157
  batch 167 loss: 0.23393051620728955
  batch 168 loss: 0.23373013601771422
  batch 169 loss: 0.23368156097344392
  batch 170 loss: 0.23352427684208926
  batch 171 loss: 0.23348585128435614
  batch 172 loss: 0.23342585797573245
  batch 173 loss: 0.23320728307859057
  batch 174 loss: 0.2332803261862404
  batch 175 loss: 0.2333160547699247
  batch 176 loss: 0.23302629386836832
  batch 177 loss: 0.2329439703522429
  batch 178 loss: 0.23316023180658899
  batch 179 loss: 0.233291880961237
  batch 180 loss: 0.23312128691209688
  batch 181 loss: 0.23332756765968893
  batch 182 loss: 0.233426913864665
  batch 183 loss: 0.23333131982003405
  batch 184 loss: 0.23319953307509422
  batch 185 loss: 0.23326967785487304
  batch 186 loss: 0.2334615576171106
  batch 187 loss: 0.2333629300250089
  batch 188 loss: 0.23313474766117462
  batch 189 loss: 0.23283075750189483
  batch 190 loss: 0.2328203983997044
  batch 191 loss: 0.23266941199752048
  batch 192 loss: 0.23244471739356717
  batch 193 loss: 0.2322631311540159
  batch 194 loss: 0.23250382370555522
  batch 195 loss: 0.2325408841554935
  batch 196 loss: 0.2324067101794846
  batch 197 loss: 0.23245164857903108
  batch 198 loss: 0.23278504850888493
  batch 199 loss: 0.23255235720519443
  batch 200 loss: 0.23273410618305207
  batch 201 loss: 0.23277713699423852
  batch 202 loss: 0.23284240022744282
  batch 203 loss: 0.23282588908237778
  batch 204 loss: 0.23274407323961163
  batch 205 loss: 0.23289701698756798
  batch 206 loss: 0.23294361029724472
  batch 207 loss: 0.23301054904426355
  batch 208 loss: 0.2330603921929231
  batch 209 loss: 0.23286369528496664
  batch 210 loss: 0.2329486280679703
  batch 211 loss: 0.23276895143409476
  batch 212 loss: 0.232574749316247
  batch 213 loss: 0.23242227984985836
  batch 214 loss: 0.23246322899499786
  batch 215 loss: 0.23234767927679906
  batch 216 loss: 0.2321234306803456
  batch 217 loss: 0.23195305844331118
  batch 218 loss: 0.23187130682785576
  batch 219 loss: 0.23200137613843022
  batch 220 loss: 0.23189673247662457
  batch 221 loss: 0.23202321014253263
  batch 222 loss: 0.23194442843800192
  batch 223 loss: 0.23185308250878423
  batch 224 loss: 0.2317678521254233
  batch 225 loss: 0.23169055898984273
  batch 226 loss: 0.23175134101008946
  batch 227 loss: 0.23171124256129833
  batch 228 loss: 0.2316154119999785
  batch 229 loss: 0.2316060314792733
  batch 230 loss: 0.2317081551189008
  batch 231 loss: 0.23158912405823218
  batch 232 loss: 0.2314655751889122
  batch 233 loss: 0.23145289482476886
  batch 234 loss: 0.2313490447580305
  batch 235 loss: 0.23126115976495945
  batch 236 loss: 0.2311234931319447
  batch 237 loss: 0.23118437568849654
  batch 238 loss: 0.23113218832667135
  batch 239 loss: 0.23102506951557542
  batch 240 loss: 0.2308891142408053
  batch 241 loss: 0.2310233999090076
  batch 242 loss: 0.23090699124188463
  batch 243 loss: 0.23087670252401643
  batch 244 loss: 0.23085334683295156
  batch 245 loss: 0.23080039991407977
  batch 246 loss: 0.2308858121192552
  batch 247 loss: 0.23098744934628368
  batch 248 loss: 0.23101522503120284
  batch 249 loss: 0.23092131915102043
  batch 250 loss: 0.2308987142443657
  batch 251 loss: 0.23089872769625538
  batch 252 loss: 0.23071872738618698
  batch 253 loss: 0.23070108277995596
  batch 254 loss: 0.2305255285280896
  batch 255 loss: 0.23050895362508064
  batch 256 loss: 0.23063442175043747
  batch 257 loss: 0.23068378048184318
  batch 258 loss: 0.23060267285783162
  batch 259 loss: 0.23055716612624386
  batch 260 loss: 0.23050668417261197
  batch 261 loss: 0.23044124239934358
  batch 262 loss: 0.23033845623713414
  batch 263 loss: 0.23037106359639548
  batch 264 loss: 0.2302483810732762
  batch 265 loss: 0.2300502634835693
  batch 266 loss: 0.23010479053832533
  batch 267 loss: 0.23016214052612863
  batch 268 loss: 0.2299820031247922
  batch 269 loss: 0.22990294011330517
  batch 270 loss: 0.23008547210031086
  batch 271 loss: 0.2299916027766752
  batch 272 loss: 0.22995467451127136
  batch 273 loss: 0.2298500289519628
  batch 274 loss: 0.22988635731221985
  batch 275 loss: 0.2298124540394003
  batch 276 loss: 0.22969808753417886
  batch 277 loss: 0.22957992365429117
  batch 278 loss: 0.22938044288818785
  batch 279 loss: 0.22940932102100822
  batch 280 loss: 0.22927192018500397
  batch 281 loss: 0.2291624842483378
  batch 282 loss: 0.22915628610561925
  batch 283 loss: 0.22898442621997725
  batch 284 loss: 0.22897531601115012
  batch 285 loss: 0.22887268442856637
  batch 286 loss: 0.2287869083506244
  batch 287 loss: 0.228760254611537
  batch 288 loss: 0.22865368151623341
  batch 289 loss: 0.22875230901175303
  batch 290 loss: 0.22855551083540093
  batch 291 loss: 0.2285243499627228
  batch 292 loss: 0.22855965180756294
  batch 293 loss: 0.22849135492442005
  batch 294 loss: 0.2283911199188557
  batch 295 loss: 0.22837093290636096
  batch 296 loss: 0.22847203417001544
  batch 297 loss: 0.2284059760847477
  batch 298 loss: 0.228264163454507
  batch 299 loss: 0.228167292764753
  batch 300 loss: 0.22812429830431938
  batch 301 loss: 0.22817728349140712
  batch 302 loss: 0.22806775066631518
  batch 303 loss: 0.2279013510387723
  batch 304 loss: 0.22779210740210196
  batch 305 loss: 0.22758993882624828
  batch 306 loss: 0.2277606295701725
  batch 307 loss: 0.22774789790571318
  batch 308 loss: 0.22780621985529925
  batch 309 loss: 0.2277429086008504
  batch 310 loss: 0.22766170333470068
  batch 311 loss: 0.2277699984442383
  batch 312 loss: 0.2279018437346587
  batch 313 loss: 0.22795194996812473
  batch 314 loss: 0.22785658987274596
  batch 315 loss: 0.22783283865641032
  batch 316 loss: 0.2277810704198819
  batch 317 loss: 0.22771655399152532
  batch 318 loss: 0.22768698538999138
  batch 319 loss: 0.22763604276045737
  batch 320 loss: 0.2275995469186455
  batch 321 loss: 0.22763439437310642
  batch 322 loss: 0.22762920716719598
  batch 323 loss: 0.2275287097913931
  batch 324 loss: 0.22734633856165556
  batch 325 loss: 0.2272754653600546
  batch 326 loss: 0.2272257229850336
  batch 327 loss: 0.2272515926703765
  batch 328 loss: 0.2270311405200784
  batch 329 loss: 0.22710338488538215
  batch 330 loss: 0.22710122664769491
  batch 331 loss: 0.2271008490615741
  batch 332 loss: 0.22701075390340333
  batch 333 loss: 0.22706899829991944
  batch 334 loss: 0.22701121019032186
  batch 335 loss: 0.2270457678766393
  batch 336 loss: 0.22702371479854697
  batch 337 loss: 0.2270027877492084
  batch 338 loss: 0.22708359166715272
  batch 339 loss: 0.2270767684355598
  batch 340 loss: 0.22714149864280925
  batch 341 loss: 0.22723148881165514
  batch 342 loss: 0.2271399624776422
  batch 343 loss: 0.227154221201082
  batch 344 loss: 0.22716865419994953
  batch 345 loss: 0.2272024507107942
  batch 346 loss: 0.22706015778414776
  batch 347 loss: 0.22703550674386258
  batch 348 loss: 0.22700283710641422
  batch 349 loss: 0.22690758053586954
  batch 350 loss: 0.22695443583386285
  batch 351 loss: 0.22691918756717291
  batch 352 loss: 0.22701096056367864
  batch 353 loss: 0.22701362877155498
  batch 354 loss: 0.22725539368256337
  batch 355 loss: 0.22721069782552586
  batch 356 loss: 0.2271778282154812
  batch 357 loss: 0.2271874759878431
  batch 358 loss: 0.22724640981278607
  batch 359 loss: 0.22725031734509057
  batch 360 loss: 0.22716961109803782
  batch 361 loss: 0.2271477637446158
  batch 362 loss: 0.2270240440800045
  batch 363 loss: 0.2269506648223919
  batch 364 loss: 0.22686132583971863
  batch 365 loss: 0.22677633003012776
  batch 366 loss: 0.22670660127218956
  batch 367 loss: 0.2266928236474783
  batch 368 loss: 0.22662050742655993
  batch 369 loss: 0.2265275882350074
  batch 370 loss: 0.22637452994649473
  batch 371 loss: 0.22641754909506384
  batch 372 loss: 0.22638805498999934
  batch 373 loss: 0.2262604296127529
  batch 374 loss: 0.2261692144494643
  batch 375 loss: 0.2260436323483785
  batch 376 loss: 0.22610036644371265
  batch 377 loss: 0.22607887878183977
  batch 378 loss: 0.22610411241098685
  batch 379 loss: 0.22604584980923143
  batch 380 loss: 0.2260872993422182
  batch 381 loss: 0.2260585081937119
  batch 382 loss: 0.22601460029629514
  batch 383 loss: 0.2260211935709413
  batch 384 loss: 0.22598336970744035
  batch 385 loss: 0.22609172672420352
  batch 386 loss: 0.22605024081283281
  batch 387 loss: 0.2261396777799271
  batch 388 loss: 0.22615498120022803
  batch 389 loss: 0.22612256777010725
  batch 390 loss: 0.2260771292906541
  batch 391 loss: 0.22616273987933497
  batch 392 loss: 0.22606612548080027
  batch 393 loss: 0.22598505039099823
  batch 394 loss: 0.2259769280474198
  batch 395 loss: 0.2259099359376521
  batch 396 loss: 0.22584631320352505
  batch 397 loss: 0.2258578924373055
  batch 398 loss: 0.2257700562851513
  batch 399 loss: 0.22581864815009267
  batch 400 loss: 0.22584739834070205
  batch 401 loss: 0.22575106874962994
  batch 402 loss: 0.22575504495878124
  batch 403 loss: 0.2258106637607437
  batch 404 loss: 0.2258566578868592
  batch 405 loss: 0.2258751638509609
  batch 406 loss: 0.22594624996332113
  batch 407 loss: 0.22596298392486808
  batch 408 loss: 0.22598125694283083
  batch 409 loss: 0.2259667226111102
  batch 410 loss: 0.22607990179846926
  batch 411 loss: 0.2260360897602536
  batch 412 loss: 0.22603960385889682
  batch 413 loss: 0.2261400885934114
  batch 414 loss: 0.2262067177877334
  batch 415 loss: 0.2261625447546143
  batch 416 loss: 0.2262114056457694
  batch 417 loss: 0.22620460012262103
  batch 418 loss: 0.22620236153522746
  batch 419 loss: 0.22626418430082554
  batch 420 loss: 0.22620901862780254
  batch 421 loss: 0.22611653461450637
  batch 422 loss: 0.22627690077816706
  batch 423 loss: 0.22623268932315474
  batch 424 loss: 0.2261618879043831
  batch 425 loss: 0.2260602139374789
  batch 426 loss: 0.22604644372009894
  batch 427 loss: 0.2260564964884059
  batch 428 loss: 0.2261898833736081
  batch 429 loss: 0.2262725360465772
  batch 430 loss: 0.22629110865121663
  batch 431 loss: 0.2264752543235323
  batch 432 loss: 0.22657253182734605
  batch 433 loss: 0.22669944556012717
  batch 434 loss: 0.2267825198681673
  batch 435 loss: 0.22678701853615113
  batch 436 loss: 0.22682441883814444
  batch 437 loss: 0.22700757344063688
  batch 438 loss: 0.22715824185849326
  batch 439 loss: 0.22719153656639107
  batch 440 loss: 0.22727934389629148
  batch 441 loss: 0.2272210153930582
  batch 442 loss: 0.22718041983529993
  batch 443 loss: 0.2272046913748132
  batch 444 loss: 0.2271360885855314
  batch 445 loss: 0.22713022680764788
  batch 446 loss: 0.22710815606630436
  batch 447 loss: 0.22708307309972092
  batch 448 loss: 0.2271237094953124
  batch 449 loss: 0.22719950024826754
  batch 450 loss: 0.22724935157431497
  batch 451 loss: 0.22730794010431962
  batch 452 loss: 0.2274434155397183
  batch 453 loss: 0.22748242325209098
  batch 454 loss: 0.22747786868248743
  batch 455 loss: 0.22760286128127968
  batch 456 loss: 0.22769468081624886
  batch 457 loss: 0.2277060379345516
  batch 458 loss: 0.22769667611632285
  batch 459 loss: 0.2278117827730241
  batch 460 loss: 0.22797766476869583
  batch 461 loss: 0.22800005115811064
  batch 462 loss: 0.2280094132020876
  batch 463 loss: 0.22796816412624993
  batch 464 loss: 0.22808530875321092
  batch 465 loss: 0.2280186511816517
  batch 466 loss: 0.2279316416497906
  batch 467 loss: 0.22801615703029346
  batch 468 loss: 0.22794635692595416
  batch 469 loss: 0.2280826782430413
  batch 470 loss: 0.22805965279645107
  batch 471 loss: 0.22812524890317532
  batch 472 loss: 0.22804539508612479
LOSS train 0.22804539508612479 valid 0.24752488732337952
LOSS train 0.22804539508612479 valid 0.2404015213251114
LOSS train 0.22804539508612479 valid 0.23929266134897867
LOSS train 0.22804539508612479 valid 0.23397377878427505
LOSS train 0.22804539508612479 valid 0.22908917367458342
LOSS train 0.22804539508612479 valid 0.22963102410236994
LOSS train 0.22804539508612479 valid 0.2398548913853509
LOSS train 0.22804539508612479 valid 0.23757370375096798
LOSS train 0.22804539508612479 valid 0.23897692395581138
LOSS train 0.22804539508612479 valid 0.24003067016601562
LOSS train 0.22804539508612479 valid 0.23743716153231534
LOSS train 0.22804539508612479 valid 0.2392395113905271
LOSS train 0.22804539508612479 valid 0.23796546229949364
LOSS train 0.22804539508612479 valid 0.23752443279538835
LOSS train 0.22804539508612479 valid 0.23169133961200714
LOSS train 0.22804539508612479 valid 0.23202656023204327
LOSS train 0.22804539508612479 valid 0.23316004872322083
LOSS train 0.22804539508612479 valid 0.23450250923633575
LOSS train 0.22804539508612479 valid 0.23635811241049515
LOSS train 0.22804539508612479 valid 0.23487207517027855
LOSS train 0.22804539508612479 valid 0.2342115959950856
LOSS train 0.22804539508612479 valid 0.23274389857595618
LOSS train 0.22804539508612479 valid 0.23381293079127435
LOSS train 0.22804539508612479 valid 0.23243652346233526
LOSS train 0.22804539508612479 valid 0.23095573246479034
LOSS train 0.22804539508612479 valid 0.23079089189951235
LOSS train 0.22804539508612479 valid 0.23125570902117976
LOSS train 0.22804539508612479 valid 0.23159524000116757
LOSS train 0.22804539508612479 valid 0.23157354932406854
LOSS train 0.22804539508612479 valid 0.2326507752140363
LOSS train 0.22804539508612479 valid 0.2349877977563489
LOSS train 0.22804539508612479 valid 0.23507647775113583
LOSS train 0.22804539508612479 valid 0.235955996946855
LOSS train 0.22804539508612479 valid 0.23588054013602874
LOSS train 0.22804539508612479 valid 0.23724441400596075
LOSS train 0.22804539508612479 valid 0.23695588898327616
LOSS train 0.22804539508612479 valid 0.23733961944644516
LOSS train 0.22804539508612479 valid 0.23836360911005422
LOSS train 0.22804539508612479 valid 0.23847838930594614
LOSS train 0.22804539508612479 valid 0.23840114697813988
LOSS train 0.22804539508612479 valid 0.23950729500956652
LOSS train 0.22804539508612479 valid 0.2397771520274026
LOSS train 0.22804539508612479 valid 0.23906937830669936
LOSS train 0.22804539508612479 valid 0.23917718841270966
LOSS train 0.22804539508612479 valid 0.23889499538474612
LOSS train 0.22804539508612479 valid 0.23983166366815567
LOSS train 0.22804539508612479 valid 0.24079386953343737
LOSS train 0.22804539508612479 valid 0.24085864704102278
LOSS train 0.22804539508612479 valid 0.2411348889676892
LOSS train 0.22804539508612479 valid 0.24019840627908706
LOSS train 0.22804539508612479 valid 0.2398480594742532
LOSS train 0.22804539508612479 valid 0.23995008147679842
LOSS train 0.22804539508612479 valid 0.24016150382329832
LOSS train 0.22804539508612479 valid 0.24023087598659373
LOSS train 0.22804539508612479 valid 0.24031755544922567
LOSS train 0.22804539508612479 valid 0.2395793151642595
LOSS train 0.22804539508612479 valid 0.23902569685065955
LOSS train 0.22804539508612479 valid 0.23842152131014857
LOSS train 0.22804539508612479 valid 0.23905943157309192
LOSS train 0.22804539508612479 valid 0.23890805318951608
LOSS train 0.22804539508612479 valid 0.2384422260229705
LOSS train 0.22804539508612479 valid 0.23925225052141375
LOSS train 0.22804539508612479 valid 0.23950266223105174
LOSS train 0.22804539508612479 valid 0.24057650938630104
LOSS train 0.22804539508612479 valid 0.2413286002782675
LOSS train 0.22804539508612479 valid 0.24100998472986798
LOSS train 0.22804539508612479 valid 0.24026879334627693
LOSS train 0.22804539508612479 valid 0.2405612326281912
LOSS train 0.22804539508612479 valid 0.23973300793896551
LOSS train 0.22804539508612479 valid 0.24028367357594627
LOSS train 0.22804539508612479 valid 0.2399503445961106
LOSS train 0.22804539508612479 valid 0.24024532238642374
LOSS train 0.22804539508612479 valid 0.23985210101898402
LOSS train 0.22804539508612479 valid 0.23958418337074486
LOSS train 0.22804539508612479 valid 0.23958163221677145
LOSS train 0.22804539508612479 valid 0.23966530847706294
LOSS train 0.22804539508612479 valid 0.23972632010261735
LOSS train 0.22804539508612479 valid 0.24028031527996063
LOSS train 0.22804539508612479 valid 0.24060139437265035
LOSS train 0.22804539508612479 valid 0.23980942294001578
LOSS train 0.22804539508612479 valid 0.23886225952042472
LOSS train 0.22804539508612479 valid 0.23958204686641693
LOSS train 0.22804539508612479 valid 0.2395446241261011
LOSS train 0.22804539508612479 valid 0.23937653253475824
LOSS train 0.22804539508612479 valid 0.23911229775232426
LOSS train 0.22804539508612479 valid 0.2385661979747373
LOSS train 0.22804539508612479 valid 0.23858485430821605
LOSS train 0.22804539508612479 valid 0.23806121251122517
LOSS train 0.22804539508612479 valid 0.2386051354448447
LOSS train 0.22804539508612479 valid 0.23892941822608313
LOSS train 0.22804539508612479 valid 0.2390578364605432
LOSS train 0.22804539508612479 valid 0.23926351170824922
LOSS train 0.22804539508612479 valid 0.23908907719837721
LOSS train 0.22804539508612479 valid 0.23934001903584662
LOSS train 0.22804539508612479 valid 0.23921171426773072
LOSS train 0.22804539508612479 valid 0.239676329617699
LOSS train 0.22804539508612479 valid 0.23959328280281775
LOSS train 0.22804539508612479 valid 0.23972456309260154
LOSS train 0.22804539508612479 valid 0.24011941118673844
LOSS train 0.22804539508612479 valid 0.2401159892976284
LOSS train 0.22804539508612479 valid 0.2403827561895446
LOSS train 0.22804539508612479 valid 0.2404165397964272
LOSS train 0.22804539508612479 valid 0.24035786077814195
LOSS train 0.22804539508612479 valid 0.240468454762147
LOSS train 0.22804539508612479 valid 0.24052814188457672
LOSS train 0.22804539508612479 valid 0.24087151885032654
LOSS train 0.22804539508612479 valid 0.24080884930129362
LOSS train 0.22804539508612479 valid 0.24076263192627165
LOSS train 0.22804539508612479 valid 0.24119022105811933
LOSS train 0.22804539508612479 valid 0.24141907664862547
LOSS train 0.22804539508612479 valid 0.24094046182460613
LOSS train 0.22804539508612479 valid 0.2407160611557109
LOSS train 0.22804539508612479 valid 0.24061811220856894
LOSS train 0.22804539508612479 valid 0.2405113806588608
LOSS train 0.22804539508612479 valid 0.240645959455034
LOSS train 0.22804539508612479 valid 0.2404974882715735
LOSS train 0.22804539508612479 valid 0.24060348631479803
LOSS train 0.22804539508612479 valid 0.2404753266502235
LOSS train 0.22804539508612479 valid 0.24041677735933736
LOSS train 0.22804539508612479 valid 0.24016596240301927
LOSS train 0.22804539508612479 valid 0.24026683502453416
LOSS train 0.22804539508612479 valid 0.2400283450718786
LOSS train 0.22804539508612479 valid 0.2399699646767562
LOSS train 0.22804539508612479 valid 0.24043747974980262
LOSS train 0.22804539508612479 valid 0.24038569951057434
LOSS train 0.22804539508612479 valid 0.24059376167872595
LOSS train 0.22804539508612479 valid 0.24064093394072975
LOSS train 0.22804539508612479 valid 0.2410930598853156
LOSS train 0.22804539508612479 valid 0.24107829729715982
LOSS train 0.22804539508612479 valid 0.2408423111988948
LOSS train 0.22804539508612479 valid 0.24077175045741422
LOSS train 0.22804539508612479 valid 0.24058595017501802
LOSS train 0.22804539508612479 valid 0.24066409233369326
LOSS train 0.22804539508612479 valid 0.24071088866956197
LOSS train 0.22804539508612479 valid 0.24056436000046907
LOSS train 0.22804539508612479 valid 0.24033627133159077
LOSS train 0.22804539508612479 valid 0.24020743109013912
LOSS train 0.22804539508612479 valid 0.2402742086113363
LOSS train 0.22804539508612479 valid 0.24027066275799017
LOSS train 0.22804539508612479 valid 0.24022421102438654
LOSS train 0.22804539508612479 valid 0.24027286681300358
LOSS train 0.22804539508612479 valid 0.24060956115873766
LOSS train 0.22804539508612479 valid 0.24059659284311574
LOSS train 0.22804539508612479 valid 0.2403935640015536
LOSS train 0.22804539508612479 valid 0.24016899137661374
LOSS train 0.22804539508612479 valid 0.24024401931730036
LOSS train 0.22804539508612479 valid 0.24007834220419125
LOSS train 0.22804539508612479 valid 0.24103826546185725
LOSS train 0.22804539508612479 valid 0.2411741034296535
LOSS train 0.22804539508612479 valid 0.24142348130544028
LOSS train 0.22804539508612479 valid 0.24143318239821504
LOSS train 0.22804539508612479 valid 0.2409610517911221
LOSS train 0.22804539508612479 valid 0.24107997976486978
LOSS train 0.22804539508612479 valid 0.24098454657700155
LOSS train 0.22804539508612479 valid 0.24100411322809034
LOSS train 0.22804539508612479 valid 0.241039805018749
LOSS train 0.22804539508612479 valid 0.24084745451902886
LOSS train 0.22804539508612479 valid 0.24093267529071133
LOSS train 0.22804539508612479 valid 0.24092230556895897
LOSS train 0.22804539508612479 valid 0.24086874444037676
LOSS train 0.22804539508612479 valid 0.2407627217702984
LOSS train 0.22804539508612479 valid 0.24076317885407694
LOSS train 0.22804539508612479 valid 0.24053669743742678
LOSS train 0.22804539508612479 valid 0.24018526040926214
LOSS train 0.22804539508612479 valid 0.23985785327174447
LOSS train 0.22804539508612479 valid 0.2399551045284214
LOSS train 0.22804539508612479 valid 0.2403698266444806
LOSS train 0.22804539508612479 valid 0.24014022494001047
LOSS train 0.22804539508612479 valid 0.2402653229483486
LOSS train 0.22804539508612479 valid 0.24032374155872008
LOSS train 0.22804539508612479 valid 0.2403866881341265
LOSS train 0.22804539508612479 valid 0.2401220336729704
LOSS train 0.22804539508612479 valid 0.2400696537742725
LOSS train 0.22804539508612479 valid 0.2400287166237831
LOSS train 0.22804539508612479 valid 0.23981235529695238
LOSS train 0.22804539508612479 valid 0.23980443983931432
LOSS train 0.22804539508612479 valid 0.23964342887455461
LOSS train 0.22804539508612479 valid 0.23983034667339218
LOSS train 0.22804539508612479 valid 0.23964932011492426
LOSS train 0.22804539508612479 valid 0.23955064308312204
LOSS train 0.22804539508612479 valid 0.23970737320612806
LOSS train 0.22804539508612479 valid 0.2398465219271052
LOSS train 0.22804539508612479 valid 0.23995347874737827
LOSS train 0.22804539508612479 valid 0.2399459517682376
LOSS train 0.22804539508612479 valid 0.23960343784577137
LOSS train 0.22804539508612479 valid 0.2396095399895022
LOSS train 0.22804539508612479 valid 0.23949640096827624
LOSS train 0.22804539508612479 valid 0.2396554193915205
LOSS train 0.22804539508612479 valid 0.2395443531571242
LOSS train 0.22804539508612479 valid 0.23964862745059165
LOSS train 0.22804539508612479 valid 0.23946965957811367
LOSS train 0.22804539508612479 valid 0.23955843861525258
LOSS train 0.22804539508612479 valid 0.23947320469302835
LOSS train 0.22804539508612479 valid 0.23937505122619807
LOSS train 0.22804539508612479 valid 0.23909447292486827
LOSS train 0.22804539508612479 valid 0.2389667421123203
LOSS train 0.22804539508612479 valid 0.23906703958959144
LOSS train 0.22804539508612479 valid 0.23880357144757955
LOSS train 0.22804539508612479 valid 0.23887158540924588
LOSS train 0.22804539508612479 valid 0.23886009998619556
LOSS train 0.22804539508612479 valid 0.2386948118607203
LOSS train 0.22804539508612479 valid 0.23857882272193928
LOSS train 0.22804539508612479 valid 0.23846625371519567
LOSS train 0.22804539508612479 valid 0.23839190624216022
LOSS train 0.22804539508612479 valid 0.2380276471376419
LOSS train 0.22804539508612479 valid 0.23808762380509702
LOSS train 0.22804539508612479 valid 0.23793943881412635
LOSS train 0.22804539508612479 valid 0.2377428308559152
LOSS train 0.22804539508612479 valid 0.23759236817724966
LOSS train 0.22804539508612479 valid 0.23763149849006107
LOSS train 0.22804539508612479 valid 0.2378250179697552
LOSS train 0.22804539508612479 valid 0.23777690438448257
LOSS train 0.22804539508612479 valid 0.23794482760306254
LOSS train 0.22804539508612479 valid 0.23794134309358686
LOSS train 0.22804539508612479 valid 0.23776922364567601
LOSS train 0.22804539508612479 valid 0.23770787835948998
LOSS train 0.22804539508612479 valid 0.2377531152853768
LOSS train 0.22804539508612479 valid 0.23788386904591813
LOSS train 0.22804539508612479 valid 0.2380581979201809
LOSS train 0.22804539508612479 valid 0.23813410746780309
LOSS train 0.22804539508612479 valid 0.23808958209477937
LOSS train 0.22804539508612479 valid 0.23792870060817614
LOSS train 0.22804539508612479 valid 0.2379677021717277
LOSS train 0.22804539508612479 valid 0.23802731425634452
LOSS train 0.22804539508612479 valid 0.237933306561576
LOSS train 0.22804539508612479 valid 0.2379459592357146
LOSS train 0.22804539508612479 valid 0.23821686332971514
LOSS train 0.22804539508612479 valid 0.23819443204423837
LOSS train 0.22804539508612479 valid 0.23826344578026684
LOSS train 0.22804539508612479 valid 0.23836154963659203
LOSS train 0.22804539508612479 valid 0.2383811967971521
LOSS train 0.22804539508612479 valid 0.2383786140587823
LOSS train 0.22804539508612479 valid 0.23833578626741156
LOSS train 0.22804539508612479 valid 0.23830256891301554
LOSS train 0.22804539508612479 valid 0.23857129197171395
LOSS train 0.22804539508612479 valid 0.23835250312241457
LOSS train 0.22804539508612479 valid 0.23846665093918892
LOSS train 0.22804539508612479 valid 0.23854613410575048
LOSS train 0.22804539508612479 valid 0.23836450715194687
LOSS train 0.22804539508612479 valid 0.23835175577551126
LOSS train 0.22804539508612479 valid 0.23839238094343684
LOSS train 0.22804539508612479 valid 0.23816461676408437
LOSS train 0.22804539508612479 valid 0.23835114711596642
LOSS train 0.22804539508612479 valid 0.23875217811494578
LOSS train 0.22804539508612479 valid 0.23905110626804585
LOSS train 0.22804539508612479 valid 0.23906051270603165
LOSS train 0.22804539508612479 valid 0.23891700321604847
LOSS train 0.22804539508612479 valid 0.23889668454085627
LOSS train 0.22804539508612479 valid 0.2388219688551493
LOSS train 0.22804539508612479 valid 0.23909967744350433
LOSS train 0.22804539508612479 valid 0.23918924922962112
LOSS train 0.22804539508612479 valid 0.23927394064172866
LOSS train 0.22804539508612479 valid 0.23927059999332126
LOSS train 0.22804539508612479 valid 0.23916844466305154
LOSS train 0.22804539508612479 valid 0.2391858868154825
LOSS train 0.22804539508612479 valid 0.23931038397131488
LOSS train 0.22804539508612479 valid 0.23914899824890182
LOSS train 0.22804539508612479 valid 0.23923585621654525
LOSS train 0.22804539508612479 valid 0.2392690352031163
LOSS train 0.22804539508612479 valid 0.2391916632079161
LOSS train 0.22804539508612479 valid 0.23935215026711137
LOSS train 0.22804539508612479 valid 0.23938686844274287
LOSS train 0.22804539508612479 valid 0.2394267122895092
LOSS train 0.22804539508612479 valid 0.23941968087897156
LOSS train 0.22804539508612479 valid 0.23931489047014487
LOSS train 0.22804539508612479 valid 0.23940975919253843
LOSS train 0.22804539508612479 valid 0.23951888430431095
LOSS train 0.22804539508612479 valid 0.23975041897884056
LOSS train 0.22804539508612479 valid 0.2397899288670281
LOSS train 0.22804539508612479 valid 0.23970808877989097
LOSS train 0.22804539508612479 valid 0.23980343677240984
LOSS train 0.22804539508612479 valid 0.24004993965739713
LOSS train 0.22804539508612479 valid 0.2401007884915495
LOSS train 0.22804539508612479 valid 0.24004538036393422
LOSS train 0.22804539508612479 valid 0.23995791792869567
LOSS train 0.22804539508612479 valid 0.23983307015420735
LOSS train 0.22804539508612479 valid 0.23969669704618007
LOSS train 0.22804539508612479 valid 0.23955444064286116
LOSS train 0.22804539508612479 valid 0.2395347905842634
LOSS train 0.22804539508612479 valid 0.2394925284598555
LOSS train 0.22804539508612479 valid 0.2393300752304627
LOSS train 0.22804539508612479 valid 0.23915314938582427
LOSS train 0.22804539508612479 valid 0.23901071633761847
LOSS train 0.22804539508612479 valid 0.23910974527538661
LOSS train 0.22804539508612479 valid 0.23919023019179964
LOSS train 0.22804539508612479 valid 0.23918407519797344
LOSS train 0.22804539508612479 valid 0.2391259772021596
LOSS train 0.22804539508612479 valid 0.23908517033689553
LOSS train 0.22804539508612479 valid 0.23920636936042547
LOSS train 0.22804539508612479 valid 0.2391902461134154
LOSS train 0.22804539508612479 valid 0.23914463377695314
LOSS train 0.22804539508612479 valid 0.2392775968023359
LOSS train 0.22804539508612479 valid 0.2392793997562786
LOSS train 0.22804539508612479 valid 0.2393929184902282
LOSS train 0.22804539508612479 valid 0.23944072208162082
LOSS train 0.22804539508612479 valid 0.23931587754270514
LOSS train 0.22804539508612479 valid 0.239361750206562
LOSS train 0.22804539508612479 valid 0.2393380059991907
LOSS train 0.22804539508612479 valid 0.2393410116633444
LOSS train 0.22804539508612479 valid 0.2392571446299553
LOSS train 0.22804539508612479 valid 0.23933513487296246
LOSS train 0.22804539508612479 valid 0.23939256527960695
LOSS train 0.22804539508612479 valid 0.23950470329904713
LOSS train 0.22804539508612479 valid 0.23947864109160086
LOSS train 0.22804539508612479 valid 0.2394187261823748
LOSS train 0.22804539508612479 valid 0.2393989121310072
LOSS train 0.22804539508612479 valid 0.23927618767034736
LOSS train 0.22804539508612479 valid 0.23921163375307988
LOSS train 0.22804539508612479 valid 0.23910227205761042
LOSS train 0.22804539508612479 valid 0.23911868295361918
LOSS train 0.22804539508612479 valid 0.2390501242837722
LOSS train 0.22804539508612479 valid 0.23904204187102807
LOSS train 0.22804539508612479 valid 0.23897813246273003
LOSS train 0.22804539508612479 valid 0.23899713827736058
LOSS train 0.22804539508612479 valid 0.2389549703352035
LOSS train 0.22804539508612479 valid 0.2388831702874431
LOSS train 0.22804539508612479 valid 0.23892722873469632
LOSS train 0.22804539508612479 valid 0.23903811254403876
LOSS train 0.22804539508612479 valid 0.23916968100683816
LOSS train 0.22804539508612479 valid 0.23910487531684338
LOSS train 0.22804539508612479 valid 0.23932722698305256
LOSS train 0.22804539508612479 valid 0.23915791654994029
LOSS train 0.22804539508612479 valid 0.2390578729172609
LOSS train 0.22804539508612479 valid 0.23910427392448907
LOSS train 0.22804539508612479 valid 0.2392444331370867
LOSS train 0.22804539508612479 valid 0.23933170191897937
LOSS train 0.22804539508612479 valid 0.2393529194119509
LOSS train 0.22804539508612479 valid 0.23934758331899236
LOSS train 0.22804539508612479 valid 0.2393853740942152
LOSS train 0.22804539508612479 valid 0.23943487338044428
LOSS train 0.22804539508612479 valid 0.239311226860274
LOSS train 0.22804539508612479 valid 0.23924120286681566
LOSS train 0.22804539508612479 valid 0.23927092579033998
LOSS train 0.22804539508612479 valid 0.23940524655187914
LOSS train 0.22804539508612479 valid 0.23936715428508928
LOSS train 0.22804539508612479 valid 0.23917841955664612
LOSS train 0.22804539508612479 valid 0.23916643420206687
LOSS train 0.22804539508612479 valid 0.2391190000275183
LOSS train 0.22804539508612479 valid 0.23900734688152606
LOSS train 0.22804539508612479 valid 0.23904778076445354
LOSS train 0.22804539508612479 valid 0.23894502768768244
LOSS train 0.22804539508612479 valid 0.2389442024872317
LOSS train 0.22804539508612479 valid 0.23887192968376872
LOSS train 0.22804539508612479 valid 0.23904977175731992
LOSS train 0.22804539508612479 valid 0.23903203187645344
LOSS train 0.22804539508612479 valid 0.2389958684757955
LOSS train 0.22804539508612479 valid 0.23883078361107224
LOSS train 0.22804539508612479 valid 0.23873952767629733
LOSS train 0.22804539508612479 valid 0.23881781126844848
LOSS train 0.22804539508612479 valid 0.23875106219734465
LOSS train 0.22804539508612479 valid 0.23870583052648778
LOSS train 0.22804539508612479 valid 0.2386654495176944
LOSS train 0.22804539508612479 valid 0.23866467173646597
LOSS train 0.22804539508612479 valid 0.23871389258716066
LOSS train 0.22804539508612479 valid 0.2388326667564016
LOSS train 0.22804539508612479 valid 0.23894246533680497
LOSS train 0.22804539508612479 valid 0.2389529266444241
LOSS train 0.22804539508612479 valid 0.2389299506997929
LOSS train 0.22804539508612479 valid 0.23890257056045
LOSS train 0.22804539508612479 valid 0.2388938015533818
LOSS train 0.22804539508612479 valid 0.23892860168235125
LOSS train 0.22804539508612479 valid 0.23903253485155368
LOSS train 0.22804539508612479 valid 0.2388557010527813
LOSS train 0.22804539508612479 valid 0.23883353145076677
LOSS train 0.22804539508612479 valid 0.23888108628253416
LOSS train 0.22804539508612479 valid 0.2388496876902919
LOSS train 0.22804539508612479 valid 0.23873850577371322
LOSS train 0.22804539508612479 valid 0.23872743125842966
LOSS train 0.22804539508612479 valid 0.23884916588219846
EPOCH 22:
  batch 1 loss: 0.27357929944992065
  batch 2 loss: 0.2374202162027359
  batch 3 loss: 0.24000600477059683
  batch 4 loss: 0.23940382525324821
  batch 5 loss: 0.23848765790462495
  batch 6 loss: 0.2419929876923561
  batch 7 loss: 0.2469464008297239
  batch 8 loss: 0.25312495417892933
  batch 9 loss: 0.25267979668246376
  batch 10 loss: 0.25141052454710006
  batch 11 loss: 0.251159975474531
  batch 12 loss: 0.24731851120789847
  batch 13 loss: 0.24523934263449448
  batch 14 loss: 0.2429280685526984
  batch 15 loss: 0.24269663294156393
  batch 16 loss: 0.24113798048347235
  batch 17 loss: 0.23724066071650562
  batch 18 loss: 0.23643539763159221
  batch 19 loss: 0.23547442414258657
  batch 20 loss: 0.23403099402785302
  batch 21 loss: 0.23471185068289438
  batch 22 loss: 0.23331940309567886
  batch 23 loss: 0.23239333474117777
  batch 24 loss: 0.2303799595683813
  batch 25 loss: 0.23180215656757355
  batch 26 loss: 0.22969441058544013
  batch 27 loss: 0.22958259063738365
  batch 28 loss: 0.2283706313797406
  batch 29 loss: 0.22923019014555832
  batch 30 loss: 0.22891333748896917
  batch 31 loss: 0.22957827823777352
  batch 32 loss: 0.2293742666952312
  batch 33 loss: 0.22888194385803107
  batch 34 loss: 0.2275591733701089
  batch 35 loss: 0.22738396951130457
  batch 36 loss: 0.22846312903695637
  batch 37 loss: 0.2281510801734151
  batch 38 loss: 0.2289692983031273
  batch 39 loss: 0.22864074393724784
  batch 40 loss: 0.22875794135034083
  batch 41 loss: 0.22863927628935837
  batch 42 loss: 0.2283857131288165
  batch 43 loss: 0.22900063977685087
  batch 44 loss: 0.2288508875803514
  batch 45 loss: 0.22883520225683848
  batch 46 loss: 0.22909870063481125
  batch 47 loss: 0.22870673270935707
  batch 48 loss: 0.2278051913405458
  batch 49 loss: 0.22787404334058567
  batch 50 loss: 0.22784966468811035
  batch 51 loss: 0.22775186247685375
  batch 52 loss: 0.22788998054770324
  batch 53 loss: 0.22774112027771068
  batch 54 loss: 0.2280231167872747
  batch 55 loss: 0.22757818319580772
  batch 56 loss: 0.22725764050015382
  batch 57 loss: 0.22697324632552632
  batch 58 loss: 0.2277026888111542
  batch 59 loss: 0.22781830293647312
  batch 60 loss: 0.22778755351901053
  batch 61 loss: 0.22808164016145174
  batch 62 loss: 0.22889471486691507
  batch 63 loss: 0.22863832045169102
  batch 64 loss: 0.229603216284886
  batch 65 loss: 0.2299037169951659
  batch 66 loss: 0.22984931076114828
  batch 67 loss: 0.2307318842677928
  batch 68 loss: 0.23085589159060926
  batch 69 loss: 0.23132761557033096
  batch 70 loss: 0.23174808344670705
  batch 71 loss: 0.23161776636687803
  batch 72 loss: 0.23213962465524673
  batch 73 loss: 0.23263768218968012
  batch 74 loss: 0.23252108048748327
  batch 75 loss: 0.2321926518281301
  batch 76 loss: 0.23288385766117195
  batch 77 loss: 0.23247302623538227
  batch 78 loss: 0.23285376528898874
  batch 79 loss: 0.2329532323004324
  batch 80 loss: 0.23266872260719537
  batch 81 loss: 0.23328072035018307
  batch 82 loss: 0.23416787113358334
  batch 83 loss: 0.23393619599112545
  batch 84 loss: 0.233881447996412
  batch 85 loss: 0.23392963286708382
  batch 86 loss: 0.23493359064640001
  batch 87 loss: 0.23494000064915624
  batch 88 loss: 0.23497685180468994
  batch 89 loss: 0.23551078726736346
  batch 90 loss: 0.23588242398367987
  batch 91 loss: 0.23579468291539413
  batch 92 loss: 0.2355816203938878
  batch 93 loss: 0.23583050136284162
  batch 94 loss: 0.23632907058964384
  batch 95 loss: 0.23596006678907494
  batch 96 loss: 0.23582697131981453
  batch 97 loss: 0.2359337746789775
  batch 98 loss: 0.23626486820225812
  batch 99 loss: 0.23682343523309687
  batch 100 loss: 0.23653524711728097
  batch 101 loss: 0.23636448117766048
  batch 102 loss: 0.23671235407100005
  batch 103 loss: 0.23708685740683844
  batch 104 loss: 0.23717859444709924
  batch 105 loss: 0.23702464515254612
  batch 106 loss: 0.23711683769833367
  batch 107 loss: 0.23704536426290174
  batch 108 loss: 0.23759582486969452
  batch 109 loss: 0.2374602116029197
  batch 110 loss: 0.23763747377829117
  batch 111 loss: 0.23778845356391357
  batch 112 loss: 0.2376792998984456
  batch 113 loss: 0.2375508120893377
  batch 114 loss: 0.2377436421158021
  batch 115 loss: 0.2377761855073597
  batch 116 loss: 0.23820098836360307
  batch 117 loss: 0.23833795859772935
  batch 118 loss: 0.2382063561324346
  batch 119 loss: 0.23821053109249146
  batch 120 loss: 0.2377412263303995
  batch 121 loss: 0.23760054377484913
  batch 122 loss: 0.23770836239955465
  batch 123 loss: 0.2375848800913105
  batch 124 loss: 0.23777591352981906
  batch 125 loss: 0.2376471289396286
  batch 126 loss: 0.23780351108501827
  batch 127 loss: 0.23824970400708867
  batch 128 loss: 0.2380219188053161
  batch 129 loss: 0.23809297117151956
  batch 130 loss: 0.2378605841444089
  batch 131 loss: 0.23780817350813452
  batch 132 loss: 0.23767107262304335
  batch 133 loss: 0.23792343401819244
  batch 134 loss: 0.2379596677066675
  batch 135 loss: 0.2376829669431404
  batch 136 loss: 0.23777736427591128
  batch 137 loss: 0.23764490918086392
  batch 138 loss: 0.23762893277233926
  batch 139 loss: 0.23772212359116232
  batch 140 loss: 0.2375281116792134
  batch 141 loss: 0.23771340732878826
  batch 142 loss: 0.2375525068229353
  batch 143 loss: 0.2372051626443863
  batch 144 loss: 0.23708448900530735
  batch 145 loss: 0.23706456885255617
  batch 146 loss: 0.2368351138413769
  batch 147 loss: 0.23707785300251577
  batch 148 loss: 0.23708945452361493
  batch 149 loss: 0.23678851947688417
  batch 150 loss: 0.23665004978577295
  batch 151 loss: 0.23659802805508998
  batch 152 loss: 0.23668356837802812
  batch 153 loss: 0.23645729216095668
  batch 154 loss: 0.23657786207539694
  batch 155 loss: 0.23640217454202714
  batch 156 loss: 0.23635713937572944
  batch 157 loss: 0.23658839959627503
  batch 158 loss: 0.23642252970345412
  batch 159 loss: 0.23631461386410696
  batch 160 loss: 0.2360359186306596
  batch 161 loss: 0.23613560532931216
  batch 162 loss: 0.23593347326472955
  batch 163 loss: 0.23583147240562674
  batch 164 loss: 0.23539926738637248
  batch 165 loss: 0.23527407095287786
  batch 166 loss: 0.23531060656869268
  batch 167 loss: 0.23519683669427197
  batch 168 loss: 0.23503735767943518
  batch 169 loss: 0.23483387729119973
  batch 170 loss: 0.23465536324416889
  batch 171 loss: 0.23456001246881764
  batch 172 loss: 0.2345200338682463
  batch 173 loss: 0.23437194267793887
  batch 174 loss: 0.23439913440024715
  batch 175 loss: 0.23450168762888227
  batch 176 loss: 0.23435715569013899
  batch 177 loss: 0.23442542536110528
  batch 178 loss: 0.23454299552387067
  batch 179 loss: 0.23461121029361
  batch 180 loss: 0.23451826431685024
  batch 181 loss: 0.23474979672313395
  batch 182 loss: 0.23464456552660073
  batch 183 loss: 0.23450196953744837
  batch 184 loss: 0.23450558883664402
  batch 185 loss: 0.23459310233592987
  batch 186 loss: 0.23470165820852404
  batch 187 loss: 0.23456222536092136
  batch 188 loss: 0.23426013733161258
  batch 189 loss: 0.2339481370001243
  batch 190 loss: 0.23391982089532048
  batch 191 loss: 0.23366702859002259
  batch 192 loss: 0.23348395192685226
  batch 193 loss: 0.23328341312050202
  batch 194 loss: 0.23337290098064953
  batch 195 loss: 0.23333450189003577
  batch 196 loss: 0.2330659859794743
  batch 197 loss: 0.23297610644459119
  batch 198 loss: 0.2332268108171646
  batch 199 loss: 0.2329737485653192
  batch 200 loss: 0.23324540242552758
  batch 201 loss: 0.2333046216425018
  batch 202 loss: 0.23341439258639174
  batch 203 loss: 0.2333146876155449
  batch 204 loss: 0.23324300253800317
  batch 205 loss: 0.23336321835110826
  batch 206 loss: 0.2333695868988639
  batch 207 loss: 0.23344506215358127
  batch 208 loss: 0.23349777723734194
  batch 209 loss: 0.23330686936538186
  batch 210 loss: 0.23338975210984547
  batch 211 loss: 0.2331962266945726
  batch 212 loss: 0.23307050117906533
  batch 213 loss: 0.23293288115044714
  batch 214 loss: 0.23287716507911682
  batch 215 loss: 0.23274786721828372
  batch 216 loss: 0.23244284824640662
  batch 217 loss: 0.23224997808856349
  batch 218 loss: 0.23218811631476113
  batch 219 loss: 0.23222915957507478
  batch 220 loss: 0.23213126639073545
  batch 221 loss: 0.2321618903950868
  batch 222 loss: 0.23208678131167954
  batch 223 loss: 0.23197478263100166
  batch 224 loss: 0.231920389258968
  batch 225 loss: 0.23188201281759474
  batch 226 loss: 0.23192512553877534
  batch 227 loss: 0.23185702265621808
  batch 228 loss: 0.23176429361889236
  batch 229 loss: 0.23176405660189917
  batch 230 loss: 0.23183999314256337
  batch 231 loss: 0.23168374404504702
  batch 232 loss: 0.23154336066338524
  batch 233 loss: 0.23147120195652793
  batch 234 loss: 0.23134121528038612
  batch 235 loss: 0.23124511216549162
  batch 236 loss: 0.2310677884241282
  batch 237 loss: 0.23108051937340685
  batch 238 loss: 0.23104746063717274
  batch 239 loss: 0.23085863938142065
  batch 240 loss: 0.23073515885819992
  batch 241 loss: 0.23075901064635312
  batch 242 loss: 0.23065924059507276
  batch 243 loss: 0.23061056269539726
  batch 244 loss: 0.2306065488545621
  batch 245 loss: 0.23046099294205102
  batch 246 loss: 0.2305381394377569
  batch 247 loss: 0.23062381130239742
  batch 248 loss: 0.23070910939526174
  batch 249 loss: 0.23066278274758273
  batch 250 loss: 0.23060631215572358
  batch 251 loss: 0.23066625278072053
  batch 252 loss: 0.23050757367459554
  batch 253 loss: 0.23042734320691452
  batch 254 loss: 0.2302595975361471
  batch 255 loss: 0.23022515329660154
  batch 256 loss: 0.2304108222015202
  batch 257 loss: 0.2303871319915534
  batch 258 loss: 0.23027375005474388
  batch 259 loss: 0.23022652765498658
  batch 260 loss: 0.23025261811338937
  batch 261 loss: 0.23026126309144543
  batch 262 loss: 0.2302373802957644
  batch 263 loss: 0.23034218389951683
  batch 264 loss: 0.23027975913701634
  batch 265 loss: 0.23011257446037148
  batch 266 loss: 0.23014356179120846
  batch 267 loss: 0.230189872870731
  batch 268 loss: 0.2300432658573585
  batch 269 loss: 0.23000834500036274
  batch 270 loss: 0.23020880829404902
  batch 271 loss: 0.23016134432321106
  batch 272 loss: 0.23016225294593504
  batch 273 loss: 0.23010374436448344
  batch 274 loss: 0.23018242233861103
  batch 275 loss: 0.23011509250510823
  batch 276 loss: 0.230027605118095
  batch 277 loss: 0.22996693171749047
  batch 278 loss: 0.22979548911182143
  batch 279 loss: 0.22980407964585076
  batch 280 loss: 0.22963518095867974
  batch 281 loss: 0.22959964958374188
  batch 282 loss: 0.22957224145214608
  batch 283 loss: 0.22943985083077906
  batch 284 loss: 0.22941266488231404
  batch 285 loss: 0.22933104629056494
  batch 286 loss: 0.22933564215273292
  batch 287 loss: 0.22929978282401786
  batch 288 loss: 0.22916512382734153
  batch 289 loss: 0.22925523897974548
  batch 290 loss: 0.22907698046544503
  batch 291 loss: 0.22908071820268927
  batch 292 loss: 0.2290937336136217
  batch 293 loss: 0.229003126456469
  batch 294 loss: 0.22889453512268002
  batch 295 loss: 0.22888652232743925
  batch 296 loss: 0.22892637148096756
  batch 297 loss: 0.22882368367930453
  batch 298 loss: 0.22866656046185718
  batch 299 loss: 0.22856335297077396
  batch 300 loss: 0.22857854415973028
  batch 301 loss: 0.22858685933276268
  batch 302 loss: 0.22847228966011907
  batch 303 loss: 0.2283166769904272
  batch 304 loss: 0.2282256607554461
  batch 305 loss: 0.22806101021219471
  batch 306 loss: 0.2281644214796864
  batch 307 loss: 0.2281920735144848
  batch 308 loss: 0.22823986778785657
  batch 309 loss: 0.22821665986841935
  batch 310 loss: 0.22813024160362058
  batch 311 loss: 0.22817562673827843
  batch 312 loss: 0.22828727420897055
  batch 313 loss: 0.22833484163680395
  batch 314 loss: 0.22820388777240827
  batch 315 loss: 0.2281400311087805
  batch 316 loss: 0.22803914311188686
  batch 317 loss: 0.22794692500344585
  batch 318 loss: 0.22792702627444417
  batch 319 loss: 0.22788047421501723
  batch 320 loss: 0.22785274633206426
  batch 321 loss: 0.22788047275253545
  batch 322 loss: 0.22782036925880064
  batch 323 loss: 0.22768798142948388
  batch 324 loss: 0.22747572741390745
  batch 325 loss: 0.22742337125998277
  batch 326 loss: 0.2273813004325504
  batch 327 loss: 0.2273692822237627
  batch 328 loss: 0.22719273498145545
  batch 329 loss: 0.22723232209682465
  batch 330 loss: 0.22718164505380573
  batch 331 loss: 0.22716268329822045
  batch 332 loss: 0.22706294732998653
  batch 333 loss: 0.2270748651063478
  batch 334 loss: 0.22695416029163462
  batch 335 loss: 0.22691649581069376
  batch 336 loss: 0.22686750095869815
  batch 337 loss: 0.2268189618071038
  batch 338 loss: 0.2268671462609923
  batch 339 loss: 0.2267981801061152
  batch 340 loss: 0.226784418216523
  batch 341 loss: 0.22682351166726558
  batch 342 loss: 0.22670384531306942
  batch 343 loss: 0.22670046411693617
  batch 344 loss: 0.22668832692122737
  batch 345 loss: 0.2267364424207936
  batch 346 loss: 0.22662822626574192
  batch 347 loss: 0.22657984593931468
  batch 348 loss: 0.22650620606781421
  batch 349 loss: 0.22640875272218
  batch 350 loss: 0.2264426293543407
  batch 351 loss: 0.22639738524231814
  batch 352 loss: 0.2264063620347191
  batch 353 loss: 0.22636204430300522
  batch 354 loss: 0.22655234620564402
  batch 355 loss: 0.2265185042166374
  batch 356 loss: 0.22642521702506568
  batch 357 loss: 0.22638431692323766
  batch 358 loss: 0.2264604110624537
  batch 359 loss: 0.2264813625131809
  batch 360 loss: 0.22642287500202657
  batch 361 loss: 0.22637380081695863
  batch 362 loss: 0.22625743931169667
  batch 363 loss: 0.22618293733472009
  batch 364 loss: 0.22610212072402566
  batch 365 loss: 0.22609692734398254
  batch 366 loss: 0.22602002033607557
  batch 367 loss: 0.22594550580841968
  batch 368 loss: 0.22587410846482153
  batch 369 loss: 0.22584492581969678
  batch 370 loss: 0.22572847745708516
  batch 371 loss: 0.2257564886801969
  batch 372 loss: 0.225772793134374
  batch 373 loss: 0.22568107230573814
  batch 374 loss: 0.22558574968162068
  batch 375 loss: 0.22546339106559754
  batch 376 loss: 0.22553004387845385
  batch 377 loss: 0.22552007835961146
  batch 378 loss: 0.2254861601130672
  batch 379 loss: 0.2253550965741945
  batch 380 loss: 0.22540391373791194
  batch 381 loss: 0.22538088359857794
  batch 382 loss: 0.22532849150334353
  batch 383 loss: 0.22533573291942904
  batch 384 loss: 0.22525872429832816
  batch 385 loss: 0.22537888605873307
  batch 386 loss: 0.22535278397211755
  batch 387 loss: 0.22543510748434437
  batch 388 loss: 0.22540044438900406
  batch 389 loss: 0.2253612943357551
  batch 390 loss: 0.2253273007197258
  batch 391 loss: 0.22541526043811416
  batch 392 loss: 0.225360312860231
  batch 393 loss: 0.2252886011703627
  batch 394 loss: 0.2252853449982435
  batch 395 loss: 0.22522268581993973
  batch 396 loss: 0.225099536994792
  batch 397 loss: 0.22511974750297797
  batch 398 loss: 0.2250441353974031
  batch 399 loss: 0.22509021306396426
  batch 400 loss: 0.2251209742948413
  batch 401 loss: 0.22501745039685409
  batch 402 loss: 0.22502968493682235
  batch 403 loss: 0.22509004711808991
  batch 404 loss: 0.2251074810576911
  batch 405 loss: 0.22509513835848113
  batch 406 loss: 0.2251825513951297
  batch 407 loss: 0.225235969157711
  batch 408 loss: 0.22527998857492326
  batch 409 loss: 0.22530747192267392
  batch 410 loss: 0.22538262994551078
  batch 411 loss: 0.225338404522325
  batch 412 loss: 0.2253412405047023
  batch 413 loss: 0.22548024803760844
  batch 414 loss: 0.22558233491032595
  batch 415 loss: 0.2255378702677876
  batch 416 loss: 0.22563741588965058
  batch 417 loss: 0.2256382041626411
  batch 418 loss: 0.22563001399405264
  batch 419 loss: 0.2256831408329397
  batch 420 loss: 0.22561076655983925
  batch 421 loss: 0.22550572900902346
  batch 422 loss: 0.2256422504358947
  batch 423 loss: 0.22561167613834354
  batch 424 loss: 0.2255655366154212
  batch 425 loss: 0.22546425500336814
  batch 426 loss: 0.2254011834941
  batch 427 loss: 0.22542323904936432
  batch 428 loss: 0.2255416883277559
  batch 429 loss: 0.22561172696399245
  batch 430 loss: 0.22563563900631528
  batch 431 loss: 0.22577930737952345
  batch 432 loss: 0.22587514382407622
  batch 433 loss: 0.2260190229561808
  batch 434 loss: 0.226165859208953
  batch 435 loss: 0.22618471010663044
  batch 436 loss: 0.2262236196035092
  batch 437 loss: 0.22640459955556988
  batch 438 loss: 0.2266062563753019
  batch 439 loss: 0.22665775440005342
  batch 440 loss: 0.22675361477515915
  batch 441 loss: 0.226671597077733
  batch 442 loss: 0.2266269738183302
  batch 443 loss: 0.22662960406619861
  batch 444 loss: 0.22658147797122732
  batch 445 loss: 0.22655939637275224
  batch 446 loss: 0.22656447981638758
  batch 447 loss: 0.22662817508448957
  batch 448 loss: 0.2266738046226757
  batch 449 loss: 0.22675675465694248
  batch 450 loss: 0.22680046326584286
  batch 451 loss: 0.22690703315111063
  batch 452 loss: 0.22709729885105537
  batch 453 loss: 0.22717276958966623
  batch 454 loss: 0.2272011937352004
  batch 455 loss: 0.22732514224864625
  batch 456 loss: 0.22742818678288082
  batch 457 loss: 0.22745760414480382
  batch 458 loss: 0.22745624285467847
  batch 459 loss: 0.2275688460254981
  batch 460 loss: 0.22771863875829657
  batch 461 loss: 0.22776887121402262
  batch 462 loss: 0.22779559589180595
  batch 463 loss: 0.22777335538848686
  batch 464 loss: 0.22780630320053677
  batch 465 loss: 0.22771770210676295
  batch 466 loss: 0.22767890666815344
  batch 467 loss: 0.22783083684056424
  batch 468 loss: 0.22782045351261768
  batch 469 loss: 0.22797039332293245
  batch 470 loss: 0.22795642256102663
  batch 471 loss: 0.22799789756852856
  batch 472 loss: 0.22787594675260076
LOSS train 0.22787594675260076 valid 0.29480260610580444
LOSS train 0.22787594675260076 valid 0.30119258165359497
LOSS train 0.22787594675260076 valid 0.2979169686635335
LOSS train 0.22787594675260076 valid 0.2966647297143936
LOSS train 0.22787594675260076 valid 0.29281423091888426
LOSS train 0.22787594675260076 valid 0.2951497087876002
LOSS train 0.22787594675260076 valid 0.30469654713358196
LOSS train 0.22787594675260076 valid 0.3035762496292591
LOSS train 0.22787594675260076 valid 0.30320601993136936
LOSS train 0.22787594675260076 valid 0.30534655451774595
LOSS train 0.22787594675260076 valid 0.30266429619355634
LOSS train 0.22787594675260076 valid 0.3022591720024745
LOSS train 0.22787594675260076 valid 0.2995672592749962
LOSS train 0.22787594675260076 valid 0.2985164054802486
LOSS train 0.22787594675260076 valid 0.29327478309472405
LOSS train 0.22787594675260076 valid 0.2936463365331292
LOSS train 0.22787594675260076 valid 0.29579490686164184
LOSS train 0.22787594675260076 valid 0.2983245857887798
LOSS train 0.22787594675260076 valid 0.30042444012667
LOSS train 0.22787594675260076 valid 0.29864813312888144
LOSS train 0.22787594675260076 valid 0.29793586546466466
LOSS train 0.22787594675260076 valid 0.29570193453268573
LOSS train 0.22787594675260076 valid 0.2964625799137613
LOSS train 0.22787594675260076 valid 0.294574083139499
LOSS train 0.22787594675260076 valid 0.29375825643539427
LOSS train 0.22787594675260076 valid 0.2934189152259093
LOSS train 0.22787594675260076 valid 0.29294552957570114
LOSS train 0.22787594675260076 valid 0.293100651885782
LOSS train 0.22787594675260076 valid 0.29262489285962334
LOSS train 0.22787594675260076 valid 0.294328639904658
LOSS train 0.22787594675260076 valid 0.2968210866374354
LOSS train 0.22787594675260076 valid 0.29729986749589443
LOSS train 0.22787594675260076 valid 0.2986491737943707
LOSS train 0.22787594675260076 valid 0.2989436440608081
LOSS train 0.22787594675260076 valid 0.30068841917174205
LOSS train 0.22787594675260076 valid 0.3006195541885164
LOSS train 0.22787594675260076 valid 0.30089662284464447
LOSS train 0.22787594675260076 valid 0.3022821655398921
LOSS train 0.22787594675260076 valid 0.3025214626238896
LOSS train 0.22787594675260076 valid 0.30265143513679504
LOSS train 0.22787594675260076 valid 0.3040597678684607
LOSS train 0.22787594675260076 valid 0.3048530384188607
LOSS train 0.22787594675260076 valid 0.3045093493406163
LOSS train 0.22787594675260076 valid 0.3049764497713609
LOSS train 0.22787594675260076 valid 0.30423418349689907
LOSS train 0.22787594675260076 valid 0.30519929139510443
LOSS train 0.22787594675260076 valid 0.30633770087932016
LOSS train 0.22787594675260076 valid 0.3067067575951417
LOSS train 0.22787594675260076 valid 0.3069825403544368
LOSS train 0.22787594675260076 valid 0.30592846930027007
LOSS train 0.22787594675260076 valid 0.30545632687269475
LOSS train 0.22787594675260076 valid 0.30523628340317654
LOSS train 0.22787594675260076 valid 0.30524987207268767
LOSS train 0.22787594675260076 valid 0.30525015736067734
LOSS train 0.22787594675260076 valid 0.3051195746118372
LOSS train 0.22787594675260076 valid 0.30433500185608864
LOSS train 0.22787594675260076 valid 0.30416375084927205
LOSS train 0.22787594675260076 valid 0.30349831488625756
LOSS train 0.22787594675260076 valid 0.30388524118116345
LOSS train 0.22787594675260076 valid 0.3039100895325343
LOSS train 0.22787594675260076 valid 0.30327527200589416
LOSS train 0.22787594675260076 valid 0.30477073740574623
LOSS train 0.22787594675260076 valid 0.3052929512092045
LOSS train 0.22787594675260076 valid 0.30641793040558696
LOSS train 0.22787594675260076 valid 0.30740114129506624
LOSS train 0.22787594675260076 valid 0.30734590915116394
LOSS train 0.22787594675260076 valid 0.30651766284188225
LOSS train 0.22787594675260076 valid 0.3068516210598104
LOSS train 0.22787594675260076 valid 0.3057073708893596
LOSS train 0.22787594675260076 valid 0.30616113032613484
LOSS train 0.22787594675260076 valid 0.3058844057606979
LOSS train 0.22787594675260076 valid 0.3062473063667615
LOSS train 0.22787594675260076 valid 0.3061378271612403
LOSS train 0.22787594675260076 valid 0.3060201221221202
LOSS train 0.22787594675260076 valid 0.30587186018625895
LOSS train 0.22787594675260076 valid 0.3063157643926771
LOSS train 0.22787594675260076 valid 0.3066052221632623
LOSS train 0.22787594675260076 valid 0.30722690392763186
LOSS train 0.22787594675260076 valid 0.3078661651550969
LOSS train 0.22787594675260076 valid 0.3069075457751751
LOSS train 0.22787594675260076 valid 0.3056903604739978
LOSS train 0.22787594675260076 valid 0.30621210303975316
LOSS train 0.22787594675260076 valid 0.30624746001628506
LOSS train 0.22787594675260076 valid 0.30601054802536964
LOSS train 0.22787594675260076 valid 0.30574361594284283
LOSS train 0.22787594675260076 valid 0.30500198658122574
LOSS train 0.22787594675260076 valid 0.30473688313330727
LOSS train 0.22787594675260076 valid 0.30399839681657875
LOSS train 0.22787594675260076 valid 0.30437850315919085
LOSS train 0.22787594675260076 valid 0.30489985446135204
LOSS train 0.22787594675260076 valid 0.3049789247932015
LOSS train 0.22787594675260076 valid 0.30532799369615055
LOSS train 0.22787594675260076 valid 0.3051736079236513
LOSS train 0.22787594675260076 valid 0.30532031648970664
LOSS train 0.22787594675260076 valid 0.30492404762067293
LOSS train 0.22787594675260076 valid 0.3056200776870052
LOSS train 0.22787594675260076 valid 0.30590067696325557
LOSS train 0.22787594675260076 valid 0.30636170993045886
LOSS train 0.22787594675260076 valid 0.30663656676658474
LOSS train 0.22787594675260076 valid 0.306582093834877
LOSS train 0.22787594675260076 valid 0.3068846868406428
LOSS train 0.22787594675260076 valid 0.30711517935874416
LOSS train 0.22787594675260076 valid 0.3070479237917558
LOSS train 0.22787594675260076 valid 0.3071816944732116
LOSS train 0.22787594675260076 valid 0.30728647623743327
LOSS train 0.22787594675260076 valid 0.30755054388406144
LOSS train 0.22787594675260076 valid 0.3072288974980328
LOSS train 0.22787594675260076 valid 0.3074177351814729
LOSS train 0.22787594675260076 valid 0.30791765342064953
LOSS train 0.22787594675260076 valid 0.3081986134702509
LOSS train 0.22787594675260076 valid 0.3075342726063084
LOSS train 0.22787594675260076 valid 0.30733824521303177
LOSS train 0.22787594675260076 valid 0.30726197124582477
LOSS train 0.22787594675260076 valid 0.3069715753459094
LOSS train 0.22787594675260076 valid 0.30738412452780683
LOSS train 0.22787594675260076 valid 0.3070362600786933
LOSS train 0.22787594675260076 valid 0.30714332293241453
LOSS train 0.22787594675260076 valid 0.3069757301423509
LOSS train 0.22787594675260076 valid 0.3069099844002924
LOSS train 0.22787594675260076 valid 0.30646869987249375
LOSS train 0.22787594675260076 valid 0.30648931836293747
LOSS train 0.22787594675260076 valid 0.3062453223545043
LOSS train 0.22787594675260076 valid 0.3062297730911069
LOSS train 0.22787594675260076 valid 0.3067535166778872
LOSS train 0.22787594675260076 valid 0.3066989197731018
LOSS train 0.22787594675260076 valid 0.3067544632487827
LOSS train 0.22787594675260076 valid 0.3068520412670346
LOSS train 0.22787594675260076 valid 0.3074335940182209
LOSS train 0.22787594675260076 valid 0.3075907636982526
LOSS train 0.22787594675260076 valid 0.30740796212966626
LOSS train 0.22787594675260076 valid 0.30733256895123545
LOSS train 0.22787594675260076 valid 0.3070702173493125
LOSS train 0.22787594675260076 valid 0.3073016505940516
LOSS train 0.22787594675260076 valid 0.30721640342207096
LOSS train 0.22787594675260076 valid 0.306987898879581
LOSS train 0.22787594675260076 valid 0.306772094219923
LOSS train 0.22787594675260076 valid 0.30654002127856234
LOSS train 0.22787594675260076 valid 0.30638509211332904
LOSS train 0.22787594675260076 valid 0.30620360460212764
LOSS train 0.22787594675260076 valid 0.3064133697322437
LOSS train 0.22787594675260076 valid 0.30639545397555573
LOSS train 0.22787594675260076 valid 0.306790517459453
LOSS train 0.22787594675260076 valid 0.3066307464679638
LOSS train 0.22787594675260076 valid 0.30646714692314464
LOSS train 0.22787594675260076 valid 0.3062255493525801
LOSS train 0.22787594675260076 valid 0.30639480483042053
LOSS train 0.22787594675260076 valid 0.3061363796393077
LOSS train 0.22787594675260076 valid 0.30720970779657364
LOSS train 0.22787594675260076 valid 0.3072553427027376
LOSS train 0.22787594675260076 valid 0.307462396423022
LOSS train 0.22787594675260076 valid 0.30755932875816394
LOSS train 0.22787594675260076 valid 0.3071495887676352
LOSS train 0.22787594675260076 valid 0.3072899795240826
LOSS train 0.22787594675260076 valid 0.3072327786645332
LOSS train 0.22787594675260076 valid 0.3073156136658884
LOSS train 0.22787594675260076 valid 0.3074304321064399
LOSS train 0.22787594675260076 valid 0.3072068082868673
LOSS train 0.22787594675260076 valid 0.30722952634096146
LOSS train 0.22787594675260076 valid 0.30731998815101647
LOSS train 0.22787594675260076 valid 0.3071983019821346
LOSS train 0.22787594675260076 valid 0.3070971842693246
LOSS train 0.22787594675260076 valid 0.30685542183525766
LOSS train 0.22787594675260076 valid 0.30663393529280564
LOSS train 0.22787594675260076 valid 0.30605651392805866
LOSS train 0.22787594675260076 valid 0.3057071173732931
LOSS train 0.22787594675260076 valid 0.3057122755661068
LOSS train 0.22787594675260076 valid 0.3061526613142676
LOSS train 0.22787594675260076 valid 0.30593220916177544
LOSS train 0.22787594675260076 valid 0.3060881113688621
LOSS train 0.22787594675260076 valid 0.3062219014062601
LOSS train 0.22787594675260076 valid 0.30625501492915796
LOSS train 0.22787594675260076 valid 0.30600046617693677
LOSS train 0.22787594675260076 valid 0.30593966740059714
LOSS train 0.22787594675260076 valid 0.30593703267560607
LOSS train 0.22787594675260076 valid 0.3056178344147546
LOSS train 0.22787594675260076 valid 0.3055681963027878
LOSS train 0.22787594675260076 valid 0.3054946187020695
LOSS train 0.22787594675260076 valid 0.30559052987379975
LOSS train 0.22787594675260076 valid 0.3054619908499318
LOSS train 0.22787594675260076 valid 0.3053416921860642
LOSS train 0.22787594675260076 valid 0.30546194240862495
LOSS train 0.22787594675260076 valid 0.30564945551392797
LOSS train 0.22787594675260076 valid 0.30570738864401
LOSS train 0.22787594675260076 valid 0.30566552516234957
LOSS train 0.22787594675260076 valid 0.30524804439093617
LOSS train 0.22787594675260076 valid 0.30513457064667054
LOSS train 0.22787594675260076 valid 0.3050134126993424
LOSS train 0.22787594675260076 valid 0.3051094354942758
LOSS train 0.22787594675260076 valid 0.30506294295585973
LOSS train 0.22787594675260076 valid 0.3052057127419271
LOSS train 0.22787594675260076 valid 0.3050345546756115
LOSS train 0.22787594675260076 valid 0.30500003454896313
LOSS train 0.22787594675260076 valid 0.3048836305505871
LOSS train 0.22787594675260076 valid 0.3046369988125624
LOSS train 0.22787594675260076 valid 0.3043691233946727
LOSS train 0.22787594675260076 valid 0.30422642682583967
LOSS train 0.22787594675260076 valid 0.3044373454327511
LOSS train 0.22787594675260076 valid 0.30410117866716
LOSS train 0.22787594675260076 valid 0.3041882556137727
LOSS train 0.22787594675260076 valid 0.30410457275807856
LOSS train 0.22787594675260076 valid 0.30399816968844307
LOSS train 0.22787594675260076 valid 0.3038093888080946
LOSS train 0.22787594675260076 valid 0.30360311967105114
LOSS train 0.22787594675260076 valid 0.3035345203882339
LOSS train 0.22787594675260076 valid 0.30317983721814507
LOSS train 0.22787594675260076 valid 0.30331867824769715
LOSS train 0.22787594675260076 valid 0.30327618834765063
LOSS train 0.22787594675260076 valid 0.30305283337544936
LOSS train 0.22787594675260076 valid 0.30287198708103036
LOSS train 0.22787594675260076 valid 0.30292450068962007
LOSS train 0.22787594675260076 valid 0.30309088471658985
LOSS train 0.22787594675260076 valid 0.30310201595695513
LOSS train 0.22787594675260076 valid 0.30338979767801616
LOSS train 0.22787594675260076 valid 0.30347458941635685
LOSS train 0.22787594675260076 valid 0.30329718846221304
LOSS train 0.22787594675260076 valid 0.30323717198162164
LOSS train 0.22787594675260076 valid 0.3033017249013971
LOSS train 0.22787594675260076 valid 0.30328729823915235
LOSS train 0.22787594675260076 valid 0.3034699772182665
LOSS train 0.22787594675260076 valid 0.3035836956040426
LOSS train 0.22787594675260076 valid 0.3035605426422611
LOSS train 0.22787594675260076 valid 0.30342235443022875
LOSS train 0.22787594675260076 valid 0.3036049098578269
LOSS train 0.22787594675260076 valid 0.3036408492896174
LOSS train 0.22787594675260076 valid 0.3035252398252487
LOSS train 0.22787594675260076 valid 0.30356336899299535
LOSS train 0.22787594675260076 valid 0.30380810521510204
LOSS train 0.22787594675260076 valid 0.30387726249663455
LOSS train 0.22787594675260076 valid 0.3039064940276625
LOSS train 0.22787594675260076 valid 0.30398065506116206
LOSS train 0.22787594675260076 valid 0.3039953967322519
LOSS train 0.22787594675260076 valid 0.30399232736692344
LOSS train 0.22787594675260076 valid 0.303920755762399
LOSS train 0.22787594675260076 valid 0.3038595404762488
LOSS train 0.22787594675260076 valid 0.3041773184182796
LOSS train 0.22787594675260076 valid 0.30391095837546606
LOSS train 0.22787594675260076 valid 0.30414190630621046
LOSS train 0.22787594675260076 valid 0.3042785151540732
LOSS train 0.22787594675260076 valid 0.3041151695296355
LOSS train 0.22787594675260076 valid 0.30411571407069765
LOSS train 0.22787594675260076 valid 0.30419111962882317
LOSS train 0.22787594675260076 valid 0.3039097498394241
LOSS train 0.22787594675260076 valid 0.30416461095643144
LOSS train 0.22787594675260076 valid 0.30460168830439693
LOSS train 0.22787594675260076 valid 0.30487368064267295
LOSS train 0.22787594675260076 valid 0.3048342761348903
LOSS train 0.22787594675260076 valid 0.3047912100910658
LOSS train 0.22787594675260076 valid 0.30469502478597627
LOSS train 0.22787594675260076 valid 0.30454879418673764
LOSS train 0.22787594675260076 valid 0.30484116381406784
LOSS train 0.22787594675260076 valid 0.3049858556206958
LOSS train 0.22787594675260076 valid 0.30521533868852113
LOSS train 0.22787594675260076 valid 0.30515575096776837
LOSS train 0.22787594675260076 valid 0.3050233433683088
LOSS train 0.22787594675260076 valid 0.3050727767687218
LOSS train 0.22787594675260076 valid 0.3051960729644634
LOSS train 0.22787594675260076 valid 0.30508258527123044
LOSS train 0.22787594675260076 valid 0.3052194177988888
LOSS train 0.22787594675260076 valid 0.3052391808580708
LOSS train 0.22787594675260076 valid 0.3051821371110586
LOSS train 0.22787594675260076 valid 0.30535613925292576
LOSS train 0.22787594675260076 valid 0.30537569301046485
LOSS train 0.22787594675260076 valid 0.3054162215710593
LOSS train 0.22787594675260076 valid 0.30538944679905067
LOSS train 0.22787594675260076 valid 0.3052533922892696
LOSS train 0.22787594675260076 valid 0.30544066445944007
LOSS train 0.22787594675260076 valid 0.30551770826180774
LOSS train 0.22787594675260076 valid 0.3056917919485427
LOSS train 0.22787594675260076 valid 0.30561749371232594
LOSS train 0.22787594675260076 valid 0.3054906746303594
LOSS train 0.22787594675260076 valid 0.3056414926294031
LOSS train 0.22787594675260076 valid 0.3059300214380902
LOSS train 0.22787594675260076 valid 0.3060133717544786
LOSS train 0.22787594675260076 valid 0.3059370159341471
LOSS train 0.22787594675260076 valid 0.30586580336093905
LOSS train 0.22787594675260076 valid 0.3056758563911569
LOSS train 0.22787594675260076 valid 0.3055556245982001
LOSS train 0.22787594675260076 valid 0.3053640321754723
LOSS train 0.22787594675260076 valid 0.3054128430757044
LOSS train 0.22787594675260076 valid 0.3053544833723988
LOSS train 0.22787594675260076 valid 0.3051489435693123
LOSS train 0.22787594675260076 valid 0.3049197265653745
LOSS train 0.22787594675260076 valid 0.30480859694548296
LOSS train 0.22787594675260076 valid 0.3048969331971357
LOSS train 0.22787594675260076 valid 0.3051102321398886
LOSS train 0.22787594675260076 valid 0.3050441138602637
LOSS train 0.22787594675260076 valid 0.3049774617475915
LOSS train 0.22787594675260076 valid 0.3049491171000732
LOSS train 0.22787594675260076 valid 0.3051319238842565
LOSS train 0.22787594675260076 valid 0.30516983137048526
LOSS train 0.22787594675260076 valid 0.305094360485929
LOSS train 0.22787594675260076 valid 0.30522329748085103
LOSS train 0.22787594675260076 valid 0.30521366905433733
LOSS train 0.22787594675260076 valid 0.3053371977238428
LOSS train 0.22787594675260076 valid 0.3054221393698353
LOSS train 0.22787594675260076 valid 0.3053189636887731
LOSS train 0.22787594675260076 valid 0.3053572484941194
LOSS train 0.22787594675260076 valid 0.3052840391861512
LOSS train 0.22787594675260076 valid 0.3052717944650746
LOSS train 0.22787594675260076 valid 0.3051614034175873
LOSS train 0.22787594675260076 valid 0.3051968791160077
LOSS train 0.22787594675260076 valid 0.3052269602256105
LOSS train 0.22787594675260076 valid 0.30535526362189364
LOSS train 0.22787594675260076 valid 0.3053234370523377
LOSS train 0.22787594675260076 valid 0.3052601614936453
LOSS train 0.22787594675260076 valid 0.3052102088733436
LOSS train 0.22787594675260076 valid 0.3051189693837678
LOSS train 0.22787594675260076 valid 0.3050436998729582
LOSS train 0.22787594675260076 valid 0.304975575903087
LOSS train 0.22787594675260076 valid 0.30495522781725853
LOSS train 0.22787594675260076 valid 0.3048762172173074
LOSS train 0.22787594675260076 valid 0.3048497159511615
LOSS train 0.22787594675260076 valid 0.3047879953353931
LOSS train 0.22787594675260076 valid 0.3048990879468857
LOSS train 0.22787594675260076 valid 0.3048591158692799
LOSS train 0.22787594675260076 valid 0.30470855015365383
LOSS train 0.22787594675260076 valid 0.30479640708736816
LOSS train 0.22787594675260076 valid 0.30491250266069136
LOSS train 0.22787594675260076 valid 0.3050559614332492
LOSS train 0.22787594675260076 valid 0.30499088345095515
LOSS train 0.22787594675260076 valid 0.30526953787075767
LOSS train 0.22787594675260076 valid 0.30509808761362706
LOSS train 0.22787594675260076 valid 0.3049595017366734
LOSS train 0.22787594675260076 valid 0.3050464390788549
LOSS train 0.22787594675260076 valid 0.3051344458873455
LOSS train 0.22787594675260076 valid 0.3051876959625197
LOSS train 0.22787594675260076 valid 0.3052449683895169
LOSS train 0.22787594675260076 valid 0.3051953821829179
LOSS train 0.22787594675260076 valid 0.3052637754602635
LOSS train 0.22787594675260076 valid 0.305319507374908
LOSS train 0.22787594675260076 valid 0.305177385083138
LOSS train 0.22787594675260076 valid 0.30513620699744626
LOSS train 0.22787594675260076 valid 0.30508538606288554
LOSS train 0.22787594675260076 valid 0.3052604128857573
LOSS train 0.22787594675260076 valid 0.3052240952626983
LOSS train 0.22787594675260076 valid 0.30499501601748524
LOSS train 0.22787594675260076 valid 0.3049639775155206
LOSS train 0.22787594675260076 valid 0.3048902581164823
LOSS train 0.22787594675260076 valid 0.304781894988015
LOSS train 0.22787594675260076 valid 0.30476274933008585
LOSS train 0.22787594675260076 valid 0.304623381876526
LOSS train 0.22787594675260076 valid 0.3045861486193032
LOSS train 0.22787594675260076 valid 0.304461401118829
LOSS train 0.22787594675260076 valid 0.3046452541250822
LOSS train 0.22787594675260076 valid 0.30470418588838716
LOSS train 0.22787594675260076 valid 0.3045905075235174
LOSS train 0.22787594675260076 valid 0.3043859907203174
LOSS train 0.22787594675260076 valid 0.3042681983233183
LOSS train 0.22787594675260076 valid 0.3043196991641064
LOSS train 0.22787594675260076 valid 0.30419772450413024
LOSS train 0.22787594675260076 valid 0.30415808080438195
LOSS train 0.22787594675260076 valid 0.30410453656011005
LOSS train 0.22787594675260076 valid 0.30408990598434094
LOSS train 0.22787594675260076 valid 0.3041113498436529
LOSS train 0.22787594675260076 valid 0.30433378677133105
LOSS train 0.22787594675260076 valid 0.3044379082707207
LOSS train 0.22787594675260076 valid 0.3044306400025926
LOSS train 0.22787594675260076 valid 0.3044163114781486
LOSS train 0.22787594675260076 valid 0.30443946867766153
LOSS train 0.22787594675260076 valid 0.30441983180741466
LOSS train 0.22787594675260076 valid 0.30444012739156423
LOSS train 0.22787594675260076 valid 0.3045822688640811
LOSS train 0.22787594675260076 valid 0.30443218305255426
LOSS train 0.22787594675260076 valid 0.3043889666569757
LOSS train 0.22787594675260076 valid 0.3044527686213794
LOSS train 0.22787594675260076 valid 0.3044197929442906
LOSS train 0.22787594675260076 valid 0.3042709031205736
LOSS train 0.22787594675260076 valid 0.3042747931878852
LOSS train 0.22787594675260076 valid 0.3044292815736316
EPOCH 23:
  batch 1 loss: 0.2892928123474121
  batch 2 loss: 0.2505992352962494
  batch 3 loss: 0.2453740338484446
  batch 4 loss: 0.25002793967723846
  batch 5 loss: 0.2522980093955994
  batch 6 loss: 0.24911131213108698
  batch 7 loss: 0.2501457525151117
  batch 8 loss: 0.25467156805098057
  batch 9 loss: 0.25436829030513763
  batch 10 loss: 0.25340299159288404
  batch 11 loss: 0.2531573081558401
  batch 12 loss: 0.24937453245123228
  batch 13 loss: 0.2470039243881519
  batch 14 loss: 0.24600280395575932
  batch 15 loss: 0.24642548561096192
  batch 16 loss: 0.24487737007439137
  batch 17 loss: 0.24067257520030527
  batch 18 loss: 0.24031885051065022
  batch 19 loss: 0.23694312572479248
  batch 20 loss: 0.2343124806880951
  batch 21 loss: 0.23389442548865363
  batch 22 loss: 0.23288857733661478
  batch 23 loss: 0.23276066067426102
  batch 24 loss: 0.23105721858640513
  batch 25 loss: 0.23158861815929413
  batch 26 loss: 0.22951179914749587
  batch 27 loss: 0.22983903796584518
  batch 28 loss: 0.22856522724032402
  batch 29 loss: 0.22841310192798747
  batch 30 loss: 0.22803365488847097
  batch 31 loss: 0.2289340053835223
  batch 32 loss: 0.22894668951630592
  batch 33 loss: 0.228988429813674
  batch 34 loss: 0.2277026785647168
  batch 35 loss: 0.2277650283915656
  batch 36 loss: 0.22844682095779312
  batch 37 loss: 0.2276833246688585
  batch 38 loss: 0.2282081575770127
  batch 39 loss: 0.22788846683807862
  batch 40 loss: 0.22857465110719205
  batch 41 loss: 0.22839066277189954
  batch 42 loss: 0.22870023122855596
  batch 43 loss: 0.22911333240741907
  batch 44 loss: 0.22857847200198608
  batch 45 loss: 0.22801576885912153
  batch 46 loss: 0.2283953541646833
  batch 47 loss: 0.22789771284194701
  batch 48 loss: 0.22703160531818867
  batch 49 loss: 0.22684915394199137
  batch 50 loss: 0.22690990209579467
  batch 51 loss: 0.22691930567517
  batch 52 loss: 0.22719920197358498
  batch 53 loss: 0.22691259968955563
  batch 54 loss: 0.22697523760574836
  batch 55 loss: 0.2263680089603771
  batch 56 loss: 0.22594736995441572
  batch 57 loss: 0.2256175355430235
  batch 58 loss: 0.22623228744186205
  batch 59 loss: 0.2267377192186097
  batch 60 loss: 0.226878272742033
  batch 61 loss: 0.22737743107021832
  batch 62 loss: 0.22781100412530284
  batch 63 loss: 0.2271355130369701
  batch 64 loss: 0.22751357313245535
  batch 65 loss: 0.22759017462913805
  batch 66 loss: 0.2277457657637018
  batch 67 loss: 0.22843731761868322
  batch 68 loss: 0.22861753886236863
  batch 69 loss: 0.22920525635498157
  batch 70 loss: 0.22988014902387346
  batch 71 loss: 0.2297974256142764
  batch 72 loss: 0.22990447965761027
  batch 73 loss: 0.2302174884570788
  batch 74 loss: 0.230121038048654
  batch 75 loss: 0.22972685833772025
  batch 76 loss: 0.2304068421454806
  batch 77 loss: 0.22985566378413858
  batch 78 loss: 0.2301077160697717
  batch 79 loss: 0.23042127546630328
  batch 80 loss: 0.23029103633016348
  batch 81 loss: 0.23064545220063057
  batch 82 loss: 0.2313188795999783
  batch 83 loss: 0.23113588701529675
  batch 84 loss: 0.23082876436057545
  batch 85 loss: 0.23059495494646184
  batch 86 loss: 0.23146020621061325
  batch 87 loss: 0.23140825365466633
  batch 88 loss: 0.23112441870299252
  batch 89 loss: 0.23136122441023924
  batch 90 loss: 0.23165862262248993
  batch 91 loss: 0.23153105672899182
  batch 92 loss: 0.23124909012213998
  batch 93 loss: 0.2314560596019991
  batch 94 loss: 0.2318058191461766
  batch 95 loss: 0.23145446071499273
  batch 96 loss: 0.23131178334976235
  batch 97 loss: 0.23137180476459032
  batch 98 loss: 0.23159460556142183
  batch 99 loss: 0.23192958533763885
  batch 100 loss: 0.2316158004105091
  batch 101 loss: 0.23146323136763997
  batch 102 loss: 0.23187602472071553
  batch 103 loss: 0.23230783742608377
  batch 104 loss: 0.23249900484314331
  batch 105 loss: 0.23227752645810446
  batch 106 loss: 0.23244663145182268
  batch 107 loss: 0.2322712217535928
  batch 108 loss: 0.23273264754701545
  batch 109 loss: 0.232668397076633
  batch 110 loss: 0.23282595601948824
  batch 111 loss: 0.23304440497278092
  batch 112 loss: 0.23299059391553914
  batch 113 loss: 0.2330551330758407
  batch 114 loss: 0.23326028844243601
  batch 115 loss: 0.23334387579689855
  batch 116 loss: 0.2336935368848258
  batch 117 loss: 0.23387928205168146
  batch 118 loss: 0.23365886282112638
  batch 119 loss: 0.23364121385482178
  batch 120 loss: 0.2332066711038351
  batch 121 loss: 0.23309763046828183
  batch 122 loss: 0.2330939702567507
  batch 123 loss: 0.2330338197752712
  batch 124 loss: 0.23326904290626127
  batch 125 loss: 0.23296507036685943
  batch 126 loss: 0.2328990920195504
  batch 127 loss: 0.23312122568370788
  batch 128 loss: 0.23291477793827653
  batch 129 loss: 0.2331266280754592
  batch 130 loss: 0.23292646935352912
  batch 131 loss: 0.2328711861872491
  batch 132 loss: 0.23279837439909126
  batch 133 loss: 0.2330894491502217
  batch 134 loss: 0.23314826501839198
  batch 135 loss: 0.23287724068871252
  batch 136 loss: 0.23288875949733398
  batch 137 loss: 0.23283335327231972
  batch 138 loss: 0.23291583542806515
  batch 139 loss: 0.23305117108410212
  batch 140 loss: 0.2329877022121634
  batch 141 loss: 0.2331691567145341
  batch 142 loss: 0.23308951470633626
  batch 143 loss: 0.23277031145729385
  batch 144 loss: 0.23274793413778147
  batch 145 loss: 0.23268219232559204
  batch 146 loss: 0.2325362617226496
  batch 147 loss: 0.23281296651785066
  batch 148 loss: 0.23278518665481257
  batch 149 loss: 0.23255091445558024
  batch 150 loss: 0.23238984882831573
  batch 151 loss: 0.23222781974353537
  batch 152 loss: 0.232346593647411
  batch 153 loss: 0.23207574099107506
  batch 154 loss: 0.2321173089084687
  batch 155 loss: 0.23199987046180232
  batch 156 loss: 0.23182247645961931
  batch 157 loss: 0.2320297769490321
  batch 158 loss: 0.23192313426657568
  batch 159 loss: 0.23181092420464042
  batch 160 loss: 0.23149199616163968
  batch 161 loss: 0.23159705046911416
  batch 162 loss: 0.23141805202136806
  batch 163 loss: 0.23135808073669856
  batch 164 loss: 0.23107407905343103
  batch 165 loss: 0.23093849971438898
  batch 166 loss: 0.23095425397875796
  batch 167 loss: 0.23087275545754118
  batch 168 loss: 0.23078227433420362
  batch 169 loss: 0.2306232123508961
  batch 170 loss: 0.23041190987124163
  batch 171 loss: 0.23023298391473224
  batch 172 loss: 0.23034624824690264
  batch 173 loss: 0.23026947677135468
  batch 174 loss: 0.2303151398383338
  batch 175 loss: 0.23050536045006342
  batch 176 loss: 0.2305305346169255
  batch 177 loss: 0.23058174601045706
  batch 178 loss: 0.23066152431321948
  batch 179 loss: 0.23073407391596107
  batch 180 loss: 0.23057345839010346
  batch 181 loss: 0.23081527880871494
  batch 182 loss: 0.23075193191295143
  batch 183 loss: 0.23059470585135164
  batch 184 loss: 0.23061507073757442
  batch 185 loss: 0.23081051425353902
  batch 186 loss: 0.23094251968206897
  batch 187 loss: 0.2307484037576512
  batch 188 loss: 0.23042993968788614
  batch 189 loss: 0.23015785879558986
  batch 190 loss: 0.2300863946738996
  batch 191 loss: 0.22998304095567834
  batch 192 loss: 0.22980761885022125
  batch 193 loss: 0.22960431757986235
  batch 194 loss: 0.22970569940264693
  batch 195 loss: 0.229787974861952
  batch 196 loss: 0.2295429780319029
  batch 197 loss: 0.2295346753246288
  batch 198 loss: 0.22973685869664856
  batch 199 loss: 0.22953277541764416
  batch 200 loss: 0.22984660357236864
  batch 201 loss: 0.2299437382031436
  batch 202 loss: 0.22998257637909142
  batch 203 loss: 0.22990817771169353
  batch 204 loss: 0.22985709560852424
  batch 205 loss: 0.23003723839434181
  batch 206 loss: 0.23008905235424781
  batch 207 loss: 0.23019853119113018
  batch 208 loss: 0.23031950025604322
  batch 209 loss: 0.23021271696501372
  batch 210 loss: 0.23028717502242044
  batch 211 loss: 0.2301092200087145
  batch 212 loss: 0.22998615108289808
  batch 213 loss: 0.22985897664452942
  batch 214 loss: 0.2297829840088559
  batch 215 loss: 0.22960878007633742
  batch 216 loss: 0.229320018172816
  batch 217 loss: 0.22926043234937202
  batch 218 loss: 0.22926447895962163
  batch 219 loss: 0.22930544471904024
  batch 220 loss: 0.22924226461486383
  batch 221 loss: 0.22934456129149614
  batch 222 loss: 0.22929562581283552
  batch 223 loss: 0.22914942537722566
  batch 224 loss: 0.22901560120018466
  batch 225 loss: 0.2289399171537823
  batch 226 loss: 0.22895571741118895
  batch 227 loss: 0.22885282784043956
  batch 228 loss: 0.22877900237054155
  batch 229 loss: 0.22879376031425844
  batch 230 loss: 0.22889604581438977
  batch 231 loss: 0.22878990157858117
  batch 232 loss: 0.228657527409237
  batch 233 loss: 0.22858321327764078
  batch 234 loss: 0.22851014621237403
  batch 235 loss: 0.2284512665677578
  batch 236 loss: 0.22826886398054785
  batch 237 loss: 0.22831546147412893
  batch 238 loss: 0.22837674461242533
  batch 239 loss: 0.22825466495428126
  batch 240 loss: 0.22808484143267074
  batch 241 loss: 0.22817492788016056
  batch 242 loss: 0.22821735467546242
  batch 243 loss: 0.2282097283214208
  batch 244 loss: 0.22811202423982932
  batch 245 loss: 0.22801320607564887
  batch 246 loss: 0.22811240836129926
  batch 247 loss: 0.22818554503473676
  batch 248 loss: 0.22816120448612398
  batch 249 loss: 0.2280820360983232
  batch 250 loss: 0.22803145503997801
  batch 251 loss: 0.22805354385024523
  batch 252 loss: 0.22792764025784673
  batch 253 loss: 0.22782259160586496
  batch 254 loss: 0.22774203344592897
  batch 255 loss: 0.22779939829134474
  batch 256 loss: 0.22790871211327612
  batch 257 loss: 0.2278633085206325
  batch 258 loss: 0.22775067833735962
  batch 259 loss: 0.22767941694001895
  batch 260 loss: 0.22765349229941
  batch 261 loss: 0.22762626215415896
  batch 262 loss: 0.22759582034049144
  batch 263 loss: 0.22772346272668004
  batch 264 loss: 0.22765731901833505
  batch 265 loss: 0.22745377747517712
  batch 266 loss: 0.2274410637249624
  batch 267 loss: 0.2275214528434732
  batch 268 loss: 0.2273792459337569
  batch 269 loss: 0.2273809985711229
  batch 270 loss: 0.2275887639986144
  batch 271 loss: 0.22754797465906812
  batch 272 loss: 0.2275426431065973
  batch 273 loss: 0.22760703815863684
  batch 274 loss: 0.22765004129087837
  batch 275 loss: 0.22759382345459678
  batch 276 loss: 0.2274862822091234
  batch 277 loss: 0.22748272572828976
  batch 278 loss: 0.2273927064572307
  batch 279 loss: 0.2273703843996089
  batch 280 loss: 0.22723643923444406
  batch 281 loss: 0.22716624440883826
  batch 282 loss: 0.22722843511307494
  batch 283 loss: 0.22709286649951665
  batch 284 loss: 0.2270799722574966
  batch 285 loss: 0.22702738538123013
  batch 286 loss: 0.2269740634031229
  batch 287 loss: 0.2269692978584808
  batch 288 loss: 0.22688617733203703
  batch 289 loss: 0.22699093756791217
  batch 290 loss: 0.2268093605493677
  batch 291 loss: 0.22682518954948871
  batch 292 loss: 0.22686701783374563
  batch 293 loss: 0.22678603709964623
  batch 294 loss: 0.2267247631537671
  batch 295 loss: 0.2266960844650107
  batch 296 loss: 0.22675226221012101
  batch 297 loss: 0.22667206051173033
  batch 298 loss: 0.22656925152612212
  batch 299 loss: 0.2264864517494189
  batch 300 loss: 0.22645007332166037
  batch 301 loss: 0.22645489619023776
  batch 302 loss: 0.22631861072107656
  batch 303 loss: 0.22614821526083614
  batch 304 loss: 0.2260537354863788
  batch 305 loss: 0.2258622854459481
  batch 306 loss: 0.22595123771358938
  batch 307 loss: 0.2259125453738514
  batch 308 loss: 0.2259515943852338
  batch 309 loss: 0.225910411104801
  batch 310 loss: 0.22576965093612672
  batch 311 loss: 0.2258462601152647
  batch 312 loss: 0.22593671838060403
  batch 313 loss: 0.22596640451647604
  batch 314 loss: 0.22586614757206788
  batch 315 loss: 0.22585835636608184
  batch 316 loss: 0.22577605594562578
  batch 317 loss: 0.22570630099495126
  batch 318 loss: 0.22569846643029517
  batch 319 loss: 0.22566030868168535
  batch 320 loss: 0.2255672050639987
  batch 321 loss: 0.2255475918451945
  batch 322 loss: 0.22545789436709066
  batch 323 loss: 0.2253475804631555
  batch 324 loss: 0.22518097222955138
  batch 325 loss: 0.22508322779948894
  batch 326 loss: 0.2250381453545547
  batch 327 loss: 0.2250738594145585
  batch 328 loss: 0.22497539476650516
  batch 329 loss: 0.2249529991287591
  batch 330 loss: 0.22485445340474447
  batch 331 loss: 0.22488323056625817
  batch 332 loss: 0.2249121000788298
  batch 333 loss: 0.2250124629553374
  batch 334 loss: 0.22486036770536513
  batch 335 loss: 0.22485046947180334
  batch 336 loss: 0.2249795969220854
  batch 337 loss: 0.22501946658515082
  batch 338 loss: 0.22500488475053268
  batch 339 loss: 0.2249386371236987
  batch 340 loss: 0.22500705911832697
  batch 341 loss: 0.22517495788087594
  batch 342 loss: 0.2251098475783889
  batch 343 loss: 0.22512790773596084
  batch 344 loss: 0.2251193195927975
  batch 345 loss: 0.2252055264901424
  batch 346 loss: 0.22521593951420976
  batch 347 loss: 0.2252018549600321
  batch 348 loss: 0.22514879891927214
  batch 349 loss: 0.22503363958060912
  batch 350 loss: 0.22511308457170215
  batch 351 loss: 0.22516439812645273
  batch 352 loss: 0.22518729719079353
  batch 353 loss: 0.22512756165634135
  batch 354 loss: 0.22527903411011238
  batch 355 loss: 0.2252633342440699
  batch 356 loss: 0.22517721145675423
  batch 357 loss: 0.22514317428865352
  batch 358 loss: 0.22522916120357353
  batch 359 loss: 0.22520372325497418
  batch 360 loss: 0.22522556599643495
  batch 361 loss: 0.22518736384086663
  batch 362 loss: 0.22510024481221458
  batch 363 loss: 0.22495524396416897
  batch 364 loss: 0.22488743251005372
  batch 365 loss: 0.22490431694951776
  batch 366 loss: 0.22484842797771828
  batch 367 loss: 0.22479840199531587
  batch 368 loss: 0.2247081917265187
  batch 369 loss: 0.22467047557598208
  batch 370 loss: 0.22459971703387596
  batch 371 loss: 0.22463624568962343
  batch 372 loss: 0.22454924609071464
  batch 373 loss: 0.22442016181293825
  batch 374 loss: 0.2243493132811179
  batch 375 loss: 0.2242582967678706
  batch 376 loss: 0.22419917880696186
  batch 377 loss: 0.22420386875181678
  batch 378 loss: 0.2241126665126079
  batch 379 loss: 0.22404907573024327
  batch 380 loss: 0.224085562519337
  batch 381 loss: 0.2240450991576738
  batch 382 loss: 0.22401461722963142
  batch 383 loss: 0.22404912900053178
  batch 384 loss: 0.22399019453829774
  batch 385 loss: 0.22406609813114264
  batch 386 loss: 0.22403094987023062
  batch 387 loss: 0.2241823032921907
  batch 388 loss: 0.22421166999745615
  batch 389 loss: 0.2241560171686285
  batch 390 loss: 0.22409339849001322
  batch 391 loss: 0.2241193314876093
  batch 392 loss: 0.22410974754210639
  batch 393 loss: 0.22401498159531116
  batch 394 loss: 0.22400975737928738
  batch 395 loss: 0.22397458738164056
  batch 396 loss: 0.22388226527607802
  batch 397 loss: 0.22393015259759552
  batch 398 loss: 0.22381409077937878
  batch 399 loss: 0.22383883143576763
  batch 400 loss: 0.22389820575714112
  batch 401 loss: 0.223806456734712
  batch 402 loss: 0.22378646978987984
  batch 403 loss: 0.2238301815968885
  batch 404 loss: 0.22388823643916905
  batch 405 loss: 0.22394018416051512
  batch 406 loss: 0.22394721088885086
  batch 407 loss: 0.22397359579581885
  batch 408 loss: 0.2240296652650132
  batch 409 loss: 0.22407744981548897
  batch 410 loss: 0.2242547498243611
  batch 411 loss: 0.22424311040381736
  batch 412 loss: 0.22424634734114396
  batch 413 loss: 0.22441507792934667
  batch 414 loss: 0.2245270905719287
  batch 415 loss: 0.22457475927938897
  batch 416 loss: 0.22463489685637447
  batch 417 loss: 0.22465537963725393
  batch 418 loss: 0.22468503429131073
  batch 419 loss: 0.22486619196103125
  batch 420 loss: 0.22483923495525404
  batch 421 loss: 0.22474947808898826
  batch 422 loss: 0.22489655063756833
  batch 423 loss: 0.22490954349790623
  batch 424 loss: 0.2249361256242923
  batch 425 loss: 0.22491913777940414
  batch 426 loss: 0.22486584234825321
  batch 427 loss: 0.22486164174816928
  batch 428 loss: 0.2250262324776605
  batch 429 loss: 0.22510417385812684
  batch 430 loss: 0.2250888248169145
  batch 431 loss: 0.2251394183273382
  batch 432 loss: 0.22515208826020913
  batch 433 loss: 0.2252183935796416
  batch 434 loss: 0.22529291110928706
  batch 435 loss: 0.22532080631146487
  batch 436 loss: 0.22533853563967102
  batch 437 loss: 0.2253818021281633
  batch 438 loss: 0.22550694538986302
  batch 439 loss: 0.2255554169687975
  batch 440 loss: 0.22564310583878647
  batch 441 loss: 0.22559749890887548
  batch 442 loss: 0.22552855929772778
  batch 443 loss: 0.22551050297831843
  batch 444 loss: 0.2254983540792186
  batch 445 loss: 0.2255228384157245
  batch 446 loss: 0.22544405545888996
  batch 447 loss: 0.2254066094922806
  batch 448 loss: 0.22554162493906915
  batch 449 loss: 0.22558613947876843
  batch 450 loss: 0.22555226153797572
  batch 451 loss: 0.22554728884390346
  batch 452 loss: 0.2257831521925673
  batch 453 loss: 0.22589728111199722
  batch 454 loss: 0.2259218324845583
  batch 455 loss: 0.2259650476358749
  batch 456 loss: 0.22602507246560172
  batch 457 loss: 0.22607219802863748
  batch 458 loss: 0.22608160783890555
  batch 459 loss: 0.2262019303369626
  batch 460 loss: 0.22628498368937036
  batch 461 loss: 0.22628579726705322
  batch 462 loss: 0.22626726900215272
  batch 463 loss: 0.22629020438724667
  batch 464 loss: 0.22633277624845505
  batch 465 loss: 0.22626593513514406
  batch 466 loss: 0.22615666649116467
  batch 467 loss: 0.22630306772042139
  batch 468 loss: 0.22626701022824672
  batch 469 loss: 0.226354363567031
  batch 470 loss: 0.22627932904882633
  batch 471 loss: 0.2263238242097721
  batch 472 loss: 0.2261959945751449
LOSS train 0.2261959945751449 valid 0.3729366660118103
LOSS train 0.2261959945751449 valid 0.3740662634372711
LOSS train 0.2261959945751449 valid 0.3640580077966054
LOSS train 0.2261959945751449 valid 0.36487747728824615
LOSS train 0.2261959945751449 valid 0.3609460651874542
LOSS train 0.2261959945751449 valid 0.3664395213127136
LOSS train 0.2261959945751449 valid 0.377352706023625
LOSS train 0.2261959945751449 valid 0.3772532194852829
LOSS train 0.2261959945751449 valid 0.37739389803674483
LOSS train 0.2261959945751449 valid 0.3801027566194534
LOSS train 0.2261959945751449 valid 0.377238005399704
LOSS train 0.2261959945751449 valid 0.3763089006145795
LOSS train 0.2261959945751449 valid 0.3741056529375223
LOSS train 0.2261959945751449 valid 0.3730728860412325
LOSS train 0.2261959945751449 valid 0.36674967606862385
LOSS train 0.2261959945751449 valid 0.36762673035264015
LOSS train 0.2261959945751449 valid 0.3704354131923002
LOSS train 0.2261959945751449 valid 0.37424150771564907
LOSS train 0.2261959945751449 valid 0.37629072132863495
LOSS train 0.2261959945751449 valid 0.37503781318664553
LOSS train 0.2261959945751449 valid 0.373530612105415
LOSS train 0.2261959945751449 valid 0.37058833783323114
LOSS train 0.2261959945751449 valid 0.37103040322013525
LOSS train 0.2261959945751449 valid 0.36890751123428345
LOSS train 0.2261959945751449 valid 0.3679016542434692
LOSS train 0.2261959945751449 valid 0.3676018382494266
LOSS train 0.2261959945751449 valid 0.36752955781088936
LOSS train 0.2261959945751449 valid 0.36768680065870285
LOSS train 0.2261959945751449 valid 0.3674780222876319
LOSS train 0.2261959945751449 valid 0.3694209267695745
LOSS train 0.2261959945751449 valid 0.37248816028718024
LOSS train 0.2261959945751449 valid 0.3729903772473335
LOSS train 0.2261959945751449 valid 0.3746679172371373
LOSS train 0.2261959945751449 valid 0.37453560092869925
LOSS train 0.2261959945751449 valid 0.3768501571246556
LOSS train 0.2261959945751449 valid 0.37687767793734867
LOSS train 0.2261959945751449 valid 0.3772578255550281
LOSS train 0.2261959945751449 valid 0.3783574010196485
LOSS train 0.2261959945751449 valid 0.37790951438439196
LOSS train 0.2261959945751449 valid 0.37826152294874194
LOSS train 0.2261959945751449 valid 0.37948737638752633
LOSS train 0.2261959945751449 valid 0.38026662383760723
LOSS train 0.2261959945751449 valid 0.37978965044021606
LOSS train 0.2261959945751449 valid 0.3800346370447766
LOSS train 0.2261959945751449 valid 0.3788083844714695
LOSS train 0.2261959945751449 valid 0.37964248138925305
LOSS train 0.2261959945751449 valid 0.3809384481703981
LOSS train 0.2261959945751449 valid 0.38140557892620564
LOSS train 0.2261959945751449 valid 0.38159689672139224
LOSS train 0.2261959945751449 valid 0.3803021043539047
LOSS train 0.2261959945751449 valid 0.3798048285877003
LOSS train 0.2261959945751449 valid 0.37946283244169676
LOSS train 0.2261959945751449 valid 0.37958929786142315
LOSS train 0.2261959945751449 valid 0.3798139928667634
LOSS train 0.2261959945751449 valid 0.3794298605485396
LOSS train 0.2261959945751449 valid 0.3788867784397943
LOSS train 0.2261959945751449 valid 0.37845378068455476
LOSS train 0.2261959945751449 valid 0.37765006001653345
LOSS train 0.2261959945751449 valid 0.37798863699880697
LOSS train 0.2261959945751449 valid 0.37808202852805456
LOSS train 0.2261959945751449 valid 0.3774427185293104
LOSS train 0.2261959945751449 valid 0.3787762157378658
LOSS train 0.2261959945751449 valid 0.3792204861603086
LOSS train 0.2261959945751449 valid 0.380471458658576
LOSS train 0.2261959945751449 valid 0.38136171973668614
LOSS train 0.2261959945751449 valid 0.3812179560914184
LOSS train 0.2261959945751449 valid 0.38027601188688137
LOSS train 0.2261959945751449 valid 0.38056502973332124
LOSS train 0.2261959945751449 valid 0.37924505143925763
LOSS train 0.2261959945751449 valid 0.3797902933188847
LOSS train 0.2261959945751449 valid 0.3793506605524412
LOSS train 0.2261959945751449 valid 0.37958256486389375
LOSS train 0.2261959945751449 valid 0.3793849275536733
LOSS train 0.2261959945751449 valid 0.37908699343333374
LOSS train 0.2261959945751449 valid 0.37892208019892376
LOSS train 0.2261959945751449 valid 0.37948649415844365
LOSS train 0.2261959945751449 valid 0.37978033005417167
LOSS train 0.2261959945751449 valid 0.38042586621565694
LOSS train 0.2261959945751449 valid 0.38085430068305776
LOSS train 0.2261959945751449 valid 0.3796204078942537
LOSS train 0.2261959945751449 valid 0.37848725473439254
LOSS train 0.2261959945751449 valid 0.3791302235388174
LOSS train 0.2261959945751449 valid 0.37911224652485676
LOSS train 0.2261959945751449 valid 0.37891185886803125
LOSS train 0.2261959945751449 valid 0.37879968145314385
LOSS train 0.2261959945751449 valid 0.377905327913373
LOSS train 0.2261959945751449 valid 0.37753136678673754
LOSS train 0.2261959945751449 valid 0.37687679955905135
LOSS train 0.2261959945751449 valid 0.37750719805781763
LOSS train 0.2261959945751449 valid 0.3779364724953969
LOSS train 0.2261959945751449 valid 0.3780554821203043
LOSS train 0.2261959945751449 valid 0.37850034269301785
LOSS train 0.2261959945751449 valid 0.3782415540628536
LOSS train 0.2261959945751449 valid 0.3784677979159862
LOSS train 0.2261959945751449 valid 0.37804098411610254
LOSS train 0.2261959945751449 valid 0.37874038393298787
LOSS train 0.2261959945751449 valid 0.3790877028224395
LOSS train 0.2261959945751449 valid 0.3795317882785992
LOSS train 0.2261959945751449 valid 0.37979720397429034
LOSS train 0.2261959945751449 valid 0.3799587252736092
LOSS train 0.2261959945751449 valid 0.3801341201409255
LOSS train 0.2261959945751449 valid 0.3802019839777666
LOSS train 0.2261959945751449 valid 0.38021374067056524
LOSS train 0.2261959945751449 valid 0.38033648838217443
LOSS train 0.2261959945751449 valid 0.38039439888227555
LOSS train 0.2261959945751449 valid 0.3807637025162859
LOSS train 0.2261959945751449 valid 0.3804113277208025
LOSS train 0.2261959945751449 valid 0.3807666630104736
LOSS train 0.2261959945751449 valid 0.381278013690896
LOSS train 0.2261959945751449 valid 0.3815337511626157
LOSS train 0.2261959945751449 valid 0.38080740619350123
LOSS train 0.2261959945751449 valid 0.38059206147279057
LOSS train 0.2261959945751449 valid 0.3807019636166834
LOSS train 0.2261959945751449 valid 0.38029889997683075
LOSS train 0.2261959945751449 valid 0.38076796505762184
LOSS train 0.2261959945751449 valid 0.38049712237612954
LOSS train 0.2261959945751449 valid 0.38072985792771363
LOSS train 0.2261959945751449 valid 0.3804500150983616
LOSS train 0.2261959945751449 valid 0.38024971289794984
LOSS train 0.2261959945751449 valid 0.37977558771769204
LOSS train 0.2261959945751449 valid 0.3796342621164874
LOSS train 0.2261959945751449 valid 0.37915754562518633
LOSS train 0.2261959945751449 valid 0.37919497950290276
LOSS train 0.2261959945751449 valid 0.37973738461732864
LOSS train 0.2261959945751449 valid 0.3797070491313934
LOSS train 0.2261959945751449 valid 0.37975894742541844
LOSS train 0.2261959945751449 valid 0.3797764904855743
LOSS train 0.2261959945751449 valid 0.3804761047940701
LOSS train 0.2261959945751449 valid 0.38061988376831823
LOSS train 0.2261959945751449 valid 0.38041994755084696
LOSS train 0.2261959945751449 valid 0.38032870438262706
LOSS train 0.2261959945751449 valid 0.37994379234133346
LOSS train 0.2261959945751449 valid 0.3800792523792812
LOSS train 0.2261959945751449 valid 0.3799797365469719
LOSS train 0.2261959945751449 valid 0.3796816132686756
LOSS train 0.2261959945751449 valid 0.37957600372679096
LOSS train 0.2261959945751449 valid 0.3794249100406675
LOSS train 0.2261959945751449 valid 0.3792971769968669
LOSS train 0.2261959945751449 valid 0.37918588540536896
LOSS train 0.2261959945751449 valid 0.37933146144662583
LOSS train 0.2261959945751449 valid 0.37935857506508525
LOSS train 0.2261959945751449 valid 0.37976152964041265
LOSS train 0.2261959945751449 valid 0.379577003784113
LOSS train 0.2261959945751449 valid 0.37944063254528576
LOSS train 0.2261959945751449 valid 0.3791914347944588
LOSS train 0.2261959945751449 valid 0.37935929347390995
LOSS train 0.2261959945751449 valid 0.3790879575979142
LOSS train 0.2261959945751449 valid 0.3801985820000236
LOSS train 0.2261959945751449 valid 0.3802097501770762
LOSS train 0.2261959945751449 valid 0.3804426324367523
LOSS train 0.2261959945751449 valid 0.3804642998224852
LOSS train 0.2261959945751449 valid 0.38012119872789635
LOSS train 0.2261959945751449 valid 0.3802553643198574
LOSS train 0.2261959945751449 valid 0.3802488841019668
LOSS train 0.2261959945751449 valid 0.38047342723415745
LOSS train 0.2261959945751449 valid 0.3806766049984174
LOSS train 0.2261959945751449 valid 0.3804718772317194
LOSS train 0.2261959945751449 valid 0.38057234524926054
LOSS train 0.2261959945751449 valid 0.3805350428857144
LOSS train 0.2261959945751449 valid 0.38050059173256157
LOSS train 0.2261959945751449 valid 0.38045157890142123
LOSS train 0.2261959945751449 valid 0.3802145423344624
LOSS train 0.2261959945751449 valid 0.37999680327491525
LOSS train 0.2261959945751449 valid 0.3793865271457812
LOSS train 0.2261959945751449 valid 0.3790092027548588
LOSS train 0.2261959945751449 valid 0.37909361181488954
LOSS train 0.2261959945751449 valid 0.37949918183737885
LOSS train 0.2261959945751449 valid 0.37924876862338613
LOSS train 0.2261959945751449 valid 0.37938275933265686
LOSS train 0.2261959945751449 valid 0.3795350486741346
LOSS train 0.2261959945751449 valid 0.37955588294051545
LOSS train 0.2261959945751449 valid 0.3792193403770757
LOSS train 0.2261959945751449 valid 0.37925504087712725
LOSS train 0.2261959945751449 valid 0.37924698611785623
LOSS train 0.2261959945751449 valid 0.37895517434392656
LOSS train 0.2261959945751449 valid 0.37887890145859937
LOSS train 0.2261959945751449 valid 0.37882886515498837
LOSS train 0.2261959945751449 valid 0.37893395963009824
LOSS train 0.2261959945751449 valid 0.37871413190937575
LOSS train 0.2261959945751449 valid 0.3786372337076399
LOSS train 0.2261959945751449 valid 0.3788157974817476
LOSS train 0.2261959945751449 valid 0.3790285860771661
LOSS train 0.2261959945751449 valid 0.3791327241991387
LOSS train 0.2261959945751449 valid 0.37912766959356226
LOSS train 0.2261959945751449 valid 0.378557305400436
LOSS train 0.2261959945751449 valid 0.37855092764541665
LOSS train 0.2261959945751449 valid 0.3783727065127164
LOSS train 0.2261959945751449 valid 0.37846601358119475
LOSS train 0.2261959945751449 valid 0.37827738283803225
LOSS train 0.2261959945751449 valid 0.37843690843958605
LOSS train 0.2261959945751449 valid 0.37822604273002186
LOSS train 0.2261959945751449 valid 0.3782013552263379
LOSS train 0.2261959945751449 valid 0.3780567161159812
LOSS train 0.2261959945751449 valid 0.377757842700506
LOSS train 0.2261959945751449 valid 0.3774233851677332
LOSS train 0.2261959945751449 valid 0.37730170421454373
LOSS train 0.2261959945751449 valid 0.3775230292136294
LOSS train 0.2261959945751449 valid 0.37718747089607546
LOSS train 0.2261959945751449 valid 0.37725897979496714
LOSS train 0.2261959945751449 valid 0.3771240271627903
LOSS train 0.2261959945751449 valid 0.3769565669458304
LOSS train 0.2261959945751449 valid 0.37682171018406896
LOSS train 0.2261959945751449 valid 0.37654623770948703
LOSS train 0.2261959945751449 valid 0.3764726120759459
LOSS train 0.2261959945751449 valid 0.3760385722648807
LOSS train 0.2261959945751449 valid 0.3761356604620091
LOSS train 0.2261959945751449 valid 0.376117962282061
LOSS train 0.2261959945751449 valid 0.3758401581301139
LOSS train 0.2261959945751449 valid 0.3757021508433602
LOSS train 0.2261959945751449 valid 0.3758290357532955
LOSS train 0.2261959945751449 valid 0.3760135043853832
LOSS train 0.2261959945751449 valid 0.3759720430902715
LOSS train 0.2261959945751449 valid 0.3763017724377449
LOSS train 0.2261959945751449 valid 0.3764751443517542
LOSS train 0.2261959945751449 valid 0.3761421773322793
LOSS train 0.2261959945751449 valid 0.3760742928694796
LOSS train 0.2261959945751449 valid 0.3760774300395069
LOSS train 0.2261959945751449 valid 0.376098048112808
LOSS train 0.2261959945751449 valid 0.37624225238142495
LOSS train 0.2261959945751449 valid 0.3763615910302509
LOSS train 0.2261959945751449 valid 0.3763303860550013
LOSS train 0.2261959945751449 valid 0.3761698292182373
LOSS train 0.2261959945751449 valid 0.37641210834007094
LOSS train 0.2261959945751449 valid 0.3764444814462747
LOSS train 0.2261959945751449 valid 0.3762952631049686
LOSS train 0.2261959945751449 valid 0.3763830992764076
LOSS train 0.2261959945751449 valid 0.37672929459206334
LOSS train 0.2261959945751449 valid 0.37678805960897815
LOSS train 0.2261959945751449 valid 0.37685709710204446
LOSS train 0.2261959945751449 valid 0.3769259927065476
LOSS train 0.2261959945751449 valid 0.37688641540415874
LOSS train 0.2261959945751449 valid 0.3769166947438799
LOSS train 0.2261959945751449 valid 0.37681908413064324
LOSS train 0.2261959945751449 valid 0.37670541166240334
LOSS train 0.2261959945751449 valid 0.37706687463090777
LOSS train 0.2261959945751449 valid 0.3767860853823565
LOSS train 0.2261959945751449 valid 0.3769815752777872
LOSS train 0.2261959945751449 valid 0.37701434710947407
LOSS train 0.2261959945751449 valid 0.37686432318208607
LOSS train 0.2261959945751449 valid 0.376905691002806
LOSS train 0.2261959945751449 valid 0.3770274794447966
LOSS train 0.2261959945751449 valid 0.3767169514470849
LOSS train 0.2261959945751449 valid 0.37700584359129763
LOSS train 0.2261959945751449 valid 0.3775808248119276
LOSS train 0.2261959945751449 valid 0.37789937488886777
LOSS train 0.2261959945751449 valid 0.37787294315128794
LOSS train 0.2261959945751449 valid 0.37788528252227105
LOSS train 0.2261959945751449 valid 0.377761528737122
LOSS train 0.2261959945751449 valid 0.3775705963253496
LOSS train 0.2261959945751449 valid 0.37786373841762544
LOSS train 0.2261959945751449 valid 0.3780091985525838
LOSS train 0.2261959945751449 valid 0.3782978941287313
LOSS train 0.2261959945751449 valid 0.37826008005104517
LOSS train 0.2261959945751449 valid 0.37808362400437906
LOSS train 0.2261959945751449 valid 0.37815833220294875
LOSS train 0.2261959945751449 valid 0.3782677697017789
LOSS train 0.2261959945751449 valid 0.37810769607584765
LOSS train 0.2261959945751449 valid 0.378293533080308
LOSS train 0.2261959945751449 valid 0.37829061353068555
LOSS train 0.2261959945751449 valid 0.3782096095956289
LOSS train 0.2261959945751449 valid 0.378383232030832
LOSS train 0.2261959945751449 valid 0.3783647141611303
LOSS train 0.2261959945751449 valid 0.37839954683535904
LOSS train 0.2261959945751449 valid 0.3783946679622838
LOSS train 0.2261959945751449 valid 0.37833350581942865
LOSS train 0.2261959945751449 valid 0.378616729961302
LOSS train 0.2261959945751449 valid 0.378764833515503
LOSS train 0.2261959945751449 valid 0.37897290356123625
LOSS train 0.2261959945751449 valid 0.37891735673837057
LOSS train 0.2261959945751449 valid 0.3787595082212378
LOSS train 0.2261959945751449 valid 0.3788945506639586
LOSS train 0.2261959945751449 valid 0.3792654683484751
LOSS train 0.2261959945751449 valid 0.37932497131955495
LOSS train 0.2261959945751449 valid 0.3793436270343126
LOSS train 0.2261959945751449 valid 0.3792765694314783
LOSS train 0.2261959945751449 valid 0.37907286603813584
LOSS train 0.2261959945751449 valid 0.3788958823422663
LOSS train 0.2261959945751449 valid 0.3786269517468034
LOSS train 0.2261959945751449 valid 0.37873646008071077
LOSS train 0.2261959945751449 valid 0.37868087302361214
LOSS train 0.2261959945751449 valid 0.3784437408650897
LOSS train 0.2261959945751449 valid 0.37824080303205665
LOSS train 0.2261959945751449 valid 0.37810301338404734
LOSS train 0.2261959945751449 valid 0.3782189502052858
LOSS train 0.2261959945751449 valid 0.37844739221689994
LOSS train 0.2261959945751449 valid 0.37835558081840304
LOSS train 0.2261959945751449 valid 0.37828738203447454
LOSS train 0.2261959945751449 valid 0.3782810314248006
LOSS train 0.2261959945751449 valid 0.3784712319968068
LOSS train 0.2261959945751449 valid 0.37849246880103804
LOSS train 0.2261959945751449 valid 0.37840148151125697
LOSS train 0.2261959945751449 valid 0.3785575358834985
LOSS train 0.2261959945751449 valid 0.37857646136560535
LOSS train 0.2261959945751449 valid 0.3787237134109549
LOSS train 0.2261959945751449 valid 0.3788073857962075
LOSS train 0.2261959945751449 valid 0.37872715616548386
LOSS train 0.2261959945751449 valid 0.37874211185307616
LOSS train 0.2261959945751449 valid 0.3787127609220927
LOSS train 0.2261959945751449 valid 0.3787173581960608
LOSS train 0.2261959945751449 valid 0.3786025286714236
LOSS train 0.2261959945751449 valid 0.3786659778352592
LOSS train 0.2261959945751449 valid 0.3787108289682313
LOSS train 0.2261959945751449 valid 0.378876665342759
LOSS train 0.2261959945751449 valid 0.378838714604315
LOSS train 0.2261959945751449 valid 0.3787702617098074
LOSS train 0.2261959945751449 valid 0.378735483861437
LOSS train 0.2261959945751449 valid 0.3786892612322146
LOSS train 0.2261959945751449 valid 0.3785820905264322
LOSS train 0.2261959945751449 valid 0.37851537093761284
LOSS train 0.2261959945751449 valid 0.37851868300668656
LOSS train 0.2261959945751449 valid 0.37843191911169954
LOSS train 0.2261959945751449 valid 0.37840536676156217
LOSS train 0.2261959945751449 valid 0.3783379994070949
LOSS train 0.2261959945751449 valid 0.3784644872329797
LOSS train 0.2261959945751449 valid 0.37836845791529095
LOSS train 0.2261959945751449 valid 0.3782213486825364
LOSS train 0.2261959945751449 valid 0.37834268582732145
LOSS train 0.2261959945751449 valid 0.3784248067897821
LOSS train 0.2261959945751449 valid 0.3786004721931529
LOSS train 0.2261959945751449 valid 0.3785238823853433
LOSS train 0.2261959945751449 valid 0.37881046394321405
LOSS train 0.2261959945751449 valid 0.378637885455019
LOSS train 0.2261959945751449 valid 0.37849930417796035
LOSS train 0.2261959945751449 valid 0.37855357281219815
LOSS train 0.2261959945751449 valid 0.37863313583227304
LOSS train 0.2261959945751449 valid 0.37868473070896475
LOSS train 0.2261959945751449 valid 0.37867367367861104
LOSS train 0.2261959945751449 valid 0.3786886772368012
LOSS train 0.2261959945751449 valid 0.3787928854078507
LOSS train 0.2261959945751449 valid 0.3788205511642225
LOSS train 0.2261959945751449 valid 0.37872068921604907
LOSS train 0.2261959945751449 valid 0.3786279723407274
LOSS train 0.2261959945751449 valid 0.378545972588542
LOSS train 0.2261959945751449 valid 0.37875106555972987
LOSS train 0.2261959945751449 valid 0.37872134295862114
LOSS train 0.2261959945751449 valid 0.3784617608679192
LOSS train 0.2261959945751449 valid 0.3784574633712938
LOSS train 0.2261959945751449 valid 0.37835631770847816
LOSS train 0.2261959945751449 valid 0.3782937733297151
LOSS train 0.2261959945751449 valid 0.3782833393005764
LOSS train 0.2261959945751449 valid 0.3780822804596417
LOSS train 0.2261959945751449 valid 0.37805603418433875
LOSS train 0.2261959945751449 valid 0.3779285587479005
LOSS train 0.2261959945751449 valid 0.37816344643401545
LOSS train 0.2261959945751449 valid 0.37825975107110066
LOSS train 0.2261959945751449 valid 0.37811835015440265
LOSS train 0.2261959945751449 valid 0.377896062922409
LOSS train 0.2261959945751449 valid 0.37780184255934307
LOSS train 0.2261959945751449 valid 0.37782591606962646
LOSS train 0.2261959945751449 valid 0.3776957592793873
LOSS train 0.2261959945751449 valid 0.3776709555390893
LOSS train 0.2261959945751449 valid 0.37759455835277383
LOSS train 0.2261959945751449 valid 0.37757516498268495
LOSS train 0.2261959945751449 valid 0.3776206993787302
LOSS train 0.2261959945751449 valid 0.37787037130812523
LOSS train 0.2261959945751449 valid 0.3779783130026935
LOSS train 0.2261959945751449 valid 0.37796195431583735
LOSS train 0.2261959945751449 valid 0.37791202837528465
LOSS train 0.2261959945751449 valid 0.3779383243624546
LOSS train 0.2261959945751449 valid 0.37793012104100654
LOSS train 0.2261959945751449 valid 0.37793495574156005
LOSS train 0.2261959945751449 valid 0.37806166757865506
LOSS train 0.2261959945751449 valid 0.37784622153959985
LOSS train 0.2261959945751449 valid 0.37781142942853024
LOSS train 0.2261959945751449 valid 0.3778753078147157
LOSS train 0.2261959945751449 valid 0.37784549520641075
LOSS train 0.2261959945751449 valid 0.37767285563315617
LOSS train 0.2261959945751449 valid 0.3776318493258694
LOSS train 0.2261959945751449 valid 0.3777835835932393
EPOCH 24:
  batch 1 loss: 0.2704531252384186
  batch 2 loss: 0.2504415363073349
  batch 3 loss: 0.24271360536416373
  batch 4 loss: 0.24759471789002419
  batch 5 loss: 0.2530817180871964
  batch 6 loss: 0.24831456939379373
  batch 7 loss: 0.24878919976098196
  batch 8 loss: 0.25209980458021164
  batch 9 loss: 0.24972928232616848
  batch 10 loss: 0.24857460111379623
  batch 11 loss: 0.24823465672406284
  batch 12 loss: 0.2440990929802259
  batch 13 loss: 0.24238331272051886
  batch 14 loss: 0.2421002217701503
  batch 15 loss: 0.24297467072804768
  batch 16 loss: 0.24174352455884218
  batch 17 loss: 0.23754152480293722
  batch 18 loss: 0.23720311456256443
  batch 19 loss: 0.23346175096536936
  batch 20 loss: 0.23069170489907265
  batch 21 loss: 0.22979859723931267
  batch 22 loss: 0.22860714454542508
  batch 23 loss: 0.22748332114323325
  batch 24 loss: 0.22503196013470492
  batch 25 loss: 0.22632260739803314
  batch 26 loss: 0.22428331638757998
  batch 27 loss: 0.2245026429494222
  batch 28 loss: 0.22288640109556063
  batch 29 loss: 0.22275085541708717
  batch 30 loss: 0.22187753319740294
  batch 31 loss: 0.22318957313414542
  batch 32 loss: 0.22291282657533884
  batch 33 loss: 0.22250687850244116
  batch 34 loss: 0.2210986430154127
  batch 35 loss: 0.22081917907510484
  batch 36 loss: 0.22153950110077858
  batch 37 loss: 0.2214107054310876
  batch 38 loss: 0.22130769099059858
  batch 39 loss: 0.22096006572246552
  batch 40 loss: 0.22145485393702985
  batch 41 loss: 0.22148360166607833
  batch 42 loss: 0.22131194280726568
  batch 43 loss: 0.22220716122971024
  batch 44 loss: 0.2215080244297331
  batch 45 loss: 0.22106430596775478
  batch 46 loss: 0.22147907737804495
  batch 47 loss: 0.22110657489046137
  batch 48 loss: 0.22019414262225231
  batch 49 loss: 0.21995848843029567
  batch 50 loss: 0.21993882834911346
  batch 51 loss: 0.21995670129271114
  batch 52 loss: 0.2200933640392927
  batch 53 loss: 0.21986987961913054
  batch 54 loss: 0.2199986339719207
  batch 55 loss: 0.2196129934354262
  batch 56 loss: 0.21933536417782307
  batch 57 loss: 0.21915490271752341
  batch 58 loss: 0.2196208374767468
  batch 59 loss: 0.2199316186419988
  batch 60 loss: 0.22000599900881448
  batch 61 loss: 0.22040472900281188
  batch 62 loss: 0.22076462113088177
  batch 63 loss: 0.22019163672886197
  batch 64 loss: 0.22080565011128783
  batch 65 loss: 0.22059634580061985
  batch 66 loss: 0.22043938641295288
  batch 67 loss: 0.22107209354194243
  batch 68 loss: 0.22140029423377094
  batch 69 loss: 0.22172319673109744
  batch 70 loss: 0.22255312757832663
  batch 71 loss: 0.2224331216912874
  batch 72 loss: 0.22291235170430607
  batch 73 loss: 0.2233516022767106
  batch 74 loss: 0.22322268743772763
  batch 75 loss: 0.22287980437278748
  batch 76 loss: 0.22373945266008377
  batch 77 loss: 0.22333035879320912
  batch 78 loss: 0.2235422908113553
  batch 79 loss: 0.22337345005590706
  batch 80 loss: 0.2231209620833397
  batch 81 loss: 0.2235263976050012
  batch 82 loss: 0.22440947810324227
  batch 83 loss: 0.22421945505831614
  batch 84 loss: 0.22397699668293908
  batch 85 loss: 0.22403239169541528
  batch 86 loss: 0.22482303183439167
  batch 87 loss: 0.22453366733830551
  batch 88 loss: 0.2241974098777229
  batch 89 loss: 0.22440981195214088
  batch 90 loss: 0.22457217027743656
  batch 91 loss: 0.22457529031313384
  batch 92 loss: 0.22425372218308243
  batch 93 loss: 0.22441921695586173
  batch 94 loss: 0.2247026521474757
  batch 95 loss: 0.22429858101041694
  batch 96 loss: 0.22426525286088386
  batch 97 loss: 0.22433523480425177
  batch 98 loss: 0.2246114070318183
  batch 99 loss: 0.2250819793253234
  batch 100 loss: 0.22475753754377364
  batch 101 loss: 0.2247580561307397
  batch 102 loss: 0.22531435536403283
  batch 103 loss: 0.22574306518128776
  batch 104 loss: 0.22586828441574022
  batch 105 loss: 0.22560102073919205
  batch 106 loss: 0.22572828449730603
  batch 107 loss: 0.2255545850390586
  batch 108 loss: 0.22586473118927744
  batch 109 loss: 0.2257327168086253
  batch 110 loss: 0.2258126906373284
  batch 111 loss: 0.22576700755067775
  batch 112 loss: 0.22571450392050402
  batch 113 loss: 0.22584340846644038
  batch 114 loss: 0.22618230837478973
  batch 115 loss: 0.22636642196904058
  batch 116 loss: 0.22644011213861662
  batch 117 loss: 0.22644362452193204
  batch 118 loss: 0.22621564137733588
  batch 119 loss: 0.2263223217064593
  batch 120 loss: 0.22587756055096786
  batch 121 loss: 0.22567072227474086
  batch 122 loss: 0.22562304310134199
  batch 123 loss: 0.2255251952787725
  batch 124 loss: 0.22586978058661183
  batch 125 loss: 0.22549571299552917
  batch 126 loss: 0.2253823389136602
  batch 127 loss: 0.22567112403591788
  batch 128 loss: 0.2253551329486072
  batch 129 loss: 0.225599514883618
  batch 130 loss: 0.22545596418472436
  batch 131 loss: 0.22547678831424423
  batch 132 loss: 0.2253595216933525
  batch 133 loss: 0.22564677983746492
  batch 134 loss: 0.2256370781740146
  batch 135 loss: 0.22530477753391973
  batch 136 loss: 0.22541773669859944
  batch 137 loss: 0.22536992907089037
  batch 138 loss: 0.22528353657411493
  batch 139 loss: 0.2254780065670288
  batch 140 loss: 0.22555036842823029
  batch 141 loss: 0.2257931682657688
  batch 142 loss: 0.22567264578292068
  batch 143 loss: 0.22545251198165067
  batch 144 loss: 0.2255117484471864
  batch 145 loss: 0.22552460226519355
  batch 146 loss: 0.2254184289541963
  batch 147 loss: 0.2257034252492749
  batch 148 loss: 0.22575505379889463
  batch 149 loss: 0.22559041104860753
  batch 150 loss: 0.2255113399028778
  batch 151 loss: 0.22543022135235616
  batch 152 loss: 0.22564633876869553
  batch 153 loss: 0.2253907405667835
  batch 154 loss: 0.22538726234977896
  batch 155 loss: 0.22525951016333795
  batch 156 loss: 0.22513472269742918
  batch 157 loss: 0.22537937950176798
  batch 158 loss: 0.22530344288937654
  batch 159 loss: 0.2253877873878059
  batch 160 loss: 0.22501983875408768
  batch 161 loss: 0.2250714132689541
  batch 162 loss: 0.22493648418673762
  batch 163 loss: 0.2249662402583046
  batch 164 loss: 0.22464867635834507
  batch 165 loss: 0.22461362444993221
  batch 166 loss: 0.22449710543256207
  batch 167 loss: 0.22434214221503207
  batch 168 loss: 0.22417084367147513
  batch 169 loss: 0.22390858758483412
  batch 170 loss: 0.22379081450841007
  batch 171 loss: 0.2236611329846912
  batch 172 loss: 0.22365286989614022
  batch 173 loss: 0.22349767180191987
  batch 174 loss: 0.22362309582959647
  batch 175 loss: 0.22364062981946128
  batch 176 loss: 0.22354709517888047
  batch 177 loss: 0.22355671673171265
  batch 178 loss: 0.22370402347505763
  batch 179 loss: 0.22360511298952157
  batch 180 loss: 0.2234205083714591
  batch 181 loss: 0.22357893994499967
  batch 182 loss: 0.22357904116858493
  batch 183 loss: 0.22339496757489086
  batch 184 loss: 0.22342317907706552
  batch 185 loss: 0.22364018623893325
  batch 186 loss: 0.22384856577201556
  batch 187 loss: 0.2237395437006007
  batch 188 loss: 0.2233381949048093
  batch 189 loss: 0.2230272035277079
  batch 190 loss: 0.22299751349185643
  batch 191 loss: 0.22295390475170776
  batch 192 loss: 0.22285973637675247
  batch 193 loss: 0.22269827582984392
  batch 194 loss: 0.22278866548206389
  batch 195 loss: 0.22289428137815914
  batch 196 loss: 0.22270839494101854
  batch 197 loss: 0.22264052995570421
  batch 198 loss: 0.22292959434215467
  batch 199 loss: 0.22270491973838613
  batch 200 loss: 0.22291961520910264
  batch 201 loss: 0.22305072021128527
  batch 202 loss: 0.22302839196849578
  batch 203 loss: 0.22296875788660472
  batch 204 loss: 0.22288731514823204
  batch 205 loss: 0.22308038298676652
  batch 206 loss: 0.2231486077302868
  batch 207 loss: 0.22324073429844807
  batch 208 loss: 0.22331042351344457
  batch 209 loss: 0.22322802174319492
  batch 210 loss: 0.2233186281153134
  batch 211 loss: 0.2232051327188998
  batch 212 loss: 0.22315360676005203
  batch 213 loss: 0.22301086048844834
  batch 214 loss: 0.22304282636842995
  batch 215 loss: 0.2229274141927098
  batch 216 loss: 0.22261279786902446
  batch 217 loss: 0.22247551821069234
  batch 218 loss: 0.22231987009354687
  batch 219 loss: 0.2223505829567234
  batch 220 loss: 0.2222721429033713
  batch 221 loss: 0.22237857744704545
  batch 222 loss: 0.222323102389907
  batch 223 loss: 0.2222395012063296
  batch 224 loss: 0.22213034637804543
  batch 225 loss: 0.2220685370100869
  batch 226 loss: 0.22216901262249567
  batch 227 loss: 0.22208096264217395
  batch 228 loss: 0.22196861284605243
  batch 229 loss: 0.22197078441688586
  batch 230 loss: 0.22211598060701204
  batch 231 loss: 0.2221117891145475
  batch 232 loss: 0.22197331526669964
  batch 233 loss: 0.22198507861262226
  batch 234 loss: 0.22192264278220314
  batch 235 loss: 0.22186126404620232
  batch 236 loss: 0.22166037540567124
  batch 237 loss: 0.22168243610406224
  batch 238 loss: 0.22169946647491776
  batch 239 loss: 0.22157690377913758
  batch 240 loss: 0.2214666365335385
  batch 241 loss: 0.22156201358652708
  batch 242 loss: 0.2215441884088122
  batch 243 loss: 0.22148688131399116
  batch 244 loss: 0.22143509098496594
  batch 245 loss: 0.221325845560249
  batch 246 loss: 0.22137077903844476
  batch 247 loss: 0.2214535031965387
  batch 248 loss: 0.22149861183378003
  batch 249 loss: 0.22151518855468336
  batch 250 loss: 0.22146666735410692
  batch 251 loss: 0.22144291707244051
  batch 252 loss: 0.22136826771828863
  batch 253 loss: 0.22136053484183527
  batch 254 loss: 0.2212937960591842
  batch 255 loss: 0.22132087571948184
  batch 256 loss: 0.2215379198314622
  batch 257 loss: 0.22148472204050665
  batch 258 loss: 0.22140122459147327
  batch 259 loss: 0.22137196546125595
  batch 260 loss: 0.221456284649097
  batch 261 loss: 0.22143387897261257
  batch 262 loss: 0.22136360839123034
  batch 263 loss: 0.22149889743373422
  batch 264 loss: 0.2215049187793876
  batch 265 loss: 0.22137989165647975
  batch 266 loss: 0.22141731227923156
  batch 267 loss: 0.22145578170313818
  batch 268 loss: 0.22136370921090467
  batch 269 loss: 0.22145339957400326
  batch 270 loss: 0.22181868497972135
  batch 271 loss: 0.22182336845521117
  batch 272 loss: 0.22180367228301132
  batch 273 loss: 0.22187536589173606
  batch 274 loss: 0.2220655066797333
  batch 275 loss: 0.22212460913441398
  batch 276 loss: 0.22207764303986577
  batch 277 loss: 0.22212289187667172
  batch 278 loss: 0.22213772271605706
  batch 279 loss: 0.22217310393582962
  batch 280 loss: 0.22209345756896903
  batch 281 loss: 0.2220717627700962
  batch 282 loss: 0.22214134676870725
  batch 283 loss: 0.22207741886903878
  batch 284 loss: 0.2221292561628449
  batch 285 loss: 0.22208020530248943
  batch 286 loss: 0.22199892700760515
  batch 287 loss: 0.22200257899661513
  batch 288 loss: 0.22200243713127243
  batch 289 loss: 0.22214168042047627
  batch 290 loss: 0.22196560111539118
  batch 291 loss: 0.2219944992630752
  batch 292 loss: 0.22205835601238355
  batch 293 loss: 0.22201437525985185
  batch 294 loss: 0.22197243231697147
  batch 295 loss: 0.22196474383443088
  batch 296 loss: 0.22198607807827964
  batch 297 loss: 0.22194005167644834
  batch 298 loss: 0.22184367997934354
  batch 299 loss: 0.2218043846529862
  batch 300 loss: 0.22185126816233
  batch 301 loss: 0.22189171686521003
  batch 302 loss: 0.22179899384446491
  batch 303 loss: 0.22167485711401444
  batch 304 loss: 0.22158572523805656
  batch 305 loss: 0.22147460101080724
  batch 306 loss: 0.22159164939440934
  batch 307 loss: 0.2215719299712476
  batch 308 loss: 0.22162531398527033
  batch 309 loss: 0.22166359873072614
  batch 310 loss: 0.2215476867172026
  batch 311 loss: 0.22161338313024528
  batch 312 loss: 0.22170165945322085
  batch 313 loss: 0.22175767194158352
  batch 314 loss: 0.22168239163365333
  batch 315 loss: 0.22163187856712038
  batch 316 loss: 0.22157642174559303
  batch 317 loss: 0.22150320294151546
  batch 318 loss: 0.22150480901857592
  batch 319 loss: 0.22148988178718051
  batch 320 loss: 0.22141123171895744
  batch 321 loss: 0.22142398598780885
  batch 322 loss: 0.22138208285066652
  batch 323 loss: 0.2212539610017564
  batch 324 loss: 0.22111160441497227
  batch 325 loss: 0.2209622886089178
  batch 326 loss: 0.220904642498932
  batch 327 loss: 0.22084485746304924
  batch 328 loss: 0.22075060364313243
  batch 329 loss: 0.22073785820268207
  batch 330 loss: 0.22062432567278545
  batch 331 loss: 0.22053922384167005
  batch 332 loss: 0.22051042982612748
  batch 333 loss: 0.22058506211540005
  batch 334 loss: 0.22036844594571406
  batch 335 loss: 0.2203418715676265
  batch 336 loss: 0.2203719745434466
  batch 337 loss: 0.22044489738318615
  batch 338 loss: 0.22046280251099512
  batch 339 loss: 0.2204479304826365
  batch 340 loss: 0.22046530890990707
  batch 341 loss: 0.22055691039282554
  batch 342 loss: 0.2204791548830724
  batch 343 loss: 0.22051772540408043
  batch 344 loss: 0.22048689654573453
  batch 345 loss: 0.2205299830955008
  batch 346 loss: 0.22047389393424713
  batch 347 loss: 0.22048388709252437
  batch 348 loss: 0.22048130374530267
  batch 349 loss: 0.2204110593799192
  batch 350 loss: 0.220470937533038
  batch 351 loss: 0.22048802143148546
  batch 352 loss: 0.22057572481307117
  batch 353 loss: 0.22051644861360448
  batch 354 loss: 0.2206958148018115
  batch 355 loss: 0.2207009689488881
  batch 356 loss: 0.22062188303202726
  batch 357 loss: 0.22062041412214606
  batch 358 loss: 0.22071289400148658
  batch 359 loss: 0.22070264276687812
  batch 360 loss: 0.22073208428919316
  batch 361 loss: 0.22072928128480251
  batch 362 loss: 0.2206963443953688
  batch 363 loss: 0.22051539770827805
  batch 364 loss: 0.22037225388563597
  batch 365 loss: 0.2203901136982931
  batch 366 loss: 0.22036672232906676
  batch 367 loss: 0.22035299042589981
  batch 368 loss: 0.2202750184856679
  batch 369 loss: 0.22020803819988477
  batch 370 loss: 0.22016937893790167
  batch 371 loss: 0.22023739442831744
  batch 372 loss: 0.22017818901647804
  batch 373 loss: 0.22003737883018104
  batch 374 loss: 0.2199618302803626
  batch 375 loss: 0.21989075501759847
  batch 376 loss: 0.2198760544366025
  batch 377 loss: 0.2198473615497746
  batch 378 loss: 0.2197939602155534
  batch 379 loss: 0.21978936000360977
  batch 380 loss: 0.21984413164226632
  batch 381 loss: 0.2197840862230366
  batch 382 loss: 0.2197277455429756
  batch 383 loss: 0.21979119556690943
  batch 384 loss: 0.21973989500353733
  batch 385 loss: 0.21981025949701086
  batch 386 loss: 0.21976126579886274
  batch 387 loss: 0.2198409909040737
  batch 388 loss: 0.2198770999831637
  batch 389 loss: 0.2198627187001368
  batch 390 loss: 0.21977987782313274
  batch 391 loss: 0.21986844575466097
  batch 392 loss: 0.21979316069307375
  batch 393 loss: 0.21968453830586743
  batch 394 loss: 0.21966589620421986
  batch 395 loss: 0.21959626987765107
  batch 396 loss: 0.21950124776122545
  batch 397 loss: 0.21955024670593684
  batch 398 loss: 0.21948770351296093
  batch 399 loss: 0.21959554794288816
  batch 400 loss: 0.21965479832142593
  batch 401 loss: 0.21957498495269595
  batch 402 loss: 0.21963077134902204
  batch 403 loss: 0.21962974987757708
  batch 404 loss: 0.21966625882847474
  batch 405 loss: 0.2197448879112432
  batch 406 loss: 0.2198258735276208
  batch 407 loss: 0.2198659644372926
  batch 408 loss: 0.2199276135686566
  batch 409 loss: 0.21999541063471936
  batch 410 loss: 0.2201618385751073
  batch 411 loss: 0.22020287597846522
  batch 412 loss: 0.22018211468764878
  batch 413 loss: 0.22026505016385786
  batch 414 loss: 0.22032980106159109
  batch 415 loss: 0.220388708775302
  batch 416 loss: 0.2204935231890816
  batch 417 loss: 0.22049142252341164
  batch 418 loss: 0.2204883811933002
  batch 419 loss: 0.22061804431292778
  batch 420 loss: 0.22061785356629462
  batch 421 loss: 0.22052723395852747
  batch 422 loss: 0.2206806969162412
  batch 423 loss: 0.2207045513546495
  batch 424 loss: 0.22075766697525978
  batch 425 loss: 0.2207446429308723
  batch 426 loss: 0.2206467569853778
  batch 427 loss: 0.2205796881428368
  batch 428 loss: 0.2206797743581723
  batch 429 loss: 0.22086083267257486
  batch 430 loss: 0.22086966197158014
  batch 431 loss: 0.22095821414082342
  batch 432 loss: 0.2209744970831606
  batch 433 loss: 0.22108546443128696
  batch 434 loss: 0.22125643429942943
  batch 435 loss: 0.2213328216267728
  batch 436 loss: 0.22136664831446945
  batch 437 loss: 0.22138425189515817
  batch 438 loss: 0.221504766987339
  batch 439 loss: 0.22165254600227285
  batch 440 loss: 0.22171870801936497
  batch 441 loss: 0.2216964603957135
  batch 442 loss: 0.22163318947159866
  batch 443 loss: 0.22162465209616494
  batch 444 loss: 0.22161806750673432
  batch 445 loss: 0.2216677266225386
  batch 446 loss: 0.22165206037962917
  batch 447 loss: 0.22163449151137266
  batch 448 loss: 0.22171707983527864
  batch 449 loss: 0.22174848762147942
  batch 450 loss: 0.22175406949387658
  batch 451 loss: 0.22173265911258774
  batch 452 loss: 0.221867777341235
  batch 453 loss: 0.22188869411450612
  batch 454 loss: 0.22192581651100504
  batch 455 loss: 0.22193775540524788
  batch 456 loss: 0.2220018405075136
  batch 457 loss: 0.22199690951690632
  batch 458 loss: 0.22196769717608997
  batch 459 loss: 0.2221426110966273
  batch 460 loss: 0.22222909930607546
  batch 461 loss: 0.22220541902064242
  batch 462 loss: 0.2221776886935874
  batch 463 loss: 0.22216339560615836
  batch 464 loss: 0.2222085557235726
  batch 465 loss: 0.2221599297177407
  batch 466 loss: 0.22208156110518992
  batch 467 loss: 0.22208592490287102
  batch 468 loss: 0.22204386446083713
  batch 469 loss: 0.22215504207209483
  batch 470 loss: 0.2220862313153896
  batch 471 loss: 0.22210083072985307
  batch 472 loss: 0.22194875423180854
LOSS train 0.22194875423180854 valid 0.3737648129463196
LOSS train 0.22194875423180854 valid 0.3643898069858551
LOSS train 0.22194875423180854 valid 0.3582090437412262
LOSS train 0.22194875423180854 valid 0.3540272042155266
LOSS train 0.22194875423180854 valid 0.35137348175048827
LOSS train 0.22194875423180854 valid 0.35698258876800537
LOSS train 0.22194875423180854 valid 0.3685905507632664
LOSS train 0.22194875423180854 valid 0.3677177391946316
LOSS train 0.22194875423180854 valid 0.36877035432391697
LOSS train 0.22194875423180854 valid 0.3711011201143265
LOSS train 0.22194875423180854 valid 0.3702405907891013
LOSS train 0.22194875423180854 valid 0.36889904489119846
LOSS train 0.22194875423180854 valid 0.36699864039054286
LOSS train 0.22194875423180854 valid 0.3652791678905487
LOSS train 0.22194875423180854 valid 0.35935183564821876
LOSS train 0.22194875423180854 valid 0.3604556582868099
LOSS train 0.22194875423180854 valid 0.36325308505226583
LOSS train 0.22194875423180854 valid 0.3664999869134691
LOSS train 0.22194875423180854 valid 0.3685427731589267
LOSS train 0.22194875423180854 valid 0.36702012866735456
LOSS train 0.22194875423180854 valid 0.3658705594993773
LOSS train 0.22194875423180854 valid 0.36339326338334516
LOSS train 0.22194875423180854 valid 0.3643026766569718
LOSS train 0.22194875423180854 valid 0.3622579276561737
LOSS train 0.22194875423180854 valid 0.3612177813053131
LOSS train 0.22194875423180854 valid 0.3608646175036064
LOSS train 0.22194875423180854 valid 0.3610510285253878
LOSS train 0.22194875423180854 valid 0.3617820494941303
LOSS train 0.22194875423180854 valid 0.3613047044852684
LOSS train 0.22194875423180854 valid 0.3625969608624776
LOSS train 0.22194875423180854 valid 0.36519340257490834
LOSS train 0.22194875423180854 valid 0.3651080820709467
LOSS train 0.22194875423180854 valid 0.36670823169477057
LOSS train 0.22194875423180854 valid 0.3668919267023311
LOSS train 0.22194875423180854 valid 0.3693564636366708
LOSS train 0.22194875423180854 valid 0.36942069894737667
LOSS train 0.22194875423180854 valid 0.36961852457072286
LOSS train 0.22194875423180854 valid 0.3705601284378453
LOSS train 0.22194875423180854 valid 0.3700535954573216
LOSS train 0.22194875423180854 valid 0.370364335924387
LOSS train 0.22194875423180854 valid 0.3723626434803009
LOSS train 0.22194875423180854 valid 0.373457873860995
LOSS train 0.22194875423180854 valid 0.3727121318495551
LOSS train 0.22194875423180854 valid 0.37316096099940216
LOSS train 0.22194875423180854 valid 0.3725007931391398
LOSS train 0.22194875423180854 valid 0.37326825053795526
LOSS train 0.22194875423180854 valid 0.37467092465847096
LOSS train 0.22194875423180854 valid 0.374854468430082
LOSS train 0.22194875423180854 valid 0.3750829751394233
LOSS train 0.22194875423180854 valid 0.3735814845561981
LOSS train 0.22194875423180854 valid 0.3729550277485567
LOSS train 0.22194875423180854 valid 0.3726881341292308
LOSS train 0.22194875423180854 valid 0.3727706507691797
LOSS train 0.22194875423180854 valid 0.3726252394693869
LOSS train 0.22194875423180854 valid 0.37255474708297037
LOSS train 0.22194875423180854 valid 0.3720216618052551
LOSS train 0.22194875423180854 valid 0.3715210337387888
LOSS train 0.22194875423180854 valid 0.37102280043322466
LOSS train 0.22194875423180854 valid 0.37145229618428116
LOSS train 0.22194875423180854 valid 0.3714080810546875
LOSS train 0.22194875423180854 valid 0.37040944177596297
LOSS train 0.22194875423180854 valid 0.3716788248669717
LOSS train 0.22194875423180854 valid 0.3723307545222933
LOSS train 0.22194875423180854 valid 0.37344064144417644
LOSS train 0.22194875423180854 valid 0.37422566688977754
LOSS train 0.22194875423180854 valid 0.3742115380186023
LOSS train 0.22194875423180854 valid 0.3734246385631277
LOSS train 0.22194875423180854 valid 0.37363596216720696
LOSS train 0.22194875423180854 valid 0.37251107113948767
LOSS train 0.22194875423180854 valid 0.37290970725672584
LOSS train 0.22194875423180854 valid 0.3724552115923922
LOSS train 0.22194875423180854 valid 0.3727313122815556
LOSS train 0.22194875423180854 valid 0.37268348507685206
LOSS train 0.22194875423180854 valid 0.37231081885260503
LOSS train 0.22194875423180854 valid 0.3721934640407562
LOSS train 0.22194875423180854 valid 0.3726767415278836
LOSS train 0.22194875423180854 valid 0.3730720153876713
LOSS train 0.22194875423180854 valid 0.37359831845149016
LOSS train 0.22194875423180854 valid 0.3741556326799755
LOSS train 0.22194875423180854 valid 0.3728340819478035
LOSS train 0.22194875423180854 valid 0.37161093287997776
LOSS train 0.22194875423180854 valid 0.3722649667321182
LOSS train 0.22194875423180854 valid 0.37190376419618904
LOSS train 0.22194875423180854 valid 0.37165434197300956
LOSS train 0.22194875423180854 valid 0.37139804398312287
LOSS train 0.22194875423180854 valid 0.37062156408332114
LOSS train 0.22194875423180854 valid 0.3702783601722498
LOSS train 0.22194875423180854 valid 0.36969053000211716
LOSS train 0.22194875423180854 valid 0.37037199400783927
LOSS train 0.22194875423180854 valid 0.37070151931709716
LOSS train 0.22194875423180854 valid 0.3707905067847325
LOSS train 0.22194875423180854 valid 0.3711612078806628
LOSS train 0.22194875423180854 valid 0.37090894836251453
LOSS train 0.22194875423180854 valid 0.3710670328520714
LOSS train 0.22194875423180854 valid 0.37069863708395706
LOSS train 0.22194875423180854 valid 0.37145947851240635
LOSS train 0.22194875423180854 valid 0.3718150639042412
LOSS train 0.22194875423180854 valid 0.3723042409638969
LOSS train 0.22194875423180854 valid 0.3725609604758446
LOSS train 0.22194875423180854 valid 0.3727404567599297
LOSS train 0.22194875423180854 valid 0.3727613814396433
LOSS train 0.22194875423180854 valid 0.3728255705506194
LOSS train 0.22194875423180854 valid 0.37297771162199744
LOSS train 0.22194875423180854 valid 0.3731302341016439
LOSS train 0.22194875423180854 valid 0.3731279966377077
LOSS train 0.22194875423180854 valid 0.37351722537346604
LOSS train 0.22194875423180854 valid 0.37336887043213174
LOSS train 0.22194875423180854 valid 0.3734361700437687
LOSS train 0.22194875423180854 valid 0.37374876589950073
LOSS train 0.22194875423180854 valid 0.3738917963071303
LOSS train 0.22194875423180854 valid 0.373250075288721
LOSS train 0.22194875423180854 valid 0.373089661821723
LOSS train 0.22194875423180854 valid 0.3730314793312444
LOSS train 0.22194875423180854 valid 0.37259267244422645
LOSS train 0.22194875423180854 valid 0.3730267250019571
LOSS train 0.22194875423180854 valid 0.3727649396349644
LOSS train 0.22194875423180854 valid 0.37291884396830177
LOSS train 0.22194875423180854 valid 0.3726708921840635
LOSS train 0.22194875423180854 valid 0.3722942038243558
LOSS train 0.22194875423180854 valid 0.371859352538983
LOSS train 0.22194875423180854 valid 0.3716897538378219
LOSS train 0.22194875423180854 valid 0.3712244595660538
LOSS train 0.22194875423180854 valid 0.3711260840660188
LOSS train 0.22194875423180854 valid 0.37174414242467574
LOSS train 0.22194875423180854 valid 0.3716586065292358
LOSS train 0.22194875423180854 valid 0.37180450771536144
LOSS train 0.22194875423180854 valid 0.3718282366831472
LOSS train 0.22194875423180854 valid 0.37243407289497554
LOSS train 0.22194875423180854 valid 0.3725346786569255
LOSS train 0.22194875423180854 valid 0.3722394429720365
LOSS train 0.22194875423180854 valid 0.37223562052231707
LOSS train 0.22194875423180854 valid 0.3718455901200121
LOSS train 0.22194875423180854 valid 0.37185191570368026
LOSS train 0.22194875423180854 valid 0.371766940648876
LOSS train 0.22194875423180854 valid 0.3715568255495142
LOSS train 0.22194875423180854 valid 0.3714255905326675
LOSS train 0.22194875423180854 valid 0.3713431632431754
LOSS train 0.22194875423180854 valid 0.37129104072632996
LOSS train 0.22194875423180854 valid 0.3713014539626005
LOSS train 0.22194875423180854 valid 0.3714761633958135
LOSS train 0.22194875423180854 valid 0.3714643619584699
LOSS train 0.22194875423180854 valid 0.3717187309349087
LOSS train 0.22194875423180854 valid 0.3715415344788478
LOSS train 0.22194875423180854 valid 0.3714752108272579
LOSS train 0.22194875423180854 valid 0.3712003938082991
LOSS train 0.22194875423180854 valid 0.3713602860496469
LOSS train 0.22194875423180854 valid 0.3711868117050249
LOSS train 0.22194875423180854 valid 0.37224926759262345
LOSS train 0.22194875423180854 valid 0.37233848779793555
LOSS train 0.22194875423180854 valid 0.37258641322453817
LOSS train 0.22194875423180854 valid 0.37250293130116746
LOSS train 0.22194875423180854 valid 0.3721881895080993
LOSS train 0.22194875423180854 valid 0.37236502532865484
LOSS train 0.22194875423180854 valid 0.37229961214901564
LOSS train 0.22194875423180854 valid 0.3724320469364043
LOSS train 0.22194875423180854 valid 0.3726516148218742
LOSS train 0.22194875423180854 valid 0.3725648026937132
LOSS train 0.22194875423180854 valid 0.3727037474701676
LOSS train 0.22194875423180854 valid 0.372704916030356
LOSS train 0.22194875423180854 valid 0.3727271923795342
LOSS train 0.22194875423180854 valid 0.3726530939525699
LOSS train 0.22194875423180854 valid 0.37245071762137943
LOSS train 0.22194875423180854 valid 0.3722199601995433
LOSS train 0.22194875423180854 valid 0.37176289627464804
LOSS train 0.22194875423180854 valid 0.3713630735874176
LOSS train 0.22194875423180854 valid 0.37154022631156874
LOSS train 0.22194875423180854 valid 0.37191402483843045
LOSS train 0.22194875423180854 valid 0.37163390822353815
LOSS train 0.22194875423180854 valid 0.3717839559154398
LOSS train 0.22194875423180854 valid 0.3719525666797862
LOSS train 0.22194875423180854 valid 0.3718978748684041
LOSS train 0.22194875423180854 valid 0.37152721576912456
LOSS train 0.22194875423180854 valid 0.37146486350566665
LOSS train 0.22194875423180854 valid 0.37151426744872124
LOSS train 0.22194875423180854 valid 0.37132092884608675
LOSS train 0.22194875423180854 valid 0.3712557571178133
LOSS train 0.22194875423180854 valid 0.37119035515408055
LOSS train 0.22194875423180854 valid 0.3714146155319857
LOSS train 0.22194875423180854 valid 0.37118701062388926
LOSS train 0.22194875423180854 valid 0.3710601101318995
LOSS train 0.22194875423180854 valid 0.37129556638759803
LOSS train 0.22194875423180854 valid 0.37154609292418095
LOSS train 0.22194875423180854 valid 0.37156951460030563
LOSS train 0.22194875423180854 valid 0.3715208660325278
LOSS train 0.22194875423180854 valid 0.3709746750625404
LOSS train 0.22194875423180854 valid 0.37089371905531937
LOSS train 0.22194875423180854 valid 0.3708268120327098
LOSS train 0.22194875423180854 valid 0.3708876518810049
LOSS train 0.22194875423180854 valid 0.3706381325683897
LOSS train 0.22194875423180854 valid 0.3707218297218022
LOSS train 0.22194875423180854 valid 0.3705245120675152
LOSS train 0.22194875423180854 valid 0.370532860668997
LOSS train 0.22194875423180854 valid 0.3703633738305285
LOSS train 0.22194875423180854 valid 0.37007604262877986
LOSS train 0.22194875423180854 valid 0.3697993686565986
LOSS train 0.22194875423180854 valid 0.36972708696005296
LOSS train 0.22194875423180854 valid 0.369953190917291
LOSS train 0.22194875423180854 valid 0.36958679075192924
LOSS train 0.22194875423180854 valid 0.3696756839153156
LOSS train 0.22194875423180854 valid 0.3695800006389618
LOSS train 0.22194875423180854 valid 0.3693613623505208
LOSS train 0.22194875423180854 valid 0.3692182548860512
LOSS train 0.22194875423180854 valid 0.36894433310466446
LOSS train 0.22194875423180854 valid 0.36887603165472255
LOSS train 0.22194875423180854 valid 0.3684484840893164
LOSS train 0.22194875423180854 valid 0.36852263492866627
LOSS train 0.22194875423180854 valid 0.36847896074903186
LOSS train 0.22194875423180854 valid 0.36814400424750954
LOSS train 0.22194875423180854 valid 0.36805235954563015
LOSS train 0.22194875423180854 valid 0.36824292015461696
LOSS train 0.22194875423180854 valid 0.36840445452956794
LOSS train 0.22194875423180854 valid 0.36833810735985917
LOSS train 0.22194875423180854 valid 0.3685780052847705
LOSS train 0.22194875423180854 valid 0.36864820958297945
LOSS train 0.22194875423180854 valid 0.3683387365452079
LOSS train 0.22194875423180854 valid 0.36829596371562395
LOSS train 0.22194875423180854 valid 0.36828603337986676
LOSS train 0.22194875423180854 valid 0.36829171514292375
LOSS train 0.22194875423180854 valid 0.36843113961829443
LOSS train 0.22194875423180854 valid 0.36851909174160524
LOSS train 0.22194875423180854 valid 0.3685520710718578
LOSS train 0.22194875423180854 valid 0.3684403830551886
LOSS train 0.22194875423180854 valid 0.36871288362639904
LOSS train 0.22194875423180854 valid 0.36878211862806765
LOSS train 0.22194875423180854 valid 0.36862968405087787
LOSS train 0.22194875423180854 valid 0.36874251964345445
LOSS train 0.22194875423180854 valid 0.36906989594913264
LOSS train 0.22194875423180854 valid 0.36913979144995673
LOSS train 0.22194875423180854 valid 0.3691760068637315
LOSS train 0.22194875423180854 valid 0.3692579205917276
LOSS train 0.22194875423180854 valid 0.3693102746556848
LOSS train 0.22194875423180854 valid 0.36938327705038004
LOSS train 0.22194875423180854 valid 0.3693095389353871
LOSS train 0.22194875423180854 valid 0.3692708364409259
LOSS train 0.22194875423180854 valid 0.36961685165445857
LOSS train 0.22194875423180854 valid 0.36935157631918536
LOSS train 0.22194875423180854 valid 0.369615899611123
LOSS train 0.22194875423180854 valid 0.3696818624724861
LOSS train 0.22194875423180854 valid 0.3695313894349661
LOSS train 0.22194875423180854 valid 0.36953028875092664
LOSS train 0.22194875423180854 valid 0.36966207796607276
LOSS train 0.22194875423180854 valid 0.36931900187464783
LOSS train 0.22194875423180854 valid 0.3696168613286666
LOSS train 0.22194875423180854 valid 0.37014076003774266
LOSS train 0.22194875423180854 valid 0.3704529445998523
LOSS train 0.22194875423180854 valid 0.37043127902155
LOSS train 0.22194875423180854 valid 0.3704356230222262
LOSS train 0.22194875423180854 valid 0.37024232180368516
LOSS train 0.22194875423180854 valid 0.3700783426263725
LOSS train 0.22194875423180854 valid 0.3704072359800339
LOSS train 0.22194875423180854 valid 0.37056825039871183
LOSS train 0.22194875423180854 valid 0.3708653182737411
LOSS train 0.22194875423180854 valid 0.37091727749161096
LOSS train 0.22194875423180854 valid 0.37077514240591547
LOSS train 0.22194875423180854 valid 0.3708457365924237
LOSS train 0.22194875423180854 valid 0.37099367345217615
LOSS train 0.22194875423180854 valid 0.37080855151558667
LOSS train 0.22194875423180854 valid 0.37101096596366673
LOSS train 0.22194875423180854 valid 0.3709845305869938
LOSS train 0.22194875423180854 valid 0.3708725822659639
LOSS train 0.22194875423180854 valid 0.37101485343271745
LOSS train 0.22194875423180854 valid 0.37102043822066477
LOSS train 0.22194875423180854 valid 0.3710455990789508
LOSS train 0.22194875423180854 valid 0.3710448452920625
LOSS train 0.22194875423180854 valid 0.37095752763298323
LOSS train 0.22194875423180854 valid 0.3711991301156524
LOSS train 0.22194875423180854 valid 0.37130209401752173
LOSS train 0.22194875423180854 valid 0.3714851384509855
LOSS train 0.22194875423180854 valid 0.3714493514215193
LOSS train 0.22194875423180854 valid 0.3713363086735761
LOSS train 0.22194875423180854 valid 0.3714917110780948
LOSS train 0.22194875423180854 valid 0.3718576681087999
LOSS train 0.22194875423180854 valid 0.3719378790575943
LOSS train 0.22194875423180854 valid 0.3719220398551356
LOSS train 0.22194875423180854 valid 0.3718516743183136
LOSS train 0.22194875423180854 valid 0.37164390270692715
LOSS train 0.22194875423180854 valid 0.3714725428970282
LOSS train 0.22194875423180854 valid 0.37122803283252304
LOSS train 0.22194875423180854 valid 0.3712972747595934
LOSS train 0.22194875423180854 valid 0.37126818725040983
LOSS train 0.22194875423180854 valid 0.371038584204331
LOSS train 0.22194875423180854 valid 0.37079961460532873
LOSS train 0.22194875423180854 valid 0.3706594446522608
LOSS train 0.22194875423180854 valid 0.3707135577856655
LOSS train 0.22194875423180854 valid 0.3708962238671487
LOSS train 0.22194875423180854 valid 0.37076825174418365
LOSS train 0.22194875423180854 valid 0.3707414886261943
LOSS train 0.22194875423180854 valid 0.37075303743282956
LOSS train 0.22194875423180854 valid 0.37096292842630696
LOSS train 0.22194875423180854 valid 0.3710040026697619
LOSS train 0.22194875423180854 valid 0.37089263348235296
LOSS train 0.22194875423180854 valid 0.37104952294532567
LOSS train 0.22194875423180854 valid 0.3710207861844997
LOSS train 0.22194875423180854 valid 0.3711609424782448
LOSS train 0.22194875423180854 valid 0.37121065430721994
LOSS train 0.22194875423180854 valid 0.3711105013215864
LOSS train 0.22194875423180854 valid 0.37110524998369443
LOSS train 0.22194875423180854 valid 0.3711039124919264
LOSS train 0.22194875423180854 valid 0.3711584094774763
LOSS train 0.22194875423180854 valid 0.3710074856877327
LOSS train 0.22194875423180854 valid 0.3710218890758844
LOSS train 0.22194875423180854 valid 0.37110358111511005
LOSS train 0.22194875423180854 valid 0.3712471115707171
LOSS train 0.22194875423180854 valid 0.37124314737555225
LOSS train 0.22194875423180854 valid 0.3711570033284484
LOSS train 0.22194875423180854 valid 0.3711419160069983
LOSS train 0.22194875423180854 valid 0.3710680817548149
LOSS train 0.22194875423180854 valid 0.370942280857594
LOSS train 0.22194875423180854 valid 0.37088939952618866
LOSS train 0.22194875423180854 valid 0.3709218876015755
LOSS train 0.22194875423180854 valid 0.3708129077670658
LOSS train 0.22194875423180854 valid 0.37085026684097755
LOSS train 0.22194875423180854 valid 0.37077045164550076
LOSS train 0.22194875423180854 valid 0.3709192941333078
LOSS train 0.22194875423180854 valid 0.3708278282294198
LOSS train 0.22194875423180854 valid 0.37066297902713846
LOSS train 0.22194875423180854 valid 0.37078210279016466
LOSS train 0.22194875423180854 valid 0.37088129458562386
LOSS train 0.22194875423180854 valid 0.3710326364608394
LOSS train 0.22194875423180854 valid 0.3709535481408238
LOSS train 0.22194875423180854 valid 0.37127147489619033
LOSS train 0.22194875423180854 valid 0.3711078133642303
LOSS train 0.22194875423180854 valid 0.3709134232518105
LOSS train 0.22194875423180854 valid 0.3709370860898936
LOSS train 0.22194875423180854 valid 0.37102115814502423
LOSS train 0.22194875423180854 valid 0.37110607015208963
LOSS train 0.22194875423180854 valid 0.37109042207400006
LOSS train 0.22194875423180854 valid 0.37112827172003143
LOSS train 0.22194875423180854 valid 0.3712106819029637
LOSS train 0.22194875423180854 valid 0.3712362621769761
LOSS train 0.22194875423180854 valid 0.3711040094719913
LOSS train 0.22194875423180854 valid 0.3710303688085223
LOSS train 0.22194875423180854 valid 0.3709664002016142
LOSS train 0.22194875423180854 valid 0.37117720185639613
LOSS train 0.22194875423180854 valid 0.3711030920939659
LOSS train 0.22194875423180854 valid 0.3708806848596959
LOSS train 0.22194875423180854 valid 0.3708627113425767
LOSS train 0.22194875423180854 valid 0.37074484477734426
LOSS train 0.22194875423180854 valid 0.3706472372762573
LOSS train 0.22194875423180854 valid 0.3706195608657949
LOSS train 0.22194875423180854 valid 0.37041157816162557
LOSS train 0.22194875423180854 valid 0.3703564974473931
LOSS train 0.22194875423180854 valid 0.37022451304833326
LOSS train 0.22194875423180854 valid 0.37046950925574745
LOSS train 0.22194875423180854 valid 0.37053466646567634
LOSS train 0.22194875423180854 valid 0.3704116090356959
LOSS train 0.22194875423180854 valid 0.3702377286184075
LOSS train 0.22194875423180854 valid 0.3701527079288987
LOSS train 0.22194875423180854 valid 0.37017797492978227
LOSS train 0.22194875423180854 valid 0.3700786054134369
LOSS train 0.22194875423180854 valid 0.37001532545456517
LOSS train 0.22194875423180854 valid 0.3699243121187795
LOSS train 0.22194875423180854 valid 0.36989332283860227
LOSS train 0.22194875423180854 valid 0.3699498531003456
LOSS train 0.22194875423180854 valid 0.37015619781655323
LOSS train 0.22194875423180854 valid 0.3702936631240202
LOSS train 0.22194875423180854 valid 0.37025607909475056
LOSS train 0.22194875423180854 valid 0.3701945414256783
LOSS train 0.22194875423180854 valid 0.3701910290213348
LOSS train 0.22194875423180854 valid 0.3701657994753785
LOSS train 0.22194875423180854 valid 0.37014410040054957
LOSS train 0.22194875423180854 valid 0.37027344189954725
LOSS train 0.22194875423180854 valid 0.37013611928162166
LOSS train 0.22194875423180854 valid 0.3701383631144251
LOSS train 0.22194875423180854 valid 0.370222399332752
LOSS train 0.22194875423180854 valid 0.37011232703435615
LOSS train 0.22194875423180854 valid 0.36998510985998107
LOSS train 0.22194875423180854 valid 0.3699647963208997
LOSS train 0.22194875423180854 valid 0.3701202185334875
EPOCH 25:
  batch 1 loss: 0.2556968331336975
  batch 2 loss: 0.24628466367721558
  batch 3 loss: 0.23183601597944895
  batch 4 loss: 0.2348191700875759
  batch 5 loss: 0.24170600473880768
  batch 6 loss: 0.23746803402900696
  batch 7 loss: 0.23801399980272567
  batch 8 loss: 0.2407846339046955
  batch 9 loss: 0.23822334739896986
  batch 10 loss: 0.2377088576555252
  batch 11 loss: 0.23618778044527228
  batch 12 loss: 0.23293305560946465
  batch 13 loss: 0.23246703698084906
  batch 14 loss: 0.2324243879743985
  batch 15 loss: 0.23292795519034068
  batch 16 loss: 0.23300485126674175
  batch 17 loss: 0.22914504391305587
  batch 18 loss: 0.22978121125035816
  batch 19 loss: 0.22649674117565155
  batch 20 loss: 0.22287337854504585
  batch 21 loss: 0.22208178540070853
  batch 22 loss: 0.22122639349915765
  batch 23 loss: 0.2199909667605939
  batch 24 loss: 0.21799058529237905
  batch 25 loss: 0.21946514546871185
  batch 26 loss: 0.21802583967263883
  batch 27 loss: 0.21846869146382367
  batch 28 loss: 0.2167909698826926
  batch 29 loss: 0.21675879893631772
  batch 30 loss: 0.21621172080437343
  batch 31 loss: 0.2165236218321708
  batch 32 loss: 0.21648874133825302
  batch 33 loss: 0.21643061258576132
  batch 34 loss: 0.21562058478593826
  batch 35 loss: 0.21537392139434813
  batch 36 loss: 0.21645264575878778
  batch 37 loss: 0.21632457182214065
  batch 38 loss: 0.2168396970159129
  batch 39 loss: 0.21638634342413682
  batch 40 loss: 0.21717193350195885
  batch 41 loss: 0.21717735979615188
  batch 42 loss: 0.217120437395005
  batch 43 loss: 0.21780002463695614
  batch 44 loss: 0.21726893159476193
  batch 45 loss: 0.2171842399570677
  batch 46 loss: 0.2178279776935992
  batch 47 loss: 0.21746032320438546
  batch 48 loss: 0.21632645403345427
  batch 49 loss: 0.2161687649026209
  batch 50 loss: 0.21621858596801757
  batch 51 loss: 0.21606950315774656
  batch 52 loss: 0.21613371888032326
  batch 53 loss: 0.2162317658932704
  batch 54 loss: 0.21655738712460906
  batch 55 loss: 0.2163245203820142
  batch 56 loss: 0.2162976467183658
  batch 57 loss: 0.2164706666218607
  batch 58 loss: 0.2171714691252544
  batch 59 loss: 0.21732377096758052
  batch 60 loss: 0.2170819451411565
  batch 61 loss: 0.21780994534492493
  batch 62 loss: 0.2181333625508893
  batch 63 loss: 0.2176292839031371
  batch 64 loss: 0.21808756818063557
  batch 65 loss: 0.21786520549884208
  batch 66 loss: 0.21756101602857764
  batch 67 loss: 0.2181740131840777
  batch 68 loss: 0.21840583007125294
  batch 69 loss: 0.21866466752860858
  batch 70 loss: 0.21943470814398358
  batch 71 loss: 0.21925179996121097
  batch 72 loss: 0.2192621390438742
  batch 73 loss: 0.21947988847347155
  batch 74 loss: 0.2191655895597226
  batch 75 loss: 0.21888566076755522
  batch 76 loss: 0.21958687842676514
  batch 77 loss: 0.21905014634906472
  batch 78 loss: 0.21942301304676595
  batch 79 loss: 0.21949276252637936
  batch 80 loss: 0.2190871199592948
  batch 81 loss: 0.21941443891436965
  batch 82 loss: 0.220021776128106
  batch 83 loss: 0.22011275984436632
  batch 84 loss: 0.2197687719904241
  batch 85 loss: 0.21964319874258603
  batch 86 loss: 0.22062536415665648
  batch 87 loss: 0.220338120885279
  batch 88 loss: 0.22002350217239422
  batch 89 loss: 0.22012730177198903
  batch 90 loss: 0.2205314909418424
  batch 91 loss: 0.22063788837128942
  batch 92 loss: 0.22037238951610483
  batch 93 loss: 0.22066394776426335
  batch 94 loss: 0.22104205000907817
  batch 95 loss: 0.22083989475902757
  batch 96 loss: 0.22064554318785667
  batch 97 loss: 0.22074042383542994
  batch 98 loss: 0.2209618140240105
  batch 99 loss: 0.2213293058101577
  batch 100 loss: 0.22103159934282302
  batch 101 loss: 0.22079941025464841
  batch 102 loss: 0.2211353850423121
  batch 103 loss: 0.22157401150291406
  batch 104 loss: 0.22177669649513868
  batch 105 loss: 0.22149185964039395
  batch 106 loss: 0.2214851663360056
  batch 107 loss: 0.22123694851576725
  batch 108 loss: 0.22149858416782486
  batch 109 loss: 0.2214753035831889
  batch 110 loss: 0.2215715076435696
  batch 111 loss: 0.22167098025480905
  batch 112 loss: 0.22146091211055005
  batch 113 loss: 0.22152484610544898
  batch 114 loss: 0.22178836049217926
  batch 115 loss: 0.22204508535240008
  batch 116 loss: 0.22217223985955634
  batch 117 loss: 0.22226148741876978
  batch 118 loss: 0.22200162418312946
  batch 119 loss: 0.22214252219981506
  batch 120 loss: 0.2216665230691433
  batch 121 loss: 0.2215142910145531
  batch 122 loss: 0.22129147231090265
  batch 123 loss: 0.2210830297411942
  batch 124 loss: 0.2212549925090805
  batch 125 loss: 0.22098434519767762
  batch 126 loss: 0.22085374782955836
  batch 127 loss: 0.22106727476664415
  batch 128 loss: 0.2207421981729567
  batch 129 loss: 0.22096478338389433
  batch 130 loss: 0.2209303679374548
  batch 131 loss: 0.220878551365765
  batch 132 loss: 0.22080749182990103
  batch 133 loss: 0.22115576087980343
  batch 134 loss: 0.22113456666024764
  batch 135 loss: 0.22092730811348668
  batch 136 loss: 0.22104172912590644
  batch 137 loss: 0.2209766076429047
  batch 138 loss: 0.22082802588525025
  batch 139 loss: 0.22091699632809317
  batch 140 loss: 0.2208075928900923
  batch 141 loss: 0.22099935131292817
  batch 142 loss: 0.22087470148230942
  batch 143 loss: 0.22062078357993306
  batch 144 loss: 0.22063738531950447
  batch 145 loss: 0.2206489668837909
  batch 146 loss: 0.22062144428491592
  batch 147 loss: 0.22081410408425492
  batch 148 loss: 0.22084726977187233
  batch 149 loss: 0.22069662379338437
  batch 150 loss: 0.22067110548416774
  batch 151 loss: 0.22067200861229802
  batch 152 loss: 0.2207427969888637
  batch 153 loss: 0.22049739314060585
  batch 154 loss: 0.22059064639079107
  batch 155 loss: 0.2204574275401331
  batch 156 loss: 0.2203380471238723
  batch 157 loss: 0.22057824947272137
  batch 158 loss: 0.22047618701110913
  batch 159 loss: 0.22046967534898962
  batch 160 loss: 0.220181904733181
  batch 161 loss: 0.22026370484266222
  batch 162 loss: 0.22003698891695636
  batch 163 loss: 0.22008362567863582
  batch 164 loss: 0.21977247370452416
  batch 165 loss: 0.21966856862559464
  batch 166 loss: 0.21953943766743303
  batch 167 loss: 0.21938875371110653
  batch 168 loss: 0.21924797055267153
  batch 169 loss: 0.21897520336526385
  batch 170 loss: 0.21888368883553674
  batch 171 loss: 0.21880934531228585
  batch 172 loss: 0.21875820404221846
  batch 173 loss: 0.2185045725348368
  batch 174 loss: 0.21862774530703993
  batch 175 loss: 0.21866448385374887
  batch 176 loss: 0.21853305170820517
  batch 177 loss: 0.2185464985458191
  batch 178 loss: 0.218735267858157
  batch 179 loss: 0.2186386987317208
  batch 180 loss: 0.21842806827690867
  batch 181 loss: 0.21852913540041907
  batch 182 loss: 0.21854595275043132
  batch 183 loss: 0.21842236483031935
  batch 184 loss: 0.21844644326230753
  batch 185 loss: 0.2186882057705441
  batch 186 loss: 0.21891545464274703
  batch 187 loss: 0.21883090350398404
  batch 188 loss: 0.21853595250781546
  batch 189 loss: 0.21825517335581401
  batch 190 loss: 0.2182177753040665
  batch 191 loss: 0.21826703120900698
  batch 192 loss: 0.21812080948924026
  batch 193 loss: 0.2179764493759432
  batch 194 loss: 0.21806489584064975
  batch 195 loss: 0.21815989651741124
  batch 196 loss: 0.21807240124563781
  batch 197 loss: 0.21801950453501667
  batch 198 loss: 0.21818263241738983
  batch 199 loss: 0.21802629567869944
  batch 200 loss: 0.21820572063326835
  batch 201 loss: 0.21834558923149583
  batch 202 loss: 0.21833029204961096
  batch 203 loss: 0.2182957474234069
  batch 204 loss: 0.21817832308657029
  batch 205 loss: 0.2183354088445989
  batch 206 loss: 0.21836963165732262
  batch 207 loss: 0.21848620164797503
  batch 208 loss: 0.21861702038978154
  batch 209 loss: 0.2185284123181156
  batch 210 loss: 0.21866598476966223
  batch 211 loss: 0.2184806905369058
  batch 212 loss: 0.2184360555179839
  batch 213 loss: 0.2183034490671516
  batch 214 loss: 0.21832518335257736
  batch 215 loss: 0.21823605222757472
  batch 216 loss: 0.21792622759110397
  batch 217 loss: 0.21784068300702056
  batch 218 loss: 0.21768522228396267
  batch 219 loss: 0.21770864765938014
  batch 220 loss: 0.21760823665694756
  batch 221 loss: 0.21760931685229773
  batch 222 loss: 0.21747697856243667
  batch 223 loss: 0.21744326391829502
  batch 224 loss: 0.21737902638103282
  batch 225 loss: 0.2173463021384345
  batch 226 loss: 0.2174339596412878
  batch 227 loss: 0.21733343273007397
  batch 228 loss: 0.2173590228745812
  batch 229 loss: 0.2173610408363384
  batch 230 loss: 0.21755401874366015
  batch 231 loss: 0.21753831001329216
  batch 232 loss: 0.21741993802374807
  batch 233 loss: 0.21735816963752452
  batch 234 loss: 0.21731342903823933
  batch 235 loss: 0.21729091323436575
  batch 236 loss: 0.21715351067862268
  batch 237 loss: 0.21716460881102437
  batch 238 loss: 0.21724121015863257
  batch 239 loss: 0.21708428099065644
  batch 240 loss: 0.2170190902426839
  batch 241 loss: 0.21703362681311691
  batch 242 loss: 0.2168308631324571
  batch 243 loss: 0.21685635853451465
  batch 244 loss: 0.21681890325223813
  batch 245 loss: 0.21670439060853453
  batch 246 loss: 0.21677300164370034
  batch 247 loss: 0.21697685774038677
  batch 248 loss: 0.21703650015256098
  batch 249 loss: 0.21705507268627963
  batch 250 loss: 0.21696296578645707
  batch 251 loss: 0.21695516694826908
  batch 252 loss: 0.2170024219722975
  batch 253 loss: 0.21703773242211624
  batch 254 loss: 0.21696850616396882
  batch 255 loss: 0.21707302957188848
  batch 256 loss: 0.21736907871672884
  batch 257 loss: 0.217387948112729
  batch 258 loss: 0.2172820690528367
  batch 259 loss: 0.2172919257497235
  batch 260 loss: 0.21732141215067644
  batch 261 loss: 0.21735433623251788
  batch 262 loss: 0.21731667192155169
  batch 263 loss: 0.21742954946516133
  batch 264 loss: 0.21745376379200906
  batch 265 loss: 0.2173832692627637
  batch 266 loss: 0.2173790928340496
  batch 267 loss: 0.217370309242595
  batch 268 loss: 0.21721259443395174
  batch 269 loss: 0.21728826882006066
  batch 270 loss: 0.21764674379869745
  batch 271 loss: 0.21770530730156
  batch 272 loss: 0.21773368388633518
  batch 273 loss: 0.217795777288112
  batch 274 loss: 0.218004962769303
  batch 275 loss: 0.21815216655080968
  batch 276 loss: 0.21818157765960347
  batch 277 loss: 0.218200434900363
  batch 278 loss: 0.21827897552749237
  batch 279 loss: 0.21841870899909713
  batch 280 loss: 0.21848405855042594
  batch 281 loss: 0.21843361780312562
  batch 282 loss: 0.2184524671405765
  batch 283 loss: 0.21848880876079466
  batch 284 loss: 0.21869459286542006
  batch 285 loss: 0.21881761362678126
  batch 286 loss: 0.2187799622754117
  batch 287 loss: 0.21880217664956217
  batch 288 loss: 0.21888216238261926
  batch 289 loss: 0.21898540405872371
  batch 290 loss: 0.21893079198639967
  batch 291 loss: 0.21896411048382827
  batch 292 loss: 0.21899854580629363
  batch 293 loss: 0.21899639131062673
  batch 294 loss: 0.21904716225100213
  batch 295 loss: 0.21914828909655748
  batch 296 loss: 0.21923610084765666
  batch 297 loss: 0.21923385184220592
  batch 298 loss: 0.21919205514776627
  batch 299 loss: 0.21921424652421753
  batch 300 loss: 0.21942964335282644
  batch 301 loss: 0.21946129029573397
  batch 302 loss: 0.21943446727383215
  batch 303 loss: 0.21931743425111172
  batch 304 loss: 0.21928662365596546
  batch 305 loss: 0.21922119284262423
  batch 306 loss: 0.21943901396460003
  batch 307 loss: 0.21956270928297447
  batch 308 loss: 0.2196230117183227
  batch 309 loss: 0.21968729914584978
  batch 310 loss: 0.21960607459468226
  batch 311 loss: 0.2198339382551874
  batch 312 loss: 0.21991837583482265
  batch 313 loss: 0.21999593540883292
  batch 314 loss: 0.21996337464850418
  batch 315 loss: 0.2200186201031246
  batch 316 loss: 0.21999906192097482
  batch 317 loss: 0.21999055828769876
  batch 318 loss: 0.2200412315861234
  batch 319 loss: 0.22006822561956124
  batch 320 loss: 0.22012433386407793
  batch 321 loss: 0.22017126500235168
  batch 322 loss: 0.22016916368503747
  batch 323 loss: 0.22014044252896087
  batch 324 loss: 0.22004274591619585
  batch 325 loss: 0.2199785406772907
  batch 326 loss: 0.21994959951544102
  batch 327 loss: 0.21986161778461677
  batch 328 loss: 0.21976750489415192
  batch 329 loss: 0.21985473282250229
  batch 330 loss: 0.21969480677084488
  batch 331 loss: 0.21966650163659157
  batch 332 loss: 0.2197857505406242
  batch 333 loss: 0.22002485636118296
  batch 334 loss: 0.21993488336573103
  batch 335 loss: 0.21980368828595573
  batch 336 loss: 0.21986579952672833
  batch 337 loss: 0.2200189360613639
  batch 338 loss: 0.22006999079645032
  batch 339 loss: 0.22014857287955494
  batch 340 loss: 0.2202113737078274
  batch 341 loss: 0.2203613201544082
  batch 342 loss: 0.22039591385955698
  batch 343 loss: 0.22052245810845156
  batch 344 loss: 0.2205993991431802
  batch 345 loss: 0.22070460060368413
  batch 346 loss: 0.22070818049887012
  batch 347 loss: 0.22073740922236648
  batch 348 loss: 0.2208000806470712
  batch 349 loss: 0.22078462299439833
  batch 350 loss: 0.22083002588578632
  batch 351 loss: 0.22081896696674858
  batch 352 loss: 0.22088698445903984
  batch 353 loss: 0.2208454524491394
  batch 354 loss: 0.2210033966154702
  batch 355 loss: 0.22097854740183118
  batch 356 loss: 0.2209385789595963
  batch 357 loss: 0.2210027230720894
  batch 358 loss: 0.22110065218456632
  batch 359 loss: 0.2210686278210377
  batch 360 loss: 0.22115067491928736
  batch 361 loss: 0.22116128109663807
  batch 362 loss: 0.22115665126735992
  batch 363 loss: 0.2210450021360203
  batch 364 loss: 0.22096474083897832
  batch 365 loss: 0.22098934544275886
  batch 366 loss: 0.22103680059558056
  batch 367 loss: 0.22101193889772536
  batch 368 loss: 0.22097869723549354
  batch 369 loss: 0.220941207152072
  batch 370 loss: 0.22083351289903796
  batch 371 loss: 0.22092480198392328
  batch 372 loss: 0.22096685071786246
  batch 373 loss: 0.22087749211621988
  batch 374 loss: 0.22070774734817086
  batch 375 loss: 0.220588006178538
  batch 376 loss: 0.22065495854204006
  batch 377 loss: 0.22068292949971216
  batch 378 loss: 0.22065330935376032
  batch 379 loss: 0.22061428777617956
  batch 380 loss: 0.22069185089908147
  batch 381 loss: 0.2206760206482229
  batch 382 loss: 0.22065206593711964
  batch 383 loss: 0.22063313311758614
  batch 384 loss: 0.22060604048116753
  batch 385 loss: 0.22067373394966125
  batch 386 loss: 0.22064712817341553
  batch 387 loss: 0.2207190119865945
  batch 388 loss: 0.2207306635364429
  batch 389 loss: 0.22071804948974696
  batch 390 loss: 0.22067085749063736
  batch 391 loss: 0.22074327699821014
  batch 392 loss: 0.22071042865970913
  batch 393 loss: 0.2206089304755359
  batch 394 loss: 0.22061138172742678
  batch 395 loss: 0.22055197984357425
  batch 396 loss: 0.22044710122575664
  batch 397 loss: 0.22046264795872667
  batch 398 loss: 0.22041718362264298
  batch 399 loss: 0.2204857262080176
  batch 400 loss: 0.22058136079460383
  batch 401 loss: 0.22046178230026417
  batch 402 loss: 0.22046216217736106
  batch 403 loss: 0.22044618564563118
  batch 404 loss: 0.22045433086038815
  batch 405 loss: 0.22044646154951167
  batch 406 loss: 0.2205048712163136
  batch 407 loss: 0.22048476481174076
  batch 408 loss: 0.22050439456806464
  batch 409 loss: 0.22053193302784105
  batch 410 loss: 0.2206430097178715
  batch 411 loss: 0.22063517429097726
  batch 412 loss: 0.22062479389958012
  batch 413 loss: 0.22068802124795844
  batch 414 loss: 0.2207472996003386
  batch 415 loss: 0.2207646320742297
  batch 416 loss: 0.22085681310496652
  batch 417 loss: 0.22084189340365018
  batch 418 loss: 0.2208367313066738
  batch 419 loss: 0.2208736809170616
  batch 420 loss: 0.22082432416223344
  batch 421 loss: 0.22072597970022442
  batch 422 loss: 0.22086000527250824
  batch 423 loss: 0.22081670612449059
  batch 424 loss: 0.22076826307149428
  batch 425 loss: 0.22069372510208804
  batch 426 loss: 0.22064965987849122
  batch 427 loss: 0.22063565233273025
  batch 428 loss: 0.22062429256528338
  batch 429 loss: 0.22069604948406174
  batch 430 loss: 0.22065985687943393
  batch 431 loss: 0.2207050632918794
  batch 432 loss: 0.22068854052297496
  batch 433 loss: 0.2207302723690764
  batch 434 loss: 0.22082649919843894
  batch 435 loss: 0.22086492184249834
  batch 436 loss: 0.2209242994495488
  batch 437 loss: 0.2209682220793425
  batch 438 loss: 0.22103214029171694
  batch 439 loss: 0.22103647942016227
  batch 440 loss: 0.22110262841663578
  batch 441 loss: 0.22111388439494195
  batch 442 loss: 0.22106713434150316
  batch 443 loss: 0.22105514138735174
  batch 444 loss: 0.22104570524649578
  batch 445 loss: 0.22107184187749798
  batch 446 loss: 0.22103523763706867
  batch 447 loss: 0.22098864808311933
  batch 448 loss: 0.22104748587922327
  batch 449 loss: 0.2210473622670418
  batch 450 loss: 0.2210251232650545
  batch 451 loss: 0.22100385270203296
  batch 452 loss: 0.22111078806683027
  batch 453 loss: 0.2211652457977236
  batch 454 loss: 0.2211525089677735
  batch 455 loss: 0.2211291416988268
  batch 456 loss: 0.22115170890301988
  batch 457 loss: 0.22112972346385146
  batch 458 loss: 0.2210885192304199
  batch 459 loss: 0.22121294245351114
  batch 460 loss: 0.22127254964864773
  batch 461 loss: 0.22122501573826384
  batch 462 loss: 0.221218302039615
  batch 463 loss: 0.22119468852845434
  batch 464 loss: 0.22120588205369382
  batch 465 loss: 0.22110829122604864
  batch 466 loss: 0.2210441090390406
  batch 467 loss: 0.2210208970806073
  batch 468 loss: 0.22097952148089042
  batch 469 loss: 0.2210744616192287
  batch 470 loss: 0.22098300342864177
  batch 471 loss: 0.220965758345689
  batch 472 loss: 0.2207684945630825
LOSS train 0.2207684945630825 valid 0.35620754957199097
LOSS train 0.2207684945630825 valid 0.3445127159357071
LOSS train 0.2207684945630825 valid 0.3365783492724101
LOSS train 0.2207684945630825 valid 0.331976480782032
LOSS train 0.2207684945630825 valid 0.32766595482826233
LOSS train 0.2207684945630825 valid 0.3329729437828064
LOSS train 0.2207684945630825 valid 0.34307060071400236
LOSS train 0.2207684945630825 valid 0.3421422317624092
LOSS train 0.2207684945630825 valid 0.34111713038550484
LOSS train 0.2207684945630825 valid 0.3428893178701401
LOSS train 0.2207684945630825 valid 0.3405838771299882
LOSS train 0.2207684945630825 valid 0.3407331208388011
LOSS train 0.2207684945630825 valid 0.3397644620675307
LOSS train 0.2207684945630825 valid 0.3393151249204363
LOSS train 0.2207684945630825 valid 0.3328947573900223
LOSS train 0.2207684945630825 valid 0.3336117034777999
LOSS train 0.2207684945630825 valid 0.3364635451751597
LOSS train 0.2207684945630825 valid 0.3395076385802693
LOSS train 0.2207684945630825 valid 0.34143007978012685
LOSS train 0.2207684945630825 valid 0.3400949902832508
LOSS train 0.2207684945630825 valid 0.3386305818955104
LOSS train 0.2207684945630825 valid 0.3364126905798912
LOSS train 0.2207684945630825 valid 0.33754006481688953
LOSS train 0.2207684945630825 valid 0.33545696921646595
LOSS train 0.2207684945630825 valid 0.3350570219755173
LOSS train 0.2207684945630825 valid 0.3343848545963948
LOSS train 0.2207684945630825 valid 0.3349162411910516
LOSS train 0.2207684945630825 valid 0.33534329012036324
LOSS train 0.2207684945630825 valid 0.33488808675058956
LOSS train 0.2207684945630825 valid 0.3360823219021161
LOSS train 0.2207684945630825 valid 0.33826840741019093
LOSS train 0.2207684945630825 valid 0.33825033297762275
LOSS train 0.2207684945630825 valid 0.3397811321598111
LOSS train 0.2207684945630825 valid 0.33950255592079726
LOSS train 0.2207684945630825 valid 0.34166430958679744
LOSS train 0.2207684945630825 valid 0.34130963476167786
LOSS train 0.2207684945630825 valid 0.3409869852098259
LOSS train 0.2207684945630825 valid 0.34202642778032705
LOSS train 0.2207684945630825 valid 0.3412438650161792
LOSS train 0.2207684945630825 valid 0.34129366241395476
LOSS train 0.2207684945630825 valid 0.34289720690831904
LOSS train 0.2207684945630825 valid 0.3439862469122523
LOSS train 0.2207684945630825 valid 0.34351270108721976
LOSS train 0.2207684945630825 valid 0.34450839629227464
LOSS train 0.2207684945630825 valid 0.34386676450570425
LOSS train 0.2207684945630825 valid 0.3445726597438688
LOSS train 0.2207684945630825 valid 0.3460140199737346
LOSS train 0.2207684945630825 valid 0.3462461701904734
LOSS train 0.2207684945630825 valid 0.3466725991088517
LOSS train 0.2207684945630825 valid 0.3450791409611702
LOSS train 0.2207684945630825 valid 0.3445345218859467
LOSS train 0.2207684945630825 valid 0.34414462315348476
LOSS train 0.2207684945630825 valid 0.3440814639599818
LOSS train 0.2207684945630825 valid 0.34413718928893405
LOSS train 0.2207684945630825 valid 0.34380129277706145
LOSS train 0.2207684945630825 valid 0.342966638771551
LOSS train 0.2207684945630825 valid 0.34299738809727787
LOSS train 0.2207684945630825 valid 0.34255137602830754
LOSS train 0.2207684945630825 valid 0.34318437793497314
LOSS train 0.2207684945630825 valid 0.34304030016064646
LOSS train 0.2207684945630825 valid 0.3421181032403571
LOSS train 0.2207684945630825 valid 0.343559650403838
LOSS train 0.2207684945630825 valid 0.3440795297187472
LOSS train 0.2207684945630825 valid 0.34540461213327944
LOSS train 0.2207684945630825 valid 0.34596622700874624
LOSS train 0.2207684945630825 valid 0.3458659547296437
LOSS train 0.2207684945630825 valid 0.3450287662780107
LOSS train 0.2207684945630825 valid 0.3450543707346215
LOSS train 0.2207684945630825 valid 0.3439949446398279
LOSS train 0.2207684945630825 valid 0.34439115673303605
LOSS train 0.2207684945630825 valid 0.34385506892707984
LOSS train 0.2207684945630825 valid 0.34405493963923717
LOSS train 0.2207684945630825 valid 0.34396661287301206
LOSS train 0.2207684945630825 valid 0.3438740648530625
LOSS train 0.2207684945630825 valid 0.34368108133474984
LOSS train 0.2207684945630825 valid 0.34422750241662325
LOSS train 0.2207684945630825 valid 0.3445196556193488
LOSS train 0.2207684945630825 valid 0.3448411549131076
LOSS train 0.2207684945630825 valid 0.34524857488614097
LOSS train 0.2207684945630825 valid 0.34403083603829143
LOSS train 0.2207684945630825 valid 0.34301661470054107
LOSS train 0.2207684945630825 valid 0.3438272030978668
LOSS train 0.2207684945630825 valid 0.343618935310697
LOSS train 0.2207684945630825 valid 0.3433545371961026
LOSS train 0.2207684945630825 valid 0.3428915600566303
LOSS train 0.2207684945630825 valid 0.3423354939319367
LOSS train 0.2207684945630825 valid 0.3421459083241978
LOSS train 0.2207684945630825 valid 0.3414500492878936
LOSS train 0.2207684945630825 valid 0.34211099633340086
LOSS train 0.2207684945630825 valid 0.34250315378109614
LOSS train 0.2207684945630825 valid 0.3427168947982264
LOSS train 0.2207684945630825 valid 0.34291370715136116
LOSS train 0.2207684945630825 valid 0.34271641844703304
LOSS train 0.2207684945630825 valid 0.34289974338830786
LOSS train 0.2207684945630825 valid 0.3426724833877463
LOSS train 0.2207684945630825 valid 0.34342398727312684
LOSS train 0.2207684945630825 valid 0.3438689372281438
LOSS train 0.2207684945630825 valid 0.34421162596162486
LOSS train 0.2207684945630825 valid 0.3445388522714075
LOSS train 0.2207684945630825 valid 0.3446726249158382
LOSS train 0.2207684945630825 valid 0.3448430439918348
LOSS train 0.2207684945630825 valid 0.34493085727387784
LOSS train 0.2207684945630825 valid 0.345147978767608
LOSS train 0.2207684945630825 valid 0.34532156113821727
LOSS train 0.2207684945630825 valid 0.3453246109542393
LOSS train 0.2207684945630825 valid 0.34567726628397993
LOSS train 0.2207684945630825 valid 0.34548220748656266
LOSS train 0.2207684945630825 valid 0.3456797932309133
LOSS train 0.2207684945630825 valid 0.34615715258165236
LOSS train 0.2207684945630825 valid 0.34619574397802355
LOSS train 0.2207684945630825 valid 0.3454761154748298
LOSS train 0.2207684945630825 valid 0.3452761400757091
LOSS train 0.2207684945630825 valid 0.3451901108553979
LOSS train 0.2207684945630825 valid 0.3448203034829675
LOSS train 0.2207684945630825 valid 0.3452341837727505
LOSS train 0.2207684945630825 valid 0.3448388475025522
LOSS train 0.2207684945630825 valid 0.3449912011368662
LOSS train 0.2207684945630825 valid 0.34470926995499657
LOSS train 0.2207684945630825 valid 0.3443675075008088
LOSS train 0.2207684945630825 valid 0.34398406036198137
LOSS train 0.2207684945630825 valid 0.3438368564055971
LOSS train 0.2207684945630825 valid 0.3434866944053134
LOSS train 0.2207684945630825 valid 0.3434874087087507
LOSS train 0.2207684945630825 valid 0.3439630261592327
LOSS train 0.2207684945630825 valid 0.34383930265903473
LOSS train 0.2207684945630825 valid 0.34391277330735376
LOSS train 0.2207684945630825 valid 0.34395386630625235
LOSS train 0.2207684945630825 valid 0.3444285368314013
LOSS train 0.2207684945630825 valid 0.3446990342796311
LOSS train 0.2207684945630825 valid 0.3443583544630271
LOSS train 0.2207684945630825 valid 0.34426541694702995
LOSS train 0.2207684945630825 valid 0.343935363784884
LOSS train 0.2207684945630825 valid 0.3439092556560846
LOSS train 0.2207684945630825 valid 0.3437459145241709
LOSS train 0.2207684945630825 valid 0.34343220889568327
LOSS train 0.2207684945630825 valid 0.3432954388725407
LOSS train 0.2207684945630825 valid 0.343096550037391
LOSS train 0.2207684945630825 valid 0.3430442221570706
LOSS train 0.2207684945630825 valid 0.3429889103277124
LOSS train 0.2207684945630825 valid 0.3430255859025887
LOSS train 0.2207684945630825 valid 0.34305331628796054
LOSS train 0.2207684945630825 valid 0.34337317576290854
LOSS train 0.2207684945630825 valid 0.34325575359634586
LOSS train 0.2207684945630825 valid 0.3431957799734341
LOSS train 0.2207684945630825 valid 0.3428958610214036
LOSS train 0.2207684945630825 valid 0.343109412246371
LOSS train 0.2207684945630825 valid 0.3429464919834721
LOSS train 0.2207684945630825 valid 0.3440337776130921
LOSS train 0.2207684945630825 valid 0.3440570108242483
LOSS train 0.2207684945630825 valid 0.3441970925529798
LOSS train 0.2207684945630825 valid 0.34414390430150443
LOSS train 0.2207684945630825 valid 0.3437956325513752
LOSS train 0.2207684945630825 valid 0.3438938395454993
LOSS train 0.2207684945630825 valid 0.3437668341514352
LOSS train 0.2207684945630825 valid 0.34398670359965294
LOSS train 0.2207684945630825 valid 0.34417377077998257
LOSS train 0.2207684945630825 valid 0.34409988505445466
LOSS train 0.2207684945630825 valid 0.3441614907564996
LOSS train 0.2207684945630825 valid 0.3441762748941685
LOSS train 0.2207684945630825 valid 0.3441958178766072
LOSS train 0.2207684945630825 valid 0.3440808823204929
LOSS train 0.2207684945630825 valid 0.3438834506604407
LOSS train 0.2207684945630825 valid 0.34360865864651335
LOSS train 0.2207684945630825 valid 0.3431240248425705
LOSS train 0.2207684945630825 valid 0.34273632593227155
LOSS train 0.2207684945630825 valid 0.3428550753069211
LOSS train 0.2207684945630825 valid 0.34324194116149837
LOSS train 0.2207684945630825 valid 0.34302047081291676
LOSS train 0.2207684945630825 valid 0.3432490413887261
LOSS train 0.2207684945630825 valid 0.3433218539637678
LOSS train 0.2207684945630825 valid 0.3432720231208188
LOSS train 0.2207684945630825 valid 0.3429823953571708
LOSS train 0.2207684945630825 valid 0.34287684527091206
LOSS train 0.2207684945630825 valid 0.34293365110268537
LOSS train 0.2207684945630825 valid 0.34270317137241363
LOSS train 0.2207684945630825 valid 0.34266100595281884
LOSS train 0.2207684945630825 valid 0.34262302273747614
LOSS train 0.2207684945630825 valid 0.3428212425179696
LOSS train 0.2207684945630825 valid 0.34258693864558665
LOSS train 0.2207684945630825 valid 0.3424577054878076
LOSS train 0.2207684945630825 valid 0.342616079508929
LOSS train 0.2207684945630825 valid 0.34291423505151664
LOSS train 0.2207684945630825 valid 0.3429695178087943
LOSS train 0.2207684945630825 valid 0.3429246174738459
LOSS train 0.2207684945630825 valid 0.3424829569217321
LOSS train 0.2207684945630825 valid 0.34239478160937625
LOSS train 0.2207684945630825 valid 0.34230919277922994
LOSS train 0.2207684945630825 valid 0.3423279175891521
LOSS train 0.2207684945630825 valid 0.3421059063975773
LOSS train 0.2207684945630825 valid 0.3422482240356897
LOSS train 0.2207684945630825 valid 0.34211135620534106
LOSS train 0.2207684945630825 valid 0.34212536946870387
LOSS train 0.2207684945630825 valid 0.3419269041539474
LOSS train 0.2207684945630825 valid 0.34173672077889294
LOSS train 0.2207684945630825 valid 0.3414904413314966
LOSS train 0.2207684945630825 valid 0.3413875070305503
LOSS train 0.2207684945630825 valid 0.3415606039278398
LOSS train 0.2207684945630825 valid 0.3412078121545339
LOSS train 0.2207684945630825 valid 0.34122519105223553
LOSS train 0.2207684945630825 valid 0.34117045409977437
LOSS train 0.2207684945630825 valid 0.34098822879257484
LOSS train 0.2207684945630825 valid 0.34094562145447965
LOSS train 0.2207684945630825 valid 0.34068698050646945
LOSS train 0.2207684945630825 valid 0.3406900343503438
LOSS train 0.2207684945630825 valid 0.3402972358755949
LOSS train 0.2207684945630825 valid 0.3404226348672098
LOSS train 0.2207684945630825 valid 0.3404318473621267
LOSS train 0.2207684945630825 valid 0.3401698340446903
LOSS train 0.2207684945630825 valid 0.34006906817689464
LOSS train 0.2207684945630825 valid 0.34024045304173517
LOSS train 0.2207684945630825 valid 0.3404774895352775
LOSS train 0.2207684945630825 valid 0.34046570879389654
LOSS train 0.2207684945630825 valid 0.3407428009269383
LOSS train 0.2207684945630825 valid 0.34084397784181847
LOSS train 0.2207684945630825 valid 0.3405342927505804
LOSS train 0.2207684945630825 valid 0.3405260419542039
LOSS train 0.2207684945630825 valid 0.34053228726859464
LOSS train 0.2207684945630825 valid 0.34053760391036303
LOSS train 0.2207684945630825 valid 0.3406770956706783
LOSS train 0.2207684945630825 valid 0.3407095246016979
LOSS train 0.2207684945630825 valid 0.34072468743335066
LOSS train 0.2207684945630825 valid 0.340570398748995
LOSS train 0.2207684945630825 valid 0.3408639799185398
LOSS train 0.2207684945630825 valid 0.340940614563546
LOSS train 0.2207684945630825 valid 0.340803209344546
LOSS train 0.2207684945630825 valid 0.3408852254526805
LOSS train 0.2207684945630825 valid 0.3412202338683973
LOSS train 0.2207684945630825 valid 0.34130823108972164
LOSS train 0.2207684945630825 valid 0.34139122589446574
LOSS train 0.2207684945630825 valid 0.34147133082151415
LOSS train 0.2207684945630825 valid 0.3414849776339221
LOSS train 0.2207684945630825 valid 0.34152846430139294
LOSS train 0.2207684945630825 valid 0.3414031162921963
LOSS train 0.2207684945630825 valid 0.34135557080690676
LOSS train 0.2207684945630825 valid 0.3417197042957265
LOSS train 0.2207684945630825 valid 0.34150328884943054
LOSS train 0.2207684945630825 valid 0.34169681923550393
LOSS train 0.2207684945630825 valid 0.34169547779469933
LOSS train 0.2207684945630825 valid 0.3415229756338327
LOSS train 0.2207684945630825 valid 0.341537163220346
LOSS train 0.2207684945630825 valid 0.341627205741356
LOSS train 0.2207684945630825 valid 0.34134465930136765
LOSS train 0.2207684945630825 valid 0.34164105404796913
LOSS train 0.2207684945630825 valid 0.3421457034276157
LOSS train 0.2207684945630825 valid 0.3424622166521695
LOSS train 0.2207684945630825 valid 0.34245550396238883
LOSS train 0.2207684945630825 valid 0.342470389327057
LOSS train 0.2207684945630825 valid 0.3422871088428843
LOSS train 0.2207684945630825 valid 0.342184723919654
LOSS train 0.2207684945630825 valid 0.3425348020195961
LOSS train 0.2207684945630825 valid 0.34272260251510667
LOSS train 0.2207684945630825 valid 0.3429569120090159
LOSS train 0.2207684945630825 valid 0.34287939965724945
LOSS train 0.2207684945630825 valid 0.3426971402107261
LOSS train 0.2207684945630825 valid 0.3427542827292985
LOSS train 0.2207684945630825 valid 0.34287986758863553
LOSS train 0.2207684945630825 valid 0.3427803806873611
LOSS train 0.2207684945630825 valid 0.3429582676337671
LOSS train 0.2207684945630825 valid 0.343009138026753
LOSS train 0.2207684945630825 valid 0.34285365027877
LOSS train 0.2207684945630825 valid 0.34300596703743114
LOSS train 0.2207684945630825 valid 0.3430041997828556
LOSS train 0.2207684945630825 valid 0.34310257587822673
LOSS train 0.2207684945630825 valid 0.34310776655646885
LOSS train 0.2207684945630825 valid 0.343032066046067
LOSS train 0.2207684945630825 valid 0.34333389510113493
LOSS train 0.2207684945630825 valid 0.34339619000976007
LOSS train 0.2207684945630825 valid 0.34358336815415924
LOSS train 0.2207684945630825 valid 0.34360493941156395
LOSS train 0.2207684945630825 valid 0.34346481059436446
LOSS train 0.2207684945630825 valid 0.3436193694377737
LOSS train 0.2207684945630825 valid 0.34395322997999545
LOSS train 0.2207684945630825 valid 0.34396566974592735
LOSS train 0.2207684945630825 valid 0.34394532904355196
LOSS train 0.2207684945630825 valid 0.343862681443041
LOSS train 0.2207684945630825 valid 0.3436469153325627
LOSS train 0.2207684945630825 valid 0.3434779851122453
LOSS train 0.2207684945630825 valid 0.34327683351237137
LOSS train 0.2207684945630825 valid 0.34333797408047545
LOSS train 0.2207684945630825 valid 0.3433165836014918
LOSS train 0.2207684945630825 valid 0.3431287282196228
LOSS train 0.2207684945630825 valid 0.3429239810252866
LOSS train 0.2207684945630825 valid 0.3428359613507038
LOSS train 0.2207684945630825 valid 0.34287789773563265
LOSS train 0.2207684945630825 valid 0.34302239559198683
LOSS train 0.2207684945630825 valid 0.3429115009787199
LOSS train 0.2207684945630825 valid 0.3428474558251245
LOSS train 0.2207684945630825 valid 0.34287275761986774
LOSS train 0.2207684945630825 valid 0.34304147747354935
LOSS train 0.2207684945630825 valid 0.34309891477741045
LOSS train 0.2207684945630825 valid 0.3429914046706203
LOSS train 0.2207684945630825 valid 0.3431606612691324
LOSS train 0.2207684945630825 valid 0.34312822240001106
LOSS train 0.2207684945630825 valid 0.34324412397584136
LOSS train 0.2207684945630825 valid 0.3433200568970987
LOSS train 0.2207684945630825 valid 0.3432013836966173
LOSS train 0.2207684945630825 valid 0.34324465240492963
LOSS train 0.2207684945630825 valid 0.3431924218999459
LOSS train 0.2207684945630825 valid 0.34323112267913625
LOSS train 0.2207684945630825 valid 0.34310653910040856
LOSS train 0.2207684945630825 valid 0.34310331995107013
LOSS train 0.2207684945630825 valid 0.3431480642876878
LOSS train 0.2207684945630825 valid 0.34329377606560296
LOSS train 0.2207684945630825 valid 0.3432884599131189
LOSS train 0.2207684945630825 valid 0.3431777010687062
LOSS train 0.2207684945630825 valid 0.34316523465650534
LOSS train 0.2207684945630825 valid 0.3430866034586189
LOSS train 0.2207684945630825 valid 0.3430013142332628
LOSS train 0.2207684945630825 valid 0.34291276880645444
LOSS train 0.2207684945630825 valid 0.34293299908599545
LOSS train 0.2207684945630825 valid 0.3428749530740873
LOSS train 0.2207684945630825 valid 0.3428417265128631
LOSS train 0.2207684945630825 valid 0.34277518855306666
LOSS train 0.2207684945630825 valid 0.34291521649641593
LOSS train 0.2207684945630825 valid 0.34283381657941003
LOSS train 0.2207684945630825 valid 0.3426234546247162
LOSS train 0.2207684945630825 valid 0.34274994212934273
LOSS train 0.2207684945630825 valid 0.3428887856550187
LOSS train 0.2207684945630825 valid 0.34308223622532846
LOSS train 0.2207684945630825 valid 0.3430344423744828
LOSS train 0.2207684945630825 valid 0.3433203753374076
LOSS train 0.2207684945630825 valid 0.3431213919698081
LOSS train 0.2207684945630825 valid 0.3429207333562544
LOSS train 0.2207684945630825 valid 0.34295038258035976
LOSS train 0.2207684945630825 valid 0.3430196934021436
LOSS train 0.2207684945630825 valid 0.3431162585052976
LOSS train 0.2207684945630825 valid 0.3431237766046407
LOSS train 0.2207684945630825 valid 0.34319439498571364
LOSS train 0.2207684945630825 valid 0.34328091910001357
LOSS train 0.2207684945630825 valid 0.34329902082681657
LOSS train 0.2207684945630825 valid 0.34317014895357034
LOSS train 0.2207684945630825 valid 0.3430545054017061
LOSS train 0.2207684945630825 valid 0.34298232578121507
LOSS train 0.2207684945630825 valid 0.34313411060386073
LOSS train 0.2207684945630825 valid 0.34306673981360536
LOSS train 0.2207684945630825 valid 0.3428446845195833
LOSS train 0.2207684945630825 valid 0.3428551853145265
LOSS train 0.2207684945630825 valid 0.34274499889837917
LOSS train 0.2207684945630825 valid 0.3427084036896714
LOSS train 0.2207684945630825 valid 0.3427001395207994
LOSS train 0.2207684945630825 valid 0.3425018521260656
LOSS train 0.2207684945630825 valid 0.3424142013080636
LOSS train 0.2207684945630825 valid 0.34227619517822655
LOSS train 0.2207684945630825 valid 0.34247841473755447
LOSS train 0.2207684945630825 valid 0.3425814879545267
LOSS train 0.2207684945630825 valid 0.3424969410706807
LOSS train 0.2207684945630825 valid 0.34230915068377676
LOSS train 0.2207684945630825 valid 0.3422293942635772
LOSS train 0.2207684945630825 valid 0.34226521657496944
LOSS train 0.2207684945630825 valid 0.34216031700372695
LOSS train 0.2207684945630825 valid 0.34208954896172905
LOSS train 0.2207684945630825 valid 0.3420174500735646
LOSS train 0.2207684945630825 valid 0.3419819242471338
LOSS train 0.2207684945630825 valid 0.3420113634507535
LOSS train 0.2207684945630825 valid 0.34220736459107465
LOSS train 0.2207684945630825 valid 0.3423569056341487
LOSS train 0.2207684945630825 valid 0.342323350931416
LOSS train 0.2207684945630825 valid 0.3422737753840798
LOSS train 0.2207684945630825 valid 0.3422964790986441
LOSS train 0.2207684945630825 valid 0.3422610615276628
LOSS train 0.2207684945630825 valid 0.34221551420781093
LOSS train 0.2207684945630825 valid 0.3423465298931243
LOSS train 0.2207684945630825 valid 0.3422148697714503
LOSS train 0.2207684945630825 valid 0.34220171633821267
LOSS train 0.2207684945630825 valid 0.34226919188074867
LOSS train 0.2207684945630825 valid 0.34218597131185846
LOSS train 0.2207684945630825 valid 0.3420397915778433
LOSS train 0.2207684945630825 valid 0.34205167615057336
LOSS train 0.2207684945630825 valid 0.3422503455625317
EPOCH 26:
  batch 1 loss: 0.25091224908828735
  batch 2 loss: 0.23080384731292725
  batch 3 loss: 0.2206683357556661
  batch 4 loss: 0.2206054925918579
  batch 5 loss: 0.2260592520236969
  batch 6 loss: 0.22311006983121237
  batch 7 loss: 0.22474896056311472
  batch 8 loss: 0.22924157604575157
  batch 9 loss: 0.2290705011950599
  batch 10 loss: 0.22767306864261627
  batch 11 loss: 0.22754498232494702
  batch 12 loss: 0.22396019846200943
  batch 13 loss: 0.22401414123865274
  batch 14 loss: 0.2249792484300477
  batch 15 loss: 0.22573701242605845
  batch 16 loss: 0.22595339454710484
  batch 17 loss: 0.22232495861894944
  batch 18 loss: 0.2221123543050554
  batch 19 loss: 0.2189454773538991
  batch 20 loss: 0.2162082202732563
  batch 21 loss: 0.21543384591738382
  batch 22 loss: 0.21426240490241485
  batch 23 loss: 0.2129327188367429
  batch 24 loss: 0.2113199277470509
  batch 25 loss: 0.21283937573432923
  batch 26 loss: 0.21156705973240045
  batch 27 loss: 0.21124141911665598
  batch 28 loss: 0.20961924429450715
  batch 29 loss: 0.2103517589897945
  batch 30 loss: 0.20986472368240355
  batch 31 loss: 0.21021832381525346
  batch 32 loss: 0.20983705669641495
  batch 33 loss: 0.20984794786482147
  batch 34 loss: 0.2085866621311973
  batch 35 loss: 0.2084464920418603
  batch 36 loss: 0.20931657610668075
  batch 37 loss: 0.20871153272487022
  batch 38 loss: 0.20929147107036492
  batch 39 loss: 0.2086257625084657
  batch 40 loss: 0.2093131233006716
  batch 41 loss: 0.20907409852597772
  batch 42 loss: 0.209030905294986
  batch 43 loss: 0.20956263951090878
  batch 44 loss: 0.2089493064717813
  batch 45 loss: 0.20859632392724356
  batch 46 loss: 0.2091196072490319
  batch 47 loss: 0.20857175296925484
  batch 48 loss: 0.2077749433616797
  batch 49 loss: 0.20764554732916307
  batch 50 loss: 0.20761257112026216
  batch 51 loss: 0.2077121635278066
  batch 52 loss: 0.20827963690345103
  batch 53 loss: 0.20839982297060625
  batch 54 loss: 0.20881539776369376
  batch 55 loss: 0.20854558104818519
  batch 56 loss: 0.20820511132478714
  batch 57 loss: 0.20803488634134593
  batch 58 loss: 0.20895675723922663
  batch 59 loss: 0.2094871358851255
  batch 60 loss: 0.20922512089212736
  batch 61 loss: 0.20980258579136896
  batch 62 loss: 0.21006025253765045
  batch 63 loss: 0.20991407050972893
  batch 64 loss: 0.2105037623550743
  batch 65 loss: 0.21048179200062386
  batch 66 loss: 0.21031988005746494
  batch 67 loss: 0.21102997816320676
  batch 68 loss: 0.21140172222957893
  batch 69 loss: 0.21162023492481397
  batch 70 loss: 0.21256632719721114
  batch 71 loss: 0.2123749056752299
  batch 72 loss: 0.2125896658334467
  batch 73 loss: 0.21264270152131173
  batch 74 loss: 0.21243911619121963
  batch 75 loss: 0.21223658283551533
  batch 76 loss: 0.21285959764530785
  batch 77 loss: 0.21228904383523123
  batch 78 loss: 0.21255500882099837
  batch 79 loss: 0.2125030410817907
  batch 80 loss: 0.2121891789138317
  batch 81 loss: 0.21249222221933764
  batch 82 loss: 0.21329278753298084
  batch 83 loss: 0.21329455975308476
  batch 84 loss: 0.21286770062787191
  batch 85 loss: 0.2128669104155372
  batch 86 loss: 0.21361912579037423
  batch 87 loss: 0.21329789874197422
  batch 88 loss: 0.2130061291496862
  batch 89 loss: 0.21327814577000864
  batch 90 loss: 0.21327765501207777
  batch 91 loss: 0.2132816309784795
  batch 92 loss: 0.21310894628581795
  batch 93 loss: 0.21323397018576182
  batch 94 loss: 0.21339535840014193
  batch 95 loss: 0.21315431500736035
  batch 96 loss: 0.21296271417910853
  batch 97 loss: 0.21304015492655567
  batch 98 loss: 0.2133365049958229
  batch 99 loss: 0.2137180053525501
  batch 100 loss: 0.21315218195319174
  batch 101 loss: 0.2131495329708156
  batch 102 loss: 0.21383812044765435
  batch 103 loss: 0.21420823500573055
  batch 104 loss: 0.21437625615642622
  batch 105 loss: 0.21403940660612925
  batch 106 loss: 0.21419861898669657
  batch 107 loss: 0.21405426141257597
  batch 108 loss: 0.21432301346902494
  batch 109 loss: 0.2143664082529348
  batch 110 loss: 0.21440801498564807
  batch 111 loss: 0.21464538413125114
  batch 112 loss: 0.21449423980500018
  batch 113 loss: 0.2144407777659661
  batch 114 loss: 0.21471294577707323
  batch 115 loss: 0.21494715926439867
  batch 116 loss: 0.2150984289574212
  batch 117 loss: 0.21518598980883247
  batch 118 loss: 0.21500464389889928
  batch 119 loss: 0.21513778910416514
  batch 120 loss: 0.21468154191970826
  batch 121 loss: 0.21461040446580934
  batch 122 loss: 0.21449196656219294
  batch 123 loss: 0.2144937510412883
  batch 124 loss: 0.21479604801824015
  batch 125 loss: 0.21465490877628327
  batch 126 loss: 0.21460502015219796
  batch 127 loss: 0.21514627196657377
  batch 128 loss: 0.21496197627857327
  batch 129 loss: 0.2151944431685662
  batch 130 loss: 0.21506356275998628
  batch 131 loss: 0.21501675703143344
  batch 132 loss: 0.215017958459529
  batch 133 loss: 0.2153215575263016
  batch 134 loss: 0.21534345020998769
  batch 135 loss: 0.2151004244883855
  batch 136 loss: 0.21526795055936365
  batch 137 loss: 0.2152334615914491
  batch 138 loss: 0.2151443194867908
  batch 139 loss: 0.21535913303172846
  batch 140 loss: 0.21530374639800617
  batch 141 loss: 0.21558436085569097
  batch 142 loss: 0.21545946629534304
  batch 143 loss: 0.21515080447380358
  batch 144 loss: 0.21517677905244958
  batch 145 loss: 0.21524603726535008
  batch 146 loss: 0.2151793415007526
  batch 147 loss: 0.21536116575708195
  batch 148 loss: 0.21540081782920942
  batch 149 loss: 0.21521033836691172
  batch 150 loss: 0.21511857698361078
  batch 151 loss: 0.21507541341892142
  batch 152 loss: 0.21524582989513874
  batch 153 loss: 0.21498815988013947
  batch 154 loss: 0.2150441270757031
  batch 155 loss: 0.21491047826505477
  batch 156 loss: 0.2149451652016395
  batch 157 loss: 0.21517129783417768
  batch 158 loss: 0.21503387702794013
  batch 159 loss: 0.21509088397775805
  batch 160 loss: 0.2147213513031602
  batch 161 loss: 0.21492474817711374
  batch 162 loss: 0.21468604346852244
  batch 163 loss: 0.2146962971592242
  batch 164 loss: 0.214434544942001
  batch 165 loss: 0.21435443180980104
  batch 166 loss: 0.21441918090883508
  batch 167 loss: 0.21427283877741077
  batch 168 loss: 0.214150937185401
  batch 169 loss: 0.21382838046762365
  batch 170 loss: 0.21371046637787539
  batch 171 loss: 0.2135470775310059
  batch 172 loss: 0.2135257277377816
  batch 173 loss: 0.213340108794284
  batch 174 loss: 0.21331577251354852
  batch 175 loss: 0.21335230248314993
  batch 176 loss: 0.2132390456443483
  batch 177 loss: 0.21326312229121472
  batch 178 loss: 0.2134486029154799
  batch 179 loss: 0.2133717407893868
  batch 180 loss: 0.21315003368589613
  batch 181 loss: 0.21327216972632962
  batch 182 loss: 0.21329510670441848
  batch 183 loss: 0.21314653062103875
  batch 184 loss: 0.21323585226807906
  batch 185 loss: 0.21356803090185733
  batch 186 loss: 0.2137837651115592
  batch 187 loss: 0.21374210898570198
  batch 188 loss: 0.21353688368454893
  batch 189 loss: 0.213274552472054
  batch 190 loss: 0.21328807112417722
  batch 191 loss: 0.21338159688480238
  batch 192 loss: 0.2133597336554279
  batch 193 loss: 0.2132982363818223
  batch 194 loss: 0.2133666554216257
  batch 195 loss: 0.213609223258801
  batch 196 loss: 0.21365367507143895
  batch 197 loss: 0.21370617523411203
  batch 198 loss: 0.2140693533601183
  batch 199 loss: 0.2139532022440254
  batch 200 loss: 0.2142341786623001
  batch 201 loss: 0.2145184693644889
  batch 202 loss: 0.21470459764546687
  batch 203 loss: 0.21462968094595547
  batch 204 loss: 0.2146358235063506
  batch 205 loss: 0.2150364825638329
  batch 206 loss: 0.2152555911141692
  batch 207 loss: 0.21556861281107012
  batch 208 loss: 0.21575959721723428
  batch 209 loss: 0.21568045512055667
  batch 210 loss: 0.21588312096538997
  batch 211 loss: 0.2158264949988415
  batch 212 loss: 0.21582520570395128
  batch 213 loss: 0.215760877364678
  batch 214 loss: 0.21577527568997623
  batch 215 loss: 0.21568328703558723
  batch 216 loss: 0.21542171926962006
  batch 217 loss: 0.21540043554547744
  batch 218 loss: 0.21532699465751648
  batch 219 loss: 0.21535155537738104
  batch 220 loss: 0.21524632383476605
  batch 221 loss: 0.21529512559126945
  batch 222 loss: 0.21524429442109289
  batch 223 loss: 0.21522357511948043
  batch 224 loss: 0.21518213508118475
  batch 225 loss: 0.2150890702671475
  batch 226 loss: 0.215134933788692
  batch 227 loss: 0.2150770765831817
  batch 228 loss: 0.21502802860841416
  batch 229 loss: 0.21499800148489173
  batch 230 loss: 0.21513634239849838
  batch 231 loss: 0.21510417533643317
  batch 232 loss: 0.21498028522935406
  batch 233 loss: 0.21493818000150852
  batch 234 loss: 0.21493514799154723
  batch 235 loss: 0.2148701273380442
  batch 236 loss: 0.21473275093456445
  batch 237 loss: 0.21470838929781935
  batch 238 loss: 0.2147884040194399
  batch 239 loss: 0.2145935014576094
  batch 240 loss: 0.2144643448914091
  batch 241 loss: 0.2144691426600658
  batch 242 loss: 0.21435318505468448
  batch 243 loss: 0.2143227786071016
  batch 244 loss: 0.21420089454680194
  batch 245 loss: 0.214079086148009
  batch 246 loss: 0.21408684678920886
  batch 247 loss: 0.21419743047310755
  batch 248 loss: 0.21423868444417754
  batch 249 loss: 0.2143136670431459
  batch 250 loss: 0.21418330514431
  batch 251 loss: 0.21410172512806744
  batch 252 loss: 0.2140293661209326
  batch 253 loss: 0.21410539970096393
  batch 254 loss: 0.21404807251973415
  batch 255 loss: 0.2140692800283432
  batch 256 loss: 0.21430897613754496
  batch 257 loss: 0.2143318534245287
  batch 258 loss: 0.21424431007268818
  batch 259 loss: 0.21424677979531895
  batch 260 loss: 0.21426023800785726
  batch 261 loss: 0.21424408896444402
  batch 262 loss: 0.21420008586790726
  batch 263 loss: 0.2142835714404574
  batch 264 loss: 0.2143091754705617
  batch 265 loss: 0.21419983680518168
  batch 266 loss: 0.21424516520105807
  batch 267 loss: 0.21424906033924904
  batch 268 loss: 0.2140889659857572
  batch 269 loss: 0.214127308650974
  batch 270 loss: 0.214427115464652
  batch 271 loss: 0.21452688983885565
  batch 272 loss: 0.21454337540575685
  batch 273 loss: 0.21456922721731794
  batch 274 loss: 0.21476788584985873
  batch 275 loss: 0.21496691850098695
  batch 276 loss: 0.21508225728420244
  batch 277 loss: 0.21505484302336558
  batch 278 loss: 0.21505051947754922
  batch 279 loss: 0.21518649988704258
  batch 280 loss: 0.21538727028029306
  batch 281 loss: 0.2154028027507334
  batch 282 loss: 0.2154662065366481
  batch 283 loss: 0.21540617579495527
  batch 284 loss: 0.2155870811515291
  batch 285 loss: 0.21577411571092772
  batch 286 loss: 0.215748059582877
  batch 287 loss: 0.2157842568300327
  batch 288 loss: 0.21578525518998504
  batch 289 loss: 0.2159787562376075
  batch 290 loss: 0.21607050417826093
  batch 291 loss: 0.21619089080910503
  batch 292 loss: 0.21625166452706676
  batch 293 loss: 0.21628010262808295
  batch 294 loss: 0.21640740223482352
  batch 295 loss: 0.21652480929584828
  batch 296 loss: 0.21677598014876648
  batch 297 loss: 0.2168406407640438
  batch 298 loss: 0.21684888745314324
  batch 299 loss: 0.21691996482104361
  batch 300 loss: 0.2171108833452066
  batch 301 loss: 0.21722150863801126
  batch 302 loss: 0.21725116941510447
  batch 303 loss: 0.21720820906335372
  batch 304 loss: 0.217185069365721
  batch 305 loss: 0.21712984281485198
  batch 306 loss: 0.21731415405577303
  batch 307 loss: 0.21743972754827928
  batch 308 loss: 0.21758915382352742
  batch 309 loss: 0.21759837325722653
  batch 310 loss: 0.21757943216831455
  batch 311 loss: 0.21769750645306332
  batch 312 loss: 0.21781814690583792
  batch 313 loss: 0.21787239830143534
  batch 314 loss: 0.21783654258889
  batch 315 loss: 0.2178287472989824
  batch 316 loss: 0.21780423565378673
  batch 317 loss: 0.2177876515155335
  batch 318 loss: 0.21782102815385135
  batch 319 loss: 0.217787298149076
  batch 320 loss: 0.2177857920527458
  batch 321 loss: 0.21784733741825615
  batch 322 loss: 0.21787378071628002
  batch 323 loss: 0.21783609314599642
  batch 324 loss: 0.21776485024594966
  batch 325 loss: 0.21768788855809432
  batch 326 loss: 0.21766488807150192
  batch 327 loss: 0.21760270751397545
  batch 328 loss: 0.2174570598162529
  batch 329 loss: 0.2175429546271414
  batch 330 loss: 0.21745062398188042
  batch 331 loss: 0.21739327155751403
  batch 332 loss: 0.2173715582094997
  batch 333 loss: 0.2176264856491719
  batch 334 loss: 0.21766481646699107
  batch 335 loss: 0.21756604100341229
  batch 336 loss: 0.21749505156739837
  batch 337 loss: 0.21757202845298926
  batch 338 loss: 0.21770793129001142
  batch 339 loss: 0.21787758703428736
  batch 340 loss: 0.21790934989557548
  batch 341 loss: 0.21800257540867826
  batch 342 loss: 0.21797235885210203
  batch 343 loss: 0.2181125616888263
  batch 344 loss: 0.21821808191232903
  batch 345 loss: 0.21836007522500078
  batch 346 loss: 0.21837620712773648
  batch 347 loss: 0.21842089072084564
  batch 348 loss: 0.21848418173947554
  batch 349 loss: 0.21847587734204651
  batch 350 loss: 0.21860854868377957
  batch 351 loss: 0.21862920566841407
  batch 352 loss: 0.21870216888121582
  batch 353 loss: 0.21872194904950118
  batch 354 loss: 0.21895423502066716
  batch 355 loss: 0.21889484235098663
  batch 356 loss: 0.21885691784071118
  batch 357 loss: 0.21890833907100668
  batch 358 loss: 0.21908839321669255
  batch 359 loss: 0.21910732278916828
  batch 360 loss: 0.21918035513824888
  batch 361 loss: 0.2192124047850638
  batch 362 loss: 0.21925409335787124
  batch 363 loss: 0.21917231865329848
  batch 364 loss: 0.21909631956573372
  batch 365 loss: 0.2191497366314065
  batch 366 loss: 0.21915274874760154
  batch 367 loss: 0.2190992763971438
  batch 368 loss: 0.21903051727491876
  batch 369 loss: 0.21897347687382684
  batch 370 loss: 0.21889912631060626
  batch 371 loss: 0.21895083218732614
  batch 372 loss: 0.21900082856256475
  batch 373 loss: 0.21887973634071708
  batch 374 loss: 0.2187431136713946
  batch 375 loss: 0.21861496837933858
  batch 376 loss: 0.21860350299864373
  batch 377 loss: 0.21859295464637107
  batch 378 loss: 0.21859568499383472
  batch 379 loss: 0.21851687868541023
  batch 380 loss: 0.21856017873475425
  batch 381 loss: 0.21853142100682096
  batch 382 loss: 0.21853436845603413
  batch 383 loss: 0.21854690444842953
  batch 384 loss: 0.21853187823823342
  batch 385 loss: 0.2185950047009951
  batch 386 loss: 0.21852459638847588
  batch 387 loss: 0.2185933978058571
  batch 388 loss: 0.21861536290074132
  batch 389 loss: 0.2186209562505119
  batch 390 loss: 0.21858341032877945
  batch 391 loss: 0.21864655610088193
  batch 392 loss: 0.21861982828348267
  batch 393 loss: 0.2185222564156122
  batch 394 loss: 0.21856181622307919
  batch 395 loss: 0.2184985739143589
  batch 396 loss: 0.21834092162022686
  batch 397 loss: 0.2183480649736126
  batch 398 loss: 0.21830137186313994
  batch 399 loss: 0.218401705262655
  batch 400 loss: 0.21847936164587736
  batch 401 loss: 0.218354644546485
  batch 402 loss: 0.21836571617802578
  batch 403 loss: 0.21840066375862577
  batch 404 loss: 0.21836010067917333
  batch 405 loss: 0.21833201624729015
  batch 406 loss: 0.21838501448114517
  batch 407 loss: 0.21836558756869315
  batch 408 loss: 0.2183979746757769
  batch 409 loss: 0.21846232794607764
  batch 410 loss: 0.21857030747867212
  batch 411 loss: 0.21855420718952978
  batch 412 loss: 0.21854543939088156
  batch 413 loss: 0.2185922383396158
  batch 414 loss: 0.21866153141006756
  batch 415 loss: 0.21867737594139144
  batch 416 loss: 0.21875379316938612
  batch 417 loss: 0.2187396118537985
  batch 418 loss: 0.21875818515793558
  batch 419 loss: 0.21879821404067884
  batch 420 loss: 0.21873993515258744
  batch 421 loss: 0.21861745852472664
  batch 422 loss: 0.21875352260625758
  batch 423 loss: 0.21873846304895747
  batch 424 loss: 0.21870374173488258
  batch 425 loss: 0.21862662844798145
  batch 426 loss: 0.21859918303892645
  batch 427 loss: 0.2185665170816962
  batch 428 loss: 0.2185406739884448
  batch 429 loss: 0.21857982481415178
  batch 430 loss: 0.21857991624017095
  batch 431 loss: 0.21861829880080477
  batch 432 loss: 0.21858666371554136
  batch 433 loss: 0.21860611256358253
  batch 434 loss: 0.21870009632017207
  batch 435 loss: 0.21872095681470016
  batch 436 loss: 0.2187464001050236
  batch 437 loss: 0.2187879549624991
  batch 438 loss: 0.21884955816209045
  batch 439 loss: 0.21885486497015508
  batch 440 loss: 0.21890266937288372
  batch 441 loss: 0.21887225297843518
  batch 442 loss: 0.21880950856262743
  batch 443 loss: 0.21882147915476333
  batch 444 loss: 0.2187522454438983
  batch 445 loss: 0.21873462735936883
  batch 446 loss: 0.21864802300128167
  batch 447 loss: 0.2185909300375838
  batch 448 loss: 0.21863791250091577
  batch 449 loss: 0.21861956973782096
  batch 450 loss: 0.21864950898620816
  batch 451 loss: 0.21860863658250568
  batch 452 loss: 0.2186918516140596
  batch 453 loss: 0.21873601711875293
  batch 454 loss: 0.2187076978024407
  batch 455 loss: 0.2187214909346549
  batch 456 loss: 0.21874562791434296
  batch 457 loss: 0.21872857702117518
  batch 458 loss: 0.21868482433812586
  batch 459 loss: 0.2188086727216093
  batch 460 loss: 0.21886432073686435
  batch 461 loss: 0.21881907609434811
  batch 462 loss: 0.2188042782214813
  batch 463 loss: 0.21874712198393392
  batch 464 loss: 0.2187749589953957
  batch 465 loss: 0.21869779514369145
  batch 466 loss: 0.21858839748243405
  batch 467 loss: 0.21860563349800397
  batch 468 loss: 0.21854917838787422
  batch 469 loss: 0.21862182529496232
  batch 470 loss: 0.2185546227592103
  batch 471 loss: 0.21860134139688658
  batch 472 loss: 0.21840487875170628
LOSS train 0.21840487875170628 valid 0.3543452024459839
LOSS train 0.21840487875170628 valid 0.34211266040802
LOSS train 0.21840487875170628 valid 0.33594895402590436
LOSS train 0.21840487875170628 valid 0.3324214965105057
LOSS train 0.21840487875170628 valid 0.3274615406990051
LOSS train 0.21840487875170628 valid 0.3307383408149083
LOSS train 0.21840487875170628 valid 0.341794137443815
LOSS train 0.21840487875170628 valid 0.3405738063156605
LOSS train 0.21840487875170628 valid 0.34005587961938644
LOSS train 0.21840487875170628 valid 0.34382022321224215
LOSS train 0.21840487875170628 valid 0.34157517010515387
LOSS train 0.21840487875170628 valid 0.3417952011028926
LOSS train 0.21840487875170628 valid 0.3412880118076618
LOSS train 0.21840487875170628 valid 0.3411973459380014
LOSS train 0.21840487875170628 valid 0.334948127468427
LOSS train 0.21840487875170628 valid 0.33572776708751917
LOSS train 0.21840487875170628 valid 0.3384067617795047
LOSS train 0.21840487875170628 valid 0.3419548562831349
LOSS train 0.21840487875170628 valid 0.34462176263332367
LOSS train 0.21840487875170628 valid 0.34389454051852225
LOSS train 0.21840487875170628 valid 0.342358152781214
LOSS train 0.21840487875170628 valid 0.3397520123557611
LOSS train 0.21840487875170628 valid 0.34027062615622644
LOSS train 0.21840487875170628 valid 0.3375541176646948
LOSS train 0.21840487875170628 valid 0.3360480636358261
LOSS train 0.21840487875170628 valid 0.3354883211163374
LOSS train 0.21840487875170628 valid 0.33650826911131543
LOSS train 0.21840487875170628 valid 0.3369773950959955
LOSS train 0.21840487875170628 valid 0.3363931379441557
LOSS train 0.21840487875170628 valid 0.3375742857654889
LOSS train 0.21840487875170628 valid 0.3399595421168112
LOSS train 0.21840487875170628 valid 0.3393276450224221
LOSS train 0.21840487875170628 valid 0.3407626662290458
LOSS train 0.21840487875170628 valid 0.3410922941916129
LOSS train 0.21840487875170628 valid 0.34328826112406596
LOSS train 0.21840487875170628 valid 0.3425903531412284
LOSS train 0.21840487875170628 valid 0.34241605248000173
LOSS train 0.21840487875170628 valid 0.3436508527711818
LOSS train 0.21840487875170628 valid 0.34305201929349166
LOSS train 0.21840487875170628 valid 0.343081920966506
LOSS train 0.21840487875170628 valid 0.34508762236048535
LOSS train 0.21840487875170628 valid 0.34584355105956394
LOSS train 0.21840487875170628 valid 0.3455895577059236
LOSS train 0.21840487875170628 valid 0.3463077230209654
LOSS train 0.21840487875170628 valid 0.3453967210319307
LOSS train 0.21840487875170628 valid 0.3461892932005551
LOSS train 0.21840487875170628 valid 0.34764077530262316
LOSS train 0.21840487875170628 valid 0.34760665738334257
LOSS train 0.21840487875170628 valid 0.34819721780261215
LOSS train 0.21840487875170628 valid 0.34678396910429
LOSS train 0.21840487875170628 valid 0.3460683200289221
LOSS train 0.21840487875170628 valid 0.3459499068558216
LOSS train 0.21840487875170628 valid 0.34582578770394595
LOSS train 0.21840487875170628 valid 0.34569124739479135
LOSS train 0.21840487875170628 valid 0.34558455429293894
LOSS train 0.21840487875170628 valid 0.34491906873881817
LOSS train 0.21840487875170628 valid 0.3448914323459592
LOSS train 0.21840487875170628 valid 0.3444034552265858
LOSS train 0.21840487875170628 valid 0.3452373482918335
LOSS train 0.21840487875170628 valid 0.344838701436917
LOSS train 0.21840487875170628 valid 0.3439395591372349
LOSS train 0.21840487875170628 valid 0.34535383288898774
LOSS train 0.21840487875170628 valid 0.3461735870630022
LOSS train 0.21840487875170628 valid 0.3474583828356117
LOSS train 0.21840487875170628 valid 0.34821986854076387
LOSS train 0.21840487875170628 valid 0.34803721150665573
LOSS train 0.21840487875170628 valid 0.34732861638958773
LOSS train 0.21840487875170628 valid 0.34745335732312765
LOSS train 0.21840487875170628 valid 0.3463304459616758
LOSS train 0.21840487875170628 valid 0.3467223258955138
LOSS train 0.21840487875170628 valid 0.3463863027347645
LOSS train 0.21840487875170628 valid 0.34627599372631973
LOSS train 0.21840487875170628 valid 0.34595560149787224
LOSS train 0.21840487875170628 valid 0.3458543196320534
LOSS train 0.21840487875170628 valid 0.3456902227799098
LOSS train 0.21840487875170628 valid 0.3460316550182669
LOSS train 0.21840487875170628 valid 0.34638187618224653
LOSS train 0.21840487875170628 valid 0.3467402035991351
LOSS train 0.21840487875170628 valid 0.3471430096822449
LOSS train 0.21840487875170628 valid 0.3459730686619878
LOSS train 0.21840487875170628 valid 0.3448920774239081
LOSS train 0.21840487875170628 valid 0.3458054383716932
LOSS train 0.21840487875170628 valid 0.3455953905022288
LOSS train 0.21840487875170628 valid 0.3452255241572857
LOSS train 0.21840487875170628 valid 0.3448883216170704
LOSS train 0.21840487875170628 valid 0.3440986932363621
LOSS train 0.21840487875170628 valid 0.3438801982964592
LOSS train 0.21840487875170628 valid 0.34328644231639127
LOSS train 0.21840487875170628 valid 0.3440612107515335
LOSS train 0.21840487875170628 valid 0.34440034015311133
LOSS train 0.21840487875170628 valid 0.344683418070877
LOSS train 0.21840487875170628 valid 0.34486383118707203
LOSS train 0.21840487875170628 valid 0.344634188919939
LOSS train 0.21840487875170628 valid 0.34492038681786114
LOSS train 0.21840487875170628 valid 0.3446677259708706
LOSS train 0.21840487875170628 valid 0.34540942202632624
LOSS train 0.21840487875170628 valid 0.34565555573124246
LOSS train 0.21840487875170628 valid 0.3461235972995661
LOSS train 0.21840487875170628 valid 0.3465423061691149
LOSS train 0.21840487875170628 valid 0.3467373718321323
LOSS train 0.21840487875170628 valid 0.34687956592234054
LOSS train 0.21840487875170628 valid 0.34685035444357815
LOSS train 0.21840487875170628 valid 0.3468846078347234
LOSS train 0.21840487875170628 valid 0.34710789514848817
LOSS train 0.21840487875170628 valid 0.3471193647100812
LOSS train 0.21840487875170628 valid 0.34752842095100656
LOSS train 0.21840487875170628 valid 0.34713920129236775
LOSS train 0.21840487875170628 valid 0.3474073132707013
LOSS train 0.21840487875170628 valid 0.34776748470756985
LOSS train 0.21840487875170628 valid 0.3478579752824523
LOSS train 0.21840487875170628 valid 0.34711969798212655
LOSS train 0.21840487875170628 valid 0.34691629465669394
LOSS train 0.21840487875170628 valid 0.3468349084126211
LOSS train 0.21840487875170628 valid 0.34638458372730957
LOSS train 0.21840487875170628 valid 0.3467678568933321
LOSS train 0.21840487875170628 valid 0.3464661193048132
LOSS train 0.21840487875170628 valid 0.3466617063834117
LOSS train 0.21840487875170628 valid 0.34645113109026926
LOSS train 0.21840487875170628 valid 0.34618627686961356
LOSS train 0.21840487875170628 valid 0.3458609803269307
LOSS train 0.21840487875170628 valid 0.34582738332019364
LOSS train 0.21840487875170628 valid 0.34546122675547836
LOSS train 0.21840487875170628 valid 0.34547688745386235
LOSS train 0.21840487875170628 valid 0.34589632203982723
LOSS train 0.21840487875170628 valid 0.3458034051656723
LOSS train 0.21840487875170628 valid 0.34590671731839107
LOSS train 0.21840487875170628 valid 0.34586465393933724
LOSS train 0.21840487875170628 valid 0.3464681307086721
LOSS train 0.21840487875170628 valid 0.3466097427199977
LOSS train 0.21840487875170628 valid 0.34625960691617086
LOSS train 0.21840487875170628 valid 0.3462140415688507
LOSS train 0.21840487875170628 valid 0.3458396719034874
LOSS train 0.21840487875170628 valid 0.34584369325548187
LOSS train 0.21840487875170628 valid 0.3458245021860991
LOSS train 0.21840487875170628 valid 0.3455413892313286
LOSS train 0.21840487875170628 valid 0.3454905639895621
LOSS train 0.21840487875170628 valid 0.34536150674315264
LOSS train 0.21840487875170628 valid 0.3454068690754365
LOSS train 0.21840487875170628 valid 0.3452747705385839
LOSS train 0.21840487875170628 valid 0.34533338897994587
LOSS train 0.21840487875170628 valid 0.34529231995978255
LOSS train 0.21840487875170628 valid 0.34558777017912395
LOSS train 0.21840487875170628 valid 0.3455497779837855
LOSS train 0.21840487875170628 valid 0.34537639644824797
LOSS train 0.21840487875170628 valid 0.34500838014586216
LOSS train 0.21840487875170628 valid 0.3451691515845795
LOSS train 0.21840487875170628 valid 0.34501794132651115
LOSS train 0.21840487875170628 valid 0.3460960159632
LOSS train 0.21840487875170628 valid 0.3461109280786258
LOSS train 0.21840487875170628 valid 0.34633034298817317
LOSS train 0.21840487875170628 valid 0.3463001834438337
LOSS train 0.21840487875170628 valid 0.3459554374414055
LOSS train 0.21840487875170628 valid 0.3461010196045333
LOSS train 0.21840487875170628 valid 0.3460197463244587
LOSS train 0.21840487875170628 valid 0.34626263197391266
LOSS train 0.21840487875170628 valid 0.34641996876169473
LOSS train 0.21840487875170628 valid 0.3463002884653723
LOSS train 0.21840487875170628 valid 0.3464643831683111
LOSS train 0.21840487875170628 valid 0.34646506719994097
LOSS train 0.21840487875170628 valid 0.3464287362061441
LOSS train 0.21840487875170628 valid 0.3464077245559752
LOSS train 0.21840487875170628 valid 0.3461726290392287
LOSS train 0.21840487875170628 valid 0.3459505774309299
LOSS train 0.21840487875170628 valid 0.34548915877211384
LOSS train 0.21840487875170628 valid 0.3450982096520337
LOSS train 0.21840487875170628 valid 0.34523781801921777
LOSS train 0.21840487875170628 valid 0.34570433413554097
LOSS train 0.21840487875170628 valid 0.345432245926488
LOSS train 0.21840487875170628 valid 0.3456254087432602
LOSS train 0.21840487875170628 valid 0.34568960675421884
LOSS train 0.21840487875170628 valid 0.3455913574897755
LOSS train 0.21840487875170628 valid 0.34528701657126115
LOSS train 0.21840487875170628 valid 0.34517801815719273
LOSS train 0.21840487875170628 valid 0.34516362323500643
LOSS train 0.21840487875170628 valid 0.3448966589144298
LOSS train 0.21840487875170628 valid 0.34482081090523437
LOSS train 0.21840487875170628 valid 0.344703789737265
LOSS train 0.21840487875170628 valid 0.3448998558889614
LOSS train 0.21840487875170628 valid 0.3446813068409872
LOSS train 0.21840487875170628 valid 0.34458402287628914
LOSS train 0.21840487875170628 valid 0.34483952260478423
LOSS train 0.21840487875170628 valid 0.345183854329062
LOSS train 0.21840487875170628 valid 0.34520403943100914
LOSS train 0.21840487875170628 valid 0.34511999842589314
LOSS train 0.21840487875170628 valid 0.3446848175815634
LOSS train 0.21840487875170628 valid 0.3445845074070397
LOSS train 0.21840487875170628 valid 0.34445782707018013
LOSS train 0.21840487875170628 valid 0.3444883657738249
LOSS train 0.21840487875170628 valid 0.34430432926725457
LOSS train 0.21840487875170628 valid 0.34445961815746207
LOSS train 0.21840487875170628 valid 0.3442814669215866
LOSS train 0.21840487875170628 valid 0.34434168265822035
LOSS train 0.21840487875170628 valid 0.34417061801092613
LOSS train 0.21840487875170628 valid 0.34400704572188484
LOSS train 0.21840487875170628 valid 0.34367486460086627
LOSS train 0.21840487875170628 valid 0.3435005421693228
LOSS train 0.21840487875170628 valid 0.34368521844977656
LOSS train 0.21840487875170628 valid 0.34331030985622696
LOSS train 0.21840487875170628 valid 0.34340644129856146
LOSS train 0.21840487875170628 valid 0.3433501509577036
LOSS train 0.21840487875170628 valid 0.3431651048844133
LOSS train 0.21840487875170628 valid 0.34314871431872396
LOSS train 0.21840487875170628 valid 0.34292867287920026
LOSS train 0.21840487875170628 valid 0.34296953524736795
LOSS train 0.21840487875170628 valid 0.34252639899893506
LOSS train 0.21840487875170628 valid 0.3426340719305196
LOSS train 0.21840487875170628 valid 0.34255952254873545
LOSS train 0.21840487875170628 valid 0.34228869506086296
LOSS train 0.21840487875170628 valid 0.3421871896708411
LOSS train 0.21840487875170628 valid 0.34231787331047514
LOSS train 0.21840487875170628 valid 0.34256258147870194
LOSS train 0.21840487875170628 valid 0.3425495411568093
LOSS train 0.21840487875170628 valid 0.34273649346380725
LOSS train 0.21840487875170628 valid 0.34280989402643985
LOSS train 0.21840487875170628 valid 0.34245422711206036
LOSS train 0.21840487875170628 valid 0.3424445683895438
LOSS train 0.21840487875170628 valid 0.3424439723453214
LOSS train 0.21840487875170628 valid 0.3424613226307641
LOSS train 0.21840487875170628 valid 0.3426074800017762
LOSS train 0.21840487875170628 valid 0.3426335896280679
LOSS train 0.21840487875170628 valid 0.3426716043803487
LOSS train 0.21840487875170628 valid 0.3425033736202094
LOSS train 0.21840487875170628 valid 0.34276130935803656
LOSS train 0.21840487875170628 valid 0.34288178470784
LOSS train 0.21840487875170628 valid 0.34270783338281846
LOSS train 0.21840487875170628 valid 0.34281360621737167
LOSS train 0.21840487875170628 valid 0.3432291947666244
LOSS train 0.21840487875170628 valid 0.3432934265256974
LOSS train 0.21840487875170628 valid 0.34343763489650325
LOSS train 0.21840487875170628 valid 0.34355785386717835
LOSS train 0.21840487875170628 valid 0.34357647339761
LOSS train 0.21840487875170628 valid 0.3436080340039113
LOSS train 0.21840487875170628 valid 0.34350505746229526
LOSS train 0.21840487875170628 valid 0.34349320988115084
LOSS train 0.21840487875170628 valid 0.3438477005730284
LOSS train 0.21840487875170628 valid 0.34361436363246484
LOSS train 0.21840487875170628 valid 0.34382076687199153
LOSS train 0.21840487875170628 valid 0.3438016250729561
LOSS train 0.21840487875170628 valid 0.3436846394543867
LOSS train 0.21840487875170628 valid 0.34366963847229876
LOSS train 0.21840487875170628 valid 0.34379479431265125
LOSS train 0.21840487875170628 valid 0.3435549827769768
LOSS train 0.21840487875170628 valid 0.34380387064115503
LOSS train 0.21840487875170628 valid 0.3443261561457251
LOSS train 0.21840487875170628 valid 0.34468298669980496
LOSS train 0.21840487875170628 valid 0.3447166954477628
LOSS train 0.21840487875170628 valid 0.34471210927857077
LOSS train 0.21840487875170628 valid 0.3445808664565125
LOSS train 0.21840487875170628 valid 0.3444681674481396
LOSS train 0.21840487875170628 valid 0.3447681878209114
LOSS train 0.21840487875170628 valid 0.34494939203043856
LOSS train 0.21840487875170628 valid 0.3451688235832585
LOSS train 0.21840487875170628 valid 0.34517583999002405
LOSS train 0.21840487875170628 valid 0.3450185824918935
LOSS train 0.21840487875170628 valid 0.34511352432709114
LOSS train 0.21840487875170628 valid 0.34524484403664246
LOSS train 0.21840487875170628 valid 0.3450450694862507
LOSS train 0.21840487875170628 valid 0.34518158915199976
LOSS train 0.21840487875170628 valid 0.34516229713515423
LOSS train 0.21840487875170628 valid 0.3450229460803362
LOSS train 0.21840487875170628 valid 0.3451762941034361
LOSS train 0.21840487875170628 valid 0.34519649638246946
LOSS train 0.21840487875170628 valid 0.34525655561300284
LOSS train 0.21840487875170628 valid 0.34524539649260766
LOSS train 0.21840487875170628 valid 0.34513742097143857
LOSS train 0.21840487875170628 valid 0.3453764654976085
LOSS train 0.21840487875170628 valid 0.34553900154565603
LOSS train 0.21840487875170628 valid 0.3457269460074048
LOSS train 0.21840487875170628 valid 0.3456985840234615
LOSS train 0.21840487875170628 valid 0.3456022193586385
LOSS train 0.21840487875170628 valid 0.34575089619828325
LOSS train 0.21840487875170628 valid 0.3461112828706117
LOSS train 0.21840487875170628 valid 0.3461361678637864
LOSS train 0.21840487875170628 valid 0.34607770837789037
LOSS train 0.21840487875170628 valid 0.3459849650751461
LOSS train 0.21840487875170628 valid 0.3457835114196591
LOSS train 0.21840487875170628 valid 0.34561142513683124
LOSS train 0.21840487875170628 valid 0.34537918645915366
LOSS train 0.21840487875170628 valid 0.34545495481260363
LOSS train 0.21840487875170628 valid 0.3453856993466616
LOSS train 0.21840487875170628 valid 0.34521219680529897
LOSS train 0.21840487875170628 valid 0.3450138328451637
LOSS train 0.21840487875170628 valid 0.3449591993759041
LOSS train 0.21840487875170628 valid 0.3450291684081017
LOSS train 0.21840487875170628 valid 0.3451743458969551
LOSS train 0.21840487875170628 valid 0.3450639695450143
LOSS train 0.21840487875170628 valid 0.34497816819139476
LOSS train 0.21840487875170628 valid 0.34500890706355375
LOSS train 0.21840487875170628 valid 0.3451740962926904
LOSS train 0.21840487875170628 valid 0.34519839374155836
LOSS train 0.21840487875170628 valid 0.3450887101315141
LOSS train 0.21840487875170628 valid 0.3452428350199575
LOSS train 0.21840487875170628 valid 0.3452325642820918
LOSS train 0.21840487875170628 valid 0.3453225746345358
LOSS train 0.21840487875170628 valid 0.34534807978040083
LOSS train 0.21840487875170628 valid 0.34523860793057326
LOSS train 0.21840487875170628 valid 0.34527253868804636
LOSS train 0.21840487875170628 valid 0.34526476279001106
LOSS train 0.21840487875170628 valid 0.3452404963232602
LOSS train 0.21840487875170628 valid 0.3451189039647579
LOSS train 0.21840487875170628 valid 0.3451064536440808
LOSS train 0.21840487875170628 valid 0.34517061429110585
LOSS train 0.21840487875170628 valid 0.34531653099524307
LOSS train 0.21840487875170628 valid 0.34528763067761536
LOSS train 0.21840487875170628 valid 0.34517099060973183
LOSS train 0.21840487875170628 valid 0.34516415092485403
LOSS train 0.21840487875170628 valid 0.3450840733435721
LOSS train 0.21840487875170628 valid 0.3449686849368857
LOSS train 0.21840487875170628 valid 0.3448734336011232
LOSS train 0.21840487875170628 valid 0.34492396178745455
LOSS train 0.21840487875170628 valid 0.3448658735518287
LOSS train 0.21840487875170628 valid 0.34488161151798874
LOSS train 0.21840487875170628 valid 0.3447965424948226
LOSS train 0.21840487875170628 valid 0.34494935882509137
LOSS train 0.21840487875170628 valid 0.3449058935282722
LOSS train 0.21840487875170628 valid 0.3447629780614678
LOSS train 0.21840487875170628 valid 0.3448766135736971
LOSS train 0.21840487875170628 valid 0.3449876889961321
LOSS train 0.21840487875170628 valid 0.3451264746790769
LOSS train 0.21840487875170628 valid 0.34505917872302233
LOSS train 0.21840487875170628 valid 0.34536431971183074
LOSS train 0.21840487875170628 valid 0.3451891889946061
LOSS train 0.21840487875170628 valid 0.34500424914500294
LOSS train 0.21840487875170628 valid 0.34502818228469956
LOSS train 0.21840487875170628 valid 0.3451204612163397
LOSS train 0.21840487875170628 valid 0.34520193381360703
LOSS train 0.21840487875170628 valid 0.3452063957941277
LOSS train 0.21840487875170628 valid 0.34526913217836763
LOSS train 0.21840487875170628 valid 0.34541474237449266
LOSS train 0.21840487875170628 valid 0.34550180222952004
LOSS train 0.21840487875170628 valid 0.3453668847995222
LOSS train 0.21840487875170628 valid 0.34526438988655445
LOSS train 0.21840487875170628 valid 0.34518504764761654
LOSS train 0.21840487875170628 valid 0.3453379795847539
LOSS train 0.21840487875170628 valid 0.34523807599473355
LOSS train 0.21840487875170628 valid 0.3450354212185457
LOSS train 0.21840487875170628 valid 0.3450337435707494
LOSS train 0.21840487875170628 valid 0.3449344667192747
LOSS train 0.21840487875170628 valid 0.3448632379285002
LOSS train 0.21840487875170628 valid 0.344856527053258
LOSS train 0.21840487875170628 valid 0.34466783614976665
LOSS train 0.21840487875170628 valid 0.3445896786468768
LOSS train 0.21840487875170628 valid 0.34447574072433285
LOSS train 0.21840487875170628 valid 0.3447005789900242
LOSS train 0.21840487875170628 valid 0.3447929354249567
LOSS train 0.21840487875170628 valid 0.34472696788910495
LOSS train 0.21840487875170628 valid 0.3445388570384952
LOSS train 0.21840487875170628 valid 0.3444247516835558
LOSS train 0.21840487875170628 valid 0.34446598393015326
LOSS train 0.21840487875170628 valid 0.34436336589711053
LOSS train 0.21840487875170628 valid 0.3443089990619241
LOSS train 0.21840487875170628 valid 0.3442418343726207
LOSS train 0.21840487875170628 valid 0.3442070401001922
LOSS train 0.21840487875170628 valid 0.3442342961154415
LOSS train 0.21840487875170628 valid 0.344424399985394
LOSS train 0.21840487875170628 valid 0.3445841047619836
LOSS train 0.21840487875170628 valid 0.344568553299797
LOSS train 0.21840487875170628 valid 0.34452792040629093
LOSS train 0.21840487875170628 valid 0.34453946132015717
LOSS train 0.21840487875170628 valid 0.34447982356780105
LOSS train 0.21840487875170628 valid 0.3444474946676529
LOSS train 0.21840487875170628 valid 0.34462972565744465
LOSS train 0.21840487875170628 valid 0.34450137217018556
LOSS train 0.21840487875170628 valid 0.3445608018600679
LOSS train 0.21840487875170628 valid 0.3446483853336883
LOSS train 0.21840487875170628 valid 0.3445220771627348
LOSS train 0.21840487875170628 valid 0.3443852373543487
LOSS train 0.21840487875170628 valid 0.34439973415725905
LOSS train 0.21840487875170628 valid 0.3446066266072152
EPOCH 27:
  batch 1 loss: 0.240949809551239
  batch 2 loss: 0.23076453804969788
  batch 3 loss: 0.22206108272075653
  batch 4 loss: 0.2215384766459465
  batch 5 loss: 0.22488389611244203
  batch 6 loss: 0.22208244850238165
  batch 7 loss: 0.22548152293477738
  batch 8 loss: 0.23087996616959572
  batch 9 loss: 0.22883536252710554
  batch 10 loss: 0.22916317731142044
  batch 11 loss: 0.22783604670654645
  batch 12 loss: 0.2241303746898969
  batch 13 loss: 0.22341022927027482
  batch 14 loss: 0.22421636006661824
  batch 15 loss: 0.22449398537476858
  batch 16 loss: 0.22405292838811874
  batch 17 loss: 0.22089126530815573
  batch 18 loss: 0.22009053909116322
  batch 19 loss: 0.21674465427273198
  batch 20 loss: 0.21430553421378135
  batch 21 loss: 0.21327631956055051
  batch 22 loss: 0.21209926225922324
  batch 23 loss: 0.21071043869723444
  batch 24 loss: 0.20858569629490376
  batch 25 loss: 0.2100777357816696
  batch 26 loss: 0.20930791130432716
  batch 27 loss: 0.2099655485815472
  batch 28 loss: 0.20877942921859877
  batch 29 loss: 0.20916124664503952
  batch 30 loss: 0.20949818044900895
  batch 31 loss: 0.21025587666419246
  batch 32 loss: 0.21015195827931166
  batch 33 loss: 0.20988468238801666
  batch 34 loss: 0.20893178704906912
  batch 35 loss: 0.20907875001430512
  batch 36 loss: 0.21026573785477215
  batch 37 loss: 0.2100648138974164
  batch 38 loss: 0.21015204175522453
  batch 39 loss: 0.2097145460354976
  batch 40 loss: 0.21048734411597253
  batch 41 loss: 0.2102178734977071
  batch 42 loss: 0.2096783309465363
  batch 43 loss: 0.21020814880382183
  batch 44 loss: 0.20945910533720796
  batch 45 loss: 0.2090371049112744
  batch 46 loss: 0.20952242612838745
  batch 47 loss: 0.20913060358230104
  batch 48 loss: 0.20832667561868826
  batch 49 loss: 0.20802735674137973
  batch 50 loss: 0.20787566781044006
  batch 51 loss: 0.2081516870096618
  batch 52 loss: 0.20831031180345094
  batch 53 loss: 0.20804788762668394
  batch 54 loss: 0.20832078186450181
  batch 55 loss: 0.2077967112714594
  batch 56 loss: 0.2075041819896017
  batch 57 loss: 0.20744446364411137
  batch 58 loss: 0.20807324015888676
  batch 59 loss: 0.2082034735861471
  batch 60 loss: 0.2081028530995051
  batch 61 loss: 0.20854179951988283
  batch 62 loss: 0.20880230972843786
  batch 63 loss: 0.20859233893099285
  batch 64 loss: 0.20881485613062978
  batch 65 loss: 0.20889956928216494
  batch 66 loss: 0.2087972771489259
  batch 67 loss: 0.20960968888517637
  batch 68 loss: 0.20985394453301148
  batch 69 loss: 0.20996537156727002
  batch 70 loss: 0.21052897508655274
  batch 71 loss: 0.21040114823361517
  batch 72 loss: 0.21058256117006144
  batch 73 loss: 0.21079016772851553
  batch 74 loss: 0.21042402994793816
  batch 75 loss: 0.21010375420252483
  batch 76 loss: 0.21084134986526087
  batch 77 loss: 0.21046396780323673
  batch 78 loss: 0.21069602591869158
  batch 79 loss: 0.21078334711020505
  batch 80 loss: 0.21023166384547948
  batch 81 loss: 0.21045718480039527
  batch 82 loss: 0.21108383630833974
  batch 83 loss: 0.21085471597062536
  batch 84 loss: 0.21029919066599437
  batch 85 loss: 0.2104787305873983
  batch 86 loss: 0.2114240632847298
  batch 87 loss: 0.21102580582273417
  batch 88 loss: 0.2108087119731036
  batch 89 loss: 0.21108665141496766
  batch 90 loss: 0.21123569856087368
  batch 91 loss: 0.21119297696993902
  batch 92 loss: 0.21110456436872482
  batch 93 loss: 0.21117395835538064
  batch 94 loss: 0.21145150921446212
  batch 95 loss: 0.21110750656378896
  batch 96 loss: 0.2110603122661511
  batch 97 loss: 0.21098584827688552
  batch 98 loss: 0.21128396157707488
  batch 99 loss: 0.21148880428135997
  batch 100 loss: 0.21105039179325102
  batch 101 loss: 0.2109420551817016
  batch 102 loss: 0.21141387829009226
  batch 103 loss: 0.21169932186603546
  batch 104 loss: 0.21185296816894642
  batch 105 loss: 0.2115722324166979
  batch 106 loss: 0.21172914935170478
  batch 107 loss: 0.21146907260484785
  batch 108 loss: 0.21162647588385475
  batch 109 loss: 0.2116595536743829
  batch 110 loss: 0.21193749918179078
  batch 111 loss: 0.21229329557569177
  batch 112 loss: 0.2121754651889205
  batch 113 loss: 0.21237943222564934
  batch 114 loss: 0.21276871716244178
  batch 115 loss: 0.21300934203293012
  batch 116 loss: 0.2132379150596158
  batch 117 loss: 0.21338455671938056
  batch 118 loss: 0.21320418937731597
  batch 119 loss: 0.21323299433002954
  batch 120 loss: 0.21293317551414173
  batch 121 loss: 0.21286850305628185
  batch 122 loss: 0.21269190103792754
  batch 123 loss: 0.21256112095301713
  batch 124 loss: 0.21292400756670582
  batch 125 loss: 0.21270194351673127
  batch 126 loss: 0.21265210826245565
  batch 127 loss: 0.21290885310942734
  batch 128 loss: 0.21262497350107878
  batch 129 loss: 0.21285018243992976
  batch 130 loss: 0.21274536378108538
  batch 131 loss: 0.21275910416632207
  batch 132 loss: 0.21271385997533798
  batch 133 loss: 0.2130607111113412
  batch 134 loss: 0.2131500170746846
  batch 135 loss: 0.21288198232650757
  batch 136 loss: 0.21303102198769064
  batch 137 loss: 0.21305021306459052
  batch 138 loss: 0.21291090958360312
  batch 139 loss: 0.21316470195063583
  batch 140 loss: 0.2132338676069464
  batch 141 loss: 0.2136273647242404
  batch 142 loss: 0.21355040255986468
  batch 143 loss: 0.21328555724837564
  batch 144 loss: 0.2133636206595434
  batch 145 loss: 0.2135046351572563
  batch 146 loss: 0.2135679126806455
  batch 147 loss: 0.2138142384031192
  batch 148 loss: 0.21380979678518064
  batch 149 loss: 0.2136322260903032
  batch 150 loss: 0.21365273396174114
  batch 151 loss: 0.21371565838128526
  batch 152 loss: 0.21404589526355267
  batch 153 loss: 0.21388249469348808
  batch 154 loss: 0.2139942158352245
  batch 155 loss: 0.21396100646065128
  batch 156 loss: 0.2139113842485807
  batch 157 loss: 0.2140897999333728
  batch 158 loss: 0.21401434919879406
  batch 159 loss: 0.2140679939550424
  batch 160 loss: 0.21375612802803517
  batch 161 loss: 0.2139122362092415
  batch 162 loss: 0.2137384150683144
  batch 163 loss: 0.21376020877273536
  batch 164 loss: 0.21354544090061653
  batch 165 loss: 0.21350526854847418
  batch 166 loss: 0.21347645957426853
  batch 167 loss: 0.21339403851303512
  batch 168 loss: 0.21324779306139266
  batch 169 loss: 0.2130400411476045
  batch 170 loss: 0.21292820274829866
  batch 171 loss: 0.21284894444789107
  batch 172 loss: 0.21292205579405607
  batch 173 loss: 0.21283575771860994
  batch 174 loss: 0.21281679334311648
  batch 175 loss: 0.21292677147047862
  batch 176 loss: 0.21281156316399574
  batch 177 loss: 0.21286368016469276
  batch 178 loss: 0.21308574314867512
  batch 179 loss: 0.2130062079795912
  batch 180 loss: 0.21276871214310328
  batch 181 loss: 0.2127671701967387
  batch 182 loss: 0.21266576917944374
  batch 183 loss: 0.2125041685143455
  batch 184 loss: 0.2125213924471451
  batch 185 loss: 0.21269814235133094
  batch 186 loss: 0.2128526086929024
  batch 187 loss: 0.21276625560566703
  batch 188 loss: 0.21249518075839002
  batch 189 loss: 0.21221459471674822
  batch 190 loss: 0.21216427001513932
  batch 191 loss: 0.2121748233808897
  batch 192 loss: 0.21211061680999896
  batch 193 loss: 0.21197418250877004
  batch 194 loss: 0.211976315403722
  batch 195 loss: 0.21201943694016873
  batch 196 loss: 0.211968189219431
  batch 197 loss: 0.2118787230725216
  batch 198 loss: 0.21214127141718914
  batch 199 loss: 0.21197875458091947
  batch 200 loss: 0.21222726337611675
  batch 201 loss: 0.21250213801267728
  batch 202 loss: 0.21265459503277695
  batch 203 loss: 0.21260822832290763
  batch 204 loss: 0.21254839032304054
  batch 205 loss: 0.21284140842716867
  batch 206 loss: 0.21305051590632468
  batch 207 loss: 0.21332354793226085
  batch 208 loss: 0.21344169250761086
  batch 209 loss: 0.21348583063725649
  batch 210 loss: 0.2136981134613355
  batch 211 loss: 0.21364985420523097
  batch 212 loss: 0.21369110357086613
  batch 213 loss: 0.21362016290566171
  batch 214 loss: 0.21362702715619702
  batch 215 loss: 0.21352433950402017
  batch 216 loss: 0.2132166079762909
  batch 217 loss: 0.2132005891354952
  batch 218 loss: 0.21313620502248817
  batch 219 loss: 0.21319426153892795
  batch 220 loss: 0.21304351673884825
  batch 221 loss: 0.21305384227323315
  batch 222 loss: 0.21298509903319246
  batch 223 loss: 0.21297449281130137
  batch 224 loss: 0.21287150301837496
  batch 225 loss: 0.21286120858457352
  batch 226 loss: 0.2129538830007072
  batch 227 loss: 0.21292049060308985
  batch 228 loss: 0.21294771239422916
  batch 229 loss: 0.21294905329895852
  batch 230 loss: 0.21306173775507056
  batch 231 loss: 0.21311355166104964
  batch 232 loss: 0.2130875762166648
  batch 233 loss: 0.21308914466477258
  batch 234 loss: 0.21311274903197575
  batch 235 loss: 0.21304983642507105
  batch 236 loss: 0.21290052587450561
  batch 237 loss: 0.21286589561132438
  batch 238 loss: 0.21288190112144006
  batch 239 loss: 0.21269555308828794
  batch 240 loss: 0.21257265073557693
  batch 241 loss: 0.21259896673602188
  batch 242 loss: 0.21239974134224504
  batch 243 loss: 0.21237538337952805
  batch 244 loss: 0.21228392245095284
  batch 245 loss: 0.21217308737793747
  batch 246 loss: 0.21212017445302592
  batch 247 loss: 0.21226113015099576
  batch 248 loss: 0.21226395534411555
  batch 249 loss: 0.21221887059958586
  batch 250 loss: 0.21210969758033751
  batch 251 loss: 0.21203927856042565
  batch 252 loss: 0.21199999873836836
  batch 253 loss: 0.21197806522544665
  batch 254 loss: 0.211909527619054
  batch 255 loss: 0.2119538795714285
  batch 256 loss: 0.21218983212020248
  batch 257 loss: 0.21222651729555908
  batch 258 loss: 0.2121312078232913
  batch 259 loss: 0.21222324905248222
  batch 260 loss: 0.21222361274636709
  batch 261 loss: 0.2121630329281891
  batch 262 loss: 0.21206697958116313
  batch 263 loss: 0.2121393622780934
  batch 264 loss: 0.21208408881317486
  batch 265 loss: 0.21203538010705192
  batch 266 loss: 0.2120971262903142
  batch 267 loss: 0.21213119507728892
  batch 268 loss: 0.21208510431113528
  batch 269 loss: 0.21208042515697975
  batch 270 loss: 0.2122677426647257
  batch 271 loss: 0.21236027958648232
  batch 272 loss: 0.21236360599012935
  batch 273 loss: 0.2123265817161008
  batch 274 loss: 0.21243323543428505
  batch 275 loss: 0.21256125038320367
  batch 276 loss: 0.2126283294696739
  batch 277 loss: 0.21259408645896705
  batch 278 loss: 0.21260484511689318
  batch 279 loss: 0.2126475079619329
  batch 280 loss: 0.2127310256340674
  batch 281 loss: 0.21266704792441846
  batch 282 loss: 0.21270529503095234
  batch 283 loss: 0.21263777446199222
  batch 284 loss: 0.21279878657258733
  batch 285 loss: 0.21290679621069056
  batch 286 loss: 0.21287641012585246
  batch 287 loss: 0.21290192615487435
  batch 288 loss: 0.21280296493528617
  batch 289 loss: 0.21291069557510034
  batch 290 loss: 0.2128689472017617
  batch 291 loss: 0.21297042332973676
  batch 292 loss: 0.21300651520898897
  batch 293 loss: 0.21300671542056998
  batch 294 loss: 0.21309065555228668
  batch 295 loss: 0.21314918499881938
  batch 296 loss: 0.21333515503116557
  batch 297 loss: 0.21333902647800318
  batch 298 loss: 0.21329767937028168
  batch 299 loss: 0.21330048981119556
  batch 300 loss: 0.21337353085478147
  batch 301 loss: 0.2134139034340152
  batch 302 loss: 0.21345346973629187
  batch 303 loss: 0.21334979779059343
  batch 304 loss: 0.21331610236512988
  batch 305 loss: 0.21326900759681328
  batch 306 loss: 0.21345010058942185
  batch 307 loss: 0.21349181783315802
  batch 308 loss: 0.21355182058238364
  batch 309 loss: 0.2135495909016495
  batch 310 loss: 0.21349687518612032
  batch 311 loss: 0.21378667101599397
  batch 312 loss: 0.21387453530079278
  batch 313 loss: 0.21391377157677477
  batch 314 loss: 0.21388518444861576
  batch 315 loss: 0.21391301377425118
  batch 316 loss: 0.2138942404166807
  batch 317 loss: 0.2138695404732641
  batch 318 loss: 0.21390187037440966
  batch 319 loss: 0.21385744013195876
  batch 320 loss: 0.21390830534510313
  batch 321 loss: 0.21397620170287254
  batch 322 loss: 0.21400000085556728
  batch 323 loss: 0.21397903390897685
  batch 324 loss: 0.21388428672044366
  batch 325 loss: 0.2137866498873784
  batch 326 loss: 0.21371265948732937
  batch 327 loss: 0.21367442489399457
  batch 328 loss: 0.21351003351553186
  batch 329 loss: 0.21351353534268028
  batch 330 loss: 0.213373429712021
  batch 331 loss: 0.21331088884179325
  batch 332 loss: 0.213218130261065
  batch 333 loss: 0.213468745216593
  batch 334 loss: 0.2134212597223099
  batch 335 loss: 0.2132848689360405
  batch 336 loss: 0.2131998652060117
  batch 337 loss: 0.21324603745951498
  batch 338 loss: 0.21322453498134952
  batch 339 loss: 0.21330261546953588
  batch 340 loss: 0.21336444064098245
  batch 341 loss: 0.21345222267237576
  batch 342 loss: 0.21340767185241855
  batch 343 loss: 0.2134667506370878
  batch 344 loss: 0.2135443281988765
  batch 345 loss: 0.21366981803507046
  batch 346 loss: 0.2136465654293926
  batch 347 loss: 0.21362803210786166
  batch 348 loss: 0.21364665194146934
  batch 349 loss: 0.2136540739959837
  batch 350 loss: 0.21380282035895756
  batch 351 loss: 0.2138168391508934
  batch 352 loss: 0.21386651516976682
  batch 353 loss: 0.2138848381998181
  batch 354 loss: 0.21410915748042575
  batch 355 loss: 0.21408170528814827
  batch 356 loss: 0.21406239187449552
  batch 357 loss: 0.2140961938879403
  batch 358 loss: 0.21416636398218197
  batch 359 loss: 0.21412141807896182
  batch 360 loss: 0.2141502963172065
  batch 361 loss: 0.21417098477936847
  batch 362 loss: 0.21419801682398465
  batch 363 loss: 0.21415091344968676
  batch 364 loss: 0.2140633460175205
  batch 365 loss: 0.21412226374018684
  batch 366 loss: 0.21415221389851283
  batch 367 loss: 0.21413173422988818
  batch 368 loss: 0.21408858931744876
  batch 369 loss: 0.21403689500762196
  batch 370 loss: 0.21396675407886506
  batch 371 loss: 0.2140207851752438
  batch 372 loss: 0.21403731397723638
  batch 373 loss: 0.21388268538678298
  batch 374 loss: 0.21373150593297366
  batch 375 loss: 0.21361406910419464
  batch 376 loss: 0.21359154843586556
  batch 377 loss: 0.21356088855854713
  batch 378 loss: 0.21353176594884307
  batch 379 loss: 0.213515864749068
  batch 380 loss: 0.2135325791804414
  batch 381 loss: 0.21350098063000858
  batch 382 loss: 0.21348479366739384
  batch 383 loss: 0.21348789213687258
  batch 384 loss: 0.21349261658421406
  batch 385 loss: 0.21359541907713012
  batch 386 loss: 0.21354336951680752
  batch 387 loss: 0.21360538195269976
  batch 388 loss: 0.21362187529040366
  batch 389 loss: 0.2136416921732052
  batch 390 loss: 0.21366480990098072
  batch 391 loss: 0.2137095325666925
  batch 392 loss: 0.21365394941246024
  batch 393 loss: 0.21358097043201213
  batch 394 loss: 0.2135891198431175
  batch 395 loss: 0.21351966842820372
  batch 396 loss: 0.21340332801143327
  batch 397 loss: 0.2133951654437207
  batch 398 loss: 0.2133368254456688
  batch 399 loss: 0.21344210853552759
  batch 400 loss: 0.21354257375001906
  batch 401 loss: 0.21343316334738696
  batch 402 loss: 0.21348006385772383
  batch 403 loss: 0.21348581831005609
  batch 404 loss: 0.21344485417893616
  batch 405 loss: 0.2134457602545067
  batch 406 loss: 0.21351529334859895
  batch 407 loss: 0.2135438812159789
  batch 408 loss: 0.21365217440852932
  batch 409 loss: 0.21369996885507206
  batch 410 loss: 0.21379351986617576
  batch 411 loss: 0.21379869534586468
  batch 412 loss: 0.21383241958265164
  batch 413 loss: 0.2139331658368538
  batch 414 loss: 0.21401836258777673
  batch 415 loss: 0.21402161405029066
  batch 416 loss: 0.21412009849714544
  batch 417 loss: 0.2141034491270852
  batch 418 loss: 0.21410312332462467
  batch 419 loss: 0.21414008028894166
  batch 420 loss: 0.21407935101361503
  batch 421 loss: 0.21400191856110182
  batch 422 loss: 0.21414049344040206
  batch 423 loss: 0.21412381656626436
  batch 424 loss: 0.21407774639017177
  batch 425 loss: 0.213995724215227
  batch 426 loss: 0.21396488951685283
  batch 427 loss: 0.21393389089046094
  batch 428 loss: 0.21392087088288547
  batch 429 loss: 0.21398323812545875
  batch 430 loss: 0.21394805502752925
  batch 431 loss: 0.21396580404167662
  batch 432 loss: 0.21394977860014747
  batch 433 loss: 0.21400368475198195
  batch 434 loss: 0.2141336380474029
  batch 435 loss: 0.21416708967466463
  batch 436 loss: 0.2141834466008965
  batch 437 loss: 0.21423265975590974
  batch 438 loss: 0.21428786310022824
  batch 439 loss: 0.21428326070987555
  batch 440 loss: 0.21433792168443852
  batch 441 loss: 0.2143135166087118
  batch 442 loss: 0.2142575298566624
  batch 443 loss: 0.21424760933519754
  batch 444 loss: 0.21417037716454213
  batch 445 loss: 0.2141478293732311
  batch 446 loss: 0.21406927394091815
  batch 447 loss: 0.21402885299801028
  batch 448 loss: 0.21404859736295684
  batch 449 loss: 0.21405435491245414
  batch 450 loss: 0.2140528095761935
  batch 451 loss: 0.21402580798200915
  batch 452 loss: 0.21413332326090442
  batch 453 loss: 0.21417651346845606
  batch 454 loss: 0.21419282319918603
  batch 455 loss: 0.21421547266808186
  batch 456 loss: 0.21429878175912195
  batch 457 loss: 0.21428298405667104
  batch 458 loss: 0.21422416313376488
  batch 459 loss: 0.2143537503766598
  batch 460 loss: 0.21442468636061834
  batch 461 loss: 0.21441366088752375
  batch 462 loss: 0.21442755924545842
  batch 463 loss: 0.2143811565495979
  batch 464 loss: 0.21441292033755574
  batch 465 loss: 0.21434248506381948
  batch 466 loss: 0.21424803719755917
  batch 467 loss: 0.21427768092339247
  batch 468 loss: 0.21423431829764292
  batch 469 loss: 0.21427943006253192
  batch 470 loss: 0.21425377219281297
  batch 471 loss: 0.21425283951744153
  batch 472 loss: 0.21408692463222195
LOSS train 0.21408692463222195 valid 0.35649460554122925
LOSS train 0.21408692463222195 valid 0.3411737233400345
LOSS train 0.21408692463222195 valid 0.33526862661043805
LOSS train 0.21408692463222195 valid 0.33539123833179474
LOSS train 0.21408692463222195 valid 0.33057135343551636
LOSS train 0.21408692463222195 valid 0.3333703229824702
LOSS train 0.21408692463222195 valid 0.3435614790235247
LOSS train 0.21408692463222195 valid 0.34283485263586044
LOSS train 0.21408692463222195 valid 0.34374502963489956
LOSS train 0.21408692463222195 valid 0.34719612300395963
LOSS train 0.21408692463222195 valid 0.3439024795185436
LOSS train 0.21408692463222195 valid 0.34175904591878253
LOSS train 0.21408692463222195 valid 0.3410299993478335
LOSS train 0.21408692463222195 valid 0.34100126794406344
LOSS train 0.21408692463222195 valid 0.33510397672653197
LOSS train 0.21408692463222195 valid 0.3363357838243246
LOSS train 0.21408692463222195 valid 0.33984794336206775
LOSS train 0.21408692463222195 valid 0.34286220206154716
LOSS train 0.21408692463222195 valid 0.34515300236250224
LOSS train 0.21408692463222195 valid 0.34476574808359145
LOSS train 0.21408692463222195 valid 0.3429828683535258
LOSS train 0.21408692463222195 valid 0.3405276333743876
LOSS train 0.21408692463222195 valid 0.3416149940179742
LOSS train 0.21408692463222195 valid 0.33890804275870323
LOSS train 0.21408692463222195 valid 0.33710271000862124
LOSS train 0.21408692463222195 valid 0.33627390288389647
LOSS train 0.21408692463222195 valid 0.3370416329966651
LOSS train 0.21408692463222195 valid 0.33774379747254507
LOSS train 0.21408692463222195 valid 0.33703266238344126
LOSS train 0.21408692463222195 valid 0.3377885282039642
LOSS train 0.21408692463222195 valid 0.34051616634092025
LOSS train 0.21408692463222195 valid 0.340397453866899
LOSS train 0.21408692463222195 valid 0.34144551103765314
LOSS train 0.21408692463222195 valid 0.3417298627250335
LOSS train 0.21408692463222195 valid 0.34403349246297565
LOSS train 0.21408692463222195 valid 0.3435861948463652
LOSS train 0.21408692463222195 valid 0.34312284073314153
LOSS train 0.21408692463222195 valid 0.3440821625684437
LOSS train 0.21408692463222195 valid 0.34368040469976574
LOSS train 0.21408692463222195 valid 0.34342181533575056
LOSS train 0.21408692463222195 valid 0.3453645538992998
LOSS train 0.21408692463222195 valid 0.34602393209934235
LOSS train 0.21408692463222195 valid 0.34573116690613503
LOSS train 0.21408692463222195 valid 0.34636100042950024
LOSS train 0.21408692463222195 valid 0.3455334809091356
LOSS train 0.21408692463222195 valid 0.3463716455127882
LOSS train 0.21408692463222195 valid 0.34824250797007944
LOSS train 0.21408692463222195 valid 0.3483063379923503
LOSS train 0.21408692463222195 valid 0.34909566202942205
LOSS train 0.21408692463222195 valid 0.3476173448562622
LOSS train 0.21408692463222195 valid 0.3471145033836365
LOSS train 0.21408692463222195 valid 0.34734196903613895
LOSS train 0.21408692463222195 valid 0.3472470112566678
LOSS train 0.21408692463222195 valid 0.34707121385468376
LOSS train 0.21408692463222195 valid 0.34687580466270446
LOSS train 0.21408692463222195 valid 0.34618432926280157
LOSS train 0.21408692463222195 valid 0.3460530586409987
LOSS train 0.21408692463222195 valid 0.3457329160180585
LOSS train 0.21408692463222195 valid 0.34659518124693534
LOSS train 0.21408692463222195 valid 0.34627785633007685
LOSS train 0.21408692463222195 valid 0.3452814705059177
LOSS train 0.21408692463222195 valid 0.34672977414823347
LOSS train 0.21408692463222195 valid 0.34740134316777427
LOSS train 0.21408692463222195 valid 0.3485607258044183
LOSS train 0.21408692463222195 valid 0.349305522441864
LOSS train 0.21408692463222195 valid 0.34912140531973407
LOSS train 0.21408692463222195 valid 0.34840032057975656
LOSS train 0.21408692463222195 valid 0.34841113844338584
LOSS train 0.21408692463222195 valid 0.34720066362533014
LOSS train 0.21408692463222195 valid 0.3475154093333653
LOSS train 0.21408692463222195 valid 0.3473320653740789
LOSS train 0.21408692463222195 valid 0.34733237367537284
LOSS train 0.21408692463222195 valid 0.34699555945723026
LOSS train 0.21408692463222195 valid 0.3468771198311368
LOSS train 0.21408692463222195 valid 0.34682517449061073
LOSS train 0.21408692463222195 valid 0.3470339190803076
LOSS train 0.21408692463222195 valid 0.34737800313280776
LOSS train 0.21408692463222195 valid 0.34789156417051953
LOSS train 0.21408692463222195 valid 0.34855873335765886
LOSS train 0.21408692463222195 valid 0.3473866920918226
LOSS train 0.21408692463222195 valid 0.3462770555490329
LOSS train 0.21408692463222195 valid 0.34727114984175056
LOSS train 0.21408692463222195 valid 0.3470218680709241
LOSS train 0.21408692463222195 valid 0.3467097555597623
LOSS train 0.21408692463222195 valid 0.34632688340018775
LOSS train 0.21408692463222195 valid 0.3457113247971202
LOSS train 0.21408692463222195 valid 0.3454686691021097
LOSS train 0.21408692463222195 valid 0.3447731987319209
LOSS train 0.21408692463222195 valid 0.3455138219876236
LOSS train 0.21408692463222195 valid 0.3458909385734134
LOSS train 0.21408692463222195 valid 0.34610073350288056
LOSS train 0.21408692463222195 valid 0.34630815296069434
LOSS train 0.21408692463222195 valid 0.34607054437360457
LOSS train 0.21408692463222195 valid 0.3462848742591574
LOSS train 0.21408692463222195 valid 0.3460645543901544
LOSS train 0.21408692463222195 valid 0.3468966943522294
LOSS train 0.21408692463222195 valid 0.34719850660599383
LOSS train 0.21408692463222195 valid 0.34777566547296485
LOSS train 0.21408692463222195 valid 0.3480907681614462
LOSS train 0.21408692463222195 valid 0.34822002798318863
LOSS train 0.21408692463222195 valid 0.34848838159353424
LOSS train 0.21408692463222195 valid 0.34851980413876327
LOSS train 0.21408692463222195 valid 0.3487734438724888
LOSS train 0.21408692463222195 valid 0.3490043036066569
LOSS train 0.21408692463222195 valid 0.3490283991609301
LOSS train 0.21408692463222195 valid 0.34941810088337594
LOSS train 0.21408692463222195 valid 0.3489774479487232
LOSS train 0.21408692463222195 valid 0.3490581032302644
LOSS train 0.21408692463222195 valid 0.3493776851837788
LOSS train 0.21408692463222195 valid 0.34940516352653506
LOSS train 0.21408692463222195 valid 0.34867621219909944
LOSS train 0.21408692463222195 valid 0.34839351874377045
LOSS train 0.21408692463222195 valid 0.3483525522514782
LOSS train 0.21408692463222195 valid 0.34784318297578576
LOSS train 0.21408692463222195 valid 0.3482561181420865
LOSS train 0.21408692463222195 valid 0.3479332607881776
LOSS train 0.21408692463222195 valid 0.34806458537395185
LOSS train 0.21408692463222195 valid 0.34783090398473254
LOSS train 0.21408692463222195 valid 0.34744079022848307
LOSS train 0.21408692463222195 valid 0.34706182753046355
LOSS train 0.21408692463222195 valid 0.34697777697862675
LOSS train 0.21408692463222195 valid 0.3466387540102005
LOSS train 0.21408692463222195 valid 0.34671686480684977
LOSS train 0.21408692463222195 valid 0.3471569671265541
LOSS train 0.21408692463222195 valid 0.34699836325645445
LOSS train 0.21408692463222195 valid 0.3470367725406374
LOSS train 0.21408692463222195 valid 0.34691428410725333
LOSS train 0.21408692463222195 valid 0.34754346078261733
LOSS train 0.21408692463222195 valid 0.3476998794448468
LOSS train 0.21408692463222195 valid 0.3473486774242841
LOSS train 0.21408692463222195 valid 0.3472499258190621
LOSS train 0.21408692463222195 valid 0.34688448002844147
LOSS train 0.21408692463222195 valid 0.3469822709273575
LOSS train 0.21408692463222195 valid 0.34691799287475755
LOSS train 0.21408692463222195 valid 0.34662395671561913
LOSS train 0.21408692463222195 valid 0.3464731181807378
LOSS train 0.21408692463222195 valid 0.34635058610978786
LOSS train 0.21408692463222195 valid 0.3463647354772125
LOSS train 0.21408692463222195 valid 0.34626710136159716
LOSS train 0.21408692463222195 valid 0.346366514478411
LOSS train 0.21408692463222195 valid 0.3463241950417241
LOSS train 0.21408692463222195 valid 0.3466886109869245
LOSS train 0.21408692463222195 valid 0.3466726281009354
LOSS train 0.21408692463222195 valid 0.34653172621296513
LOSS train 0.21408692463222195 valid 0.3461457815663568
LOSS train 0.21408692463222195 valid 0.34621800890524096
LOSS train 0.21408692463222195 valid 0.34604407836790796
LOSS train 0.21408692463222195 valid 0.34704830658596914
LOSS train 0.21408692463222195 valid 0.34703569064204326
LOSS train 0.21408692463222195 valid 0.3472911506891251
LOSS train 0.21408692463222195 valid 0.34730192308394325
LOSS train 0.21408692463222195 valid 0.34687996694916173
LOSS train 0.21408692463222195 valid 0.347170622325411
LOSS train 0.21408692463222195 valid 0.34703677079894324
LOSS train 0.21408692463222195 valid 0.3472654425328778
LOSS train 0.21408692463222195 valid 0.34739266412380415
LOSS train 0.21408692463222195 valid 0.347290556521932
LOSS train 0.21408692463222195 valid 0.34743892561785783
LOSS train 0.21408692463222195 valid 0.34737820201699837
LOSS train 0.21408692463222195 valid 0.3473391404375434
LOSS train 0.21408692463222195 valid 0.34725762949967237
LOSS train 0.21408692463222195 valid 0.34704765530280124
LOSS train 0.21408692463222195 valid 0.3468310990216542
LOSS train 0.21408692463222195 valid 0.3463411832728037
LOSS train 0.21408692463222195 valid 0.34598167069030533
LOSS train 0.21408692463222195 valid 0.3460960416908724
LOSS train 0.21408692463222195 valid 0.3464696327012456
LOSS train 0.21408692463222195 valid 0.3461256112371172
LOSS train 0.21408692463222195 valid 0.3463869408742916
LOSS train 0.21408692463222195 valid 0.34639061075799604
LOSS train 0.21408692463222195 valid 0.3463965978538781
LOSS train 0.21408692463222195 valid 0.346145783572696
LOSS train 0.21408692463222195 valid 0.3461061657508674
LOSS train 0.21408692463222195 valid 0.34609301679435817
LOSS train 0.21408692463222195 valid 0.3458124308926719
LOSS train 0.21408692463222195 valid 0.3458111013539813
LOSS train 0.21408692463222195 valid 0.3455873731165956
LOSS train 0.21408692463222195 valid 0.3457709703217731
LOSS train 0.21408692463222195 valid 0.34553394254359454
LOSS train 0.21408692463222195 valid 0.3454369854595926
LOSS train 0.21408692463222195 valid 0.3456734038518937
LOSS train 0.21408692463222195 valid 0.3460980779849566
LOSS train 0.21408692463222195 valid 0.3461156844115648
LOSS train 0.21408692463222195 valid 0.3460694324711095
LOSS train 0.21408692463222195 valid 0.34556677728085905
LOSS train 0.21408692463222195 valid 0.3454770876835751
LOSS train 0.21408692463222195 valid 0.3454842097300259
LOSS train 0.21408692463222195 valid 0.34547064548477213
LOSS train 0.21408692463222195 valid 0.3452885067336774
LOSS train 0.21408692463222195 valid 0.3453688522702769
LOSS train 0.21408692463222195 valid 0.34514778212727054
LOSS train 0.21408692463222195 valid 0.3452046617555122
LOSS train 0.21408692463222195 valid 0.3450376386778342
LOSS train 0.21408692463222195 valid 0.34490205869846735
LOSS train 0.21408692463222195 valid 0.34456281799536487
LOSS train 0.21408692463222195 valid 0.344352768848137
LOSS train 0.21408692463222195 valid 0.34450582910310196
LOSS train 0.21408692463222195 valid 0.34409609030593524
LOSS train 0.21408692463222195 valid 0.34416929037127664
LOSS train 0.21408692463222195 valid 0.3441716794669628
LOSS train 0.21408692463222195 valid 0.34399569612830433
LOSS train 0.21408692463222195 valid 0.34386573466334014
LOSS train 0.21408692463222195 valid 0.3436836476983695
LOSS train 0.21408692463222195 valid 0.34363701457486434
LOSS train 0.21408692463222195 valid 0.3431322225710241
LOSS train 0.21408692463222195 valid 0.3432591110467911
LOSS train 0.21408692463222195 valid 0.34316980219693577
LOSS train 0.21408692463222195 valid 0.3428330182169492
LOSS train 0.21408692463222195 valid 0.3426982605001002
LOSS train 0.21408692463222195 valid 0.3428578061716897
LOSS train 0.21408692463222195 valid 0.34308281865730106
LOSS train 0.21408692463222195 valid 0.343113493244603
LOSS train 0.21408692463222195 valid 0.34332171921998683
LOSS train 0.21408692463222195 valid 0.34341272224332686
LOSS train 0.21408692463222195 valid 0.3430826723575592
LOSS train 0.21408692463222195 valid 0.3430875985434762
LOSS train 0.21408692463222195 valid 0.34309482725534574
LOSS train 0.21408692463222195 valid 0.3431174407858367
LOSS train 0.21408692463222195 valid 0.343265938976584
LOSS train 0.21408692463222195 valid 0.34330487562851475
LOSS train 0.21408692463222195 valid 0.3434092693738808
LOSS train 0.21408692463222195 valid 0.3431981169694179
LOSS train 0.21408692463222195 valid 0.34332725485878673
LOSS train 0.21408692463222195 valid 0.3434218969196081
LOSS train 0.21408692463222195 valid 0.343281538883845
LOSS train 0.21408692463222195 valid 0.3433303033883593
LOSS train 0.21408692463222195 valid 0.34376188036103605
LOSS train 0.21408692463222195 valid 0.3438206742468633
LOSS train 0.21408692463222195 valid 0.34394872435836293
LOSS train 0.21408692463222195 valid 0.3440986124069794
LOSS train 0.21408692463222195 valid 0.3441846671300533
LOSS train 0.21408692463222195 valid 0.3441833449078017
LOSS train 0.21408692463222195 valid 0.34403192395816035
LOSS train 0.21408692463222195 valid 0.34399480799324494
LOSS train 0.21408692463222195 valid 0.34436460581231626
LOSS train 0.21408692463222195 valid 0.3441407887612359
LOSS train 0.21408692463222195 valid 0.34439415255176364
LOSS train 0.21408692463222195 valid 0.3444197622417402
LOSS train 0.21408692463222195 valid 0.34432969881400904
LOSS train 0.21408692463222195 valid 0.34430502677957214
LOSS train 0.21408692463222195 valid 0.34446708987857294
LOSS train 0.21408692463222195 valid 0.3442446564839891
LOSS train 0.21408692463222195 valid 0.34458034040996566
LOSS train 0.21408692463222195 valid 0.3451601550227306
LOSS train 0.21408692463222195 valid 0.34550456027595366
LOSS train 0.21408692463222195 valid 0.3455874161507056
LOSS train 0.21408692463222195 valid 0.34561729817255304
LOSS train 0.21408692463222195 valid 0.34546936183206495
LOSS train 0.21408692463222195 valid 0.34536194514079266
LOSS train 0.21408692463222195 valid 0.34564075446128845
LOSS train 0.21408692463222195 valid 0.34575585884402
LOSS train 0.21408692463222195 valid 0.345976574080331
LOSS train 0.21408692463222195 valid 0.34597724051814777
LOSS train 0.21408692463222195 valid 0.3458230271348803
LOSS train 0.21408692463222195 valid 0.3459110202742558
LOSS train 0.21408692463222195 valid 0.3460254864767194
LOSS train 0.21408692463222195 valid 0.3457702044615022
LOSS train 0.21408692463222195 valid 0.34590091007624485
LOSS train 0.21408692463222195 valid 0.3458812563575833
LOSS train 0.21408692463222195 valid 0.3457852677657054
LOSS train 0.21408692463222195 valid 0.3459338953440217
LOSS train 0.21408692463222195 valid 0.3459966272570705
LOSS train 0.21408692463222195 valid 0.3461459611078632
LOSS train 0.21408692463222195 valid 0.34611160308122635
LOSS train 0.21408692463222195 valid 0.3459986184003218
LOSS train 0.21408692463222195 valid 0.34615943631283325
LOSS train 0.21408692463222195 valid 0.34632380513216227
LOSS train 0.21408692463222195 valid 0.34653047216472344
LOSS train 0.21408692463222195 valid 0.3464936511002509
LOSS train 0.21408692463222195 valid 0.34634881792245087
LOSS train 0.21408692463222195 valid 0.34649086196484163
LOSS train 0.21408692463222195 valid 0.3468118763583548
LOSS train 0.21408692463222195 valid 0.34684756845781656
LOSS train 0.21408692463222195 valid 0.3467734358171477
LOSS train 0.21408692463222195 valid 0.3466567122936249
LOSS train 0.21408692463222195 valid 0.346463970516039
LOSS train 0.21408692463222195 valid 0.34628845595280616
LOSS train 0.21408692463222195 valid 0.3460557613012602
LOSS train 0.21408692463222195 valid 0.3460868388734838
LOSS train 0.21408692463222195 valid 0.346009304693767
LOSS train 0.21408692463222195 valid 0.34588332309841685
LOSS train 0.21408692463222195 valid 0.34562567119480025
LOSS train 0.21408692463222195 valid 0.3455105606111115
LOSS train 0.21408692463222195 valid 0.3456146629973197
LOSS train 0.21408692463222195 valid 0.345733194183885
LOSS train 0.21408692463222195 valid 0.34564581544666023
LOSS train 0.21408692463222195 valid 0.3456173481010809
LOSS train 0.21408692463222195 valid 0.3456090319280823
LOSS train 0.21408692463222195 valid 0.3457891761637889
LOSS train 0.21408692463222195 valid 0.34581252860611883
LOSS train 0.21408692463222195 valid 0.34570643735915113
LOSS train 0.21408692463222195 valid 0.345844518240184
LOSS train 0.21408692463222195 valid 0.3457741126062113
LOSS train 0.21408692463222195 valid 0.3458858547972984
LOSS train 0.21408692463222195 valid 0.34588990979275464
LOSS train 0.21408692463222195 valid 0.3457422231298846
LOSS train 0.21408692463222195 valid 0.34578273161894546
LOSS train 0.21408692463222195 valid 0.3457259619796036
LOSS train 0.21408692463222195 valid 0.3457255177075249
LOSS train 0.21408692463222195 valid 0.3455975737174352
LOSS train 0.21408692463222195 valid 0.3455483201532269
LOSS train 0.21408692463222195 valid 0.3456565622264976
LOSS train 0.21408692463222195 valid 0.3457770614144039
LOSS train 0.21408692463222195 valid 0.34580077103486184
LOSS train 0.21408692463222195 valid 0.3456321814021126
LOSS train 0.21408692463222195 valid 0.34565339174145965
LOSS train 0.21408692463222195 valid 0.34553460733121694
LOSS train 0.21408692463222195 valid 0.3454041832259723
LOSS train 0.21408692463222195 valid 0.3452964361431529
LOSS train 0.21408692463222195 valid 0.34531239001981673
LOSS train 0.21408692463222195 valid 0.34525870807301196
LOSS train 0.21408692463222195 valid 0.34525448246262014
LOSS train 0.21408692463222195 valid 0.34514773472810323
LOSS train 0.21408692463222195 valid 0.3452944427159182
LOSS train 0.21408692463222195 valid 0.34525616178436885
LOSS train 0.21408692463222195 valid 0.345162683555597
LOSS train 0.21408692463222195 valid 0.3452888496854704
LOSS train 0.21408692463222195 valid 0.3454429420097819
LOSS train 0.21408692463222195 valid 0.34556761720337464
LOSS train 0.21408692463222195 valid 0.34548447383567693
LOSS train 0.21408692463222195 valid 0.34576021548000824
LOSS train 0.21408692463222195 valid 0.3455978126629539
LOSS train 0.21408692463222195 valid 0.3453587566735944
LOSS train 0.21408692463222195 valid 0.3453807483116786
LOSS train 0.21408692463222195 valid 0.3454640383903797
LOSS train 0.21408692463222195 valid 0.3455511612212
LOSS train 0.21408692463222195 valid 0.34554432574033006
LOSS train 0.21408692463222195 valid 0.3456008636915102
LOSS train 0.21408692463222195 valid 0.34569669847792767
LOSS train 0.21408692463222195 valid 0.34574825113469904
LOSS train 0.21408692463222195 valid 0.34563647809345555
LOSS train 0.21408692463222195 valid 0.34551841684852735
LOSS train 0.21408692463222195 valid 0.34546473416480217
LOSS train 0.21408692463222195 valid 0.3456364536356783
LOSS train 0.21408692463222195 valid 0.34553911783801977
LOSS train 0.21408692463222195 valid 0.3452879708437693
LOSS train 0.21408692463222195 valid 0.3452835625109404
LOSS train 0.21408692463222195 valid 0.3451713001763327
LOSS train 0.21408692463222195 valid 0.34508014841768947
LOSS train 0.21408692463222195 valid 0.34509885898407766
LOSS train 0.21408692463222195 valid 0.34493605980425635
LOSS train 0.21408692463222195 valid 0.3448656843593943
LOSS train 0.21408692463222195 valid 0.34473394582987527
LOSS train 0.21408692463222195 valid 0.34493786599053894
LOSS train 0.21408692463222195 valid 0.3450423158597255
LOSS train 0.21408692463222195 valid 0.34497782889473644
LOSS train 0.21408692463222195 valid 0.3447602535530882
LOSS train 0.21408692463222195 valid 0.3446533542083598
LOSS train 0.21408692463222195 valid 0.3446613258140476
LOSS train 0.21408692463222195 valid 0.34455094822815485
LOSS train 0.21408692463222195 valid 0.3444661665336359
LOSS train 0.21408692463222195 valid 0.3444104523143985
LOSS train 0.21408692463222195 valid 0.344361729898804
LOSS train 0.21408692463222195 valid 0.3444061472927783
LOSS train 0.21408692463222195 valid 0.344565031981804
LOSS train 0.21408692463222195 valid 0.3447165248936482
LOSS train 0.21408692463222195 valid 0.3447848502494374
LOSS train 0.21408692463222195 valid 0.3447747318771298
LOSS train 0.21408692463222195 valid 0.3448041845164923
LOSS train 0.21408692463222195 valid 0.3447457406255934
LOSS train 0.21408692463222195 valid 0.34468585962734066
LOSS train 0.21408692463222195 valid 0.3449184398954086
LOSS train 0.21408692463222195 valid 0.3447170057244209
LOSS train 0.21408692463222195 valid 0.34471474432355753
LOSS train 0.21408692463222195 valid 0.34479734783303245
LOSS train 0.21408692463222195 valid 0.34469599256424305
LOSS train 0.21408692463222195 valid 0.3445293726323411
LOSS train 0.21408692463222195 valid 0.3445489285108836
LOSS train 0.21408692463222195 valid 0.34473805359708587
EPOCH 28:
  batch 1 loss: 0.21870830655097961
  batch 2 loss: 0.20837652683258057
  batch 3 loss: 0.20770945648352304
  batch 4 loss: 0.21238932758569717
  batch 5 loss: 0.21271005570888518
  batch 6 loss: 0.2114211842417717
  batch 7 loss: 0.21492066766534532
  batch 8 loss: 0.2209803108125925
  batch 9 loss: 0.22172592083613077
  batch 10 loss: 0.22207984179258347
  batch 11 loss: 0.22229302335869183
  batch 12 loss: 0.2188724180062612
  batch 13 loss: 0.2196476046855633
  batch 14 loss: 0.22021119509424483
  batch 15 loss: 0.2202422986427943
  batch 16 loss: 0.22085120994597673
  batch 17 loss: 0.2178110708208645
  batch 18 loss: 0.2176091977291637
  batch 19 loss: 0.2157118979253267
  batch 20 loss: 0.21270232871174813
  batch 21 loss: 0.21191143918605077
  batch 22 loss: 0.2106465602462942
  batch 23 loss: 0.20918694721615833
  batch 24 loss: 0.20707352148989835
  batch 25 loss: 0.20879741966724397
  batch 26 loss: 0.20805536267849115
  batch 27 loss: 0.2085096118626771
  batch 28 loss: 0.2067024697150503
  batch 29 loss: 0.2066757175429114
  batch 30 loss: 0.20627406885226568
  batch 31 loss: 0.2068252707681348
  batch 32 loss: 0.20697729848325253
  batch 33 loss: 0.20716308102463232
  batch 34 loss: 0.2063631297034376
  batch 35 loss: 0.20624373597758158
  batch 36 loss: 0.20764977195196682
  batch 37 loss: 0.20753889188573166
  batch 38 loss: 0.20766141932261617
  batch 39 loss: 0.20700759994678009
  batch 40 loss: 0.20819098129868507
  batch 41 loss: 0.20796428729848163
  batch 42 loss: 0.20747480080241248
  batch 43 loss: 0.20807053599246714
  batch 44 loss: 0.2075763517482714
  batch 45 loss: 0.20712660352389017
  batch 46 loss: 0.20825103249238885
  batch 47 loss: 0.20749561139877806
  batch 48 loss: 0.20662419560054937
  batch 49 loss: 0.20618537068367004
  batch 50 loss: 0.20601433664560317
  batch 51 loss: 0.2062808599542169
  batch 52 loss: 0.20665170481571785
  batch 53 loss: 0.20668814345350806
  batch 54 loss: 0.20726933385486956
  batch 55 loss: 0.20704819560050963
  batch 56 loss: 0.20687698892184667
  batch 57 loss: 0.20697614655160068
  batch 58 loss: 0.20768188505337157
  batch 59 loss: 0.20764426356655055
  batch 60 loss: 0.207591217259566
  batch 61 loss: 0.20821288960878967
  batch 62 loss: 0.20862217271520245
  batch 63 loss: 0.20852468056338175
  batch 64 loss: 0.20931059238500893
  batch 65 loss: 0.20923779881917515
  batch 66 loss: 0.2091260954286113
  batch 67 loss: 0.2096501788986263
  batch 68 loss: 0.20999268793007908
  batch 69 loss: 0.21038238250690958
  batch 70 loss: 0.21098384048257554
  batch 71 loss: 0.21077704471601566
  batch 72 loss: 0.21071380149159166
  batch 73 loss: 0.21063078450013514
  batch 74 loss: 0.21049975201084808
  batch 75 loss: 0.21028387467066448
  batch 76 loss: 0.21103178356823168
  batch 77 loss: 0.21055545744957863
  batch 78 loss: 0.21079438408980003
  batch 79 loss: 0.21073992644684225
  batch 80 loss: 0.21038937326520682
  batch 81 loss: 0.21049300884758984
  batch 82 loss: 0.21113547846311476
  batch 83 loss: 0.21122723297182336
  batch 84 loss: 0.21075691318228132
  batch 85 loss: 0.21082809094120475
  batch 86 loss: 0.211831767025382
  batch 87 loss: 0.21153612177947473
  batch 88 loss: 0.2112192464145747
  batch 89 loss: 0.21140217998724306
  batch 90 loss: 0.21168967882792156
  batch 91 loss: 0.21181486317744622
  batch 92 loss: 0.21156772954956346
  batch 93 loss: 0.21151646859543297
  batch 94 loss: 0.21197605275727333
  batch 95 loss: 0.21170350830805928
  batch 96 loss: 0.2116124426635603
  batch 97 loss: 0.21152886179919095
  batch 98 loss: 0.21187589104686463
  batch 99 loss: 0.21227655401735596
  batch 100 loss: 0.21173889815807342
  batch 101 loss: 0.21148283514055874
  batch 102 loss: 0.21186633335024702
  batch 103 loss: 0.2123572217318618
  batch 104 loss: 0.21259112641788447
  batch 105 loss: 0.21244615188666752
  batch 106 loss: 0.21252231552915754
  batch 107 loss: 0.21237083378239213
  batch 108 loss: 0.21251666601057406
  batch 109 loss: 0.2124988994467149
  batch 110 loss: 0.21261653412472117
  batch 111 loss: 0.21285797145452584
  batch 112 loss: 0.21272696674402272
  batch 113 loss: 0.21281560991717652
  batch 114 loss: 0.21319117959131273
  batch 115 loss: 0.21338014874769293
  batch 116 loss: 0.21348017055926652
  batch 117 loss: 0.21363245129075825
  batch 118 loss: 0.21348515645427218
  batch 119 loss: 0.21361918171413807
  batch 120 loss: 0.21314280144870282
  batch 121 loss: 0.21311620210320498
  batch 122 loss: 0.21297143326431026
  batch 123 loss: 0.21292877330528043
  batch 124 loss: 0.21316887738723908
  batch 125 loss: 0.21304520845413208
  batch 126 loss: 0.21304198567356383
  batch 127 loss: 0.21326839970791434
  batch 128 loss: 0.21295290137641132
  batch 129 loss: 0.21318416828794998
  batch 130 loss: 0.21313133973341722
  batch 131 loss: 0.2130693448636368
  batch 132 loss: 0.2129714989075155
  batch 133 loss: 0.21326591836330586
  batch 134 loss: 0.2133665847689358
  batch 135 loss: 0.21307164353353006
  batch 136 loss: 0.21323891805813594
  batch 137 loss: 0.21322600391224353
  batch 138 loss: 0.2132063145222871
  batch 139 loss: 0.2133194394677663
  batch 140 loss: 0.21325386528457915
  batch 141 loss: 0.2135339434265245
  batch 142 loss: 0.21345698770502924
  batch 143 loss: 0.2131224923825764
  batch 144 loss: 0.2131195930350158
  batch 145 loss: 0.2132324794243122
  batch 146 loss: 0.21309309861023132
  batch 147 loss: 0.21335215055618156
  batch 148 loss: 0.21334641668442134
  batch 149 loss: 0.21310243600566917
  batch 150 loss: 0.21309888243675232
  batch 151 loss: 0.21302412381235336
  batch 152 loss: 0.213380353427247
  batch 153 loss: 0.2131134896886115
  batch 154 loss: 0.21318489680816602
  batch 155 loss: 0.21305078710279157
  batch 156 loss: 0.21303720590777886
  batch 157 loss: 0.21325784266754322
  batch 158 loss: 0.21322188035974018
  batch 159 loss: 0.2133502110182864
  batch 160 loss: 0.21305062873288988
  batch 161 loss: 0.21306966809752564
  batch 162 loss: 0.2128959775522903
  batch 163 loss: 0.21297666494466044
  batch 164 loss: 0.2127622391392545
  batch 165 loss: 0.21275061856616628
  batch 166 loss: 0.21271650475191783
  batch 167 loss: 0.2126423356061924
  batch 168 loss: 0.21251783058756873
  batch 169 loss: 0.21227281527406366
  batch 170 loss: 0.2121403984287206
  batch 171 loss: 0.21204540621467502
  batch 172 loss: 0.21191049739718437
  batch 173 loss: 0.2118097052753316
  batch 174 loss: 0.21185708559792618
  batch 175 loss: 0.21188331621033804
  batch 176 loss: 0.21175236932256006
  batch 177 loss: 0.2118215716659686
  batch 178 loss: 0.21210142660341905
  batch 179 loss: 0.2119908457694773
  batch 180 loss: 0.2116949020160569
  batch 181 loss: 0.21180996578701294
  batch 182 loss: 0.21187149708742622
  batch 183 loss: 0.21162185292752062
  batch 184 loss: 0.21173358489961727
  batch 185 loss: 0.21192480398190988
  batch 186 loss: 0.21208040520388594
  batch 187 loss: 0.21192962656365358
  batch 188 loss: 0.21170396714451464
  batch 189 loss: 0.21141543180223496
  batch 190 loss: 0.21142793125227877
  batch 191 loss: 0.2113993382422712
  batch 192 loss: 0.2113008681529512
  batch 193 loss: 0.21122301736643895
  batch 194 loss: 0.2112562946744801
  batch 195 loss: 0.21129187528903667
  batch 196 loss: 0.21124213264912975
  batch 197 loss: 0.21114197774284382
  batch 198 loss: 0.21144605172101896
  batch 199 loss: 0.2111414778771712
  batch 200 loss: 0.2114740365743637
  batch 201 loss: 0.21173406433110215
  batch 202 loss: 0.21175437966490737
  batch 203 loss: 0.21164490099023717
  batch 204 loss: 0.21152901941654728
  batch 205 loss: 0.21177530434073472
  batch 206 loss: 0.21194063763595322
  batch 207 loss: 0.21219241719890908
  batch 208 loss: 0.21237410934498677
  batch 209 loss: 0.21235974774691477
  batch 210 loss: 0.21257109067269733
  batch 211 loss: 0.21255544611910507
  batch 212 loss: 0.2125985414352057
  batch 213 loss: 0.2125799450935892
  batch 214 loss: 0.2125970276298924
  batch 215 loss: 0.2124841186196305
  batch 216 loss: 0.21218326805090462
  batch 217 loss: 0.212018076450594
  batch 218 loss: 0.21192852086430297
  batch 219 loss: 0.21191334758447186
  batch 220 loss: 0.21175209324468267
  batch 221 loss: 0.21189244469096757
  batch 222 loss: 0.2119226724849091
  batch 223 loss: 0.2119242111262719
  batch 224 loss: 0.21189887362665363
  batch 225 loss: 0.21185694913069408
  batch 226 loss: 0.21187250744716257
  batch 227 loss: 0.2118290241331781
  batch 228 loss: 0.21184914637553065
  batch 229 loss: 0.21181842574125814
  batch 230 loss: 0.21192815102960752
  batch 231 loss: 0.21190884657752462
  batch 232 loss: 0.21182866484440607
  batch 233 loss: 0.21188432847993058
  batch 234 loss: 0.2118839462980246
  batch 235 loss: 0.21183930052087663
  batch 236 loss: 0.21169204397474306
  batch 237 loss: 0.2116641190987599
  batch 238 loss: 0.21165313371339767
  batch 239 loss: 0.21148995787029984
  batch 240 loss: 0.2114094006518523
  batch 241 loss: 0.21143881172807386
  batch 242 loss: 0.2112553361033605
  batch 243 loss: 0.2112619578225132
  batch 244 loss: 0.2112455796145025
  batch 245 loss: 0.2110717410943946
  batch 246 loss: 0.2109909846288402
  batch 247 loss: 0.21117733304317182
  batch 248 loss: 0.21119115213232656
  batch 249 loss: 0.21116940928510872
  batch 250 loss: 0.21108509081602098
  batch 251 loss: 0.21103530350434352
  batch 252 loss: 0.21101784895336817
  batch 253 loss: 0.2109999406950276
  batch 254 loss: 0.21098426793973277
  batch 255 loss: 0.2110615629191492
  batch 256 loss: 0.2113319956115447
  batch 257 loss: 0.21146201362637695
  batch 258 loss: 0.21136697809132496
  batch 259 loss: 0.21150124792196576
  batch 260 loss: 0.21155847150545853
  batch 261 loss: 0.21148694771916474
  batch 262 loss: 0.2114060699712229
  batch 263 loss: 0.2115359490475274
  batch 264 loss: 0.21142731623893435
  batch 265 loss: 0.21140021209446888
  batch 266 loss: 0.21142263592858063
  batch 267 loss: 0.21141464321800832
  batch 268 loss: 0.2114599789701291
  batch 269 loss: 0.21155127794326017
  batch 270 loss: 0.21182655416153096
  batch 271 loss: 0.21190333905255224
  batch 272 loss: 0.21191839057513895
  batch 273 loss: 0.21187445319397546
  batch 274 loss: 0.21195195278112036
  batch 275 loss: 0.21197860159657217
  batch 276 loss: 0.21201572237887245
  batch 277 loss: 0.212019225804384
  batch 278 loss: 0.2120125763815084
  batch 279 loss: 0.21201854864115355
  batch 280 loss: 0.21193790403859955
  batch 281 loss: 0.21188028803711684
  batch 282 loss: 0.21200034878355392
  batch 283 loss: 0.2119126484166607
  batch 284 loss: 0.21197242258300245
  batch 285 loss: 0.21200256420854938
  batch 286 loss: 0.21198047166104084
  batch 287 loss: 0.2120253961883771
  batch 288 loss: 0.21191861832307446
  batch 289 loss: 0.2120224826269909
  batch 290 loss: 0.21195603866001655
  batch 291 loss: 0.21218826219797954
  batch 292 loss: 0.2122383149825547
  batch 293 loss: 0.21216580566692678
  batch 294 loss: 0.2122315007204912
  batch 295 loss: 0.21225770024930016
  batch 296 loss: 0.2124711404982451
  batch 297 loss: 0.21247663456783553
  batch 298 loss: 0.2124649292770648
  batch 299 loss: 0.2124777805047689
  batch 300 loss: 0.2125261456767718
  batch 301 loss: 0.21255113733963316
  batch 302 loss: 0.21259462315317812
  batch 303 loss: 0.21248637670927709
  batch 304 loss: 0.2124593079972424
  batch 305 loss: 0.21239597499370574
  batch 306 loss: 0.2125822959674729
  batch 307 loss: 0.21267969018084995
  batch 308 loss: 0.21273913534430713
  batch 309 loss: 0.2126917907045883
  batch 310 loss: 0.21259696767214806
  batch 311 loss: 0.21283501362685606
  batch 312 loss: 0.2128714304894973
  batch 313 loss: 0.21292226040325227
  batch 314 loss: 0.21284976824643506
  batch 315 loss: 0.21285895000374505
  batch 316 loss: 0.21284168859637237
  batch 317 loss: 0.21279574197361523
  batch 318 loss: 0.2128102829996145
  batch 319 loss: 0.21276378622249376
  batch 320 loss: 0.21277322242967783
  batch 321 loss: 0.21286233809321098
  batch 322 loss: 0.21287454965507022
  batch 323 loss: 0.2128012153570866
  batch 324 loss: 0.2127083179851373
  batch 325 loss: 0.21259886076817144
  batch 326 loss: 0.21258546869081954
  batch 327 loss: 0.21254122330143546
  batch 328 loss: 0.2123918550134432
  batch 329 loss: 0.21235651584019413
  batch 330 loss: 0.2122250685186097
  batch 331 loss: 0.2121712338528244
  batch 332 loss: 0.2120672423078353
  batch 333 loss: 0.2122794846156696
  batch 334 loss: 0.21222586885183872
  batch 335 loss: 0.21209166094438353
  batch 336 loss: 0.21199653528275944
  batch 337 loss: 0.21203782880341618
  batch 338 loss: 0.21204531435254056
  batch 339 loss: 0.21216126595099064
  batch 340 loss: 0.21227301538867108
  batch 341 loss: 0.2123855702426077
  batch 342 loss: 0.21235835617571547
  batch 343 loss: 0.21243666442594444
  batch 344 loss: 0.21254661588301493
  batch 345 loss: 0.2126690052125765
  batch 346 loss: 0.21263062531892965
  batch 347 loss: 0.21259718811305867
  batch 348 loss: 0.21260081038906656
  batch 349 loss: 0.21256784317657393
  batch 350 loss: 0.21269798989806857
  batch 351 loss: 0.2127063972742809
  batch 352 loss: 0.21273340212858535
  batch 353 loss: 0.21273280823703528
  batch 354 loss: 0.21293725717370793
  batch 355 loss: 0.21284721011007335
  batch 356 loss: 0.21284778198499357
  batch 357 loss: 0.2129244849878867
  batch 358 loss: 0.2130532782920246
  batch 359 loss: 0.2130156026162145
  batch 360 loss: 0.21306768767535686
  batch 361 loss: 0.21308179761068974
  batch 362 loss: 0.21307056129637345
  batch 363 loss: 0.21300468423478203
  batch 364 loss: 0.21288523671554996
  batch 365 loss: 0.21290274380821073
  batch 366 loss: 0.21290818504134162
  batch 367 loss: 0.21285839171760412
  batch 368 loss: 0.21279272477587927
  batch 369 loss: 0.21269964404545502
  batch 370 loss: 0.21262116444271964
  batch 371 loss: 0.21264008125365583
  batch 372 loss: 0.2126652466593891
  batch 373 loss: 0.2125500815723281
  batch 374 loss: 0.2123927696742476
  batch 375 loss: 0.21229708536465963
  batch 376 loss: 0.21228907756665918
  batch 377 loss: 0.2122557331222438
  batch 378 loss: 0.21218785513488073
  batch 379 loss: 0.21215483101974378
  batch 380 loss: 0.21217445323341771
  batch 381 loss: 0.21213898696298675
  batch 382 loss: 0.21208010832364646
  batch 383 loss: 0.2120624488078583
  batch 384 loss: 0.21204172043750683
  batch 385 loss: 0.21214539981507635
  batch 386 loss: 0.21213042805540747
  batch 387 loss: 0.2121961837206084
  batch 388 loss: 0.21221129346600512
  batch 389 loss: 0.2122167101402822
  batch 390 loss: 0.21218083485578879
  batch 391 loss: 0.2122121036357587
  batch 392 loss: 0.21214815229177475
  batch 393 loss: 0.21206823859657647
  batch 394 loss: 0.21206213965028675
  batch 395 loss: 0.21199558188643636
  batch 396 loss: 0.21187043822172916
  batch 397 loss: 0.21184765748323062
  batch 398 loss: 0.21179558860896222
  batch 399 loss: 0.21186849661638266
  batch 400 loss: 0.21199863851070405
  batch 401 loss: 0.21188320230664756
  batch 402 loss: 0.2119805934417307
  batch 403 loss: 0.2119795336173131
  batch 404 loss: 0.21195465060743954
  batch 405 loss: 0.21203808273062294
  batch 406 loss: 0.21209476675306047
  batch 407 loss: 0.2120926963755774
  batch 408 loss: 0.21216645683435834
  batch 409 loss: 0.21220269030492872
  batch 410 loss: 0.21231970318206927
  batch 411 loss: 0.21234757817574662
  batch 412 loss: 0.21235473048918455
  batch 413 loss: 0.21247528230018245
  batch 414 loss: 0.2125424703826075
  batch 415 loss: 0.21258963235171444
  batch 416 loss: 0.21266376739367843
  batch 417 loss: 0.21267671690141554
  batch 418 loss: 0.21271366757924476
  batch 419 loss: 0.21280167730156163
  batch 420 loss: 0.21274623675715357
  batch 421 loss: 0.21264722086754662
  batch 422 loss: 0.21279707367386297
  batch 423 loss: 0.21282072826762008
  batch 424 loss: 0.21280357301375777
  batch 425 loss: 0.21269947826862334
  batch 426 loss: 0.2126495925963205
  batch 427 loss: 0.21266069184700834
  batch 428 loss: 0.2126504650391708
  batch 429 loss: 0.21269053158226547
  batch 430 loss: 0.21267602987760723
  batch 431 loss: 0.2127190351555353
  batch 432 loss: 0.21268190788450064
  batch 433 loss: 0.21273214430236376
  batch 434 loss: 0.21282539719260782
  batch 435 loss: 0.2128135962732907
  batch 436 loss: 0.21280850169308688
  batch 437 loss: 0.21286189494744046
  batch 438 loss: 0.21290153972634443
  batch 439 loss: 0.21290097808240485
  batch 440 loss: 0.21295663778754798
  batch 441 loss: 0.21288755177910906
  batch 442 loss: 0.21281608699808294
  batch 443 loss: 0.21283503522587682
  batch 444 loss: 0.21280567260744335
  batch 445 loss: 0.21278849840164185
  batch 446 loss: 0.2127032921119121
  batch 447 loss: 0.21267327082903859
  batch 448 loss: 0.2127131098947887
  batch 449 loss: 0.2127069388505876
  batch 450 loss: 0.2126813624633683
  batch 451 loss: 0.2126155163588915
  batch 452 loss: 0.21269572410831408
  batch 453 loss: 0.21272681965996362
  batch 454 loss: 0.21274576155624725
  batch 455 loss: 0.2127497591815152
  batch 456 loss: 0.21280782829904765
  batch 457 loss: 0.2127971770792091
  batch 458 loss: 0.21274727706565608
  batch 459 loss: 0.21283839892977463
  batch 460 loss: 0.21291139566380043
  batch 461 loss: 0.21288059561345685
  batch 462 loss: 0.21286167504338474
  batch 463 loss: 0.21282197741015169
  batch 464 loss: 0.2128576624110855
  batch 465 loss: 0.21277243914783642
  batch 466 loss: 0.21268773999848592
  batch 467 loss: 0.212736807939072
  batch 468 loss: 0.21271372717033085
  batch 469 loss: 0.2127989146437472
  batch 470 loss: 0.21275751089796108
  batch 471 loss: 0.2127751243911731
  batch 472 loss: 0.21258502150491132
LOSS train 0.21258502150491132 valid 0.3411670923233032
LOSS train 0.21258502150491132 valid 0.33058665692806244
LOSS train 0.21258502150491132 valid 0.3271600008010864
LOSS train 0.21258502150491132 valid 0.32703355699777603
LOSS train 0.21258502150491132 valid 0.32495045065879824
LOSS train 0.21258502150491132 valid 0.326043963432312
LOSS train 0.21258502150491132 valid 0.3352572960512979
LOSS train 0.21258502150491132 valid 0.3353336565196514
LOSS train 0.21258502150491132 valid 0.33578963412178886
LOSS train 0.21258502150491132 valid 0.3392515003681183
LOSS train 0.21258502150491132 valid 0.3360675735907121
LOSS train 0.21258502150491132 valid 0.3342994973063469
LOSS train 0.21258502150491132 valid 0.33410818989460284
LOSS train 0.21258502150491132 valid 0.3338015526533127
LOSS train 0.21258502150491132 valid 0.3279126544793447
LOSS train 0.21258502150491132 valid 0.32942537777125835
LOSS train 0.21258502150491132 valid 0.33192604078966026
LOSS train 0.21258502150491132 valid 0.3343156460258696
LOSS train 0.21258502150491132 valid 0.33690655702038813
LOSS train 0.21258502150491132 valid 0.33632658421993256
LOSS train 0.21258502150491132 valid 0.3350757715247926
LOSS train 0.21258502150491132 valid 0.3327851512215354
LOSS train 0.21258502150491132 valid 0.3338827153910761
LOSS train 0.21258502150491132 valid 0.3323914172748725
LOSS train 0.21258502150491132 valid 0.33101961612701414
LOSS train 0.21258502150491132 valid 0.33009907832512486
LOSS train 0.21258502150491132 valid 0.3307656535395869
LOSS train 0.21258502150491132 valid 0.33092790309871944
LOSS train 0.21258502150491132 valid 0.33047933516831235
LOSS train 0.21258502150491132 valid 0.3314006358385086
LOSS train 0.21258502150491132 valid 0.3343072370175392
LOSS train 0.21258502150491132 valid 0.33376345038414
LOSS train 0.21258502150491132 valid 0.33502637256275525
LOSS train 0.21258502150491132 valid 0.334947390591397
LOSS train 0.21258502150491132 valid 0.3366741452898298
LOSS train 0.21258502150491132 valid 0.3363187437256177
LOSS train 0.21258502150491132 valid 0.33568948346215327
LOSS train 0.21258502150491132 valid 0.3366209684233916
LOSS train 0.21258502150491132 valid 0.3359669057222513
LOSS train 0.21258502150491132 valid 0.335350563377142
LOSS train 0.21258502150491132 valid 0.3370043562679756
LOSS train 0.21258502150491132 valid 0.3378605388459705
LOSS train 0.21258502150491132 valid 0.3374294167341188
LOSS train 0.21258502150491132 valid 0.33839366314086045
LOSS train 0.21258502150491132 valid 0.33757607142130536
LOSS train 0.21258502150491132 valid 0.3385782222384992
LOSS train 0.21258502150491132 valid 0.34016643686497466
LOSS train 0.21258502150491132 valid 0.34030551587541896
LOSS train 0.21258502150491132 valid 0.34094407911203345
LOSS train 0.21258502150491132 valid 0.3396552872657776
LOSS train 0.21258502150491132 valid 0.33934741043577005
LOSS train 0.21258502150491132 valid 0.33949626752963435
LOSS train 0.21258502150491132 valid 0.339352196117617
LOSS train 0.21258502150491132 valid 0.3394379146673061
LOSS train 0.21258502150491132 valid 0.3392098145051436
LOSS train 0.21258502150491132 valid 0.33846060026969227
LOSS train 0.21258502150491132 valid 0.3381384381076746
LOSS train 0.21258502150491132 valid 0.33793950132254896
LOSS train 0.21258502150491132 valid 0.3385883120156951
LOSS train 0.21258502150491132 valid 0.33843414386113485
LOSS train 0.21258502150491132 valid 0.3374455473462089
LOSS train 0.21258502150491132 valid 0.33881665910443953
LOSS train 0.21258502150491132 valid 0.339261998259832
LOSS train 0.21258502150491132 valid 0.34060376696288586
LOSS train 0.21258502150491132 valid 0.3414385272906377
LOSS train 0.21258502150491132 valid 0.34145619742798083
LOSS train 0.21258502150491132 valid 0.3404928815008989
LOSS train 0.21258502150491132 valid 0.3403597824713763
LOSS train 0.21258502150491132 valid 0.3392327712929767
LOSS train 0.21258502150491132 valid 0.33968542175633565
LOSS train 0.21258502150491132 valid 0.3395487494032148
LOSS train 0.21258502150491132 valid 0.3394677709374163
LOSS train 0.21258502150491132 valid 0.33888165877289966
LOSS train 0.21258502150491132 valid 0.3388057842447951
LOSS train 0.21258502150491132 valid 0.33873445669809976
LOSS train 0.21258502150491132 valid 0.3388543395619643
LOSS train 0.21258502150491132 valid 0.3393057228682877
LOSS train 0.21258502150491132 valid 0.3399892319471408
LOSS train 0.21258502150491132 valid 0.3406553483462032
LOSS train 0.21258502150491132 valid 0.3394306482747197
LOSS train 0.21258502150491132 valid 0.33832348239274673
LOSS train 0.21258502150491132 valid 0.3392492489116948
LOSS train 0.21258502150491132 valid 0.33905183586729576
LOSS train 0.21258502150491132 valid 0.3386267349123955
LOSS train 0.21258502150491132 valid 0.338378494977951
LOSS train 0.21258502150491132 valid 0.3376360993052638
LOSS train 0.21258502150491132 valid 0.33749464805098783
LOSS train 0.21258502150491132 valid 0.3367747207256881
LOSS train 0.21258502150491132 valid 0.3375784969061948
LOSS train 0.21258502150491132 valid 0.33784276213910847
LOSS train 0.21258502150491132 valid 0.33797460663449636
LOSS train 0.21258502150491132 valid 0.33815223788437637
LOSS train 0.21258502150491132 valid 0.33786789640303583
LOSS train 0.21258502150491132 valid 0.33812926043855385
LOSS train 0.21258502150491132 valid 0.3376963894618185
LOSS train 0.21258502150491132 valid 0.3385747369999687
LOSS train 0.21258502150491132 valid 0.33862011979535683
LOSS train 0.21258502150491132 valid 0.3391274168175094
LOSS train 0.21258502150491132 valid 0.3394170945340937
LOSS train 0.21258502150491132 valid 0.33973665356636046
LOSS train 0.21258502150491132 valid 0.3401156268497505
LOSS train 0.21258502150491132 valid 0.34022260062834797
LOSS train 0.21258502150491132 valid 0.34053159685968193
LOSS train 0.21258502150491132 valid 0.3408077178666225
LOSS train 0.21258502150491132 valid 0.3408130932421911
LOSS train 0.21258502150491132 valid 0.34116205067004796
LOSS train 0.21258502150491132 valid 0.3406486480592567
LOSS train 0.21258502150491132 valid 0.3405743616598624
LOSS train 0.21258502150491132 valid 0.34081239765937177
LOSS train 0.21258502150491132 valid 0.3407712402668866
LOSS train 0.21258502150491132 valid 0.340091235734321
LOSS train 0.21258502150491132 valid 0.3398229961416551
LOSS train 0.21258502150491132 valid 0.33974613367983725
LOSS train 0.21258502150491132 valid 0.3392334564736015
LOSS train 0.21258502150491132 valid 0.3397663098314534
LOSS train 0.21258502150491132 valid 0.339418077777172
LOSS train 0.21258502150491132 valid 0.3396110598348145
LOSS train 0.21258502150491132 valid 0.3396350016533318
LOSS train 0.21258502150491132 valid 0.3392865119361076
LOSS train 0.21258502150491132 valid 0.3389449827373028
LOSS train 0.21258502150491132 valid 0.3387705353665943
LOSS train 0.21258502150491132 valid 0.3385817799411836
LOSS train 0.21258502150491132 valid 0.3385731451879672
LOSS train 0.21258502150491132 valid 0.33898459711382467
LOSS train 0.21258502150491132 valid 0.33881901478767396
LOSS train 0.21258502150491132 valid 0.3387651750965724
LOSS train 0.21258502150491132 valid 0.33862360701786254
LOSS train 0.21258502150491132 valid 0.339220296125859
LOSS train 0.21258502150491132 valid 0.33923338781031526
LOSS train 0.21258502150491132 valid 0.3388984556381519
LOSS train 0.21258502150491132 valid 0.33881959523863464
LOSS train 0.21258502150491132 valid 0.3384932774034413
LOSS train 0.21258502150491132 valid 0.3384113504474324
LOSS train 0.21258502150491132 valid 0.3383561568473702
LOSS train 0.21258502150491132 valid 0.33794418374697366
LOSS train 0.21258502150491132 valid 0.33791083469986916
LOSS train 0.21258502150491132 valid 0.33774912596619044
LOSS train 0.21258502150491132 valid 0.3377981766842414
LOSS train 0.21258502150491132 valid 0.33772965708224895
LOSS train 0.21258502150491132 valid 0.33780530095100403
LOSS train 0.21258502150491132 valid 0.3378069333150877
LOSS train 0.21258502150491132 valid 0.33814172635615714
LOSS train 0.21258502150491132 valid 0.3380643381522252
LOSS train 0.21258502150491132 valid 0.337968572974205
LOSS train 0.21258502150491132 valid 0.33765798354971
LOSS train 0.21258502150491132 valid 0.3378014237913367
LOSS train 0.21258502150491132 valid 0.3376658619666586
LOSS train 0.21258502150491132 valid 0.33870229024339366
LOSS train 0.21258502150491132 valid 0.33868995348879155
LOSS train 0.21258502150491132 valid 0.3389530195792516
LOSS train 0.21258502150491132 valid 0.3389479247544775
LOSS train 0.21258502150491132 valid 0.3384432337786022
LOSS train 0.21258502150491132 valid 0.33865351244515063
LOSS train 0.21258502150491132 valid 0.33857291395013983
LOSS train 0.21258502150491132 valid 0.3388022445863293
LOSS train 0.21258502150491132 valid 0.33876250015619475
LOSS train 0.21258502150491132 valid 0.3386938999033278
LOSS train 0.21258502150491132 valid 0.3389720149055312
LOSS train 0.21258502150491132 valid 0.33892077263796105
LOSS train 0.21258502150491132 valid 0.3387962277978659
LOSS train 0.21258502150491132 valid 0.3387632257079486
LOSS train 0.21258502150491132 valid 0.3386038891327234
LOSS train 0.21258502150491132 valid 0.338432445847915
LOSS train 0.21258502150491132 valid 0.3379983976483345
LOSS train 0.21258502150491132 valid 0.3375914216041565
LOSS train 0.21258502150491132 valid 0.3376170405781413
LOSS train 0.21258502150491132 valid 0.3379995681925448
LOSS train 0.21258502150491132 valid 0.33764840396387236
LOSS train 0.21258502150491132 valid 0.3378279441912499
LOSS train 0.21258502150491132 valid 0.33789967579000135
LOSS train 0.21258502150491132 valid 0.33790222176334317
LOSS train 0.21258502150491132 valid 0.33759854785924737
LOSS train 0.21258502150491132 valid 0.33753572947028054
LOSS train 0.21258502150491132 valid 0.3375430125957248
LOSS train 0.21258502150491132 valid 0.33721007347106935
LOSS train 0.21258502150491132 valid 0.33732403306798503
LOSS train 0.21258502150491132 valid 0.3371251688501929
LOSS train 0.21258502150491132 valid 0.3372286189807935
LOSS train 0.21258502150491132 valid 0.337054689170262
LOSS train 0.21258502150491132 valid 0.33691801329453785
LOSS train 0.21258502150491132 valid 0.3371503381110028
LOSS train 0.21258502150491132 valid 0.3374924712128692
LOSS train 0.21258502150491132 valid 0.33749724053294283
LOSS train 0.21258502150491132 valid 0.337480618092029
LOSS train 0.21258502150491132 valid 0.33701532177022986
LOSS train 0.21258502150491132 valid 0.33698230100575316
LOSS train 0.21258502150491132 valid 0.3369155267980647
LOSS train 0.21258502150491132 valid 0.33693563462571896
LOSS train 0.21258502150491132 valid 0.3368030959966952
LOSS train 0.21258502150491132 valid 0.33698061249758066
LOSS train 0.21258502150491132 valid 0.3368343200671111
LOSS train 0.21258502150491132 valid 0.3369551934301853
LOSS train 0.21258502150491132 valid 0.33673783256599943
LOSS train 0.21258502150491132 valid 0.33660538847913446
LOSS train 0.21258502150491132 valid 0.3362949409545996
LOSS train 0.21258502150491132 valid 0.3360976252932938
LOSS train 0.21258502150491132 valid 0.33626259326329694
LOSS train 0.21258502150491132 valid 0.3358156128664209
LOSS train 0.21258502150491132 valid 0.33586561290463013
LOSS train 0.21258502150491132 valid 0.33586316898465157
LOSS train 0.21258502150491132 valid 0.3356465182790709
LOSS train 0.21258502150491132 valid 0.33543929619954366
LOSS train 0.21258502150491132 valid 0.3352831069178182
LOSS train 0.21258502150491132 valid 0.33518088010011937
LOSS train 0.21258502150491132 valid 0.33468723173548537
LOSS train 0.21258502150491132 valid 0.33487382335859595
LOSS train 0.21258502150491132 valid 0.3347928189281104
LOSS train 0.21258502150491132 valid 0.33449525421915144
LOSS train 0.21258502150491132 valid 0.3342927089005566
LOSS train 0.21258502150491132 valid 0.33443492828380494
LOSS train 0.21258502150491132 valid 0.33466544895657996
LOSS train 0.21258502150491132 valid 0.3346708080256885
LOSS train 0.21258502150491132 valid 0.33484097062981744
LOSS train 0.21258502150491132 valid 0.3348857435249837
LOSS train 0.21258502150491132 valid 0.3345502704381943
LOSS train 0.21258502150491132 valid 0.33467402357470105
LOSS train 0.21258502150491132 valid 0.3346742221974008
LOSS train 0.21258502150491132 valid 0.3346923070899937
LOSS train 0.21258502150491132 valid 0.33486698554258915
LOSS train 0.21258502150491132 valid 0.3348946868695996
LOSS train 0.21258502150491132 valid 0.334921882635328
LOSS train 0.21258502150491132 valid 0.3346859064322334
LOSS train 0.21258502150491132 valid 0.3348849292159615
LOSS train 0.21258502150491132 valid 0.33497420997758
LOSS train 0.21258502150491132 valid 0.33482904546790654
LOSS train 0.21258502150491132 valid 0.3349306539228532
LOSS train 0.21258502150491132 valid 0.3353468914257797
LOSS train 0.21258502150491132 valid 0.3354832686222436
LOSS train 0.21258502150491132 valid 0.33568108205451713
LOSS train 0.21258502150491132 valid 0.3358182753557744
LOSS train 0.21258502150491132 valid 0.33589231819559484
LOSS train 0.21258502150491132 valid 0.33587867824424955
LOSS train 0.21258502150491132 valid 0.33572220361028104
LOSS train 0.21258502150491132 valid 0.3356291132732334
LOSS train 0.21258502150491132 valid 0.33598264284590457
LOSS train 0.21258502150491132 valid 0.33571173294873563
LOSS train 0.21258502150491132 valid 0.3359427002537603
LOSS train 0.21258502150491132 valid 0.3359672628155276
LOSS train 0.21258502150491132 valid 0.33587148317981463
LOSS train 0.21258502150491132 valid 0.33581353618452947
LOSS train 0.21258502150491132 valid 0.33598233771769337
LOSS train 0.21258502150491132 valid 0.33572541946960877
LOSS train 0.21258502150491132 valid 0.3361424397171279
LOSS train 0.21258502150491132 valid 0.33673023988233236
LOSS train 0.21258502150491132 valid 0.3370520524224456
LOSS train 0.21258502150491132 valid 0.337115591921942
LOSS train 0.21258502150491132 valid 0.3371115302146688
LOSS train 0.21258502150491132 valid 0.3369688717948814
LOSS train 0.21258502150491132 valid 0.3368156617544741
LOSS train 0.21258502150491132 valid 0.33702673071622846
LOSS train 0.21258502150491132 valid 0.33718085093089784
LOSS train 0.21258502150491132 valid 0.3375054156141622
LOSS train 0.21258502150491132 valid 0.3375394386147322
LOSS train 0.21258502150491132 valid 0.33740133733495953
LOSS train 0.21258502150491132 valid 0.33743462837209887
LOSS train 0.21258502150491132 valid 0.3375838399515487
LOSS train 0.21258502150491132 valid 0.3373492249371013
LOSS train 0.21258502150491132 valid 0.3374880407323209
LOSS train 0.21258502150491132 valid 0.33750506090611565
LOSS train 0.21258502150491132 valid 0.3374408141351663
LOSS train 0.21258502150491132 valid 0.3375216806414484
LOSS train 0.21258502150491132 valid 0.3375932201743126
LOSS train 0.21258502150491132 valid 0.3377470944657525
LOSS train 0.21258502150491132 valid 0.33771548753209185
LOSS train 0.21258502150491132 valid 0.33760655348031027
LOSS train 0.21258502150491132 valid 0.33779823225467726
LOSS train 0.21258502150491132 valid 0.33798909684022266
LOSS train 0.21258502150491132 valid 0.3381907877081366
LOSS train 0.21258502150491132 valid 0.33815699389211307
LOSS train 0.21258502150491132 valid 0.3379775814987995
LOSS train 0.21258502150491132 valid 0.338070003034004
LOSS train 0.21258502150491132 valid 0.3384301233488847
LOSS train 0.21258502150491132 valid 0.33851327274963533
LOSS train 0.21258502150491132 valid 0.33842919228503304
LOSS train 0.21258502150491132 valid 0.3382729764960029
LOSS train 0.21258502150491132 valid 0.33812402377741924
LOSS train 0.21258502150491132 valid 0.3379494079291175
LOSS train 0.21258502150491132 valid 0.3377159815147626
LOSS train 0.21258502150491132 valid 0.3377183158231038
LOSS train 0.21258502150491132 valid 0.3376686980149576
LOSS train 0.21258502150491132 valid 0.33756293398841847
LOSS train 0.21258502150491132 valid 0.3373381732731846
LOSS train 0.21258502150491132 valid 0.33724817965350806
LOSS train 0.21258502150491132 valid 0.33739308262584916
LOSS train 0.21258502150491132 valid 0.33754874420793435
LOSS train 0.21258502150491132 valid 0.33744062134227554
LOSS train 0.21258502150491132 valid 0.33736091535473534
LOSS train 0.21258502150491132 valid 0.33730393508449197
LOSS train 0.21258502150491132 valid 0.3374373753487445
LOSS train 0.21258502150491132 valid 0.3375209624911177
LOSS train 0.21258502150491132 valid 0.3374679185373267
LOSS train 0.21258502150491132 valid 0.3375938415833532
LOSS train 0.21258502150491132 valid 0.33751014765008724
LOSS train 0.21258502150491132 valid 0.33763110085206777
LOSS train 0.21258502150491132 valid 0.3375856265678244
LOSS train 0.21258502150491132 valid 0.3374752244735892
LOSS train 0.21258502150491132 valid 0.33749272420952214
LOSS train 0.21258502150491132 valid 0.33743594971879215
LOSS train 0.21258502150491132 valid 0.337452871295122
LOSS train 0.21258502150491132 valid 0.33736338570714
LOSS train 0.21258502150491132 valid 0.3372986094401128
LOSS train 0.21258502150491132 valid 0.3373656864493888
LOSS train 0.21258502150491132 valid 0.33746874996341103
LOSS train 0.21258502150491132 valid 0.33747760397627163
LOSS train 0.21258502150491132 valid 0.3373150020837784
LOSS train 0.21258502150491132 valid 0.33745105530506647
LOSS train 0.21258502150491132 valid 0.3373763021991773
LOSS train 0.21258502150491132 valid 0.3372028541158546
LOSS train 0.21258502150491132 valid 0.3370868879518077
LOSS train 0.21258502150491132 valid 0.33714503717999306
LOSS train 0.21258502150491132 valid 0.3370929436572495
LOSS train 0.21258502150491132 valid 0.33700818124298865
LOSS train 0.21258502150491132 valid 0.3368845640565641
LOSS train 0.21258502150491132 valid 0.3369993685631995
LOSS train 0.21258502150491132 valid 0.33693394126400117
LOSS train 0.21258502150491132 valid 0.336829930496744
LOSS train 0.21258502150491132 valid 0.3369543265187026
LOSS train 0.21258502150491132 valid 0.33707331819166925
LOSS train 0.21258502150491132 valid 0.3371835267375629
LOSS train 0.21258502150491132 valid 0.3371062864083797
LOSS train 0.21258502150491132 valid 0.33744809964549877
LOSS train 0.21258502150491132 valid 0.3372588014658193
LOSS train 0.21258502150491132 valid 0.33701877932614954
LOSS train 0.21258502150491132 valid 0.3370376598595837
LOSS train 0.21258502150491132 valid 0.33714266442335566
LOSS train 0.21258502150491132 valid 0.33719389693685836
LOSS train 0.21258502150491132 valid 0.33723121658195415
LOSS train 0.21258502150491132 valid 0.3372931959698113
LOSS train 0.21258502150491132 valid 0.3374058451605423
LOSS train 0.21258502150491132 valid 0.33745973593357836
LOSS train 0.21258502150491132 valid 0.3373708825367095
LOSS train 0.21258502150491132 valid 0.3372366717870695
LOSS train 0.21258502150491132 valid 0.33724224249700885
LOSS train 0.21258502150491132 valid 0.3373774808948625
LOSS train 0.21258502150491132 valid 0.33725440052907857
LOSS train 0.21258502150491132 valid 0.33700633310668526
LOSS train 0.21258502150491132 valid 0.33702083577564984
LOSS train 0.21258502150491132 valid 0.3368651644277149
LOSS train 0.21258502150491132 valid 0.3367780811403353
LOSS train 0.21258502150491132 valid 0.3367824282278033
LOSS train 0.21258502150491132 valid 0.33661925403778153
LOSS train 0.21258502150491132 valid 0.3365695101388714
LOSS train 0.21258502150491132 valid 0.3364579711902246
LOSS train 0.21258502150491132 valid 0.33664592299177204
LOSS train 0.21258502150491132 valid 0.3372380598299745
LOSS train 0.21258502150491132 valid 0.33712855511145784
LOSS train 0.21258502150491132 valid 0.3369102741266877
LOSS train 0.21258502150491132 valid 0.33682333827875127
LOSS train 0.21258502150491132 valid 0.3368240906123104
LOSS train 0.21258502150491132 valid 0.33671650286231725
LOSS train 0.21258502150491132 valid 0.33667724980757785
LOSS train 0.21258502150491132 valid 0.33668115324425424
LOSS train 0.21258502150491132 valid 0.3366164259886944
LOSS train 0.21258502150491132 valid 0.3366543557600113
LOSS train 0.21258502150491132 valid 0.3367808183733846
LOSS train 0.21258502150491132 valid 0.33696461824721163
LOSS train 0.21258502150491132 valid 0.3370151713031347
LOSS train 0.21258502150491132 valid 0.33695451819530414
LOSS train 0.21258502150491132 valid 0.3369875804710521
LOSS train 0.21258502150491132 valid 0.3369025436954366
LOSS train 0.21258502150491132 valid 0.3368631780890547
LOSS train 0.21258502150491132 valid 0.3371257885302628
LOSS train 0.21258502150491132 valid 0.33740922673182056
LOSS train 0.21258502150491132 valid 0.3374200129656346
LOSS train 0.21258502150491132 valid 0.33751930582196743
LOSS train 0.21258502150491132 valid 0.3374133896225137
LOSS train 0.21258502150491132 valid 0.3372443866908388
LOSS train 0.21258502150491132 valid 0.33727612593413697
LOSS train 0.21258502150491132 valid 0.33747732118377843
EPOCH 29:
  batch 1 loss: 0.2399105429649353
  batch 2 loss: 0.22400452196598053
  batch 3 loss: 0.2111679663260778
  batch 4 loss: 0.21114374324679375
  batch 5 loss: 0.21484932005405427
  batch 6 loss: 0.21044452488422394
  batch 7 loss: 0.2138770478112357
  batch 8 loss: 0.2182244248688221
  batch 9 loss: 0.2181417809592353
  batch 10 loss: 0.21959597617387772
  batch 11 loss: 0.2187972989949313
  batch 12 loss: 0.21633433426419893
  batch 13 loss: 0.21734498670467964
  batch 14 loss: 0.21915869627680098
  batch 15 loss: 0.21928616364796957
  batch 16 loss: 0.21846929378807545
  batch 17 loss: 0.21580833196640015
  batch 18 loss: 0.21633068223794302
  batch 19 loss: 0.2152982511018452
  batch 20 loss: 0.21266358643770217
  batch 21 loss: 0.2122742902664911
  batch 22 loss: 0.21142170578241348
  batch 23 loss: 0.21086345353852148
  batch 24 loss: 0.20904756461580595
  batch 25 loss: 0.2103124278783798
  batch 26 loss: 0.20895119240650764
  batch 27 loss: 0.20950468822761817
  batch 28 loss: 0.20825391582080297
  batch 29 loss: 0.20973910545480662
  batch 30 loss: 0.20922412226597467
  batch 31 loss: 0.20995350374329474
  batch 32 loss: 0.20955610275268555
  batch 33 loss: 0.20994653168952826
  batch 34 loss: 0.20910751600475871
  batch 35 loss: 0.2087407044001988
  batch 36 loss: 0.20944619675477347
  batch 37 loss: 0.2098875404209704
  batch 38 loss: 0.21007723282826574
  batch 39 loss: 0.21048621221994743
  batch 40 loss: 0.21121810227632523
  batch 41 loss: 0.21113886120842723
  batch 42 loss: 0.21077053674629756
  batch 43 loss: 0.21127163393552914
  batch 44 loss: 0.2109517011452805
  batch 45 loss: 0.21071589257982043
  batch 46 loss: 0.21192897791447846
  batch 47 loss: 0.2112470364316981
  batch 48 loss: 0.21042237399766842
  batch 49 loss: 0.2105214239991441
  batch 50 loss: 0.21017559200525285
  batch 51 loss: 0.2105995337752735
  batch 52 loss: 0.21116492639367396
  batch 53 loss: 0.2110133024881471
  batch 54 loss: 0.21121899397284896
  batch 55 loss: 0.2106746795502576
  batch 56 loss: 0.21039396072072641
  batch 57 loss: 0.21029518492389143
  batch 58 loss: 0.21099544676213428
  batch 59 loss: 0.2109330331875106
  batch 60 loss: 0.21081723794341087
  batch 61 loss: 0.21121951620109747
  batch 62 loss: 0.21141706286899506
  batch 63 loss: 0.2112775986629819
  batch 64 loss: 0.2115627694875002
  batch 65 loss: 0.2114403234078334
  batch 66 loss: 0.2113831898931301
  batch 67 loss: 0.2119310449308424
  batch 68 loss: 0.2122789244441425
  batch 69 loss: 0.21250292421251105
  batch 70 loss: 0.21301058254071645
  batch 71 loss: 0.21279425667205326
  batch 72 loss: 0.21248254531787503
  batch 73 loss: 0.21230987857465874
  batch 74 loss: 0.21196182293666377
  batch 75 loss: 0.21178411463896432
  batch 76 loss: 0.21240871497674993
  batch 77 loss: 0.21167552142174212
  batch 78 loss: 0.21201355506976446
  batch 79 loss: 0.21198684882514085
  batch 80 loss: 0.2115623239427805
  batch 81 loss: 0.21172572525195132
  batch 82 loss: 0.21197839971722626
  batch 83 loss: 0.21202600594744625
  batch 84 loss: 0.21162864867420422
  batch 85 loss: 0.21138275037793552
  batch 86 loss: 0.21205896641625915
  batch 87 loss: 0.2118506895742197
  batch 88 loss: 0.21146166900342162
  batch 89 loss: 0.21153710296984468
  batch 90 loss: 0.21151247604025736
  batch 91 loss: 0.21141910765852248
  batch 92 loss: 0.21107822381283925
  batch 93 loss: 0.21093883789995665
  batch 94 loss: 0.21101923556403912
  batch 95 loss: 0.21062119901180268
  batch 96 loss: 0.2104648744376997
  batch 97 loss: 0.21040794760296025
  batch 98 loss: 0.21079995756854816
  batch 99 loss: 0.21103489534421402
  batch 100 loss: 0.21057897701859474
  batch 101 loss: 0.2103606899185936
  batch 102 loss: 0.21068074510377996
  batch 103 loss: 0.2110039936107339
  batch 104 loss: 0.21105306982420957
  batch 105 loss: 0.21093682178429196
  batch 106 loss: 0.21102417415043093
  batch 107 loss: 0.2108894491864142
  batch 108 loss: 0.2110638886138245
  batch 109 loss: 0.21113020072289562
  batch 110 loss: 0.21134302480654282
  batch 111 loss: 0.21147670348485312
  batch 112 loss: 0.21136350690254144
  batch 113 loss: 0.21135076049682314
  batch 114 loss: 0.2115992817439531
  batch 115 loss: 0.21187885377718055
  batch 116 loss: 0.21221362568181137
  batch 117 loss: 0.212420744009507
  batch 118 loss: 0.2123648227523949
  batch 119 loss: 0.21236792169198268
  batch 120 loss: 0.21176859475672244
  batch 121 loss: 0.2116448407577089
  batch 122 loss: 0.21147247110722495
  batch 123 loss: 0.21133873787352708
  batch 124 loss: 0.21135954318508024
  batch 125 loss: 0.21123715353012085
  batch 126 loss: 0.21114603110722133
  batch 127 loss: 0.21141054440201737
  batch 128 loss: 0.21108705224469304
  batch 129 loss: 0.21127753029035967
  batch 130 loss: 0.21125678569078446
  batch 131 loss: 0.21119168548638584
  batch 132 loss: 0.2109423673739939
  batch 133 loss: 0.2112766754134257
  batch 134 loss: 0.21140469043557322
  batch 135 loss: 0.21119593414995405
  batch 136 loss: 0.21136879592257388
  batch 137 loss: 0.21138933213957906
  batch 138 loss: 0.21135229887305826
  batch 139 loss: 0.2115586006598507
  batch 140 loss: 0.21152608426553862
  batch 141 loss: 0.21185637756865075
  batch 142 loss: 0.21181133085153472
  batch 143 loss: 0.2115231671116569
  batch 144 loss: 0.2115152394399047
  batch 145 loss: 0.21159913940676328
  batch 146 loss: 0.2116238874931858
  batch 147 loss: 0.21181040677894541
  batch 148 loss: 0.21191627995387927
  batch 149 loss: 0.21168356453812362
  batch 150 loss: 0.21172162701686223
  batch 151 loss: 0.2118106588030493
  batch 152 loss: 0.2121740637445136
  batch 153 loss: 0.21202247760264703
  batch 154 loss: 0.21219307987333894
  batch 155 loss: 0.21206541936243734
  batch 156 loss: 0.2120408028937303
  batch 157 loss: 0.21222775832862611
  batch 158 loss: 0.21213126031658316
  batch 159 loss: 0.21229553316374244
  batch 160 loss: 0.21204922515898944
  batch 161 loss: 0.21204869430627882
  batch 162 loss: 0.21185099250740475
  batch 163 loss: 0.21199404480267156
  batch 164 loss: 0.211758747878598
  batch 165 loss: 0.21173462000760165
  batch 166 loss: 0.21168772624917778
  batch 167 loss: 0.21157455533564448
  batch 168 loss: 0.21141824738255569
  batch 169 loss: 0.21124070009888982
  batch 170 loss: 0.21110239353250054
  batch 171 loss: 0.2109732887549707
  batch 172 loss: 0.21082119591707407
  batch 173 loss: 0.21070224830525458
  batch 174 loss: 0.21067466766669832
  batch 175 loss: 0.21064113114561353
  batch 176 loss: 0.2104566791518168
  batch 177 loss: 0.21042435738326465
  batch 178 loss: 0.21058569840166005
  batch 179 loss: 0.21042687791352832
  batch 180 loss: 0.2101967869533433
  batch 181 loss: 0.2101883740879554
  batch 182 loss: 0.2101649890070433
  batch 183 loss: 0.209947835226528
  batch 184 loss: 0.20985902627201183
  batch 185 loss: 0.20992985383884327
  batch 186 loss: 0.2101098350299302
  batch 187 loss: 0.21001557583477407
  batch 188 loss: 0.20967433569913216
  batch 189 loss: 0.20950615642562745
  batch 190 loss: 0.20959713913892444
  batch 191 loss: 0.20977321337342886
  batch 192 loss: 0.2097077697981149
  batch 193 loss: 0.20964397914669056
  batch 194 loss: 0.20973358695040045
  batch 195 loss: 0.20976840295852758
  batch 196 loss: 0.20977097833339048
  batch 197 loss: 0.20972049221169525
  batch 198 loss: 0.20989326169394484
  batch 199 loss: 0.20966492847881124
  batch 200 loss: 0.20984536550939084
  batch 201 loss: 0.20999906267692794
  batch 202 loss: 0.21021242144674357
  batch 203 loss: 0.21010902039523194
  batch 204 loss: 0.21001278682082308
  batch 205 loss: 0.21026345462333865
  batch 206 loss: 0.21041481652595465
  batch 207 loss: 0.21066655880875057
  batch 208 loss: 0.21081065558470213
  batch 209 loss: 0.21073160587885734
  batch 210 loss: 0.21088889624391283
  batch 211 loss: 0.21079141579533076
  batch 212 loss: 0.21083172937890268
  batch 213 loss: 0.21076808527042049
  batch 214 loss: 0.2107659246876975
  batch 215 loss: 0.21067427576974382
  batch 216 loss: 0.21044239670866066
  batch 217 loss: 0.2103363901788738
  batch 218 loss: 0.2103518072618257
  batch 219 loss: 0.21042727226536023
  batch 220 loss: 0.2102898687801578
  batch 221 loss: 0.21029300771957068
  batch 222 loss: 0.2103375552205352
  batch 223 loss: 0.21031691715321732
  batch 224 loss: 0.21029159879045828
  batch 225 loss: 0.21023356974124907
  batch 226 loss: 0.210391157694095
  batch 227 loss: 0.21030613327078881
  batch 228 loss: 0.21032061467045232
  batch 229 loss: 0.21028325729495054
  batch 230 loss: 0.21051679434983628
  batch 231 loss: 0.2104800929238786
  batch 232 loss: 0.2104220000448926
  batch 233 loss: 0.21044349152386957
  batch 234 loss: 0.21046578457467577
  batch 235 loss: 0.2104718418197429
  batch 236 loss: 0.21042956803309715
  batch 237 loss: 0.2105109250243706
  batch 238 loss: 0.21047774314129053
  batch 239 loss: 0.21031020531843897
  batch 240 loss: 0.21019389207164446
  batch 241 loss: 0.21026946424943282
  batch 242 loss: 0.2100773427850944
  batch 243 loss: 0.20996126121699565
  batch 244 loss: 0.20991770214721805
  batch 245 loss: 0.20983653396976237
  batch 246 loss: 0.2097754451196368
  batch 247 loss: 0.20985571762569522
  batch 248 loss: 0.2098829338867818
  batch 249 loss: 0.20986856262846645
  batch 250 loss: 0.2097403153181076
  batch 251 loss: 0.2097148222633567
  batch 252 loss: 0.20968343187419194
  batch 253 loss: 0.20970911529695563
  batch 254 loss: 0.20965008472833108
  batch 255 loss: 0.20965409337305554
  batch 256 loss: 0.20986524689942598
  batch 257 loss: 0.20996976710479084
  batch 258 loss: 0.20985828946496166
  batch 259 loss: 0.20995144638085458
  batch 260 loss: 0.209954621642828
  batch 261 loss: 0.20998036261947675
  batch 262 loss: 0.20996218424944477
  batch 263 loss: 0.21002157451761994
  batch 264 loss: 0.20993648870199016
  batch 265 loss: 0.20983556300963996
  batch 266 loss: 0.20984239344085967
  batch 267 loss: 0.20988669902197876
  batch 268 loss: 0.2098237388391993
  batch 269 loss: 0.20984337653369264
  batch 270 loss: 0.21007139649656084
  batch 271 loss: 0.21012794053202627
  batch 272 loss: 0.21012472169583335
  batch 273 loss: 0.21006599074099963
  batch 274 loss: 0.21022213098123996
  batch 275 loss: 0.21025980911471626
  batch 276 loss: 0.21025689065024472
  batch 277 loss: 0.21024707304011184
  batch 278 loss: 0.21032018884480427
  batch 279 loss: 0.2103843544760058
  batch 280 loss: 0.21021526509097643
  batch 281 loss: 0.21014870814581357
  batch 282 loss: 0.21020974055038277
  batch 283 loss: 0.2101018690494261
  batch 284 loss: 0.21019254661571812
  batch 285 loss: 0.210199415526892
  batch 286 loss: 0.21015412464633687
  batch 287 loss: 0.2101808344030214
  batch 288 loss: 0.21009354727963606
  batch 289 loss: 0.21019658616463618
  batch 290 loss: 0.21010465770959855
  batch 291 loss: 0.21016797341431948
  batch 292 loss: 0.21020054429361265
  batch 293 loss: 0.21012440461144105
  batch 294 loss: 0.21013993810431486
  batch 295 loss: 0.21015027044183116
  batch 296 loss: 0.2103575998665513
  batch 297 loss: 0.21035293125945712
  batch 298 loss: 0.21033393191611208
  batch 299 loss: 0.21032922054612915
  batch 300 loss: 0.21041122635205586
  batch 301 loss: 0.21049394337243812
  batch 302 loss: 0.2104749113123938
  batch 303 loss: 0.21038330727481214
  batch 304 loss: 0.21040702635716452
  batch 305 loss: 0.2102806678072351
  batch 306 loss: 0.21042121844548806
  batch 307 loss: 0.21046876344308013
  batch 308 loss: 0.2105126025324518
  batch 309 loss: 0.21039019780637377
  batch 310 loss: 0.2102833829579815
  batch 311 loss: 0.2105152664460553
  batch 312 loss: 0.21056592349822706
  batch 313 loss: 0.21063868591960627
  batch 314 loss: 0.21061987429857254
  batch 315 loss: 0.2105975274055723
  batch 316 loss: 0.21056334485736075
  batch 317 loss: 0.2105152954814562
  batch 318 loss: 0.21050798058884698
  batch 319 loss: 0.21046260587847718
  batch 320 loss: 0.21042807144112885
  batch 321 loss: 0.21047247324220117
  batch 322 loss: 0.2104900611205871
  batch 323 loss: 0.2104761746276643
  batch 324 loss: 0.2104098657950943
  batch 325 loss: 0.21030331258590404
  batch 326 loss: 0.2102874351973914
  batch 327 loss: 0.2102159681396747
  batch 328 loss: 0.21002287367676817
  batch 329 loss: 0.2099570137389163
  batch 330 loss: 0.20983196289250344
  batch 331 loss: 0.2097604611939174
  batch 332 loss: 0.20963348205908236
  batch 333 loss: 0.209911437184961
  batch 334 loss: 0.20990765433825417
  batch 335 loss: 0.20977570196585868
  batch 336 loss: 0.2096708050618569
  batch 337 loss: 0.20965887186081544
  batch 338 loss: 0.20968470819426713
  batch 339 loss: 0.20978236928098673
  batch 340 loss: 0.2098941759151571
  batch 341 loss: 0.20994461788809543
  batch 342 loss: 0.20991591686568065
  batch 343 loss: 0.21004463471580873
  batch 344 loss: 0.21011207378361113
  batch 345 loss: 0.210248957293621
  batch 346 loss: 0.21024352368075033
  batch 347 loss: 0.2102162430025315
  batch 348 loss: 0.2102658376097679
  batch 349 loss: 0.21026669084001065
  batch 350 loss: 0.210400193801948
  batch 351 loss: 0.2103943039803763
  batch 352 loss: 0.21045150624757464
  batch 353 loss: 0.21044865073293195
  batch 354 loss: 0.21062676600143734
  batch 355 loss: 0.2105448649382927
  batch 356 loss: 0.21051115019435293
  batch 357 loss: 0.21050717946695013
  batch 358 loss: 0.21062923147858187
  batch 359 loss: 0.2106192485120635
  batch 360 loss: 0.21063954246540864
  batch 361 loss: 0.21064984947030232
  batch 362 loss: 0.2107004982249513
  batch 363 loss: 0.21063509928293464
  batch 364 loss: 0.21048853689661393
  batch 365 loss: 0.210502222792743
  batch 366 loss: 0.21049624714043622
  batch 367 loss: 0.21047073980280748
  batch 368 loss: 0.21038949813531793
  batch 369 loss: 0.21031624010100275
  batch 370 loss: 0.21022565328591578
  batch 371 loss: 0.2102728193660952
  batch 372 loss: 0.21024876969155445
  batch 373 loss: 0.21012713770124933
  batch 374 loss: 0.209972883848583
  batch 375 loss: 0.20990675671895345
  batch 376 loss: 0.20981809112461322
  batch 377 loss: 0.20978567031871734
  batch 378 loss: 0.2097312595951494
  batch 379 loss: 0.2096853713089684
  batch 380 loss: 0.20973170042822237
  batch 381 loss: 0.20969307309217028
  batch 382 loss: 0.2096463221451999
  batch 383 loss: 0.20970172709335547
  batch 384 loss: 0.2096846104056264
  batch 385 loss: 0.20979331758889286
  batch 386 loss: 0.20976407165817645
  batch 387 loss: 0.2098586151535197
  batch 388 loss: 0.20990330576128566
  batch 389 loss: 0.20990404877380725
  batch 390 loss: 0.20989336459300456
  batch 391 loss: 0.20993710475047225
  batch 392 loss: 0.20992117143255107
  batch 393 loss: 0.2098476825703798
  batch 394 loss: 0.2098379622634292
  batch 395 loss: 0.20976711492749708
  batch 396 loss: 0.20963726335703725
  batch 397 loss: 0.20960210875539995
  batch 398 loss: 0.20951171077076514
  batch 399 loss: 0.20959365166219554
  batch 400 loss: 0.2097001687437296
  batch 401 loss: 0.20957547519123762
  batch 402 loss: 0.20964211937206895
  batch 403 loss: 0.2096951477063205
  batch 404 loss: 0.20968658255763573
  batch 405 loss: 0.20972510947857373
  batch 406 loss: 0.20977830791414664
  batch 407 loss: 0.20982638206470217
  batch 408 loss: 0.2099000748392998
  batch 409 loss: 0.20993515918336464
  batch 410 loss: 0.21003336677464043
  batch 411 loss: 0.21007021761288608
  batch 412 loss: 0.2101158868268277
  batch 413 loss: 0.21018868100267923
  batch 414 loss: 0.21025404110910811
  batch 415 loss: 0.21029739595321287
  batch 416 loss: 0.21041458680366093
  batch 417 loss: 0.2104256906383615
  batch 418 loss: 0.2104387736776799
  batch 419 loss: 0.21053510577126733
  batch 420 loss: 0.21045871249267034
  batch 421 loss: 0.21037081683899614
  batch 422 loss: 0.2105409747333888
  batch 423 loss: 0.21051675401821768
  batch 424 loss: 0.21048858803960513
  batch 425 loss: 0.21040517519502078
  batch 426 loss: 0.21033306181990485
  batch 427 loss: 0.21032259077601467
  batch 428 loss: 0.21033540813722343
  batch 429 loss: 0.21037656485617579
  batch 430 loss: 0.21035777808621872
  batch 431 loss: 0.2103377204483457
  batch 432 loss: 0.21029419203599295
  batch 433 loss: 0.21039667257535816
  batch 434 loss: 0.21053562563959904
  batch 435 loss: 0.2105369658990838
  batch 436 loss: 0.21050987860888515
  batch 437 loss: 0.21057224849950804
  batch 438 loss: 0.21067457510047852
  batch 439 loss: 0.21069438840370786
  batch 440 loss: 0.21076381799172272
  batch 441 loss: 0.21072910182059756
  batch 442 loss: 0.21066157981685923
  batch 443 loss: 0.21069024173589107
  batch 444 loss: 0.21061854470554772
  batch 445 loss: 0.210608286636599
  batch 446 loss: 0.21053341201468967
  batch 447 loss: 0.21051457047595806
  batch 448 loss: 0.2105395776286189
  batch 449 loss: 0.21055295588842746
  batch 450 loss: 0.21051714085870318
  batch 451 loss: 0.21044917490556342
  batch 452 loss: 0.21058619019072666
  batch 453 loss: 0.2106320130048735
  batch 454 loss: 0.2106541879967446
  batch 455 loss: 0.2106431795673056
  batch 456 loss: 0.21070689574014723
  batch 457 loss: 0.21069477270285983
  batch 458 loss: 0.21064195254874543
  batch 459 loss: 0.21072513037127866
  batch 460 loss: 0.210760563093683
  batch 461 loss: 0.2107301997323874
  batch 462 loss: 0.21071865109654217
  batch 463 loss: 0.21065781119060312
  batch 464 loss: 0.21074253047720112
  batch 465 loss: 0.21066280920659342
  batch 466 loss: 0.21061227822380515
  batch 467 loss: 0.21069206125092965
  batch 468 loss: 0.2106603618360992
  batch 469 loss: 0.21070478351385608
  batch 470 loss: 0.2106438066731108
  batch 471 loss: 0.21062391483859652
  batch 472 loss: 0.21043583113005607
LOSS train 0.21043583113005607 valid 0.2718096971511841
LOSS train 0.21043583113005607 valid 0.27045704424381256
LOSS train 0.21043583113005607 valid 0.2671528955300649
LOSS train 0.21043583113005607 valid 0.27536553144454956
LOSS train 0.21043583113005607 valid 0.26998289227485656
LOSS train 0.21043583113005607 valid 0.27310777207215625
LOSS train 0.21043583113005607 valid 0.2855472947869982
LOSS train 0.21043583113005607 valid 0.28551458939909935
LOSS train 0.21043583113005607 valid 0.2857437829176585
LOSS train 0.21043583113005607 valid 0.2879246711730957
LOSS train 0.21043583113005607 valid 0.2862299951640042
LOSS train 0.21043583113005607 valid 0.28424643476804096
LOSS train 0.21043583113005607 valid 0.2840203115573296
LOSS train 0.21043583113005607 valid 0.28398903139999937
LOSS train 0.21043583113005607 valid 0.27775844832261404
LOSS train 0.21043583113005607 valid 0.2802710561081767
LOSS train 0.21043583113005607 valid 0.2818430846228319
LOSS train 0.21043583113005607 valid 0.2831771994630496
LOSS train 0.21043583113005607 valid 0.28636172335398824
LOSS train 0.21043583113005607 valid 0.28613393381237984
LOSS train 0.21043583113005607 valid 0.28468456793399083
LOSS train 0.21043583113005607 valid 0.2830782810395414
LOSS train 0.21043583113005607 valid 0.28357288954050647
LOSS train 0.21043583113005607 valid 0.2823389812062184
LOSS train 0.21043583113005607 valid 0.2804651290178299
LOSS train 0.21043583113005607 valid 0.2802436495056519
LOSS train 0.21043583113005607 valid 0.28083494912695
LOSS train 0.21043583113005607 valid 0.28078117594122887
LOSS train 0.21043583113005607 valid 0.2809020892299455
LOSS train 0.21043583113005607 valid 0.2816090926527977
LOSS train 0.21043583113005607 valid 0.2837665220422129
LOSS train 0.21043583113005607 valid 0.28316815523430705
LOSS train 0.21043583113005607 valid 0.28442835040164716
LOSS train 0.21043583113005607 valid 0.2844477685935357
LOSS train 0.21043583113005607 valid 0.2867259404488972
LOSS train 0.21043583113005607 valid 0.2861037457154857
LOSS train 0.21043583113005607 valid 0.28545302273453893
LOSS train 0.21043583113005607 valid 0.2863852652279954
LOSS train 0.21043583113005607 valid 0.286232074483847
LOSS train 0.21043583113005607 valid 0.28579313345253465
LOSS train 0.21043583113005607 valid 0.28701209113365267
LOSS train 0.21043583113005607 valid 0.2875627048668407
LOSS train 0.21043583113005607 valid 0.28716864662114966
LOSS train 0.21043583113005607 valid 0.28765242445197975
LOSS train 0.21043583113005607 valid 0.28692914015716975
LOSS train 0.21043583113005607 valid 0.2878002657190613
LOSS train 0.21043583113005607 valid 0.2888702613876221
LOSS train 0.21043583113005607 valid 0.28862121359755594
LOSS train 0.21043583113005607 valid 0.2893083354040068
LOSS train 0.21043583113005607 valid 0.28789701372385024
LOSS train 0.21043583113005607 valid 0.287670357262387
LOSS train 0.21043583113005607 valid 0.2880286078613538
LOSS train 0.21043583113005607 valid 0.28819130529772563
LOSS train 0.21043583113005607 valid 0.2882613851516335
LOSS train 0.21043583113005607 valid 0.2881597564979033
LOSS train 0.21043583113005607 valid 0.2876205446996859
LOSS train 0.21043583113005607 valid 0.2872274358544433
LOSS train 0.21043583113005607 valid 0.2869816553489915
LOSS train 0.21043583113005607 valid 0.2875422034728325
LOSS train 0.21043583113005607 valid 0.28745047425230347
LOSS train 0.21043583113005607 valid 0.28672445675388714
LOSS train 0.21043583113005607 valid 0.28813077052754743
LOSS train 0.21043583113005607 valid 0.28852619230747223
LOSS train 0.21043583113005607 valid 0.2898186712991446
LOSS train 0.21043583113005607 valid 0.2906475890141267
LOSS train 0.21043583113005607 valid 0.2906709595611601
LOSS train 0.21043583113005607 valid 0.28980433562798286
LOSS train 0.21043583113005607 valid 0.2898501624517581
LOSS train 0.21043583113005607 valid 0.2887321801289268
LOSS train 0.21043583113005607 valid 0.28909208008221216
LOSS train 0.21043583113005607 valid 0.28905316389782326
LOSS train 0.21043583113005607 valid 0.28908943550454247
LOSS train 0.21043583113005607 valid 0.28888331537377343
LOSS train 0.21043583113005607 valid 0.2888378645922687
LOSS train 0.21043583113005607 valid 0.28882456143697105
LOSS train 0.21043583113005607 valid 0.2888999016661393
LOSS train 0.21043583113005607 valid 0.28899682071301847
LOSS train 0.21043583113005607 valid 0.28936277406337935
LOSS train 0.21043583113005607 valid 0.2897564534899555
LOSS train 0.21043583113005607 valid 0.2886953044682741
LOSS train 0.21043583113005607 valid 0.2876538733641307
LOSS train 0.21043583113005607 valid 0.28849663894350935
LOSS train 0.21043583113005607 valid 0.2882132731288312
LOSS train 0.21043583113005607 valid 0.2878254765555972
LOSS train 0.21043583113005607 valid 0.2874816473792581
LOSS train 0.21043583113005607 valid 0.286888150628223
LOSS train 0.21043583113005607 valid 0.28680689855553637
LOSS train 0.21043583113005607 valid 0.2860948538238352
LOSS train 0.21043583113005607 valid 0.28682425819086227
LOSS train 0.21043583113005607 valid 0.28703171412150064
LOSS train 0.21043583113005607 valid 0.2871324272601159
LOSS train 0.21043583113005607 valid 0.28721444989028183
LOSS train 0.21043583113005607 valid 0.28689878409908665
LOSS train 0.21043583113005607 valid 0.28711829477168144
LOSS train 0.21043583113005607 valid 0.2867684703124197
LOSS train 0.21043583113005607 valid 0.2875009002164006
LOSS train 0.21043583113005607 valid 0.28752589748077784
LOSS train 0.21043583113005607 valid 0.28805582711891253
LOSS train 0.21043583113005607 valid 0.2884293445433029
LOSS train 0.21043583113005607 valid 0.2888099232316017
LOSS train 0.21043583113005607 valid 0.2891734469645094
LOSS train 0.21043583113005607 valid 0.28926311053481757
LOSS train 0.21043583113005607 valid 0.28951089214352727
LOSS train 0.21043583113005607 valid 0.2897928082025968
LOSS train 0.21043583113005607 valid 0.2897131607646034
LOSS train 0.21043583113005607 valid 0.29008163455522284
LOSS train 0.21043583113005607 valid 0.2896650305418211
LOSS train 0.21043583113005607 valid 0.28960019129293935
LOSS train 0.21043583113005607 valid 0.28983914988850235
LOSS train 0.21043583113005607 valid 0.2900025142864747
LOSS train 0.21043583113005607 valid 0.28925673462249135
LOSS train 0.21043583113005607 valid 0.2890494713293655
LOSS train 0.21043583113005607 valid 0.28881401799421397
LOSS train 0.21043583113005607 valid 0.2884539227213776
LOSS train 0.21043583113005607 valid 0.28877845976663674
LOSS train 0.21043583113005607 valid 0.2887031423120663
LOSS train 0.21043583113005607 valid 0.2888298121273008
LOSS train 0.21043583113005607 valid 0.28882033314745303
LOSS train 0.21043583113005607 valid 0.2885191798711023
LOSS train 0.21043583113005607 valid 0.28807434216141703
LOSS train 0.21043583113005607 valid 0.28800030188126996
LOSS train 0.21043583113005607 valid 0.2878175793124027
LOSS train 0.21043583113005607 valid 0.2877126985449132
LOSS train 0.21043583113005607 valid 0.288054162215802
LOSS train 0.21043583113005607 valid 0.28798730444908144
LOSS train 0.21043583113005607 valid 0.28806901285572656
LOSS train 0.21043583113005607 valid 0.2879283878746934
LOSS train 0.21043583113005607 valid 0.28836064389906824
LOSS train 0.21043583113005607 valid 0.28851766110390653
LOSS train 0.21043583113005607 valid 0.28824791931189025
LOSS train 0.21043583113005607 valid 0.28829349907299945
LOSS train 0.21043583113005607 valid 0.2879883941601623
LOSS train 0.21043583113005607 valid 0.2880159207528695
LOSS train 0.21043583113005607 valid 0.28795034260447344
LOSS train 0.21043583113005607 valid 0.28756582218187826
LOSS train 0.21043583113005607 valid 0.28745816111126365
LOSS train 0.21043583113005607 valid 0.2873073045137155
LOSS train 0.21043583113005607 valid 0.2872762190906898
LOSS train 0.21043583113005607 valid 0.28713681146824105
LOSS train 0.21043583113005607 valid 0.28714051491447856
LOSS train 0.21043583113005607 valid 0.2870994636141662
LOSS train 0.21043583113005607 valid 0.28736025210417493
LOSS train 0.21043583113005607 valid 0.28730452404572415
LOSS train 0.21043583113005607 valid 0.287290468501548
LOSS train 0.21043583113005607 valid 0.2869999586508192
LOSS train 0.21043583113005607 valid 0.28703514486551285
LOSS train 0.21043583113005607 valid 0.2870421001092106
LOSS train 0.21043583113005607 valid 0.2878372611830363
LOSS train 0.21043583113005607 valid 0.2878777463764152
LOSS train 0.21043583113005607 valid 0.28806547353665035
LOSS train 0.21043583113005607 valid 0.2880185813895914
LOSS train 0.21043583113005607 valid 0.287519614163198
LOSS train 0.21043583113005607 valid 0.2877255902181264
LOSS train 0.21043583113005607 valid 0.28772820067870153
LOSS train 0.21043583113005607 valid 0.2878168446402396
LOSS train 0.21043583113005607 valid 0.28776241781619877
LOSS train 0.21043583113005607 valid 0.28754925841738466
LOSS train 0.21043583113005607 valid 0.2877781240245964
LOSS train 0.21043583113005607 valid 0.28775598230601857
LOSS train 0.21043583113005607 valid 0.28761666286736726
LOSS train 0.21043583113005607 valid 0.28763321930577296
LOSS train 0.21043583113005607 valid 0.2874099917617845
LOSS train 0.21043583113005607 valid 0.28722723605442635
LOSS train 0.21043583113005607 valid 0.2869098976981349
LOSS train 0.21043583113005607 valid 0.28649771936012036
LOSS train 0.21043583113005607 valid 0.28654116410088826
LOSS train 0.21043583113005607 valid 0.2869635898909883
LOSS train 0.21043583113005607 valid 0.2866444846703893
LOSS train 0.21043583113005607 valid 0.2868431450347223
LOSS train 0.21043583113005607 valid 0.28697209253030664
LOSS train 0.21043583113005607 valid 0.2869253562904938
LOSS train 0.21043583113005607 valid 0.2865967875303224
LOSS train 0.21043583113005607 valid 0.2865634626046771
LOSS train 0.21043583113005607 valid 0.28662510319002743
LOSS train 0.21043583113005607 valid 0.2863610643999917
LOSS train 0.21043583113005607 valid 0.2864540847526355
LOSS train 0.21043583113005607 valid 0.2862326973407282
LOSS train 0.21043583113005607 valid 0.2864015166846554
LOSS train 0.21043583113005607 valid 0.28617631223614654
LOSS train 0.21043583113005607 valid 0.2860536091857486
LOSS train 0.21043583113005607 valid 0.28630157333711237
LOSS train 0.21043583113005607 valid 0.2864881881645748
LOSS train 0.21043583113005607 valid 0.28653499272351707
LOSS train 0.21043583113005607 valid 0.2864437043342901
LOSS train 0.21043583113005607 valid 0.2860824475417266
LOSS train 0.21043583113005607 valid 0.28605355851111874
LOSS train 0.21043583113005607 valid 0.28596004126543667
LOSS train 0.21043583113005607 valid 0.2860029847063917
LOSS train 0.21043583113005607 valid 0.28584342280392927
LOSS train 0.21043583113005607 valid 0.2859906107187271
LOSS train 0.21043583113005607 valid 0.2858221615172181
LOSS train 0.21043583113005607 valid 0.2859465975003938
LOSS train 0.21043583113005607 valid 0.2858012022132083
LOSS train 0.21043583113005607 valid 0.28574565314140515
LOSS train 0.21043583113005607 valid 0.2854497315791937
LOSS train 0.21043583113005607 valid 0.28526095468170787
LOSS train 0.21043583113005607 valid 0.28547303839988514
LOSS train 0.21043583113005607 valid 0.28509208326688923
LOSS train 0.21043583113005607 valid 0.28510768052621105
LOSS train 0.21043583113005607 valid 0.2850967866927385
LOSS train 0.21043583113005607 valid 0.2849370145530843
LOSS train 0.21043583113005607 valid 0.28476852849863543
LOSS train 0.21043583113005607 valid 0.28466542799190936
LOSS train 0.21043583113005607 valid 0.2845700259740446
LOSS train 0.21043583113005607 valid 0.2841280625360768
LOSS train 0.21043583113005607 valid 0.28430040650865407
LOSS train 0.21043583113005607 valid 0.28414867300054303
LOSS train 0.21043583113005607 valid 0.28388530531754863
LOSS train 0.21043583113005607 valid 0.2836916958601281
LOSS train 0.21043583113005607 valid 0.2837010084163575
LOSS train 0.21043583113005607 valid 0.28387030012799663
LOSS train 0.21043583113005607 valid 0.2839083855725684
LOSS train 0.21043583113005607 valid 0.28409582544380513
LOSS train 0.21043583113005607 valid 0.2841288561575881
LOSS train 0.21043583113005607 valid 0.28385502605937246
LOSS train 0.21043583113005607 valid 0.2839216197392455
LOSS train 0.21043583113005607 valid 0.28395928154068606
LOSS train 0.21043583113005607 valid 0.28400591856569324
LOSS train 0.21043583113005607 valid 0.2841862193117403
LOSS train 0.21043583113005607 valid 0.28417790213769134
LOSS train 0.21043583113005607 valid 0.2841252727071624
LOSS train 0.21043583113005607 valid 0.2838541837828653
LOSS train 0.21043583113005607 valid 0.2839765411722286
LOSS train 0.21043583113005607 valid 0.28405168605968356
LOSS train 0.21043583113005607 valid 0.2839469665288925
LOSS train 0.21043583113005607 valid 0.28397652025507614
LOSS train 0.21043583113005607 valid 0.2843129836383895
LOSS train 0.21043583113005607 valid 0.2843676612696104
LOSS train 0.21043583113005607 valid 0.28458031836295233
LOSS train 0.21043583113005607 valid 0.2846346840262413
LOSS train 0.21043583113005607 valid 0.28473785590557826
LOSS train 0.21043583113005607 valid 0.28481028492337673
LOSS train 0.21043583113005607 valid 0.2846771570106433
LOSS train 0.21043583113005607 valid 0.2846280654144083
LOSS train 0.21043583113005607 valid 0.2849184890376761
LOSS train 0.21043583113005607 valid 0.2846489428084786
LOSS train 0.21043583113005607 valid 0.2849121355934988
LOSS train 0.21043583113005607 valid 0.28495086084644333
LOSS train 0.21043583113005607 valid 0.28481491745515847
LOSS train 0.21043583113005607 valid 0.2847045360133052
LOSS train 0.21043583113005607 valid 0.284854695077259
LOSS train 0.21043583113005607 valid 0.28458602892712126
LOSS train 0.21043583113005607 valid 0.284890742520246
LOSS train 0.21043583113005607 valid 0.2853635352288113
LOSS train 0.21043583113005607 valid 0.28571444743750046
LOSS train 0.21043583113005607 valid 0.28571139475921303
LOSS train 0.21043583113005607 valid 0.28568198329765304
LOSS train 0.21043583113005607 valid 0.2856267587912659
LOSS train 0.21043583113005607 valid 0.2855048582376725
LOSS train 0.21043583113005607 valid 0.2857176737189293
LOSS train 0.21043583113005607 valid 0.28588228010798833
LOSS train 0.21043583113005607 valid 0.286123529253971
LOSS train 0.21043583113005607 valid 0.2861797271863274
LOSS train 0.21043583113005607 valid 0.2860263944493504
LOSS train 0.21043583113005607 valid 0.2860699083875207
LOSS train 0.21043583113005607 valid 0.2861444766749628
LOSS train 0.21043583113005607 valid 0.28590638651458206
LOSS train 0.21043583113005607 valid 0.2861106906288354
LOSS train 0.21043583113005607 valid 0.28608040275721014
LOSS train 0.21043583113005607 valid 0.2860606433107303
LOSS train 0.21043583113005607 valid 0.2861551084052557
LOSS train 0.21043583113005607 valid 0.2862042980339691
LOSS train 0.21043583113005607 valid 0.2862593221120508
LOSS train 0.21043583113005607 valid 0.286237133384654
LOSS train 0.21043583113005607 valid 0.2861597459271269
LOSS train 0.21043583113005607 valid 0.28629844484472633
LOSS train 0.21043583113005607 valid 0.2865101472938552
LOSS train 0.21043583113005607 valid 0.28675032807375067
LOSS train 0.21043583113005607 valid 0.2867431297178162
LOSS train 0.21043583113005607 valid 0.2866237693362766
LOSS train 0.21043583113005607 valid 0.28674461854779854
LOSS train 0.21043583113005607 valid 0.2870722299112993
LOSS train 0.21043583113005607 valid 0.28722341986366245
LOSS train 0.21043583113005607 valid 0.28713003562314665
LOSS train 0.21043583113005607 valid 0.2870103220506148
LOSS train 0.21043583113005607 valid 0.2868708637205587
LOSS train 0.21043583113005607 valid 0.2866643297650754
LOSS train 0.21043583113005607 valid 0.28648348886975283
LOSS train 0.21043583113005607 valid 0.2864930064127009
LOSS train 0.21043583113005607 valid 0.2864459896194083
LOSS train 0.21043583113005607 valid 0.28632083020812676
LOSS train 0.21043583113005607 valid 0.28616869851207055
LOSS train 0.21043583113005607 valid 0.2860425480052355
LOSS train 0.21043583113005607 valid 0.2862109515658567
LOSS train 0.21043583113005607 valid 0.286352535088857
LOSS train 0.21043583113005607 valid 0.2862922001135099
LOSS train 0.21043583113005607 valid 0.286286876072867
LOSS train 0.21043583113005607 valid 0.2862607431080606
LOSS train 0.21043583113005607 valid 0.2864191772409789
LOSS train 0.21043583113005607 valid 0.28650142663511735
LOSS train 0.21043583113005607 valid 0.2864790999192962
LOSS train 0.21043583113005607 valid 0.2865816785456383
LOSS train 0.21043583113005607 valid 0.28654823256434026
LOSS train 0.21043583113005607 valid 0.28663721467767445
LOSS train 0.21043583113005607 valid 0.2866210602097592
LOSS train 0.21043583113005607 valid 0.2865228444539212
LOSS train 0.21043583113005607 valid 0.2865409296169024
LOSS train 0.21043583113005607 valid 0.28652789508736376
LOSS train 0.21043583113005607 valid 0.2865011619285596
LOSS train 0.21043583113005607 valid 0.28646960844596225
LOSS train 0.21043583113005607 valid 0.28642368019617276
LOSS train 0.21043583113005607 valid 0.2864832118252255
LOSS train 0.21043583113005607 valid 0.2865746527811875
LOSS train 0.21043583113005607 valid 0.28655475348626314
LOSS train 0.21043583113005607 valid 0.2863984745056903
LOSS train 0.21043583113005607 valid 0.2865128261980668
LOSS train 0.21043583113005607 valid 0.2864051655371725
LOSS train 0.21043583113005607 valid 0.286313018241486
LOSS train 0.21043583113005607 valid 0.2861629640110874
LOSS train 0.21043583113005607 valid 0.2862105339284866
LOSS train 0.21043583113005607 valid 0.28610773442067516
LOSS train 0.21043583113005607 valid 0.28602474273588413
LOSS train 0.21043583113005607 valid 0.2859138147042582
LOSS train 0.21043583113005607 valid 0.2859774641455359
LOSS train 0.21043583113005607 valid 0.28592754436863793
LOSS train 0.21043583113005607 valid 0.2858242700465872
LOSS train 0.21043583113005607 valid 0.2858882397413254
LOSS train 0.21043583113005607 valid 0.28601199486907924
LOSS train 0.21043583113005607 valid 0.28617061534458565
LOSS train 0.21043583113005607 valid 0.2861141585279256
LOSS train 0.21043583113005607 valid 0.2864052386391571
LOSS train 0.21043583113005607 valid 0.2862121625716642
LOSS train 0.21043583113005607 valid 0.2860522929913488
LOSS train 0.21043583113005607 valid 0.28603395644897295
LOSS train 0.21043583113005607 valid 0.28618575325379003
LOSS train 0.21043583113005607 valid 0.2861727370074922
LOSS train 0.21043583113005607 valid 0.2861970661247907
LOSS train 0.21043583113005607 valid 0.28623879855362383
LOSS train 0.21043583113005607 valid 0.28640710664375213
LOSS train 0.21043583113005607 valid 0.28644139315142775
LOSS train 0.21043583113005607 valid 0.2863407164722051
LOSS train 0.21043583113005607 valid 0.2862283104963331
LOSS train 0.21043583113005607 valid 0.2862469177614819
LOSS train 0.21043583113005607 valid 0.2863926730023887
LOSS train 0.21043583113005607 valid 0.28629416347439607
LOSS train 0.21043583113005607 valid 0.28607394160436733
LOSS train 0.21043583113005607 valid 0.28602361400509446
LOSS train 0.21043583113005607 valid 0.28591597163994636
LOSS train 0.21043583113005607 valid 0.2858178146557119
LOSS train 0.21043583113005607 valid 0.2858403665616232
LOSS train 0.21043583113005607 valid 0.28572423135605024
LOSS train 0.21043583113005607 valid 0.28565898864415656
LOSS train 0.21043583113005607 valid 0.2855782130456179
LOSS train 0.21043583113005607 valid 0.28574054891806705
LOSS train 0.21043583113005607 valid 0.28581311940283016
LOSS train 0.21043583113005607 valid 0.2857496658759999
LOSS train 0.21043583113005607 valid 0.2855718682066508
LOSS train 0.21043583113005607 valid 0.2854559334187672
LOSS train 0.21043583113005607 valid 0.28544168622582555
LOSS train 0.21043583113005607 valid 0.28533762548651015
LOSS train 0.21043583113005607 valid 0.28536527542307166
LOSS train 0.21043583113005607 valid 0.28535585452548484
LOSS train 0.21043583113005607 valid 0.2852808248895424
LOSS train 0.21043583113005607 valid 0.2853074486309526
LOSS train 0.21043583113005607 valid 0.2854544827635859
LOSS train 0.21043583113005607 valid 0.2856037955773011
LOSS train 0.21043583113005607 valid 0.28562323611323576
LOSS train 0.21043583113005607 valid 0.28556911247735584
LOSS train 0.21043583113005607 valid 0.2856178013063075
LOSS train 0.21043583113005607 valid 0.2855312986506356
LOSS train 0.21043583113005607 valid 0.28546935674886625
LOSS train 0.21043583113005607 valid 0.28567420061451293
LOSS train 0.21043583113005607 valid 0.2854645548609006
LOSS train 0.21043583113005607 valid 0.2855092716904787
LOSS train 0.21043583113005607 valid 0.2856127322536625
LOSS train 0.21043583113005607 valid 0.28553172587696973
LOSS train 0.21043583113005607 valid 0.2853997301827984
LOSS train 0.21043583113005607 valid 0.28546059358379117
LOSS train 0.21043583113005607 valid 0.28565679438068936
EPOCH 30:
  batch 1 loss: 0.2351405769586563
  batch 2 loss: 0.22203785181045532
  batch 3 loss: 0.2123910884062449
  batch 4 loss: 0.21189579740166664
  batch 5 loss: 0.2127690941095352
  batch 6 loss: 0.20929707338412604
  batch 7 loss: 0.21127182883875711
  batch 8 loss: 0.21270686388015747
  batch 9 loss: 0.21256608929899004
  batch 10 loss: 0.2131376475095749
  batch 11 loss: 0.21246683597564697
  batch 12 loss: 0.21009172375003496
  batch 13 loss: 0.21317787697682014
  batch 14 loss: 0.21519522688218526
  batch 15 loss: 0.2159904917081197
  batch 16 loss: 0.21545403450727463
  batch 17 loss: 0.21336773563833797
  batch 18 loss: 0.21523386653926638
  batch 19 loss: 0.21417552703305295
  batch 20 loss: 0.21109892129898072
  batch 21 loss: 0.21004798937411534
  batch 22 loss: 0.2087851200591434
  batch 23 loss: 0.207720297186271
  batch 24 loss: 0.20562595377365747
  batch 25 loss: 0.206991765499115
  batch 26 loss: 0.20548436102958825
  batch 27 loss: 0.2055022335714764
  batch 28 loss: 0.2037624455988407
  batch 29 loss: 0.20542184289159446
  batch 30 loss: 0.20516262352466583
  batch 31 loss: 0.20593359201185166
  batch 32 loss: 0.20580109022557735
  batch 33 loss: 0.20654577829621054
  batch 34 loss: 0.20575203439768622
  batch 35 loss: 0.20544233577592033
  batch 36 loss: 0.2059265582097901
  batch 37 loss: 0.20576558523886912
  batch 38 loss: 0.20625562848229156
  batch 39 loss: 0.20645351325854278
  batch 40 loss: 0.2072839204221964
  batch 41 loss: 0.20736784215380505
  batch 42 loss: 0.20727210208064034
  batch 43 loss: 0.20783399669236916
  batch 44 loss: 0.2074678452177481
  batch 45 loss: 0.20735943184958563
  batch 46 loss: 0.20867847035760465
  batch 47 loss: 0.20834351441961654
  batch 48 loss: 0.20752207593371472
  batch 49 loss: 0.20750988594123296
  batch 50 loss: 0.2076108995079994
  batch 51 loss: 0.20783564974279964
  batch 52 loss: 0.20841545496995634
  batch 53 loss: 0.20849089943013102
  batch 54 loss: 0.20907981197039285
  batch 55 loss: 0.2087368049404838
  batch 56 loss: 0.2082971701664584
  batch 57 loss: 0.2080979211288586
  batch 58 loss: 0.20899448261178774
  batch 59 loss: 0.20924731331356503
  batch 60 loss: 0.20902165323495864
  batch 61 loss: 0.20945729877127975
  batch 62 loss: 0.20961782985156582
  batch 63 loss: 0.20926603745846523
  batch 64 loss: 0.20970922685228288
  batch 65 loss: 0.20949205595713397
  batch 66 loss: 0.20934699633807846
  batch 67 loss: 0.20958797500204684
  batch 68 loss: 0.20990042848622098
  batch 69 loss: 0.2100713363160258
  batch 70 loss: 0.21040434326444354
  batch 71 loss: 0.21000445749558194
  batch 72 loss: 0.20976674639516407
  batch 73 loss: 0.20974069690867647
  batch 74 loss: 0.2093099921539023
  batch 75 loss: 0.20892850816249847
  batch 76 loss: 0.20931853706899442
  batch 77 loss: 0.20876667445356195
  batch 78 loss: 0.20915080931706306
  batch 79 loss: 0.20913480410847482
  batch 80 loss: 0.20875642839819192
  batch 81 loss: 0.20867498954873026
  batch 82 loss: 0.20891220922150264
  batch 83 loss: 0.20887316691588206
  batch 84 loss: 0.20872214365573155
  batch 85 loss: 0.2085366741699331
  batch 86 loss: 0.20910934150912042
  batch 87 loss: 0.20893175557427024
  batch 88 loss: 0.2085992439903996
  batch 89 loss: 0.20852280550458457
  batch 90 loss: 0.208480619556374
  batch 91 loss: 0.20840111014607188
  batch 92 loss: 0.20819015266454738
  batch 93 loss: 0.20832798381646475
  batch 94 loss: 0.20846549627628733
  batch 95 loss: 0.20804148206585332
  batch 96 loss: 0.20793012622743845
  batch 97 loss: 0.20796242110508004
  batch 98 loss: 0.20822037893290424
  batch 99 loss: 0.2084681810152651
  batch 100 loss: 0.20804200321435928
  batch 101 loss: 0.20794230212669562
  batch 102 loss: 0.20826280876701952
  batch 103 loss: 0.20858209442745135
  batch 104 loss: 0.20846985925275546
  batch 105 loss: 0.20836498127097175
  batch 106 loss: 0.20852627113180341
  batch 107 loss: 0.20810583275612268
  batch 108 loss: 0.20824520027747861
  batch 109 loss: 0.20831613666420684
  batch 110 loss: 0.20857438485730778
  batch 111 loss: 0.20868162327521556
  batch 112 loss: 0.20860495258654868
  batch 113 loss: 0.20865374236507753
  batch 114 loss: 0.20900035610324458
  batch 115 loss: 0.2091360725786375
  batch 116 loss: 0.2093432809001413
  batch 117 loss: 0.20941873811758482
  batch 118 loss: 0.209361769385257
  batch 119 loss: 0.20947611569857397
  batch 120 loss: 0.20887869236369927
  batch 121 loss: 0.20878627773158806
  batch 122 loss: 0.20858171325726588
  batch 123 loss: 0.2084141836418369
  batch 124 loss: 0.20852853742337996
  batch 125 loss: 0.20836593663692474
  batch 126 loss: 0.20827465397971018
  batch 127 loss: 0.20851670710120615
  batch 128 loss: 0.20826290152035654
  batch 129 loss: 0.20839320850926776
  batch 130 loss: 0.20825148282142786
  batch 131 loss: 0.20815069518471493
  batch 132 loss: 0.20799969294757553
  batch 133 loss: 0.20830103049152776
  batch 134 loss: 0.20841548569611648
  batch 135 loss: 0.2081326463708171
  batch 136 loss: 0.2083579394966364
  batch 137 loss: 0.20837599767820678
  batch 138 loss: 0.20825430869624234
  batch 139 loss: 0.2085137581653732
  batch 140 loss: 0.20850882955959865
  batch 141 loss: 0.2089013288207088
  batch 142 loss: 0.20887184300473038
  batch 143 loss: 0.20862968620303626
  batch 144 loss: 0.20870444923639297
  batch 145 loss: 0.20872324633187261
  batch 146 loss: 0.20870948899282168
  batch 147 loss: 0.20901157357254807
  batch 148 loss: 0.20894140558871063
  batch 149 loss: 0.2089350824588097
  batch 150 loss: 0.20894544412692387
  batch 151 loss: 0.20890682343615602
  batch 152 loss: 0.20923184446598353
  batch 153 loss: 0.20905628079682395
  batch 154 loss: 0.2092552502433975
  batch 155 loss: 0.2091127339870699
  batch 156 loss: 0.20897770883181158
  batch 157 loss: 0.20910543288774552
  batch 158 loss: 0.20905429379472248
  batch 159 loss: 0.20912938757137683
  batch 160 loss: 0.20885982140898704
  batch 161 loss: 0.20882983209553713
  batch 162 loss: 0.20857651274145386
  batch 163 loss: 0.20864319426516081
  batch 164 loss: 0.20848706573611353
  batch 165 loss: 0.20841426650683084
  batch 166 loss: 0.20836759044463377
  batch 167 loss: 0.208291281036988
  batch 168 loss: 0.20818354260353816
  batch 169 loss: 0.207897731538355
  batch 170 loss: 0.20773955767645555
  batch 171 loss: 0.20766555278273355
  batch 172 loss: 0.20770480076587478
  batch 173 loss: 0.20751807937732322
  batch 174 loss: 0.20758413591946678
  batch 175 loss: 0.2076140614066805
  batch 176 loss: 0.20737630459056658
  batch 177 loss: 0.20731954882710668
  batch 178 loss: 0.2075468973162469
  batch 179 loss: 0.20763177620299036
  batch 180 loss: 0.20744948552714454
  batch 181 loss: 0.20748717111447898
  batch 182 loss: 0.20738335158471222
  batch 183 loss: 0.20723074063902996
  batch 184 loss: 0.207226125523448
  batch 185 loss: 0.20731162936300845
  batch 186 loss: 0.20744658678129155
  batch 187 loss: 0.20732921784255595
  batch 188 loss: 0.20707185185970145
  batch 189 loss: 0.2068853092887414
  batch 190 loss: 0.20694554887319866
  batch 191 loss: 0.2070018017167196
  batch 192 loss: 0.20692538881363967
  batch 193 loss: 0.2067892623225642
  batch 194 loss: 0.20684393111270727
  batch 195 loss: 0.20685437612044505
  batch 196 loss: 0.20698058057804497
  batch 197 loss: 0.207061976799505
  batch 198 loss: 0.20737130894805444
  batch 199 loss: 0.2071959322421395
  batch 200 loss: 0.2074296697974205
  batch 201 loss: 0.2077047599488823
  batch 202 loss: 0.20789731758655888
  batch 203 loss: 0.20780221369172552
  batch 204 loss: 0.20770805811180787
  batch 205 loss: 0.20794833302497864
  batch 206 loss: 0.20809738432030075
  batch 207 loss: 0.20850102560243744
  batch 208 loss: 0.2086804538535384
  batch 209 loss: 0.20866479307555696
  batch 210 loss: 0.20882325725896017
  batch 211 loss: 0.20883613048006572
  batch 212 loss: 0.2089031936143929
  batch 213 loss: 0.20887719535491836
  batch 214 loss: 0.20887257367651038
  batch 215 loss: 0.20882405319879221
  batch 216 loss: 0.20862600338403825
  batch 217 loss: 0.20852598671539588
  batch 218 loss: 0.2084248458300162
  batch 219 loss: 0.2085277251729138
  batch 220 loss: 0.20847546410831538
  batch 221 loss: 0.20858376672095302
  batch 222 loss: 0.2084969933908265
  batch 223 loss: 0.20848096325793075
  batch 224 loss: 0.2083984736486205
  batch 225 loss: 0.20828166915310753
  batch 226 loss: 0.2083660657558821
  batch 227 loss: 0.2083036631739612
  batch 228 loss: 0.20829609127943977
  batch 229 loss: 0.20828887989427325
  batch 230 loss: 0.20853765127451523
  batch 231 loss: 0.20861181074922736
  batch 232 loss: 0.20850943215191364
  batch 233 loss: 0.20846408004412836
  batch 234 loss: 0.20850897134623975
  batch 235 loss: 0.20851707743837478
  batch 236 loss: 0.2083767122762688
  batch 237 loss: 0.20837385524928823
  batch 238 loss: 0.20837660697327942
  batch 239 loss: 0.20817997234885163
  batch 240 loss: 0.20808885836352906
  batch 241 loss: 0.20814863039002873
  batch 242 loss: 0.2080065171457519
  batch 243 loss: 0.20798860036541897
  batch 244 loss: 0.20796949556860767
  batch 245 loss: 0.20784230043693463
  batch 246 loss: 0.20776145160198212
  batch 247 loss: 0.20783298326889996
  batch 248 loss: 0.2077974824895782
  batch 249 loss: 0.20780781043102464
  batch 250 loss: 0.20776834338903427
  batch 251 loss: 0.20766361277416884
  batch 252 loss: 0.20761466463879932
  batch 253 loss: 0.2077275886248223
  batch 254 loss: 0.20768895751144004
  batch 255 loss: 0.20767590882731418
  batch 256 loss: 0.2078907445538789
  batch 257 loss: 0.20808726491167387
  batch 258 loss: 0.20799193011466846
  batch 259 loss: 0.20810329994639834
  batch 260 loss: 0.20813638132352094
  batch 261 loss: 0.20819508572647855
  batch 262 loss: 0.2082366678892201
  batch 263 loss: 0.2083879145492619
  batch 264 loss: 0.20829780693307068
  batch 265 loss: 0.208199824139757
  batch 266 loss: 0.20820541668655282
  batch 267 loss: 0.20826788430803278
  batch 268 loss: 0.2081746438990778
  batch 269 loss: 0.20819009358554968
  batch 270 loss: 0.20840000057661975
  batch 271 loss: 0.20840922327938996
  batch 272 loss: 0.20843396560453317
  batch 273 loss: 0.20837195768897787
  batch 274 loss: 0.2084726419327033
  batch 275 loss: 0.20844830686395818
  batch 276 loss: 0.20843311598983363
  batch 277 loss: 0.20844240163853023
  batch 278 loss: 0.20847609226437783
  batch 279 loss: 0.20850657514132906
  batch 280 loss: 0.20833574011921882
  batch 281 loss: 0.20825507149789682
  batch 282 loss: 0.2082651116429491
  batch 283 loss: 0.20816735128210628
  batch 284 loss: 0.20825616852708265
  batch 285 loss: 0.20823647876580556
  batch 286 loss: 0.20821677643310774
  batch 287 loss: 0.2081849044935213
  batch 288 loss: 0.20814542420622376
  batch 289 loss: 0.20820365933810964
  batch 290 loss: 0.20807910530731596
  batch 291 loss: 0.20811645159197018
  batch 292 loss: 0.2082093555531273
  batch 293 loss: 0.2082011077692891
  batch 294 loss: 0.20826570289272842
  batch 295 loss: 0.20825551450252533
  batch 296 loss: 0.20842791434276747
  batch 297 loss: 0.2084812300674843
  batch 298 loss: 0.20845876759130683
  batch 299 loss: 0.20843459840203607
  batch 300 loss: 0.20843352104226748
  batch 301 loss: 0.20848771083196532
  batch 302 loss: 0.2084786525822633
  batch 303 loss: 0.20844725925143404
  batch 304 loss: 0.20846109636324017
  batch 305 loss: 0.20838710709673458
  batch 306 loss: 0.20856112139482125
  batch 307 loss: 0.20859556298108364
  batch 308 loss: 0.20857418493016974
  batch 309 loss: 0.20852861242386903
  batch 310 loss: 0.2084235337472731
  batch 311 loss: 0.20863696185338917
  batch 312 loss: 0.20867304179148796
  batch 313 loss: 0.20871134640309758
  batch 314 loss: 0.20867511350068318
  batch 315 loss: 0.2086149034992097
  batch 316 loss: 0.20864122026140178
  batch 317 loss: 0.20859324494934983
  batch 318 loss: 0.20860085762896627
  batch 319 loss: 0.20856933879628076
  batch 320 loss: 0.20852836510166525
  batch 321 loss: 0.20860640876389738
  batch 322 loss: 0.20862110255297667
  batch 323 loss: 0.2085851343239055
  batch 324 loss: 0.20850253031577592
  batch 325 loss: 0.2083786066678854
  batch 326 loss: 0.20837899265479456
  batch 327 loss: 0.2083486366891715
  batch 328 loss: 0.20814595130703797
  batch 329 loss: 0.20812063154659735
  batch 330 loss: 0.20798141640244108
  batch 331 loss: 0.207927946237639
  batch 332 loss: 0.20777443962463413
  batch 333 loss: 0.20795211952190856
  batch 334 loss: 0.20787420757337957
  batch 335 loss: 0.20769338025085962
  batch 336 loss: 0.2076238463411019
  batch 337 loss: 0.20762252529049485
  batch 338 loss: 0.20759503574237315
  batch 339 loss: 0.20771641141369632
  batch 340 loss: 0.2078908182680607
  batch 341 loss: 0.20793602006001907
  batch 342 loss: 0.20788071831764535
  batch 343 loss: 0.20798355923102133
  batch 344 loss: 0.2080742645748826
  batch 345 loss: 0.2081902287144592
  batch 346 loss: 0.20818565536096606
  batch 347 loss: 0.20814876125250495
  batch 348 loss: 0.20816242172457705
  batch 349 loss: 0.20809429892496256
  batch 350 loss: 0.20831287239279067
  batch 351 loss: 0.2083542605920395
  batch 352 loss: 0.2084024544297294
  batch 353 loss: 0.20838462901182958
  batch 354 loss: 0.20856945831223397
  batch 355 loss: 0.2085438756875589
  batch 356 loss: 0.20851920875773
  batch 357 loss: 0.20845821883832039
  batch 358 loss: 0.2085840161785733
  batch 359 loss: 0.2085560879833519
  batch 360 loss: 0.20852252215974862
  batch 361 loss: 0.20848985103028633
  batch 362 loss: 0.20850759662317309
  batch 363 loss: 0.20842241058664873
  batch 364 loss: 0.20830234606842418
  batch 365 loss: 0.2083135574239574
  batch 366 loss: 0.2082949382929854
  batch 367 loss: 0.20825480513416778
  batch 368 loss: 0.2082024584321872
  batch 369 loss: 0.20814229935649933
  batch 370 loss: 0.20806473666751707
  batch 371 loss: 0.20812741576661317
  batch 372 loss: 0.20818692229447827
  batch 373 loss: 0.20809469270162864
  batch 374 loss: 0.2079446111372448
  batch 375 loss: 0.20785056503613789
  batch 376 loss: 0.20783768277219
  batch 377 loss: 0.20777524719187687
  batch 378 loss: 0.20772186942674495
  batch 379 loss: 0.20766403189906973
  batch 380 loss: 0.20770310179183357
  batch 381 loss: 0.2076796623743738
  batch 382 loss: 0.20767569327385638
  batch 383 loss: 0.2077454291964947
  batch 384 loss: 0.20778518150715777
  batch 385 loss: 0.2079086396988336
  batch 386 loss: 0.20789079896989882
  batch 387 loss: 0.20794383138033154
  batch 388 loss: 0.20798720672880253
  batch 389 loss: 0.20799471368979672
  batch 390 loss: 0.20797681972766535
  batch 391 loss: 0.20802205210299138
  batch 392 loss: 0.20796439610421658
  batch 393 loss: 0.207889438357972
  batch 394 loss: 0.20789538547018457
  batch 395 loss: 0.20780986558787432
  batch 396 loss: 0.207686251085816
  batch 397 loss: 0.20766885514823855
  batch 398 loss: 0.20758802342654473
  batch 399 loss: 0.20764672442486412
  batch 400 loss: 0.20778608381748198
  batch 401 loss: 0.20763296111860774
  batch 402 loss: 0.20763617179435284
  batch 403 loss: 0.20764428724721998
  batch 404 loss: 0.20761282874806092
  batch 405 loss: 0.2076321091916826
  batch 406 loss: 0.20771763374652769
  batch 407 loss: 0.20774466040972117
  batch 408 loss: 0.20782176142229752
  batch 409 loss: 0.20788727995818868
  batch 410 loss: 0.20798616343882026
  batch 411 loss: 0.20802097994190644
  batch 412 loss: 0.20803864202597766
  batch 413 loss: 0.20812191794340026
  batch 414 loss: 0.20820224231136017
  batch 415 loss: 0.2082205358398966
  batch 416 loss: 0.2083553858459569
  batch 417 loss: 0.20834825632812307
  batch 418 loss: 0.20841359764194944
  batch 419 loss: 0.2084830863521321
  batch 420 loss: 0.20843009828102022
  batch 421 loss: 0.20836993627197103
  batch 422 loss: 0.2084909312391733
  batch 423 loss: 0.20845624543814512
  batch 424 loss: 0.20843553110816568
  batch 425 loss: 0.20837548988706925
  batch 426 loss: 0.20836495678049857
  batch 427 loss: 0.2083421379537158
  batch 428 loss: 0.20844539408093302
  batch 429 loss: 0.20845824862137818
  batch 430 loss: 0.2084297503842864
  batch 431 loss: 0.20846283207636698
  batch 432 loss: 0.2084604560363072
  batch 433 loss: 0.2085363443674171
  batch 434 loss: 0.20864843594313767
  batch 435 loss: 0.2086660935275856
  batch 436 loss: 0.20868308647373401
  batch 437 loss: 0.20874140307614134
  batch 438 loss: 0.20880712412263705
  batch 439 loss: 0.20881369580983572
  batch 440 loss: 0.20889351960610258
  batch 441 loss: 0.2088560113628435
  batch 442 loss: 0.20879696238769124
  batch 443 loss: 0.2088061528065942
  batch 444 loss: 0.20875031074827854
  batch 445 loss: 0.20874120797334092
  batch 446 loss: 0.20867800281587737
  batch 447 loss: 0.20869988349726804
  batch 448 loss: 0.2087172534250255
  batch 449 loss: 0.2086925551394843
  batch 450 loss: 0.2087107033530871
  batch 451 loss: 0.20865816403254703
  batch 452 loss: 0.2087557957697231
  batch 453 loss: 0.20878061865997105
  batch 454 loss: 0.20878381485587175
  batch 455 loss: 0.20876531299653944
  batch 456 loss: 0.20881975729737365
  batch 457 loss: 0.20879669194018033
  batch 458 loss: 0.20874306871901432
  batch 459 loss: 0.20886600205841147
  batch 460 loss: 0.20894122502726056
  batch 461 loss: 0.2089080293046678
  batch 462 loss: 0.2089109918449348
  batch 463 loss: 0.20882327964058706
  batch 464 loss: 0.20888120350267353
  batch 465 loss: 0.20880356261166194
  batch 466 loss: 0.2087292472628053
  batch 467 loss: 0.20878806218855897
  batch 468 loss: 0.2087423471877208
  batch 469 loss: 0.2087995120520785
  batch 470 loss: 0.20868083571499965
  batch 471 loss: 0.20870103726594563
  batch 472 loss: 0.2085434349916749
LOSS train 0.2085434349916749 valid 0.25818347930908203
LOSS train 0.2085434349916749 valid 0.2623006999492645
LOSS train 0.2085434349916749 valid 0.26698357860247296
LOSS train 0.2085434349916749 valid 0.2701674997806549
LOSS train 0.2085434349916749 valid 0.2654961824417114
LOSS train 0.2085434349916749 valid 0.26654597620169324
LOSS train 0.2085434349916749 valid 0.27796046648706707
LOSS train 0.2085434349916749 valid 0.27792148664593697
LOSS train 0.2085434349916749 valid 0.27680345707469517
LOSS train 0.2085434349916749 valid 0.2797290414571762
LOSS train 0.2085434349916749 valid 0.27762997963211755
LOSS train 0.2085434349916749 valid 0.2752991256614526
LOSS train 0.2085434349916749 valid 0.27498139441013336
LOSS train 0.2085434349916749 valid 0.27503097163779394
LOSS train 0.2085434349916749 valid 0.2692870895067851
LOSS train 0.2085434349916749 valid 0.27138730324804783
LOSS train 0.2085434349916749 valid 0.2725431130212896
LOSS train 0.2085434349916749 valid 0.2730087488889694
LOSS train 0.2085434349916749 valid 0.2758451731581437
LOSS train 0.2085434349916749 valid 0.2753055036067963
LOSS train 0.2085434349916749 valid 0.27419765790303546
LOSS train 0.2085434349916749 valid 0.2727350741624832
LOSS train 0.2085434349916749 valid 0.2730716505776281
LOSS train 0.2085434349916749 valid 0.2719188829263051
LOSS train 0.2085434349916749 valid 0.26974703252315524
LOSS train 0.2085434349916749 valid 0.2689769204992514
LOSS train 0.2085434349916749 valid 0.26926222609149086
LOSS train 0.2085434349916749 valid 0.2697421551815101
LOSS train 0.2085434349916749 valid 0.26944998780201224
LOSS train 0.2085434349916749 valid 0.2695341701308886
LOSS train 0.2085434349916749 valid 0.2714902792246111
LOSS train 0.2085434349916749 valid 0.2709700488485396
LOSS train 0.2085434349916749 valid 0.2720717051715562
LOSS train 0.2085434349916749 valid 0.2722826700876741
LOSS train 0.2085434349916749 valid 0.274718764424324
LOSS train 0.2085434349916749 valid 0.2743431789179643
LOSS train 0.2085434349916749 valid 0.273537870597195
LOSS train 0.2085434349916749 valid 0.2745178259517017
LOSS train 0.2085434349916749 valid 0.27455209806943554
LOSS train 0.2085434349916749 valid 0.2740998093038797
LOSS train 0.2085434349916749 valid 0.2756635775653327
LOSS train 0.2085434349916749 valid 0.2762221179547764
LOSS train 0.2085434349916749 valid 0.2761069435712903
LOSS train 0.2085434349916749 valid 0.27690708196975966
LOSS train 0.2085434349916749 valid 0.2764343172311783
LOSS train 0.2085434349916749 valid 0.27713916256375937
LOSS train 0.2085434349916749 valid 0.27837119742910915
LOSS train 0.2085434349916749 valid 0.2783558228984475
LOSS train 0.2085434349916749 valid 0.2791329619227623
LOSS train 0.2085434349916749 valid 0.277813994884491
LOSS train 0.2085434349916749 valid 0.2774721436640796
LOSS train 0.2085434349916749 valid 0.2778315343535863
LOSS train 0.2085434349916749 valid 0.27803739118126203
LOSS train 0.2085434349916749 valid 0.2781292255277987
LOSS train 0.2085434349916749 valid 0.2779280852187764
LOSS train 0.2085434349916749 valid 0.2773880362510681
LOSS train 0.2085434349916749 valid 0.27702649172983673
LOSS train 0.2085434349916749 valid 0.2769182838242629
LOSS train 0.2085434349916749 valid 0.27764934907525274
LOSS train 0.2085434349916749 valid 0.2776087984442711
LOSS train 0.2085434349916749 valid 0.27688102839423007
LOSS train 0.2085434349916749 valid 0.2782613279358033
LOSS train 0.2085434349916749 valid 0.2789056897163391
LOSS train 0.2085434349916749 valid 0.2801354927942157
LOSS train 0.2085434349916749 valid 0.28120523461928737
LOSS train 0.2085434349916749 valid 0.28145876571987616
LOSS train 0.2085434349916749 valid 0.2804459702612749
LOSS train 0.2085434349916749 valid 0.2804497090332648
LOSS train 0.2085434349916749 valid 0.27924232530421106
LOSS train 0.2085434349916749 valid 0.2795854268329484
LOSS train 0.2085434349916749 valid 0.27951113334004307
LOSS train 0.2085434349916749 valid 0.27953908116453224
LOSS train 0.2085434349916749 valid 0.2792690982149072
LOSS train 0.2085434349916749 valid 0.27918242421504613
LOSS train 0.2085434349916749 valid 0.2789328294992447
LOSS train 0.2085434349916749 valid 0.2788630372991687
LOSS train 0.2085434349916749 valid 0.2790330979344133
LOSS train 0.2085434349916749 valid 0.27928117777292544
LOSS train 0.2085434349916749 valid 0.2796182079782969
LOSS train 0.2085434349916749 valid 0.2785597896203399
LOSS train 0.2085434349916749 valid 0.2774881967055945
LOSS train 0.2085434349916749 valid 0.27821344391601843
LOSS train 0.2085434349916749 valid 0.2779935935893691
LOSS train 0.2085434349916749 valid 0.27768158735263915
LOSS train 0.2085434349916749 valid 0.2774595832123476
LOSS train 0.2085434349916749 valid 0.276989750390829
LOSS train 0.2085434349916749 valid 0.2767256707295604
LOSS train 0.2085434349916749 valid 0.2759764556857673
LOSS train 0.2085434349916749 valid 0.2767284247982368
LOSS train 0.2085434349916749 valid 0.2769869284497367
LOSS train 0.2085434349916749 valid 0.27715832486257447
LOSS train 0.2085434349916749 valid 0.2771006317242332
LOSS train 0.2085434349916749 valid 0.276990960362137
LOSS train 0.2085434349916749 valid 0.2771259539938988
LOSS train 0.2085434349916749 valid 0.27692479585346425
LOSS train 0.2085434349916749 valid 0.27744794512788457
LOSS train 0.2085434349916749 valid 0.27738089536883165
LOSS train 0.2085434349916749 valid 0.2777662116045855
LOSS train 0.2085434349916749 valid 0.27809254358513186
LOSS train 0.2085434349916749 valid 0.2783245405554771
LOSS train 0.2085434349916749 valid 0.2788142520602387
LOSS train 0.2085434349916749 valid 0.27867553835990383
LOSS train 0.2085434349916749 valid 0.2789156228593252
LOSS train 0.2085434349916749 valid 0.2790940168958444
LOSS train 0.2085434349916749 valid 0.27909932505516777
LOSS train 0.2085434349916749 valid 0.2793565898571374
LOSS train 0.2085434349916749 valid 0.2790112119411754
LOSS train 0.2085434349916749 valid 0.27905338864635537
LOSS train 0.2085434349916749 valid 0.27933994728490846
LOSS train 0.2085434349916749 valid 0.2794323899529197
LOSS train 0.2085434349916749 valid 0.2786435232506142
LOSS train 0.2085434349916749 valid 0.27834190148860216
LOSS train 0.2085434349916749 valid 0.278125828741926
LOSS train 0.2085434349916749 valid 0.277736949293237
LOSS train 0.2085434349916749 valid 0.2779427284779756
LOSS train 0.2085434349916749 valid 0.2776993492553974
LOSS train 0.2085434349916749 valid 0.27784251300697654
LOSS train 0.2085434349916749 valid 0.2779294648918055
LOSS train 0.2085434349916749 valid 0.27755839609298383
LOSS train 0.2085434349916749 valid 0.2770743398616711
LOSS train 0.2085434349916749 valid 0.27692645431057483
LOSS train 0.2085434349916749 valid 0.27677356183040336
LOSS train 0.2085434349916749 valid 0.27678245608884144
LOSS train 0.2085434349916749 valid 0.2772838683618653
LOSS train 0.2085434349916749 valid 0.27726539623737334
LOSS train 0.2085434349916749 valid 0.27728723805575145
LOSS train 0.2085434349916749 valid 0.27708796617083664
LOSS train 0.2085434349916749 valid 0.27751010295469314
LOSS train 0.2085434349916749 valid 0.27763980684816375
LOSS train 0.2085434349916749 valid 0.2773368062881323
LOSS train 0.2085434349916749 valid 0.2773533138155027
LOSS train 0.2085434349916749 valid 0.2771054379867785
LOSS train 0.2085434349916749 valid 0.27705615915750204
LOSS train 0.2085434349916749 valid 0.27699068210907835
LOSS train 0.2085434349916749 valid 0.27664839956495496
LOSS train 0.2085434349916749 valid 0.27652933054110584
LOSS train 0.2085434349916749 valid 0.2764146317095652
LOSS train 0.2085434349916749 valid 0.27635990363964136
LOSS train 0.2085434349916749 valid 0.27623998990161813
LOSS train 0.2085434349916749 valid 0.2762702228767531
LOSS train 0.2085434349916749 valid 0.2762464705511188
LOSS train 0.2085434349916749 valid 0.2764852978813816
LOSS train 0.2085434349916749 valid 0.2764009299811783
LOSS train 0.2085434349916749 valid 0.276448387445675
LOSS train 0.2085434349916749 valid 0.2762046807798846
LOSS train 0.2085434349916749 valid 0.2762066180983635
LOSS train 0.2085434349916749 valid 0.27624195874953755
LOSS train 0.2085434349916749 valid 0.2771829302246506
LOSS train 0.2085434349916749 valid 0.27715960265005996
LOSS train 0.2085434349916749 valid 0.2773584763209025
LOSS train 0.2085434349916749 valid 0.2773117833579613
LOSS train 0.2085434349916749 valid 0.27685015981918887
LOSS train 0.2085434349916749 valid 0.2771330277125041
LOSS train 0.2085434349916749 valid 0.27703559301890335
LOSS train 0.2085434349916749 valid 0.2770559758909287
LOSS train 0.2085434349916749 valid 0.2770320272598511
LOSS train 0.2085434349916749 valid 0.2769275637948589
LOSS train 0.2085434349916749 valid 0.27714872586576245
LOSS train 0.2085434349916749 valid 0.2771587885400784
LOSS train 0.2085434349916749 valid 0.27701848968863485
LOSS train 0.2085434349916749 valid 0.27698305175171134
LOSS train 0.2085434349916749 valid 0.2767969924542639
LOSS train 0.2085434349916749 valid 0.27659151597988385
LOSS train 0.2085434349916749 valid 0.27629004137181656
LOSS train 0.2085434349916749 valid 0.27589944754586077
LOSS train 0.2085434349916749 valid 0.27591098135853387
LOSS train 0.2085434349916749 valid 0.2763017254496763
LOSS train 0.2085434349916749 valid 0.2759793167490335
LOSS train 0.2085434349916749 valid 0.27617112763181945
LOSS train 0.2085434349916749 valid 0.2762432444621535
LOSS train 0.2085434349916749 valid 0.27616468708069003
LOSS train 0.2085434349916749 valid 0.2758334737357705
LOSS train 0.2085434349916749 valid 0.275651468620824
LOSS train 0.2085434349916749 valid 0.27569150505052215
LOSS train 0.2085434349916749 valid 0.27540815796170914
LOSS train 0.2085434349916749 valid 0.27542648125778546
LOSS train 0.2085434349916749 valid 0.27528242682669796
LOSS train 0.2085434349916749 valid 0.2753796648610844
LOSS train 0.2085434349916749 valid 0.27518004503663024
LOSS train 0.2085434349916749 valid 0.27503176108002664
LOSS train 0.2085434349916749 valid 0.27527479185254533
LOSS train 0.2085434349916749 valid 0.27552180475258564
LOSS train 0.2085434349916749 valid 0.2754919524889826
LOSS train 0.2085434349916749 valid 0.2753186878626761
LOSS train 0.2085434349916749 valid 0.2749271882546915
LOSS train 0.2085434349916749 valid 0.2748926756202534
LOSS train 0.2085434349916749 valid 0.2748420383522217
LOSS train 0.2085434349916749 valid 0.27488003417532497
LOSS train 0.2085434349916749 valid 0.27472271482465127
LOSS train 0.2085434349916749 valid 0.2747984650103669
LOSS train 0.2085434349916749 valid 0.2746391153616431
LOSS train 0.2085434349916749 valid 0.2747311848991861
LOSS train 0.2085434349916749 valid 0.2745593014930814
LOSS train 0.2085434349916749 valid 0.27442008978927257
LOSS train 0.2085434349916749 valid 0.2741266450056663
LOSS train 0.2085434349916749 valid 0.2739375412312089
LOSS train 0.2085434349916749 valid 0.2741199929248258
LOSS train 0.2085434349916749 valid 0.27372839542651417
LOSS train 0.2085434349916749 valid 0.273743755928236
LOSS train 0.2085434349916749 valid 0.2737105482071638
LOSS train 0.2085434349916749 valid 0.2734922473851721
LOSS train 0.2085434349916749 valid 0.27347286923391984
LOSS train 0.2085434349916749 valid 0.27331473645318316
LOSS train 0.2085434349916749 valid 0.2731952820630634
LOSS train 0.2085434349916749 valid 0.2727984863083537
LOSS train 0.2085434349916749 valid 0.27295651496614065
LOSS train 0.2085434349916749 valid 0.2729262902540861
LOSS train 0.2085434349916749 valid 0.27270556714099187
LOSS train 0.2085434349916749 valid 0.27253359278138173
LOSS train 0.2085434349916749 valid 0.2725989505648613
LOSS train 0.2085434349916749 valid 0.27282091544420234
LOSS train 0.2085434349916749 valid 0.2729078453245028
LOSS train 0.2085434349916749 valid 0.2730545206388957
LOSS train 0.2085434349916749 valid 0.27310606050435626
LOSS train 0.2085434349916749 valid 0.2729083643403164
LOSS train 0.2085434349916749 valid 0.2729243197374874
LOSS train 0.2085434349916749 valid 0.27294957651520657
LOSS train 0.2085434349916749 valid 0.2730371854994275
LOSS train 0.2085434349916749 valid 0.2732122848023018
LOSS train 0.2085434349916749 valid 0.27322268187999726
LOSS train 0.2085434349916749 valid 0.2731362424824572
LOSS train 0.2085434349916749 valid 0.2729638300768964
LOSS train 0.2085434349916749 valid 0.2731206316851714
LOSS train 0.2085434349916749 valid 0.2732341956081135
LOSS train 0.2085434349916749 valid 0.27315024111006
LOSS train 0.2085434349916749 valid 0.27324873340868316
LOSS train 0.2085434349916749 valid 0.27358559404175714
LOSS train 0.2085434349916749 valid 0.27366223356180025
LOSS train 0.2085434349916749 valid 0.2738399521232172
LOSS train 0.2085434349916749 valid 0.2738542292429053
LOSS train 0.2085434349916749 valid 0.2739051895224171
LOSS train 0.2085434349916749 valid 0.2740052284608627
LOSS train 0.2085434349916749 valid 0.2738584592107028
LOSS train 0.2085434349916749 valid 0.27382544052397084
LOSS train 0.2085434349916749 valid 0.27410096475418577
LOSS train 0.2085434349916749 valid 0.2738722450399803
LOSS train 0.2085434349916749 valid 0.27419314253682325
LOSS train 0.2085434349916749 valid 0.27423949477051485
LOSS train 0.2085434349916749 valid 0.27411785939996713
LOSS train 0.2085434349916749 valid 0.2740708276008566
LOSS train 0.2085434349916749 valid 0.27416348302760063
LOSS train 0.2085434349916749 valid 0.27393852796190044
LOSS train 0.2085434349916749 valid 0.27419308079368293
LOSS train 0.2085434349916749 valid 0.2746172551615316
LOSS train 0.2085434349916749 valid 0.2749208399835898
LOSS train 0.2085434349916749 valid 0.274935783348917
LOSS train 0.2085434349916749 valid 0.2749021904550583
LOSS train 0.2085434349916749 valid 0.27486483266036355
LOSS train 0.2085434349916749 valid 0.274734882884715
LOSS train 0.2085434349916749 valid 0.2750256571173668
LOSS train 0.2085434349916749 valid 0.27515165602780906
LOSS train 0.2085434349916749 valid 0.27537415371764273
LOSS train 0.2085434349916749 valid 0.27541000669888355
LOSS train 0.2085434349916749 valid 0.2752791204321103
LOSS train 0.2085434349916749 valid 0.2753057387529635
LOSS train 0.2085434349916749 valid 0.2754178432514891
LOSS train 0.2085434349916749 valid 0.27521089287583467
LOSS train 0.2085434349916749 valid 0.27540555076543677
LOSS train 0.2085434349916749 valid 0.2753725455764638
LOSS train 0.2085434349916749 valid 0.27529199776741176
LOSS train 0.2085434349916749 valid 0.2753174134826295
LOSS train 0.2085434349916749 valid 0.27536727715539566
LOSS train 0.2085434349916749 valid 0.27552374880123504
LOSS train 0.2085434349916749 valid 0.27544716395663493
LOSS train 0.2085434349916749 valid 0.27533830927228026
LOSS train 0.2085434349916749 valid 0.27544937282800674
LOSS train 0.2085434349916749 valid 0.27562378056710135
LOSS train 0.2085434349916749 valid 0.2758667827653351
LOSS train 0.2085434349916749 valid 0.2758786275595094
LOSS train 0.2085434349916749 valid 0.2757577787394877
LOSS train 0.2085434349916749 valid 0.2758817376355843
LOSS train 0.2085434349916749 valid 0.27615780870923223
LOSS train 0.2085434349916749 valid 0.27627460243719404
LOSS train 0.2085434349916749 valid 0.2761630091788995
LOSS train 0.2085434349916749 valid 0.2760025999762795
LOSS train 0.2085434349916749 valid 0.2758385185422241
LOSS train 0.2085434349916749 valid 0.2756518987864794
LOSS train 0.2085434349916749 valid 0.2754538850925809
LOSS train 0.2085434349916749 valid 0.2754557569073947
LOSS train 0.2085434349916749 valid 0.27542108792279446
LOSS train 0.2085434349916749 valid 0.27530991628076684
LOSS train 0.2085434349916749 valid 0.275132472897675
LOSS train 0.2085434349916749 valid 0.2750722711688638
LOSS train 0.2085434349916749 valid 0.2751666057172795
LOSS train 0.2085434349916749 valid 0.2753464140912943
LOSS train 0.2085434349916749 valid 0.2752837578837688
LOSS train 0.2085434349916749 valid 0.2752141292606081
LOSS train 0.2085434349916749 valid 0.2751989477934937
LOSS train 0.2085434349916749 valid 0.27532912532969744
LOSS train 0.2085434349916749 valid 0.27538171898702096
LOSS train 0.2085434349916749 valid 0.2753601783432092
LOSS train 0.2085434349916749 valid 0.27545765527103044
LOSS train 0.2085434349916749 valid 0.2754241000771929
LOSS train 0.2085434349916749 valid 0.2755579638744698
LOSS train 0.2085434349916749 valid 0.275533170568741
LOSS train 0.2085434349916749 valid 0.2754037047459467
LOSS train 0.2085434349916749 valid 0.2754391154355874
LOSS train 0.2085434349916749 valid 0.2754544665249402
LOSS train 0.2085434349916749 valid 0.2754409213030219
LOSS train 0.2085434349916749 valid 0.27537545636296273
LOSS train 0.2085434349916749 valid 0.27538226089406254
LOSS train 0.2085434349916749 valid 0.27540755286714097
LOSS train 0.2085434349916749 valid 0.27554157538579244
LOSS train 0.2085434349916749 valid 0.27554523234108563
LOSS train 0.2085434349916749 valid 0.27541457878761605
LOSS train 0.2085434349916749 valid 0.275516797855399
LOSS train 0.2085434349916749 valid 0.2753729826076023
LOSS train 0.2085434349916749 valid 0.27530129128075265
LOSS train 0.2085434349916749 valid 0.27519287940560805
LOSS train 0.2085434349916749 valid 0.2751820826242047
LOSS train 0.2085434349916749 valid 0.27505298193627997
LOSS train 0.2085434349916749 valid 0.27498613737332517
LOSS train 0.2085434349916749 valid 0.2749118497386908
LOSS train 0.2085434349916749 valid 0.2749445935723129
LOSS train 0.2085434349916749 valid 0.27493890363072593
LOSS train 0.2085434349916749 valid 0.27490706100494045
LOSS train 0.2085434349916749 valid 0.27493370688676083
LOSS train 0.2085434349916749 valid 0.2750392594614869
LOSS train 0.2085434349916749 valid 0.27516084853384565
LOSS train 0.2085434349916749 valid 0.27512301364913583
LOSS train 0.2085434349916749 valid 0.2754175093872154
LOSS train 0.2085434349916749 valid 0.27525005929218316
LOSS train 0.2085434349916749 valid 0.2750922879751991
LOSS train 0.2085434349916749 valid 0.275062053107921
LOSS train 0.2085434349916749 valid 0.2751761374106774
LOSS train 0.2085434349916749 valid 0.2751776361209483
LOSS train 0.2085434349916749 valid 0.2752402462179143
LOSS train 0.2085434349916749 valid 0.27527028776523543
LOSS train 0.2085434349916749 valid 0.27539772510890903
LOSS train 0.2085434349916749 valid 0.2754189313361139
LOSS train 0.2085434349916749 valid 0.27528721503800857
LOSS train 0.2085434349916749 valid 0.275196534054107
LOSS train 0.2085434349916749 valid 0.27520302546632897
LOSS train 0.2085434349916749 valid 0.2753303791591507
LOSS train 0.2085434349916749 valid 0.27522750977259963
LOSS train 0.2085434349916749 valid 0.27503397679399877
LOSS train 0.2085434349916749 valid 0.2750042118905914
LOSS train 0.2085434349916749 valid 0.27487919713265796
LOSS train 0.2085434349916749 valid 0.27481308087135137
LOSS train 0.2085434349916749 valid 0.2748303012812839
LOSS train 0.2085434349916749 valid 0.27471335341503894
LOSS train 0.2085434349916749 valid 0.27465964784050545
LOSS train 0.2085434349916749 valid 0.27458786312762223
LOSS train 0.2085434349916749 valid 0.2747633662507978
LOSS train 0.2085434349916749 valid 0.27483770329019297
LOSS train 0.2085434349916749 valid 0.27473485370764156
LOSS train 0.2085434349916749 valid 0.27457296577757306
LOSS train 0.2085434349916749 valid 0.27449685887529934
LOSS train 0.2085434349916749 valid 0.274534823773242
LOSS train 0.2085434349916749 valid 0.2744274240306446
LOSS train 0.2085434349916749 valid 0.274442714631048
LOSS train 0.2085434349916749 valid 0.27443010796030815
LOSS train 0.2085434349916749 valid 0.2743696464441991
LOSS train 0.2085434349916749 valid 0.2743828936064311
LOSS train 0.2085434349916749 valid 0.2745203567642561
LOSS train 0.2085434349916749 valid 0.27466425111287096
LOSS train 0.2085434349916749 valid 0.27470624542870775
LOSS train 0.2085434349916749 valid 0.27464592194590487
LOSS train 0.2085434349916749 valid 0.2746804952206386
LOSS train 0.2085434349916749 valid 0.27461321060028343
LOSS train 0.2085434349916749 valid 0.2745895838555867
LOSS train 0.2085434349916749 valid 0.2747621004835018
LOSS train 0.2085434349916749 valid 0.27459801868958905
LOSS train 0.2085434349916749 valid 0.2746136996608514
LOSS train 0.2085434349916749 valid 0.2747081083794163
LOSS train 0.2085434349916749 valid 0.2746252212368074
LOSS train 0.2085434349916749 valid 0.27447785985242446
LOSS train 0.2085434349916749 valid 0.27455456935517164
LOSS train 0.2085434349916749 valid 0.2747019014061305
Training bichrom
DEVICE = cpu
####################
Total Parameters = 606342
Total Trainable Parameters = 1157
bimodal_network(
  (base_model): bichrom_seq(
    (conv1d): Conv1d(4, 256, kernel_size=(24,), stride=(1,))
    (relu): ReLU()
    (batchNorm1d): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (maxPool1d): MaxPool1d(kernel_size=15, stride=15, padding=0, dilation=1, ceil_mode=True)
    (lstm): LSTM(256, 32, batch_first=True)
    (tanh): Tanh()
    (model_dense_repeat): Sequential(
      (0): Linear(in_features=32, out_features=512, bias=True)
      (1): ReLU()
      (2): Dropout(p=0.5, inplace=False)
      (3): Linear(in_features=512, out_features=512, bias=True)
      (4): ReLU()
      (5): Dropout(p=0.5, inplace=False)
      (6): Linear(in_features=512, out_features=512, bias=True)
      (7): ReLU()
      (8): Dropout(p=0.5, inplace=False)
    )
    (linear): Linear(in_features=512, out_features=1, bias=True)
    (sigmoid): Sigmoid()
  )
  (linear): Linear(in_features=512, out_features=1, bias=True)
  (tanh): Tanh()
  (model): bichrom_chrom(
    (_reshape): _reshape()
    (conv1d): Conv1d(12, 15, kernel_size=(1,), stride=(1,), padding=valid)
    (relu): ReLU()
    (lstm): LSTM(15, 5, batch_first=True)
    (relu2): ReLU()
    (linear): Linear(in_features=5, out_features=1, bias=True)
    (tanh): Tanh()
  )
  (linear2): Linear(in_features=2, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)
base_model.conv1d.weight False
base_model.conv1d.bias False
base_model.batchNorm1d.weight False
base_model.batchNorm1d.bias False
base_model.lstm.weight_ih_l0 False
base_model.lstm.weight_hh_l0 False
base_model.lstm.bias_ih_l0 False
base_model.lstm.bias_hh_l0 False
base_model.model_dense_repeat.0.weight False
base_model.model_dense_repeat.0.bias False
base_model.model_dense_repeat.3.weight False
base_model.model_dense_repeat.3.bias False
base_model.model_dense_repeat.6.weight False
base_model.model_dense_repeat.6.bias False
base_model.linear.weight False
base_model.linear.bias False
linear.weight True
linear.bias True
model.conv1d.weight True
model.conv1d.bias True
model.lstm.weight_ih_l0 True
model.lstm.weight_hh_l0 True
model.lstm.bias_ih_l0 True
model.lstm.bias_hh_l0 True
model.linear.weight True
model.linear.bias True
linear2.weight True
linear2.bias True
####################
EPOCH 1:
  batch 1 loss: 0.7358123660087585
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.6908904612064362
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.6493513584136963
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.6183043122291565
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.5971921205520629
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.580206165711085
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.566474369594029
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.5559940859675407
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.5463327136304643
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.5381122797727584
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.5303270735523917
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.5240218664209048
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.5177020659813514
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.5126095158713204
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.5068399210770925
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.5014539733529091
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.4969480458427878
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.4931988368431727
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.4895747131422946
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.4852763622999191
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.4821443387440273
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.47898650711232965
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.47616972353147424
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.4737301630278428
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.47048897624015806
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.46782492101192474
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.46456013675089236
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.46110966375895907
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.457777280232002
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.4548621227343877
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.4524438092785497
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.45021586678922176
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.4464841214093295
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.4448314940228182
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.4434313007763454
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.4409210822648472
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.4387634952326079
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.43606271477122055
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.4336848518787286
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.4314044497907162
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.42931257224664454
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.427430849699747
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.42620020896889443
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.4236839826811444
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.42177807887395224
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.4196581101935843
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.41757888870036347
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.416007824242115
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.41429736297957753
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.4122810614109039
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.4105228752482171
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.40849565943846333
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.4069671332836151
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.4051620485606017
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.4037477747960524
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.40181289134281023
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.4004239246510623
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.3983761575715295
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.39737300499010897
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.39604537735382717
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.3949845003300026
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.39343854784965515
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.3921079441668495
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.3905247487127781
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.3894248756078573
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.3880452527241273
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.3869123361003933
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.3857943998540149
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.38451027179109876
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.38367889523506166
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.3826958948457745
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.3815774574047989
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.3804461841713892
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.37922428991343526
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.37832595864931745
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.3771874520339464
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.3761136961447728
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.3748884055859003
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.374163419008255
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.3731429733335972
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.372228134561468
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.37101846460889026
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.3699826479676258
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.3689781225153378
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.36815991401672366
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.36701052445311877
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.36598504959851846
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.3651919331062924
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.3641867018147801
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.3635040081209607
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.36288839545878737
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.36190632503965625
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.36125148272001617
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.3606675536708629
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.35999660868393746
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.3590657766908407
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.35789002263054404
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.35705365681526613
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.3564320524232556
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.35558757320046425
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.35472574490721864
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.35403888175884884
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.35347004987082437
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.3526193523922792
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.35164208795343127
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.35087238631720813
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.3502541280516954
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.34965196155287603
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.3490660916501229
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.3484754579988393
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.34752176366410814
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.34673157068235533
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.3459282013694797
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.3456438429522933
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.3447824079057445
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.34402863707008036
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.34362760288083655
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.3428941356428599
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.34223740516590473
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.34141638576984407
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.3408181489991748
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.34009697349345097
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.33963663112826464
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.33897093779617743
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.33855719232559206
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.33815348242956494
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.3375673899500389
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.3368761364836246
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.3362446436586306
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.3357373315554399
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.33506134345786265
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.3343815050567641
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.33394258217255873
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.3335373391633603
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.3329641400663941
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.33253261151120944
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.33234793241441685
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.33187803701646085
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.3315865602210271
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.33111496259059225
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.33075506668141547
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.3300297534171964
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.3297399089678184
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.32947838503039545
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.328956817861261
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.32824892173074693
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.32769228933619804
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.32719533749528834
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.3268513817515149
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.32648924668629964
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.3261184808828973
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.3256296352728417
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.3250204127014073
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.3244413614466593
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.3241036371838662
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.32372807472562176
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.32329998910427094
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.3229085602149179
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.3227610062315779
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.3223943023942411
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.32196430177051827
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.32150142419117467
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.3211624180795225
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.3209483004743006
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.3205113059643543
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.3199501289840204
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.3195032869092005
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.31920272581988857
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.3188523220769047
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.3187153392854859
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.3182716651095284
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.31775086208484893
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.3171354464196056
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.31665074696828577
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.31630190636430466
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.31580048105256125
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.31541001476810476
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.31484784169143504
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.3147189355762311
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.31425396195716326
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.31395425412865635
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.3136754261104615
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.3132074357219081
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.3128842642611783
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.312542224333093
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.31210274257326637
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.31175856985510353
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.31137865076356747
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.3110566914239258
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.3106197158757009
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.310332966802632
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.3098648397717625
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.30964494407794635
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.3091668679388528
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.3088708474850043
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.3087468858124042
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.30857214853545734
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.30831850816806156
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.3080531218692885
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.3078298484534025
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.30749839358958436
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.3071665175923026
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.30692628253563287
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.30658384974972874
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.3063205004465289
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.3059395612443535
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.30575743922288867
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.30517391896305174
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.3047716459303952
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.3046214214393071
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.3042670861945898
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.3040038632615557
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.3038560927194049
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.3034201307552997
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.3031036581410918
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.3026677156212153
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.30234645911350777
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.3021760184420358
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.3019698958282601
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.30170095278458164
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.3014623483651364
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.3011697334077981
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.30082088327995865
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.3006211316346058
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.30029155141777464
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.2999269057032281
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.2996469994079699
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.29926798459992077
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.29900053579474123
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.29887624946625335
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.2986592621385277
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.2982982138878313
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.29814233762000253
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.2979399445984099
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.2977421203826336
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.2975095016723972
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.29732337367685535
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.29712587181760486
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.2969080267092174
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.2965322396407525
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.2962480934947358
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.2960168493311267
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.2957744311403345
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.29549332689799246
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.2952316824878965
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.29513700578997776
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.29493023666293033
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.29468171893348616
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.2945133138253507
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.29423588663339617
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.2940795612168977
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.29387173594699967
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.2937086117950824
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.2934237559597323
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.29321262106007223
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.292975869087968
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.2926528016648868
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.29236714593892876
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.29221384641516623
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.29203814388467714
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.2918615372016512
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.2915433021333381
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.29128111808245627
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.2908959329466928
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.29060161288054487
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.2905043061626585
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.29030384073096716
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.29012310160184973
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.289754260095965
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.2895054802850441
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.28924227074283515
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.28900858940666213
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.28880012848656694
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.2884718669156959
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.2881370202519677
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.28806339609234227
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.28786253186769867
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.28775366100904753
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.2875481937414429
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.2872708598417895
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.28701417715524014
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.2867970993966921
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.2864733278330139
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.2861965796057607
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.2860107441220367
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.28585982109074826
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.28557768656194005
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.28532689282049734
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.2851152821189392
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.28489616991117084
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.28468515956934376
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.2845887289675948
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.28446924299917126
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.28425931819036704
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.284116560422768
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.2839500570317378
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.2837119935337542
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.28344392516468997
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.28311881811722467
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.2829053354760011
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.2827131334333325
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.28247156527066075
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.28229484832522894
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.282058246237667
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.2818865416968455
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.2816669929748267
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.2815154613229273
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.28126349817816315
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.2810543220502273
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.28081025486030886
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.2806463010633107
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.2804238417973885
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.28013311331264507
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.27986404817005633
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.2796896390971683
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.2795729510297504
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.27938534737761467
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.27917457442238647
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.27891307700203505
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.2787788230460137
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.278548724647623
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.2782674022713063
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.278095510445143
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.27788218800668363
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.2776720986457971
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.27741703555627834
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.2772208542451946
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.277088514824466
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.27686579878750545
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.27680630282019125
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.2767275743344039
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.2765847109258175
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.2763395189522981
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.27616648375988007
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.2760052724560695
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.2758304728195071
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.275604689598791
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.2754085559464065
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.2751764716651939
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.27499489744796474
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.274839336396662
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.2748415913672475
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.2746988752145461
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.2745643669609414
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.274357791568922
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.2740644623520057
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.2738912620444806
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.27366254979680327
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.2734171918948264
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.2731392377614975
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.2729703632430134
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.27282336743717844
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.2726774636561067
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.27248418360611815
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.2723498581160962
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.2721182601003165
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.2719355073212242
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.2718175052347796
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.27154353148923943
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.2713193494826555
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.2711020099241648
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.2709320791024529
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.27072144917548524
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.2705018955927629
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.27034528377937944
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.270162027919553
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.26999287230117447
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.2699026230155774
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.26972856919778393
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.26954521105901613
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.2693512218419432
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.2691303312137563
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.268999334234974
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.2688742809117159
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.2686899273395538
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.26858592108684654
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.2684510751807721
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.2683167990553316
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.2681331878287182
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.26794218675870646
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.2677381990228112
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.2675434360719476
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.2674065334600817
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.26721245089235407
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.26699201046646415
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.26680701666082124
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.26668393769775556
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.26649917420191865
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.26629501562038854
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.26611932733884225
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.26595059425934503
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.2657565932188715
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.2656370524231714
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.2654506424205557
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.2653427472597436
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.2651413469528309
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.26488092312734735
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.26476433577399755
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.2645532067183564
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.26439549643546345
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.264209221463251
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.2640217557104666
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.26387022590962594
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.26374999861611
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.26358742202505653
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.2634057565936314
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.26328400392497203
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.2631308000315638
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.26295865299794957
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.2628210086284614
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.2626281251582496
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.2625229412201539
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.2623668928195432
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.2622749881395971
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.26215326599327915
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.26204998261080337
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.2619321120442818
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.261747306471236
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.26164570742404547
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.2614389425232297
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.2613634479159131
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.26123349573374927
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.26100301143689075
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.26082324830569187
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.26067234975450176
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.2604413438990642
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.2602571504456656
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.26006419907941997
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.25981615574070904
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.2595843109627103
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.25948417940438484
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.25928514781925416
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.2590774169350882
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.258988991826086
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.25878101868190984
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.2587352057730933
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.25865835677704496
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.25846238148539036
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.2582184612615233
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.2580698213455352
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.2579353141541384
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.2577654735534979
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.25762228765283157
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.2575082221479566
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.25741185294778157
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.2572927213369998
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.25710409906353193
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.2570049123439406
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.2568647960711163
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.2566713887453079
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.256480635335863
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.25639596640799955
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.25619670715816234
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.2560356707037283
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.2558550407925805
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.2556904283466569
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.25554605506572475
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.25545015163296697
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.2552810502208136
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.2551852559265883
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.2550440715734974
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.25493393364277755
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.2547397814674975
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.25463395800184585
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.25451124528402924
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.2543742163626421
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.2542261320467424
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.2541463392603601
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.2540205128030228
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.2539031269702506
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.2538019736671144
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.25370461866259575
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.25370461866259575 valid 0.2388806939125061
LOSS train 0.25370461866259575 valid 0.22756628692150116
LOSS train 0.25370461866259575 valid 0.22216690083344778
LOSS train 0.25370461866259575 valid 0.21756235137581825
LOSS train 0.25370461866259575 valid 0.21454570293426514
LOSS train 0.25370461866259575 valid 0.2225282589594523
LOSS train 0.25370461866259575 valid 0.23191574215888977
LOSS train 0.25370461866259575 valid 0.2295387163758278
LOSS train 0.25370461866259575 valid 0.22844740251700082
LOSS train 0.25370461866259575 valid 0.23046514242887498
LOSS train 0.25370461866259575 valid 0.23028577999635177
LOSS train 0.25370461866259575 valid 0.23022524267435074
LOSS train 0.25370461866259575 valid 0.22977454845721906
LOSS train 0.25370461866259575 valid 0.2291689089366368
LOSS train 0.25370461866259575 valid 0.22587132255236309
LOSS train 0.25370461866259575 valid 0.22642838209867477
LOSS train 0.25370461866259575 valid 0.22704988805686727
LOSS train 0.25370461866259575 valid 0.22802295121881697
LOSS train 0.25370461866259575 valid 0.22974124864528053
LOSS train 0.25370461866259575 valid 0.22894039303064345
LOSS train 0.25370461866259575 valid 0.22845443728424253
LOSS train 0.25370461866259575 valid 0.22687967866659164
LOSS train 0.25370461866259575 valid 0.2270588933125786
LOSS train 0.25370461866259575 valid 0.2263990187396606
LOSS train 0.25370461866259575 valid 0.22575759947299956
LOSS train 0.25370461866259575 valid 0.22551955855809724
LOSS train 0.25370461866259575 valid 0.22540107921317773
LOSS train 0.25370461866259575 valid 0.22563827995743072
LOSS train 0.25370461866259575 valid 0.22546460916256084
LOSS train 0.25370461866259575 valid 0.2264776239792506
LOSS train 0.25370461866259575 valid 0.22694615058360562
LOSS train 0.25370461866259575 valid 0.2261199983768165
LOSS train 0.25370461866259575 valid 0.22710763279235724
LOSS train 0.25370461866259575 valid 0.22678205546210795
LOSS train 0.25370461866259575 valid 0.22771029898098538
LOSS train 0.25370461866259575 valid 0.2275413316157129
LOSS train 0.25370461866259575 valid 0.22778734081500285
LOSS train 0.25370461866259575 valid 0.22856624189176059
LOSS train 0.25370461866259575 valid 0.22813933934920874
LOSS train 0.25370461866259575 valid 0.22794750854372978
LOSS train 0.25370461866259575 valid 0.22865400154416154
LOSS train 0.25370461866259575 valid 0.2289586304908707
LOSS train 0.25370461866259575 valid 0.22877577536327895
LOSS train 0.25370461866259575 valid 0.22916212373159148
LOSS train 0.25370461866259575 valid 0.22869132757186889
LOSS train 0.25370461866259575 valid 0.22899526325256928
LOSS train 0.25370461866259575 valid 0.22937476381342461
LOSS train 0.25370461866259575 valid 0.2295582192018628
LOSS train 0.25370461866259575 valid 0.22987852169542897
LOSS train 0.25370461866259575 valid 0.22911847174167632
LOSS train 0.25370461866259575 valid 0.22915660487670525
LOSS train 0.25370461866259575 valid 0.22880621942190024
LOSS train 0.25370461866259575 valid 0.22890612848524777
LOSS train 0.25370461866259575 valid 0.2289312595018634
LOSS train 0.25370461866259575 valid 0.22870737910270691
LOSS train 0.25370461866259575 valid 0.22789352041270053
LOSS train 0.25370461866259575 valid 0.22756529560214594
LOSS train 0.25370461866259575 valid 0.22723167723622814
LOSS train 0.25370461866259575 valid 0.22747069546731852
LOSS train 0.25370461866259575 valid 0.22754499365886052
LOSS train 0.25370461866259575 valid 0.22728417251930863
LOSS train 0.25370461866259575 valid 0.22776617590458162
LOSS train 0.25370461866259575 valid 0.227391580267558
LOSS train 0.25370461866259575 valid 0.22820846689864993
LOSS train 0.25370461866259575 valid 0.2284469370658581
LOSS train 0.25370461866259575 valid 0.2283971600911834
LOSS train 0.25370461866259575 valid 0.2276385335335091
LOSS train 0.25370461866259575 valid 0.22772249272641013
LOSS train 0.25370461866259575 valid 0.22736628979876422
LOSS train 0.25370461866259575 valid 0.22746978657586234
LOSS train 0.25370461866259575 valid 0.22718943046851897
LOSS train 0.25370461866259575 valid 0.2274104712737931
LOSS train 0.25370461866259575 valid 0.22742675959247433
LOSS train 0.25370461866259575 valid 0.22749534007665273
LOSS train 0.25370461866259575 valid 0.22772608399391175
LOSS train 0.25370461866259575 valid 0.22812284646849884
LOSS train 0.25370461866259575 valid 0.22824901303687653
LOSS train 0.25370461866259575 valid 0.22827165440107003
LOSS train 0.25370461866259575 valid 0.22815829968150658
LOSS train 0.25370461866259575 valid 0.2275245115160942
LOSS train 0.25370461866259575 valid 0.22694152428044212
LOSS train 0.25370461866259575 valid 0.22741112930745613
LOSS train 0.25370461866259575 valid 0.22725595085017652
LOSS train 0.25370461866259575 valid 0.22705288302330745
LOSS train 0.25370461866259575 valid 0.22705732583999633
LOSS train 0.25370461866259575 valid 0.22661717062772707
LOSS train 0.25370461866259575 valid 0.22641434066597071
LOSS train 0.25370461866259575 valid 0.2260043334893205
LOSS train 0.25370461866259575 valid 0.22648494514856446
LOSS train 0.25370461866259575 valid 0.22640830328067144
LOSS train 0.25370461866259575 valid 0.22638922323892405
LOSS train 0.25370461866259575 valid 0.22657853089596913
LOSS train 0.25370461866259575 valid 0.22647227266783354
LOSS train 0.25370461866259575 valid 0.22656194263316215
LOSS train 0.25370461866259575 valid 0.2264603448541541
LOSS train 0.25370461866259575 valid 0.22669707844033837
LOSS train 0.25370461866259575 valid 0.22673198334949532
LOSS train 0.25370461866259575 valid 0.22685160563916576
LOSS train 0.25370461866259575 valid 0.22700503784598727
LOSS train 0.25370461866259575 valid 0.2270883022248745
LOSS train 0.25370461866259575 valid 0.2272616815153915
LOSS train 0.25370461866259575 valid 0.22747437495226955
LOSS train 0.25370461866259575 valid 0.2273057502450295
LOSS train 0.25370461866259575 valid 0.22732128088290876
LOSS train 0.25370461866259575 valid 0.22740797088259743
LOSS train 0.25370461866259575 valid 0.22766455895495866
LOSS train 0.25370461866259575 valid 0.22754647241574583
LOSS train 0.25370461866259575 valid 0.22771575867577834
LOSS train 0.25370461866259575 valid 0.22798849532910442
LOSS train 0.25370461866259575 valid 0.2281863568858667
LOSS train 0.25370461866259575 valid 0.22791622297183886
LOSS train 0.25370461866259575 valid 0.22785400253321444
LOSS train 0.25370461866259575 valid 0.22788615340152674
LOSS train 0.25370461866259575 valid 0.2279007046630508
LOSS train 0.25370461866259575 valid 0.22797081846257913
LOSS train 0.25370461866259575 valid 0.22789341917839542
LOSS train 0.25370461866259575 valid 0.22801444889643255
LOSS train 0.25370461866259575 valid 0.22783671521534354
LOSS train 0.25370461866259575 valid 0.2276485490949214
LOSS train 0.25370461866259575 valid 0.22737344056367875
LOSS train 0.25370461866259575 valid 0.22723875188630474
LOSS train 0.25370461866259575 valid 0.22717895798507284
LOSS train 0.25370461866259575 valid 0.2269871953299375
LOSS train 0.25370461866259575 valid 0.2272825749410737
LOSS train 0.25370461866259575 valid 0.22713685190677643
LOSS train 0.25370461866259575 valid 0.22743359669333413
LOSS train 0.25370461866259575 valid 0.22733429226818985
LOSS train 0.25370461866259575 valid 0.22760531271342188
LOSS train 0.25370461866259575 valid 0.22771251178527063
LOSS train 0.25370461866259575 valid 0.22757143928454474
LOSS train 0.25370461866259575 valid 0.2275051381296784
LOSS train 0.25370461866259575 valid 0.22727487349148953
LOSS train 0.25370461866259575 valid 0.22728075187905392
LOSS train 0.25370461866259575 valid 0.2273698365732805
LOSS train 0.25370461866259575 valid 0.22724386464666438
LOSS train 0.25370461866259575 valid 0.22721772816251307
LOSS train 0.25370461866259575 valid 0.22703134447988801
LOSS train 0.25370461866259575 valid 0.2269684176730073
LOSS train 0.25370461866259575 valid 0.22684439051923133
LOSS train 0.25370461866259575 valid 0.22693176695278713
LOSS train 0.25370461866259575 valid 0.22687645288223915
LOSS train 0.25370461866259575 valid 0.22704849297731694
LOSS train 0.25370461866259575 valid 0.2269954833534214
LOSS train 0.25370461866259575 valid 0.22698848052985138
LOSS train 0.25370461866259575 valid 0.22679447058973642
LOSS train 0.25370461866259575 valid 0.2270158633385619
LOSS train 0.25370461866259575 valid 0.22698320316619613
LOSS train 0.25370461866259575 valid 0.22755953246677244
LOSS train 0.25370461866259575 valid 0.2276101269377958
LOSS train 0.25370461866259575 valid 0.22762996405363084
LOSS train 0.25370461866259575 valid 0.22771394065279044
LOSS train 0.25370461866259575 valid 0.2274776243261601
LOSS train 0.25370461866259575 valid 0.22756016049899308
LOSS train 0.25370461866259575 valid 0.22743213167051216
LOSS train 0.25370461866259575 valid 0.22744341069652188
LOSS train 0.25370461866259575 valid 0.22754814189213973
LOSS train 0.25370461866259575 valid 0.22755551622931364
LOSS train 0.25370461866259575 valid 0.22762926460444172
LOSS train 0.25370461866259575 valid 0.2276662253546265
LOSS train 0.25370461866259575 valid 0.22762134298682213
LOSS train 0.25370461866259575 valid 0.22755336557856257
LOSS train 0.25370461866259575 valid 0.22744890771530293
LOSS train 0.25370461866259575 valid 0.22727379641649914
LOSS train 0.25370461866259575 valid 0.22709181386886573
LOSS train 0.25370461866259575 valid 0.22698797782262167
LOSS train 0.25370461866259575 valid 0.2270757713949824
LOSS train 0.25370461866259575 valid 0.22732896326544755
LOSS train 0.25370461866259575 valid 0.22726288313666979
LOSS train 0.25370461866259575 valid 0.2273789772973258
LOSS train 0.25370461866259575 valid 0.22745511715903002
LOSS train 0.25370461866259575 valid 0.22747201834157196
LOSS train 0.25370461866259575 valid 0.22736641192852064
LOSS train 0.25370461866259575 valid 0.22747533264532255
LOSS train 0.25370461866259575 valid 0.22746604955059357
LOSS train 0.25370461866259575 valid 0.2272059312037059
LOSS train 0.25370461866259575 valid 0.22715422790497541
LOSS train 0.25370461866259575 valid 0.2272392550935853
LOSS train 0.25370461866259575 valid 0.22740133918738098
LOSS train 0.25370461866259575 valid 0.22740016511365688
LOSS train 0.25370461866259575 valid 0.22730008719695938
LOSS train 0.25370461866259575 valid 0.2274427353841824
LOSS train 0.25370461866259575 valid 0.22742298859488833
LOSS train 0.25370461866259575 valid 0.227459117770195
LOSS train 0.25370461866259575 valid 0.22739582735559213
LOSS train 0.25370461866259575 valid 0.2271938182212211
LOSS train 0.25370461866259575 valid 0.22723854974072466
LOSS train 0.25370461866259575 valid 0.22711786858856997
LOSS train 0.25370461866259575 valid 0.22709344937763315
LOSS train 0.25370461866259575 valid 0.22705572630677903
LOSS train 0.25370461866259575 valid 0.22713487187498493
LOSS train 0.25370461866259575 valid 0.22710070006198285
LOSS train 0.25370461866259575 valid 0.22714304784312844
LOSS train 0.25370461866259575 valid 0.2269903559604457
LOSS train 0.25370461866259575 valid 0.22689612378779145
LOSS train 0.25370461866259575 valid 0.22677802856151874
LOSS train 0.25370461866259575 valid 0.22678031597514542
LOSS train 0.25370461866259575 valid 0.22700297325698252
LOSS train 0.25370461866259575 valid 0.22694026355189506
LOSS train 0.25370461866259575 valid 0.2270233509828098
LOSS train 0.25370461866259575 valid 0.22687152869999408
LOSS train 0.25370461866259575 valid 0.22670680469837948
LOSS train 0.25370461866259575 valid 0.22665058865700619
LOSS train 0.25370461866259575 valid 0.22659402872834886
LOSS train 0.25370461866259575 valid 0.22663774911095114
LOSS train 0.25370461866259575 valid 0.22642855796872116
LOSS train 0.25370461866259575 valid 0.2264805196703059
LOSS train 0.25370461866259575 valid 0.2264548049888749
LOSS train 0.25370461866259575 valid 0.22629591930084503
LOSS train 0.25370461866259575 valid 0.22621672411569568
LOSS train 0.25370461866259575 valid 0.22624151905377707
LOSS train 0.25370461866259575 valid 0.22630928117799534
LOSS train 0.25370461866259575 valid 0.22630351015700484
LOSS train 0.25370461866259575 valid 0.2263307767294942
LOSS train 0.25370461866259575 valid 0.22634474383057834
LOSS train 0.25370461866259575 valid 0.22624769300915473
LOSS train 0.25370461866259575 valid 0.22621242543337522
LOSS train 0.25370461866259575 valid 0.22620988462103128
LOSS train 0.25370461866259575 valid 0.22631595680199632
LOSS train 0.25370461866259575 valid 0.22636294119978603
LOSS train 0.25370461866259575 valid 0.22644561271775854
LOSS train 0.25370461866259575 valid 0.22642963376250203
LOSS train 0.25370461866259575 valid 0.22646207644327268
LOSS train 0.25370461866259575 valid 0.22659059309905955
LOSS train 0.25370461866259575 valid 0.22663841476397856
LOSS train 0.25370461866259575 valid 0.22664708031548395
LOSS train 0.25370461866259575 valid 0.22676549021121675
LOSS train 0.25370461866259575 valid 0.22692384462524615
LOSS train 0.25370461866259575 valid 0.22702935821654505
LOSS train 0.25370461866259575 valid 0.2270968145158093
LOSS train 0.25370461866259575 valid 0.22712735248648602
LOSS train 0.25370461866259575 valid 0.22714084593248574
LOSS train 0.25370461866259575 valid 0.22723310954611878
LOSS train 0.25370461866259575 valid 0.22729129270678425
LOSS train 0.25370461866259575 valid 0.2273050860589386
LOSS train 0.25370461866259575 valid 0.2274092565825645
LOSS train 0.25370461866259575 valid 0.2272915201798334
LOSS train 0.25370461866259575 valid 0.22731804778555778
LOSS train 0.25370461866259575 valid 0.2273048807342513
LOSS train 0.25370461866259575 valid 0.22720271174378973
LOSS train 0.25370461866259575 valid 0.22716317661106586
LOSS train 0.25370461866259575 valid 0.22725267974172886
LOSS train 0.25370461866259575 valid 0.22707790850607817
LOSS train 0.25370461866259575 valid 0.22719716461597647
LOSS train 0.25370461866259575 valid 0.22739820793026783
LOSS train 0.25370461866259575 valid 0.22753234408339676
LOSS train 0.25370461866259575 valid 0.2274853517490674
LOSS train 0.25370461866259575 valid 0.22760485148864237
LOSS train 0.25370461866259575 valid 0.227549085573804
LOSS train 0.25370461866259575 valid 0.227548206067947
LOSS train 0.25370461866259575 valid 0.22764388906955718
LOSS train 0.25370461866259575 valid 0.22757109999656677
LOSS train 0.25370461866259575 valid 0.22777735288181003
LOSS train 0.25370461866259575 valid 0.22774101228346466
LOSS train 0.25370461866259575 valid 0.22764406722830974
LOSS train 0.25370461866259575 valid 0.22765976371718388
LOSS train 0.25370461866259575 valid 0.22772534762043506
LOSS train 0.25370461866259575 valid 0.2276136403185848
LOSS train 0.25370461866259575 valid 0.22775050905323768
LOSS train 0.25370461866259575 valid 0.22773778933355707
LOSS train 0.25370461866259575 valid 0.22772385959441846
LOSS train 0.25370461866259575 valid 0.22780847931958706
LOSS train 0.25370461866259575 valid 0.22781412115761343
LOSS train 0.25370461866259575 valid 0.22782903654720393
LOSS train 0.25370461866259575 valid 0.22789637829092416
LOSS train 0.25370461866259575 valid 0.2278212493320681
LOSS train 0.25370461866259575 valid 0.22787580779172423
LOSS train 0.25370461866259575 valid 0.22792672792847238
LOSS train 0.25370461866259575 valid 0.2280847175019001
LOSS train 0.25370461866259575 valid 0.22816684360176215
LOSS train 0.25370461866259575 valid 0.22815305513364298
LOSS train 0.25370461866259575 valid 0.22818666570081042
LOSS train 0.25370461866259575 valid 0.2284114141126766
LOSS train 0.25370461866259575 valid 0.2284971142099017
LOSS train 0.25370461866259575 valid 0.22846200322582774
LOSS train 0.25370461866259575 valid 0.228464066602967
LOSS train 0.25370461866259575 valid 0.22832000995243806
LOSS train 0.25370461866259575 valid 0.2281435977143071
LOSS train 0.25370461866259575 valid 0.22799552333869522
LOSS train 0.25370461866259575 valid 0.22799411918100063
LOSS train 0.25370461866259575 valid 0.228011520419802
LOSS train 0.25370461866259575 valid 0.2278894328668024
LOSS train 0.25370461866259575 valid 0.22770468446802586
LOSS train 0.25370461866259575 valid 0.22765498858458583
LOSS train 0.25370461866259575 valid 0.22767241902544466
LOSS train 0.25370461866259575 valid 0.22774223465668528
LOSS train 0.25370461866259575 valid 0.22772265465317906
LOSS train 0.25370461866259575 valid 0.22766842166098153
LOSS train 0.25370461866259575 valid 0.22764867787352866
LOSS train 0.25370461866259575 valid 0.22766732545666216
LOSS train 0.25370461866259575 valid 0.22767119253503865
LOSS train 0.25370461866259575 valid 0.22759610598849267
LOSS train 0.25370461866259575 valid 0.22761923238022685
LOSS train 0.25370461866259575 valid 0.22764571941753298
LOSS train 0.25370461866259575 valid 0.22775246770608992
LOSS train 0.25370461866259575 valid 0.22779912271742092
LOSS train 0.25370461866259575 valid 0.22776733771771998
LOSS train 0.25370461866259575 valid 0.22777943579034773
LOSS train 0.25370461866259575 valid 0.2277562353735002
LOSS train 0.25370461866259575 valid 0.22781859243593885
LOSS train 0.25370461866259575 valid 0.22782799432675044
LOSS train 0.25370461866259575 valid 0.22782682832887402
LOSS train 0.25370461866259575 valid 0.22781340013079296
LOSS train 0.25370461866259575 valid 0.22790992628819873
LOSS train 0.25370461866259575 valid 0.22787054270309837
LOSS train 0.25370461866259575 valid 0.2278050356223935
LOSS train 0.25370461866259575 valid 0.2277907450604283
LOSS train 0.25370461866259575 valid 0.22774749101179043
LOSS train 0.25370461866259575 valid 0.227654453754038
LOSS train 0.25370461866259575 valid 0.227669525734815
LOSS train 0.25370461866259575 valid 0.22773936092853547
LOSS train 0.25370461866259575 valid 0.2277386128135816
LOSS train 0.25370461866259575 valid 0.2277321059925434
LOSS train 0.25370461866259575 valid 0.22780698709213695
LOSS train 0.25370461866259575 valid 0.22786432979213203
LOSS train 0.25370461866259575 valid 0.22780154569754524
LOSS train 0.25370461866259575 valid 0.22773608749237242
LOSS train 0.25370461866259575 valid 0.22776833848810346
LOSS train 0.25370461866259575 valid 0.2277946838136739
LOSS train 0.25370461866259575 valid 0.22789515171865685
LOSS train 0.25370461866259575 valid 0.22782896044664086
LOSS train 0.25370461866259575 valid 0.22792690727755288
LOSS train 0.25370461866259575 valid 0.22785775960417268
LOSS train 0.25370461866259575 valid 0.22782051618623292
LOSS train 0.25370461866259575 valid 0.22782677071697918
LOSS train 0.25370461866259575 valid 0.22781283681209272
LOSS train 0.25370461866259575 valid 0.22791274952376547
LOSS train 0.25370461866259575 valid 0.22794052612161783
LOSS train 0.25370461866259575 valid 0.22794783728696952
LOSS train 0.25370461866259575 valid 0.228002334845827
LOSS train 0.25370461866259575 valid 0.2279989036195206
LOSS train 0.25370461866259575 valid 0.22790449434536822
LOSS train 0.25370461866259575 valid 0.2277914029826601
LOSS train 0.25370461866259575 valid 0.2278241433717825
LOSS train 0.25370461866259575 valid 0.22797440492107482
LOSS train 0.25370461866259575 valid 0.2279169181834406
LOSS train 0.25370461866259575 valid 0.22790651648704494
LOSS train 0.25370461866259575 valid 0.22787464443229427
LOSS train 0.25370461866259575 valid 0.2278386465543826
LOSS train 0.25370461866259575 valid 0.2278256191264915
LOSS train 0.25370461866259575 valid 0.22780056275865612
LOSS train 0.25370461866259575 valid 0.2276931284896789
LOSS train 0.25370461866259575 valid 0.2277106865804795
LOSS train 0.25370461866259575 valid 0.2277158701645737
LOSS train 0.25370461866259575 valid 0.22789414803129296
LOSS train 0.25370461866259575 valid 0.2279690541650938
LOSS train 0.25370461866259575 valid 0.2279649709776647
LOSS train 0.25370461866259575 valid 0.22782469396975955
LOSS train 0.25370461866259575 valid 0.22777106946912304
LOSS train 0.25370461866259575 valid 0.2278133212751509
LOSS train 0.25370461866259575 valid 0.2277769905754498
LOSS train 0.25370461866259575 valid 0.22771218788420033
LOSS train 0.25370461866259575 valid 0.22771367511119356
LOSS train 0.25370461866259575 valid 0.22772351962648776
LOSS train 0.25370461866259575 valid 0.22777138898770014
LOSS train 0.25370461866259575 valid 0.22785821191861597
LOSS train 0.25370461866259575 valid 0.2278892139203093
LOSS train 0.25370461866259575 valid 0.2279026571776019
LOSS train 0.25370461866259575 valid 0.22786558506875065
LOSS train 0.25370461866259575 valid 0.22791395608263096
LOSS train 0.25370461866259575 valid 0.22785923079484038
LOSS train 0.25370461866259575 valid 0.22784236275753486
LOSS train 0.25370461866259575 valid 0.22790212048351435
LOSS train 0.25370461866259575 valid 0.22783960594947017
LOSS train 0.25370461866259575 valid 0.22787575984557906
LOSS train 0.25370461866259575 valid 0.22790268696334265
LOSS train 0.25370461866259575 valid 0.2278498107862603
LOSS train 0.25370461866259575 valid 0.22774684944821963
LOSS train 0.25370461866259575 valid 0.22775162890067566
LOSS train 0.25370461866259575 valid 0.2278294970753393
EPOCH 2:
  batch 1 loss: 0.1730082929134369
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.17321227490901947
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.1711504360040029
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.176975067704916
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.1768489807844162
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.17705084880193075
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.17514800599643163
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.17694171890616417
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.17691979143354628
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.17637054920196532
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.17506840825080872
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.17539818584918976
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.1742392537685541
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.1739170817392213
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.17266750236352285
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.17321057617664337
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.1723198908216813
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.17444524665673575
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.1746155337283486
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.17385369241237641
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.17438837389151254
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.1748673380775885
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.17541420459747314
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.17724275154372057
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.17728155076503754
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.17757598769206268
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.17767492598957485
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.17699155743632997
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.17665728515592113
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.17688976526260375
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.17747277550158963
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.17779575800523162
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.1764576660864281
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.17746766349848578
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.17907403154032572
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.1784882690343592
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.17856717270773811
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.1780647035492094
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.17796581601485228
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.178018207103014
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.17800031965825616
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.17808685593661808
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.17915926942991656
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.17839584838260303
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.1784972541862064
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.17793766862672308
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.17771888762078386
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.17781645525246859
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.17781777284583267
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.17775502800941467
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.17760697065615186
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.1773146175994323
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.1773284282886757
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.1770772037130815
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.17726491174914621
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.17712685811732495
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.1772489375189731
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.17663007229566574
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.17707126044620902
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.17704487492640814
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.17736967347684454
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.17704693516415934
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.17711387787546432
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.1768067351076752
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.176936896947714
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.1766965865637317
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.17659579795687946
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.1767045261667055
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.1767155063756998
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.17694082813603537
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.17725160382163357
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.177273608951105
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.17723653169527445
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.17708611548752398
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.17718045850594838
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.17711640208175308
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.17707852148390435
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.17669894011356893
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.17700483961195884
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.17717472910881044
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.1771680690624096
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.17679363307429524
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.17660581634705325
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.17667757347226143
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.17656507264165316
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.1765095579416253
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.1764842701711874
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.17661821960725568
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.17639702474803068
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.17660739802651934
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.17672192788386082
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.17651613121447357
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.17671297907188374
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.17664686844069907
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.1769781934587579
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.1769330439468225
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.1765670224870603
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.17641044012746032
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.17654970935498826
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.17639079287648202
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.1761760089066949
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.1759977656252244
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.17588715616939138
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.17588201809961063
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.17546927971499307
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.17542218501275442
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.17549981511084833
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.17536600789538137
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.17545961882543126
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.17556058019399642
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.17528974271572387
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.1751230283241187
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.17488331050999398
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.1751193822475902
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.17484223181786746
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.17470049061651888
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.17490735955727407
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.17481679545115617
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.1746513169853627
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.1744845699518919
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.1744564275357349
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.17439309006831685
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.17441816615864514
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.17427340606527944
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.17430981564521789
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.17436089307542832
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.17434601380130438
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.17407004837878048
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.173918912230536
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.17381591945886612
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.1736331072472434
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.17350741080713994
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.17362337811548906
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.17366076650014564
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.17341443256095604
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.1732651225345976
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.17342793386783042
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.17330843793309253
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.1734540932041278
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.17340518033930233
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.17337209895147498
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.1731110244569644
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.17320491821615847
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.17328795894152588
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.1731386820817816
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.17301783333085988
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.1729448286651754
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.17280013345786044
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.1729776208232713
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.1730192549029986
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.17292041494356875
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.17290970869362354
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.17279874586981106
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.17249162930559803
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.17254110593949595
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.17247845843816415
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.17247544950360705
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.17238682989455476
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.17250055421448354
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.17246884498745202
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.1724409446212816
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.17232048566694613
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.17234324613597496
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.1724479719087845
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.1724551240603129
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.17229264085910406
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.17213140517294764
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.17207921766454265
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.1719404480161046
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.17210034438792396
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.1720365755564985
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.17194300786007283
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.17168549814320713
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.17155447011363917
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.17162230670452117
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.17141992353241553
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.17123466957423647
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.17094404685698197
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.17115530962717598
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.17096673506829474
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.17097778610103037
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.1709563925371065
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.17077625603949437
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.17067223835898482
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.17067086334164078
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.17056223990455752
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.17053249860829847
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.1703849954015397
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.17035883657200626
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.1702913031766289
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.17023638709989516
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.17009475866022208
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.17003303743088183
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.1697634747347881
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.169728094797868
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.16983497036354883
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.1698799617399419
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.1699342920322611
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.16986211026134204
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.16991883620619774
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.16986954471661678
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.16981343895491988
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.16983187536300698
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.16980019598907115
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.1697292974809321
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.16956751504280035
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.16956056323316362
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.16931831851028478
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.1692832800332439
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.16938889367239815
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.16931937896244897
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.16928558325711288
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.16929463629431568
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.16913525398089507
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.16912628190461979
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.16895184127820861
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.1688849537328641
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.1689707730478103
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.16897968410356948
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.16892358965494417
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.16884431485676657
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.16880814650574247
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.1686907824914017
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.16871105167748673
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.16859474937121074
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.1685133776833526
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.16851641552826382
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.16829427429719976
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.16820991637925395
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.1683500600249871
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.16843322564513136
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.1682854708412598
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.1683148686082578
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.16830891861110672
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.16832717244929454
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.16831848101090577
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.16825848402856272
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.16826048311816544
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.16823734548800162
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.1681230265026291
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.1681548572180182
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.16810075372211203
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.1680934291808203
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.1680590750985458
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.16799832673705353
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.168042061532416
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.16795214316864246
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.1679651502880358
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.16796922246854468
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.16786236798763274
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.16794461421520113
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.16794979856127784
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.16791774102821652
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.1678537544537717
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.1677862224625606
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.16773735347669572
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.16764509254624407
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.16752738505601883
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.16755482824842902
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.16754335187948666
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.16748717290231552
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.1673914942577595
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.16731934493032244
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.16711143443756032
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.16705944318816346
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.16708703893691973
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.1670240865887774
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.16698131543486866
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.16683001483904827
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.1667832839268225
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.16671151896024541
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.1666310990557951
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.166654096472831
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.1665028743717792
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.16635763217102398
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.16647251285072687
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.16643284653928736
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.1664556877004157
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.16643367718411176
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.16637118091540679
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.16630411312461324
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.16623124897691374
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.16613643961323446
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.1660454456864948
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.16607927627730787
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.16614401486370114
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.1660298512995451
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.16602114194797146
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.16595502584450797
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.16589273507225102
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.16584761788959765
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.16593798663314074
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.16602925956249237
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.1660401353321108
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.16601142054897244
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.1659725253042337
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.16593002269565055
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.1658666891939688
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.16574757174903335
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.1657113960882028
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.16568287707048396
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.16565102970363288
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.16560165254589748
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.1655400460095782
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.16549919467480456
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.16546792025659599
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.16547651443302824
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.16537553618674156
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.16531528870071793
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.16517756509204065
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.16519816722900538
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.16513939880025694
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.16503796243248656
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.16498247377432076
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.16496812516734713
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.1650302707205845
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.1649740674894315
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.16495238118014247
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.16483496493867197
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.16486661927774549
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.1647807920926085
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.16468719434664117
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.16469265566706287
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.16466220127947537
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.16469810096117166
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.1646489847474303
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.16461700023314274
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.1646857344795291
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.1647150375651009
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.16477026736194436
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.16487439039790738
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.16488972876265826
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.1648106196442166
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.16478931930607665
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.16481184914930544
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.16480431608146145
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.16475402875540873
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.16472577493218982
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.16464058057045164
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.16460825099664575
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.1646135010677349
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.16480819937604213
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.16480783179793343
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.16482916461347147
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.16481520857499993
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.16474992661290086
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.16470565880925236
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.16465177335615816
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.16455866985642123
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.1644298541332994
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.16441423127539137
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.16446186807429927
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.1644861524444464
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.16448926906717026
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.1644690526832997
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.1643735131227903
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.1643862100918086
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.164417423994681
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.16431291905701326
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.1642497008252475
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.16418737497108465
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.16420728134927828
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.164159362863739
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.1640967457545
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.16409628760733017
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.16406268314992795
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.16406026598299556
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.1641129674187497
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.16407085904664787
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.16398134708807274
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.16397454794446092
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.1639141288776231
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.1638896086380565
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.16392680813364166
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.16389163480202357
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.1638963439085699
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.16387584953709686
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.16388964714117782
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.1638181243180285
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.1637567136436701
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.1637116716954652
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.1636381765427702
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.16367260619570007
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.16362460681314892
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.16352634397032972
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.16347274668707748
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.16347596555640223
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.16341979281266325
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.16335511224818414
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.16328620140751202
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.1632354247867299
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.1632113533702736
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.16323657153747767
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.16320675257877046
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.1632187760517567
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.1631499176863769
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.16302344242406433
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.16304122894328443
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.16297743749573715
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.16292340954765677
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.16286956556048476
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.1628173876794713
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.16277530347517052
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.16277385574977588
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.16277135928839814
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.16271800377172202
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.16269646209712696
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.1627045576734578
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.16265319981828588
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.162655676228971
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.16259845389719427
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.1626329084370032
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.1626170853256602
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.16265629852811495
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.16268049537776463
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.16272418836776453
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.16270952668192862
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.16264395093162093
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.16267663914363537
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.16259659182812486
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.16262390361899717
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.16263702379371883
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.16253407815598991
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.16250000684961396
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.16245042662410175
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.16233841723329584
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.1623067542282424
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.16224007028215956
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.1621396953548307
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.16206013106329498
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.16207502868778467
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.16200575646426943
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.16193422165808996
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.1619735464522366
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.16190065121513675
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.1619510178650738
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.16198337425901907
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.16192474076737007
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.16179895015743165
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.16176585661755366
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.16175172211023686
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.16169367735674478
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.16166329718537578
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.16163240663438766
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.16162356930167487
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.16162651999448446
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.1615738100486847
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.16159545598618155
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.1615588209842786
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.16151807114481925
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.1614598462857851
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.1614836293676522
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.1614012342711158
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.1613723694588907
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.16131054275965953
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.16129362352780605
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.16128796298276935
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.16130447831801972
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.1612657895372584
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.16129832058821036
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.16127699760643366
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.16126037255187572
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.16118107412662155
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.16119599745384064
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.16120314365753563
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.16120363469735235
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.16119822523131871
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.16124248897863758
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.1612567899863857
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.16125642823729108
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.1612871862679024
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.1613581898638
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.1613581898638 valid 0.2057798057794571
LOSS train 0.1613581898638 valid 0.19195245951414108
LOSS train 0.1613581898638 valid 0.18641558786233267
LOSS train 0.1613581898638 valid 0.18155449628829956
LOSS train 0.1613581898638 valid 0.17819803655147554
LOSS train 0.1613581898638 valid 0.18906433135271072
LOSS train 0.1613581898638 valid 0.19966774540288107
LOSS train 0.1613581898638 valid 0.19736514426767826
LOSS train 0.1613581898638 valid 0.19632904397116768
LOSS train 0.1613581898638 valid 0.1984093725681305
LOSS train 0.1613581898638 valid 0.19850292259996588
LOSS train 0.1613581898638 valid 0.19861927131811777
LOSS train 0.1613581898638 valid 0.19843652844429016
LOSS train 0.1613581898638 valid 0.197764946946076
LOSS train 0.1613581898638 valid 0.19498072763284047
LOSS train 0.1613581898638 valid 0.19544954877346754
LOSS train 0.1613581898638 valid 0.19650435097077312
LOSS train 0.1613581898638 valid 0.19616367585129207
LOSS train 0.1613581898638 valid 0.19825675613001773
LOSS train 0.1613581898638 valid 0.19801397249102592
LOSS train 0.1613581898638 valid 0.1975435877130145
LOSS train 0.1613581898638 valid 0.19618111645633524
LOSS train 0.1613581898638 valid 0.19600083387416342
LOSS train 0.1613581898638 valid 0.19572277553379536
LOSS train 0.1613581898638 valid 0.19490753710269929
LOSS train 0.1613581898638 valid 0.194645259815913
LOSS train 0.1613581898638 valid 0.19456163655828546
LOSS train 0.1613581898638 valid 0.19475817414266722
LOSS train 0.1613581898638 valid 0.19442502280761456
LOSS train 0.1613581898638 valid 0.1952084019780159
LOSS train 0.1613581898638 valid 0.1957371518496544
LOSS train 0.1613581898638 valid 0.19482072768732905
LOSS train 0.1613581898638 valid 0.19536474934130005
LOSS train 0.1613581898638 valid 0.1948636761483024
LOSS train 0.1613581898638 valid 0.1959996589592525
LOSS train 0.1613581898638 valid 0.19602025010519558
LOSS train 0.1613581898638 valid 0.19653292121113958
LOSS train 0.1613581898638 valid 0.19705017381592801
LOSS train 0.1613581898638 valid 0.1963880650508098
LOSS train 0.1613581898638 valid 0.19614971429109573
LOSS train 0.1613581898638 valid 0.19668735309344967
LOSS train 0.1613581898638 valid 0.19714648028214773
LOSS train 0.1613581898638 valid 0.19703597350175991
LOSS train 0.1613581898638 valid 0.19752168249000202
LOSS train 0.1613581898638 valid 0.1972888575659858
LOSS train 0.1613581898638 valid 0.19749894349471384
LOSS train 0.1613581898638 valid 0.1979589563734988
LOSS train 0.1613581898638 valid 0.19804982592662176
LOSS train 0.1613581898638 valid 0.19864368773236565
LOSS train 0.1613581898638 valid 0.19797707468271256
LOSS train 0.1613581898638 valid 0.19813925729078405
LOSS train 0.1613581898638 valid 0.1976483495762715
LOSS train 0.1613581898638 valid 0.19796681291652177
LOSS train 0.1613581898638 valid 0.1980619287049329
LOSS train 0.1613581898638 valid 0.19792055677283893
LOSS train 0.1613581898638 valid 0.19713461079767772
LOSS train 0.1613581898638 valid 0.19695619794360378
LOSS train 0.1613581898638 valid 0.19670249367582388
LOSS train 0.1613581898638 valid 0.19695567352286839
LOSS train 0.1613581898638 valid 0.19706033989787103
LOSS train 0.1613581898638 valid 0.1966744724844323
LOSS train 0.1613581898638 valid 0.19722720668200525
LOSS train 0.1613581898638 valid 0.19668630593352848
LOSS train 0.1613581898638 valid 0.19758724397979677
LOSS train 0.1613581898638 valid 0.1979288942538775
LOSS train 0.1613581898638 valid 0.19781743735074997
LOSS train 0.1613581898638 valid 0.19706315069056268
LOSS train 0.1613581898638 valid 0.1971256246461588
LOSS train 0.1613581898638 valid 0.19676378131776617
LOSS train 0.1613581898638 valid 0.1970356415425028
LOSS train 0.1613581898638 valid 0.19664146908572022
LOSS train 0.1613581898638 valid 0.19686725673576197
LOSS train 0.1613581898638 valid 0.19686178597685408
LOSS train 0.1613581898638 valid 0.19700462008650238
LOSS train 0.1613581898638 valid 0.19731345474720002
LOSS train 0.1613581898638 valid 0.19768503563184486
LOSS train 0.1613581898638 valid 0.19759980772996877
LOSS train 0.1613581898638 valid 0.19747854883854205
LOSS train 0.1613581898638 valid 0.19716811821430544
LOSS train 0.1613581898638 valid 0.19657063242048026
LOSS train 0.1613581898638 valid 0.19622825270081745
LOSS train 0.1613581898638 valid 0.19659250683900786
LOSS train 0.1613581898638 valid 0.19632328005440264
LOSS train 0.1613581898638 valid 0.19619235662477358
LOSS train 0.1613581898638 valid 0.19615945868632373
LOSS train 0.1613581898638 valid 0.19579777312140131
LOSS train 0.1613581898638 valid 0.19559726992557788
LOSS train 0.1613581898638 valid 0.19522494060749357
LOSS train 0.1613581898638 valid 0.19587373147519788
LOSS train 0.1613581898638 valid 0.19581523372067344
LOSS train 0.1613581898638 valid 0.19582000243794787
LOSS train 0.1613581898638 valid 0.19578778031079666
LOSS train 0.1613581898638 valid 0.19560537078688223
LOSS train 0.1613581898638 valid 0.19576810712509968
LOSS train 0.1613581898638 valid 0.19571751607091803
LOSS train 0.1613581898638 valid 0.19582468286777535
LOSS train 0.1613581898638 valid 0.1957517402995493
LOSS train 0.1613581898638 valid 0.19591565080443207
LOSS train 0.1613581898638 valid 0.19608689483368036
LOSS train 0.1613581898638 valid 0.19610093906521797
LOSS train 0.1613581898638 valid 0.19631663333661487
LOSS train 0.1613581898638 valid 0.19656583973590067
LOSS train 0.1613581898638 valid 0.1963623115449276
LOSS train 0.1613581898638 valid 0.19645314319775656
LOSS train 0.1613581898638 valid 0.19648357601392838
LOSS train 0.1613581898638 valid 0.19685603003456908
LOSS train 0.1613581898638 valid 0.1968028837832335
LOSS train 0.1613581898638 valid 0.19694838286549957
LOSS train 0.1613581898638 valid 0.19725483902003788
LOSS train 0.1613581898638 valid 0.19743504591963507
LOSS train 0.1613581898638 valid 0.1973096138990677
LOSS train 0.1613581898638 valid 0.19721176382154226
LOSS train 0.1613581898638 valid 0.19727228674213443
LOSS train 0.1613581898638 valid 0.19738553751978957
LOSS train 0.1613581898638 valid 0.1975107922502186
LOSS train 0.1613581898638 valid 0.19759414653325902
LOSS train 0.1613581898638 valid 0.1976760355835287
LOSS train 0.1613581898638 valid 0.19736729537026357
LOSS train 0.1613581898638 valid 0.1970604926097293
LOSS train 0.1613581898638 valid 0.19679102078080177
LOSS train 0.1613581898638 valid 0.19661535359611196
LOSS train 0.1613581898638 valid 0.1966420910397514
LOSS train 0.1613581898638 valid 0.19645291809144058
LOSS train 0.1613581898638 valid 0.19675316325118464
LOSS train 0.1613581898638 valid 0.1965759369134903
LOSS train 0.1613581898638 valid 0.1969396293399826
LOSS train 0.1613581898638 valid 0.19679785619570514
LOSS train 0.1613581898638 valid 0.19688571337610483
LOSS train 0.1613581898638 valid 0.19708127903845885
LOSS train 0.1613581898638 valid 0.19686047182633326
LOSS train 0.1613581898638 valid 0.19666366267750282
LOSS train 0.1613581898638 valid 0.1963602381222176
LOSS train 0.1613581898638 valid 0.19637798792437502
LOSS train 0.1613581898638 valid 0.19643882695418685
LOSS train 0.1613581898638 valid 0.19631714037171116
LOSS train 0.1613581898638 valid 0.1962839923799038
LOSS train 0.1613581898638 valid 0.19604085190017728
LOSS train 0.1613581898638 valid 0.19597565620273782
LOSS train 0.1613581898638 valid 0.19588641972421741
LOSS train 0.1613581898638 valid 0.1960295499435493
LOSS train 0.1613581898638 valid 0.1959180586726953
LOSS train 0.1613581898638 valid 0.19600100605420664
LOSS train 0.1613581898638 valid 0.19594566378143283
LOSS train 0.1613581898638 valid 0.19596749833888477
LOSS train 0.1613581898638 valid 0.1958091285722009
LOSS train 0.1613581898638 valid 0.1960130119568681
LOSS train 0.1613581898638 valid 0.19601387383581018
LOSS train 0.1613581898638 valid 0.1967400651727174
LOSS train 0.1613581898638 valid 0.19682627566308783
LOSS train 0.1613581898638 valid 0.19679197400808335
LOSS train 0.1613581898638 valid 0.1969583835822857
LOSS train 0.1613581898638 valid 0.19667067437579758
LOSS train 0.1613581898638 valid 0.19669093795461592
LOSS train 0.1613581898638 valid 0.1965647056311756
LOSS train 0.1613581898638 valid 0.1965165170931047
LOSS train 0.1613581898638 valid 0.19655112798015276
LOSS train 0.1613581898638 valid 0.19662570754054245
LOSS train 0.1613581898638 valid 0.196690346715571
LOSS train 0.1613581898638 valid 0.19675302477377765
LOSS train 0.1613581898638 valid 0.19671151116490365
LOSS train 0.1613581898638 valid 0.19660932482769772
LOSS train 0.1613581898638 valid 0.19646892255103146
LOSS train 0.1613581898638 valid 0.1962361207768961
LOSS train 0.1613581898638 valid 0.19609302373194112
LOSS train 0.1613581898638 valid 0.19595935597564235
LOSS train 0.1613581898638 valid 0.19600622670119067
LOSS train 0.1613581898638 valid 0.19629180145834735
LOSS train 0.1613581898638 valid 0.19624925156434378
LOSS train 0.1613581898638 valid 0.19642188087017579
LOSS train 0.1613581898638 valid 0.1964814156293869
LOSS train 0.1613581898638 valid 0.19648074702909815
LOSS train 0.1613581898638 valid 0.19635302079624908
LOSS train 0.1613581898638 valid 0.19642911808339159
LOSS train 0.1613581898638 valid 0.19640340441944956
LOSS train 0.1613581898638 valid 0.19614597022533417
LOSS train 0.1613581898638 valid 0.19611211138015444
LOSS train 0.1613581898638 valid 0.19616254709534725
LOSS train 0.1613581898638 valid 0.19630941991390807
LOSS train 0.1613581898638 valid 0.1962523417932361
LOSS train 0.1613581898638 valid 0.19617170832223363
LOSS train 0.1613581898638 valid 0.19630048032952935
LOSS train 0.1613581898638 valid 0.19618346509370174
LOSS train 0.1613581898638 valid 0.1962342519577735
LOSS train 0.1613581898638 valid 0.19610721868989262
LOSS train 0.1613581898638 valid 0.19597660826670157
LOSS train 0.1613581898638 valid 0.19602351771887913
LOSS train 0.1613581898638 valid 0.19588819050533887
LOSS train 0.1613581898638 valid 0.1958537314959029
LOSS train 0.1613581898638 valid 0.19578615698234114
LOSS train 0.1613581898638 valid 0.19588451056103956
LOSS train 0.1613581898638 valid 0.1958807127950079
LOSS train 0.1613581898638 valid 0.19592490311091146
LOSS train 0.1613581898638 valid 0.19570754429836965
LOSS train 0.1613581898638 valid 0.19560451498351147
LOSS train 0.1613581898638 valid 0.19547026401911027
LOSS train 0.1613581898638 valid 0.19546888350528113
LOSS train 0.1613581898638 valid 0.19569097134062483
LOSS train 0.1613581898638 valid 0.19563471239925634
LOSS train 0.1613581898638 valid 0.19572452759023887
LOSS train 0.1613581898638 valid 0.19556937873363495
LOSS train 0.1613581898638 valid 0.1953757444572686
LOSS train 0.1613581898638 valid 0.19527962309594202
LOSS train 0.1613581898638 valid 0.19526793437051068
LOSS train 0.1613581898638 valid 0.1953971773093822
LOSS train 0.1613581898638 valid 0.19519837978409557
LOSS train 0.1613581898638 valid 0.19525238444793572
LOSS train 0.1613581898638 valid 0.19520131739729268
LOSS train 0.1613581898638 valid 0.1950437590623131
LOSS train 0.1613581898638 valid 0.19501712041323266
LOSS train 0.1613581898638 valid 0.1949689364149457
LOSS train 0.1613581898638 valid 0.1950219234019094
LOSS train 0.1613581898638 valid 0.19500683494052798
LOSS train 0.1613581898638 valid 0.1950101077976361
LOSS train 0.1613581898638 valid 0.19502133903937918
LOSS train 0.1613581898638 valid 0.19493325879407483
LOSS train 0.1613581898638 valid 0.19485811309682
LOSS train 0.1613581898638 valid 0.19483804963700782
LOSS train 0.1613581898638 valid 0.19491749318367843
LOSS train 0.1613581898638 valid 0.19499956833445317
LOSS train 0.1613581898638 valid 0.1950247614898465
LOSS train 0.1613581898638 valid 0.19494887842581823
LOSS train 0.1613581898638 valid 0.19497922405197815
LOSS train 0.1613581898638 valid 0.19511455384337847
LOSS train 0.1613581898638 valid 0.19519490635554707
LOSS train 0.1613581898638 valid 0.19529881404505836
LOSS train 0.1613581898638 valid 0.19548340596720182
LOSS train 0.1613581898638 valid 0.19558974102730267
LOSS train 0.1613581898638 valid 0.19568591015903572
LOSS train 0.1613581898638 valid 0.19572251579646982
LOSS train 0.1613581898638 valid 0.1957478623027387
LOSS train 0.1613581898638 valid 0.19586660161420896
LOSS train 0.1613581898638 valid 0.1959169069626208
LOSS train 0.1613581898638 valid 0.19599098900868658
LOSS train 0.1613581898638 valid 0.1960173799441411
LOSS train 0.1613581898638 valid 0.1961675116356383
LOSS train 0.1613581898638 valid 0.1960421038507405
LOSS train 0.1613581898638 valid 0.19602321748492085
LOSS train 0.1613581898638 valid 0.19602031129248002
LOSS train 0.1613581898638 valid 0.19590212404727936
LOSS train 0.1613581898638 valid 0.1958996519446373
LOSS train 0.1613581898638 valid 0.19603310741824234
LOSS train 0.1613581898638 valid 0.19588355110450226
LOSS train 0.1613581898638 valid 0.19603767028316058
LOSS train 0.1613581898638 valid 0.19621638483444198
LOSS train 0.1613581898638 valid 0.19627463440505827
LOSS train 0.1613581898638 valid 0.1961681108649184
LOSS train 0.1613581898638 valid 0.1963208574756437
LOSS train 0.1613581898638 valid 0.19627635865922896
LOSS train 0.1613581898638 valid 0.19630668285381364
LOSS train 0.1613581898638 valid 0.19636643135547638
LOSS train 0.1613581898638 valid 0.19622792653828502
LOSS train 0.1613581898638 valid 0.1964515299787597
LOSS train 0.1613581898638 valid 0.19640658289311902
LOSS train 0.1613581898638 valid 0.19630798275076497
LOSS train 0.1613581898638 valid 0.19632790176307455
LOSS train 0.1613581898638 valid 0.19640123145654798
LOSS train 0.1613581898638 valid 0.19626696918029266
LOSS train 0.1613581898638 valid 0.19645913655674735
LOSS train 0.1613581898638 valid 0.19646669573305195
LOSS train 0.1613581898638 valid 0.19640712336852
LOSS train 0.1613581898638 valid 0.1965185684719305
LOSS train 0.1613581898638 valid 0.19655281458874696
LOSS train 0.1613581898638 valid 0.1965957553667714
LOSS train 0.1613581898638 valid 0.19669026339595969
LOSS train 0.1613581898638 valid 0.1966198777815081
LOSS train 0.1613581898638 valid 0.1966557670804791
LOSS train 0.1613581898638 valid 0.19669338430134992
LOSS train 0.1613581898638 valid 0.1968650944086153
LOSS train 0.1613581898638 valid 0.1969690907843494
LOSS train 0.1613581898638 valid 0.1969739497811706
LOSS train 0.1613581898638 valid 0.1970099756748474
LOSS train 0.1613581898638 valid 0.19726885027964325
LOSS train 0.1613581898638 valid 0.19742617709732754
LOSS train 0.1613581898638 valid 0.19743165683789846
LOSS train 0.1613581898638 valid 0.1974403036182577
LOSS train 0.1613581898638 valid 0.19731051588188048
LOSS train 0.1613581898638 valid 0.19715666921560515
LOSS train 0.1613581898638 valid 0.1969851759805096
LOSS train 0.1613581898638 valid 0.1969787091230406
LOSS train 0.1613581898638 valid 0.19698357709816525
LOSS train 0.1613581898638 valid 0.19688955644059436
LOSS train 0.1613581898638 valid 0.19664226984935448
LOSS train 0.1613581898638 valid 0.19656178297297272
LOSS train 0.1613581898638 valid 0.1965764192208438
LOSS train 0.1613581898638 valid 0.1966290126244227
LOSS train 0.1613581898638 valid 0.19662861494751244
LOSS train 0.1613581898638 valid 0.1965719132784767
LOSS train 0.1613581898638 valid 0.19653275096789002
LOSS train 0.1613581898638 valid 0.19653408399502711
LOSS train 0.1613581898638 valid 0.19656321940750912
LOSS train 0.1613581898638 valid 0.1964295768655862
LOSS train 0.1613581898638 valid 0.19643371485888142
LOSS train 0.1613581898638 valid 0.19645625139259235
LOSS train 0.1613581898638 valid 0.19654360454098707
LOSS train 0.1613581898638 valid 0.196625161726596
LOSS train 0.1613581898638 valid 0.19655893883995107
LOSS train 0.1613581898638 valid 0.1965828464187757
LOSS train 0.1613581898638 valid 0.1966033718429956
LOSS train 0.1613581898638 valid 0.19668314299455852
LOSS train 0.1613581898638 valid 0.19673952504992484
LOSS train 0.1613581898638 valid 0.1966843935243315
LOSS train 0.1613581898638 valid 0.19663398515507086
LOSS train 0.1613581898638 valid 0.19671249020807813
LOSS train 0.1613581898638 valid 0.19670543064804455
LOSS train 0.1613581898638 valid 0.1966450359489097
LOSS train 0.1613581898638 valid 0.19665932129411137
LOSS train 0.1613581898638 valid 0.19663284209341492
LOSS train 0.1613581898638 valid 0.19654966416684064
LOSS train 0.1613581898638 valid 0.19656233233927137
LOSS train 0.1613581898638 valid 0.19662234076569157
LOSS train 0.1613581898638 valid 0.1966299579936975
LOSS train 0.1613581898638 valid 0.1966043750349528
LOSS train 0.1613581898638 valid 0.1967448413657685
LOSS train 0.1613581898638 valid 0.19678842485140843
LOSS train 0.1613581898638 valid 0.19671410459374625
LOSS train 0.1613581898638 valid 0.19663747744281082
LOSS train 0.1613581898638 valid 0.19665814061834233
LOSS train 0.1613581898638 valid 0.1966795840259618
LOSS train 0.1613581898638 valid 0.19677568481633656
LOSS train 0.1613581898638 valid 0.19669606080278754
LOSS train 0.1613581898638 valid 0.19681912298514465
LOSS train 0.1613581898638 valid 0.1967775628248357
LOSS train 0.1613581898638 valid 0.196766775689627
LOSS train 0.1613581898638 valid 0.19684146298670474
LOSS train 0.1613581898638 valid 0.1968513719852154
LOSS train 0.1613581898638 valid 0.19697996297496959
LOSS train 0.1613581898638 valid 0.1970363608376331
LOSS train 0.1613581898638 valid 0.19702978586641753
LOSS train 0.1613581898638 valid 0.19710011309222247
LOSS train 0.1613581898638 valid 0.19708477245135742
LOSS train 0.1613581898638 valid 0.19698309866863434
LOSS train 0.1613581898638 valid 0.19687458325222315
LOSS train 0.1613581898638 valid 0.1969274047079745
LOSS train 0.1613581898638 valid 0.1970556998948851
LOSS train 0.1613581898638 valid 0.1970152052925594
LOSS train 0.1613581898638 valid 0.1970630499223868
LOSS train 0.1613581898638 valid 0.19704707378095973
LOSS train 0.1613581898638 valid 0.19701936867462813
LOSS train 0.1613581898638 valid 0.19703070647948612
LOSS train 0.1613581898638 valid 0.19699125154053462
LOSS train 0.1613581898638 valid 0.1968737855946913
LOSS train 0.1613581898638 valid 0.19688456575250068
LOSS train 0.1613581898638 valid 0.19691780196508235
LOSS train 0.1613581898638 valid 0.19711074861156386
LOSS train 0.1613581898638 valid 0.1971893410751785
LOSS train 0.1613581898638 valid 0.1972492486669149
LOSS train 0.1613581898638 valid 0.19709099975202543
LOSS train 0.1613581898638 valid 0.19703551210548687
LOSS train 0.1613581898638 valid 0.19703889238971012
LOSS train 0.1613581898638 valid 0.19698675858122963
LOSS train 0.1613581898638 valid 0.19689786629459458
LOSS train 0.1613581898638 valid 0.196892215277661
LOSS train 0.1613581898638 valid 0.19689715689538898
LOSS train 0.1613581898638 valid 0.19694153493070332
LOSS train 0.1613581898638 valid 0.1970200000934198
LOSS train 0.1613581898638 valid 0.19703872866007718
LOSS train 0.1613581898638 valid 0.19706965648994393
LOSS train 0.1613581898638 valid 0.19698006934460316
LOSS train 0.1613581898638 valid 0.19701098677673712
LOSS train 0.1613581898638 valid 0.19696813341644076
LOSS train 0.1613581898638 valid 0.19694485170689316
LOSS train 0.1613581898638 valid 0.19700777641811423
LOSS train 0.1613581898638 valid 0.19698407014539418
LOSS train 0.1613581898638 valid 0.19699902218449247
LOSS train 0.1613581898638 valid 0.1970436052916801
LOSS train 0.1613581898638 valid 0.1970371196423072
LOSS train 0.1613581898638 valid 0.19693658677531198
LOSS train 0.1613581898638 valid 0.19695626644660597
LOSS train 0.1613581898638 valid 0.19702470758904608
EPOCH 3:
  batch 1 loss: 0.13647836446762085
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.1370718851685524
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.13434261083602905
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.140073761343956
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.1430227130651474
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.1436560278137525
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.14063574373722076
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.14257491938769817
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.1425165997611152
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.14215593039989471
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.1408653354102915
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.14050801719228426
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.13994952004689437
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.13928111961909703
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.1382641648252805
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.1396225648932159
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.1386379739817451
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.14140033970276514
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.1411261487948267
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.14070970714092254
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.14123533949965522
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.14163416149941357
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.1422666881395423
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.1440388783812523
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.14380595862865447
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.14401024751938307
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.14421990679370034
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.1434655442301716
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.14322475857775788
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.14398117785652478
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.14426809284956224
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.14459668775089085
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.14376458548235171
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.14454921663684003
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.1461494694863047
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.14558229078021315
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.14571867420061216
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.14534289644736992
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.14557508761302018
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.1457658948376775
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.14587737165573167
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.14596100098320416
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.1469271954062373
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.14625293795358052
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.14657354917791154
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.14615811277990756
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.1459350662028536
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.14594437647610903
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.1461040304631603
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.14611587405204773
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.1459013997924094
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.1458310206922201
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.14577898664294547
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.1455962202615208
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.14590790732340378
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.1459626334586314
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.1460822865105512
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.1455715224146843
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.145950720976975
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.14587872996926307
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.1462040814708491
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.1459245674552456
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.1460547676635167
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.14572943979874253
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.14590116945596843
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.14566509403062589
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.14551404118537903
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.14574373557287104
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.14594316007434457
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.14621864344392504
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.14661814839067594
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.14680041621128717
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.14684466446099215
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.14689007804200455
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.14704340199629465
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.1469539811736659
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.1468806696402562
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.14658166487247515
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.14698842186716538
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.14723025355488062
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.14721527677259327
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.1468272220070769
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.146689537418894
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.14695487135932558
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.14691826964125915
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.14715458851220997
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.14725433067343702
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.14750960757109252
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.1473849258396063
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.14773537947071924
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.14786076152717675
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.14775000690766002
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.14799928024250975
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.1479888989570293
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.14842870125645086
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.14848932794605693
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.14821890219277942
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.1480604226948047
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.14831567977112953
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.14819810964167118
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.14808559174289798
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.1479591465025556
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.14778636562303432
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.1479065537882539
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.14751868751787003
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.14750495180487633
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.14768856631539692
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.14754507062887703
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.14767797270772653
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.1478921620005911
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.14767812011209694
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.14757102614800846
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.14729201253009053
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.14758720431934325
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.14733376075392185
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.1472412776844255
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.14752098561352134
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.14753988814555993
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.14745928071627096
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.14739108520249525
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.1473094644871625
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.14733454145368982
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.14742854843294717
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.14736244358843373
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.1474199378490448
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.14752890831894344
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.14757023141609402
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.14734943106304854
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.14725573615048282
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.14720587535546376
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.1470746228485617
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.14702862004439035
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.14726049100097857
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.14737417282008414
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.1471522021624777
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.14704063090988817
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.14721914315528242
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.14714071689092595
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.1473220318234224
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.1473192958427327
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.14732740824738294
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.14714138047166272
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.14726035385490296
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.14740884536877275
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.14728987150151154
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.14729382807057198
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.14727050263662728
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.14721903360977368
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.1474823337173302
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.14756779169042905
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.1474774692528295
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.14753206951641723
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.14745448232671016
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.14715956882029385
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.1472350043154532
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.1472074817866087
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.14726974805639048
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.14725001528859138
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.14744033420798164
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.1475841763895005
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.14765593821402662
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.14759448717957663
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.14760768262703725
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.14779651905523566
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.14788560329964667
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.1478331297725798
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.14772279609641628
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.14769768728209393
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.14754497048241147
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.1477489918908652
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.14775167732385167
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.14770607689265594
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.14749775142166655
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.1474063498792292
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.1475445563026837
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.14738667434589428
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.1472489603579381
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.1469615398581778
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.14717396215520093
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.14701245203614235
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.1470395190775065
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.14702991723686784
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.14691786411030044
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.14684709736510462
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.14685391550128524
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.14679223579424683
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.1468218934727225
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.14674215383352118
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.14674710983006412
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.1467563996189519
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.14676434082510584
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.1466522712726146
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.14663678299577743
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.1463667650121389
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.14636151893780783
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.1464803945273161
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.14656166130969972
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.14669361067089168
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.14664489251734622
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.14677109312266112
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.14677718567166162
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.14678783659445177
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.1468506477809892
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.146864871731868
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.14683003778137813
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.14670536981768978
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.14670716049734522
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.14652081202858916
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.14657387262183513
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.14671089454066186
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.14666805938945562
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.14666326459989232
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.14672757737653355
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.14657579351947686
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.14663910044487133
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.14651034699959886
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.1464838949010669
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.14658135350007528
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.14661770382020028
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.1465792503207922
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.14652575535607015
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.14651840358032836
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.14644678768956607
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.14649196564485983
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.1463835386104054
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.1463508570022815
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.14640389766855913
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.14623092601827362
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.14618798429919122
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.1463704855221769
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.14653601065213545
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.14641490976872115
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.14645931642710394
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.14647045674232337
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.1465195210690194
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.14654343815173132
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.1464991833590254
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.14653644961218873
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.14653811912406936
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.14646149451533955
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.1465745750057252
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.14653300924980936
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.14658209552735457
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.14659541338437893
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.14656912191789978
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.14666066421725885
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.14662490260263203
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.1466920896763763
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.14672887403084092
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.1466490548849106
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.1467811528786245
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.1468283361977055
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.14683389280860132
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.14679284587385147
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.14673868353460348
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.14673080627107993
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.1466782613139208
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.14659325824689495
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.14664611208853115
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.14666247396514967
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.14661880693901544
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.14654000973655978
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.14650432856137308
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.1463660251800761
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.14636733357636433
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.14639524207975632
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.14635152472985372
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.14634096628027177
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.1462300359150291
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.14622965049412515
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.14619442575862046
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.14615694918286273
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.14622154916489954
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.14609431211639495
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.1459898507595062
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.14615770823497704
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.1461645199073351
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.14621359335003997
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.1462146827290135
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.1461881963270051
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.14616560660223096
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.1461189512876754
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.14608624946102236
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.1460288357461842
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.1461256880509226
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.1462195671938516
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.14614665113675054
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.1461418187763128
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.14608968412793633
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.1460687432309677
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.14609132143844852
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.1461881283823758
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.1463400012600544
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.14636205704439254
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.14633533979876567
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.14633481113894567
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.14633187802150996
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.14628924684436528
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.14621376525299207
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.14619637586176396
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.14620820041807783
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.146198197977235
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.14618188818984298
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.14613173502546392
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.1461106147678172
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.14611881886140193
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.1461587847007214
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.1460934036786293
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.14607048268553507
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.1459523105813611
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.14598682034053986
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.14592995695196664
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.1458575969306044
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.1458255880672461
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.14583232256155165
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.14595100953231885
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.14590495345344304
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.14591404223966897
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.14582496575520704
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.1458866603905335
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.1458202492085944
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.14574479575482954
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.14576166775942587
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.14570122616894451
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.14568082364705892
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.14558600540632852
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.14548982928196588
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.14549236374384747
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.14553057746593712
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.14558685884782763
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.14561655836072934
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.14560808427631855
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.14552660485556176
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.14553733961935528
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.14557664067442738
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.14559180823908674
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.1455608847542757
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.14554685770848094
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.14545847148054822
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.1454448336844935
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.1454601055584695
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.14569645769327705
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.14572861349704314
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.14576246336015852
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.14576700318982636
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.14571634354877333
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.14569769428253862
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.1456600953236051
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.14558315706201816
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.14548217232738223
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.14548393943880358
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.14555921172723174
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.14561529639084683
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.1456263889111368
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.14562353508573184
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.14555684487555134
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.14562244176948103
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.14569292964632286
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.14561664138746794
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.14558582129991718
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.14555267252832899
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.1456054492508838
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.14558470702778867
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.14554032791856225
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.14558468348767659
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.14556463936068972
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.1456040715448538
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.14568381365793553
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.14566841444914258
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.14559682007576968
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.14562332626140986
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.14559487682036173
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.14560474679073762
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.14566813453155406
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.14566670020421346
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.14567616296575425
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.14567792937357166
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.14572164503039506
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.1456629038839982
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.14562470416881537
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.14560236202997798
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.14554593084058212
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.14560798335791256
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.14557919659030935
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.14549847430997082
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.14545305656182334
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.145478379395273
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.14544598155261315
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.14542601405041997
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.1453766201169063
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.14535186361626287
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.14535115959540923
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.14541750922609528
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.14541404769959185
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.14544463516036166
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.1453859099572656
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.14528570200318955
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.14532168492225547
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.1452813388821774
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.14524610294029117
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.14521540993095336
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.1451904596343859
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.14516040340768493
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.14518703968439362
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.1452274692095356
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.14519116544885002
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.14519111645529253
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.14523031915008439
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.1451830695084954
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.1451989459737045
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.14516804349175916
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.14522756399243203
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.1452425424794308
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.14531344868206747
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.145357652403504
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.14543560195642596
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.1454363767799142
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.14538789549441428
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.14542892658838508
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.14536872243597393
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.14542127115709483
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.14545944901580493
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.14538081888611434
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.14538084789407704
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.1453448378689149
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.14525441062842176
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.14524981798276967
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.1452154086850514
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.14513571395393296
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.14506341685042826
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.14509311914789705
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.14504412178777987
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.14501239252696135
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.14506716377312137
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.1450181532865283
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.14509184672198164
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.14513894271114053
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.14509586700701824
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.1449966575432747
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.1449840146201578
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.1449906532895538
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.14494488078140025
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.14494479062460347
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.1449213492105136
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.144927573003126
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.14494445452241084
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.14490715152598602
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.1449583944465433
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.1449331870854299
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.1449087909526295
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.14487240957447273
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.1449161806201513
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.14485602468975858
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.14484812972435343
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.14479944701050665
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.14479758869856596
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.1448157002061149
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.14483448337584603
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.1448246973499753
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.1448791211551946
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.14487247539122555
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.14487615500242162
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.14480511295422108
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.144836422746423
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.14486675132666865
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.14488807574809875
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.14490892253098192
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.14496789046396047
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.14501170405764569
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.14503459348640543
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.1450903975818061
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.14518065000805308
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.14518065000805308 valid 0.21001683175563812
LOSS train 0.14518065000805308 valid 0.19227759540081024
LOSS train 0.14518065000805308 valid 0.18531585236390433
LOSS train 0.14518065000805308 valid 0.17986002191901207
LOSS train 0.14518065000805308 valid 0.17606066763401032
LOSS train 0.14518065000805308 valid 0.1874284620086352
LOSS train 0.14518065000805308 valid 0.19823396418775832
LOSS train 0.14518065000805308 valid 0.19550729729235172
LOSS train 0.14518065000805308 valid 0.1945314821269777
LOSS train 0.14518065000805308 valid 0.19591184705495834
LOSS train 0.14518065000805308 valid 0.1956615759567781
LOSS train 0.14518065000805308 valid 0.1960496318837007
LOSS train 0.14518065000805308 valid 0.19577740820554587
LOSS train 0.14518065000805308 valid 0.1952213187302862
LOSS train 0.14518065000805308 valid 0.19253355165322622
LOSS train 0.14518065000805308 valid 0.1928013674914837
LOSS train 0.14518065000805308 valid 0.19382538690286524
LOSS train 0.14518065000805308 valid 0.19317497313022614
LOSS train 0.14518065000805308 valid 0.1951999334912551
LOSS train 0.14518065000805308 valid 0.19495996832847595
LOSS train 0.14518065000805308 valid 0.19436424473921457
LOSS train 0.14518065000805308 valid 0.19304349815303629
LOSS train 0.14518065000805308 valid 0.19281735821910526
LOSS train 0.14518065000805308 valid 0.19257066026329994
LOSS train 0.14518065000805308 valid 0.19154877483844757
LOSS train 0.14518065000805308 valid 0.19124151708988044
LOSS train 0.14518065000805308 valid 0.19122783563755177
LOSS train 0.14518065000805308 valid 0.1913056852562087
LOSS train 0.14518065000805308 valid 0.1908639705386655
LOSS train 0.14518065000805308 valid 0.19155073612928392
LOSS train 0.14518065000805308 valid 0.19212429033171746
LOSS train 0.14518065000805308 valid 0.19112296821549535
LOSS train 0.14518065000805308 valid 0.19155946464249582
LOSS train 0.14518065000805308 valid 0.19096752168501124
LOSS train 0.14518065000805308 valid 0.19233496912888118
LOSS train 0.14518065000805308 valid 0.1923484466969967
LOSS train 0.14518065000805308 valid 0.19287143083843025
LOSS train 0.14518065000805308 valid 0.19336208897201637
LOSS train 0.14518065000805308 valid 0.19269348566348737
LOSS train 0.14518065000805308 valid 0.19239727295935155
LOSS train 0.14518065000805308 valid 0.19291631950110924
LOSS train 0.14518065000805308 valid 0.19333086553074064
LOSS train 0.14518065000805308 valid 0.1931939679522847
LOSS train 0.14518065000805308 valid 0.1936275986107913
LOSS train 0.14518065000805308 valid 0.19344806571801504
LOSS train 0.14518065000805308 valid 0.1936371844747792
LOSS train 0.14518065000805308 valid 0.1941168339962655
LOSS train 0.14518065000805308 valid 0.1941345613449812
LOSS train 0.14518065000805308 valid 0.1947164885243591
LOSS train 0.14518065000805308 valid 0.19406638652086258
LOSS train 0.14518065000805308 valid 0.19425095471681333
LOSS train 0.14518065000805308 valid 0.19376144615503457
LOSS train 0.14518065000805308 valid 0.19414012190305963
LOSS train 0.14518065000805308 valid 0.19427960366010666
LOSS train 0.14518065000805308 valid 0.1942147509618239
LOSS train 0.14518065000805308 valid 0.19341396859713963
LOSS train 0.14518065000805308 valid 0.19329782170161866
LOSS train 0.14518065000805308 valid 0.19307154956562766
LOSS train 0.14518065000805308 valid 0.19336091436571995
LOSS train 0.14518065000805308 valid 0.1934435230990251
LOSS train 0.14518065000805308 valid 0.19301987476036198
LOSS train 0.14518065000805308 valid 0.1935800996518904
LOSS train 0.14518065000805308 valid 0.19300122937512776
LOSS train 0.14518065000805308 valid 0.19394315290264785
LOSS train 0.14518065000805308 valid 0.19425318126495067
LOSS train 0.14518065000805308 valid 0.19412366797526678
LOSS train 0.14518065000805308 valid 0.19339212130254774
LOSS train 0.14518065000805308 valid 0.19344828803749645
LOSS train 0.14518065000805308 valid 0.19304319028405176
LOSS train 0.14518065000805308 valid 0.1933743051120213
LOSS train 0.14518065000805308 valid 0.1929970536853226
LOSS train 0.14518065000805308 valid 0.19321996925605667
LOSS train 0.14518065000805308 valid 0.1932453168173359
LOSS train 0.14518065000805308 valid 0.19341200408903328
LOSS train 0.14518065000805308 valid 0.19376154839992524
LOSS train 0.14518065000805308 valid 0.1941171129675288
LOSS train 0.14518065000805308 valid 0.19396307174261515
LOSS train 0.14518065000805308 valid 0.1938089432242589
LOSS train 0.14518065000805308 valid 0.19345172956774506
LOSS train 0.14518065000805308 valid 0.1928325230255723
LOSS train 0.14518065000805308 valid 0.19250192575984532
LOSS train 0.14518065000805308 valid 0.19290898594914413
LOSS train 0.14518065000805308 valid 0.19257551629141154
LOSS train 0.14518065000805308 valid 0.1924777909048966
LOSS train 0.14518065000805308 valid 0.19246470647699693
LOSS train 0.14518065000805308 valid 0.19212205621392228
LOSS train 0.14518065000805308 valid 0.19192005882317992
LOSS train 0.14518065000805308 valid 0.19156244363297115
LOSS train 0.14518065000805308 valid 0.1922506716813934
LOSS train 0.14518065000805308 valid 0.19222909526692497
LOSS train 0.14518065000805308 valid 0.1922629048208614
LOSS train 0.14518065000805308 valid 0.19221837867213332
LOSS train 0.14518065000805308 valid 0.19204044229881737
LOSS train 0.14518065000805308 valid 0.19222255320625103
LOSS train 0.14518065000805308 valid 0.192157675090589
LOSS train 0.14518065000805308 valid 0.19224002941822013
LOSS train 0.14518065000805308 valid 0.19215861316194238
LOSS train 0.14518065000805308 valid 0.19236401346873264
LOSS train 0.14518065000805308 valid 0.19251821017024492
LOSS train 0.14518065000805308 valid 0.1925154310464859
LOSS train 0.14518065000805308 valid 0.19274643166820601
LOSS train 0.14518065000805308 valid 0.19298269117579742
LOSS train 0.14518065000805308 valid 0.19274898886101918
LOSS train 0.14518065000805308 valid 0.1928825113349236
LOSS train 0.14518065000805308 valid 0.19292467250710443
LOSS train 0.14518065000805308 valid 0.19331382931965702
LOSS train 0.14518065000805308 valid 0.1932425109025474
LOSS train 0.14518065000805308 valid 0.19338899078192534
LOSS train 0.14518065000805308 valid 0.19371097487047179
LOSS train 0.14518065000805308 valid 0.19388783154162495
LOSS train 0.14518065000805308 valid 0.193799804459821
LOSS train 0.14518065000805308 valid 0.19369333357151067
LOSS train 0.14518065000805308 valid 0.1937530569534386
LOSS train 0.14518065000805308 valid 0.19392763235067068
LOSS train 0.14518065000805308 valid 0.19407891320145648
LOSS train 0.14518065000805308 valid 0.1942104121734356
LOSS train 0.14518065000805308 valid 0.19426138622638506
LOSS train 0.14518065000805308 valid 0.19393949879933212
LOSS train 0.14518065000805308 valid 0.19359812741520024
LOSS train 0.14518065000805308 valid 0.1933068932344516
LOSS train 0.14518065000805308 valid 0.19312407248768926
LOSS train 0.14518065000805308 valid 0.19316897248146964
LOSS train 0.14518065000805308 valid 0.19296782106403412
LOSS train 0.14518065000805308 valid 0.19327437661347852
LOSS train 0.14518065000805308 valid 0.19309891271591187
LOSS train 0.14518065000805308 valid 0.19348057832509752
LOSS train 0.14518065000805308 valid 0.19333164698964966
LOSS train 0.14518065000805308 valid 0.19338377623353153
LOSS train 0.14518065000805308 valid 0.19358011271602424
LOSS train 0.14518065000805308 valid 0.1933239431335376
LOSS train 0.14518065000805308 valid 0.1930682932147543
LOSS train 0.14518065000805308 valid 0.19274588042136395
LOSS train 0.14518065000805308 valid 0.192766056361055
LOSS train 0.14518065000805308 valid 0.192844634291841
LOSS train 0.14518065000805308 valid 0.1927187771708877
LOSS train 0.14518065000805308 valid 0.19268943818614764
LOSS train 0.14518065000805308 valid 0.19243984163677605
LOSS train 0.14518065000805308 valid 0.19236864959416183
LOSS train 0.14518065000805308 valid 0.19228482846733477
LOSS train 0.14518065000805308 valid 0.19241737425327302
LOSS train 0.14518065000805308 valid 0.1923063093043388
LOSS train 0.14518065000805308 valid 0.19234963118190496
LOSS train 0.14518065000805308 valid 0.19230249670002011
LOSS train 0.14518065000805308 valid 0.1923562708414263
LOSS train 0.14518065000805308 valid 0.1922111448542825
LOSS train 0.14518065000805308 valid 0.19240520271944672
LOSS train 0.14518065000805308 valid 0.1924105325118214
LOSS train 0.14518065000805308 valid 0.19320611313388153
LOSS train 0.14518065000805308 valid 0.19330386807454514
LOSS train 0.14518065000805308 valid 0.19324655463298163
LOSS train 0.14518065000805308 valid 0.1934373432042583
LOSS train 0.14518065000805308 valid 0.1931312099883431
LOSS train 0.14518065000805308 valid 0.19313788238693685
LOSS train 0.14518065000805308 valid 0.19300544890877488
LOSS train 0.14518065000805308 valid 0.19292416072660876
LOSS train 0.14518065000805308 valid 0.19293780013536796
LOSS train 0.14518065000805308 valid 0.19302376421393863
LOSS train 0.14518065000805308 valid 0.19307451600892633
LOSS train 0.14518065000805308 valid 0.19316724898680201
LOSS train 0.14518065000805308 valid 0.19312004903331398
LOSS train 0.14518065000805308 valid 0.19300232540746654
LOSS train 0.14518065000805308 valid 0.19284950978594062
LOSS train 0.14518065000805308 valid 0.19260547726066565
LOSS train 0.14518065000805308 valid 0.19244760992686924
LOSS train 0.14518065000805308 valid 0.19229675423015247
LOSS train 0.14518065000805308 valid 0.19236388533230286
LOSS train 0.14518065000805308 valid 0.19268184231069987
LOSS train 0.14518065000805308 valid 0.19264088739596663
LOSS train 0.14518065000805308 valid 0.19282064855804104
LOSS train 0.14518065000805308 valid 0.1928656121387201
LOSS train 0.14518065000805308 valid 0.1928560624868549
LOSS train 0.14518065000805308 valid 0.19273221198209498
LOSS train 0.14518065000805308 valid 0.19278747497955498
LOSS train 0.14518065000805308 valid 0.19275305240318694
LOSS train 0.14518065000805308 valid 0.19247496715613774
LOSS train 0.14518065000805308 valid 0.19243225277486173
LOSS train 0.14518065000805308 valid 0.19248669623991863
LOSS train 0.14518065000805308 valid 0.19265604546565687
LOSS train 0.14518065000805308 valid 0.1925773568160041
LOSS train 0.14518065000805308 valid 0.19250299740168783
LOSS train 0.14518065000805308 valid 0.19263257020415522
LOSS train 0.14518065000805308 valid 0.1924849278651751
LOSS train 0.14518065000805308 valid 0.19254932418221332
LOSS train 0.14518065000805308 valid 0.19243525770371375
LOSS train 0.14518065000805308 valid 0.19231332822425945
LOSS train 0.14518065000805308 valid 0.19234542788997774
LOSS train 0.14518065000805308 valid 0.19219696697066813
LOSS train 0.14518065000805308 valid 0.19215539232530493
LOSS train 0.14518065000805308 valid 0.19207006054265158
LOSS train 0.14518065000805308 valid 0.19217167520209363
LOSS train 0.14518065000805308 valid 0.1921453397155432
LOSS train 0.14518065000805308 valid 0.19220402270245054
LOSS train 0.14518065000805308 valid 0.19195292303290393
LOSS train 0.14518065000805308 valid 0.19184393474121683
LOSS train 0.14518065000805308 valid 0.19168874766582097
LOSS train 0.14518065000805308 valid 0.1916703848662425
LOSS train 0.14518065000805308 valid 0.19188779554693833
LOSS train 0.14518065000805308 valid 0.19182394121331398
LOSS train 0.14518065000805308 valid 0.19191327384069337
LOSS train 0.14518065000805308 valid 0.19175514362752438
LOSS train 0.14518065000805308 valid 0.19155623789746962
LOSS train 0.14518065000805308 valid 0.19144669536611822
LOSS train 0.14518065000805308 valid 0.1914412481297413
LOSS train 0.14518065000805308 valid 0.1916183144146321
LOSS train 0.14518065000805308 valid 0.1914132339198415
LOSS train 0.14518065000805308 valid 0.19146484295720034
LOSS train 0.14518065000805308 valid 0.1914038555921564
LOSS train 0.14518065000805308 valid 0.19123830882689127
LOSS train 0.14518065000805308 valid 0.19122416750666058
LOSS train 0.14518065000805308 valid 0.1911612473783039
LOSS train 0.14518065000805308 valid 0.19121368405943234
LOSS train 0.14518065000805308 valid 0.19119255807039873
LOSS train 0.14518065000805308 valid 0.19118378671681938
LOSS train 0.14518065000805308 valid 0.1912054482882268
LOSS train 0.14518065000805308 valid 0.1910980912835099
LOSS train 0.14518065000805308 valid 0.19100022474648776
LOSS train 0.14518065000805308 valid 0.19096945439066207
LOSS train 0.14518065000805308 valid 0.19103661343592024
LOSS train 0.14518065000805308 valid 0.1911367073859254
LOSS train 0.14518065000805308 valid 0.19115422781218183
LOSS train 0.14518065000805308 valid 0.19107915703797232
LOSS train 0.14518065000805308 valid 0.19113046461128974
LOSS train 0.14518065000805308 valid 0.19127787749863526
LOSS train 0.14518065000805308 valid 0.19138134583564742
LOSS train 0.14518065000805308 valid 0.19150782936149174
LOSS train 0.14518065000805308 valid 0.19172014368582616
LOSS train 0.14518065000805308 valid 0.1918032587493569
LOSS train 0.14518065000805308 valid 0.19189142324683958
LOSS train 0.14518065000805308 valid 0.1919164660846302
LOSS train 0.14518065000805308 valid 0.19194057577330134
LOSS train 0.14518065000805308 valid 0.19209292189125374
LOSS train 0.14518065000805308 valid 0.19212890345731687
LOSS train 0.14518065000805308 valid 0.19220392853661156
LOSS train 0.14518065000805308 valid 0.19223194079011932
LOSS train 0.14518065000805308 valid 0.19238184994839608
LOSS train 0.14518065000805308 valid 0.1922539697360184
LOSS train 0.14518065000805308 valid 0.1922256205278107
LOSS train 0.14518065000805308 valid 0.19223394470305002
LOSS train 0.14518065000805308 valid 0.19209869667326557
LOSS train 0.14518065000805308 valid 0.19210779300580422
LOSS train 0.14518065000805308 valid 0.19225567740028823
LOSS train 0.14518065000805308 valid 0.19211092314197997
LOSS train 0.14518065000805308 valid 0.1922765759283623
LOSS train 0.14518065000805308 valid 0.19245534329140773
LOSS train 0.14518065000805308 valid 0.19250810870102475
LOSS train 0.14518065000805308 valid 0.19238110134998956
LOSS train 0.14518065000805308 valid 0.19255236509116555
LOSS train 0.14518065000805308 valid 0.19251046203557523
LOSS train 0.14518065000805308 valid 0.19254375647110153
LOSS train 0.14518065000805308 valid 0.19259830737113953
LOSS train 0.14518065000805308 valid 0.19243990816918027
LOSS train 0.14518065000805308 valid 0.19269162972295095
LOSS train 0.14518065000805308 valid 0.19263880449557022
LOSS train 0.14518065000805308 valid 0.1925405757281724
LOSS train 0.14518065000805308 valid 0.19257087467932232
LOSS train 0.14518065000805308 valid 0.19263760326430202
LOSS train 0.14518065000805308 valid 0.19249062222729396
LOSS train 0.14518065000805308 valid 0.19269491402908814
LOSS train 0.14518065000805308 valid 0.19270587186095336
LOSS train 0.14518065000805308 valid 0.1926261445077566
LOSS train 0.14518065000805308 valid 0.19276427029421503
LOSS train 0.14518065000805308 valid 0.1928096092270531
LOSS train 0.14518065000805308 valid 0.19284152276389047
LOSS train 0.14518065000805308 valid 0.1929344997148622
LOSS train 0.14518065000805308 valid 0.19286404617552488
LOSS train 0.14518065000805308 valid 0.19290771839538015
LOSS train 0.14518065000805308 valid 0.1929398656226276
LOSS train 0.14518065000805308 valid 0.19312999469798003
LOSS train 0.14518065000805308 valid 0.19322480804193418
LOSS train 0.14518065000805308 valid 0.19322383718358146
LOSS train 0.14518065000805308 valid 0.1932732239640507
LOSS train 0.14518065000805308 valid 0.19354350985411337
LOSS train 0.14518065000805308 valid 0.19372273408449614
LOSS train 0.14518065000805308 valid 0.1937316478821483
LOSS train 0.14518065000805308 valid 0.19373957547274503
LOSS train 0.14518065000805308 valid 0.19362477634264075
LOSS train 0.14518065000805308 valid 0.19347669840504547
LOSS train 0.14518065000805308 valid 0.1933093998286364
LOSS train 0.14518065000805308 valid 0.19330404341007218
LOSS train 0.14518065000805308 valid 0.19330920481256075
LOSS train 0.14518065000805308 valid 0.19321983755695438
LOSS train 0.14518065000805308 valid 0.19295389524905393
LOSS train 0.14518065000805308 valid 0.19286324397098048
LOSS train 0.14518065000805308 valid 0.19287899732065034
LOSS train 0.14518065000805308 valid 0.19292124245773282
LOSS train 0.14518065000805308 valid 0.19291842220859093
LOSS train 0.14518065000805308 valid 0.19286151196586962
LOSS train 0.14518065000805308 valid 0.19282536749314103
LOSS train 0.14518065000805308 valid 0.19283581205408465
LOSS train 0.14518065000805308 valid 0.1928662456572056
LOSS train 0.14518065000805308 valid 0.19271365338072335
LOSS train 0.14518065000805308 valid 0.19271367173386764
LOSS train 0.14518065000805308 valid 0.1927424557634181
LOSS train 0.14518065000805308 valid 0.1928305250092023
LOSS train 0.14518065000805308 valid 0.19291522768089328
LOSS train 0.14518065000805308 valid 0.19285108111295346
LOSS train 0.14518065000805308 valid 0.19289180488397778
LOSS train 0.14518065000805308 valid 0.192912669364838
LOSS train 0.14518065000805308 valid 0.19298814997126826
LOSS train 0.14518065000805308 valid 0.19305807702243327
LOSS train 0.14518065000805308 valid 0.19299914583910344
LOSS train 0.14518065000805308 valid 0.19294004648035726
LOSS train 0.14518065000805308 valid 0.1930081361148617
LOSS train 0.14518065000805308 valid 0.19300073289655542
LOSS train 0.14518065000805308 valid 0.1929300106695441
LOSS train 0.14518065000805308 valid 0.1929429123555523
LOSS train 0.14518065000805308 valid 0.19291912672088665
LOSS train 0.14518065000805308 valid 0.19283373322483008
LOSS train 0.14518065000805308 valid 0.19284814333356315
LOSS train 0.14518065000805308 valid 0.19291050950846364
LOSS train 0.14518065000805308 valid 0.19292390080606056
LOSS train 0.14518065000805308 valid 0.19289205139741683
LOSS train 0.14518065000805308 valid 0.19305229646424515
LOSS train 0.14518065000805308 valid 0.19308478079119307
LOSS train 0.14518065000805308 valid 0.19300451285782314
LOSS train 0.14518065000805308 valid 0.19293049121676367
LOSS train 0.14518065000805308 valid 0.19296249975259372
LOSS train 0.14518065000805308 valid 0.19299102600359316
LOSS train 0.14518065000805308 valid 0.19310608868221504
LOSS train 0.14518065000805308 valid 0.19303053736221046
LOSS train 0.14518065000805308 valid 0.1931734593625752
LOSS train 0.14518065000805308 valid 0.19314539319147234
LOSS train 0.14518065000805308 valid 0.19313042011146575
LOSS train 0.14518065000805308 valid 0.19322841023497375
LOSS train 0.14518065000805308 valid 0.19323915942357137
LOSS train 0.14518065000805308 valid 0.19337945410901783
LOSS train 0.14518065000805308 valid 0.19343279527688245
LOSS train 0.14518065000805308 valid 0.19341619524014433
LOSS train 0.14518065000805308 valid 0.19348706764743684
LOSS train 0.14518065000805308 valid 0.19348220863577092
LOSS train 0.14518065000805308 valid 0.19337420586462828
LOSS train 0.14518065000805308 valid 0.19327186247760272
LOSS train 0.14518065000805308 valid 0.19332559297124186
LOSS train 0.14518065000805308 valid 0.19343345855435212
LOSS train 0.14518065000805308 valid 0.19339297816379747
LOSS train 0.14518065000805308 valid 0.1934483595680268
LOSS train 0.14518065000805308 valid 0.19343611498526367
LOSS train 0.14518065000805308 valid 0.19342146066607102
LOSS train 0.14518065000805308 valid 0.19343215326028587
LOSS train 0.14518065000805308 valid 0.19338648921864873
LOSS train 0.14518065000805308 valid 0.19325502454972338
LOSS train 0.14518065000805308 valid 0.1932650603005412
LOSS train 0.14518065000805308 valid 0.19330548008485715
LOSS train 0.14518065000805308 valid 0.19350453208438878
LOSS train 0.14518065000805308 valid 0.1935831318946852
LOSS train 0.14518065000805308 valid 0.1936557574811354
LOSS train 0.14518065000805308 valid 0.19349818139924782
LOSS train 0.14518065000805308 valid 0.19343771312342978
LOSS train 0.14518065000805308 valid 0.19343896481317913
LOSS train 0.14518065000805308 valid 0.19337618485093117
LOSS train 0.14518065000805308 valid 0.1932862291266436
LOSS train 0.14518065000805308 valid 0.19327890667641026
LOSS train 0.14518065000805308 valid 0.19327901706901557
LOSS train 0.14518065000805308 valid 0.19331941465284191
LOSS train 0.14518065000805308 valid 0.19339261099066532
LOSS train 0.14518065000805308 valid 0.19340170821530767
LOSS train 0.14518065000805308 valid 0.1934426167891139
LOSS train 0.14518065000805308 valid 0.19334521675326305
LOSS train 0.14518065000805308 valid 0.19336777832099655
LOSS train 0.14518065000805308 valid 0.19332682711796628
LOSS train 0.14518065000805308 valid 0.19329935747748267
LOSS train 0.14518065000805308 valid 0.19336076008763103
LOSS train 0.14518065000805308 valid 0.1933453928463715
LOSS train 0.14518065000805308 valid 0.1933525951149372
LOSS train 0.14518065000805308 valid 0.19339562280945582
LOSS train 0.14518065000805308 valid 0.1933953693452103
LOSS train 0.14518065000805308 valid 0.1932940981896437
LOSS train 0.14518065000805308 valid 0.19331673474010566
LOSS train 0.14518065000805308 valid 0.19338609171188298
EPOCH 4:
  batch 1 loss: 0.1261122077703476
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.12757720798254013
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.12462051709493001
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.13221168145537376
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.13642563819885253
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.13767067591349283
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.1338380692260606
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.13578368816524744
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.1354618693391482
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.13512064293026924
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.13350374657999386
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.13315914881726107
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.13300727365108636
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.13219865571175302
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.13119608014822007
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.13264374574646354
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.13162307879504034
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.13445131149556902
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.13405578544265345
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.1334822691977024
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.13389364026841663
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.13445906815203754
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.13483249752417856
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.1367840077728033
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.13650608479976653
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.136829477090102
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.13715127110481262
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.13646691292524338
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.136237233877182
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.1369748910268148
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.13720532771079771
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.1375102372840047
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.13669894071239413
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.13766914014430606
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.13932925185986927
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.1387786670691437
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.13897522035482768
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.1386448584104839
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.13891704342304131
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.13904621824622154
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.13909767858865785
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.13921442876259485
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.14037665551485018
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.1396768255667253
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.13994823826683891
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.13942902923926062
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.13924491532305452
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.1391888710980614
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.13934364032988644
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.1395283180475235
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.1392946141023262
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.13931313042457288
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.13919250301595004
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.13908202835807093
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.13938638242808254
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.13938154892197677
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.13943620235250706
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.13889199932073726
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.13923607008942104
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.13915415207544962
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.13947435187511756
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.13916370621131313
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.13941180836113673
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.1392097327625379
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.13934276551008223
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.13914597068320622
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.1390229373503087
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.13933793269097805
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.1395871441649354
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.13984166053789002
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.14016393572092056
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.14034446401314604
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.14039490657718215
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.1404474185326615
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.14057397037744523
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.14052400983085758
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.1404376361857761
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.1401853723785816
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.14057683831528772
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.14082045946270227
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.14082697751345458
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.14048579798602476
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.14032230113285132
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.14057760020451887
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.14059632534489913
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.14082791127784308
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.14101411890366983
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.14131385655227033
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.14119092592697466
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.14153660477863417
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.14166449084059224
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.14155152561547962
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.14172968036064537
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.1417009301800677
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.14211394731935703
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.1421428528459122
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.14186582944749557
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.14171606051374455
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.14204000997723956
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.141952575519681
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.1418336878466134
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.1417151951760638
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.14152568564252946
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.14166102056893018
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.14125607098851886
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.14120532628500237
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.14139545799415804
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.14118645395393725
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.14133076820898494
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.14156490510160272
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.1413186699957461
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.1412557282352022
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.14102361291910695
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.14133346185349582
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.14110638155885366
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.14102787672188774
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.1413515110173796
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.14136338202377496
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.14122174210658595
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.14117412449171146
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.141119009391828
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.14115548054458665
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.14125245536972836
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.1411880876628622
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.14122759944200516
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.1413446291689835
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.14137457446085186
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.14111344941193238
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.1410335291379182
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.14099662172106597
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.14086025321984108
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.14079244880739486
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.14104931924800226
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.1411455405403429
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.14090869774421055
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.14081756348776467
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.14094297897859212
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.14087571986559508
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.14109601042896724
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.14110125862061978
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.14113094943000914
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.140956734110352
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.14107484860228492
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.14119241878183353
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.14105914774639852
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.1410668074370247
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.14107689428694395
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.14102076562876636
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.14134567800984288
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.14143222833673158
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.1413199725924738
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.14140794534040124
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.14135918371817646
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.14105038794797736
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.14112531066902223
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.141087447746824
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.14117525095582767
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.14117368381423287
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.1413602478005601
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.1414847380015999
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.14153712696355322
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.14146747187147907
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.14149391454604507
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.14169182578419767
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.14178121582125172
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.14170184143515954
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.14161942377240358
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.1415282597410537
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.1413456406554527
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.14158386110382923
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.1415971848264075
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.1415640642043463
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.14136983144145482
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.14127974177914104
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.1414300345523017
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.14125599297271532
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.14111062282391187
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.140814993990941
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.14103475296297552
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.1408513992196984
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.14088339379149906
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.14086436619470408
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.1407630858851261
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.1406872956811086
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.14070897207066818
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.14064140842165998
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.14068848436528986
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.1405856174198871
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.1405457243244484
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.14056651404029444
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.14055908311411972
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.14044366621722779
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.14041782328810717
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.1401433754597128
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.14015990239687456
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.1403030964108754
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.1403741484441733
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.14053031775836994
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.14049416312470508
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.14064981635659934
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.1406620681211723
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.14068582355238424
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.1407265908685811
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.14076722489998622
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.14072953095523322
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.14059652058968267
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.14059276997611142
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.1404044761394079
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.14048599526642613
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.14063669336693627
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.1406047412004516
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.14061495963976067
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.14068039377250582
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.14053128739801524
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.1406127975777138
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.14049703117322038
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.14047047516442665
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.14055257215412384
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.14061424578433712
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.14057749008590525
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.14055121740604418
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.14056905869159614
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.1405155366044408
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.14058962969907693
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.14047105534209145
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.14045199319984006
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.14052485497381192
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.14035425057405965
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.14031286566929005
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.14050302988161212
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.1407029978705175
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.1406081698449521
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.1406509324332675
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.1406476392577856
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.14070781234731067
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.14070614433642162
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.14062439502543034
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.14067118984310567
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.14067192762217262
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.14058312873045603
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.14069662734680652
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.14066440458140098
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.14071187186878895
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.14073085143673617
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.14071634144199138
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.1408082781647279
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.14076373626587363
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.14083081429764147
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.14085649899450173
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.14076892131567
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.14093474485722196
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.1409819671322429
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.14098139166125195
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.14095026137321953
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.14092090088947146
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.14090708724688739
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.14085814878634442
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.14076842884569205
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.14081856110726546
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.1408375073224306
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.14076713763776866
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.14068995401945733
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.14065800581708154
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.14053339926017958
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.14054778811504257
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.1405722160816641
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.14054637162314818
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.1405374489224224
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.14044143295531822
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.14045073585929693
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.140432564287388
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.14039096299230175
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.14048012078572542
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.1403592732211534
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.14025933306325566
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.14043861538495706
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.14042914857825647
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.14047583130945404
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.1404508182705517
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.14043732160436256
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.1404188484006505
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.14035959470462292
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.14035321279781024
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.14032328173413244
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.14044570005253743
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.14054839591775742
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.1404804291677392
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.14047375437803566
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.14040317510135447
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.1403994028681311
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.14043112470419547
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.140538981133332
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.14071088960654907
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.14072895812846364
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.14070067509251125
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.14070956859536268
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.14071408172548822
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.1406849607475252
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.1406155191486496
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.14060550359388194
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.14062320772694592
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.14061524311931717
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.14060155184257148
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.14055726473758878
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.14052859258944872
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.14053230248148146
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.1405759734565738
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.14051510168650708
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.14049505418268993
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.1403918695786307
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.1404450130663884
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.14039607644558716
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.14034028158496364
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.14031479543276654
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.1403374867070289
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.1404566289739141
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.14043149641165598
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.1404434673300704
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.140370400137849
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.1404316447908059
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.14037784151848975
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.14029892557156012
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.14032216319358753
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.14025054623683295
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.14024846975619976
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.140162638108606
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.14007201911568276
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.14006929946836175
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.14010335874140806
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.14015382955020125
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.14019892400755263
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.14020021742277117
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.14011307228524406
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.14010098021098238
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.1401394693931537
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.1401584386337726
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.1401364925132307
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.14012846136851423
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.14004013142314983
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.14003608557669556
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.1400459264773777
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.1402873389248611
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.1403403108608271
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.14037487766337256
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.1403951671892318
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.14035254365877609
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.14033887986233323
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.1403121773324821
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.14025019081496237
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.1401689851496901
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.14018498282571804
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.1402809079524807
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.14034798270666565
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.14037596356481483
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.14036925928273672
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.14029636324037997
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.1403772498087055
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.14045104952581103
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.14038134115154033
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.14034316684636805
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.14031264590424514
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.14036999120073423
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.14035015645598578
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.14030910520763187
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.14035011048186316
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.14033866518996452
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.1403838313243045
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.14046457450350988
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.1404387878612451
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.14037013468710152
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.14040618323572884
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.14037860160873783
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.14039821591198284
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.14046782390637833
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.14047614816824594
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.14049019842864352
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.1404860084110609
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.14054202584047165
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.14049508707661743
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.14047099434231458
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.1404431682089808
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.14038805673337731
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.14044904123320592
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.1404327600224254
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.14034743082600754
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.14029554420493429
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.14031168198554708
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.14026170417896866
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.1402432964707708
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.14020324966464287
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.14017582619967667
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.14017249345399288
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.1402401420413386
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.1402432906635219
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.14028896661121634
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.1402343497094181
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.14014065147422122
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.1401792564171942
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.14014441766461036
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.14010711329057812
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.14007755294627977
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.1400476295816068
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.14001483899118292
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.14003615120038537
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.14008819617239046
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.14004793265843626
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.14004225741513532
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.14008965352367536
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.14003282781772333
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.14004486443066017
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.14002644471878553
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.1400977104757596
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.14009683209383458
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.1401676289053355
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.1402095210839467
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.1402933822156718
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.1403075365139712
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.14026523898377943
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.14031267607126713
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.14025236316734835
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.14030296691735397
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.14035148754433432
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.14027811923277858
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.14026625793567807
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.1402350023038247
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.14014086844355847
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.1401385559898908
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.1400984229224029
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.1400163357074444
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.1399429780106212
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.1399747502499556
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.13992095602400326
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.1398930966200234
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.13994207914729823
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.1399031413868926
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.13998557099166814
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.14002922339283902
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.13998959500971994
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.13988929102727654
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.1398796967315403
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.13989165048547883
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.13985317579693923
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.139840801517662
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.1398218300126426
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.13982806428429786
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.1398498588178992
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.13982577083561512
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.13987840464272136
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.1398526815602137
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.13983701886402236
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.1398117669306679
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.1398515870910039
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.13979636497844924
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.13979255729703652
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.13975030809313385
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.13975795368222813
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.13977696223253755
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.13979037330530616
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.1397850225498993
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.13984369415303935
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.13983389302086158
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.13983629843889378
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.1397732295433852
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.13980730674390135
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.139837880416583
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.13984960381565176
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.13987420365310071
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.13994526598825413
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.13999601633055633
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.14001892174177982
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.14007926749702218
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.1401738372268313
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.1401738372268313 valid 0.20810982584953308
LOSS train 0.1401738372268313 valid 0.18895086646080017
LOSS train 0.1401738372268313 valid 0.18185841043790182
LOSS train 0.1401738372268313 valid 0.17634717375040054
LOSS train 0.1401738372268313 valid 0.17225171029567718
LOSS train 0.1401738372268313 valid 0.18390367180109024
LOSS train 0.1401738372268313 valid 0.19479124673775264
LOSS train 0.1401738372268313 valid 0.19207127578556538
LOSS train 0.1401738372268313 valid 0.1910240203142166
LOSS train 0.1401738372268313 valid 0.19215383976697922
LOSS train 0.1401738372268313 valid 0.1916991336779161
LOSS train 0.1401738372268313 valid 0.192263533671697
LOSS train 0.1401738372268313 valid 0.1918757683955706
LOSS train 0.1401738372268313 valid 0.1913068081651415
LOSS train 0.1401738372268313 valid 0.18866257667541503
LOSS train 0.1401738372268313 valid 0.18883201107382774
LOSS train 0.1401738372268313 valid 0.18983131471802206
LOSS train 0.1401738372268313 valid 0.1890428140759468
LOSS train 0.1401738372268313 valid 0.19106432873951762
LOSS train 0.1401738372268313 valid 0.1908775508403778
LOSS train 0.1401738372268313 valid 0.19025813185033344
LOSS train 0.1401738372268313 valid 0.18893165005878967
LOSS train 0.1401738372268313 valid 0.18871103421501492
LOSS train 0.1401738372268313 valid 0.18854749264816442
LOSS train 0.1401738372268313 valid 0.18743061602115632
LOSS train 0.1401738372268313 valid 0.1871112768466656
LOSS train 0.1401738372268313 valid 0.1871035866163395
LOSS train 0.1401738372268313 valid 0.18708537359322822
LOSS train 0.1401738372268313 valid 0.18653898711862235
LOSS train 0.1401738372268313 valid 0.18713449190060297
LOSS train 0.1401738372268313 valid 0.1877823419147922
LOSS train 0.1401738372268313 valid 0.18681412376463413
LOSS train 0.1401738372268313 valid 0.1871761426781163
LOSS train 0.1401738372268313 valid 0.18658368333297617
LOSS train 0.1401738372268313 valid 0.18801770423139844
LOSS train 0.1401738372268313 valid 0.18799734488129616
LOSS train 0.1401738372268313 valid 0.18854662171892217
LOSS train 0.1401738372268313 valid 0.1890156374950158
LOSS train 0.1401738372268313 valid 0.18836706494673705
LOSS train 0.1401738372268313 valid 0.18806823380291462
LOSS train 0.1401738372268313 valid 0.1885630753709049
LOSS train 0.1401738372268313 valid 0.18891420357284092
LOSS train 0.1401738372268313 valid 0.18880732634732889
LOSS train 0.1401738372268313 valid 0.18920752135190097
LOSS train 0.1401738372268313 valid 0.18910076750649346
LOSS train 0.1401738372268313 valid 0.18928566963776297
LOSS train 0.1401738372268313 valid 0.1897794065323282
LOSS train 0.1401738372268313 valid 0.18974626157432795
LOSS train 0.1401738372268313 valid 0.1903451760204471
LOSS train 0.1401738372268313 valid 0.18973080575466156
LOSS train 0.1401738372268313 valid 0.18991894786264382
LOSS train 0.1401738372268313 valid 0.18942576417556176
LOSS train 0.1401738372268313 valid 0.18983998023114115
LOSS train 0.1401738372268313 valid 0.18999318640541146
LOSS train 0.1401738372268313 valid 0.18996138274669647
LOSS train 0.1401738372268313 valid 0.18915946329278605
LOSS train 0.1401738372268313 valid 0.18908618756553583
LOSS train 0.1401738372268313 valid 0.1888597754055056
LOSS train 0.1401738372268313 valid 0.18916215558173294
LOSS train 0.1401738372268313 valid 0.18921536058187485
LOSS train 0.1401738372268313 valid 0.18878294942808932
LOSS train 0.1401738372268313 valid 0.18931904963908658
LOSS train 0.1401738372268313 valid 0.18872408923648654
LOSS train 0.1401738372268313 valid 0.18963965820148587
LOSS train 0.1401738372268313 valid 0.18992514197642987
LOSS train 0.1401738372268313 valid 0.189776468909148
LOSS train 0.1401738372268313 valid 0.18906494841646793
LOSS train 0.1401738372268313 valid 0.18910042284166112
LOSS train 0.1401738372268313 valid 0.18867434863594995
LOSS train 0.1401738372268313 valid 0.1890368457351412
LOSS train 0.1401738372268313 valid 0.18865939701946688
LOSS train 0.1401738372268313 valid 0.18887013300425476
LOSS train 0.1401738372268313 valid 0.1888792580121184
LOSS train 0.1401738372268313 valid 0.189016636360336
LOSS train 0.1401738372268313 valid 0.189388125538826
LOSS train 0.1401738372268313 valid 0.18973084794063316
LOSS train 0.1401738372268313 valid 0.18955981654006165
LOSS train 0.1401738372268313 valid 0.18939029635527196
LOSS train 0.1401738372268313 valid 0.1890142326113544
LOSS train 0.1401738372268313 valid 0.18839155063033103
LOSS train 0.1401738372268313 valid 0.18806055188179016
LOSS train 0.1401738372268313 valid 0.18849737669636563
LOSS train 0.1401738372268313 valid 0.188169349747968
LOSS train 0.1401738372268313 valid 0.1880947702697345
LOSS train 0.1401738372268313 valid 0.1880693809074514
LOSS train 0.1401738372268313 valid 0.1877556597424108
LOSS train 0.1401738372268313 valid 0.18754752596904492
LOSS train 0.1401738372268313 valid 0.1872079289433631
LOSS train 0.1401738372268313 valid 0.18789275660273735
LOSS train 0.1401738372268313 valid 0.18788360307614008
LOSS train 0.1401738372268313 valid 0.18793804894436847
LOSS train 0.1401738372268313 valid 0.18788450033120488
LOSS train 0.1401738372268313 valid 0.1876952286048602
LOSS train 0.1401738372268313 valid 0.1878821171344595
LOSS train 0.1401738372268313 valid 0.18781526245568927
LOSS train 0.1401738372268313 valid 0.18788852418462434
LOSS train 0.1401738372268313 valid 0.1877879251524345
LOSS train 0.1401738372268313 valid 0.1879951093269854
LOSS train 0.1401738372268313 valid 0.1881322493456831
LOSS train 0.1401738372268313 valid 0.1881326226890087
LOSS train 0.1401738372268313 valid 0.18835541414152276
LOSS train 0.1401738372268313 valid 0.18858916911424375
LOSS train 0.1401738372268313 valid 0.18833465613786457
LOSS train 0.1401738372268313 valid 0.18849136021274787
LOSS train 0.1401738372268313 valid 0.18854102577481952
LOSS train 0.1401738372268313 valid 0.1889410454709575
LOSS train 0.1401738372268313 valid 0.18886353869304479
LOSS train 0.1401738372268313 valid 0.18900873429245418
LOSS train 0.1401738372268313 valid 0.18934016326151856
LOSS train 0.1401738372268313 valid 0.18950303522023287
LOSS train 0.1401738372268313 valid 0.18942071081281783
LOSS train 0.1401738372268313 valid 0.18930783455393144
LOSS train 0.1401738372268313 valid 0.18936300000785727
LOSS train 0.1401738372268313 valid 0.1895712373549478
LOSS train 0.1401738372268313 valid 0.18974965992181198
LOSS train 0.1401738372268313 valid 0.1899045094333846
LOSS train 0.1401738372268313 valid 0.18993377609130663
LOSS train 0.1401738372268313 valid 0.1896081651418896
LOSS train 0.1401738372268313 valid 0.18925486379811743
LOSS train 0.1401738372268313 valid 0.18895147380729516
LOSS train 0.1401738372268313 valid 0.18876607605248444
LOSS train 0.1401738372268313 valid 0.18882155137472464
LOSS train 0.1401738372268313 valid 0.1886170743926754
LOSS train 0.1401738372268313 valid 0.18893123922809477
LOSS train 0.1401738372268313 valid 0.18875556194782256
LOSS train 0.1401738372268313 valid 0.18915118974825693
LOSS train 0.1401738372268313 valid 0.1889879669495455
LOSS train 0.1401738372268313 valid 0.18902079726103693
LOSS train 0.1401738372268313 valid 0.1892014080008795
LOSS train 0.1401738372268313 valid 0.18891452310176995
LOSS train 0.1401738372268313 valid 0.18864348690018398
LOSS train 0.1401738372268313 valid 0.18831615127397305
LOSS train 0.1401738372268313 valid 0.18833307731420473
LOSS train 0.1401738372268313 valid 0.18843495512186592
LOSS train 0.1401738372268313 valid 0.18831570247809093
LOSS train 0.1401738372268313 valid 0.18829193108660333
LOSS train 0.1401738372268313 valid 0.18802128286257278
LOSS train 0.1401738372268313 valid 0.18795364112525748
LOSS train 0.1401738372268313 valid 0.18787635026647032
LOSS train 0.1401738372268313 valid 0.18799924882394928
LOSS train 0.1401738372268313 valid 0.18787302267044148
LOSS train 0.1401738372268313 valid 0.18790557057085172
LOSS train 0.1401738372268313 valid 0.18785263920997405
LOSS train 0.1401738372268313 valid 0.18789292809863886
LOSS train 0.1401738372268313 valid 0.18775949334276135
LOSS train 0.1401738372268313 valid 0.1879510318171488
LOSS train 0.1401738372268313 valid 0.18795300645082175
LOSS train 0.1401738372268313 valid 0.18878071110796285
LOSS train 0.1401738372268313 valid 0.1888833606003115
LOSS train 0.1401738372268313 valid 0.18881944636503856
LOSS train 0.1401738372268313 valid 0.18901509855756696
LOSS train 0.1401738372268313 valid 0.18870431312212818
LOSS train 0.1401738372268313 valid 0.18870396882879967
LOSS train 0.1401738372268313 valid 0.18856990211582803
LOSS train 0.1401738372268313 valid 0.1884839173286192
LOSS train 0.1401738372268313 valid 0.18849536728782532
LOSS train 0.1401738372268313 valid 0.1885873010014273
LOSS train 0.1401738372268313 valid 0.18862157984624936
LOSS train 0.1401738372268313 valid 0.1887277030157593
LOSS train 0.1401738372268313 valid 0.18867712980136275
LOSS train 0.1401738372268313 valid 0.18854044618443672
LOSS train 0.1401738372268313 valid 0.18838852119666558
LOSS train 0.1401738372268313 valid 0.1881385499531506
LOSS train 0.1401738372268313 valid 0.1879729593127239
LOSS train 0.1401738372268313 valid 0.18781752550240718
LOSS train 0.1401738372268313 valid 0.1878946066261774
LOSS train 0.1401738372268313 valid 0.18822703925435413
LOSS train 0.1401738372268313 valid 0.1881790787336372
LOSS train 0.1401738372268313 valid 0.18837355703291808
LOSS train 0.1401738372268313 valid 0.18841224672163234
LOSS train 0.1401738372268313 valid 0.18840817854418393
LOSS train 0.1401738372268313 valid 0.1882940107999846
LOSS train 0.1401738372268313 valid 0.18832899578389406
LOSS train 0.1401738372268313 valid 0.18829150849032675
LOSS train 0.1401738372268313 valid 0.1880055260658264
LOSS train 0.1401738372268313 valid 0.18795869601043788
LOSS train 0.1401738372268313 valid 0.1880146961259303
LOSS train 0.1401738372268313 valid 0.1882007953323675
LOSS train 0.1401738372268313 valid 0.18811470228533506
LOSS train 0.1401738372268313 valid 0.18803745317790244
LOSS train 0.1401738372268313 valid 0.18816397400850748
LOSS train 0.1401738372268313 valid 0.18800620349881414
LOSS train 0.1401738372268313 valid 0.18807509354229182
LOSS train 0.1401738372268313 valid 0.1879522027042897
LOSS train 0.1401738372268313 valid 0.18783343168529304
LOSS train 0.1401738372268313 valid 0.18786305157087183
LOSS train 0.1401738372268313 valid 0.18771536528745436
LOSS train 0.1401738372268313 valid 0.1876685288041196
LOSS train 0.1401738372268313 valid 0.1875752726244548
LOSS train 0.1401738372268313 valid 0.18767061727611642
LOSS train 0.1401738372268313 valid 0.18762440807844333
LOSS train 0.1401738372268313 valid 0.18768651581679782
LOSS train 0.1401738372268313 valid 0.1874199223178656
LOSS train 0.1401738372268313 valid 0.18731667715864084
LOSS train 0.1401738372268313 valid 0.18715228820458438
LOSS train 0.1401738372268313 valid 0.18713598485503877
LOSS train 0.1401738372268313 valid 0.1873523422001582
LOSS train 0.1401738372268313 valid 0.18728972805870903
LOSS train 0.1401738372268313 valid 0.18738307635388782
LOSS train 0.1401738372268313 valid 0.18722788266837598
LOSS train 0.1401738372268313 valid 0.1870301918900428
LOSS train 0.1401738372268313 valid 0.1869182742320665
LOSS train 0.1401738372268313 valid 0.18691193044479257
LOSS train 0.1401738372268313 valid 0.18709533909956613
LOSS train 0.1401738372268313 valid 0.18689062638980586
LOSS train 0.1401738372268313 valid 0.18693684432113056
LOSS train 0.1401738372268313 valid 0.1868724560967966
LOSS train 0.1401738372268313 valid 0.18670523001884037
LOSS train 0.1401738372268313 valid 0.18669872812962418
LOSS train 0.1401738372268313 valid 0.18662877536955333
LOSS train 0.1401738372268313 valid 0.1866734670519264
LOSS train 0.1401738372268313 valid 0.18664834641341893
LOSS train 0.1401738372268313 valid 0.18663647623968796
LOSS train 0.1401738372268313 valid 0.18666101198330104
LOSS train 0.1401738372268313 valid 0.1865452645823013
LOSS train 0.1401738372268313 valid 0.18642745242902525
LOSS train 0.1401738372268313 valid 0.18639365085808363
LOSS train 0.1401738372268313 valid 0.18645557494611914
LOSS train 0.1401738372268313 valid 0.18655695742395914
LOSS train 0.1401738372268313 valid 0.18656907264481892
LOSS train 0.1401738372268313 valid 0.18648518615178933
LOSS train 0.1401738372268313 valid 0.18654290331645054
LOSS train 0.1401738372268313 valid 0.18669075809519386
LOSS train 0.1401738372268313 valid 0.18679886464295642
LOSS train 0.1401738372268313 valid 0.18692749831411573
LOSS train 0.1401738372268313 valid 0.18715352598017296
LOSS train 0.1401738372268313 valid 0.1872234924774338
LOSS train 0.1401738372268313 valid 0.1873114405755411
LOSS train 0.1401738372268313 valid 0.18733053852897544
LOSS train 0.1401738372268313 valid 0.18735999179923016
LOSS train 0.1401738372268313 valid 0.18751941563247085
LOSS train 0.1401738372268313 valid 0.18755001253609
LOSS train 0.1401738372268313 valid 0.18763132361383397
LOSS train 0.1401738372268313 valid 0.18767120644577548
LOSS train 0.1401738372268313 valid 0.18781658487117037
LOSS train 0.1401738372268313 valid 0.18769045628733555
LOSS train 0.1401738372268313 valid 0.18765405618943243
LOSS train 0.1401738372268313 valid 0.1876700514880549
LOSS train 0.1401738372268313 valid 0.18752357556979526
LOSS train 0.1401738372268313 valid 0.18753497265279293
LOSS train 0.1401738372268313 valid 0.18769133264098425
LOSS train 0.1401738372268313 valid 0.1875459961285276
LOSS train 0.1401738372268313 valid 0.1877172382648099
LOSS train 0.1401738372268313 valid 0.18789596000655753
LOSS train 0.1401738372268313 valid 0.18794598591570952
LOSS train 0.1401738372268313 valid 0.18780996814006712
LOSS train 0.1401738372268313 valid 0.18798069486975189
LOSS train 0.1401738372268313 valid 0.18793836157889135
LOSS train 0.1401738372268313 valid 0.18796877443311683
LOSS train 0.1401738372268313 valid 0.1880218009352684
LOSS train 0.1401738372268313 valid 0.18785918394170434
LOSS train 0.1401738372268313 valid 0.18810889509225648
LOSS train 0.1401738372268313 valid 0.18804982557833902
LOSS train 0.1401738372268313 valid 0.18795220372010404
LOSS train 0.1401738372268313 valid 0.18799169858296713
LOSS train 0.1401738372268313 valid 0.18805831426288933
LOSS train 0.1401738372268313 valid 0.18790297082657945
LOSS train 0.1401738372268313 valid 0.18811542581218157
LOSS train 0.1401738372268313 valid 0.18812650235003026
LOSS train 0.1401738372268313 valid 0.18803496337853945
LOSS train 0.1401738372268313 valid 0.1881793016903245
LOSS train 0.1401738372268313 valid 0.1882301373904898
LOSS train 0.1401738372268313 valid 0.18826171806783276
LOSS train 0.1401738372268313 valid 0.18835369085497927
LOSS train 0.1401738372268313 valid 0.18828767807978505
LOSS train 0.1401738372268313 valid 0.1883296321209212
LOSS train 0.1401738372268313 valid 0.18836612558543459
LOSS train 0.1401738372268313 valid 0.18855791839201058
LOSS train 0.1401738372268313 valid 0.18865149714689716
LOSS train 0.1401738372268313 valid 0.18864664805156214
LOSS train 0.1401738372268313 valid 0.18870417729734934
LOSS train 0.1401738372268313 valid 0.1889785638298182
LOSS train 0.1401738372268313 valid 0.18916764751677112
LOSS train 0.1401738372268313 valid 0.1891842963051622
LOSS train 0.1401738372268313 valid 0.18918945090337233
LOSS train 0.1401738372268313 valid 0.18908267738162607
LOSS train 0.1401738372268313 valid 0.18894065136513555
LOSS train 0.1401738372268313 valid 0.18877606501253388
LOSS train 0.1401738372268313 valid 0.18876510644899047
LOSS train 0.1401738372268313 valid 0.18877150623926095
LOSS train 0.1401738372268313 valid 0.18868512705339655
LOSS train 0.1401738372268313 valid 0.18841336157622066
LOSS train 0.1401738372268313 valid 0.18832394962491922
LOSS train 0.1401738372268313 valid 0.18833969053353222
LOSS train 0.1401738372268313 valid 0.1883761200727078
LOSS train 0.1401738372268313 valid 0.18837666925746244
LOSS train 0.1401738372268313 valid 0.18832281390237476
LOSS train 0.1401738372268313 valid 0.18828538269735873
LOSS train 0.1401738372268313 valid 0.18830092284815533
LOSS train 0.1401738372268313 valid 0.1883370096570459
LOSS train 0.1401738372268313 valid 0.18817906755045108
LOSS train 0.1401738372268313 valid 0.18817876496237435
LOSS train 0.1401738372268313 valid 0.18820447103460494
LOSS train 0.1401738372268313 valid 0.18829163369171473
LOSS train 0.1401738372268313 valid 0.18837704241780912
LOSS train 0.1401738372268313 valid 0.1883083780019267
LOSS train 0.1401738372268313 valid 0.18836246232693443
LOSS train 0.1401738372268313 valid 0.1883863060416391
LOSS train 0.1401738372268313 valid 0.1884569388418692
LOSS train 0.1401738372268313 valid 0.18853217122455437
LOSS train 0.1401738372268313 valid 0.188473636006002
LOSS train 0.1401738372268313 valid 0.18841515330980155
LOSS train 0.1401738372268313 valid 0.1884787974528747
LOSS train 0.1401738372268313 valid 0.18847189968647926
LOSS train 0.1401738372268313 valid 0.18840010447091743
LOSS train 0.1401738372268313 valid 0.188417955380834
LOSS train 0.1401738372268313 valid 0.18839923840877676
LOSS train 0.1401738372268313 valid 0.18831227552871427
LOSS train 0.1401738372268313 valid 0.188325673365863
LOSS train 0.1401738372268313 valid 0.18838623879897978
LOSS train 0.1401738372268313 valid 0.18839886159279723
LOSS train 0.1401738372268313 valid 0.18836864076841336
LOSS train 0.1401738372268313 valid 0.18853978610362487
LOSS train 0.1401738372268313 valid 0.18856639137408535
LOSS train 0.1401738372268313 valid 0.18849060793244649
LOSS train 0.1401738372268313 valid 0.1884193553624651
LOSS train 0.1401738372268313 valid 0.1884571284008703
LOSS train 0.1401738372268313 valid 0.18848489686471862
LOSS train 0.1401738372268313 valid 0.18860864721028409
LOSS train 0.1401738372268313 valid 0.1885377142811194
LOSS train 0.1401738372268313 valid 0.18868781586766614
LOSS train 0.1401738372268313 valid 0.18866263952407036
LOSS train 0.1401738372268313 valid 0.18864655939879432
LOSS train 0.1401738372268313 valid 0.18875089828154923
LOSS train 0.1401738372268313 valid 0.18875590588037786
LOSS train 0.1401738372268313 valid 0.18890062909276206
LOSS train 0.1401738372268313 valid 0.18894955381432074
LOSS train 0.1401738372268313 valid 0.18892933226122363
LOSS train 0.1401738372268313 valid 0.1889949779515933
LOSS train 0.1401738372268313 valid 0.1889932447084875
LOSS train 0.1401738372268313 valid 0.18888333113442016
LOSS train 0.1401738372268313 valid 0.18877725642039833
LOSS train 0.1401738372268313 valid 0.18883155695758425
LOSS train 0.1401738372268313 valid 0.18893056664013577
LOSS train 0.1401738372268313 valid 0.18889029406344712
LOSS train 0.1401738372268313 valid 0.18895201041318832
LOSS train 0.1401738372268313 valid 0.188941159105973
LOSS train 0.1401738372268313 valid 0.18893260078934523
LOSS train 0.1401738372268313 valid 0.18894624945248825
LOSS train 0.1401738372268313 valid 0.18889419688459705
LOSS train 0.1401738372268313 valid 0.1887587126879748
LOSS train 0.1401738372268313 valid 0.18876810339197778
LOSS train 0.1401738372268313 valid 0.18881162725193507
LOSS train 0.1401738372268313 valid 0.18901222210030916
LOSS train 0.1401738372268313 valid 0.18909192121979118
LOSS train 0.1401738372268313 valid 0.18917692695997354
LOSS train 0.1401738372268313 valid 0.18901910594200194
LOSS train 0.1401738372268313 valid 0.18895937042075328
LOSS train 0.1401738372268313 valid 0.18895922056839595
LOSS train 0.1401738372268313 valid 0.18889625112925257
LOSS train 0.1401738372268313 valid 0.1888065369338052
LOSS train 0.1401738372268313 valid 0.18880064419301396
LOSS train 0.1401738372268313 valid 0.18880266160110576
LOSS train 0.1401738372268313 valid 0.18884369333164167
LOSS train 0.1401738372268313 valid 0.18890968235865446
LOSS train 0.1401738372268313 valid 0.18891764312898846
LOSS train 0.1401738372268313 valid 0.18895975070423773
LOSS train 0.1401738372268313 valid 0.1888613334373056
LOSS train 0.1401738372268313 valid 0.1888785316377961
LOSS train 0.1401738372268313 valid 0.18883739993390108
LOSS train 0.1401738372268313 valid 0.18880326684739782
LOSS train 0.1401738372268313 valid 0.18886602503242414
LOSS train 0.1401738372268313 valid 0.1888533951535041
LOSS train 0.1401738372268313 valid 0.18885273914170134
LOSS train 0.1401738372268313 valid 0.1888953615745453
LOSS train 0.1401738372268313 valid 0.1889033288455726
LOSS train 0.1401738372268313 valid 0.1888027469087037
LOSS train 0.1401738372268313 valid 0.18882808253250044
LOSS train 0.1401738372268313 valid 0.18890361041638262
EPOCH 5:
  batch 1 loss: 0.12026859819889069
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.12240422144532204
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.12308406084775925
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.1298611294478178
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.13329228907823562
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.1345516654352347
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.13034484109708241
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.13197290431708097
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.13176014605495665
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.13093843385577203
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.12927420843731274
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.12873508284489313
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.1286367384287027
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.1278794833592006
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.12700030505657195
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.12848967220634222
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.12768572481239543
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.13053344769610298
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.13023652685315987
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.12956879436969757
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.1300651062102545
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.1304302533919161
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.1308763176202774
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.13263283483684063
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.13230518013238907
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.13256881930507147
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.13279190687117753
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.1321888238723789
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.1320356729215589
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.13284751350680987
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.13306255085814384
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.13350950018502772
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.1328860961578109
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.13395210771876223
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.13573281105075563
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.13521075724727577
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.13536692974535194
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.1349367679733979
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.1352998931438495
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.13547770418226718
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.13556169773020396
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.13570038654974528
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.1368067063564478
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.13623398779468102
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.13648485309547848
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.13595941468425418
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.13580392015741227
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.13586456390718618
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.13599407946576877
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.13605112493038177
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.13587714234987894
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.135965885164646
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.1357639417895731
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.13560130060822875
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.1360379769043489
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.13624572407986438
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.13634069839067625
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.13583261789432888
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.13619532309851404
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.13600796423852443
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.13627834068458589
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.13604520105065837
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.13623417641908403
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.13606494653504342
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.1361973055280172
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.1360167929846229
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.13581452729986676
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.1361132378087324
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.1363215995007667
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.13659840886081967
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.1368918540612073
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.13706672398580444
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.1370853847020293
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.13718692616030975
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.13728605469067892
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.13716975540706985
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.1371266394853592
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.13684848934794083
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.13722038844340964
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.13745393762364982
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.13750448482639996
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.1371037008740553
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.13698989449136229
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.13734401416565692
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.1373271497733453
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.13763327489412108
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.1378261788651861
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.13818301184272225
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.13812251832712902
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.1384737775557571
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.1385614738523305
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.13847931514939535
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.1386248409267395
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.13858659145362834
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.13903290213722933
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.13911812046232322
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.1388435783920829
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.13863814112787343
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.1389470591840118
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.13885596178472043
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.13869097865749114
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.13857274860435842
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.1383852142008763
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.13850142541699684
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.1381001998271261
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.13807395079507018
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.13831021163229631
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.13808221894281883
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.13821026012984985
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.13842275291681289
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.13817754159639548
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.13809111714363098
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.13784945340810623
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.13814099577435277
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.1379139834124109
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.13782166972242552
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.13816997345186707
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.1381762613684444
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.13802841330776697
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.13797262459993362
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.13792173527489024
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.13798137241211095
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.1380910171968181
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.13802288052055142
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.13806800210475922
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.13814779595723228
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.1381992040656683
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.1379583157831803
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.13789105126562046
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.13785676268430858
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.13771511978320494
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.1376615400341424
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.13793070435075833
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.1380233382111165
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.1377984126408895
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.13768398980883992
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.13785789787334246
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.13779055456752362
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.13799155015739606
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.1380046264401504
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.13799699517429298
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.1378041649053634
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.13795175028222423
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.1380981780692107
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.1379648546720373
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.13801637679746706
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.13803425106872508
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.1379977832170757
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.13833538567859854
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.13840254694223403
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.13827429248007717
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.13834649089135623
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.13833358513763527
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.13801613512944866
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.13812917764148405
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.13807751405506563
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.1381370156149196
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.1381118094336383
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.13825023628818164
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.13836704888381063
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.1384469508106664
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.13838665495500152
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.1384008593080234
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.13861582323727084
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.1387226378827384
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.1386529099959207
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.13853881606263316
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.1384772900048466
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.1382781927578548
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.13851639838779675
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.13851642469216507
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.13848954395845878
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.1382956308005862
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.1382012447320867
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.13832446187734604
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.13812825151465155
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.13798798591235265
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.1377039496483428
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.13793120745507031
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.13777464851737023
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.1378298107102431
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.13781369698571636
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.13771067388722155
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.13762373052051532
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.13765000713032646
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.13758998808841552
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.13764966510355792
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.13754990193596545
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.13751862923461924
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.1375456130426181
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.1375672731021936
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.13746643667885414
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.13746293765430007
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.13720333929528894
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.13722596153234823
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.1373791994184864
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.13745051531622252
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.1376108250232658
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.13755434182421047
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.1377117119729519
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.13771138862887425
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.13771774973904732
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.1377566104452011
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.13779429916073294
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.137754177901803
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.1376222471516688
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.13761836545910813
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.1374493591988889
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.13752262889625924
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.1376428603771187
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.13760447427983533
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.1376115800285677
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.13766769641581836
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.13751145278181987
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.13759391890015713
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.13747937642727737
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.13745648777841973
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.137551687828718
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.13761487783498416
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.13758885945108804
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.13755952047681377
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.1375848409627472
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.1375246048975953
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.13759508792177907
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.13748271839486229
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.13746973031521897
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.13756499459171084
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.13737757373274417
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.1373355930028524
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.13754098920718483
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.13774654036992556
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.13768609741638446
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.1377483145873434
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.137770904498732
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.13781259262815435
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.13783810559218213
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.13778102715432897
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.13784535107116738
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.13786295209095567
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.1377815417945385
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.1379347953064313
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.13787820313341362
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.1379320257853088
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.1379544565790012
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.13795808955114716
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.138052626838529
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.1380210662419014
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.13807394836218126
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.13810677946092614
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.13803535452485086
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.1382065510073031
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.1382456947827623
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.13825532202075122
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.13821459077591972
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.1381499595209664
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.1381487901962828
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.1380884636063056
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.13800147926622583
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.13806981193513024
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.13807188719511032
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.1380054792853151
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.13794512150965574
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.1379150502638672
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.13777299002377372
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.13779954114612544
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.13781823814475447
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.13778716925713008
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.13778358411544295
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.13768915228240994
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.13770824830841136
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.13768544270763539
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.13764263459426515
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.13774849594512703
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.13762879591897456
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.137518381124193
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.13768820229755796
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.13767865362042556
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.1377270919331115
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.1377014713528763
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.1376981955287712
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.13766059027553879
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.13761161354945062
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.13760346966687023
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.13756452220111665
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.13767804479912707
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.1377669074995951
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.1376879359060048
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.137692184239212
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.13762058382112674
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.1376427072646289
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.13767556244462625
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.13778754147662692
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.13794483964048554
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.13797476829416086
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.1379348733920162
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.13794805313385017
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.13793958891622146
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.13789668961759383
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.13784140615060578
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.13781415653725465
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.13782939594548407
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.13782575408255818
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.13781033448948718
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.1377833475554852
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.13775184313293362
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.1377476408736768
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.13779827348087043
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.13775089828224926
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.13772159477267837
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.137618882473438
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.13768976859724408
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.13764229779824233
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.1375891629118508
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.13756363558921086
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.13756372569099304
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.13768158994520766
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.1376583905250116
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.13764562888902687
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.1375667897277865
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.13765411803033203
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.1376017109720135
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.13753044720947372
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.13756143806888593
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.1374862371587459
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.13748813225672796
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.1374195120268804
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.13733365960897656
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.1373229478599458
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.1373661692879845
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.1374137801428636
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.13744493218256987
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.1374480808041541
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.13737012855372988
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.1373745954545315
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.13743154466597002
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.1374562676508157
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.13744067400693893
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.13744442118607328
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.13735392962233506
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.1373484725899556
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.137362837179665
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.13761814448394274
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.13766111216287918
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.13770477651336857
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.13771368861198424
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.13765262536747608
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.13764153562636472
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.13762857824906535
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.1375549903144809
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.13746441104582377
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.13746968332009438
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.13755655263296582
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.13762774223477578
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.13764475319681868
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.13763433476568948
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.13755577122478674
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.13764092541124975
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.13770859965315743
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.13763456905784713
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.13759969994425775
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.13755080723036028
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.1376135983602118
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.13758723466521125
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.13754065815809663
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.13758976571772197
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.13757206000835517
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.13761403261397126
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.1376990658716987
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.13766934484404922
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.13760585948019413
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.13762876039968347
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.1376088425637253
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.1376208979985989
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.1377005248186104
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.13772706323862074
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.13773722607800934
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.13772386302603334
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.13778612796197492
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.13772141760446152
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.13770068664299814
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.1376882380075029
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.13762909753238342
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.13771149984928086
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.13771531938497597
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.1376486339158826
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.13760506408047798
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.1376459264709044
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.13759899629068742
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.13757973229440748
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.13752606835884926
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.137516241053791
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.13752136109586882
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.13763104299552567
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.13765948091817992
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.1376931347424471
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.13765784509178966
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.1375600792261155
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.1376084320846215
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.13758436859326256
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.13755198128521443
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.13755578133382107
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.13752083332087864
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.13748669903494287
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.13751714848129465
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.13758171357122467
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.13754921155096275
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.13756146992234106
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.13760966195852734
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.13759093396456726
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.13762483936620923
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.13761747965629953
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.13769603757531318
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.13772629131055628
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.1377998800849281
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.13786157802285917
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.1379622566836098
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.13797367907328
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.13796306025397265
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.13802062720060349
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.1379929396545603
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.13807690071026107
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.13813079612868093
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.13807272509480198
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.13807697875319785
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.13806696039788863
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.13796541986750885
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.13796574090208327
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.1379178382073329
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.13785680065621864
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.13779243095669635
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.13782931753626557
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.13778059407033855
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.13776604237581236
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.13782380591683124
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.13778505150614115
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.13788311726866512
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.13795745614873461
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.13791553014421573
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.13780952927259757
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.13779886082153428
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.13780881922328825
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.13775497907561954
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.13776696401040656
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.137752021151083
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.13777125638522458
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.13778375804157
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.13776298867376058
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.1378222849951791
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.13780312303179887
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.13779412945111594
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.13776564721984505
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.13779992374500868
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.13775138086567387
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.1377423203792341
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.13769505595440393
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.1377043959504941
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.1377150906286563
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.13773238458813017
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.13773743722238832
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.13779625978482807
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.1378033988808086
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.13779247066739833
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.13772781912567292
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.1377656976050087
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.1377965253367219
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.13782338313598488
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.1378482974092506
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.13792602639868218
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.13796835824815448
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.13801227701788252
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.138065647151946
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.13815661325608775
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.13815661325608775 valid 0.20101526379585266
LOSS train 0.13815661325608775 valid 0.18122640252113342
LOSS train 0.13815661325608775 valid 0.17524034281571707
LOSS train 0.13815661325608775 valid 0.16930600628256798
LOSS train 0.13815661325608775 valid 0.16507765054702758
LOSS train 0.13815661325608775 valid 0.17662908136844635
LOSS train 0.13815661325608775 valid 0.18752711159842356
LOSS train 0.13815661325608775 valid 0.1847085990011692
LOSS train 0.13815661325608775 valid 0.18360001511043972
LOSS train 0.13815661325608775 valid 0.1843832716345787
LOSS train 0.13815661325608775 valid 0.1836110082539645
LOSS train 0.13815661325608775 valid 0.1843149041136106
LOSS train 0.13815661325608775 valid 0.18407186751182264
LOSS train 0.13815661325608775 valid 0.18338325193950109
LOSS train 0.13815661325608775 valid 0.1808344523111979
LOSS train 0.13815661325608775 valid 0.18091212399303913
LOSS train 0.13815661325608775 valid 0.18189943187377033
LOSS train 0.13815661325608775 valid 0.18102660692400402
LOSS train 0.13815661325608775 valid 0.18318123958612742
LOSS train 0.13815661325608775 valid 0.18312679305672647
LOSS train 0.13815661325608775 valid 0.18256175447077977
LOSS train 0.13815661325608775 valid 0.18131914802572943
LOSS train 0.13815661325608775 valid 0.1811646527570227
LOSS train 0.13815661325608775 valid 0.18107621185481548
LOSS train 0.13815661325608775 valid 0.17989926517009736
LOSS train 0.13815661325608775 valid 0.1796072549544848
LOSS train 0.13815661325608775 valid 0.17970212079860545
LOSS train 0.13815661325608775 valid 0.17957225814461708
LOSS train 0.13815661325608775 valid 0.1789994229530466
LOSS train 0.13815661325608775 valid 0.17956623832384747
LOSS train 0.13815661325608775 valid 0.18027470284892666
LOSS train 0.13815661325608775 valid 0.17930483585223556
LOSS train 0.13815661325608775 valid 0.17964038839845947
LOSS train 0.13815661325608775 valid 0.17911231079522302
LOSS train 0.13815661325608775 valid 0.1806369423866272
LOSS train 0.13815661325608775 valid 0.18060824068056214
LOSS train 0.13815661325608775 valid 0.18113954365253448
LOSS train 0.13815661325608775 valid 0.18166011807165647
LOSS train 0.13815661325608775 valid 0.18105289951348916
LOSS train 0.13815661325608775 valid 0.18069672919809818
LOSS train 0.13815661325608775 valid 0.18116529649350702
LOSS train 0.13815661325608775 valid 0.18146415977250963
LOSS train 0.13815661325608775 valid 0.18134226840595866
LOSS train 0.13815661325608775 valid 0.1817175302315842
LOSS train 0.13815661325608775 valid 0.18176859319210054
LOSS train 0.13815661325608775 valid 0.18198197075854178
LOSS train 0.13815661325608775 valid 0.1824655459916338
LOSS train 0.13815661325608775 valid 0.18233609230568013
LOSS train 0.13815661325608775 valid 0.1829182785384509
LOSS train 0.13815661325608775 valid 0.1823393702507019
LOSS train 0.13815661325608775 valid 0.18247673616689794
LOSS train 0.13815661325608775 valid 0.1819635686966089
LOSS train 0.13815661325608775 valid 0.18240946250141793
LOSS train 0.13815661325608775 valid 0.18257594412123715
LOSS train 0.13815661325608775 valid 0.1825449526309967
LOSS train 0.13815661325608775 valid 0.18172445840069226
LOSS train 0.13815661325608775 valid 0.1816951697855665
LOSS train 0.13815661325608775 valid 0.18147623950037464
LOSS train 0.13815661325608775 valid 0.18181921920533908
LOSS train 0.13815661325608775 valid 0.18189074322581292
LOSS train 0.13815661325608775 valid 0.18146219507592623
LOSS train 0.13815661325608775 valid 0.18197332130324456
LOSS train 0.13815661325608775 valid 0.18138419730322702
LOSS train 0.13815661325608775 valid 0.1822944930754602
LOSS train 0.13815661325608775 valid 0.18254273854769193
LOSS train 0.13815661325608775 valid 0.18239059728203397
LOSS train 0.13815661325608775 valid 0.18167948144585339
LOSS train 0.13815661325608775 valid 0.18170894266051404
LOSS train 0.13815661325608775 valid 0.18126892957134524
LOSS train 0.13815661325608775 valid 0.18166895295892443
LOSS train 0.13815661325608775 valid 0.1813089068926556
LOSS train 0.13815661325608775 valid 0.18152480634550253
LOSS train 0.13815661325608775 valid 0.1815316036139449
LOSS train 0.13815661325608775 valid 0.18164564226124738
LOSS train 0.13815661325608775 valid 0.18199629426002503
LOSS train 0.13815661325608775 valid 0.18234319259461604
LOSS train 0.13815661325608775 valid 0.18217220708921358
LOSS train 0.13815661325608775 valid 0.18198072490019676
LOSS train 0.13815661325608775 valid 0.18160467057288449
LOSS train 0.13815661325608775 valid 0.1809773040935397
LOSS train 0.13815661325608775 valid 0.18068338765038383
LOSS train 0.13815661325608775 valid 0.18115037407090023
LOSS train 0.13815661325608775 valid 0.18085064317088528
LOSS train 0.13815661325608775 valid 0.1808168011761847
LOSS train 0.13815661325608775 valid 0.18075222092516283
LOSS train 0.13815661325608775 valid 0.18048421276170154
LOSS train 0.13815661325608775 valid 0.1803305334058301
LOSS train 0.13815661325608775 valid 0.18000137907537547
LOSS train 0.13815661325608775 valid 0.1806540639882677
LOSS train 0.13815661325608775 valid 0.18063536584377288
LOSS train 0.13815661325608775 valid 0.18071713192122324
LOSS train 0.13815661325608775 valid 0.18067212580986644
LOSS train 0.13815661325608775 valid 0.1804651039582427
LOSS train 0.13815661325608775 valid 0.1806372304229026
LOSS train 0.13815661325608775 valid 0.1805422577418779
LOSS train 0.13815661325608775 valid 0.1806129147298634
LOSS train 0.13815661325608775 valid 0.18051619750937237
LOSS train 0.13815661325608775 valid 0.18073338133339978
LOSS train 0.13815661325608775 valid 0.18084708653917217
LOSS train 0.13815661325608775 valid 0.18086816877126694
LOSS train 0.13815661325608775 valid 0.18103508075865188
LOSS train 0.13815661325608775 valid 0.18125744587650486
LOSS train 0.13815661325608775 valid 0.18098707887732868
LOSS train 0.13815661325608775 valid 0.18117315049927968
LOSS train 0.13815661325608775 valid 0.18123590307576315
LOSS train 0.13815661325608775 valid 0.18163651284181848
LOSS train 0.13815661325608775 valid 0.18155152594374718
LOSS train 0.13815661325608775 valid 0.181705163170894
LOSS train 0.13815661325608775 valid 0.18203272075828061
LOSS train 0.13815661325608775 valid 0.18216005848212677
LOSS train 0.13815661325608775 valid 0.18208569167433558
LOSS train 0.13815661325608775 valid 0.18197021141116107
LOSS train 0.13815661325608775 valid 0.18203471548261896
LOSS train 0.13815661325608775 valid 0.1822764991145385
LOSS train 0.13815661325608775 valid 0.1824694511683091
LOSS train 0.13815661325608775 valid 0.18260401191896405
LOSS train 0.13815661325608775 valid 0.18261567382221547
LOSS train 0.13815661325608775 valid 0.18231418021654677
LOSS train 0.13815661325608775 valid 0.18197634991477518
LOSS train 0.13815661325608775 valid 0.18167518377304076
LOSS train 0.13815661325608775 valid 0.18149872629110478
LOSS train 0.13815661325608775 valid 0.18155206910899427
LOSS train 0.13815661325608775 valid 0.1813340718910946
LOSS train 0.13815661325608775 valid 0.18166483470028447
LOSS train 0.13815661325608775 valid 0.18147309052944183
LOSS train 0.13815661325608775 valid 0.18186770119364298
LOSS train 0.13815661325608775 valid 0.18170348361251862
LOSS train 0.13815661325608775 valid 0.18173066445160657
LOSS train 0.13815661325608775 valid 0.18191031799759977
LOSS train 0.13815661325608775 valid 0.18161458533543806
LOSS train 0.13815661325608775 valid 0.1813369657247121
LOSS train 0.13815661325608775 valid 0.18099906013318987
LOSS train 0.13815661325608775 valid 0.18099748046326458
LOSS train 0.13815661325608775 valid 0.18112780064789216
LOSS train 0.13815661325608775 valid 0.18099891675843133
LOSS train 0.13815661325608775 valid 0.180969867417041
LOSS train 0.13815661325608775 valid 0.18069508704390838
LOSS train 0.13815661325608775 valid 0.18062489468982254
LOSS train 0.13815661325608775 valid 0.18054680676340198
LOSS train 0.13815661325608775 valid 0.1806749641895294
LOSS train 0.13815661325608775 valid 0.18053321698878674
LOSS train 0.13815661325608775 valid 0.18054801939238965
LOSS train 0.13815661325608775 valid 0.18046550942467643
LOSS train 0.13815661325608775 valid 0.18048213194641802
LOSS train 0.13815661325608775 valid 0.18034388998459125
LOSS train 0.13815661325608775 valid 0.18054304731218782
LOSS train 0.13815661325608775 valid 0.1805462855465558
LOSS train 0.13815661325608775 valid 0.18138581393538294
LOSS train 0.13815661325608775 valid 0.18147435504318082
LOSS train 0.13815661325608775 valid 0.181414346297582
LOSS train 0.13815661325608775 valid 0.18163080957551667
LOSS train 0.13815661325608775 valid 0.18131670091105134
LOSS train 0.13815661325608775 valid 0.18131361531665902
LOSS train 0.13815661325608775 valid 0.18118030800447835
LOSS train 0.13815661325608775 valid 0.1811007309344507
LOSS train 0.13815661325608775 valid 0.18112503536618674
LOSS train 0.13815661325608775 valid 0.18122300420217452
LOSS train 0.13815661325608775 valid 0.18123795525937142
LOSS train 0.13815661325608775 valid 0.1813386880189368
LOSS train 0.13815661325608775 valid 0.18128071576356888
LOSS train 0.13815661325608775 valid 0.18114423353849732
LOSS train 0.13815661325608775 valid 0.18100474333321606
LOSS train 0.13815661325608775 valid 0.18075036307785408
LOSS train 0.13815661325608775 valid 0.18059109369429147
LOSS train 0.13815661325608775 valid 0.18042417927221818
LOSS train 0.13815661325608775 valid 0.18050946358097605
LOSS train 0.13815661325608775 valid 0.18084114218900305
LOSS train 0.13815661325608775 valid 0.1807821471066702
LOSS train 0.13815661325608775 valid 0.18098075150032722
LOSS train 0.13815661325608775 valid 0.18100001040626976
LOSS train 0.13815661325608775 valid 0.18100513718281572
LOSS train 0.13815661325608775 valid 0.18089737327292907
LOSS train 0.13815661325608775 valid 0.1809155416454194
LOSS train 0.13815661325608775 valid 0.18087752805701618
LOSS train 0.13815661325608775 valid 0.18057718021529062
LOSS train 0.13815661325608775 valid 0.18052932179786943
LOSS train 0.13815661325608775 valid 0.1805825961680062
LOSS train 0.13815661325608775 valid 0.1807961423745316
LOSS train 0.13815661325608775 valid 0.18071864087488398
LOSS train 0.13815661325608775 valid 0.18062961730692123
LOSS train 0.13815661325608775 valid 0.18075895358844357
LOSS train 0.13815661325608775 valid 0.18059933922448002
LOSS train 0.13815661325608775 valid 0.18067690469528155
LOSS train 0.13815661325608775 valid 0.18053950150699719
LOSS train 0.13815661325608775 valid 0.1804308276724171
LOSS train 0.13815661325608775 valid 0.1804514229297638
LOSS train 0.13815661325608775 valid 0.1802956197351058
LOSS train 0.13815661325608775 valid 0.18023029207549196
LOSS train 0.13815661325608775 valid 0.18013546123075738
LOSS train 0.13815661325608775 valid 0.18022687795915102
LOSS train 0.13815661325608775 valid 0.180152857334826
LOSS train 0.13815661325608775 valid 0.18022451658422747
LOSS train 0.13815661325608775 valid 0.17995093762874603
LOSS train 0.13815661325608775 valid 0.1798625161352846
LOSS train 0.13815661325608775 valid 0.17968722314406663
LOSS train 0.13815661325608775 valid 0.17968594989910416
LOSS train 0.13815661325608775 valid 0.17991268589411896
LOSS train 0.13815661325608775 valid 0.1798485748545088
LOSS train 0.13815661325608775 valid 0.17994803789272978
LOSS train 0.13815661325608775 valid 0.17980958238244057
LOSS train 0.13815661325608775 valid 0.1796180207782717
LOSS train 0.13815661325608775 valid 0.17949741333723068
LOSS train 0.13815661325608775 valid 0.17948920562349516
LOSS train 0.13815661325608775 valid 0.17967986680713355
LOSS train 0.13815661325608775 valid 0.17947216063010984
LOSS train 0.13815661325608775 valid 0.17950828764045124
LOSS train 0.13815661325608775 valid 0.1794419288635254
LOSS train 0.13815661325608775 valid 0.1792728711779301
LOSS train 0.13815661325608775 valid 0.17926103019258052
LOSS train 0.13815661325608775 valid 0.17919448159989856
LOSS train 0.13815661325608775 valid 0.17922774621095702
LOSS train 0.13815661325608775 valid 0.17918485585811003
LOSS train 0.13815661325608775 valid 0.17917658955278531
LOSS train 0.13815661325608775 valid 0.17920936010429792
LOSS train 0.13815661325608775 valid 0.17908992711887803
LOSS train 0.13815661325608775 valid 0.17895810271578808
LOSS train 0.13815661325608775 valid 0.178912830105575
LOSS train 0.13815661325608775 valid 0.17897677332709688
LOSS train 0.13815661325608775 valid 0.17907560783434132
LOSS train 0.13815661325608775 valid 0.17908222804015334
LOSS train 0.13815661325608775 valid 0.17899051659247456
LOSS train 0.13815661325608775 valid 0.17905377415386406
LOSS train 0.13815661325608775 valid 0.17921812244327615
LOSS train 0.13815661325608775 valid 0.17932611324691347
LOSS train 0.13815661325608775 valid 0.17946315169334412
LOSS train 0.13815661325608775 valid 0.1796995762702638
LOSS train 0.13815661325608775 valid 0.17976365464899507
LOSS train 0.13815661325608775 valid 0.17984085445079886
LOSS train 0.13815661325608775 valid 0.17985078679422103
LOSS train 0.13815661325608775 valid 0.17988764403954796
LOSS train 0.13815661325608775 valid 0.18004769764163278
LOSS train 0.13815661325608775 valid 0.18007048919540028
LOSS train 0.13815661325608775 valid 0.18015263846759633
LOSS train 0.13815661325608775 valid 0.18021021261174455
LOSS train 0.13815661325608775 valid 0.18036156258684524
LOSS train 0.13815661325608775 valid 0.18023260865928764
LOSS train 0.13815661325608775 valid 0.1801833854948921
LOSS train 0.13815661325608775 valid 0.18020603455164852
LOSS train 0.13815661325608775 valid 0.18005061829189875
LOSS train 0.13815661325608775 valid 0.18005794094254574
LOSS train 0.13815661325608775 valid 0.1802187062398032
LOSS train 0.13815661325608775 valid 0.18006475485307127
LOSS train 0.13815661325608775 valid 0.18024615219836373
LOSS train 0.13815661325608775 valid 0.18042159587389134
LOSS train 0.13815661325608775 valid 0.18048261884523897
LOSS train 0.13815661325608775 valid 0.1803477124227741
LOSS train 0.13815661325608775 valid 0.18051599165206014
LOSS train 0.13815661325608775 valid 0.1804742039812188
LOSS train 0.13815661325608775 valid 0.18049516226632528
LOSS train 0.13815661325608775 valid 0.18054782372713088
LOSS train 0.13815661325608775 valid 0.18037585629172534
LOSS train 0.13815661325608775 valid 0.18061848351406673
LOSS train 0.13815661325608775 valid 0.18055263232336685
LOSS train 0.13815661325608775 valid 0.18046717140383609
LOSS train 0.13815661325608775 valid 0.18050188007307988
LOSS train 0.13815661325608775 valid 0.18056599789997563
LOSS train 0.13815661325608775 valid 0.18040567845222089
LOSS train 0.13815661325608775 valid 0.18061139567415843
LOSS train 0.13815661325608775 valid 0.18061952494286201
LOSS train 0.13815661325608775 valid 0.18051067189528391
LOSS train 0.13815661325608775 valid 0.18067324732455256
LOSS train 0.13815661325608775 valid 0.18073240514019973
LOSS train 0.13815661325608775 valid 0.1807693256058167
LOSS train 0.13815661325608775 valid 0.1808563657104969
LOSS train 0.13815661325608775 valid 0.18079267667149598
LOSS train 0.13815661325608775 valid 0.18084618314764553
LOSS train 0.13815661325608775 valid 0.18088691639766263
LOSS train 0.13815661325608775 valid 0.18107308464041397
LOSS train 0.13815661325608775 valid 0.18116964421972467
LOSS train 0.13815661325608775 valid 0.1811581617704144
LOSS train 0.13815661325608775 valid 0.18122360990056252
LOSS train 0.13815661325608775 valid 0.18151309428846135
LOSS train 0.13815661325608775 valid 0.18170336609358315
LOSS train 0.13815661325608775 valid 0.18171853931063284
LOSS train 0.13815661325608775 valid 0.18172190406105734
LOSS train 0.13815661325608775 valid 0.1816218306944854
LOSS train 0.13815661325608775 valid 0.18148868797272982
LOSS train 0.13815661325608775 valid 0.18132733537567605
LOSS train 0.13815661325608775 valid 0.18130545264717499
LOSS train 0.13815661325608775 valid 0.18130851489092623
LOSS train 0.13815661325608775 valid 0.18122955255236914
LOSS train 0.13815661325608775 valid 0.1809560383359591
LOSS train 0.13815661325608775 valid 0.18086399575004308
LOSS train 0.13815661325608775 valid 0.18087571360905405
LOSS train 0.13815661325608775 valid 0.1809053482193696
LOSS train 0.13815661325608775 valid 0.18091160022503847
LOSS train 0.13815661325608775 valid 0.1808617364238779
LOSS train 0.13815661325608775 valid 0.1808207649220195
LOSS train 0.13815661325608775 valid 0.1808443825859512
LOSS train 0.13815661325608775 valid 0.1808807269766413
LOSS train 0.13815661325608775 valid 0.18072180506290028
LOSS train 0.13815661325608775 valid 0.18072040338222295
LOSS train 0.13815661325608775 valid 0.1807388033679728
LOSS train 0.13815661325608775 valid 0.18082526878637523
LOSS train 0.13815661325608775 valid 0.18091405988749812
LOSS train 0.13815661325608775 valid 0.1808431137957283
LOSS train 0.13815661325608775 valid 0.180914716337265
LOSS train 0.13815661325608775 valid 0.18093995979968333
LOSS train 0.13815661325608775 valid 0.18100383987394864
LOSS train 0.13815661325608775 valid 0.181082437535127
LOSS train 0.13815661325608775 valid 0.18103492363940837
LOSS train 0.13815661325608775 valid 0.18097816659322638
LOSS train 0.13815661325608775 valid 0.181038095907803
LOSS train 0.13815661325608775 valid 0.1810303288266847
LOSS train 0.13815661325608775 valid 0.18096299112820236
LOSS train 0.13815661325608775 valid 0.18099016516037236
LOSS train 0.13815661325608775 valid 0.18097761520927813
LOSS train 0.13815661325608775 valid 0.18089165877212177
LOSS train 0.13815661325608775 valid 0.18090819024923935
LOSS train 0.13815661325608775 valid 0.18096096357030253
LOSS train 0.13815661325608775 valid 0.18097730570284118
LOSS train 0.13815661325608775 valid 0.18094528977496502
LOSS train 0.13815661325608775 valid 0.18111732035589675
LOSS train 0.13815661325608775 valid 0.18114346251556068
LOSS train 0.13815661325608775 valid 0.18106931684509156
LOSS train 0.13815661325608775 valid 0.18100462844477425
LOSS train 0.13815661325608775 valid 0.1810451096822787
LOSS train 0.13815661325608775 valid 0.18106875104724235
LOSS train 0.13815661325608775 valid 0.18119466216026056
LOSS train 0.13815661325608775 valid 0.18112703338265418
LOSS train 0.13815661325608775 valid 0.1812808771742467
LOSS train 0.13815661325608775 valid 0.18126034560781087
LOSS train 0.13815661325608775 valid 0.18123897517982282
LOSS train 0.13815661325608775 valid 0.18134549536086894
LOSS train 0.13815661325608775 valid 0.1813516488442054
LOSS train 0.13815661325608775 valid 0.1814972559458639
LOSS train 0.13815661325608775 valid 0.18154303064222366
LOSS train 0.13815661325608775 valid 0.18151428118893287
LOSS train 0.13815661325608775 valid 0.18157026796717773
LOSS train 0.13815661325608775 valid 0.18157165881359216
LOSS train 0.13815661325608775 valid 0.18145940828719528
LOSS train 0.13815661325608775 valid 0.18135026398013873
LOSS train 0.13815661325608775 valid 0.18140550642400174
LOSS train 0.13815661325608775 valid 0.18149925920063864
LOSS train 0.13815661325608775 valid 0.181463682206709
LOSS train 0.13815661325608775 valid 0.18153422778206213
LOSS train 0.13815661325608775 valid 0.18152730087321306
LOSS train 0.13815661325608775 valid 0.18151913168867664
LOSS train 0.13815661325608775 valid 0.1815337867072198
LOSS train 0.13815661325608775 valid 0.18147584795951843
LOSS train 0.13815661325608775 valid 0.1813426196050784
LOSS train 0.13815661325608775 valid 0.18135360467155076
LOSS train 0.13815661325608775 valid 0.1813956973938483
LOSS train 0.13815661325608775 valid 0.181602404604471
LOSS train 0.13815661325608775 valid 0.18168572541596234
LOSS train 0.13815661325608775 valid 0.18177457489719281
LOSS train 0.13815661325608775 valid 0.18161701752404313
LOSS train 0.13815661325608775 valid 0.1815490997128788
LOSS train 0.13815661325608775 valid 0.18155160755516805
LOSS train 0.13815661325608775 valid 0.18150028411831173
LOSS train 0.13815661325608775 valid 0.18141705447282547
LOSS train 0.13815661325608775 valid 0.1814239881509407
LOSS train 0.13815661325608775 valid 0.18142942045802096
LOSS train 0.13815661325608775 valid 0.18146743734846008
LOSS train 0.13815661325608775 valid 0.18153493908089652
LOSS train 0.13815661325608775 valid 0.1815382092772575
LOSS train 0.13815661325608775 valid 0.18158124170216525
LOSS train 0.13815661325608775 valid 0.1814837674272127
LOSS train 0.13815661325608775 valid 0.18150476090257214
LOSS train 0.13815661325608775 valid 0.18146745475629966
LOSS train 0.13815661325608775 valid 0.18142283272379983
LOSS train 0.13815661325608775 valid 0.18148662001717816
LOSS train 0.13815661325608775 valid 0.18147356385370259
LOSS train 0.13815661325608775 valid 0.18146957612627154
LOSS train 0.13815661325608775 valid 0.18151197339573952
LOSS train 0.13815661325608775 valid 0.18152675675889834
LOSS train 0.13815661325608775 valid 0.1814237930469357
LOSS train 0.13815661325608775 valid 0.18144421834174707
LOSS train 0.13815661325608775 valid 0.18153205683560875
EPOCH 6:
  batch 1 loss: 0.12543854117393494
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.12733469903469086
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.12455073992411296
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.13094428926706314
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.13394214510917662
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.1351543739438057
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.1300270270024027
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.1311365468427539
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.1302950812710656
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.12970172315835954
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.12800367176532745
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.12722989854713282
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.12756491223206887
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.12676673435739108
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.125281264881293
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.12679220782592893
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.12609599399216034
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.12912910473015574
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.12907604755539642
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.1283365536481142
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.12859475080456054
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.12933717634190212
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.12995788098677344
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.13179527254154286
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.13126300245523453
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.13138800830795214
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.1316660181791694
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.131160314061812
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.13080454540663752
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.1313399478793144
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.13174008169481832
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.13222617423161864
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.13153769775773538
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.1322991276050315
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.13384396880865096
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.13329457036323017
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.133285510781649
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.13286379724740982
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.1332266968794358
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.13331383019685744
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.13338029820744585
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.13355382567360288
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.13462755257307096
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.13406713035973636
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.13430980443954468
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.13372648474962814
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.13362717565069807
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.13362116428713003
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.1336439966547246
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.13370097219944
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.13341601572784723
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.1336002008846173
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.13353221315257954
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.13342472414175668
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.1338393658399582
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.1338932186897312
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.1338985478668882
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.13341614783837877
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.1337969916856895
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.13362282663583755
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.1337748878314847
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.1335112168904274
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.13369362860444992
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.1335582199972123
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.13371269840460556
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.13352168870694708
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.13330159658816323
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.13361504323342266
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.13383187915104022
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.13414719658238547
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.13449169821302656
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.1347102644956774
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.1347074720957508
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.13474504750322652
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.13485672891139985
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.1347461558486286
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.13470559886523656
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.13443197577427596
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.13485732052145125
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.13506750706583262
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.13514572050836351
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.134731803180241
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.13457433125340795
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.13493749321926207
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.13491162149345173
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.1352056474533192
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.13543325662612915
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.13577328351410953
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.13568226507540498
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.1360550817516115
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.13624211834682212
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.13620054543666218
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.13633438864702818
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.13625225725960224
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.1367718136624286
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.13686855928972363
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.136607807475267
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.1364365737048947
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.1367509225101182
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.1366814188659191
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.1365149037377669
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.13643956505784802
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.1362513255291772
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.1363509022988952
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.1359189722509611
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.13591153561225477
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.13614198656004167
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.13589548278186056
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.13600975398076784
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.1362053008242087
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.13595011588689443
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.13585520841713464
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.13558669058622513
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.13587568360462524
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.13562791600175525
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.1355347966711069
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.13591279541580087
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.13593103414622404
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.13580238086586238
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.1357898885384202
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.13572739526506297
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.1357892086637802
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.135918248656804
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.1358228764466701
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.13586839139461518
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.13594572707301095
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.13600832634554136
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.13577596319373697
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.1356857894927032
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.13565954531614596
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.13554636940701317
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.1354780186983672
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.135760729250155
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.13584539338723936
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.13557867683746197
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.13550034483127735
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.13566101141219591
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.1355837863208591
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.1358098014653158
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.13582923273955072
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.13580855420717958
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.13564966723952496
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.13578253119558722
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.1359375575557351
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.1357974307290439
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.13587079829957388
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.13586477195324542
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.13581874714912595
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.13612823868357896
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.13621599574883778
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.1360964123776417
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.13618145598784873
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.13615468271027983
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.13585145152234412
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.1359678408791942
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.13592939040599725
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.13599912081934085
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.13595801808788807
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.1360915853354916
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.1362027619034052
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.13631550356838273
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.13622387517013668
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.13623614417263336
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.13644887425187158
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.1365748877778198
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.13649540681795902
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.13637570885127176
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.13630322686263494
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.13613024596455534
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.1363477718742455
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.13637331881892611
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.13633558323043724
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.1361147199752014
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.13603077486328696
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.1361460520539965
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.13595981751991945
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.13580002559948776
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.13551703250307714
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.1357399352186219
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.13558228433960015
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.1356465188755515
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.1356397599882477
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.13552534584302067
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.13543193345970433
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.13547713180651536
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.13542427262792023
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.1354838171027561
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.13538809707190128
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.13536990429989246
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.13540972518293481
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.13544158536102135
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.1353157424212744
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.1353181642263047
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.1350485560359414
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.13504755986042513
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.13518670991975434
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.1352666024175392
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.13542318005453458
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.13538308726183734
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.13550049886107446
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.13549642924645647
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.13550825424418592
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.1355525236118016
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.13558471677642242
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.13553348132749884
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.13541056949970792
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.1354037200411161
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.13521438457358342
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.1352969432276402
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.13540534724791845
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.13537056052006816
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.13539506635575924
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.13547179634582268
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.13529311562670726
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.13535611418097518
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.13524840816993405
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.13522479552689784
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.13531692179107885
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.13538524743220579
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.13535067544064738
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.1353379448547083
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.13536489838818172
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.13531518042622126
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.13539080095610448
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.13526904811461768
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.13526626251044527
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.13536594049914819
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.13518902685558587
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.13514478849531783
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.13537181939767753
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.1355997043383586
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.13551327085186696
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.13558798523164103
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.13558123649185538
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.13563244361826715
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.13561694797570423
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.13555552518065972
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.1355849127559101
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.13560299207475893
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.13549840437869232
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.135597949082426
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.13555138327242913
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.13558312155950217
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.1356036854510913
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.13558909385179987
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.13567889284917978
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.13564794516635809
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.13574757520109415
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.1357803179257845
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.13572368481755256
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.13591581621611737
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.1359536833174172
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.1359645506668939
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.13590851135727927
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.13585153791834326
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.13584286565310322
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.13579337803306282
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.13571180986572606
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.1357683871358518
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.13579388736532286
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.1357330771812534
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.1356680384687795
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.13564154194108433
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.13550942433489996
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.13553937310880085
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.13557629855839828
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.1355508007527737
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.1355723748400585
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.13550081840789008
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.1355295022052747
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.135530242263391
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.13549680922947385
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.13559913354151415
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.13547080328320935
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.13536312165585432
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.13554784571886927
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.13553460789609043
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.1355661631112905
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.13553746180829182
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.13552254153681653
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.1354728756585155
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.13541048012198287
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.1354138859327185
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.1353746981583011
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.13549446521098155
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.13560197309597388
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.13551446795463562
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.1355195951441096
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.13546637314207413
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.135478803309901
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.13548941597905764
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.1355903240507596
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.1357598133868325
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.13577801835577505
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.1357541985936084
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.13578076046463605
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.1357488316116911
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.13570137201939653
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.13565183082053495
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.13563567496836185
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.13564564646577518
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.13565449374697067
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.13564961442951323
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.13562104427010604
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.13558658901296677
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.13560422775305175
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.13566374844273837
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.13562013334655143
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.13557940586485137
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.13547448189027847
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.13551777831227832
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.1354738796989505
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.13543946517351718
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.1354222611352137
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.13543538971552774
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.13554820017535477
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.1355287005378621
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.13553394325686702
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.13545405673195948
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.13553669382818043
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.1354912450751783
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.13541893008517922
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.13543938972263514
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.135354827583572
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.13536983719238868
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.13528850649489216
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.1351872075360485
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.13518923868584196
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.1352424761308725
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.13530350808392871
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.1353419317534683
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.13534417360213147
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.13527556021023798
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.13528646584488674
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.13535387589415507
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.13538174054008864
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.13536339359308208
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.13535136515660398
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.13526272063849598
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.13525775607456178
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.13527085920495371
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.13551918205897712
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.13556141671303756
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.13559365612563007
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.1356050064382346
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.13556525767969257
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.1355420507125621
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.1355271636369242
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.13546993959172066
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.13538896071059364
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.13540001035246074
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.13548908941447735
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.13557607399843233
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.1355904381574884
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.13558790599796133
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.13551241990304394
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.1356070915899691
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.13565869082035964
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.13557745696857448
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.13553815179814896
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.13550980746168179
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.13557374405350475
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.1355517992016041
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.13551129087560124
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.13555486657439847
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.13554064797166268
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.13559720831687183
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.1356863207993624
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.13566867908245825
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.13560213287537162
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.13564743710094385
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.13562923567670007
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.13564038098897116
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.13572360764451843
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.1357450210849444
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.13574799671372834
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.13573655630375409
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.13580414063479535
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.1357486197887121
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.13573253419446318
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.13571989518685604
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.13564864081862085
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.1357282059310933
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.13571289915125817
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.13563447772682488
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.13558091972158362
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.1356148014774002
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.13556345269928888
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.1355557141581354
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.1354943319964103
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.13546572734251658
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.13546039359834122
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.13554340413281027
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.13555974670246168
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.1356029735524443
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.13553338262694653
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.13543912837802613
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.13548226806161992
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.13545092675172
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.13541450215503573
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.1354134151725995
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.13537755098879634
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.1353496636631471
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.13536966680595192
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.13541586101055145
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.13535851609016875
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.13535658493454977
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.1354199907030253
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.13537573156773608
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.13536971972846404
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.13533182833751622
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.13539476439501474
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.13541258117477195
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.13547215549554226
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.13551647469221828
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.1355829696672467
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.13559461815608778
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.13555712787585966
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.13560154657090762
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.1355439342380989
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.1356104670220769
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.13564177946809908
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.13556986332080606
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.13556179520234746
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.13554026824586532
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.13544511613151836
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.13544500819832714
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.13540847254829985
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.13533322631201267
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.13526623645840688
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.13530155295493154
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.13525018803085442
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.13523610861967672
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.13528636609491665
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.13524473288963582
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.1353297255454807
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.13537925058971553
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.13534751976733883
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.13524607431508415
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.13522727973759174
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.13524210676044024
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.13520101154416933
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.13518636322182942
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.13516033501238436
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.13516019807102975
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.13517239489362914
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.1351377046614952
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.13519860784124052
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.13517602392055408
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.13516023572948244
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.13512801170811684
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.13517155806509268
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.13512448780649808
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.1351158702787061
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.13507459071966318
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.13508057721743458
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.13509036854920292
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.13510841498859064
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.1351085405853579
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.13517837171321329
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.13516892088726648
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.13516211116210722
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.13509625732319402
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.13512775412728562
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.13515266377118326
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.13518253376491593
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.13520539156310818
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.135280147783904
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.13532703801957782
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.13535907488871127
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.13540627997202478
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.13548373855587284
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.13548373855587284 valid 0.21174630522727966
LOSS train 0.13548373855587284 valid 0.19086699187755585
LOSS train 0.13548373855587284 valid 0.18321731189886728
LOSS train 0.13548373855587284 valid 0.17752352729439735
LOSS train 0.13548373855587284 valid 0.17325752079486847
LOSS train 0.13548373855587284 valid 0.1836438700556755
LOSS train 0.13548373855587284 valid 0.1947020356144224
LOSS train 0.13548373855587284 valid 0.19217394851148129
LOSS train 0.13548373855587284 valid 0.19110478791925642
LOSS train 0.13548373855587284 valid 0.19186215996742248
LOSS train 0.13548373855587284 valid 0.19118220968679947
LOSS train 0.13548373855587284 valid 0.19153665999571481
LOSS train 0.13548373855587284 valid 0.19122646634395307
LOSS train 0.13548373855587284 valid 0.19065494622503007
LOSS train 0.13548373855587284 valid 0.18807428578535715
LOSS train 0.13548373855587284 valid 0.18816404324024916
LOSS train 0.13548373855587284 valid 0.18912640652235815
LOSS train 0.13548373855587284 valid 0.1881505408220821
LOSS train 0.13548373855587284 valid 0.19041503338437332
LOSS train 0.13548373855587284 valid 0.190211770683527
LOSS train 0.13548373855587284 valid 0.18952935082571848
LOSS train 0.13548373855587284 valid 0.18808289007707077
LOSS train 0.13548373855587284 valid 0.18792162187721417
LOSS train 0.13548373855587284 valid 0.1878005558003982
LOSS train 0.13548373855587284 valid 0.1866098326444626
LOSS train 0.13548373855587284 valid 0.18632629571052697
LOSS train 0.13548373855587284 valid 0.1864417200839078
LOSS train 0.13548373855587284 valid 0.1862433563385691
LOSS train 0.13548373855587284 valid 0.1856903968186214
LOSS train 0.13548373855587284 valid 0.186234050989151
LOSS train 0.13548373855587284 valid 0.18693017863458203
LOSS train 0.13548373855587284 valid 0.1860367930494249
LOSS train 0.13548373855587284 valid 0.1862878849109014
LOSS train 0.13548373855587284 valid 0.18573115459259817
LOSS train 0.13548373855587284 valid 0.18735798682485308
LOSS train 0.13548373855587284 valid 0.18717670233713257
LOSS train 0.13548373855587284 valid 0.1877821422106511
LOSS train 0.13548373855587284 valid 0.18825231610160126
LOSS train 0.13548373855587284 valid 0.18768709477705833
LOSS train 0.13548373855587284 valid 0.18738099299371241
LOSS train 0.13548373855587284 valid 0.18787433207035065
LOSS train 0.13548373855587284 valid 0.18812004228432974
LOSS train 0.13548373855587284 valid 0.18812153678993845
LOSS train 0.13548373855587284 valid 0.18854953348636627
LOSS train 0.13548373855587284 valid 0.18854932785034179
LOSS train 0.13548373855587284 valid 0.18878146746884222
LOSS train 0.13548373855587284 valid 0.18935105711855788
LOSS train 0.13548373855587284 valid 0.18923941782365242
LOSS train 0.13548373855587284 valid 0.18987199055905246
LOSS train 0.13548373855587284 valid 0.18931838363409043
LOSS train 0.13548373855587284 valid 0.18949813673309251
LOSS train 0.13548373855587284 valid 0.1889946240072067
LOSS train 0.13548373855587284 valid 0.1894303121656742
LOSS train 0.13548373855587284 valid 0.1895853148566352
LOSS train 0.13548373855587284 valid 0.18953645581548864
LOSS train 0.13548373855587284 valid 0.18873615642743452
LOSS train 0.13548373855587284 valid 0.1886746276888931
LOSS train 0.13548373855587284 valid 0.18845587480684806
LOSS train 0.13548373855587284 valid 0.1888036212678683
LOSS train 0.13548373855587284 valid 0.18881319935123125
LOSS train 0.13548373855587284 valid 0.18838592655346043
LOSS train 0.13548373855587284 valid 0.18891215877186868
LOSS train 0.13548373855587284 valid 0.18828484132176354
LOSS train 0.13548373855587284 valid 0.18919266667217016
LOSS train 0.13548373855587284 valid 0.1894519899900143
LOSS train 0.13548373855587284 valid 0.18928119404749436
LOSS train 0.13548373855587284 valid 0.18861580317589774
LOSS train 0.13548373855587284 valid 0.18863851204514503
LOSS train 0.13548373855587284 valid 0.18821131232855975
LOSS train 0.13548373855587284 valid 0.18866632474320275
LOSS train 0.13548373855587284 valid 0.1883003676021603
LOSS train 0.13548373855587284 valid 0.1885236863874727
LOSS train 0.13548373855587284 valid 0.18853823105766349
LOSS train 0.13548373855587284 valid 0.18863148020731435
LOSS train 0.13548373855587284 valid 0.1889618702729543
LOSS train 0.13548373855587284 valid 0.18933432725699326
LOSS train 0.13548373855587284 valid 0.18914270362296662
LOSS train 0.13548373855587284 valid 0.18897209412012345
LOSS train 0.13548373855587284 valid 0.18858801035941403
LOSS train 0.13548373855587284 valid 0.1879682160913944
LOSS train 0.13548373855587284 valid 0.18764422557972096
LOSS train 0.13548373855587284 valid 0.1880986083571504
LOSS train 0.13548373855587284 valid 0.1878275686358831
LOSS train 0.13548373855587284 valid 0.18778216306652343
LOSS train 0.13548373855587284 valid 0.1877045527977102
LOSS train 0.13548373855587284 valid 0.18743828250918276
LOSS train 0.13548373855587284 valid 0.1871968187477397
LOSS train 0.13548373855587284 valid 0.1868877253410491
LOSS train 0.13548373855587284 valid 0.18750440789742417
LOSS train 0.13548373855587284 valid 0.1874952518277698
LOSS train 0.13548373855587284 valid 0.1875848156082761
LOSS train 0.13548373855587284 valid 0.18754177394768465
LOSS train 0.13548373855587284 valid 0.1873006921622061
LOSS train 0.13548373855587284 valid 0.18748676840295184
LOSS train 0.13548373855587284 valid 0.1874039551145152
LOSS train 0.13548373855587284 valid 0.1874701762571931
LOSS train 0.13548373855587284 valid 0.187364000788669
LOSS train 0.13548373855587284 valid 0.18758502283266612
LOSS train 0.13548373855587284 valid 0.18769660968371113
LOSS train 0.13548373855587284 valid 0.18774951592087746
LOSS train 0.13548373855587284 valid 0.18792092608343255
LOSS train 0.13548373855587284 valid 0.1881043130860609
LOSS train 0.13548373855587284 valid 0.18784375358553765
LOSS train 0.13548373855587284 valid 0.18800927942188886
LOSS train 0.13548373855587284 valid 0.18808535309064955
LOSS train 0.13548373855587284 valid 0.1884932447716875
LOSS train 0.13548373855587284 valid 0.1884102879840637
LOSS train 0.13548373855587284 valid 0.18855906774600348
LOSS train 0.13548373855587284 valid 0.1889028737851239
LOSS train 0.13548373855587284 valid 0.18904001523147929
LOSS train 0.13548373855587284 valid 0.1889762252837688
LOSS train 0.13548373855587284 valid 0.18885889862264907
LOSS train 0.13548373855587284 valid 0.1888973193358531
LOSS train 0.13548373855587284 valid 0.1891379871389322
LOSS train 0.13548373855587284 valid 0.18934699556101922
LOSS train 0.13548373855587284 valid 0.1895241019283903
LOSS train 0.13548373855587284 valid 0.1895041706470343
LOSS train 0.13548373855587284 valid 0.18917472332210863
LOSS train 0.13548373855587284 valid 0.18883340686810116
LOSS train 0.13548373855587284 valid 0.1885129916171233
LOSS train 0.13548373855587284 valid 0.1883297051526298
LOSS train 0.13548373855587284 valid 0.1883638604986863
LOSS train 0.13548373855587284 valid 0.1881618091488272
LOSS train 0.13548373855587284 valid 0.18850069865584373
LOSS train 0.13548373855587284 valid 0.1883301192522049
LOSS train 0.13548373855587284 valid 0.18872047322137014
LOSS train 0.13548373855587284 valid 0.188546932940408
LOSS train 0.13548373855587284 valid 0.18858663830906153
LOSS train 0.13548373855587284 valid 0.18876123705575631
LOSS train 0.13548373855587284 valid 0.18843840670127135
LOSS train 0.13548373855587284 valid 0.18815955758549785
LOSS train 0.13548373855587284 valid 0.18782907713091734
LOSS train 0.13548373855587284 valid 0.18778762254948006
LOSS train 0.13548373855587284 valid 0.18790599047692855
LOSS train 0.13548373855587284 valid 0.18777000319074702
LOSS train 0.13548373855587284 valid 0.18773541303680225
LOSS train 0.13548373855587284 valid 0.1874527724554939
LOSS train 0.13548373855587284 valid 0.18738844450833142
LOSS train 0.13548373855587284 valid 0.18733154376633734
LOSS train 0.13548373855587284 valid 0.18746579842908043
LOSS train 0.13548373855587284 valid 0.1873279258291772
LOSS train 0.13548373855587284 valid 0.18735732054206686
LOSS train 0.13548373855587284 valid 0.18729073684532327
LOSS train 0.13548373855587284 valid 0.18730051536113024
LOSS train 0.13548373855587284 valid 0.1871642203166567
LOSS train 0.13548373855587284 valid 0.18736426858869318
LOSS train 0.13548373855587284 valid 0.1873341261529598
LOSS train 0.13548373855587284 valid 0.18820413161773938
LOSS train 0.13548373855587284 valid 0.18830524324970757
LOSS train 0.13548373855587284 valid 0.18823872536420821
LOSS train 0.13548373855587284 valid 0.18844746753869468
LOSS train 0.13548373855587284 valid 0.18811187971579402
LOSS train 0.13548373855587284 valid 0.18808739052878487
LOSS train 0.13548373855587284 valid 0.18794379373649497
LOSS train 0.13548373855587284 valid 0.1878670435759329
LOSS train 0.13548373855587284 valid 0.18789363747987992
LOSS train 0.13548373855587284 valid 0.1879869993704899
LOSS train 0.13548373855587284 valid 0.18800009485287003
LOSS train 0.13548373855587284 valid 0.18807129093311117
LOSS train 0.13548373855587284 valid 0.18801598297432065
LOSS train 0.13548373855587284 valid 0.18788406641586966
LOSS train 0.13548373855587284 valid 0.18774252118151866
LOSS train 0.13548373855587284 valid 0.1874737988220402
LOSS train 0.13548373855587284 valid 0.1873126909500215
LOSS train 0.13548373855587284 valid 0.18713869226701332
LOSS train 0.13548373855587284 valid 0.18721941822623633
LOSS train 0.13548373855587284 valid 0.18755083678368323
LOSS train 0.13548373855587284 valid 0.18749077866474786
LOSS train 0.13548373855587284 valid 0.18768673614990075
LOSS train 0.13548373855587284 valid 0.1877052879508804
LOSS train 0.13548373855587284 valid 0.1877266550273226
LOSS train 0.13548373855587284 valid 0.18760456465357958
LOSS train 0.13548373855587284 valid 0.1876056950560884
LOSS train 0.13548373855587284 valid 0.18756169765845113
LOSS train 0.13548373855587284 valid 0.18725960612297057
LOSS train 0.13548373855587284 valid 0.1872162384573709
LOSS train 0.13548373855587284 valid 0.18727364597347496
LOSS train 0.13548373855587284 valid 0.18748916241894947
LOSS train 0.13548373855587284 valid 0.1873998811791063
LOSS train 0.13548373855587284 valid 0.18731787386867735
LOSS train 0.13548373855587284 valid 0.18745018895818383
LOSS train 0.13548373855587284 valid 0.18728838991987837
LOSS train 0.13548373855587284 valid 0.1873652973298818
LOSS train 0.13548373855587284 valid 0.1872283954821203
LOSS train 0.13548373855587284 valid 0.18709933902766254
LOSS train 0.13548373855587284 valid 0.1871139609044598
LOSS train 0.13548373855587284 valid 0.18695814357721868
LOSS train 0.13548373855587284 valid 0.18691731894269903
LOSS train 0.13548373855587284 valid 0.1868111123799016
LOSS train 0.13548373855587284 valid 0.18690312238116014
LOSS train 0.13548373855587284 valid 0.18683299757735267
LOSS train 0.13548373855587284 valid 0.18690235734296343
LOSS train 0.13548373855587284 valid 0.18662894822155257
LOSS train 0.13548373855587284 valid 0.18653304375631294
LOSS train 0.13548373855587284 valid 0.18634765843550363
LOSS train 0.13548373855587284 valid 0.18633048464449084
LOSS train 0.13548373855587284 valid 0.18655038424554815
LOSS train 0.13548373855587284 valid 0.1864894860320621
LOSS train 0.13548373855587284 valid 0.1865942171919885
LOSS train 0.13548373855587284 valid 0.18643528901040554
LOSS train 0.13548373855587284 valid 0.18623810702591986
LOSS train 0.13548373855587284 valid 0.18613823591777595
LOSS train 0.13548373855587284 valid 0.1861288908667165
LOSS train 0.13548373855587284 valid 0.18631770542147114
LOSS train 0.13548373855587284 valid 0.1861061245203018
LOSS train 0.13548373855587284 valid 0.18612830680840223
LOSS train 0.13548373855587284 valid 0.18607166603855466
LOSS train 0.13548373855587284 valid 0.18590113405997938
LOSS train 0.13548373855587284 valid 0.18588609572803005
LOSS train 0.13548373855587284 valid 0.18581232393071764
LOSS train 0.13548373855587284 valid 0.18583901410136744
LOSS train 0.13548373855587284 valid 0.18580737618624038
LOSS train 0.13548373855587284 valid 0.1858046375949618
LOSS train 0.13548373855587284 valid 0.18583532951981108
LOSS train 0.13548373855587284 valid 0.18571005428946297
LOSS train 0.13548373855587284 valid 0.18557784230344826
LOSS train 0.13548373855587284 valid 0.18552897499728313
LOSS train 0.13548373855587284 valid 0.18558515280211738
LOSS train 0.13548373855587284 valid 0.18568643737057028
LOSS train 0.13548373855587284 valid 0.185687930746512
LOSS train 0.13548373855587284 valid 0.18559894741120922
LOSS train 0.13548373855587284 valid 0.1856428355247051
LOSS train 0.13548373855587284 valid 0.1857974090918297
LOSS train 0.13548373855587284 valid 0.18590652889439038
LOSS train 0.13548373855587284 valid 0.18604648735788135
LOSS train 0.13548373855587284 valid 0.18628939274138054
LOSS train 0.13548373855587284 valid 0.1863598907571532
LOSS train 0.13548373855587284 valid 0.1864398389793279
LOSS train 0.13548373855587284 valid 0.18645442195854853
LOSS train 0.13548373855587284 valid 0.1864947619645492
LOSS train 0.13548373855587284 valid 0.18664698251140066
LOSS train 0.13548373855587284 valid 0.18666473307229323
LOSS train 0.13548373855587284 valid 0.186751526888348
LOSS train 0.13548373855587284 valid 0.1867982129383291
LOSS train 0.13548373855587284 valid 0.18694650467405927
LOSS train 0.13548373855587284 valid 0.18681760864742733
LOSS train 0.13548373855587284 valid 0.18677190549765962
LOSS train 0.13548373855587284 valid 0.18679941927685456
LOSS train 0.13548373855587284 valid 0.18662729734656203
LOSS train 0.13548373855587284 valid 0.18663357409338158
LOSS train 0.13548373855587284 valid 0.18680698167980953
LOSS train 0.13548373855587284 valid 0.1866436949815632
LOSS train 0.13548373855587284 valid 0.18681442130495002
LOSS train 0.13548373855587284 valid 0.18699074945733196
LOSS train 0.13548373855587284 valid 0.1870519819916511
LOSS train 0.13548373855587284 valid 0.18691058693135656
LOSS train 0.13548373855587284 valid 0.18707047994078896
LOSS train 0.13548373855587284 valid 0.18702048015209935
LOSS train 0.13548373855587284 valid 0.18704325882306538
LOSS train 0.13548373855587284 valid 0.18709593391418458
LOSS train 0.13548373855587284 valid 0.1869377043261471
LOSS train 0.13548373855587284 valid 0.18719069239875627
LOSS train 0.13548373855587284 valid 0.1871314376239249
LOSS train 0.13548373855587284 valid 0.18704086970391237
LOSS train 0.13548373855587284 valid 0.18708112029468313
LOSS train 0.13548373855587284 valid 0.18715298298047855
LOSS train 0.13548373855587284 valid 0.1869770251939269
LOSS train 0.13548373855587284 valid 0.18720019943723382
LOSS train 0.13548373855587284 valid 0.1872077416269015
LOSS train 0.13548373855587284 valid 0.18708268868235442
LOSS train 0.13548373855587284 valid 0.18724596574617072
LOSS train 0.13548373855587284 valid 0.18729708389244007
LOSS train 0.13548373855587284 valid 0.18733310200868905
LOSS train 0.13548373855587284 valid 0.18743498448395368
LOSS train 0.13548373855587284 valid 0.18736824612572509
LOSS train 0.13548373855587284 valid 0.18741914971654577
LOSS train 0.13548373855587284 valid 0.187468188643902
LOSS train 0.13548373855587284 valid 0.18765581110075338
LOSS train 0.13548373855587284 valid 0.18775212886608222
LOSS train 0.13548373855587284 valid 0.18774386710590787
LOSS train 0.13548373855587284 valid 0.18781604400640045
LOSS train 0.13548373855587284 valid 0.1881059643209857
LOSS train 0.13548373855587284 valid 0.1883075563680558
LOSS train 0.13548373855587284 valid 0.18832787664702338
LOSS train 0.13548373855587284 valid 0.18833254088055004
LOSS train 0.13548373855587284 valid 0.1882379740692567
LOSS train 0.13548373855587284 valid 0.18809812168997547
LOSS train 0.13548373855587284 valid 0.18794138558048137
LOSS train 0.13548373855587284 valid 0.1879254574737241
LOSS train 0.13548373855587284 valid 0.18792255792234625
LOSS train 0.13548373855587284 valid 0.1878326021373484
LOSS train 0.13548373855587284 valid 0.18755093079509463
LOSS train 0.13548373855587284 valid 0.18747453796989927
LOSS train 0.13548373855587284 valid 0.18748815294722437
LOSS train 0.13548373855587284 valid 0.18752382847300747
LOSS train 0.13548373855587284 valid 0.18752219288290797
LOSS train 0.13548373855587284 valid 0.18747536609604798
LOSS train 0.13548373855587284 valid 0.1874391851015389
LOSS train 0.13548373855587284 valid 0.18745495172726653
LOSS train 0.13548373855587284 valid 0.18748342076252247
LOSS train 0.13548373855587284 valid 0.18733254710013925
LOSS train 0.13548373855587284 valid 0.1873327418141169
LOSS train 0.13548373855587284 valid 0.18735673260770153
LOSS train 0.13548373855587284 valid 0.18744816990936694
LOSS train 0.13548373855587284 valid 0.1875365726018356
LOSS train 0.13548373855587284 valid 0.18746353992940606
LOSS train 0.13548373855587284 valid 0.18753749518482773
LOSS train 0.13548373855587284 valid 0.18757635430001574
LOSS train 0.13548373855587284 valid 0.18763661304843865
LOSS train 0.13548373855587284 valid 0.1877116334438324
LOSS train 0.13548373855587284 valid 0.18766281008720398
LOSS train 0.13548373855587284 valid 0.18760689614427012
LOSS train 0.13548373855587284 valid 0.18766595639608089
LOSS train 0.13548373855587284 valid 0.18766325192624017
LOSS train 0.13548373855587284 valid 0.18759033391710186
LOSS train 0.13548373855587284 valid 0.18762459011833652
LOSS train 0.13548373855587284 valid 0.1876118038978173
LOSS train 0.13548373855587284 valid 0.18751682931339586
LOSS train 0.13548373855587284 valid 0.1875282107433455
LOSS train 0.13548373855587284 valid 0.18758098175448756
LOSS train 0.13548373855587284 valid 0.18758197171895066
LOSS train 0.13548373855587284 valid 0.18755421596459854
LOSS train 0.13548373855587284 valid 0.18772838386102988
LOSS train 0.13548373855587284 valid 0.18774839044566366
LOSS train 0.13548373855587284 valid 0.18767046020144507
LOSS train 0.13548373855587284 valid 0.1876062751287901
LOSS train 0.13548373855587284 valid 0.18765715408588435
LOSS train 0.13548373855587284 valid 0.18768040198575026
LOSS train 0.13548373855587284 valid 0.18781476311362275
LOSS train 0.13548373855587284 valid 0.18774323170073332
LOSS train 0.13548373855587284 valid 0.1878996338706893
LOSS train 0.13548373855587284 valid 0.187877379265261
LOSS train 0.13548373855587284 valid 0.18784533943923265
LOSS train 0.13548373855587284 valid 0.18794997123840415
LOSS train 0.13548373855587284 valid 0.1879535920344866
LOSS train 0.13548373855587284 valid 0.18810904876220447
LOSS train 0.13548373855587284 valid 0.1881643400188615
LOSS train 0.13548373855587284 valid 0.18814471254988416
LOSS train 0.13548373855587284 valid 0.18820536304208646
LOSS train 0.13548373855587284 valid 0.18820735798640684
LOSS train 0.13548373855587284 valid 0.1880993102072229
LOSS train 0.13548373855587284 valid 0.1879844055567161
LOSS train 0.13548373855587284 valid 0.18804143557140418
LOSS train 0.13548373855587284 valid 0.18813515432223588
LOSS train 0.13548373855587284 valid 0.188093120749317
LOSS train 0.13548373855587284 valid 0.18815861895148242
LOSS train 0.13548373855587284 valid 0.18814816124715153
LOSS train 0.13548373855587284 valid 0.18813597710703958
LOSS train 0.13548373855587284 valid 0.1881452389610904
LOSS train 0.13548373855587284 valid 0.1880874476450331
LOSS train 0.13548373855587284 valid 0.18794230998785963
LOSS train 0.13548373855587284 valid 0.1879521932518273
LOSS train 0.13548373855587284 valid 0.18799354092546525
LOSS train 0.13548373855587284 valid 0.1882071952053974
LOSS train 0.13548373855587284 valid 0.18829497224178868
LOSS train 0.13548373855587284 valid 0.18837819623120258
LOSS train 0.13548373855587284 valid 0.18822371461549478
LOSS train 0.13548373855587284 valid 0.1881521312070304
LOSS train 0.13548373855587284 valid 0.18814960963234179
LOSS train 0.13548373855587284 valid 0.1880899772473744
LOSS train 0.13548373855587284 valid 0.18800758020660477
LOSS train 0.13548373855587284 valid 0.1880011609332128
LOSS train 0.13548373855587284 valid 0.18800992723396073
LOSS train 0.13548373855587284 valid 0.18804358383693265
LOSS train 0.13548373855587284 valid 0.18810947470262018
LOSS train 0.13548373855587284 valid 0.18812411617529526
LOSS train 0.13548373855587284 valid 0.18816445659355624
LOSS train 0.13548373855587284 valid 0.1880661563417099
LOSS train 0.13548373855587284 valid 0.18807820224496316
LOSS train 0.13548373855587284 valid 0.1880363155156374
LOSS train 0.13548373855587284 valid 0.18799654850504074
LOSS train 0.13548373855587284 valid 0.18805158035366576
LOSS train 0.13548373855587284 valid 0.18804042673472202
LOSS train 0.13548373855587284 valid 0.18802632395546515
LOSS train 0.13548373855587284 valid 0.18807324134323694
LOSS train 0.13548373855587284 valid 0.18809161233445987
LOSS train 0.13548373855587284 valid 0.1879897328342339
LOSS train 0.13548373855587284 valid 0.18801721254282672
LOSS train 0.13548373855587284 valid 0.18809755009202775
EPOCH 7:
  batch 1 loss: 0.12111777067184448
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.12147592753171921
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.11932011693716049
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.12688875384628773
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.1302530959248543
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.13166478897134462
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.1268646898014205
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.12850350234657526
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.1275866933994823
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.1273721508681774
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.12564767693931406
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.12476638828714688
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.12528360119232765
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.12500114419630595
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.12380506992340087
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.1253798883408308
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.12437238062129301
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.12734846356842253
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.12701060466076197
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.1263811681419611
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.12666428550368264
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.12723368846557356
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.12768029745506204
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.12956133453796306
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.12905854016542434
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.12930366310935754
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.12950989962727935
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.12901320335056102
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.12872419146628217
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.12936894074082375
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.12965949720913364
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.1300877460744232
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.12929446092157654
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.13026620491462595
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.13176646190030233
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.1311232532478041
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.13108911647184476
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.13076035658779897
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.13109179547963998
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.13122466076165437
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.13129034293134037
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.1315196371149449
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.13261696644300638
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.13206501118838787
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.1322526291012764
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.13166552409529686
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.13155555709245356
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.1316817137412727
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.13180085423649573
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.13186351403594018
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.13156909743944803
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.13174200487824586
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.13156492319309487
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.13146310423811278
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.13183718987486578
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.1319809490814805
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.13202182072819324
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.1315024763602635
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.13193904709512905
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.13178291469812392
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.13195480797134462
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.13177966747072437
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.13194963205901403
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.1317009994527325
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.1318503811955452
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.13161847142107558
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.1314103556657905
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.131666036651415
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.13192947576011438
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.13225768549101694
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.13262358118950482
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.13282149554126793
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.1328726739507832
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.13293709021967812
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.13301578402519226
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.1329831079040703
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.1329890296056673
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.13267037750054628
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.13308273727380776
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.13332170508801938
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.13339796018453293
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.133032450952181
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.1328649680657559
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.1332126875363645
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.13323312243994545
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.1335242335186448
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.1337238872188261
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.1340709552168846
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.13399150800169185
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.13438101841343775
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.13456893458471195
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.13450684835729393
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.13461987417872234
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.13452911757408303
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.13497493800364044
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.13508493965491652
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.13488087826168416
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.13468896202286895
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.13501201795809198
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.1349167588353157
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.1347731421194454
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.13464096918994306
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.1344454004926589
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.13456736189814714
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.1341337101090522
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.1341255636850618
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.1343457802573097
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.13414195669745957
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.13423451719754334
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.13446570858359336
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.13420931639166567
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.1341563752586288
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.13392003295959626
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.1342273844046551
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.1339785916649777
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.1338683911438646
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.13418767149122351
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.13417110996226134
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.13402725223984038
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.1340147842342655
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.13397671854939341
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.13401241337911027
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.134151086392926
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.13407281257452502
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.13413002061843873
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.13423026234857618
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.1342780559551059
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.13400718325283378
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.13392196676527807
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.13388938559935643
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.13376781145352443
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.13370374900599322
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.13401772905337184
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.1340819115180578
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.1338244253286609
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.13371494949302254
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.13386620113449374
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.13380658032669537
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.13401824353838995
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.1340345998959882
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.13403609056844779
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.13386547995705
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.13399963810310497
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.13418556015110678
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.13406647906221192
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.13409921394227303
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.1341519391252881
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.13410522506849185
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.13445961515375432
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.1345322440067927
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.13443260521486106
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.13450968123384213
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.13446828727823457
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.13416847306025492
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.13425380164577114
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.13422229933815125
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.1343372234493304
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.13429399806110165
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.13440415947317327
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.13451325306668876
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.13461817708444893
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.13449816695517963
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.1345281670887046
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.13474647205595564
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.13486387264547925
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.13479005262614732
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.1346583490421672
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.13461664815743765
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.13442884350316764
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.13468387118157218
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.13471153770622454
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.13470515071652656
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.134487528375463
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.13441167510617738
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.13452765085867474
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.13432835542004218
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.1341706241591502
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.13387636091099697
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.13408138161764466
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.13391053051584298
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.13397306940667536
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.1339484966145112
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.13384842037991748
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.1337575651705265
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.1337946653366089
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.13373456558873575
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.133791016942677
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.1336957151268391
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.1336769490014939
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.1337148922054391
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.13373477850596943
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.1336385204922408
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.13365667355801775
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.13337694753691093
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.1333815907056515
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.1335170830086786
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.1336235558321028
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.1337832992877623
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.133734772927198
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.13386485777795315
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.13387467389676108
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.13392472495832067
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.13395025496706
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.1339793014614021
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.13394138035250874
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.1337943589947756
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.13376988909670695
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.13360143162739965
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.13368631125779815
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.13379142798838162
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.1337765245146661
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.13380693450991837
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.13390210386313184
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.13370530359516633
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.13378343682649524
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.1336926547465501
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.1336684805182268
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.13374092047094205
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.13379901531896635
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.1337892013517293
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.13377319202164178
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.13380280378702525
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.1337588810666794
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.1338355019355991
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.13372076839208602
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.1337288343471236
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.13382011018529338
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.13365329364151285
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.13360492546745784
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.13385221660137175
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.1340674160620867
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.13397603748558923
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.13403830095294209
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.13403839219966504
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.1340548851705612
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.13405142879208265
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.1340038985768451
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.1340437542114939
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.13405475194982905
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.13395281104991832
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.1340594751706262
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.13401387247049118
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.1340536772898195
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.13407330114211216
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.13404088412620582
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.1341083222712443
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.13409277636874542
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.13417001065587805
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.1342228772230895
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.13414141753315925
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.13430706328486067
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.1343376613739464
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.13433692032994018
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.13429646774774462
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.13423743534321878
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.13423002162016928
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.13416550949157907
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.13408302475315656
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.13414203533794888
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.13417257976073485
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.1341017039913784
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.1340230702785135
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.13399255179407024
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.133855061520907
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.13388190980789796
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.1339117203159888
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.13388028671120883
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.1338931299023219
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.13379323662546044
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.1338000184683888
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.1338009603074116
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.13375805381356792
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.1338531425966448
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.13371817933490676
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.13361549721522764
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.13381171293988608
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.13380463617695798
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.13383395533017117
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.13379605824802085
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.13379601261445453
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.13375702129140019
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.1336886960458248
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.13368730099588738
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.1336534477140702
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.1337775216813673
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.13390803076587357
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.13382963059982772
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.13382172235287726
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.1337700843759474
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.1337897348763614
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.1338114513922803
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.1339101838585857
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.13408645461138605
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.13410544164833568
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.13407568555262128
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.13410083579553947
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.13407654270078195
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.13403434243878262
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.1339841577470502
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.1339640863488118
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.13398261355898308
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.13399042916909748
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.13398571619794708
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.13394148086540794
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.13389078515474914
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.13390730964202507
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.1339640299446808
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.1339116816396837
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.13386718786553659
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.13376929142302083
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.13383191768858593
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.13380144708431685
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.13374993109855407
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.13373557682249956
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.13375221136070434
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.13386342848969413
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.1338301947581279
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.1338196203534333
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.1337313868782737
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.13381941616535187
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.13376734366298093
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.13369850911523984
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.13372191163962102
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.13363309549512686
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.1336365479689378
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.13355009824586062
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.13344671987338896
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.13344880827225564
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.13349162420331526
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.13355577671618174
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.13359406691753611
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.13359882330230202
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.13353267280726103
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.1335483279503034
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.13361572507602065
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.1336298024370557
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.13361906947827834
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.13362560006817417
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.13354181474113183
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.13354201347512357
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.13356578022328988
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.13380444542183514
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.13385871934647464
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.13390943390685459
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.13391233725824217
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.13386586446293516
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.13385366899651135
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.13383753893190417
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.13377536858904326
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.1337074498619352
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.1337014939370658
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.13378310245885092
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.13387520062518188
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.13389310590123052
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.1338913127989836
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.13381818564755193
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.13391223737672597
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.13398153551297481
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.13390669170611416
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.13387686512950395
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.1338407460011934
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.13391202593541277
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.13388404613437732
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.13383872012843143
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.13388031508824597
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.13387796376409428
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.13393451898721648
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.13403578548003797
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.1340074491936986
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.1339301916191707
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.13397447835162002
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.13395837167658473
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.13397587685978124
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.1340474288532122
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.13406987430651982
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.13406575667334997
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.13406818802777273
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.13412240287535404
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.13405799317249836
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.1340481118348084
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.13401922642246006
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.1339566896021054
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.13403297065910408
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.13401653020021817
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.13393341136443151
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.1338805157324502
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.13391642189688152
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.13387152412426226
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.13385355275287114
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.13379601251620513
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.13376222278379724
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.133759853582145
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.13384819745486626
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.13387065076359034
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.13390171235497994
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.13384452411396938
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.13374472032265938
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.1337924427423046
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.1337681301591689
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.1337380161508918
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.13372161274687608
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.13368650963205603
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.13364038511038417
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.1336704378597217
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.13371731182675303
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.1336667189016718
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.1336879979553621
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.13373543858966408
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.13369760773438405
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.13370751026200084
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.1336868788929171
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.13374306174736578
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.13377093280198787
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.133832103136369
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.13388342699372624
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.13396775292662474
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.13396940521484943
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.13392041956907824
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.13397387514748835
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.13392541734945207
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.13397883006201222
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.13401878575719364
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.13394447870499698
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.13393894212215013
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.1339153146217851
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.13381995251970671
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.13382008450162494
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.1337797980860015
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.13370023716440846
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.13363195907237918
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.13367994634430258
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.13363591755774837
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.13362045467381664
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.13366662074000604
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.1336224729816119
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.13371572498266304
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.13376628583642383
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.13373201097187387
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.1336271124372178
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.13360982147807424
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.1336376835970111
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.13360084252794405
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.13358365833490213
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.1335771661755201
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.13358153737662884
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.1335878548934855
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.13355290279692453
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.1336079199598836
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.13358892068963804
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.13358621229728063
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.1335493965185931
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.1335927336682788
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.13353888473368639
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.13354530388957078
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.13349693849846556
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.1335011231794692
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.13351371297429374
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.13353559010414057
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.13353778516025586
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.13359765452535255
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.133597760099651
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.13358732470960327
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.13352243135608813
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.13354831336644188
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.13357035562556277
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.1335942379394826
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.1336262302365497
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.13371801424102905
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.1337706965169927
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.13379957324012798
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.13384520770258204
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.1339355497542074
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.1339355497542074 valid 0.2099883109331131
LOSS train 0.1339355497542074 valid 0.18748380988836288
LOSS train 0.1339355497542074 valid 0.18025765816370645
LOSS train 0.1339355497542074 valid 0.1741316244006157
LOSS train 0.1339355497542074 valid 0.16990234851837158
LOSS train 0.1339355497542074 valid 0.17994154493014017
LOSS train 0.1339355497542074 valid 0.19049006700515747
LOSS train 0.1339355497542074 valid 0.1880404707044363
LOSS train 0.1339355497542074 valid 0.1872501489188936
LOSS train 0.1339355497542074 valid 0.1880168378353119
LOSS train 0.1339355497542074 valid 0.1873296851461584
LOSS train 0.1339355497542074 valid 0.18769052748878798
LOSS train 0.1339355497542074 valid 0.1874510943889618
LOSS train 0.1339355497542074 valid 0.18692756124905177
LOSS train 0.1339355497542074 valid 0.18443920214970908
LOSS train 0.1339355497542074 valid 0.18451111111789942
LOSS train 0.1339355497542074 valid 0.1854402852409026
LOSS train 0.1339355497542074 valid 0.18439450280533898
LOSS train 0.1339355497542074 valid 0.18669178219218002
LOSS train 0.1339355497542074 valid 0.1865081138908863
LOSS train 0.1339355497542074 valid 0.18583422615414574
LOSS train 0.1339355497542074 valid 0.1844366951422258
LOSS train 0.1339355497542074 valid 0.18425743994505508
LOSS train 0.1339355497542074 valid 0.18422968747715154
LOSS train 0.1339355497542074 valid 0.18300260424613954
LOSS train 0.1339355497542074 valid 0.18273568268005663
LOSS train 0.1339355497542074 valid 0.18286352797790809
LOSS train 0.1339355497542074 valid 0.18266516072409494
LOSS train 0.1339355497542074 valid 0.1820855644242517
LOSS train 0.1339355497542074 valid 0.18259756068388622
LOSS train 0.1339355497542074 valid 0.18332054922657628
LOSS train 0.1339355497542074 valid 0.18245473690330982
LOSS train 0.1339355497542074 valid 0.18267529886780362
LOSS train 0.1339355497542074 valid 0.18215356504215913
LOSS train 0.1339355497542074 valid 0.18373860759394509
LOSS train 0.1339355497542074 valid 0.18355755507946014
LOSS train 0.1339355497542074 valid 0.18420045238894386
LOSS train 0.1339355497542074 valid 0.18464264273643494
LOSS train 0.1339355497542074 valid 0.1841383805641761
LOSS train 0.1339355497542074 valid 0.1838154673576355
LOSS train 0.1339355497542074 valid 0.18432812610777413
LOSS train 0.1339355497542074 valid 0.1845992203979265
LOSS train 0.1339355497542074 valid 0.18461644614851752
LOSS train 0.1339355497542074 valid 0.18503310057249936
LOSS train 0.1339355497542074 valid 0.18502174814542136
LOSS train 0.1339355497542074 valid 0.18528123037970584
LOSS train 0.1339355497542074 valid 0.1858799086606249
LOSS train 0.1339355497542074 valid 0.1857560882344842
LOSS train 0.1339355497542074 valid 0.18636191134550134
LOSS train 0.1339355497542074 valid 0.18582070291042327
LOSS train 0.1339355497542074 valid 0.18601094536921559
LOSS train 0.1339355497542074 valid 0.1855297527061059
LOSS train 0.1339355497542074 valid 0.1859816922894064
LOSS train 0.1339355497542074 valid 0.18610694259405136
LOSS train 0.1339355497542074 valid 0.18606571853160858
LOSS train 0.1339355497542074 valid 0.18527489661106042
LOSS train 0.1339355497542074 valid 0.18524382407205148
LOSS train 0.1339355497542074 valid 0.18504542163733778
LOSS train 0.1339355497542074 valid 0.18538489533683
LOSS train 0.1339355497542074 valid 0.1853531278669834
LOSS train 0.1339355497542074 valid 0.1849410726887281
LOSS train 0.1339355497542074 valid 0.18546823796726042
LOSS train 0.1339355497542074 valid 0.18483092955180577
LOSS train 0.1339355497542074 valid 0.18573823012411594
LOSS train 0.1339355497542074 valid 0.18599971784995153
LOSS train 0.1339355497542074 valid 0.1858084172461972
LOSS train 0.1339355497542074 valid 0.18515564679209864
LOSS train 0.1339355497542074 valid 0.18518231458523693
LOSS train 0.1339355497542074 valid 0.18475079363670902
LOSS train 0.1339355497542074 valid 0.1852181055716106
LOSS train 0.1339355497542074 valid 0.1848591599665897
LOSS train 0.1339355497542074 valid 0.18509729310042328
LOSS train 0.1339355497542074 valid 0.18511089008964904
LOSS train 0.1339355497542074 valid 0.18518170674104947
LOSS train 0.1339355497542074 valid 0.18548579494158426
LOSS train 0.1339355497542074 valid 0.18586493440364538
LOSS train 0.1339355497542074 valid 0.18568088636769878
LOSS train 0.1339355497542074 valid 0.18551095536886117
LOSS train 0.1339355497542074 valid 0.1851357208399833
LOSS train 0.1339355497542074 valid 0.1845115279778838
LOSS train 0.1339355497542074 valid 0.1841954270630707
LOSS train 0.1339355497542074 valid 0.18463978135004278
LOSS train 0.1339355497542074 valid 0.18437292758958884
LOSS train 0.1339355497542074 valid 0.18433176903497606
LOSS train 0.1339355497542074 valid 0.18422848459552316
LOSS train 0.1339355497542074 valid 0.18398779647987942
LOSS train 0.1339355497542074 valid 0.18373360544785686
LOSS train 0.1339355497542074 valid 0.18343788470056924
LOSS train 0.1339355497542074 valid 0.18401819099201244
LOSS train 0.1339355497542074 valid 0.1839976022640864
LOSS train 0.1339355497542074 valid 0.18408759183936066
LOSS train 0.1339355497542074 valid 0.1840541534449743
LOSS train 0.1339355497542074 valid 0.1838196479184653
LOSS train 0.1339355497542074 valid 0.18400892599465998
LOSS train 0.1339355497542074 valid 0.18393491883026927
LOSS train 0.1339355497542074 valid 0.18401910504326224
LOSS train 0.1339355497542074 valid 0.18389701735727565
LOSS train 0.1339355497542074 valid 0.18411520807718745
LOSS train 0.1339355497542074 valid 0.18423779865708015
LOSS train 0.1339355497542074 valid 0.18429768323898316
LOSS train 0.1339355497542074 valid 0.18446732673904684
LOSS train 0.1339355497542074 valid 0.18466023793991873
LOSS train 0.1339355497542074 valid 0.18439041367433603
LOSS train 0.1339355497542074 valid 0.1845526731071564
LOSS train 0.1339355497542074 valid 0.18463258502029237
LOSS train 0.1339355497542074 valid 0.1850527548846209
LOSS train 0.1339355497542074 valid 0.1849627181469837
LOSS train 0.1339355497542074 valid 0.18511777522939224
LOSS train 0.1339355497542074 valid 0.1854721356969361
LOSS train 0.1339355497542074 valid 0.18559977073561063
LOSS train 0.1339355497542074 valid 0.1855392878925478
LOSS train 0.1339355497542074 valid 0.1854276740923524
LOSS train 0.1339355497542074 valid 0.185465760858713
LOSS train 0.1339355497542074 valid 0.1857361555622335
LOSS train 0.1339355497542074 valid 0.18595235244087552
LOSS train 0.1339355497542074 valid 0.18613312406272725
LOSS train 0.1339355497542074 valid 0.18611775644314596
LOSS train 0.1339355497542074 valid 0.18577510648864812
LOSS train 0.1339355497542074 valid 0.18542744882968293
LOSS train 0.1339355497542074 valid 0.1851001429061095
LOSS train 0.1339355497542074 valid 0.18492980055079972
LOSS train 0.1339355497542074 valid 0.18496881462022907
LOSS train 0.1339355497542074 valid 0.18476223206616998
LOSS train 0.1339355497542074 valid 0.18508727451966656
LOSS train 0.1339355497542074 valid 0.18492632246017457
LOSS train 0.1339355497542074 valid 0.18532595960866838
LOSS train 0.1339355497542074 valid 0.18516116372243627
LOSS train 0.1339355497542074 valid 0.1851957148173824
LOSS train 0.1339355497542074 valid 0.1853666205045789
LOSS train 0.1339355497542074 valid 0.18503768833783957
LOSS train 0.1339355497542074 valid 0.18475907426754026
LOSS train 0.1339355497542074 valid 0.18442812765186484
LOSS train 0.1339355497542074 valid 0.18437138558330393
LOSS train 0.1339355497542074 valid 0.18449011600729245
LOSS train 0.1339355497542074 valid 0.18435840363855716
LOSS train 0.1339355497542074 valid 0.1843294236151611
LOSS train 0.1339355497542074 valid 0.18404000236170134
LOSS train 0.1339355497542074 valid 0.1839808569006298
LOSS train 0.1339355497542074 valid 0.1839283080838567
LOSS train 0.1339355497542074 valid 0.1840483403631619
LOSS train 0.1339355497542074 valid 0.18389951252768225
LOSS train 0.1339355497542074 valid 0.1839193130882693
LOSS train 0.1339355497542074 valid 0.18383857065981085
LOSS train 0.1339355497542074 valid 0.1838397021508879
LOSS train 0.1339355497542074 valid 0.18369959520882573
LOSS train 0.1339355497542074 valid 0.18390874197221782
LOSS train 0.1339355497542074 valid 0.18385925301078226
LOSS train 0.1339355497542074 valid 0.18473375246331497
LOSS train 0.1339355497542074 valid 0.18484643481721813
LOSS train 0.1339355497542074 valid 0.18478738407293954
LOSS train 0.1339355497542074 valid 0.18499497112059435
LOSS train 0.1339355497542074 valid 0.184658047007887
LOSS train 0.1339355497542074 valid 0.18462496012254478
LOSS train 0.1339355497542074 valid 0.18448110737583853
LOSS train 0.1339355497542074 valid 0.1844015154146379
LOSS train 0.1339355497542074 valid 0.1844319395529918
LOSS train 0.1339355497542074 valid 0.18452064664500534
LOSS train 0.1339355497542074 valid 0.1845319374829908
LOSS train 0.1339355497542074 valid 0.18459550174152325
LOSS train 0.1339355497542074 valid 0.18453512797132135
LOSS train 0.1339355497542074 valid 0.18439872127882442
LOSS train 0.1339355497542074 valid 0.1842625022486404
LOSS train 0.1339355497542074 valid 0.183995860028852
LOSS train 0.1339355497542074 valid 0.18383332523630885
LOSS train 0.1339355497542074 valid 0.18366414407889048
LOSS train 0.1339355497542074 valid 0.1837450936436653
LOSS train 0.1339355497542074 valid 0.18407206353313194
LOSS train 0.1339355497542074 valid 0.1840004116474163
LOSS train 0.1339355497542074 valid 0.18419258437565797
LOSS train 0.1339355497542074 valid 0.18420614793020137
LOSS train 0.1339355497542074 valid 0.18422136102852069
LOSS train 0.1339355497542074 valid 0.18409543432468592
LOSS train 0.1339355497542074 valid 0.18409994141214844
LOSS train 0.1339355497542074 valid 0.18405324828693237
LOSS train 0.1339355497542074 valid 0.18374408057757785
LOSS train 0.1339355497542074 valid 0.18370404466986656
LOSS train 0.1339355497542074 valid 0.18376660321728658
LOSS train 0.1339355497542074 valid 0.18399699643421707
LOSS train 0.1339355497542074 valid 0.18391646045213306
LOSS train 0.1339355497542074 valid 0.18383207412229643
LOSS train 0.1339355497542074 valid 0.18397115681382173
LOSS train 0.1339355497542074 valid 0.18380698553480945
LOSS train 0.1339355497542074 valid 0.18388168122924742
LOSS train 0.1339355497542074 valid 0.18375200859230498
LOSS train 0.1339355497542074 valid 0.18361185450811643
LOSS train 0.1339355497542074 valid 0.18363132768420762
LOSS train 0.1339355497542074 valid 0.1834803354293905
LOSS train 0.1339355497542074 valid 0.1834473968186277
LOSS train 0.1339355497542074 valid 0.1833268404006958
LOSS train 0.1339355497542074 valid 0.183423980835237
LOSS train 0.1339355497542074 valid 0.1833430697936662
LOSS train 0.1339355497542074 valid 0.18341877893544734
LOSS train 0.1339355497542074 valid 0.18314223547364764
LOSS train 0.1339355497542074 valid 0.18304984478913633
LOSS train 0.1339355497542074 valid 0.18285829860430497
LOSS train 0.1339355497542074 valid 0.18283260750527286
LOSS train 0.1339355497542074 valid 0.1830292077839072
LOSS train 0.1339355497542074 valid 0.1829738871316717
LOSS train 0.1339355497542074 valid 0.1830880770731212
LOSS train 0.1339355497542074 valid 0.18293182939291
LOSS train 0.1339355497542074 valid 0.182735284762596
LOSS train 0.1339355497542074 valid 0.18264041531203998
LOSS train 0.1339355497542074 valid 0.18263486086441377
LOSS train 0.1339355497542074 valid 0.18282495015392117
LOSS train 0.1339355497542074 valid 0.1826098604900081
LOSS train 0.1339355497542074 valid 0.1826364454424497
LOSS train 0.1339355497542074 valid 0.18258108061868789
LOSS train 0.1339355497542074 valid 0.1824134967934627
LOSS train 0.1339355497542074 valid 0.18239523826888873
LOSS train 0.1339355497542074 valid 0.1823197352034705
LOSS train 0.1339355497542074 valid 0.18233452857388138
LOSS train 0.1339355497542074 valid 0.1822937437385883
LOSS train 0.1339355497542074 valid 0.1822761467225115
LOSS train 0.1339355497542074 valid 0.18230852449887266
LOSS train 0.1339355497542074 valid 0.1821824249833129
LOSS train 0.1339355497542074 valid 0.18205253421156495
LOSS train 0.1339355497542074 valid 0.1819966670280228
LOSS train 0.1339355497542074 valid 0.1820419646047671
LOSS train 0.1339355497542074 valid 0.18214268036628967
LOSS train 0.1339355497542074 valid 0.1821345915171233
LOSS train 0.1339355497542074 valid 0.18204273377878094
LOSS train 0.1339355497542074 valid 0.18208524618331376
LOSS train 0.1339355497542074 valid 0.18224331115126075
LOSS train 0.1339355497542074 valid 0.1823543070017227
LOSS train 0.1339355497542074 valid 0.1825023219982783
LOSS train 0.1339355497542074 valid 0.1827552862531316
LOSS train 0.1339355497542074 valid 0.1828272022339741
LOSS train 0.1339355497542074 valid 0.18290235474705696
LOSS train 0.1339355497542074 valid 0.18292118504838653
LOSS train 0.1339355497542074 valid 0.18296449437089587
LOSS train 0.1339355497542074 valid 0.18311095050661078
LOSS train 0.1339355497542074 valid 0.1831269701621656
LOSS train 0.1339355497542074 valid 0.18321750105194778
LOSS train 0.1339355497542074 valid 0.1832605315069867
LOSS train 0.1339355497542074 valid 0.1834114521107775
LOSS train 0.1339355497542074 valid 0.18328342315251545
LOSS train 0.1339355497542074 valid 0.18323928218350632
LOSS train 0.1339355497542074 valid 0.18327568239775024
LOSS train 0.1339355497542074 valid 0.18309993087996
LOSS train 0.1339355497542074 valid 0.18310479503124952
LOSS train 0.1339355497542074 valid 0.18328891972783196
LOSS train 0.1339355497542074 valid 0.18312834104723183
LOSS train 0.1339355497542074 valid 0.1833124964693446
LOSS train 0.1339355497542074 valid 0.1834936926110846
LOSS train 0.1339355497542074 valid 0.18355952434393824
LOSS train 0.1339355497542074 valid 0.18341679677246062
LOSS train 0.1339355497542074 valid 0.18357539979311135
LOSS train 0.1339355497542074 valid 0.18353041898339026
LOSS train 0.1339355497542074 valid 0.18354533510993284
LOSS train 0.1339355497542074 valid 0.18359506911039353
LOSS train 0.1339355497542074 valid 0.1834352278258221
LOSS train 0.1339355497542074 valid 0.18369308267793957
LOSS train 0.1339355497542074 valid 0.18363591763577441
LOSS train 0.1339355497542074 valid 0.1835336180066499
LOSS train 0.1339355497542074 valid 0.18357572181552065
LOSS train 0.1339355497542074 valid 0.18365133117185906
LOSS train 0.1339355497542074 valid 0.18347357619365365
LOSS train 0.1339355497542074 valid 0.18370477075493613
LOSS train 0.1339355497542074 valid 0.18371183157657564
LOSS train 0.1339355497542074 valid 0.1835809883589928
LOSS train 0.1339355497542074 valid 0.1837384663664975
LOSS train 0.1339355497542074 valid 0.18379454320396177
LOSS train 0.1339355497542074 valid 0.1838323848442433
LOSS train 0.1339355497542074 valid 0.18392845897963553
LOSS train 0.1339355497542074 valid 0.1838601381148932
LOSS train 0.1339355497542074 valid 0.18389971274182312
LOSS train 0.1339355497542074 valid 0.18394683920935298
LOSS train 0.1339355497542074 valid 0.18414213141398644
LOSS train 0.1339355497542074 valid 0.18423691480576326
LOSS train 0.1339355497542074 valid 0.1842310170332591
LOSS train 0.1339355497542074 valid 0.18430413267269347
LOSS train 0.1339355497542074 valid 0.1845989728017765
LOSS train 0.1339355497542074 valid 0.18480569369845337
LOSS train 0.1339355497542074 valid 0.18483011930310814
LOSS train 0.1339355497542074 valid 0.18482374326749282
LOSS train 0.1339355497542074 valid 0.18473245273681654
LOSS train 0.1339355497542074 valid 0.18459380802695072
LOSS train 0.1339355497542074 valid 0.18444216739145114
LOSS train 0.1339355497542074 valid 0.1844244472228498
LOSS train 0.1339355497542074 valid 0.18442244827747345
LOSS train 0.1339355497542074 valid 0.1843352117890565
LOSS train 0.1339355497542074 valid 0.18405115057496316
LOSS train 0.1339355497542074 valid 0.1839725391761574
LOSS train 0.1339355497542074 valid 0.18398573100042176
LOSS train 0.1339355497542074 valid 0.18402308332815506
LOSS train 0.1339355497542074 valid 0.18401752670223898
LOSS train 0.1339355497542074 valid 0.1839704483695562
LOSS train 0.1339355497542074 valid 0.18393995439530247
LOSS train 0.1339355497542074 valid 0.18395385401368552
LOSS train 0.1339355497542074 valid 0.18398047842342277
LOSS train 0.1339355497542074 valid 0.18382576919605642
LOSS train 0.1339355497542074 valid 0.18382829542539708
LOSS train 0.1339355497542074 valid 0.18385268689320763
LOSS train 0.1339355497542074 valid 0.1839461324318331
LOSS train 0.1339355497542074 valid 0.1840309347389108
LOSS train 0.1339355497542074 valid 0.1839579924487987
LOSS train 0.1339355497542074 valid 0.18403504274609916
LOSS train 0.1339355497542074 valid 0.18406791427491495
LOSS train 0.1339355497542074 valid 0.1841191164775437
LOSS train 0.1339355497542074 valid 0.1842004737009605
LOSS train 0.1339355497542074 valid 0.18415299897079057
LOSS train 0.1339355497542074 valid 0.1840994061766476
LOSS train 0.1339355497542074 valid 0.18415998790425436
LOSS train 0.1339355497542074 valid 0.18415746683450906
LOSS train 0.1339355497542074 valid 0.1840828073073606
LOSS train 0.1339355497542074 valid 0.1841160836903488
LOSS train 0.1339355497542074 valid 0.18410280917191738
LOSS train 0.1339355497542074 valid 0.18400908307498925
LOSS train 0.1339355497542074 valid 0.1840186358028631
LOSS train 0.1339355497542074 valid 0.18407315133560087
LOSS train 0.1339355497542074 valid 0.18406961472567254
LOSS train 0.1339355497542074 valid 0.18403960972164685
LOSS train 0.1339355497542074 valid 0.1842145520134475
LOSS train 0.1339355497542074 valid 0.18422993236951007
LOSS train 0.1339355497542074 valid 0.18415027712545698
LOSS train 0.1339355497542074 valid 0.18409416067732287
LOSS train 0.1339355497542074 valid 0.1841528144685628
LOSS train 0.1339355497542074 valid 0.1841769001721961
LOSS train 0.1339355497542074 valid 0.18431707317459173
LOSS train 0.1339355497542074 valid 0.1842490600189194
LOSS train 0.1339355497542074 valid 0.1844110472876335
LOSS train 0.1339355497542074 valid 0.1843930789911599
LOSS train 0.1339355497542074 valid 0.1843619626419094
LOSS train 0.1339355497542074 valid 0.18447081420432637
LOSS train 0.1339355497542074 valid 0.18447488865027062
LOSS train 0.1339355497542074 valid 0.1846328094555922
LOSS train 0.1339355497542074 valid 0.18468307886010646
LOSS train 0.1339355497542074 valid 0.18465930684583215
LOSS train 0.1339355497542074 valid 0.18471741415899937
LOSS train 0.1339355497542074 valid 0.18471569899808277
LOSS train 0.1339355497542074 valid 0.18460800638281688
LOSS train 0.1339355497542074 valid 0.18449399564190802
LOSS train 0.1339355497542074 valid 0.18454938099996462
LOSS train 0.1339355497542074 valid 0.18464021141568343
LOSS train 0.1339355497542074 valid 0.1845926342837846
LOSS train 0.1339355497542074 valid 0.18466463048631945
LOSS train 0.1339355497542074 valid 0.18465599822272888
LOSS train 0.1339355497542074 valid 0.18464887285461792
LOSS train 0.1339355497542074 valid 0.18465874792464013
LOSS train 0.1339355497542074 valid 0.1846026525120525
LOSS train 0.1339355497542074 valid 0.1844515066998096
LOSS train 0.1339355497542074 valid 0.18446303230890057
LOSS train 0.1339355497542074 valid 0.18450447671528114
LOSS train 0.1339355497542074 valid 0.18472368371954492
LOSS train 0.1339355497542074 valid 0.1848186825712522
LOSS train 0.1339355497542074 valid 0.18490014007756475
LOSS train 0.1339355497542074 valid 0.18474618869401535
LOSS train 0.1339355497542074 valid 0.1846760859734368
LOSS train 0.1339355497542074 valid 0.18467058226423483
LOSS train 0.1339355497542074 valid 0.1846059982904366
LOSS train 0.1339355497542074 valid 0.18452304158030752
LOSS train 0.1339355497542074 valid 0.18451065665364944
LOSS train 0.1339355497542074 valid 0.18451694019202133
LOSS train 0.1339355497542074 valid 0.1845489035092168
LOSS train 0.1339355497542074 valid 0.18461308821406164
LOSS train 0.1339355497542074 valid 0.18462323855734272
LOSS train 0.1339355497542074 valid 0.18466384393744775
LOSS train 0.1339355497542074 valid 0.18456562606399285
LOSS train 0.1339355497542074 valid 0.18457606042577032
LOSS train 0.1339355497542074 valid 0.1845286745785011
LOSS train 0.1339355497542074 valid 0.18448196196737712
LOSS train 0.1339355497542074 valid 0.1845373593023798
LOSS train 0.1339355497542074 valid 0.1845293308379893
LOSS train 0.1339355497542074 valid 0.18451583534888513
LOSS train 0.1339355497542074 valid 0.18456929483234066
LOSS train 0.1339355497542074 valid 0.18459156499615784
LOSS train 0.1339355497542074 valid 0.18449043076515848
LOSS train 0.1339355497542074 valid 0.18452147785169276
LOSS train 0.1339355497542074 valid 0.1845966122620474
EPOCH 8:
  batch 1 loss: 0.1161772608757019
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.11778827011585236
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.11566390842199326
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.123495789244771
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.1264507219195366
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.1281514378885428
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.12353477201291493
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.12575882952660322
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.12503801700141695
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.12442608028650284
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.12298613448034633
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.12281562946736813
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.12344317195507196
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.12274093553423882
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.12188005894422531
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.12381340702995658
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.12298860839184593
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.1257951743900776
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.12553808328352475
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.1249015673995018
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.12537496146701632
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.12590611929243262
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.12640172955782517
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.12818786688148975
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.1275574380159378
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.1277811315197211
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.1281323747502433
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.12759671466691153
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.12730476414335185
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.12796581437190374
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.12840527872885427
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.12892548460513353
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.12815766804146045
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.1289900195072679
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.1306334227323532
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.12996553981469738
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.12993114119445956
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.12959635630249977
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.1298978594251168
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.1300114532932639
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.13009489872833577
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.13024625377286048
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.13137020673169647
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.13080344535410404
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.13106121238734986
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.13057498131757198
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.13047957975179592
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.13058792163307467
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.13058659723218607
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.13069246366620063
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.1304644330167303
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.13068635002351725
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.13043357530292474
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.13030348276650464
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.13073078150098974
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.13083228016538279
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.13085784906880898
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.13034640381048465
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.13068608434523565
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.13052589471141499
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.13072434531860663
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.13055053449446155
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.1307448609953835
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.13052624743431807
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.13067414370866923
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.13045773587443613
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.13022923113694831
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.13047552831909237
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.13071845126324805
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.1309991168124335
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.13132641978666815
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.13153813696569866
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.1316323988649943
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.13167780698151202
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.13180077294508616
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.13170496344958482
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.13172820065315669
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.13147508075986153
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.13184688259151917
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.132108695525676
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.13210240015645086
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.1317361588703423
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.1315521828560944
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.13192138741058962
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.13200724063550726
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.13232586333571478
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.13256415013951817
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.1329524934461171
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.13289822847320792
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.13329192772507667
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.1334420070052147
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.1333771510778562
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.13351329175695295
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.13343070407814167
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.13385722519535767
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.13390131085179746
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.13367938857103132
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.1334642942766754
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.1337972463983478
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.13369754672050477
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.13353360023829017
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.13336215160933196
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.13315711864858
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.13330920348660305
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.13291811921766827
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.13294978418721343
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.1331899357594062
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.13295787835010775
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.133076075567018
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.13328828852285038
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.13302409957658062
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.13294208262647902
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.13266180224914467
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.13298425607775388
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.1327385495538297
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.13263362761715364
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.13299383808914414
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.13297561177257763
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.13283764195291936
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.13278624024242164
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.1327794755170168
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.13286462236867577
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.1330442041522119
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.1329858677401658
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.13303826254606246
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.13311627241117613
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.13316859158239966
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.13290099782170728
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.13282197066979815
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.132835646546804
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.13273797440164872
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.1326697879668438
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.1329901471622008
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.13307013040158286
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.13280996393274377
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.13269936282406836
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.13288256678267987
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.13281194066655808
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.13304630661611078
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.13307599297591619
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.13306152926269152
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.13290182380399235
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.13301632011478598
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.13314687703839606
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.13302559349043616
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.13308868210201394
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.1330905734073548
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.1330301543647373
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.13336800453846887
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.13343312030037244
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.1333319257130686
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.13343221159945978
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.13339315184385947
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.1331002057179228
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.13318113290494488
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.1331083426872889
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.1331876522985993
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.13316047370811052
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.13329978212245605
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.13342224843800068
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.13353407447752746
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.1334411163388947
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.13346439298310894
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.13367631168263713
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.13380147697347583
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.1336846930614437
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.13355171377073505
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.13349645274380842
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.13330626179128002
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.13356102705001832
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.1335867970136174
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.13355340514072153
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.1333253928433264
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.1332707807592962
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.13341958139623913
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.13322293635627086
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.1330547486405588
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.1327575034556094
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.1329792879896457
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.13280956049760181
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.13287678171587253
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.13283791040981208
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.13275400935952128
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.13267973810434341
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.13268198387042895
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.13263360298769447
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.13272243523023983
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.1326396976538161
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.1326008721318825
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.13262560618551153
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.13264620592768903
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.1325475937143589
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.13259864474979707
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.13234569827459522
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.13236833149806046
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.1325026490174386
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.13260680826636134
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.13276452559864882
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.13272514710923536
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.13287885766476393
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.13289566119363652
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.13292862722041585
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.1329649867021979
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.1329956291718226
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.13295328409933463
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.13280944146432924
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.13277027734379837
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.13260404199648362
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.1326722298084834
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.13278011970576786
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.13277661645016964
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.13280139174663796
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.1328933704906786
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.13269095060145744
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.13277005574037862
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.1326724193231375
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.13264743376025406
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.13274322221175247
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.13279650135807794
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.13277596929533916
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.13276259577517055
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.13280744968099636
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.13275033683252976
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.13282599905505776
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.13272714611556796
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.13272590595668396
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.13283004467314036
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.1326691934740857
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.13261948125721587
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.13287073798153712
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.13310414446251734
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.13300377271812538
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.13307265751863243
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.13306147344092017
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.1330835906749076
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.13308552583142863
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.1330387071403773
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.1330781090973305
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.1330864047592654
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.13298130643864473
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.13310029039244434
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.13304777591189076
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.13309606480500336
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.13313338777325193
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.1331252233106263
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.13319613796666385
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.13317595307643598
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.13324022509397998
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.1332952592626633
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.13323979783058165
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.13342095181286573
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.1334543131764919
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.1334655550510987
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.13343829808272714
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.13339320029698165
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.1333973887958564
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.1333283172043381
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.1332538230994413
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.1332991321402167
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.13332709776094326
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.13323881588448053
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.1331703735103134
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.13314923247457003
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.1330073227485021
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.13302900369437234
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.13305879647570446
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.1330308417479197
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.13303653250879316
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.13294257976973367
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.13296607042904254
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.13297116261566697
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.13294609575806296
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.13303273692454173
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.13289469333678267
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.13278869287534192
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.13300411683925684
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.13299069755344184
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.1330151212729996
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.13297016822522686
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.13297666353838786
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.13294132512553306
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.13288167515333663
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.13286951400155314
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.13283132310484497
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.13295605125134452
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.13308532244258828
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.1330089697075638
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.13299154228944746
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.132935512694933
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.13296392657633485
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.13297688213410655
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.13309588870161201
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.13326340379771925
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.13328583812227054
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.13325295508918117
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.1332670532771059
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.1332434038521866
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.13320130529819718
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.1331512746105226
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.1331241584320863
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.13314186393026498
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.13314747302145358
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.13314514885051024
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.1330980890626578
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.1330618157005701
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.13307415394514216
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.13314201697367412
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.13309746351722
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.13304412871309854
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.1329566169650324
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.13303994499985428
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.13299876246123743
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.13294404814132868
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.1329308627469904
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.13295463430030005
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.1330757776063077
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.13305922071170356
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.1330471971078114
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.13296219849007257
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.13303987917024643
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.13299765108251868
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.13292298852379278
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.132930081072422
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.1328464823970456
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.13284570393654016
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.1327518698947927
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.13265488198773212
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.1326380713229499
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.13269343252058813
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.13276191371859927
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.13279553608591824
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.1327866499413209
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.13272037665854702
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.13274062297747521
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.13280193314178665
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.13282618163863108
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.13279987776580127
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.13279603798625736
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.13270656467802758
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.13268466892049594
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.132697408871392
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.13295271323873983
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.13298429151231275
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.13301755052579697
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.13302073161239208
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.13298593836195896
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.1329724555056789
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.1329560594140798
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.13288620697136935
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.132806986953531
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.1328107321109527
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.13289874559268355
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.13298061038380643
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.13300603118825094
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.1330046639056273
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.132925813159581
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.13299894620891378
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.13305859654808844
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.13298001146582175
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.13294415718151464
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.13291621228829645
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.13299050918929484
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.1329627790932156
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.13291318008451017
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.13296116351673049
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.13294334202760555
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.1330041681753842
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.1331085634174878
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.1330740896222714
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.13299360506840655
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.13305258656528118
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.13303938121484812
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.13305313935148813
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.13313646636465015
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.13315823942422866
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.1331510559873695
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.13313968904533816
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.1332005341414106
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.1331414583687103
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.13312230504264957
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.13309188515651882
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.133024493456667
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.13310933605264436
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.13310269217860574
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.13302436627737887
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.13297095228893768
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.1330076009227632
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.1329592620166614
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.13294389826702888
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.13287547043500803
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.1328351430575866
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.13283883536956748
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.13294393150254363
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.1329591052635067
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.1329962657222265
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.13293708954947164
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.13283677462966978
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.1328704229850865
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.1328374821888773
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.13280873149633407
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.13279667266289194
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.13275685433798762
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.1327158840507195
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.13274793422753267
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.13278991686709132
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.1327521887795972
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.13276010738909977
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.1328217828938482
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.1327857362939851
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.13278391279461907
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.13275839803500186
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.13281260197530093
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.13282404423265134
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.13286897959412583
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.13291825517473452
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.1330000021172544
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.13300020769417142
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.1329598685450246
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.13301640796988562
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.1329771793846573
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.13303731971214616
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.13306209859901694
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.13297993702105032
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.13297348975572945
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.13294541646452512
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.13285260103607963
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.1328505612124604
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.13280247223224037
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.1327264241717912
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.1326559983540413
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.1326969718677262
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.13265088571373512
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.1326353980301434
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.13268351360880834
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.13264354962384564
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.1327287153332332
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.1327818419520042
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.13273861007467253
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.13263204939884043
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.13261855442754247
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.1326309178793241
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.13257727588261414
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.1325581546359891
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.13253741751651507
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.13253819503810968
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.13254166347697177
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.13250652065613125
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.13256522178250765
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.13255393913301966
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.13253330787022907
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.13249948257353247
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.13254922638292863
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.13250106453040314
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.13251033003569174
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.13246455710012833
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.1324642981614983
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.13247848522219668
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.1325011245223112
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.13249436238362639
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.13256331601220628
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.13255114924493944
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.13254227656951714
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.13248415532688856
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.1325175802640874
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.1325415878526626
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.1325738052377885
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.13259839478728602
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.13268693214935115
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.1327379934632702
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.13278258040230326
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.13281925142823764
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.1329154227623495
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.1329154227623495 valid 0.21452048420906067
LOSS train 0.1329154227623495 valid 0.19131141901016235
LOSS train 0.1329154227623495 valid 0.18389189740022024
LOSS train 0.1329154227623495 valid 0.17696800082921982
LOSS train 0.1329154227623495 valid 0.17254373133182527
LOSS train 0.1329154227623495 valid 0.1825457289814949
LOSS train 0.1329154227623495 valid 0.19304172268935613
LOSS train 0.1329154227623495 valid 0.1907333042472601
LOSS train 0.1329154227623495 valid 0.18999630543920729
LOSS train 0.1329154227623495 valid 0.19070555865764618
LOSS train 0.1329154227623495 valid 0.18991143866018814
LOSS train 0.1329154227623495 valid 0.19022415205836296
LOSS train 0.1329154227623495 valid 0.18995408026071695
LOSS train 0.1329154227623495 valid 0.18944306990930013
LOSS train 0.1329154227623495 valid 0.18700035711129506
LOSS train 0.1329154227623495 valid 0.18698306940495968
LOSS train 0.1329154227623495 valid 0.1878360737772549
LOSS train 0.1329154227623495 valid 0.18678766902950075
LOSS train 0.1329154227623495 valid 0.1890996216159118
LOSS train 0.1329154227623495 valid 0.18889726400375367
LOSS train 0.1329154227623495 valid 0.18814904349190847
LOSS train 0.1329154227623495 valid 0.1867238696325909
LOSS train 0.1329154227623495 valid 0.1865510564783345
LOSS train 0.1329154227623495 valid 0.18654836403826872
LOSS train 0.1329154227623495 valid 0.18531400084495545
LOSS train 0.1329154227623495 valid 0.1850509781103868
LOSS train 0.1329154227623495 valid 0.18520656669581378
LOSS train 0.1329154227623495 valid 0.18498693513018744
LOSS train 0.1329154227623495 valid 0.18441549508736052
LOSS train 0.1329154227623495 valid 0.184922523299853
LOSS train 0.1329154227623495 valid 0.18567971933272578
LOSS train 0.1329154227623495 valid 0.18481704546138644
LOSS train 0.1329154227623495 valid 0.1850177257349997
LOSS train 0.1329154227623495 valid 0.18449317006503835
LOSS train 0.1329154227623495 valid 0.1860896681036268
LOSS train 0.1329154227623495 valid 0.18587422453694874
LOSS train 0.1329154227623495 valid 0.18654198259920687
LOSS train 0.1329154227623495 valid 0.1869875176956779
LOSS train 0.1329154227623495 valid 0.1865301223901602
LOSS train 0.1329154227623495 valid 0.18619722053408622
LOSS train 0.1329154227623495 valid 0.1867151093192217
LOSS train 0.1329154227623495 valid 0.18696723878383636
LOSS train 0.1329154227623495 valid 0.1869980879994326
LOSS train 0.1329154227623495 valid 0.18744339421391487
LOSS train 0.1329154227623495 valid 0.1874362763431337
LOSS train 0.1329154227623495 valid 0.18774646261463995
LOSS train 0.1329154227623495 valid 0.1883515401723537
LOSS train 0.1329154227623495 valid 0.1882182558377584
LOSS train 0.1329154227623495 valid 0.1888511266027178
LOSS train 0.1329154227623495 valid 0.18833970993757249
LOSS train 0.1329154227623495 valid 0.18854578073118247
LOSS train 0.1329154227623495 valid 0.1880534365773201
LOSS train 0.1329154227623495 valid 0.18852186287349126
LOSS train 0.1329154227623495 valid 0.18863907704750696
LOSS train 0.1329154227623495 valid 0.18859394842928107
LOSS train 0.1329154227623495 valid 0.18781086429953575
LOSS train 0.1329154227623495 valid 0.18777041503211908
LOSS train 0.1329154227623495 valid 0.18757342983936443
LOSS train 0.1329154227623495 valid 0.18792224031383709
LOSS train 0.1329154227623495 valid 0.18787069395184516
LOSS train 0.1329154227623495 valid 0.1874622807639544
LOSS train 0.1329154227623495 valid 0.18798174588910996
LOSS train 0.1329154227623495 valid 0.1873378786775801
LOSS train 0.1329154227623495 valid 0.1882411555852741
LOSS train 0.1329154227623495 valid 0.18846930792698494
LOSS train 0.1329154227623495 valid 0.18825816430828787
LOSS train 0.1329154227623495 valid 0.18763012272208485
LOSS train 0.1329154227623495 valid 0.18765667212360046
LOSS train 0.1329154227623495 valid 0.18722272459147632
LOSS train 0.1329154227623495 valid 0.18768791684082575
LOSS train 0.1329154227623495 valid 0.187324658036232
LOSS train 0.1329154227623495 valid 0.187550642217199
LOSS train 0.1329154227623495 valid 0.187560499530949
LOSS train 0.1329154227623495 valid 0.18760452198015676
LOSS train 0.1329154227623495 valid 0.18792066991329193
LOSS train 0.1329154227623495 valid 0.18830436919080584
LOSS train 0.1329154227623495 valid 0.18813773718747226
LOSS train 0.1329154227623495 valid 0.1879644105449701
LOSS train 0.1329154227623495 valid 0.18758391711530806
LOSS train 0.1329154227623495 valid 0.18695688005536795
LOSS train 0.1329154227623495 valid 0.18664270676212547
LOSS train 0.1329154227623495 valid 0.18709373037989546
LOSS train 0.1329154227623495 valid 0.18685291743422128
LOSS train 0.1329154227623495 valid 0.18683474386731783
LOSS train 0.1329154227623495 valid 0.1867206787361818
LOSS train 0.1329154227623495 valid 0.18648883423139884
LOSS train 0.1329154227623495 valid 0.18624229489386768
LOSS train 0.1329154227623495 valid 0.18594598025083542
LOSS train 0.1329154227623495 valid 0.1865102658780773
LOSS train 0.1329154227623495 valid 0.18649689455827076
LOSS train 0.1329154227623495 valid 0.1865862760242525
LOSS train 0.1329154227623495 valid 0.1865468656887179
LOSS train 0.1329154227623495 valid 0.186323078409318
LOSS train 0.1329154227623495 valid 0.18650990994052685
LOSS train 0.1329154227623495 valid 0.1864403989754225
LOSS train 0.1329154227623495 valid 0.18651185557246208
LOSS train 0.1329154227623495 valid 0.1863823695895598
LOSS train 0.1329154227623495 valid 0.1865871709828474
LOSS train 0.1329154227623495 valid 0.18669860215500148
LOSS train 0.1329154227623495 valid 0.1867736265063286
LOSS train 0.1329154227623495 valid 0.18695136668658494
LOSS train 0.1329154227623495 valid 0.1871517300605774
LOSS train 0.1329154227623495 valid 0.18687413705205455
LOSS train 0.1329154227623495 valid 0.187026548700837
LOSS train 0.1329154227623495 valid 0.18710304725737797
LOSS train 0.1329154227623495 valid 0.18753062975856494
LOSS train 0.1329154227623495 valid 0.1874330978248721
LOSS train 0.1329154227623495 valid 0.18759041734867626
LOSS train 0.1329154227623495 valid 0.1879439504321562
LOSS train 0.1329154227623495 valid 0.18805958425456828
LOSS train 0.1329154227623495 valid 0.18800374073488219
LOSS train 0.1329154227623495 valid 0.18788328820041247
LOSS train 0.1329154227623495 valid 0.1879288877269863
LOSS train 0.1329154227623495 valid 0.18821017496418535
LOSS train 0.1329154227623495 valid 0.1884225965841957
LOSS train 0.1329154227623495 valid 0.18860075956788555
LOSS train 0.1329154227623495 valid 0.18857206009392047
LOSS train 0.1329154227623495 valid 0.18821699715266793
LOSS train 0.1329154227623495 valid 0.18787096803929626
LOSS train 0.1329154227623495 valid 0.18755385714272657
LOSS train 0.1329154227623495 valid 0.1873899078812481
LOSS train 0.1329154227623495 valid 0.18743120854506728
LOSS train 0.1329154227623495 valid 0.18722826181873073
LOSS train 0.1329154227623495 valid 0.18755226654391136
LOSS train 0.1329154227623495 valid 0.1873855288028717
LOSS train 0.1329154227623495 valid 0.1877821855365284
LOSS train 0.1329154227623495 valid 0.18761806894005753
LOSS train 0.1329154227623495 valid 0.1876628209138289
LOSS train 0.1329154227623495 valid 0.1878326000400292
LOSS train 0.1329154227623495 valid 0.187489636357014
LOSS train 0.1329154227623495 valid 0.18720365480612253
LOSS train 0.1329154227623495 valid 0.18686784013654245
LOSS train 0.1329154227623495 valid 0.18678969705015197
LOSS train 0.1329154227623495 valid 0.18691883981227875
LOSS train 0.1329154227623495 valid 0.18679789713135472
LOSS train 0.1329154227623495 valid 0.1867810451589963
LOSS train 0.1329154227623495 valid 0.18648284554046435
LOSS train 0.1329154227623495 valid 0.18641955686220224
LOSS train 0.1329154227623495 valid 0.18636285272433603
LOSS train 0.1329154227623495 valid 0.18648558386734554
LOSS train 0.1329154227623495 valid 0.18633771883257738
LOSS train 0.1329154227623495 valid 0.1863580166663922
LOSS train 0.1329154227623495 valid 0.18627157025820726
LOSS train 0.1329154227623495 valid 0.18627203804337317
LOSS train 0.1329154227623495 valid 0.18613979734223465
LOSS train 0.1329154227623495 valid 0.1863537998232123
LOSS train 0.1329154227623495 valid 0.1862972213500211
LOSS train 0.1329154227623495 valid 0.18718062693605553
LOSS train 0.1329154227623495 valid 0.1873008623819223
LOSS train 0.1329154227623495 valid 0.18724005728960036
LOSS train 0.1329154227623495 valid 0.18744406429742347
LOSS train 0.1329154227623495 valid 0.18710645347049362
LOSS train 0.1329154227623495 valid 0.18707079766622556
LOSS train 0.1329154227623495 valid 0.18692991969647346
LOSS train 0.1329154227623495 valid 0.1868483462641316
LOSS train 0.1329154227623495 valid 0.18688048145327812
LOSS train 0.1329154227623495 valid 0.18697168103828551
LOSS train 0.1329154227623495 valid 0.18698151852888398
LOSS train 0.1329154227623495 valid 0.1870403746201557
LOSS train 0.1329154227623495 valid 0.18698121337220072
LOSS train 0.1329154227623495 valid 0.18684454780557883
LOSS train 0.1329154227623495 valid 0.18670937216576236
LOSS train 0.1329154227623495 valid 0.18643931569131605
LOSS train 0.1329154227623495 valid 0.18627870146457742
LOSS train 0.1329154227623495 valid 0.18610923118663555
LOSS train 0.1329154227623495 valid 0.1861829571874745
LOSS train 0.1329154227623495 valid 0.18650410150339503
LOSS train 0.1329154227623495 valid 0.18643581813999585
LOSS train 0.1329154227623495 valid 0.1866233286243924
LOSS train 0.1329154227623495 valid 0.18663507419473985
LOSS train 0.1329154227623495 valid 0.18664781479110495
LOSS train 0.1329154227623495 valid 0.18651793697892233
LOSS train 0.1329154227623495 valid 0.1865202750084717
LOSS train 0.1329154227623495 valid 0.1864772030505641
LOSS train 0.1329154227623495 valid 0.18617426770074028
LOSS train 0.1329154227623495 valid 0.18613505160266702
LOSS train 0.1329154227623495 valid 0.18621166622908103
LOSS train 0.1329154227623495 valid 0.18644910707567516
LOSS train 0.1329154227623495 valid 0.18636291078682052
LOSS train 0.1329154227623495 valid 0.18628230434325005
LOSS train 0.1329154227623495 valid 0.18641924446458974
LOSS train 0.1329154227623495 valid 0.18625769993433586
LOSS train 0.1329154227623495 valid 0.1863352230500654
LOSS train 0.1329154227623495 valid 0.18620448162698228
LOSS train 0.1329154227623495 valid 0.18606392708984582
LOSS train 0.1329154227623495 valid 0.18609293077581673
LOSS train 0.1329154227623495 valid 0.1859443816431066
LOSS train 0.1329154227623495 valid 0.1859210933301043
LOSS train 0.1329154227623495 valid 0.1857999951908828
LOSS train 0.1329154227623495 valid 0.185892002362954
LOSS train 0.1329154227623495 valid 0.18580717698753815
LOSS train 0.1329154227623495 valid 0.18587981505940357
LOSS train 0.1329154227623495 valid 0.18560282075343354
LOSS train 0.1329154227623495 valid 0.18550244995306447
LOSS train 0.1329154227623495 valid 0.18531086635895264
LOSS train 0.1329154227623495 valid 0.18529008572198907
LOSS train 0.1329154227623495 valid 0.18549039869139036
LOSS train 0.1329154227623495 valid 0.18543651421563795
LOSS train 0.1329154227623495 valid 0.18555635820381605
LOSS train 0.1329154227623495 valid 0.18540354616940022
LOSS train 0.1329154227623495 valid 0.18521175952396582
LOSS train 0.1329154227623495 valid 0.18512651213619968
LOSS train 0.1329154227623495 valid 0.1851228291324794
LOSS train 0.1329154227623495 valid 0.18532166841859912
LOSS train 0.1329154227623495 valid 0.1851017714273639
LOSS train 0.1329154227623495 valid 0.18513001436458051
LOSS train 0.1329154227623495 valid 0.18507651754335505
LOSS train 0.1329154227623495 valid 0.18491031480236694
LOSS train 0.1329154227623495 valid 0.18490148695747247
LOSS train 0.1329154227623495 valid 0.1848248249008542
LOSS train 0.1329154227623495 valid 0.18483186905135476
LOSS train 0.1329154227623495 valid 0.18479224723183885
LOSS train 0.1329154227623495 valid 0.18477041123898377
LOSS train 0.1329154227623495 valid 0.18480196581265637
LOSS train 0.1329154227623495 valid 0.18467551275741223
LOSS train 0.1329154227623495 valid 0.18454231335609048
LOSS train 0.1329154227623495 valid 0.18448419456932402
LOSS train 0.1329154227623495 valid 0.18452582897943096
LOSS train 0.1329154227623495 valid 0.18462567113033712
LOSS train 0.1329154227623495 valid 0.1846135226162997
LOSS train 0.1329154227623495 valid 0.18452463760904597
LOSS train 0.1329154227623495 valid 0.18456595015149932
LOSS train 0.1329154227623495 valid 0.18472935740455917
LOSS train 0.1329154227623495 valid 0.18483667255246214
LOSS train 0.1329154227623495 valid 0.18498721102873483
LOSS train 0.1329154227623495 valid 0.18523919041705342
LOSS train 0.1329154227623495 valid 0.1853104833726841
LOSS train 0.1329154227623495 valid 0.18538222893288261
LOSS train 0.1329154227623495 valid 0.18539810649172186
LOSS train 0.1329154227623495 valid 0.18544491587773612
LOSS train 0.1329154227623495 valid 0.18559431339497173
LOSS train 0.1329154227623495 valid 0.1856120559547482
LOSS train 0.1329154227623495 valid 0.1857047836284269
LOSS train 0.1329154227623495 valid 0.1857529044406027
LOSS train 0.1329154227623495 valid 0.18590757314195025
LOSS train 0.1329154227623495 valid 0.1857814036183438
LOSS train 0.1329154227623495 valid 0.1857331833381693
LOSS train 0.1329154227623495 valid 0.18576554759961217
LOSS train 0.1329154227623495 valid 0.18558075793368048
LOSS train 0.1329154227623495 valid 0.1855859111373623
LOSS train 0.1329154227623495 valid 0.18577076488510702
LOSS train 0.1329154227623495 valid 0.18560878883216006
LOSS train 0.1329154227623495 valid 0.1857998521975529
LOSS train 0.1329154227623495 valid 0.18598274196513365
LOSS train 0.1329154227623495 valid 0.18604447239515734
LOSS train 0.1329154227623495 valid 0.1859001357380937
LOSS train 0.1329154227623495 valid 0.18605959638651565
LOSS train 0.1329154227623495 valid 0.1860086362688772
LOSS train 0.1329154227623495 valid 0.1860226164141812
LOSS train 0.1329154227623495 valid 0.18606661182641984
LOSS train 0.1329154227623495 valid 0.18590914953752344
LOSS train 0.1329154227623495 valid 0.18617101770544808
LOSS train 0.1329154227623495 valid 0.18611400029640424
LOSS train 0.1329154227623495 valid 0.18601283671583715
LOSS train 0.1329154227623495 valid 0.18605776440863517
LOSS train 0.1329154227623495 valid 0.1861338464077562
LOSS train 0.1329154227623495 valid 0.1859523471343378
LOSS train 0.1329154227623495 valid 0.18619101783332898
LOSS train 0.1329154227623495 valid 0.18619902938259153
LOSS train 0.1329154227623495 valid 0.1860630981051005
LOSS train 0.1329154227623495 valid 0.1862175577102493
LOSS train 0.1329154227623495 valid 0.18627862984897525
LOSS train 0.1329154227623495 valid 0.18631958105944862
LOSS train 0.1329154227623495 valid 0.18641586415469646
LOSS train 0.1329154227623495 valid 0.18635668777069955
LOSS train 0.1329154227623495 valid 0.18639060602824492
LOSS train 0.1329154227623495 valid 0.1864346767521083
LOSS train 0.1329154227623495 valid 0.18663211845195116
LOSS train 0.1329154227623495 valid 0.18672394636173673
LOSS train 0.1329154227623495 valid 0.18672108159021095
LOSS train 0.1329154227623495 valid 0.1868018495860575
LOSS train 0.1329154227623495 valid 0.18710214326925137
LOSS train 0.1329154227623495 valid 0.18731464674839607
LOSS train 0.1329154227623495 valid 0.1873471815220631
LOSS train 0.1329154227623495 valid 0.18734097995541313
LOSS train 0.1329154227623495 valid 0.18726145535491515
LOSS train 0.1329154227623495 valid 0.18712912725842817
LOSS train 0.1329154227623495 valid 0.18697380976711245
LOSS train 0.1329154227623495 valid 0.18695393065825158
LOSS train 0.1329154227623495 valid 0.18694859482347964
LOSS train 0.1329154227623495 valid 0.18686496903887848
LOSS train 0.1329154227623495 valid 0.18657574464454718
LOSS train 0.1329154227623495 valid 0.1865037319516967
LOSS train 0.1329154227623495 valid 0.18651743841842866
LOSS train 0.1329154227623495 valid 0.1865560165622778
LOSS train 0.1329154227623495 valid 0.18655031674600148
LOSS train 0.1329154227623495 valid 0.1865074688030037
LOSS train 0.1329154227623495 valid 0.18647986655640933
LOSS train 0.1329154227623495 valid 0.18649680211263545
LOSS train 0.1329154227623495 valid 0.18652478862425376
LOSS train 0.1329154227623495 valid 0.18636972582626998
LOSS train 0.1329154227623495 valid 0.18636906422572594
LOSS train 0.1329154227623495 valid 0.18638749451157177
LOSS train 0.1329154227623495 valid 0.18647253493062493
LOSS train 0.1329154227623495 valid 0.1865608862901138
LOSS train 0.1329154227623495 valid 0.186485563430029
LOSS train 0.1329154227623495 valid 0.186564129410368
LOSS train 0.1329154227623495 valid 0.18660043209991198
LOSS train 0.1329154227623495 valid 0.18664964395621947
LOSS train 0.1329154227623495 valid 0.1867346042394638
LOSS train 0.1329154227623495 valid 0.1866883795423761
LOSS train 0.1329154227623495 valid 0.18663344394094897
LOSS train 0.1329154227623495 valid 0.18669161672639376
LOSS train 0.1329154227623495 valid 0.18669114654001437
LOSS train 0.1329154227623495 valid 0.18662259881613685
LOSS train 0.1329154227623495 valid 0.18665903115194607
LOSS train 0.1329154227623495 valid 0.1866429825267885
LOSS train 0.1329154227623495 valid 0.18654497296779188
LOSS train 0.1329154227623495 valid 0.1865555051364559
LOSS train 0.1329154227623495 valid 0.18661174067566472
LOSS train 0.1329154227623495 valid 0.18661031797767835
LOSS train 0.1329154227623495 valid 0.18658449782583958
LOSS train 0.1329154227623495 valid 0.1867643585696388
LOSS train 0.1329154227623495 valid 0.18677754325282042
LOSS train 0.1329154227623495 valid 0.18669767370299686
LOSS train 0.1329154227623495 valid 0.18664947864186915
LOSS train 0.1329154227623495 valid 0.18671219821796056
LOSS train 0.1329154227623495 valid 0.18673491731004896
LOSS train 0.1329154227623495 valid 0.18687788464806296
LOSS train 0.1329154227623495 valid 0.18680818383581937
LOSS train 0.1329154227623495 valid 0.18697153263931335
LOSS train 0.1329154227623495 valid 0.18695230527509074
LOSS train 0.1329154227623495 valid 0.18691724455762573
LOSS train 0.1329154227623495 valid 0.18702941948984875
LOSS train 0.1329154227623495 valid 0.18702550484583927
LOSS train 0.1329154227623495 valid 0.18718433069305185
LOSS train 0.1329154227623495 valid 0.18723560578050233
LOSS train 0.1329154227623495 valid 0.1872068533628452
LOSS train 0.1329154227623495 valid 0.18726511249788627
LOSS train 0.1329154227623495 valid 0.18726241660840584
LOSS train 0.1329154227623495 valid 0.18715428013996055
LOSS train 0.1329154227623495 valid 0.18704236904720226
LOSS train 0.1329154227623495 valid 0.1870979022782844
LOSS train 0.1329154227623495 valid 0.18718397938562725
LOSS train 0.1329154227623495 valid 0.18713517811760974
LOSS train 0.1329154227623495 valid 0.18720738935683454
LOSS train 0.1329154227623495 valid 0.18719754127083618
LOSS train 0.1329154227623495 valid 0.18719324786987532
LOSS train 0.1329154227623495 valid 0.18720463709493654
LOSS train 0.1329154227623495 valid 0.18714884331121165
LOSS train 0.1329154227623495 valid 0.1869952472860862
LOSS train 0.1329154227623495 valid 0.18700266080467323
LOSS train 0.1329154227623495 valid 0.1870440336775154
LOSS train 0.1329154227623495 valid 0.18726179601494655
LOSS train 0.1329154227623495 valid 0.1873598903849505
LOSS train 0.1329154227623495 valid 0.18744695625897778
LOSS train 0.1329154227623495 valid 0.18729751623673124
LOSS train 0.1329154227623495 valid 0.18722781401941146
LOSS train 0.1329154227623495 valid 0.18722202379075026
LOSS train 0.1329154227623495 valid 0.18715633979865484
LOSS train 0.1329154227623495 valid 0.18707327676294874
LOSS train 0.1329154227623495 valid 0.1870607407340272
LOSS train 0.1329154227623495 valid 0.18706906559447053
LOSS train 0.1329154227623495 valid 0.18709819439777547
LOSS train 0.1329154227623495 valid 0.1871609542571323
LOSS train 0.1329154227623495 valid 0.1871692385901226
LOSS train 0.1329154227623495 valid 0.1872086489067024
LOSS train 0.1329154227623495 valid 0.18710467152755353
LOSS train 0.1329154227623495 valid 0.1871151382461564
LOSS train 0.1329154227623495 valid 0.187068282523089
LOSS train 0.1329154227623495 valid 0.18702121702257615
LOSS train 0.1329154227623495 valid 0.1870729006950368
LOSS train 0.1329154227623495 valid 0.18707182453355184
LOSS train 0.1329154227623495 valid 0.18705675660908877
LOSS train 0.1329154227623495 valid 0.1871118767620766
LOSS train 0.1329154227623495 valid 0.18713501791973583
LOSS train 0.1329154227623495 valid 0.18703460928854565
LOSS train 0.1329154227623495 valid 0.18707074828283943
LOSS train 0.1329154227623495 valid 0.1871447954882128
EPOCH 9:
  batch 1 loss: 0.11574520170688629
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.11836040019989014
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.11631022145350774
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.12428010813891888
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.12722991555929183
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.1285342959066232
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.12322133885962623
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.12535421084612608
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.12431464509831534
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.12393366619944572
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.12215712598778984
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.12176705710589886
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.12193124282818574
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.1212634529386248
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.12018706500530243
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.1218960452824831
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.12089147813179914
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.12427721586492327
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.12401761075383738
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.12348687686026097
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.12396404679332461
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.12444182519208301
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.12466358458218367
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.12649421052386364
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.12584778636693955
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.12621318119076583
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.12654475895343004
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.1260729700859104
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.12582907620175132
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.126536542425553
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.12701933782908223
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.12759806285612285
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.12690843686913
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.1278334347640767
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.1294047406741551
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.12883168934947914
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.1289544683453199
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.1286591640428493
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.1290346498672779
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.1290101334452629
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.1290580252321755
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.12921572228272757
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.13038819369881652
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.12992709604176608
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.13014041417174868
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.1295997926398464
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.1295246036445841
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.12966048391535878
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.12967269107395288
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.12972263976931572
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.12942390494486866
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.12952502616322958
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.12929155663499292
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.1292204627836192
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.129606278647076
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.12981464314673627
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.1297985435577861
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.12928919930910243
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.12968310057106663
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.129508450999856
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.12965469367680002
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.12953108729374024
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.1296798282909015
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.1294348945375532
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.1296747278708678
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.12943668979586978
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.12921323182422723
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.12945325705496705
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.12971950603136118
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.13003648764320783
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.13039319248686373
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.13061216773672235
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.13072635304846175
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.13075111373453527
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.13082555085420608
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.130727750298224
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.13076651173752624
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.1304759617226246
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.13089981175298931
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.13114356780424713
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.1312077127305078
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.13086035602339885
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.13066426494035377
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.13103460839816503
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.13112720952314488
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.13143057878627334
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.1316375076428227
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.13204209658909927
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.13202323320876347
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.13238549795415666
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.1325707155597079
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.13250412808164305
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.1326123788472145
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.13250050987017917
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.13291263227400027
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.13300743598180512
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.13279559847313105
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.13258150866141125
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.13287691433321347
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.13276384614408016
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.13261674860916514
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.13246455021640835
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.13223648715077094
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.13233826488543016
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.13192357867956161
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.1319347563076694
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.132149899883248
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.13191066109747798
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.1320259883452993
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.13223484652963552
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.13199755999150578
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.13195541308128408
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.1316743145606159
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.13201260128826425
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.1317958111996236
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.13172197194191917
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.13205751120789438
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.13206271025336394
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.13196031849424378
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.13189628043522436
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.13187839983662297
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.13194573591234254
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.13210142743054445
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.1320578382020035
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.13212466567754746
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.13224415770835346
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.13231575013849678
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.1320805968134664
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.1320220728715261
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.13200406191440728
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.13187183997103277
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.1318177686383327
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.13208785423434766
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.13215319373047174
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.13191519291312606
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.1318589409484583
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.1320212005916303
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.13196109089514482
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.1322343808819922
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.13227989040315152
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.13226042946813799
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.13207921474962167
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.1321884304597661
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.13231505686417222
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.13220660907441173
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.13227201727767512
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.13230260370337232
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.1322510192623815
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.1325987997971125
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.13267106369137763
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.13256193653045112
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.13266797947060122
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.13263432481904436
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.13232443234943725
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.13240154794146938
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.132361731515863
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.13248780383995384
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.13245322974988177
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.1325767920733248
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.13269710266031326
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.13280755758100415
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.13270360856880375
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.13270099570780444
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.13295091088952088
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.13308227983388035
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.13295390663376774
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.13281543013043032
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.13272347970910015
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.1325360891469837
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.13280080122982754
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.13282230300338646
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.13278128600917583
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.13256888565300518
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.13247574908637452
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.1326263506923403
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.13244057810780677
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.13226105175954475
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.13196884062183037
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.1321780639986752
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.13199929710891511
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.13208588776667474
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.13205421151040675
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.13194268587671343
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.13188870012274254
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.13192388918754216
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.1318701937874799
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.13195191630545786
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.13189174373574714
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.13187243828855494
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.1319319913653951
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.13197376298186667
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.13190297041243562
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.13196650058648746
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.13170552457269935
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.1317489178134845
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.13186266558358864
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.13201862448816978
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.1321819902896279
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.13212921316899248
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.13223753429949284
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.13224423954735942
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.13232135920241328
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.13236050814243372
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.13240993088659117
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.13235785764891927
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.13223679891778428
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.1322638367973088
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.13213183272343415
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.13223307433596068
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.13238617799111774
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.13241091250525833
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.1324328156715294
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.1325261867801908
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.13234442869357974
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.13242440944494202
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.13233677601372754
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.13234335060493188
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.13244793933192525
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.13252676166083716
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.13253587125377222
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.13252071658680342
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.13252388612107113
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.1324845311553489
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.1325890633078026
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.13251615865363015
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.13252655993652554
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.1326385848388273
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.1325023383751773
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.1324736284311682
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.13270782714952592
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.13292722729764458
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.13286341990119424
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.13292447231358212
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.13293705727809516
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.13295877854874794
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.1329439834896791
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.13290043712794025
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.1329244589041762
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.13291524235299443
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.13281480250880123
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.13291565526694182
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.13285496387599913
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.13291705077812996
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.1329181957073876
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.13289923613168755
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.13295828654029504
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.1329378612369661
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.13306367253103563
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.13308547927911982
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.13302589228749276
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.13321566376909316
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.13323897835872475
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.13325313567055072
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.13322820294443077
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.1331791193169706
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.1331670580257196
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.13308813585265603
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.1330224384171094
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.13310583476624432
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.1331341505050659
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.13306918737417894
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.13300781162413022
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.1329802712106886
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.13285267790497252
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.1328715983145642
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.13291781051154422
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.13289375751094426
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.13286627220240102
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.1327501331840306
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.1327751865817441
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.13277379692810487
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.1327181773753289
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.13282398429218228
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.13269693244003902
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.13259417942979118
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.13280570239800474
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.1327768208909551
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.13279717562974785
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.13275365889286056
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.13275210841425827
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.13271474429933203
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.13263607225942273
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.13262905227843105
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.13260245454353345
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.13272502375276465
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.13283748586069455
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.1327664430288903
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.1327546537698557
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.13270558380750636
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.1327321687135203
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.13272199823274644
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.13283177893863965
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.13302937760287992
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.13305177818350242
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.13300789453215517
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.13303290201804122
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.13300208002328873
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.1329446151202557
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.13289760125420963
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.13286649170021217
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.13291034946508978
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.13290465709488122
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.1328904946202492
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.13284985113300776
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.13281649541659432
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.13283428301413855
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.1328873758110239
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.13285336868426242
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.1328215683210629
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.1327358286707632
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.1327885083638587
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.1327590873130621
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.13270890427092774
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.13269922393522446
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.13271580518238127
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.13283160253416135
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.13280885100176656
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.13281821782858866
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.13273065516193833
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.13282354595139623
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.13276648895112897
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.1327075655832424
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.1327199291195663
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.13261948109684904
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.1326295437491857
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.1325379323694238
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.1324328330177415
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.13243945508559302
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.1324841887166435
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.13253199925476855
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.13256606655448586
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.13257019129772502
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.13250081281404238
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.13253044198730035
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.13258930231208232
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.13262398514364446
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.13259449103731077
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.13258687505972455
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.13250049814625828
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.1324964192641132
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.13252004869609985
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.13276461483529436
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.13281204399405694
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.13285823710002873
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.13286815810462702
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.1328355565968621
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.13282199777254797
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.13281313362049646
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.13274618259559046
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.1326647887485368
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.13266536334471146
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.13275717478245497
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.13284756766669137
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.1328788592064448
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.1328769476900638
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.13279091866080012
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.1328751503043816
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.13293986669715557
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.1328588515146529
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.13281331629388862
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.13278808081496785
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.132869144273891
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.1328392038660601
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.13279694658550587
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.1328441170388705
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.13283525285173636
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.13290106909151622
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.1330050651146018
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.13297365244407316
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.13289371167083044
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.1329354858502866
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.13291627523158828
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.13293795784980297
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.13300791089945935
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.1330431967775027
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.13303265373836806
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.13303086768331832
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.13309395189086595
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.1330361533762597
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.13302044488097492
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.1330006665836169
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.13293545306977178
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.1330210155279767
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.1330118920886889
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.1329348777795767
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.13288077117556735
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.13291486412363768
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.1328639465662622
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.1328562442165414
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.13278952745290903
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.13276034553566246
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.1327557722106576
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.13286504469830873
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.13286842801017204
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.1329008938202375
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.1328442587771199
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.13273972374215834
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.13278420808626779
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.13276267774347075
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.13272922346368432
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.132723768174351
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.1326833505053722
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.1326359608776812
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.1326678451335076
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.13270810382601655
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.1326625003236268
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.13266907895958688
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.13272103652650236
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.13267311260723544
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.13267287284862705
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.1326490913284376
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.1327149694767392
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.13272988067412203
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.1327927509583713
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.13284524463745484
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.13291632298093575
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.13290936340816872
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.13286465691607535
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.13290428496488238
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.13285777721376646
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.13290668185136664
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.1329345364141238
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.13285451678308188
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.13284503856568403
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.1328236863017082
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.13272613271231382
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.13272950714519488
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.13268688246259622
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.13260394447680676
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.13253423538665438
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.13257019693710412
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.13252114495952372
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.1325123970317235
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.1325657252170798
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.1325236730363177
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.13260958212162924
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.1326571124130583
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.132622557534883
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.13251924025985265
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.1325125809081576
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.13253490238232948
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.13248661472314624
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.1324772853782699
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.13245703452812121
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.13245747985129946
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.13245495206638835
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.1324191242623116
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.13247089798096567
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.1324605207832194
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.13244069072935316
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.1324085868283809
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.13244733239103734
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.13239944704847356
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.13240624645864385
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.13235778726719238
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.13234963895458923
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.1323564877525722
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.13237366301524067
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.1323757281360543
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.13243738199057786
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.13241941027682672
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.1324199123945071
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.13235604275444413
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.13237366814102078
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.13239543820901584
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.13242201895608924
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.13243947596123692
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.13252516082909882
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.1325768401532539
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.13261830094963947
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.1326580488472987
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.13275483435303984
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.13275483435303984 valid 0.2164585143327713
LOSS train 0.13275483435303984 valid 0.19224025309085846
LOSS train 0.13275483435303984 valid 0.18435502549012503
LOSS train 0.13275483435303984 valid 0.1773161180317402
LOSS train 0.13275483435303984 valid 0.1726224273443222
LOSS train 0.13275483435303984 valid 0.1825059875845909
LOSS train 0.13275483435303984 valid 0.1932153297322137
LOSS train 0.13275483435303984 valid 0.19089153595268726
LOSS train 0.13275483435303984 valid 0.19023524887031978
LOSS train 0.13275483435303984 valid 0.1908413290977478
LOSS train 0.13275483435303984 valid 0.1898858682675795
LOSS train 0.13275483435303984 valid 0.19025798017779985
LOSS train 0.13275483435303984 valid 0.18994946548571953
LOSS train 0.13275483435303984 valid 0.18940120935440063
LOSS train 0.13275483435303984 valid 0.186922354499499
LOSS train 0.13275483435303984 valid 0.18685075361281633
LOSS train 0.13275483435303984 valid 0.18778730841243968
LOSS train 0.13275483435303984 valid 0.1867378842499521
LOSS train 0.13275483435303984 valid 0.18908377776020452
LOSS train 0.13275483435303984 valid 0.18876999691128732
LOSS train 0.13275483435303984 valid 0.18796612535204207
LOSS train 0.13275483435303984 valid 0.1865451681342992
LOSS train 0.13275483435303984 valid 0.18640892337197842
LOSS train 0.13275483435303984 valid 0.18645192993183932
LOSS train 0.13275483435303984 valid 0.18518985867500304
LOSS train 0.13275483435303984 valid 0.18494920948377022
LOSS train 0.13275483435303984 valid 0.18509516451093885
LOSS train 0.13275483435303984 valid 0.18482373920934542
LOSS train 0.13275483435303984 valid 0.18424830868326383
LOSS train 0.13275483435303984 valid 0.18473987728357316
LOSS train 0.13275483435303984 valid 0.18549350625084293
LOSS train 0.13275483435303984 valid 0.1846426036208868
LOSS train 0.13275483435303984 valid 0.18484644636963354
LOSS train 0.13275483435303984 valid 0.18430427812478123
LOSS train 0.13275483435303984 valid 0.1859033499445234
LOSS train 0.13275483435303984 valid 0.18567715461055437
LOSS train 0.13275483435303984 valid 0.1863514305772008
LOSS train 0.13275483435303984 valid 0.18678648338506096
LOSS train 0.13275483435303984 valid 0.18634407566143915
LOSS train 0.13275483435303984 valid 0.18601510785520076
LOSS train 0.13275483435303984 valid 0.18655261906181894
LOSS train 0.13275483435303984 valid 0.18682188930965604
LOSS train 0.13275483435303984 valid 0.1868713064942249
LOSS train 0.13275483435303984 valid 0.1873026740821925
LOSS train 0.13275483435303984 valid 0.18730012211534713
LOSS train 0.13275483435303984 valid 0.18760376844717108
LOSS train 0.13275483435303984 valid 0.18824887751264774
LOSS train 0.13275483435303984 valid 0.1881041865174969
LOSS train 0.13275483435303984 valid 0.18874391639719204
LOSS train 0.13275483435303984 valid 0.1882353714108467
LOSS train 0.13275483435303984 valid 0.1884184470948051
LOSS train 0.13275483435303984 valid 0.1879398630788693
LOSS train 0.13275483435303984 valid 0.18841413984883507
LOSS train 0.13275483435303984 valid 0.18852623165757568
LOSS train 0.13275483435303984 valid 0.18849628486416556
LOSS train 0.13275483435303984 valid 0.18770924396812916
LOSS train 0.13275483435303984 valid 0.18766501336766964
LOSS train 0.13275483435303984 valid 0.18748562423319654
LOSS train 0.13275483435303984 valid 0.18783416586407162
LOSS train 0.13275483435303984 valid 0.18776826361815135
LOSS train 0.13275483435303984 valid 0.18735117501899845
LOSS train 0.13275483435303984 valid 0.18788967449818889
LOSS train 0.13275483435303984 valid 0.18724225083040813
LOSS train 0.13275483435303984 valid 0.18816731381230056
LOSS train 0.13275483435303984 valid 0.18841360738644233
LOSS train 0.13275483435303984 valid 0.18821403903491568
LOSS train 0.13275483435303984 valid 0.1875883738941221
LOSS train 0.13275483435303984 valid 0.18761010183130994
LOSS train 0.13275483435303984 valid 0.18716141149617624
LOSS train 0.13275483435303984 valid 0.1876452128802027
LOSS train 0.13275483435303984 valid 0.1872824961450738
LOSS train 0.13275483435303984 valid 0.18752008055647215
LOSS train 0.13275483435303984 valid 0.18751977792341415
LOSS train 0.13275483435303984 valid 0.1875635228447012
LOSS train 0.13275483435303984 valid 0.18789634128411611
LOSS train 0.13275483435303984 valid 0.18828437496957026
LOSS train 0.13275483435303984 valid 0.18811786426352217
LOSS train 0.13275483435303984 valid 0.18793900368305352
LOSS train 0.13275483435303984 valid 0.18755160196672513
LOSS train 0.13275483435303984 valid 0.18692715372890234
LOSS train 0.13275483435303984 valid 0.18658992592935209
LOSS train 0.13275483435303984 valid 0.1870310729960116
LOSS train 0.13275483435303984 valid 0.18678204033030085
LOSS train 0.13275483435303984 valid 0.18676097389487994
LOSS train 0.13275483435303984 valid 0.1866300880908966
LOSS train 0.13275483435303984 valid 0.18640247420516126
LOSS train 0.13275483435303984 valid 0.18614593233870363
LOSS train 0.13275483435303984 valid 0.18584321083670313
LOSS train 0.13275483435303984 valid 0.18643584495849824
LOSS train 0.13275483435303984 valid 0.1864253126912647
LOSS train 0.13275483435303984 valid 0.18652512362370124
LOSS train 0.13275483435303984 valid 0.18649219737752623
LOSS train 0.13275483435303984 valid 0.18626576277517504
LOSS train 0.13275483435303984 valid 0.18647549428204271
LOSS train 0.13275483435303984 valid 0.1864037678429955
LOSS train 0.13275483435303984 valid 0.18649018245438734
LOSS train 0.13275483435303984 valid 0.18635193834599761
LOSS train 0.13275483435303984 valid 0.18656292436074237
LOSS train 0.13275483435303984 valid 0.18667095162049688
LOSS train 0.13275483435303984 valid 0.18674580201506616
LOSS train 0.13275483435303984 valid 0.18693193158890942
LOSS train 0.13275483435303984 valid 0.1871177627175462
LOSS train 0.13275483435303984 valid 0.1868326322546283
LOSS train 0.13275483435303984 valid 0.18698490296418852
LOSS train 0.13275483435303984 valid 0.1870618374574752
LOSS train 0.13275483435303984 valid 0.18750333350222065
LOSS train 0.13275483435303984 valid 0.1873991546508308
LOSS train 0.13275483435303984 valid 0.1875573403581425
LOSS train 0.13275483435303984 valid 0.1879194972165134
LOSS train 0.13275483435303984 valid 0.18803844478997317
LOSS train 0.13275483435303984 valid 0.18797368670369055
LOSS train 0.13275483435303984 valid 0.18785190196441753
LOSS train 0.13275483435303984 valid 0.1878963046896774
LOSS train 0.13275483435303984 valid 0.18818587469950057
LOSS train 0.13275483435303984 valid 0.18839397806188335
LOSS train 0.13275483435303984 valid 0.18858141914523882
LOSS train 0.13275483435303984 valid 0.1885417292260716
LOSS train 0.13275483435303984 valid 0.18816428921990475
LOSS train 0.13275483435303984 valid 0.18781363538333348
LOSS train 0.13275483435303984 valid 0.18747892441848915
LOSS train 0.13275483435303984 valid 0.18730945417211076
LOSS train 0.13275483435303984 valid 0.18735111688004166
LOSS train 0.13275483435303984 valid 0.1871392583943964
LOSS train 0.13275483435303984 valid 0.18746586732806697
LOSS train 0.13275483435303984 valid 0.1873153246641159
LOSS train 0.13275483435303984 valid 0.18771341265667052
LOSS train 0.13275483435303984 valid 0.18754504222100174
LOSS train 0.13275483435303984 valid 0.1875909409718588
LOSS train 0.13275483435303984 valid 0.18775045375029245
LOSS train 0.13275483435303984 valid 0.18739695835572023
LOSS train 0.13275483435303984 valid 0.1871023780971993
LOSS train 0.13275483435303984 valid 0.18677248105858313
LOSS train 0.13275483435303984 valid 0.186697690446574
LOSS train 0.13275483435303984 valid 0.1868315949591238
LOSS train 0.13275483435303984 valid 0.18671370446681976
LOSS train 0.13275483435303984 valid 0.1867016977904474
LOSS train 0.13275483435303984 valid 0.1863958160155011
LOSS train 0.13275483435303984 valid 0.18633766742288202
LOSS train 0.13275483435303984 valid 0.18628275608844894
LOSS train 0.13275483435303984 valid 0.1864025062748364
LOSS train 0.13275483435303984 valid 0.18625242934159353
LOSS train 0.13275483435303984 valid 0.18628150549992709
LOSS train 0.13275483435303984 valid 0.18619480718682696
LOSS train 0.13275483435303984 valid 0.18619225815766388
LOSS train 0.13275483435303984 valid 0.1860487059272569
LOSS train 0.13275483435303984 valid 0.18626497073532783
LOSS train 0.13275483435303984 valid 0.1861975554909025
LOSS train 0.13275483435303984 valid 0.18708740486889272
LOSS train 0.13275483435303984 valid 0.18721778350788476
LOSS train 0.13275483435303984 valid 0.18716553221146265
LOSS train 0.13275483435303984 valid 0.18736488406626595
LOSS train 0.13275483435303984 valid 0.18702478520572186
LOSS train 0.13275483435303984 valid 0.18698544624973745
LOSS train 0.13275483435303984 valid 0.18684378508236502
LOSS train 0.13275483435303984 valid 0.18675700474169946
LOSS train 0.13275483435303984 valid 0.18678955428111246
LOSS train 0.13275483435303984 valid 0.18687913884782487
LOSS train 0.13275483435303984 valid 0.18688917395812046
LOSS train 0.13275483435303984 valid 0.186952791030302
LOSS train 0.13275483435303984 valid 0.18689504135400056
LOSS train 0.13275483435303984 valid 0.1867544453891908
LOSS train 0.13275483435303984 valid 0.18662062480493827
LOSS train 0.13275483435303984 valid 0.18634518569598169
LOSS train 0.13275483435303984 valid 0.18617062843063983
LOSS train 0.13275483435303984 valid 0.18599965870380403
LOSS train 0.13275483435303984 valid 0.18608395934823047
LOSS train 0.13275483435303984 valid 0.1864058247761812
LOSS train 0.13275483435303984 valid 0.1863294911703893
LOSS train 0.13275483435303984 valid 0.1865163262955536
LOSS train 0.13275483435303984 valid 0.18652692919268327
LOSS train 0.13275483435303984 valid 0.18654233844656692
LOSS train 0.13275483435303984 valid 0.18641212106097577
LOSS train 0.13275483435303984 valid 0.1864106356236287
LOSS train 0.13275483435303984 valid 0.18635859734368052
LOSS train 0.13275483435303984 valid 0.18604136237076352
LOSS train 0.13275483435303984 valid 0.18599897944791752
LOSS train 0.13275483435303984 valid 0.1860805295281491
LOSS train 0.13275483435303984 valid 0.18632686891582576
LOSS train 0.13275483435303984 valid 0.18624183152640997
LOSS train 0.13275483435303984 valid 0.1861615753836102
LOSS train 0.13275483435303984 valid 0.18631104886202524
LOSS train 0.13275483435303984 valid 0.18615209057435886
LOSS train 0.13275483435303984 valid 0.18622889658792421
LOSS train 0.13275483435303984 valid 0.18610228006930454
LOSS train 0.13275483435303984 valid 0.18595349901431316
LOSS train 0.13275483435303984 valid 0.18598527797768194
LOSS train 0.13275483435303984 valid 0.18583629858047568
LOSS train 0.13275483435303984 valid 0.18581666852882567
LOSS train 0.13275483435303984 valid 0.18568850202219828
LOSS train 0.13275483435303984 valid 0.1857877613682496
LOSS train 0.13275483435303984 valid 0.18569457218909138
LOSS train 0.13275483435303984 valid 0.18576333105253676
LOSS train 0.13275483435303984 valid 0.18547790254335947
LOSS train 0.13275483435303984 valid 0.18537331088302061
LOSS train 0.13275483435303984 valid 0.1851772292302205
LOSS train 0.13275483435303984 valid 0.1851502073054411
LOSS train 0.13275483435303984 valid 0.185346424882182
LOSS train 0.13275483435303984 valid 0.18529332986082694
LOSS train 0.13275483435303984 valid 0.18541894209145302
LOSS train 0.13275483435303984 valid 0.18525860257446766
LOSS train 0.13275483435303984 valid 0.18505970229260363
LOSS train 0.13275483435303984 valid 0.18497571954042605
LOSS train 0.13275483435303984 valid 0.18496197440060488
LOSS train 0.13275483435303984 valid 0.18516105654485085
LOSS train 0.13275483435303984 valid 0.18493405834930698
LOSS train 0.13275483435303984 valid 0.18496645516851573
LOSS train 0.13275483435303984 valid 0.18490814932302577
LOSS train 0.13275483435303984 valid 0.18474950075436097
LOSS train 0.13275483435303984 valid 0.1847398160604769
LOSS train 0.13275483435303984 valid 0.18465457899229867
LOSS train 0.13275483435303984 valid 0.18466336389571006
LOSS train 0.13275483435303984 valid 0.18462348497419986
LOSS train 0.13275483435303984 valid 0.18459661486842824
LOSS train 0.13275483435303984 valid 0.18463209536031028
LOSS train 0.13275483435303984 valid 0.1845015724037969
LOSS train 0.13275483435303984 valid 0.18436949693218427
LOSS train 0.13275483435303984 valid 0.18431020229368167
LOSS train 0.13275483435303984 valid 0.1843491696299763
LOSS train 0.13275483435303984 valid 0.18445121044437635
LOSS train 0.13275483435303984 valid 0.18444198735735634
LOSS train 0.13275483435303984 valid 0.1843524470048792
LOSS train 0.13275483435303984 valid 0.18439092023952589
LOSS train 0.13275483435303984 valid 0.18455455321902117
LOSS train 0.13275483435303984 valid 0.18466335601572478
LOSS train 0.13275483435303984 valid 0.18481239206261105
LOSS train 0.13275483435303984 valid 0.18507458249815797
LOSS train 0.13275483435303984 valid 0.18514993360126597
LOSS train 0.13275483435303984 valid 0.185216011381463
LOSS train 0.13275483435303984 valid 0.18523255348465847
LOSS train 0.13275483435303984 valid 0.18527960550525915
LOSS train 0.13275483435303984 valid 0.1854245907816536
LOSS train 0.13275483435303984 valid 0.1854382398185031
LOSS train 0.13275483435303984 valid 0.18553691021618413
LOSS train 0.13275483435303984 valid 0.1855830347690827
LOSS train 0.13275483435303984 valid 0.18574067493702504
LOSS train 0.13275483435303984 valid 0.1856120723157616
LOSS train 0.13275483435303984 valid 0.18556618897975247
LOSS train 0.13275483435303984 valid 0.1856025555679778
LOSS train 0.13275483435303984 valid 0.18541080925504533
LOSS train 0.13275483435303984 valid 0.18540913766870895
LOSS train 0.13275483435303984 valid 0.18559905750622888
LOSS train 0.13275483435303984 valid 0.1854405492421024
LOSS train 0.13275483435303984 valid 0.1856399312186143
LOSS train 0.13275483435303984 valid 0.18582426225308513
LOSS train 0.13275483435303984 valid 0.1858872734162272
LOSS train 0.13275483435303984 valid 0.18573933579330523
LOSS train 0.13275483435303984 valid 0.18590054881234883
LOSS train 0.13275483435303984 valid 0.1858516425615357
LOSS train 0.13275483435303984 valid 0.18586036646701246
LOSS train 0.13275483435303984 valid 0.18590898942947387
LOSS train 0.13275483435303984 valid 0.1857466383046838
LOSS train 0.13275483435303984 valid 0.18601174380571123
LOSS train 0.13275483435303984 valid 0.18595723354298135
LOSS train 0.13275483435303984 valid 0.1858474602262805
LOSS train 0.13275483435303984 valid 0.18589513103167216
LOSS train 0.13275483435303984 valid 0.18597521260380745
LOSS train 0.13275483435303984 valid 0.18579128164261696
LOSS train 0.13275483435303984 valid 0.18603532512982687
LOSS train 0.13275483435303984 valid 0.18604049730945277
LOSS train 0.13275483435303984 valid 0.18589736028359485
LOSS train 0.13275483435303984 valid 0.18605436732942573
LOSS train 0.13275483435303984 valid 0.1861181230717943
LOSS train 0.13275483435303984 valid 0.18615763144348058
LOSS train 0.13275483435303984 valid 0.1862534015919223
LOSS train 0.13275483435303984 valid 0.1861923225645749
LOSS train 0.13275483435303984 valid 0.18621997255131714
LOSS train 0.13275483435303984 valid 0.18626405795415243
LOSS train 0.13275483435303984 valid 0.186462721495486
LOSS train 0.13275483435303984 valid 0.18655337299112937
LOSS train 0.13275483435303984 valid 0.18655037350124784
LOSS train 0.13275483435303984 valid 0.18662998949029788
LOSS train 0.13275483435303984 valid 0.1869322697029394
LOSS train 0.13275483435303984 valid 0.18714409062277265
LOSS train 0.13275483435303984 valid 0.18717559008267673
LOSS train 0.13275483435303984 valid 0.1871644815531644
LOSS train 0.13275483435303984 valid 0.187079974166725
LOSS train 0.13275483435303984 valid 0.18694969785772936
LOSS train 0.13275483435303984 valid 0.18679455785657004
LOSS train 0.13275483435303984 valid 0.18677998358203518
LOSS train 0.13275483435303984 valid 0.18677448566470828
LOSS train 0.13275483435303984 valid 0.18668938473238214
LOSS train 0.13275483435303984 valid 0.18639631056827857
LOSS train 0.13275483435303984 valid 0.18632251255925047
LOSS train 0.13275483435303984 valid 0.1863349636780544
LOSS train 0.13275483435303984 valid 0.18637427394850212
LOSS train 0.13275483435303984 valid 0.1863693605873968
LOSS train 0.13275483435303984 valid 0.1863259297214734
LOSS train 0.13275483435303984 valid 0.18629812438868815
LOSS train 0.13275483435303984 valid 0.1863134924618843
LOSS train 0.13275483435303984 valid 0.18634314696336615
LOSS train 0.13275483435303984 valid 0.18618659818500177
LOSS train 0.13275483435303984 valid 0.1861822367326854
LOSS train 0.13275483435303984 valid 0.1861998053337527
LOSS train 0.13275483435303984 valid 0.18628639281809736
LOSS train 0.13275483435303984 valid 0.18637300098346452
LOSS train 0.13275483435303984 valid 0.18629472959484603
LOSS train 0.13275483435303984 valid 0.18637853354115277
LOSS train 0.13275483435303984 valid 0.18640969278628394
LOSS train 0.13275483435303984 valid 0.18645456702414165
LOSS train 0.13275483435303984 valid 0.18653887396057448
LOSS train 0.13275483435303984 valid 0.18649345205669784
LOSS train 0.13275483435303984 valid 0.18643840688545973
LOSS train 0.13275483435303984 valid 0.1864961062130755
LOSS train 0.13275483435303984 valid 0.18649788990028596
LOSS train 0.13275483435303984 valid 0.1864255460070782
LOSS train 0.13275483435303984 valid 0.18645696419905994
LOSS train 0.13275483435303984 valid 0.18643924433750128
LOSS train 0.13275483435303984 valid 0.1863430705550429
LOSS train 0.13275483435303984 valid 0.18635132826840609
LOSS train 0.13275483435303984 valid 0.18640693690507643
LOSS train 0.13275483435303984 valid 0.1864039644934357
LOSS train 0.13275483435303984 valid 0.18637768632899493
LOSS train 0.13275483435303984 valid 0.18656209882455893
LOSS train 0.13275483435303984 valid 0.18657653489310272
LOSS train 0.13275483435303984 valid 0.1864955131496702
LOSS train 0.13275483435303984 valid 0.1864487732984597
LOSS train 0.13275483435303984 valid 0.18651147670926355
LOSS train 0.13275483435303984 valid 0.1865381842607972
LOSS train 0.13275483435303984 valid 0.1866872867913829
LOSS train 0.13275483435303984 valid 0.18662061942741276
LOSS train 0.13275483435303984 valid 0.18678986432024994
LOSS train 0.13275483435303984 valid 0.1867702796777583
LOSS train 0.13275483435303984 valid 0.18673243066843817
LOSS train 0.13275483435303984 valid 0.18684685754555244
LOSS train 0.13275483435303984 valid 0.18684630568210894
LOSS train 0.13275483435303984 valid 0.18700822438199097
LOSS train 0.13275483435303984 valid 0.18705867654686675
LOSS train 0.13275483435303984 valid 0.18703129210668365
LOSS train 0.13275483435303984 valid 0.18709048266468803
LOSS train 0.13275483435303984 valid 0.18708606773253644
LOSS train 0.13275483435303984 valid 0.18697604856642352
LOSS train 0.13275483435303984 valid 0.1868615861847458
LOSS train 0.13275483435303984 valid 0.18691580104935276
LOSS train 0.13275483435303984 valid 0.18700408168181687
LOSS train 0.13275483435303984 valid 0.1869521053424522
LOSS train 0.13275483435303984 valid 0.18702599836424702
LOSS train 0.13275483435303984 valid 0.18701577332500888
LOSS train 0.13275483435303984 valid 0.18700928064669378
LOSS train 0.13275483435303984 valid 0.18701873842769667
LOSS train 0.13275483435303984 valid 0.1869631930747453
LOSS train 0.13275483435303984 valid 0.18680741657609465
LOSS train 0.13275483435303984 valid 0.18681730587057202
LOSS train 0.13275483435303984 valid 0.18685719409941237
LOSS train 0.13275483435303984 valid 0.18707897752350153
LOSS train 0.13275483435303984 valid 0.187181390584379
LOSS train 0.13275483435303984 valid 0.18726759266129808
LOSS train 0.13275483435303984 valid 0.18711956412201314
LOSS train 0.13275483435303984 valid 0.1870483243516807
LOSS train 0.13275483435303984 valid 0.1870381002952171
LOSS train 0.13275483435303984 valid 0.1869680363365582
LOSS train 0.13275483435303984 valid 0.18688370431760098
LOSS train 0.13275483435303984 valid 0.18687066016718745
LOSS train 0.13275483435303984 valid 0.18687817002659818
LOSS train 0.13275483435303984 valid 0.18690554957605351
LOSS train 0.13275483435303984 valid 0.1869702804676244
LOSS train 0.13275483435303984 valid 0.186981281286545
LOSS train 0.13275483435303984 valid 0.1870204459051458
LOSS train 0.13275483435303984 valid 0.18691883889656494
LOSS train 0.13275483435303984 valid 0.18692772509659897
LOSS train 0.13275483435303984 valid 0.18687794100907112
LOSS train 0.13275483435303984 valid 0.186830195115874
LOSS train 0.13275483435303984 valid 0.18688545483087307
LOSS train 0.13275483435303984 valid 0.18688225865199876
LOSS train 0.13275483435303984 valid 0.18686540420730036
LOSS train 0.13275483435303984 valid 0.18692009191807002
LOSS train 0.13275483435303984 valid 0.18694507094517432
LOSS train 0.13275483435303984 valid 0.18684819450332943
LOSS train 0.13275483435303984 valid 0.18688506556107945
LOSS train 0.13275483435303984 valid 0.18695843070341642
EPOCH 10:
  batch 1 loss: 0.11629799008369446
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.11809265986084938
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.11642878750960033
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.124229796230793
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.1275111585855484
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.12926595409711203
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.12377977796963283
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.12529819458723068
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.12427939474582672
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.12382330447435379
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.1220764395865527
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.1212165014197429
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.1217046924508535
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.12131709126489502
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.12020793259143829
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.12188529316335917
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.1209171225919443
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.12393022535575761
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.12387618697003315
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.12339108549058438
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.12380818348555338
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.12443414296616208
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.12478343073440634
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.12648729452242455
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.1257845064997673
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.12618809222028807
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.12646354652113384
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.12597878064428056
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.125652644140967
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.1263351892431577
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.1267581731081009
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.12717628572136164
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.1265138407999819
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.12739472989650333
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.1290532505937985
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.12832649445368183
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.12825353665126338
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.12795514495749222
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.12831324415329176
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.12835577316582203
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.128368409668527
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.12851704337767192
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.129561705298202
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.12897987308149989
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.12917460054159163
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.12870708626249563
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.12847596216709056
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.1285379584878683
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.12850104485239303
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.1285659372806549
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.12816516675201117
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.12832521905119604
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.12814993059860086
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.1280704273117913
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.12851357053626666
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.12864025788647787
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.1286742804866088
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.1281326497937071
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.12852138988042283
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.12830848606924217
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.1284821555018425
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.12835899199689588
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.12853096129875335
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.12833901832345873
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.12846032002797494
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.12827345841761792
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.1280714244540058
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.12829369328477802
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.12851007563480432
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.12882313302585058
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.12918932987770565
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.12940248909095922
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.1294654462843725
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.12952172091683825
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.12958692689736684
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.12951762386058507
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.1294863572368374
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.12919697461602014
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.12956478654206555
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.12982937125489116
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.12990299262750296
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.12950243964427854
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.12933655370430774
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.1296957532564799
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.12979375334346996
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.13011052857997807
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.1303644449203864
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.1307509666816755
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.13074085183357925
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.13109055774079428
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.13128418549076543
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.13119540144891842
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.13131065562527666
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.13123261952336798
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.13170145099100314
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.1317695741696904
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.13152512109156736
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.13132543998713397
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.13169016275140974
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.13156953811645508
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.13141101650377313
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.13129755284856348
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.13108501742476397
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.13120825509898937
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.1308042280021168
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.1308487314238863
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.13107542547388612
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.13085399823332275
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.1309699597298552
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.13118211444128644
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.13092765306030307
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.13089688481496914
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.13063772642506963
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.1309604648696749
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.1307254187438799
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.13060774011858578
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.13094024118195233
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.1309319593138614
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.13082001564883383
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.13077482680479685
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.13076521148366377
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.1308331833999665
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.13100747012995123
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.13093722191068433
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.13098871552944183
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.13107750020802966
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.131119403548128
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.13087414822075516
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.13080939442612405
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.13081832253015957
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.13071064275639657
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.1306640716326056
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.13098481964123876
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.13109753419881437
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.13088198409036353
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.1308129646212739
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.13095176279762366
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.1309332855693672
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.13118556500982037
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.13121919466980866
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.13120782898461564
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.1310645421101174
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.1311935614143218
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.13134982348937127
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.13122383289296052
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.13128582781104192
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.1313120464788002
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.13127853978123213
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.13162274333654633
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.1316883920133114
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.13155702087066032
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.13165432200031846
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.1316261485803361
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.13131598204180792
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.13136092688768142
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.13131050913570783
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.1314235279799267
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.13135307678316213
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.13149136567265732
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.13164387717843057
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.1317813384421864
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.13168867042771093
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.13170681610429213
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.13193524029196763
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.1320837555509625
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.13200094169342375
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.13185649980863412
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.13180010211432264
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.1316152431908444
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.13188410909736858
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.13192391012147156
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.13192739874817605
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.13171736344781226
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.13164905073313876
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.13177593682493483
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.1315803325057707
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.13139337297044906
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.13110103713495008
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.1312944001242435
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.1311075867050224
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.13117821868611962
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.13114821026613424
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.13104288389936822
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.13096742125471
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.13100846711848232
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.13094587604044586
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.13103996834812318
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.13094733611858905
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.13090965344950004
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.13095099726005605
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.1309654702924933
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.13088249717839062
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.13089317075637955
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.13063633568661728
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.13066419191085377
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.13081719448827966
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.13089759023812822
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.13107095933472268
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.130995028731811
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.13115973368287087
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.13118599844512654
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.13124896711347125
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.1312799826631405
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.13132856216500788
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.13127580572192263
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.13115941396760708
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.13111526634238194
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.13093874513959655
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.13103917986154556
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.1311639610145773
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.13117211015444796
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.1311891834901751
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.1312803319292449
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.13106354148449184
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.13114340509786163
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.1310475812190109
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.13100842207944888
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.13108069186500454
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.13114078856630412
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.13115922453051262
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.1311470397255
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.1311720870435238
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.1311234096106927
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.13119465445301362
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.13109009272522396
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.1311115416805301
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.13122462287610848
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.1310670184330982
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.1310069577272282
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.13124927133321762
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.1314752952102975
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.13137428982761398
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.13143057858995102
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.13143156570756537
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.13143055876518817
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.1314505828007803
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.13139757438551022
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.13143330912630097
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.1314288276758154
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.1313299068870644
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.13146832322554963
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.13142098064634425
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.13148150629835365
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.131501936613292
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.13149561605283192
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.1315492527574543
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.1315486481496197
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.13160985777334822
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.13167076713469134
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.13163706356287003
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.13183736118424935
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.13186522602798448
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.13190253308639224
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.13187302869018608
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.13183007447742948
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.13183212382136844
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.13175882112075382
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.1316918121116568
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.1317476691483991
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.1317872293580037
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.13169681554895707
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.13162717533134322
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.13158958824528488
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.13144947139715607
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.13145831556252713
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.1314850419591692
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.13143976819649172
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.1314445667858444
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.13136731107549596
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.13138958858671013
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.13138905775173124
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.131345189149108
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.1314615561684846
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.1313387214811179
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.13122712446884677
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.13142042107664156
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.13138901267456235
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.1313942429080284
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.13135130102595977
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.13135425115802457
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.1313214950608189
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.13126776737312898
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.1312613657843519
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.13124850114256564
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.13137131484976985
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.13147033745800699
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.13138195524439994
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.1313605289388862
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.13129998844078256
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.13132574494028915
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.13133808015445664
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.13147410839694004
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.13164103303244495
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.1316707166431307
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.13163295575117662
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.1316437740885728
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.1316402639303143
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.1315930886256615
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.13153970428434103
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.131516506994764
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.13154154988717398
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.13154397670500326
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.13153073258034073
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.13149449322372675
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.1314624210605856
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.13148179462824772
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.13153540255677817
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.13150384122288072
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.13146457594478786
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.13137949024957996
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.13143966380518732
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.1313994116603564
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.13135749284927836
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.13135592105567076
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.13137188135158448
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.13149482754495326
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.13147750621432386
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.13146890528828092
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.13138612868924127
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.13147165400441735
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.1314344954592788
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.13137129335577444
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.13138092498384393
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.13128329689304033
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.1312813282929934
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.13119655758416726
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.13108991749516322
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.13109986845222188
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.1311500683245688
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.13120756562460553
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.13122444401784966
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.1312325000089694
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.13116204036069704
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.13118486333928422
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.13125254445111573
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.1312746171883884
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.13124906552474647
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.13124516006757522
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.13115896242821815
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.131151306957883
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.13115897405444701
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.13141385415754123
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.13145283653072296
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.1314813228497325
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.13148948336425034
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.1314570559457892
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.13145042739014118
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.13144307752707224
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.13138516593116062
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.13130610144564084
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.13130902923495003
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.13138926149853927
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.13147127681092569
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.1314995177090168
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.1314939246421129
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.13139855424256136
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.13146720473923268
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.13153192119974663
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.13145651641330347
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.13140287146800095
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.13137386045792757
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.13144659650259913
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.13142437851133426
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.1313792614286745
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.13142133381677001
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.13141871343463496
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.13149116346637948
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.1315866622304463
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.13154950231152177
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.1314806002016003
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.13153263060953418
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.1315135034702478
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.13153041146156297
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.13160282176207094
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.13163941142956415
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.13162731020612287
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.131617624282995
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.13168136998222618
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.13162368946858635
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.1316160618670677
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.13159708240682996
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.13153948595854625
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.13163053880472408
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.13163905579131097
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.13155947329549048
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.13151509027175334
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.1315491305572734
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.13149637511940962
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.13148055225610733
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.13141627401495592
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.1314027588576307
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.13140331385467124
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.13153480385778515
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.13155531143854718
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.13157996846527992
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.13153159296648068
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.1314383597870918
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.13148157970614768
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.1314602088472598
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.1314247332327068
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.1314256278022269
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.1313937975288327
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.13134877041684784
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.13138875153173904
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.1314242058146147
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.13138687217910888
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.13141006216663106
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.1314598917595896
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.13142167558224102
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.13141592790077372
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.1313918980513756
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.13144844148344206
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.13146738816260137
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.13152607310797282
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.13158351753849581
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.1316612598199684
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.131645215036486
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.13160278708740855
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.13164031868333748
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.13159575423314457
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.13164491862978903
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.13168061612906615
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.13160432075495979
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.1315883347448313
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.13156317542580998
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.13146561312493585
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.13147325881820093
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.13144019254805328
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.13135990910318904
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.13128687763629956
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.13133594625372344
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.1312920302842502
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.1312925440472213
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.13134895405873726
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.1313114837668408
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.13141282446203975
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.13146520823717664
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.13143461194212577
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.13132744199322677
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.13131551712074063
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.13132967102149176
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.13128252165619606
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.13125673052486933
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.13122821579108368
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.13122940903969024
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.13123619409420031
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.13120711661491885
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.13126845835774606
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.13125618819338175
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.1312411367230945
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.1312042478050731
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.13125507025855832
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.13119938714680557
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.1312021741947151
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.13115810547228698
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.13115197977279885
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.13116257168205875
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.1311818865416612
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.13118469557666051
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.13125984349652478
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.13125051268721868
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.13123013282364066
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.13116492379303885
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.13118307944387197
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.1311915156661823
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.13120338758813466
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.1312215352147754
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.1312982436810803
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.1313463359562827
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.13139244172167272
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.13143714920730348
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.13153212839516543
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.13153212839516543 valid 0.21409043669700623
LOSS train 0.13153212839516543 valid 0.19010621309280396
LOSS train 0.13153212839516543 valid 0.18255946536858877
LOSS train 0.13153212839516543 valid 0.1748017556965351
LOSS train 0.13153212839516543 valid 0.1702544391155243
LOSS train 0.13153212839516543 valid 0.18018326411644617
LOSS train 0.13153212839516543 valid 0.19050984936101095
LOSS train 0.13153212839516543 valid 0.18833491951227188
LOSS train 0.13153212839516543 valid 0.18774104780620998
LOSS train 0.13153212839516543 valid 0.18826209604740143
LOSS train 0.13153212839516543 valid 0.1872771206227216
LOSS train 0.13153212839516543 valid 0.1876184418797493
LOSS train 0.13153212839516543 valid 0.1872792931703421
LOSS train 0.13153212839516543 valid 0.1867730479155268
LOSS train 0.13153212839516543 valid 0.18441109855969748
LOSS train 0.13153212839516543 valid 0.1843085316941142
LOSS train 0.13153212839516543 valid 0.18515951493207147
LOSS train 0.13153212839516543 valid 0.1841096729040146
LOSS train 0.13153212839516543 valid 0.1864571147843411
LOSS train 0.13153212839516543 valid 0.18617720007896424
LOSS train 0.13153212839516543 valid 0.18532565519923255
LOSS train 0.13153212839516543 valid 0.18389577892693607
LOSS train 0.13153212839516543 valid 0.18370875392271124
LOSS train 0.13153212839516543 valid 0.18377865540484586
LOSS train 0.13153212839516543 valid 0.18252279341220856
LOSS train 0.13153212839516543 valid 0.1823240936948703
LOSS train 0.13153212839516543 valid 0.18252446364473413
LOSS train 0.13153212839516543 valid 0.18223028576799802
LOSS train 0.13153212839516543 valid 0.18164902565808133
LOSS train 0.13153212839516543 valid 0.18214161396026612
LOSS train 0.13153212839516543 valid 0.18295371244030614
LOSS train 0.13153212839516543 valid 0.18212705943733454
LOSS train 0.13153212839516543 valid 0.18229899261936997
LOSS train 0.13153212839516543 valid 0.18179157376289368
LOSS train 0.13153212839516543 valid 0.18329570421150754
LOSS train 0.13153212839516543 valid 0.18308160909348065
LOSS train 0.13153212839516543 valid 0.18375048484351184
LOSS train 0.13153212839516543 valid 0.18417600856015556
LOSS train 0.13153212839516543 valid 0.18377594879040351
LOSS train 0.13153212839516543 valid 0.18343920670449734
LOSS train 0.13153212839516543 valid 0.1839717877347295
LOSS train 0.13153212839516543 valid 0.18422700854994
LOSS train 0.13153212839516543 valid 0.18426430329333904
LOSS train 0.13153212839516543 valid 0.18471358140761202
LOSS train 0.13153212839516543 valid 0.18471902410189311
LOSS train 0.13153212839516543 valid 0.1850486747596575
LOSS train 0.13153212839516543 valid 0.1856871247291565
LOSS train 0.13153212839516543 valid 0.18554253410547972
LOSS train 0.13153212839516543 valid 0.18616981074518088
LOSS train 0.13153212839516543 valid 0.1856771832704544
LOSS train 0.13153212839516543 valid 0.1858448041420357
LOSS train 0.13153212839516543 valid 0.18536707730247423
LOSS train 0.13153212839516543 valid 0.18587224522851548
LOSS train 0.13153212839516543 valid 0.1859993815973953
LOSS train 0.13153212839516543 valid 0.185968632589687
LOSS train 0.13153212839516543 valid 0.18520263715514115
LOSS train 0.13153212839516543 valid 0.18516314709395693
LOSS train 0.13153212839516543 valid 0.1849864256279222
LOSS train 0.13153212839516543 valid 0.18533234146691985
LOSS train 0.13153212839516543 valid 0.18522723739345867
LOSS train 0.13153212839516543 valid 0.1848182763721122
LOSS train 0.13153212839516543 valid 0.18533720100118267
LOSS train 0.13153212839516543 valid 0.1846992371101228
LOSS train 0.13153212839516543 valid 0.18561026407405734
LOSS train 0.13153212839516543 valid 0.18584066721109244
LOSS train 0.13153212839516543 valid 0.18563333785895145
LOSS train 0.13153212839516543 valid 0.18503521360568145
LOSS train 0.13153212839516543 valid 0.18503895336214235
LOSS train 0.13153212839516543 valid 0.18460364229437234
LOSS train 0.13153212839516543 valid 0.1851073058588164
LOSS train 0.13153212839516543 valid 0.18475883905316742
LOSS train 0.13153212839516543 valid 0.18497199772132766
LOSS train 0.13153212839516543 valid 0.18497500987085577
LOSS train 0.13153212839516543 valid 0.18499824465126605
LOSS train 0.13153212839516543 valid 0.1853184022506078
LOSS train 0.13153212839516543 valid 0.18570565098994657
LOSS train 0.13153212839516543 valid 0.18556117372853415
LOSS train 0.13153212839516543 valid 0.1853793483131971
LOSS train 0.13153212839516543 valid 0.18500340531898452
LOSS train 0.13153212839516543 valid 0.18437738493084907
LOSS train 0.13153212839516543 valid 0.18405341292604988
LOSS train 0.13153212839516543 valid 0.18450033682875516
LOSS train 0.13153212839516543 valid 0.18428049281419043
LOSS train 0.13153212839516543 valid 0.18428014626815206
LOSS train 0.13153212839516543 valid 0.18412544516956106
LOSS train 0.13153212839516543 valid 0.18391642840795738
LOSS train 0.13153212839516543 valid 0.1836751457946054
LOSS train 0.13153212839516543 valid 0.18338350782340224
LOSS train 0.13153212839516543 valid 0.18394828445456002
LOSS train 0.13153212839516543 valid 0.18393551806608835
LOSS train 0.13153212839516543 valid 0.18403760057229263
LOSS train 0.13153212839516543 valid 0.18400247880946036
LOSS train 0.13153212839516543 valid 0.18378443455183377
LOSS train 0.13153212839516543 valid 0.18397903252155223
LOSS train 0.13153212839516543 valid 0.18391282119249042
LOSS train 0.13153212839516543 valid 0.1839952397470673
LOSS train 0.13153212839516543 valid 0.1838469517599676
LOSS train 0.13153212839516543 valid 0.18405110890768012
LOSS train 0.13153212839516543 valid 0.18415786340983228
LOSS train 0.13153212839516543 valid 0.1842350846529007
LOSS train 0.13153212839516543 valid 0.18442040194969367
LOSS train 0.13153212839516543 valid 0.18461308832846435
LOSS train 0.13153212839516543 valid 0.1843247002768285
LOSS train 0.13153212839516543 valid 0.18447937386540267
LOSS train 0.13153212839516543 valid 0.1845560113588969
LOSS train 0.13153212839516543 valid 0.18499921782399123
LOSS train 0.13153212839516543 valid 0.18489457541537063
LOSS train 0.13153212839516543 valid 0.18505179012815157
LOSS train 0.13153212839516543 valid 0.18541570992097942
LOSS train 0.13153212839516543 valid 0.1855220991102132
LOSS train 0.13153212839516543 valid 0.1854716120539485
LOSS train 0.13153212839516543 valid 0.18534140860927956
LOSS train 0.13153212839516543 valid 0.18538552918265352
LOSS train 0.13153212839516543 valid 0.18568626229177443
LOSS train 0.13153212839516543 valid 0.185892261111218
LOSS train 0.13153212839516543 valid 0.18607581869281573
LOSS train 0.13153212839516543 valid 0.18603659733238384
LOSS train 0.13153212839516543 valid 0.18566078541137404
LOSS train 0.13153212839516543 valid 0.1853144990295923
LOSS train 0.13153212839516543 valid 0.18498801216483116
LOSS train 0.13153212839516543 valid 0.18482821921179118
LOSS train 0.13153212839516543 valid 0.1848739614007903
LOSS train 0.13153212839516543 valid 0.18466861109908034
LOSS train 0.13153212839516543 valid 0.1849783553231147
LOSS train 0.13153212839516543 valid 0.1848232330083847
LOSS train 0.13153212839516543 valid 0.1852171104105692
LOSS train 0.13153212839516543 valid 0.18505581849672664
LOSS train 0.13153212839516543 valid 0.18510860402602702
LOSS train 0.13153212839516543 valid 0.18526561073092526
LOSS train 0.13153212839516543 valid 0.18490609939281757
LOSS train 0.13153212839516543 valid 0.18461203870882514
LOSS train 0.13153212839516543 valid 0.1842845497709332
LOSS train 0.13153212839516543 valid 0.1841963660672195
LOSS train 0.13153212839516543 valid 0.18432783246485154
LOSS train 0.13153212839516543 valid 0.1842186349409598
LOSS train 0.13153212839516543 valid 0.18421108659137697
LOSS train 0.13153212839516543 valid 0.183906203965201
LOSS train 0.13153212839516543 valid 0.1838464044790337
LOSS train 0.13153212839516543 valid 0.18379116326356107
LOSS train 0.13153212839516543 valid 0.18388853722384998
LOSS train 0.13153212839516543 valid 0.1837402714271072
LOSS train 0.13153212839516543 valid 0.18376085718332882
LOSS train 0.13153212839516543 valid 0.1836666515865526
LOSS train 0.13153212839516543 valid 0.18366621217379966
LOSS train 0.13153212839516543 valid 0.18353742319962074
LOSS train 0.13153212839516543 valid 0.18375615132589862
LOSS train 0.13153212839516543 valid 0.18368566482245516
LOSS train 0.13153212839516543 valid 0.1845755081724476
LOSS train 0.13153212839516543 valid 0.18470827005053528
LOSS train 0.13153212839516543 valid 0.1846568489074707
LOSS train 0.13153212839516543 valid 0.18486739862833593
LOSS train 0.13153212839516543 valid 0.18453080422784152
LOSS train 0.13153212839516543 valid 0.18448855927567076
LOSS train 0.13153212839516543 valid 0.18435121472779806
LOSS train 0.13153212839516543 valid 0.18425943313106413
LOSS train 0.13153212839516543 valid 0.18429133267356798
LOSS train 0.13153212839516543 valid 0.18438311661504636
LOSS train 0.13153212839516543 valid 0.184389272539676
LOSS train 0.13153212839516543 valid 0.18445547759158057
LOSS train 0.13153212839516543 valid 0.18439533989876508
LOSS train 0.13153212839516543 valid 0.18425987022263662
LOSS train 0.13153212839516543 valid 0.184130236874392
LOSS train 0.13153212839516543 valid 0.1838589445944944
LOSS train 0.13153212839516543 valid 0.1836876227724843
LOSS train 0.13153212839516543 valid 0.18351729503183653
LOSS train 0.13153212839516543 valid 0.1835916062974068
LOSS train 0.13153212839516543 valid 0.18391148148182623
LOSS train 0.13153212839516543 valid 0.18383521329434144
LOSS train 0.13153212839516543 valid 0.1840192231200856
LOSS train 0.13153212839516543 valid 0.18402635165873696
LOSS train 0.13153212839516543 valid 0.1840403058201249
LOSS train 0.13153212839516543 valid 0.1839119903743267
LOSS train 0.13153212839516543 valid 0.18390801772905913
LOSS train 0.13153212839516543 valid 0.18386418372392654
LOSS train 0.13153212839516543 valid 0.1835455971956253
LOSS train 0.13153212839516543 valid 0.18350355454127898
LOSS train 0.13153212839516543 valid 0.18359234188236084
LOSS train 0.13153212839516543 valid 0.1838346649421735
LOSS train 0.13153212839516543 valid 0.18374913631204787
LOSS train 0.13153212839516543 valid 0.18366651584704716
LOSS train 0.13153212839516543 valid 0.18381021892168245
LOSS train 0.13153212839516543 valid 0.1836494362943775
LOSS train 0.13153212839516543 valid 0.1837255313096802
LOSS train 0.13153212839516543 valid 0.18359823913677878
LOSS train 0.13153212839516543 valid 0.18345445555609627
LOSS train 0.13153212839516543 valid 0.18348821797358092
LOSS train 0.13153212839516543 valid 0.18334234517844603
LOSS train 0.13153212839516543 valid 0.18332884516170683
LOSS train 0.13153212839516543 valid 0.1832007906108937
LOSS train 0.13153212839516543 valid 0.18329489348750366
LOSS train 0.13153212839516543 valid 0.18319563005919232
LOSS train 0.13153212839516543 valid 0.18326071471286318
LOSS train 0.13153212839516543 valid 0.18297939597016172
LOSS train 0.13153212839516543 valid 0.1828825427699335
LOSS train 0.13153212839516543 valid 0.18268836262898566
LOSS train 0.13153212839516543 valid 0.18265918034071826
LOSS train 0.13153212839516543 valid 0.18284718397305097
LOSS train 0.13153212839516543 valid 0.18279933312324562
LOSS train 0.13153212839516543 valid 0.18292565426634783
LOSS train 0.13153212839516543 valid 0.18276868581771852
LOSS train 0.13153212839516543 valid 0.18257863189450543
LOSS train 0.13153212839516543 valid 0.1825037325667863
LOSS train 0.13153212839516543 valid 0.18249595958023823
LOSS train 0.13153212839516543 valid 0.18269610171224557
LOSS train 0.13153212839516543 valid 0.18246899261707214
LOSS train 0.13153212839516543 valid 0.18250067821405466
LOSS train 0.13153212839516543 valid 0.18244131766079705
LOSS train 0.13153212839516543 valid 0.18228205515501592
LOSS train 0.13153212839516543 valid 0.18227649968491788
LOSS train 0.13153212839516543 valid 0.1821956613234111
LOSS train 0.13153212839516543 valid 0.1821984864806677
LOSS train 0.13153212839516543 valid 0.18215796456865543
LOSS train 0.13153212839516543 valid 0.18212264174586731
LOSS train 0.13153212839516543 valid 0.18215747247232456
LOSS train 0.13153212839516543 valid 0.1820316637671271
LOSS train 0.13153212839516543 valid 0.18189656941427124
LOSS train 0.13153212839516543 valid 0.18183166564609599
LOSS train 0.13153212839516543 valid 0.18186541523681868
LOSS train 0.13153212839516543 valid 0.181962057335736
LOSS train 0.13153212839516543 valid 0.18194664019075307
LOSS train 0.13153212839516543 valid 0.181858702550107
LOSS train 0.13153212839516543 valid 0.18189826681538745
LOSS train 0.13153212839516543 valid 0.18206662607834478
LOSS train 0.13153212839516543 valid 0.18216891094510043
LOSS train 0.13153212839516543 valid 0.18232123176256815
LOSS train 0.13153212839516543 valid 0.1825794466828878
LOSS train 0.13153212839516543 valid 0.1826481935342503
LOSS train 0.13153212839516543 valid 0.18271318234895406
LOSS train 0.13153212839516543 valid 0.18272252868877228
LOSS train 0.13153212839516543 valid 0.1827748668582543
LOSS train 0.13153212839516543 valid 0.18291982937426793
LOSS train 0.13153212839516543 valid 0.18293785400174814
LOSS train 0.13153212839516543 valid 0.18303406667044234
LOSS train 0.13153212839516543 valid 0.18308432120033818
LOSS train 0.13153212839516543 valid 0.18324127051424474
LOSS train 0.13153212839516543 valid 0.1831179580067174
LOSS train 0.13153212839516543 valid 0.18306854700488884
LOSS train 0.13153212839516543 valid 0.18310249659444103
LOSS train 0.13153212839516543 valid 0.1829105866254623
LOSS train 0.13153212839516543 valid 0.1829125774403413
LOSS train 0.13153212839516543 valid 0.18310001851117463
LOSS train 0.13153212839516543 valid 0.18294081818466343
LOSS train 0.13153212839516543 valid 0.1831470132487301
LOSS train 0.13153212839516543 valid 0.18333259023359563
LOSS train 0.13153212839516543 valid 0.18339206253995702
LOSS train 0.13153212839516543 valid 0.18324404718672357
LOSS train 0.13153212839516543 valid 0.18340418092634997
LOSS train 0.13153212839516543 valid 0.1833511260488341
LOSS train 0.13153212839516543 valid 0.1833586828655986
LOSS train 0.13153212839516543 valid 0.1833984135389328
LOSS train 0.13153212839516543 valid 0.18323811348453461
LOSS train 0.13153212839516543 valid 0.18350109830498695
LOSS train 0.13153212839516543 valid 0.1834456772201146
LOSS train 0.13153212839516543 valid 0.18333508807608462
LOSS train 0.13153212839516543 valid 0.1833828864144344
LOSS train 0.13153212839516543 valid 0.1834603127790615
LOSS train 0.13153212839516543 valid 0.18327676519345681
LOSS train 0.13153212839516543 valid 0.18352035126944846
LOSS train 0.13153212839516543 valid 0.18352705827090732
LOSS train 0.13153212839516543 valid 0.1833805402310995
LOSS train 0.13153212839516543 valid 0.1835339793871189
LOSS train 0.13153212839516543 valid 0.1835980504637456
LOSS train 0.13153212839516543 valid 0.1836358186976538
LOSS train 0.13153212839516543 valid 0.1837301254836899
LOSS train 0.13153212839516543 valid 0.18367712402118827
LOSS train 0.13153212839516543 valid 0.18370034601679422
LOSS train 0.13153212839516543 valid 0.1837465197740869
LOSS train 0.13153212839516543 valid 0.18394293530441042
LOSS train 0.13153212839516543 valid 0.18403104401875606
LOSS train 0.13153212839516543 valid 0.18403124102839716
LOSS train 0.13153212839516543 valid 0.18412036396480574
LOSS train 0.13153212839516543 valid 0.18442357780740543
LOSS train 0.13153212839516543 valid 0.18463573673050918
LOSS train 0.13153212839516543 valid 0.18467438966035843
LOSS train 0.13153212839516543 valid 0.18466035030104896
LOSS train 0.13153212839516543 valid 0.18458453279690465
LOSS train 0.13153212839516543 valid 0.18445933596752181
LOSS train 0.13153212839516543 valid 0.1843059765670797
LOSS train 0.13153212839516543 valid 0.18429137873179596
LOSS train 0.13153212839516543 valid 0.18428496729050364
LOSS train 0.13153212839516543 valid 0.18420676266595562
LOSS train 0.13153212839516543 valid 0.1839138481845247
LOSS train 0.13153212839516543 valid 0.1838459988048978
LOSS train 0.13153212839516543 valid 0.1838605061278377
LOSS train 0.13153212839516543 valid 0.1839025347379216
LOSS train 0.13153212839516543 valid 0.18389866152009765
LOSS train 0.13153212839516543 valid 0.18385583884001608
LOSS train 0.13153212839516543 valid 0.18383055263095432
LOSS train 0.13153212839516543 valid 0.18384498832135052
LOSS train 0.13153212839516543 valid 0.18387691414561766
LOSS train 0.13153212839516543 valid 0.18371848147554495
LOSS train 0.13153212839516543 valid 0.1837122804191831
LOSS train 0.13153212839516543 valid 0.18372775426089968
LOSS train 0.13153212839516543 valid 0.1838093931679012
LOSS train 0.13153212839516543 valid 0.18389296188192852
LOSS train 0.13153212839516543 valid 0.18381525520738717
LOSS train 0.13153212839516543 valid 0.18390007156514962
LOSS train 0.13153212839516543 valid 0.1839330481902865
LOSS train 0.13153212839516543 valid 0.18397361832518241
LOSS train 0.13153212839516543 valid 0.1840638880431652
LOSS train 0.13153212839516543 valid 0.1840180471489992
LOSS train 0.13153212839516543 valid 0.18396372032283947
LOSS train 0.13153212839516543 valid 0.1840187253633348
LOSS train 0.13153212839516543 valid 0.18401816055962913
LOSS train 0.13153212839516543 valid 0.18394860630152654
LOSS train 0.13153212839516543 valid 0.18397742368621764
LOSS train 0.13153212839516543 valid 0.1839593954214444
LOSS train 0.13153212839516543 valid 0.18385836786844514
LOSS train 0.13153212839516543 valid 0.18386643367870725
LOSS train 0.13153212839516543 valid 0.1839222218240461
LOSS train 0.13153212839516543 valid 0.18391828727683837
LOSS train 0.13153212839516543 valid 0.18389467326685405
LOSS train 0.13153212839516543 valid 0.18408149818833264
LOSS train 0.13153212839516543 valid 0.18409382029893293
LOSS train 0.13153212839516543 valid 0.18401481953878251
LOSS train 0.13153212839516543 valid 0.18397432980658132
LOSS train 0.13153212839516543 valid 0.18404013811601824
LOSS train 0.13153212839516543 valid 0.18406716270266837
LOSS train 0.13153212839516543 valid 0.1842170672543744
LOSS train 0.13153212839516543 valid 0.1841515915002674
LOSS train 0.13153212839516543 valid 0.18431821595471223
LOSS train 0.13153212839516543 valid 0.18430301065770735
LOSS train 0.13153212839516543 valid 0.18426471510354211
LOSS train 0.13153212839516543 valid 0.18437974720641417
LOSS train 0.13153212839516543 valid 0.18437677653936238
LOSS train 0.13153212839516543 valid 0.184537611307542
LOSS train 0.13153212839516543 valid 0.18458460439964902
LOSS train 0.13153212839516543 valid 0.18455142068971947
LOSS train 0.13153212839516543 valid 0.18460795074973063
LOSS train 0.13153212839516543 valid 0.18460303368893535
LOSS train 0.13153212839516543 valid 0.1844942209583755
LOSS train 0.13153212839516543 valid 0.18437929613044463
LOSS train 0.13153212839516543 valid 0.18443491987816923
LOSS train 0.13153212839516543 valid 0.18451709443045233
LOSS train 0.13153212839516543 valid 0.18446525677816192
LOSS train 0.13153212839516543 valid 0.18453871760339963
LOSS train 0.13153212839516543 valid 0.18452952965784497
LOSS train 0.13153212839516543 valid 0.1845265404712519
LOSS train 0.13153212839516543 valid 0.18453864863136876
LOSS train 0.13153212839516543 valid 0.18448197017697726
LOSS train 0.13153212839516543 valid 0.1843261463988212
LOSS train 0.13153212839516543 valid 0.18433386980615862
LOSS train 0.13153212839516543 valid 0.18437461337786026
LOSS train 0.13153212839516543 valid 0.18459761311668296
LOSS train 0.13153212839516543 valid 0.1847004296986953
LOSS train 0.13153212839516543 valid 0.1847883980570501
LOSS train 0.13153212839516543 valid 0.1846437679673478
LOSS train 0.13153212839516543 valid 0.18457584357124635
LOSS train 0.13153212839516543 valid 0.18456558789257332
LOSS train 0.13153212839516543 valid 0.18449569974626814
LOSS train 0.13153212839516543 valid 0.18441396548367633
LOSS train 0.13153212839516543 valid 0.18440128761258992
LOSS train 0.13153212839516543 valid 0.18440931077212855
LOSS train 0.13153212839516543 valid 0.18443567331059504
LOSS train 0.13153212839516543 valid 0.18449438503930266
LOSS train 0.13153212839516543 valid 0.1844984974419133
LOSS train 0.13153212839516543 valid 0.1845366499003242
LOSS train 0.13153212839516543 valid 0.18443429137075412
LOSS train 0.13153212839516543 valid 0.18444310240758827
LOSS train 0.13153212839516543 valid 0.18439281665616566
LOSS train 0.13153212839516543 valid 0.18434538178331633
LOSS train 0.13153212839516543 valid 0.1843969267458547
LOSS train 0.13153212839516543 valid 0.18439839373935352
LOSS train 0.13153212839516543 valid 0.1843815498299651
LOSS train 0.13153212839516543 valid 0.18443407032587758
LOSS train 0.13153212839516543 valid 0.1844578229420172
LOSS train 0.13153212839516543 valid 0.18436216759909077
LOSS train 0.13153212839516543 valid 0.18440308960397606
LOSS train 0.13153212839516543 valid 0.18447240472324494
EPOCH 11:
  batch 1 loss: 0.11700981855392456
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.1165633276104927
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.1138311078151067
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.12269994989037514
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.12601317465305328
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.12816529721021652
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.12316913902759552
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.12440495379269123
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.12376801669597626
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.12363975197076797
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.1218296926129948
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.12115465973814328
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.12168857799126552
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.12105266856295722
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.11979316622018814
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.12150473101064563
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.12055325201329063
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.12364745015899341
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.12341239068068956
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.12285635098814965
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.12341481163388207
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.12395911596038124
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.12421072371627974
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.12596375371019045
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.1254655033349991
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.12578825194102067
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.12616362671057382
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.12576179020106792
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.12542767976892405
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.1261122723420461
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.12658696597622288
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.1271410142071545
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.12644853149399612
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.12726058504160712
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.12882737261908395
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.12818857427272531
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.12809766688056895
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.12779031811576141
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.12824949278281286
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.1282425496727228
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.12835417760581505
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.12851653531903312
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.12960210685120072
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.12913831509649754
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.12934589667452706
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.1288138376953809
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.12872546324704556
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.1287689725868404
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.12882327075515473
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.129008656591177
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.1287671231171664
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.12889239048728576
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.1286653589527562
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.1285207714471552
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.1288942045786164
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.1290644762505378
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.12908648856376348
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.1286337411609189
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.12901397756600785
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.1288379229605198
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.12891135157131758
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.12874161668362155
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.1288816841348769
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.12865562038496137
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.1287110624405054
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.12852196101889465
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.12832358991032217
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.1285649761557579
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.1288312716760497
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.12905236333608627
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.12934869422879017
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.12959221895370218
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.12968205425837268
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.12972971313708537
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.12979071736335754
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.12970494145625516
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.12963375642702177
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.12933866708324507
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.1297156325058092
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.12994430689141154
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.1299576129258415
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.12962196303940401
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.12943788510130114
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.12981584925381912
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.12990900111549042
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.13018871401978094
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.1303961656388195
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.13084333821792493
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.13077961502785093
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.13116097392307388
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.13133857857722503
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.13129309994047103
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.1313609760454906
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.1312785120879082
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.131731363738838
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.1318191132352998
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.13159581842188983
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.13141482665526624
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.1316973665597463
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.13157272063195705
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.13142373737427268
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.13128544419419533
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.13110628514324577
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.13120751992727703
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.13082919142075947
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.13082971473066313
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.13106328868698852
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.13084393056730428
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.13092302613028692
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.13115065118128602
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.1309283423396918
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.1308822387696377
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.13063061369203888
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.13097976644833884
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.13075934551332308
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.13063519852685518
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.13099160847755578
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.13100364103408182
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.1308904946977351
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.1308573897307118
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.13082540373910556
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.13086296477523007
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.13104719826118733
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.13101629762639921
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.131093131005764
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.13116018941241597
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.13118741884240953
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.13091816671658307
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.13081466730019842
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.1308319360590898
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.13073882756342414
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.13068397907596646
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.13097027593985536
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.13104910169964407
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.1308250922847677
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.13077335903311477
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.13094893587331702
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.1309262980585513
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.1311304258356849
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.13113182719264713
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.13111189969465242
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.1309614248154029
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.13106760617111113
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.1312180620411204
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.13110812424585738
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.13119710394985054
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.13123129424797433
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.13118987404615493
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.13153272242154052
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.1315952814122041
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.13149103734469572
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.1315705516914788
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.1315333047041706
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.13123108877183556
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.13126232475042343
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.1311917300694264
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.13129937444712705
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.1312618340966822
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.13135892663549328
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.13150619571097194
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.13163841765674744
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.13157468647868545
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.13157899522342564
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.13178839770759024
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.13193244870864984
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.13181538870894766
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.13169227370958841
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.13165002094493025
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.13146246687547694
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.13173376611050439
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.1317669505264327
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.13174149316064146
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.1315194651742891
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.1314445148254263
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.13157117264611382
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.131389689123766
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.13119482775192476
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.13090401356307307
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.1311103022714567
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.1309498269110918
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.13101785967527832
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.1309873407276777
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.13087354628929024
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.13079275227273288
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.13082481103168953
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.13077248292424345
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.1308648058357723
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.1307528496898235
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.13069392275557948
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.1307467012028945
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.13076605706314767
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.1306590357562527
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.13071408995229344
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.13045961110247778
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.13048806152282616
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.13061699947836447
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.13070960085706662
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.1308318411912581
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.1307653290916927
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.13090156424790622
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.13091910296856468
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.1309815454527293
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.13100450838287475
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.13104010223611898
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.13099945078535777
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.13085859616115256
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.13080911295137543
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.13064934127032757
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.13072036962474934
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.1308043483467329
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.13078302773536665
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.13083598010663716
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.1309113325507428
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.1307025136095341
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.1307875428781953
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.1306880554874186
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.1306605099052328
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.1307524888682256
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.13081307311172355
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.13081558891995387
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.13079877855146632
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.13082225261642053
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.13076524489930927
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.13084482581221632
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.1307354285650783
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.13074671836421553
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.13086777285463486
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.1307071076150526
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.1306471394854862
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.13087698716832244
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.1311290959194625
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.1310432269719654
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.13109494134975605
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.13109219020121118
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.13110008020984365
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.13111092652178416
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.1310621609416189
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.13109366858706756
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.13109397582678614
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.1309910277215143
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.13111459568946687
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.13108823051260524
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.1311470109800743
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.1311921778699902
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.1311824346379358
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.1312590472521336
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.13125312126237854
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.13130542915314436
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.13136137312794305
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.1312906027138233
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.1315017541150648
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.13154221435506191
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.13157359646125275
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.1315433967007896
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.13150419648371492
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.1315054162114393
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.13145117052914104
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.13138063754453216
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.13144333879229644
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.1314712405204773
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.13137318859040967
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.13130254898476237
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.13127656071358307
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.13114419370663888
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.13115993417658897
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.13118891087465717
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.13118170252007044
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.13119005139416723
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.13109411714688554
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.13111496556688237
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.13110204172970183
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.1310562453342273
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.13116475156484506
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.131037159612144
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.13093085849826985
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.13113770388282728
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.13110521648227091
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.13112936964888366
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.1310842304780919
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.13107269890606404
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.131038051244415
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.1309767481970026
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.13098003599437302
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.1309509616817387
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.13107925805083492
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.13117890690381712
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.13110583537754697
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.13109793545057377
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.1310327075499152
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.13104540790463315
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.1310470258605849
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.13115669233239677
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.1313456732004169
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.13139743993983788
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.1313627592082751
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.13137232542440697
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.13136325365164464
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.13130838006434825
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.13126064924193068
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.13122246953348318
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.13124292210190003
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.13123412008415783
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.1312317748618598
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.13118565114411085
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.13115712905027826
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.13116677370823288
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.13121326532251285
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.1311786472071688
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.13113674258626395
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.13105232153208024
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.13111216179619264
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.13107107231059137
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.1310177632747367
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.13101730749580512
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.13103387107451755
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.13116606303691108
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.131150712091652
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.1311402234875556
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.13106332457738237
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.13115644925273956
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.1311293970851512
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.1310610924818501
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.13106975725191666
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.1309923236430795
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.1310309090293371
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.13096060197046197
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.13087254762649536
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.13088297485033187
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.13094143524177168
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.13104784935712815
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.1310956337210635
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.13111448718840818
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.13107008281144295
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.13112710802230293
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.13119039177449782
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.13122584575432397
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.13123268660611145
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.13125744340775986
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.13118510053748578
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.13118213375701623
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.13118293763430586
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.13147091059482585
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.13151848489967102
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.13156936036119626
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.13156593904115152
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.13153834742187076
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.1315533823470561
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.13154467163157874
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.13149455828021112
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.13143271273800305
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.13144315543592486
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.1315362628083676
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.13163761395655002
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.1316690014959392
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.13166814641633504
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.13158977812344438
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.13170180837062895
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.13177622269901484
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.1316993236126674
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.131654224337803
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.13163581275873898
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.1317089195080225
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.13167974252264034
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.1316240123872246
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.13168161333832024
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.13167264400039866
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.13174339270153226
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.13185233013380482
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.13182518178935296
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.13176079561581483
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.1318113423502349
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.13178527178943797
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.13182962372219914
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.13190805971781838
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.13194448812802634
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.13194158487021923
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.13193567804695758
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.13200204385809167
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.13194214086733896
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.13193638450221012
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.13192084376893332
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.13186274474750015
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.13195399926396637
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.13194533901211494
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.13188045651881725
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.13182059200625346
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.13183574182118557
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.13180298817142383
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.13179461296006156
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.13173387264594053
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.13169285689321014
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.1316848746976074
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.13179370400711476
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.1318121482878167
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.13186030387878417
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.1317892163013569
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.13168326999888913
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.1317209700618557
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.13170039284050017
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.1316682191565633
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.13166618763360002
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.13164465151615998
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.13159449886595345
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.13162886082093314
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.1316648793441278
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.13162433869879822
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.13163669191355787
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.1316946131706822
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.13165799570258496
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.13164850437059636
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.13163323341494929
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.1317075596200031
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.13174249754979594
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.13179352656366744
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.13186655690871088
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.13194675654029617
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.13192860903642734
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.13188250086595568
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.13195113736362615
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.13190484227878707
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.1319632636042502
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.13198818626562
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.1319107618315
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.13190872260843808
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.1318995651778053
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.13179637498698885
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.13180560369123062
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.13176212661734252
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.13167480256035055
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.1316098406217819
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.13166660246743958
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.13162881703357454
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.1316220342734394
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.13168105804082436
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.1316419205960186
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.13172679686696703
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.13180000761212693
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.1317574637835701
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.13164926798916624
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.1316360998729413
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.13165549827473505
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.1316071918141518
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.13159296890346514
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.13156650848015472
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.13158828999554173
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.13160181247666813
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.13156950902152115
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.13162444693235947
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.1316107284020475
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.13158737412757343
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.13155491057031699
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.13160775521094292
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.13154637328375782
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.13154520332353756
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.13149840851406475
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.1315002570811071
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.13150464983033402
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.13152810430162337
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.13152966221433318
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.13159777768280195
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.13158427654016044
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.13157339574712695
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.13150072055939208
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.13152672151296302
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.13154176198026185
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.13156382723555543
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.13158222382022686
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.13166030295766318
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.13170451650233156
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.13173842144773362
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.13177621620855515
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.1318790472223092
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.1318790472223092 valid 0.2130441963672638
LOSS train 0.1318790472223092 valid 0.18824870139360428
LOSS train 0.1318790472223092 valid 0.1813831329345703
LOSS train 0.1318790472223092 valid 0.17313025891780853
LOSS train 0.1318790472223092 valid 0.1688530534505844
LOSS train 0.1318790472223092 valid 0.17876450220743814
LOSS train 0.1318790472223092 valid 0.18988254240580968
LOSS train 0.1318790472223092 valid 0.18743515014648438
LOSS train 0.1318790472223092 valid 0.18670415547158983
LOSS train 0.1318790472223092 valid 0.1872079089283943
LOSS train 0.1318790472223092 valid 0.18584620817141098
LOSS train 0.1318790472223092 valid 0.18635721504688263
LOSS train 0.1318790472223092 valid 0.18606647161337045
LOSS train 0.1318790472223092 valid 0.1856049426964351
LOSS train 0.1318790472223092 valid 0.1832893580198288
LOSS train 0.1318790472223092 valid 0.18310959544032812
LOSS train 0.1318790472223092 valid 0.18413749775465796
LOSS train 0.1318790472223092 valid 0.18309874749845928
LOSS train 0.1318790472223092 valid 0.18558328245815478
LOSS train 0.1318790472223092 valid 0.18522078543901443
LOSS train 0.1318790472223092 valid 0.1844005442800976
LOSS train 0.1318790472223092 valid 0.18316363746469672
LOSS train 0.1318790472223092 valid 0.18292333380035733
LOSS train 0.1318790472223092 valid 0.18304095106820265
LOSS train 0.1318790472223092 valid 0.18170638382434845
LOSS train 0.1318790472223092 valid 0.18145928703821623
LOSS train 0.1318790472223092 valid 0.18170760130440747
LOSS train 0.1318790472223092 valid 0.1814401357301644
LOSS train 0.1318790472223092 valid 0.18090919173997025
LOSS train 0.1318790472223092 valid 0.18147952059904734
LOSS train 0.1318790472223092 valid 0.182269444388728
LOSS train 0.1318790472223092 valid 0.18131297314539552
LOSS train 0.1318790472223092 valid 0.18159314583648334
LOSS train 0.1318790472223092 valid 0.18108102854560404
LOSS train 0.1318790472223092 valid 0.18285266927310398
LOSS train 0.1318790472223092 valid 0.18270109676652485
LOSS train 0.1318790472223092 valid 0.18332788106557485
LOSS train 0.1318790472223092 valid 0.1838662753763952
LOSS train 0.1318790472223092 valid 0.18347857434016007
LOSS train 0.1318790472223092 valid 0.1831389307975769
LOSS train 0.1318790472223092 valid 0.18364448372910663
LOSS train 0.1318790472223092 valid 0.18391785415865125
LOSS train 0.1318790472223092 valid 0.18388781159423118
LOSS train 0.1318790472223092 valid 0.18436953459273686
LOSS train 0.1318790472223092 valid 0.18447707096735635
LOSS train 0.1318790472223092 valid 0.18481840063696323
LOSS train 0.1318790472223092 valid 0.1854507082954366
LOSS train 0.1318790472223092 valid 0.18526617685953775
LOSS train 0.1318790472223092 valid 0.18587379978627575
LOSS train 0.1318790472223092 valid 0.1853622168302536
LOSS train 0.1318790472223092 valid 0.18552777989237917
LOSS train 0.1318790472223092 valid 0.1850518910930707
LOSS train 0.1318790472223092 valid 0.18552819794079042
LOSS train 0.1318790472223092 valid 0.18565456789952736
LOSS train 0.1318790472223092 valid 0.18560440079732374
LOSS train 0.1318790472223092 valid 0.18478203485054628
LOSS train 0.1318790472223092 valid 0.18471336129464602
LOSS train 0.1318790472223092 valid 0.18455883680746474
LOSS train 0.1318790472223092 valid 0.18495601870245854
LOSS train 0.1318790472223092 valid 0.18493887359897296
LOSS train 0.1318790472223092 valid 0.18455456319402475
LOSS train 0.1318790472223092 valid 0.1851178849897077
LOSS train 0.1318790472223092 valid 0.18448830786205472
LOSS train 0.1318790472223092 valid 0.1854204589035362
LOSS train 0.1318790472223092 valid 0.18565461681439327
LOSS train 0.1318790472223092 valid 0.18545210880763602
LOSS train 0.1318790472223092 valid 0.18481117931764518
LOSS train 0.1318790472223092 valid 0.18485957734725056
LOSS train 0.1318790472223092 valid 0.18437807564286218
LOSS train 0.1318790472223092 valid 0.1848812039409365
LOSS train 0.1318790472223092 valid 0.18451555975725953
LOSS train 0.1318790472223092 valid 0.18474623229768541
LOSS train 0.1318790472223092 valid 0.1847472656263064
LOSS train 0.1318790472223092 valid 0.1848018527836413
LOSS train 0.1318790472223092 valid 0.1851523596048355
LOSS train 0.1318790472223092 valid 0.18551811340608096
LOSS train 0.1318790472223092 valid 0.18534074297973088
LOSS train 0.1318790472223092 valid 0.18516187618176141
LOSS train 0.1318790472223092 valid 0.18475030853024013
LOSS train 0.1318790472223092 valid 0.18411222156137227
LOSS train 0.1318790472223092 valid 0.1838167436696865
LOSS train 0.1318790472223092 valid 0.18426441455759654
LOSS train 0.1318790472223092 valid 0.18400442241186118
LOSS train 0.1318790472223092 valid 0.18400351153243155
LOSS train 0.1318790472223092 valid 0.18384673858390135
LOSS train 0.1318790472223092 valid 0.1836264913165292
LOSS train 0.1318790472223092 valid 0.18344528681930455
LOSS train 0.1318790472223092 valid 0.18314259160648694
LOSS train 0.1318790472223092 valid 0.1837120759353209
LOSS train 0.1318790472223092 valid 0.18370161089632248
LOSS train 0.1318790472223092 valid 0.1837977097257153
LOSS train 0.1318790472223092 valid 0.18378017470240593
LOSS train 0.1318790472223092 valid 0.18354093483699266
LOSS train 0.1318790472223092 valid 0.1837469073052102
LOSS train 0.1318790472223092 valid 0.18366949934708446
LOSS train 0.1318790472223092 valid 0.18372771345699826
LOSS train 0.1318790472223092 valid 0.1835791232045164
LOSS train 0.1318790472223092 valid 0.18378662895791384
LOSS train 0.1318790472223092 valid 0.1838869149937774
LOSS train 0.1318790472223092 valid 0.1839822955429554
LOSS train 0.1318790472223092 valid 0.18416525733352887
LOSS train 0.1318790472223092 valid 0.1843645539061696
LOSS train 0.1318790472223092 valid 0.1840599276197767
LOSS train 0.1318790472223092 valid 0.18418387123025382
LOSS train 0.1318790472223092 valid 0.1842628606728145
LOSS train 0.1318790472223092 valid 0.18471525553262458
LOSS train 0.1318790472223092 valid 0.18460119842925918
LOSS train 0.1318790472223092 valid 0.184781135370334
LOSS train 0.1318790472223092 valid 0.1851546079467196
LOSS train 0.1318790472223092 valid 0.1852323752912608
LOSS train 0.1318790472223092 valid 0.18517802078444678
LOSS train 0.1318790472223092 valid 0.18503748093332564
LOSS train 0.1318790472223092 valid 0.1851120395997984
LOSS train 0.1318790472223092 valid 0.1854116737581136
LOSS train 0.1318790472223092 valid 0.1856121021768321
LOSS train 0.1318790472223092 valid 0.18578135260734066
LOSS train 0.1318790472223092 valid 0.1857210158282875
LOSS train 0.1318790472223092 valid 0.18536360789153536
LOSS train 0.1318790472223092 valid 0.18502778979409643
LOSS train 0.1318790472223092 valid 0.1846872484932343
LOSS train 0.1318790472223092 valid 0.18452879453986143
LOSS train 0.1318790472223092 valid 0.18457215999970672
LOSS train 0.1318790472223092 valid 0.18434313169824398
LOSS train 0.1318790472223092 valid 0.18470069165191344
LOSS train 0.1318790472223092 valid 0.18453784942626952
LOSS train 0.1318790472223092 valid 0.18493398882093884
LOSS train 0.1318790472223092 valid 0.1847813159931363
LOSS train 0.1318790472223092 valid 0.18481480132322758
LOSS train 0.1318790472223092 valid 0.18498391686945923
LOSS train 0.1318790472223092 valid 0.184646719121016
LOSS train 0.1318790472223092 valid 0.1843493604705534
LOSS train 0.1318790472223092 valid 0.18400361434076773
LOSS train 0.1318790472223092 valid 0.18393409431428837
LOSS train 0.1318790472223092 valid 0.18407594145678763
LOSS train 0.1318790472223092 valid 0.1839450513875043
LOSS train 0.1318790472223092 valid 0.18394549004733562
LOSS train 0.1318790472223092 valid 0.1836424035965091
LOSS train 0.1318790472223092 valid 0.18356280914251355
LOSS train 0.1318790472223092 valid 0.18349290623081674
LOSS train 0.1318790472223092 valid 0.18363889721887452
LOSS train 0.1318790472223092 valid 0.18348482451963086
LOSS train 0.1318790472223092 valid 0.18348896902211956
LOSS train 0.1318790472223092 valid 0.18337451755166886
LOSS train 0.1318790472223092 valid 0.18337142953856123
LOSS train 0.1318790472223092 valid 0.18322138210822797
LOSS train 0.1318790472223092 valid 0.183440042276905
LOSS train 0.1318790472223092 valid 0.18338927393462381
LOSS train 0.1318790472223092 valid 0.18425041185440244
LOSS train 0.1318790472223092 valid 0.18437950823131025
LOSS train 0.1318790472223092 valid 0.184335458278656
LOSS train 0.1318790472223092 valid 0.18454810167779986
LOSS train 0.1318790472223092 valid 0.18421098746751485
LOSS train 0.1318790472223092 valid 0.18416056251214222
LOSS train 0.1318790472223092 valid 0.18402909439105491
LOSS train 0.1318790472223092 valid 0.1839505966632597
LOSS train 0.1318790472223092 valid 0.18399488056699434
LOSS train 0.1318790472223092 valid 0.184089795419365
LOSS train 0.1318790472223092 valid 0.18408513276637356
LOSS train 0.1318790472223092 valid 0.1841467189713844
LOSS train 0.1318790472223092 valid 0.18407974652945996
LOSS train 0.1318790472223092 valid 0.1839434656297198
LOSS train 0.1318790472223092 valid 0.18381531766535322
LOSS train 0.1318790472223092 valid 0.1835345160558911
LOSS train 0.1318790472223092 valid 0.18338454814582336
LOSS train 0.1318790472223092 valid 0.18321156456614984
LOSS train 0.1318790472223092 valid 0.18329362068549696
LOSS train 0.1318790472223092 valid 0.18361431039022114
LOSS train 0.1318790472223092 valid 0.1835222929006531
LOSS train 0.1318790472223092 valid 0.18369041806494696
LOSS train 0.1318790472223092 valid 0.18368899007053935
LOSS train 0.1318790472223092 valid 0.18369302484724256
LOSS train 0.1318790472223092 valid 0.18354745659717295
LOSS train 0.1318790472223092 valid 0.18354839172666473
LOSS train 0.1318790472223092 valid 0.18350378638026357
LOSS train 0.1318790472223092 valid 0.18318641177245548
LOSS train 0.1318790472223092 valid 0.18314443350854245
LOSS train 0.1318790472223092 valid 0.18323715948789132
LOSS train 0.1318790472223092 valid 0.18349956553638652
LOSS train 0.1318790472223092 valid 0.18341737444507344
LOSS train 0.1318790472223092 valid 0.18333366902338133
LOSS train 0.1318790472223092 valid 0.18348797102000833
LOSS train 0.1318790472223092 valid 0.18332935656820024
LOSS train 0.1318790472223092 valid 0.1833875921906018
LOSS train 0.1318790472223092 valid 0.18325844415180062
LOSS train 0.1318790472223092 valid 0.1831339335119402
LOSS train 0.1318790472223092 valid 0.18316734133548634
LOSS train 0.1318790472223092 valid 0.18301682732003258
LOSS train 0.1318790472223092 valid 0.18298143465468225
LOSS train 0.1318790472223092 valid 0.1828590633061828
LOSS train 0.1318790472223092 valid 0.18295392660718215
LOSS train 0.1318790472223092 valid 0.18283789304538547
LOSS train 0.1318790472223092 valid 0.18290760391391814
LOSS train 0.1318790472223092 valid 0.18261934083360465
LOSS train 0.1318790472223092 valid 0.18252386321726533
LOSS train 0.1318790472223092 valid 0.1823275638696475
LOSS train 0.1318790472223092 valid 0.18231471141382138
LOSS train 0.1318790472223092 valid 0.1825435702419523
LOSS train 0.1318790472223092 valid 0.18248697031628003
LOSS train 0.1318790472223092 valid 0.18260601117982336
LOSS train 0.1318790472223092 valid 0.18245441727340223
LOSS train 0.1318790472223092 valid 0.18224748620642953
LOSS train 0.1318790472223092 valid 0.18216249029530157
LOSS train 0.1318790472223092 valid 0.18214921579865986
LOSS train 0.1318790472223092 valid 0.18237096524121715
LOSS train 0.1318790472223092 valid 0.18213013599558575
LOSS train 0.1318790472223092 valid 0.18216832068938654
LOSS train 0.1318790472223092 valid 0.18210025959544712
LOSS train 0.1318790472223092 valid 0.1819445528806402
LOSS train 0.1318790472223092 valid 0.18193361483977744
LOSS train 0.1318790472223092 valid 0.18184955879336312
LOSS train 0.1318790472223092 valid 0.18184785474250667
LOSS train 0.1318790472223092 valid 0.1817937250828968
LOSS train 0.1318790472223092 valid 0.18176548245926977
LOSS train 0.1318790472223092 valid 0.18180106796115358
LOSS train 0.1318790472223092 valid 0.18167281060717827
LOSS train 0.1318790472223092 valid 0.18154588693545926
LOSS train 0.1318790472223092 valid 0.18147705927971872
LOSS train 0.1318790472223092 valid 0.18152514216276483
LOSS train 0.1318790472223092 valid 0.18163657242848993
LOSS train 0.1318790472223092 valid 0.18160829246044158
LOSS train 0.1318790472223092 valid 0.1815156832809362
LOSS train 0.1318790472223092 valid 0.18155274110602904
LOSS train 0.1318790472223092 valid 0.18173387994146134
LOSS train 0.1318790472223092 valid 0.18184501325179422
LOSS train 0.1318790472223092 valid 0.18200615684191385
LOSS train 0.1318790472223092 valid 0.18227049932543155
LOSS train 0.1318790472223092 valid 0.1823471148502459
LOSS train 0.1318790472223092 valid 0.1824063861317802
LOSS train 0.1318790472223092 valid 0.18241519087266714
LOSS train 0.1318790472223092 valid 0.18246407256178235
LOSS train 0.1318790472223092 valid 0.18261026426569207
LOSS train 0.1318790472223092 valid 0.18262403246400685
LOSS train 0.1318790472223092 valid 0.1827226414828853
LOSS train 0.1318790472223092 valid 0.1827776276657724
LOSS train 0.1318790472223092 valid 0.18294890573684205
LOSS train 0.1318790472223092 valid 0.18281966788788973
LOSS train 0.1318790472223092 valid 0.18275903988991105
LOSS train 0.1318790472223092 valid 0.1827957724072352
LOSS train 0.1318790472223092 valid 0.18260541764013938
LOSS train 0.1318790472223092 valid 0.18259473213305075
LOSS train 0.1318790472223092 valid 0.18278069695249138
LOSS train 0.1318790472223092 valid 0.18262102910556083
LOSS train 0.1318790472223092 valid 0.18282925880249637
LOSS train 0.1318790472223092 valid 0.18302451819181442
LOSS train 0.1318790472223092 valid 0.18309044679816888
LOSS train 0.1318790472223092 valid 0.1829416044965023
LOSS train 0.1318790472223092 valid 0.1831004632267392
LOSS train 0.1318790472223092 valid 0.18305032015327485
LOSS train 0.1318790472223092 valid 0.18305140231029096
LOSS train 0.1318790472223092 valid 0.1830964857339859
LOSS train 0.1318790472223092 valid 0.18292471469635982
LOSS train 0.1318790472223092 valid 0.18318832281326491
LOSS train 0.1318790472223092 valid 0.1831312655460222
LOSS train 0.1318790472223092 valid 0.18302879541173694
LOSS train 0.1318790472223092 valid 0.18306532677482157
LOSS train 0.1318790472223092 valid 0.18314372247550637
LOSS train 0.1318790472223092 valid 0.18296381918140886
LOSS train 0.1318790472223092 valid 0.18321302099976428
LOSS train 0.1318790472223092 valid 0.18321457269339028
LOSS train 0.1318790472223092 valid 0.18307678911548395
LOSS train 0.1318790472223092 valid 0.18324238107578963
LOSS train 0.1318790472223092 valid 0.18330890133635688
LOSS train 0.1318790472223092 valid 0.18335092566312491
LOSS train 0.1318790472223092 valid 0.18343953031933669
LOSS train 0.1318790472223092 valid 0.18338264052597983
LOSS train 0.1318790472223092 valid 0.18341992285690809
LOSS train 0.1318790472223092 valid 0.18345330287231487
LOSS train 0.1318790472223092 valid 0.18365965088579192
LOSS train 0.1318790472223092 valid 0.18375682625850337
LOSS train 0.1318790472223092 valid 0.18375285979774264
LOSS train 0.1318790472223092 valid 0.1838362401152009
LOSS train 0.1318790472223092 valid 0.18415443462264888
LOSS train 0.1318790472223092 valid 0.18435590182031905
LOSS train 0.1318790472223092 valid 0.18438622078103742
LOSS train 0.1318790472223092 valid 0.18437130310318686
LOSS train 0.1318790472223092 valid 0.18429475002314732
LOSS train 0.1318790472223092 valid 0.1841687553088157
LOSS train 0.1318790472223092 valid 0.18401021907012238
LOSS train 0.1318790472223092 valid 0.18398872957861978
LOSS train 0.1318790472223092 valid 0.18397863491305283
LOSS train 0.1318790472223092 valid 0.18390068851968147
LOSS train 0.1318790472223092 valid 0.18360552337047056
LOSS train 0.1318790472223092 valid 0.18352073802227686
LOSS train 0.1318790472223092 valid 0.1835275394973201
LOSS train 0.1318790472223092 valid 0.18356284365842218
LOSS train 0.1318790472223092 valid 0.18356243289121382
LOSS train 0.1318790472223092 valid 0.18352144694390612
LOSS train 0.1318790472223092 valid 0.18349905009381473
LOSS train 0.1318790472223092 valid 0.18352099276022102
LOSS train 0.1318790472223092 valid 0.18354656133672287
LOSS train 0.1318790472223092 valid 0.1833797372954408
LOSS train 0.1318790472223092 valid 0.18337021566519182
LOSS train 0.1318790472223092 valid 0.18338025554556894
LOSS train 0.1318790472223092 valid 0.18345657571339283
LOSS train 0.1318790472223092 valid 0.1835456268514617
LOSS train 0.1318790472223092 valid 0.1834697338022493
LOSS train 0.1318790472223092 valid 0.1835576187731441
LOSS train 0.1318790472223092 valid 0.18359095945754308
LOSS train 0.1318790472223092 valid 0.18363707876424726
LOSS train 0.1318790472223092 valid 0.1837248418480158
LOSS train 0.1318790472223092 valid 0.18367830995606427
LOSS train 0.1318790472223092 valid 0.18362419117266768
LOSS train 0.1318790472223092 valid 0.18367442289496413
LOSS train 0.1318790472223092 valid 0.1836698061884626
LOSS train 0.1318790472223092 valid 0.18359881915518494
LOSS train 0.1318790472223092 valid 0.18363980425436513
LOSS train 0.1318790472223092 valid 0.18361611390055585
LOSS train 0.1318790472223092 valid 0.18352630219192473
LOSS train 0.1318790472223092 valid 0.18353261711817342
LOSS train 0.1318790472223092 valid 0.18358586971317567
LOSS train 0.1318790472223092 valid 0.18358745033047208
LOSS train 0.1318790472223092 valid 0.1835559730967268
LOSS train 0.1318790472223092 valid 0.18373080485830673
LOSS train 0.1318790472223092 valid 0.18374649356979472
LOSS train 0.1318790472223092 valid 0.1836572139745667
LOSS train 0.1318790472223092 valid 0.18361775235190422
LOSS train 0.1318790472223092 valid 0.1836810000828388
LOSS train 0.1318790472223092 valid 0.18371577263347008
LOSS train 0.1318790472223092 valid 0.18385960107967023
LOSS train 0.1318790472223092 valid 0.18378738237079234
LOSS train 0.1318790472223092 valid 0.18395801800712247
LOSS train 0.1318790472223092 valid 0.18393917087905154
LOSS train 0.1318790472223092 valid 0.18389642559245645
LOSS train 0.1318790472223092 valid 0.18401607395046288
LOSS train 0.1318790472223092 valid 0.1840182155599961
LOSS train 0.1318790472223092 valid 0.18417177383610808
LOSS train 0.1318790472223092 valid 0.1842305124580678
LOSS train 0.1318790472223092 valid 0.1841893833266889
LOSS train 0.1318790472223092 valid 0.1842439212924079
LOSS train 0.1318790472223092 valid 0.18424039575638193
LOSS train 0.1318790472223092 valid 0.18412342863410622
LOSS train 0.1318790472223092 valid 0.1840066958039281
LOSS train 0.1318790472223092 valid 0.18405789434462339
LOSS train 0.1318790472223092 valid 0.18414297148942232
LOSS train 0.1318790472223092 valid 0.18409591675249498
LOSS train 0.1318790472223092 valid 0.18417156614097102
LOSS train 0.1318790472223092 valid 0.18416539668064089
LOSS train 0.1318790472223092 valid 0.18415692040817977
LOSS train 0.1318790472223092 valid 0.18416367319569124
LOSS train 0.1318790472223092 valid 0.1841088501188685
LOSS train 0.1318790472223092 valid 0.18395501206959447
LOSS train 0.1318790472223092 valid 0.18396182678509176
LOSS train 0.1318790472223092 valid 0.18399987282888536
LOSS train 0.1318790472223092 valid 0.1842207961260926
LOSS train 0.1318790472223092 valid 0.18432965393083683
LOSS train 0.1318790472223092 valid 0.18440859995222506
LOSS train 0.1318790472223092 valid 0.18425588037077906
LOSS train 0.1318790472223092 valid 0.1841824481841819
LOSS train 0.1318790472223092 valid 0.18417607045019938
LOSS train 0.1318790472223092 valid 0.18411659638796535
LOSS train 0.1318790472223092 valid 0.18403267187525404
LOSS train 0.1318790472223092 valid 0.1840311341961338
LOSS train 0.1318790472223092 valid 0.18403859231779326
LOSS train 0.1318790472223092 valid 0.18406089948816487
LOSS train 0.1318790472223092 valid 0.1841298827612904
LOSS train 0.1318790472223092 valid 0.18413938960751122
LOSS train 0.1318790472223092 valid 0.18417718299046285
LOSS train 0.1318790472223092 valid 0.18407232349728073
LOSS train 0.1318790472223092 valid 0.18409138405306424
LOSS train 0.1318790472223092 valid 0.18404979256706105
LOSS train 0.1318790472223092 valid 0.1839945612067661
LOSS train 0.1318790472223092 valid 0.18405146795988742
LOSS train 0.1318790472223092 valid 0.1840452135389173
LOSS train 0.1318790472223092 valid 0.18403121495394262
LOSS train 0.1318790472223092 valid 0.184084209371103
LOSS train 0.1318790472223092 valid 0.18411610562537536
LOSS train 0.1318790472223092 valid 0.18401385330130685
LOSS train 0.1318790472223092 valid 0.18404949806711596
LOSS train 0.1318790472223092 valid 0.18414225279881055
EPOCH 12:
  batch 1 loss: 0.11707445234060287
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.11917320266366005
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.11632496366898219
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.12418791092932224
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.12837830632925035
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.12959335123499235
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.12397111739431109
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.12538596242666245
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.12442361563444138
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.12405422329902649
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.122307188808918
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.12122890104850133
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.1217638380252398
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.12155008209603173
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.12043591936429342
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.122101997025311
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.12127106987378176
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.1241275846130318
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.12365411888611944
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.12310906425118447
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.12357155269100553
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.12406160750172356
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.12446466023507326
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.12631596873203912
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.12567048162221908
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.12598521634936333
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.12627484649419785
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.12586518323847226
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.12534135117613035
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.1260271559158961
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.12659790006376082
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.1271004043519497
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.12632012141473364
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.12710152391125173
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.1285343783242362
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.1278506260779169
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.1277448716195854
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.12743079564289042
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.12783815062198883
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.12796461526304484
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.12798957181412998
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.12813660945920718
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.12919314569512078
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.12857626734132116
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.12876642362938986
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.1282571139542953
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.1280612825079167
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.12815370659033457
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.12830823811949515
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.1283600574731827
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.1281164438993323
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.12828266950180897
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.128131369937141
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.12799368039877326
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.12832569899884136
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.1284993491800768
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.12858377671555468
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.12810510011582538
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.12851026174375565
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.12835103596250216
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.1285284105382982
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.12839504595725768
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.12848816457248868
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.12826204136945307
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.12833324166444632
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.1281206812145132
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.12790882776477444
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.12813970742418485
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.1283837596791378
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.1285937261368547
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.12898677346152318
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.12919526205708584
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.12931738579518173
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.1293414886537436
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.12942905257145565
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.1293074260220716
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.1293247997180208
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.12903090700125083
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.1294434576094905
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.12963600009679793
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.12969449291258683
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.12933778363030132
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.12917475976857795
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.12956028147822335
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.1296337786842795
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.1299183940471605
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.13019204396626044
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.13065339083021338
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.1305999258595906
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.13092172195514043
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.13109251742179578
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.13101868525795315
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.1311419471617668
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.1310270184532125
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.13153386288567592
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.13160365261137486
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.1314108827218567
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.1312115439498911
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.13152189786084975
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.1313852556794882
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.13123459568118104
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.1310735952620413
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.13087481093927494
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.1310119156797345
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.13062631466559002
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.13067810047628745
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.1309517305170264
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.1307112704962492
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.13080217320164408
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.13105646906928584
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.13081699247295792
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.1307683932994093
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.13053904236417957
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.13088308823736092
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.13066942108714064
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.1305688724949442
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.13094775518800458
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.13095277168993222
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.13082733506164632
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.13081455981979767
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.13077148604245226
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.13081405940847318
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.13096823725031642
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.13096501864492893
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.1309914200901985
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.13106045996149382
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.13108143298410055
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.1308238516212441
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.13074576744968577
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.13077688268744028
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.1306333574288674
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.13059068736479137
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.1308531527568523
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.13089207554263854
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.13067830552657445
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.1305823554558789
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.1307244828908983
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.1306724974858588
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.1308732373680142
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.13091223676289832
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.13091124234892798
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.13075334919800222
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.13088473964195985
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.13101057672045296
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.13088555623745096
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.13097191173328113
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.13098367673604666
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.13091133353677956
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.13124806088889204
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.13133025248845417
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.13123261612772152
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.13133855742451392
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.13131224222822127
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.13099889055668534
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.13106117061068934
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.13102535019891384
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.13109355372418263
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.13103851567529426
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.13115101373233135
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.1312618767376989
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.13137624125858272
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.13127980257074037
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.13128019489942153
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.13150078598864196
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.13165098704171904
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.13156492043151913
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.13142409296092872
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.131340279022143
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.13114153633456258
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.1313836536863271
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.13140848183143905
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.13135600562185742
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.1311519004275344
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.1310736321597949
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.13123522873435703
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.13101884832775051
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.1308336190584689
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.13054174142942
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.13074929221382353
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.13057911975516212
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.13065567118686866
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.13063208357645914
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.13052216424642366
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.13045209330385146
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.1304593969035793
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.13036256828295287
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.1304414185451314
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.13035280733032428
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.13030394560918607
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.1303476666541476
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.13034548977126625
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.13025929049278298
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.13028029263637225
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.13002232738684133
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.1300450060612116
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.13017286565534922
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.1302607864292745
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.1304240765595677
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.13037426799685511
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.13052589036524295
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.13055700604891896
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.13060146857901375
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.13061384680529534
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.1306547027300386
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.13059380697767908
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.13047252255446704
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.1304442246179074
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.1302840945382531
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.1303843313997442
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.1305042223561378
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.1305011702233581
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.13053730369176506
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.13060506492713247
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.13038291047527412
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.13045192544543466
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.13035674841591605
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.13034040669691727
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.13041575266680586
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.13046912127706015
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.13047686205668882
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.1304733530024058
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.13049279381562998
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.13043601951256995
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.13050095550715923
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.1303907197051578
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.1304143676451877
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.1305340652555096
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.13035987533236804
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.13031312823295593
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.1305376080715138
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.13077022012694056
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.130676480074381
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.13073563454233014
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.1307366532392991
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.13072317781600545
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.1307366113415209
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.13067771755316088
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.13070962568666755
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.1307112101044615
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.1305911445990205
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.13070830140123724
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.13066674356371905
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.1307125894253146
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.13074608770061713
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.1307228169879135
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.13079379311179726
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.1307711914362695
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.13085538407246913
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.13090787756155772
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.13086265444755554
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.1310442893866049
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.13107619458247746
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.13109781383996896
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.13106508919690538
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.13102692584781087
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.1310154485690873
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.13095127373701868
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.13088432913140732
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.13094092813467886
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.13097310089148007
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.13086697476348658
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.13078647396946683
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.13075261599890634
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.13062706759030168
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.13064301480662147
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.13068210972206934
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.130670410472802
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.13066423022702559
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.1305613755958231
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.1305876358239739
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.1305878381227655
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.1305403371944147
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.1306580514152408
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.1305295134058399
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.1304131216081706
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.13061115402134432
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.13058097023073087
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.13061448859332278
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.13057364510058503
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.13057632113673856
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.13053034110743805
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.13046321336259234
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.1304549156777008
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.13042073877869356
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.13053300095754758
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.1306564671124195
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.1305825514816241
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.1305625884948919
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.13050480999026745
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.13054518424745265
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.13055346739763246
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.13067619826593627
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.1308527616052904
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.13088073710701903
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.13085131864931623
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.13088637975522796
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.13085121132107294
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.1307997662049012
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.13074527518904727
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.130721858193477
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.13074001669883728
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.1307467320403516
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.13074518022167408
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.13070887493851938
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.1306623058240922
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.13067539411237816
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.13071934781750172
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.1306745530432695
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.1306249738270025
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.13053915784243614
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.13061751564216
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.13057962716676486
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.13051827511372277
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.13049418237178947
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.13050023780928718
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.13061646291915374
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.13060744224286605
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.13059696653541528
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.13050887462767688
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.13062122052069752
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.13058209781334779
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.13051845454929037
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.13053827142291025
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.13045400128136447
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.13046148025072538
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.13036495230040668
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.13026509506895637
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.13026381467991485
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.13030563197509135
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.1303658473898064
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.13039345798445612
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.13039937343852348
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.13033327484900528
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.13036331010168184
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.13042328199788705
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.13045056450313755
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.13043372375056014
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.1304307404310393
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.13034730316702947
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.13033371701398316
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.13033401159859123
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.1305886616178772
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.13063682087917022
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.13068269286304712
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.13067667853573095
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.13064933029902465
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.13063777261749124
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.13064146808337893
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.13058030434529214
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.13050944104790688
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.130515796952268
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.13060617660680277
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.1306831894321077
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.1307181372691346
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.1307137225925083
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.13062742601535962
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.13072086313442022
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.130782876621578
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.13071169237240443
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.13065130445692275
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.13061523478777454
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.1306922831341048
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.13066836826236453
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.13061593687878206
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.13065285733709597
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.13065648089096846
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.13072532307793075
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.13081479333746043
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.13078238867566513
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.13070210502357096
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.13075283854557176
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.130732358243036
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.13074630285156316
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.13083199298079
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.1308468067844709
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.13083451207568672
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.13081874147018008
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.13088610204596998
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.1308285397991027
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.1308204817732698
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.1308027523278877
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.13073095694886452
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.1308162560973404
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.13081518510201326
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.13073018675113654
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.1306709463616418
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.1306856616715436
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.13062552468294336
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.13061788312343828
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.13055638160843117
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.13054086824359795
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.13055175360368224
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.13065455452024785
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.130669760976346
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.13069312953496282
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.13064571541517672
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.13054155541457216
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.13057542008221448
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.13055381089224852
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.130512150619179
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.13051121960628656
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.13048307858963512
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.13043728293319493
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.13048723391672173
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.13052446062182202
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.13048797420092992
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.13049631238130152
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.1305577095744072
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.1305162252853727
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.13051315108087005
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.1304901317124529
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.1305396367877143
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.13055293671516183
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.13060153158750512
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.1306591731776674
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.13074090763425025
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.13072988421296616
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.13068757209766424
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.13075038584433762
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.1307075924461796
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.13076554291888257
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.13079454301375348
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.13071723648701436
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.13070310325414505
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.13069221920826857
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.13058414642519794
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.13060026209304149
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.13056012871457595
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.13047474014870214
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.130405238166798
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.13044892569567376
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.1304001010161031
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.13039831815429428
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.13045454978050175
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.1304221558844906
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.13052424822652012
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.13057830401225276
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.13054957456852748
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.13044043857782467
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.1304238070310517
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.1304416640444137
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.1304030325179456
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.13038159723542883
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.13035809367828005
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.1303531456696853
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.13036290868582212
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.13033131808375886
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.13038336362556688
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.13037694068024047
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.13035294661919275
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.13033024046545283
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.13037494248586945
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.1303111065927457
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.13030976025871768
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.1302632755794368
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.1302596052716437
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.13026758120344295
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.13029397573468465
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.130299376392806
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.13038178518738436
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.13036943740997295
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.1303628653255634
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.13029639321514133
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.13031539001405754
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.1303252954797078
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.13034047533322302
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.13036002817003323
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.13044340142773259
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.13049536322289185
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.13053941246359907
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.1305778955609682
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.13068697862011397
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.13068697862011397 valid 0.21373318135738373
LOSS train 0.13068697862011397 valid 0.18846625089645386
LOSS train 0.13068697862011397 valid 0.1807310233513514
LOSS train 0.13068697862011397 valid 0.17258724942803383
LOSS train 0.13068697862011397 valid 0.16816457808017732
LOSS train 0.13068697862011397 valid 0.17797684421141943
LOSS train 0.13068697862011397 valid 0.18830418373857225
LOSS train 0.13068697862011397 valid 0.18629565462470055
LOSS train 0.13068697862011397 valid 0.18570637537373436
LOSS train 0.13068697862011397 valid 0.18600992858409882
LOSS train 0.13068697862011397 valid 0.18495795266194778
LOSS train 0.13068697862011397 valid 0.18543769915898642
LOSS train 0.13068697862011397 valid 0.18501261335152847
LOSS train 0.13068697862011397 valid 0.18452119827270508
LOSS train 0.13068697862011397 valid 0.1822036623954773
LOSS train 0.13068697862011397 valid 0.18206535279750824
LOSS train 0.13068697862011397 valid 0.18286546538857854
LOSS train 0.13068697862011397 valid 0.181779682636261
LOSS train 0.13068697862011397 valid 0.1841205036953876
LOSS train 0.13068697862011397 valid 0.18384535908699035
LOSS train 0.13068697862011397 valid 0.18296624365307035
LOSS train 0.13068697862011397 valid 0.18154629319906235
LOSS train 0.13068697862011397 valid 0.1813159915416137
LOSS train 0.13068697862011397 valid 0.1814281977713108
LOSS train 0.13068697862011397 valid 0.1801735496520996
LOSS train 0.13068697862011397 valid 0.1800206979879966
LOSS train 0.13068697862011397 valid 0.18027785475607272
LOSS train 0.13068697862011397 valid 0.1799833242382322
LOSS train 0.13068697862011397 valid 0.17941768724342871
LOSS train 0.13068697862011397 valid 0.17992762674887974
LOSS train 0.13068697862011397 valid 0.18073750743942876
LOSS train 0.13068697862011397 valid 0.17991899931803346
LOSS train 0.13068697862011397 valid 0.18011066046628085
LOSS train 0.13068697862011397 valid 0.17958621154813206
LOSS train 0.13068697862011397 valid 0.18112467059067316
LOSS train 0.13068697862011397 valid 0.18091753787464565
LOSS train 0.13068697862011397 valid 0.1816319404421626
LOSS train 0.13068697862011397 valid 0.18203667904201307
LOSS train 0.13068697862011397 valid 0.18167537450790405
LOSS train 0.13068697862011397 valid 0.18136222288012505
LOSS train 0.13068697862011397 valid 0.18188406381665206
LOSS train 0.13068697862011397 valid 0.182122803514912
LOSS train 0.13068697862011397 valid 0.182148867914843
LOSS train 0.13068697862011397 valid 0.18260802091522652
LOSS train 0.13068697862011397 valid 0.18264827529589336
LOSS train 0.13068697862011397 valid 0.18298911465250928
LOSS train 0.13068697862011397 valid 0.1836405225256656
LOSS train 0.13068697862011397 valid 0.18348033322642246
LOSS train 0.13068697862011397 valid 0.18409367574720967
LOSS train 0.13068697862011397 valid 0.18359535425901413
LOSS train 0.13068697862011397 valid 0.18377899685326746
LOSS train 0.13068697862011397 valid 0.18329986958549574
LOSS train 0.13068697862011397 valid 0.18380337756759715
LOSS train 0.13068697862011397 valid 0.18393537098610843
LOSS train 0.13068697862011397 valid 0.18388632888143713
LOSS train 0.13068697862011397 valid 0.1831363226686205
LOSS train 0.13068697862011397 valid 0.18309348377219417
LOSS train 0.13068697862011397 valid 0.1829364104517575
LOSS train 0.13068697862011397 valid 0.18328091173858965
LOSS train 0.13068697862011397 valid 0.18315089518825214
LOSS train 0.13068697862011397 valid 0.18274006320804845
LOSS train 0.13068697862011397 valid 0.18325648384709511
LOSS train 0.13068697862011397 valid 0.18261521179524678
LOSS train 0.13068697862011397 valid 0.1835122392512858
LOSS train 0.13068697862011397 valid 0.1837439802976755
LOSS train 0.13068697862011397 valid 0.18352609195492484
LOSS train 0.13068697862011397 valid 0.18294574342556855
LOSS train 0.13068697862011397 valid 0.18294682200340665
LOSS train 0.13068697862011397 valid 0.1825309328842854
LOSS train 0.13068697862011397 valid 0.1830634553517614
LOSS train 0.13068697862011397 valid 0.18272289025111937
LOSS train 0.13068697862011397 valid 0.18293027538392279
LOSS train 0.13068697862011397 valid 0.18293177265010468
LOSS train 0.13068697862011397 valid 0.18294488759459676
LOSS train 0.13068697862011397 valid 0.18327921708424885
LOSS train 0.13068697862011397 valid 0.18367160484194756
LOSS train 0.13068697862011397 valid 0.1835118905677424
LOSS train 0.13068697862011397 valid 0.18332744790957525
LOSS train 0.13068697862011397 valid 0.18296359253080585
LOSS train 0.13068697862011397 valid 0.18233534879982471
LOSS train 0.13068697862011397 valid 0.18200114516564359
LOSS train 0.13068697862011397 valid 0.18244567549810176
LOSS train 0.13068697862011397 valid 0.18223127794552998
LOSS train 0.13068697862011397 valid 0.18223149187508084
LOSS train 0.13068697862011397 valid 0.18204401398406309
LOSS train 0.13068697862011397 valid 0.18184073667886647
LOSS train 0.13068697862011397 valid 0.18160533082896266
LOSS train 0.13068697862011397 valid 0.18131752955642613
LOSS train 0.13068697862011397 valid 0.1818616929348935
LOSS train 0.13068697862011397 valid 0.18185206717915006
LOSS train 0.13068697862011397 valid 0.18195859580249577
LOSS train 0.13068697862011397 valid 0.18192193624765976
LOSS train 0.13068697862011397 valid 0.18169811432079602
LOSS train 0.13068697862011397 valid 0.18189188267322295
LOSS train 0.13068697862011397 valid 0.18183822161272953
LOSS train 0.13068697862011397 valid 0.1819174375074605
LOSS train 0.13068697862011397 valid 0.18175857890512526
LOSS train 0.13068697862011397 valid 0.18195871126895047
LOSS train 0.13068697862011397 valid 0.18205508574692889
LOSS train 0.13068697862011397 valid 0.18214465707540511
LOSS train 0.13068697862011397 valid 0.18233422684197378
LOSS train 0.13068697862011397 valid 0.18253443448566922
LOSS train 0.13068697862011397 valid 0.18223735270569624
LOSS train 0.13068697862011397 valid 0.18238251856886423
LOSS train 0.13068697862011397 valid 0.18246395673070634
LOSS train 0.13068697862011397 valid 0.18290280733468398
LOSS train 0.13068697862011397 valid 0.18279713949310444
LOSS train 0.13068697862011397 valid 0.18295867506552627
LOSS train 0.13068697862011397 valid 0.18333851703263204
LOSS train 0.13068697862011397 valid 0.1834349740635265
LOSS train 0.13068697862011397 valid 0.1833861679644198
LOSS train 0.13068697862011397 valid 0.1832384073308536
LOSS train 0.13068697862011397 valid 0.18328254834740562
LOSS train 0.13068697862011397 valid 0.1835786729005345
LOSS train 0.13068697862011397 valid 0.1837873163430587
LOSS train 0.13068697862011397 valid 0.18398812361832323
LOSS train 0.13068697862011397 valid 0.18393082802112287
LOSS train 0.13068697862011397 valid 0.1835460813116219
LOSS train 0.13068697862011397 valid 0.18319816048405752
LOSS train 0.13068697862011397 valid 0.18285957736273606
LOSS train 0.13068697862011397 valid 0.1826967903651482
LOSS train 0.13068697862011397 valid 0.18273119433004348
LOSS train 0.13068697862011397 valid 0.18251917747462668
LOSS train 0.13068697862011397 valid 0.1828445917175662
LOSS train 0.13068697862011397 valid 0.1826917362213135
LOSS train 0.13068697862011397 valid 0.18307537857502226
LOSS train 0.13068697862011397 valid 0.18291885294313506
LOSS train 0.13068697862011397 valid 0.18297092511784285
LOSS train 0.13068697862011397 valid 0.1831245239852935
LOSS train 0.13068697862011397 valid 0.182761761202262
LOSS train 0.13068697862011397 valid 0.182465958572526
LOSS train 0.13068697862011397 valid 0.18214324816609873
LOSS train 0.13068697862011397 valid 0.18205292231606363
LOSS train 0.13068697862011397 valid 0.18218209627848952
LOSS train 0.13068697862011397 valid 0.1820695944406368
LOSS train 0.13068697862011397 valid 0.1820704594035359
LOSS train 0.13068697862011397 valid 0.18176328374521575
LOSS train 0.13068697862011397 valid 0.18170417704875919
LOSS train 0.13068697862011397 valid 0.1816511708412239
LOSS train 0.13068697862011397 valid 0.18174947074481418
LOSS train 0.13068697862011397 valid 0.18160246661368837
LOSS train 0.13068697862011397 valid 0.1816174324969171
LOSS train 0.13068697862011397 valid 0.18151984223119028
LOSS train 0.13068697862011397 valid 0.181523645710614
LOSS train 0.13068697862011397 valid 0.1813972813302073
LOSS train 0.13068697862011397 valid 0.18160959547513153
LOSS train 0.13068697862011397 valid 0.18153602604557867
LOSS train 0.13068697862011397 valid 0.1824226168965971
LOSS train 0.13068697862011397 valid 0.1825596831188906
LOSS train 0.13068697862011397 valid 0.18251170645157497
LOSS train 0.13068697862011397 valid 0.18273795983254515
LOSS train 0.13068697862011397 valid 0.1823998020079575
LOSS train 0.13068697862011397 valid 0.18234779281553878
LOSS train 0.13068697862011397 valid 0.18221500932009188
LOSS train 0.13068697862011397 valid 0.18212458933553388
LOSS train 0.13068697862011397 valid 0.1821512286670697
LOSS train 0.13068697862011397 valid 0.18223844933661687
LOSS train 0.13068697862011397 valid 0.18223736227690418
LOSS train 0.13068697862011397 valid 0.18231422102676248
LOSS train 0.13068697862011397 valid 0.18225155379623176
LOSS train 0.13068697862011397 valid 0.18211162349452142
LOSS train 0.13068697862011397 valid 0.18198636873268786
LOSS train 0.13068697862011397 valid 0.18171211647475424
LOSS train 0.13068697862011397 valid 0.1815379166566744
LOSS train 0.13068697862011397 valid 0.18137065975955038
LOSS train 0.13068697862011397 valid 0.18144898332027068
LOSS train 0.13068697862011397 valid 0.18177780571455013
LOSS train 0.13068697862011397 valid 0.18169057919155984
LOSS train 0.13068697862011397 valid 0.1818701907022465
LOSS train 0.13068697862011397 valid 0.18187838372062234
LOSS train 0.13068697862011397 valid 0.18189858013426352
LOSS train 0.13068697862011397 valid 0.18176738900500675
LOSS train 0.13068697862011397 valid 0.1817581843089506
LOSS train 0.13068697862011397 valid 0.18171457509542333
LOSS train 0.13068697862011397 valid 0.18139642596244812
LOSS train 0.13068697862011397 valid 0.18135479498993268
LOSS train 0.13068697862011397 valid 0.18144776406934707
LOSS train 0.13068697862011397 valid 0.1816882680306274
LOSS train 0.13068697862011397 valid 0.18159723914535353
LOSS train 0.13068697862011397 valid 0.18151406016614702
LOSS train 0.13068697862011397 valid 0.18165525859890722
LOSS train 0.13068697862011397 valid 0.18149829369324905
LOSS train 0.13068697862011397 valid 0.18156309344403732
LOSS train 0.13068697862011397 valid 0.18143999965294547
LOSS train 0.13068697862011397 valid 0.18130141271127237
LOSS train 0.13068697862011397 valid 0.1813367846351798
LOSS train 0.13068697862011397 valid 0.1811904345444817
LOSS train 0.13068697862011397 valid 0.1811807356140715
LOSS train 0.13068697862011397 valid 0.18104839064772166
LOSS train 0.13068697862011397 valid 0.18113865311208524
LOSS train 0.13068697862011397 valid 0.18102885533065696
LOSS train 0.13068697862011397 valid 0.18109107203781605
LOSS train 0.13068697862011397 valid 0.1808142265970843
LOSS train 0.13068697862011397 valid 0.1807253077318988
LOSS train 0.13068697862011397 valid 0.1805321277716221
LOSS train 0.13068697862011397 valid 0.18050007727377268
LOSS train 0.13068697862011397 valid 0.18069681328565337
LOSS train 0.13068697862011397 valid 0.1806512712830245
LOSS train 0.13068697862011397 valid 0.18077129799516956
LOSS train 0.13068697862011397 valid 0.18060588255524634
LOSS train 0.13068697862011397 valid 0.18041382722593657
LOSS train 0.13068697862011397 valid 0.18034596446126994
LOSS train 0.13068697862011397 valid 0.18034189278856286
LOSS train 0.13068697862011397 valid 0.18053754652832069
LOSS train 0.13068697862011397 valid 0.18030968133996172
LOSS train 0.13068697862011397 valid 0.18033518671121412
LOSS train 0.13068697862011397 valid 0.18027276074252843
LOSS train 0.13068697862011397 valid 0.18011648229394966
LOSS train 0.13068697862011397 valid 0.18011411054853046
LOSS train 0.13068697862011397 valid 0.18003420169864381
LOSS train 0.13068697862011397 valid 0.18003504253676716
LOSS train 0.13068697862011397 valid 0.17999215374858873
LOSS train 0.13068697862011397 valid 0.17995587802828758
LOSS train 0.13068697862011397 valid 0.17998949758638846
LOSS train 0.13068697862011397 valid 0.17986736990684687
LOSS train 0.13068697862011397 valid 0.1797362821245635
LOSS train 0.13068697862011397 valid 0.17967067885508736
LOSS train 0.13068697862011397 valid 0.1797023170858348
LOSS train 0.13068697862011397 valid 0.17980302169442722
LOSS train 0.13068697862011397 valid 0.17977533171122725
LOSS train 0.13068697862011397 valid 0.179687036108647
LOSS train 0.13068697862011397 valid 0.1797219688022459
LOSS train 0.13068697862011397 valid 0.17989336923098884
LOSS train 0.13068697862011397 valid 0.17999752065432922
LOSS train 0.13068697862011397 valid 0.18015392707453834
LOSS train 0.13068697862011397 valid 0.1804113144763803
LOSS train 0.13068697862011397 valid 0.1804761958148511
LOSS train 0.13068697862011397 valid 0.18053921227130973
LOSS train 0.13068697862011397 valid 0.1805461865064879
LOSS train 0.13068697862011397 valid 0.18059417637793915
LOSS train 0.13068697862011397 valid 0.1807382425446531
LOSS train 0.13068697862011397 valid 0.18075396146239905
LOSS train 0.13068697862011397 valid 0.18085402672382894
LOSS train 0.13068697862011397 valid 0.18091015454031464
LOSS train 0.13068697862011397 valid 0.18106599570588863
LOSS train 0.13068697862011397 valid 0.18094738092968018
LOSS train 0.13068697862011397 valid 0.1808927681627153
LOSS train 0.13068697862011397 valid 0.18092349478427103
LOSS train 0.13068697862011397 valid 0.1807275847161664
LOSS train 0.13068697862011397 valid 0.18072692193090917
LOSS train 0.13068697862011397 valid 0.1809179461348601
LOSS train 0.13068697862011397 valid 0.18075966520989237
LOSS train 0.13068697862011397 valid 0.1809679897111139
LOSS train 0.13068697862011397 valid 0.18115675076842308
LOSS train 0.13068697862011397 valid 0.18121758875798205
LOSS train 0.13068697862011397 valid 0.1810672906477277
LOSS train 0.13068697862011397 valid 0.1812225758788074
LOSS train 0.13068697862011397 valid 0.1811676461610102
LOSS train 0.13068697862011397 valid 0.1811750398342868
LOSS train 0.13068697862011397 valid 0.18121022260189057
LOSS train 0.13068697862011397 valid 0.1810507237080084
LOSS train 0.13068697862011397 valid 0.18131079051702742
LOSS train 0.13068697862011397 valid 0.18125555761482404
LOSS train 0.13068697862011397 valid 0.18114453061359134
LOSS train 0.13068697862011397 valid 0.1811892250589296
LOSS train 0.13068697862011397 valid 0.18126392544945702
LOSS train 0.13068697862011397 valid 0.1810792412623357
LOSS train 0.13068697862011397 valid 0.18133240372173545
LOSS train 0.13068697862011397 valid 0.1813402140232587
LOSS train 0.13068697862011397 valid 0.18119309773811929
LOSS train 0.13068697862011397 valid 0.18134789374367943
LOSS train 0.13068697862011397 valid 0.18141500361775623
LOSS train 0.13068697862011397 valid 0.181453655711145
LOSS train 0.13068697862011397 valid 0.18154549265675474
LOSS train 0.13068697862011397 valid 0.18149603273508683
LOSS train 0.13068697862011397 valid 0.18151896643011192
LOSS train 0.13068697862011397 valid 0.18156439975629585
LOSS train 0.13068697862011397 valid 0.1817610929928609
LOSS train 0.13068697862011397 valid 0.18185118300542513
LOSS train 0.13068697862011397 valid 0.18185101123871628
LOSS train 0.13068697862011397 valid 0.18194358259769383
LOSS train 0.13068697862011397 valid 0.18224882767261827
LOSS train 0.13068697862011397 valid 0.18245938316588
LOSS train 0.13068697862011397 valid 0.18250043162681762
LOSS train 0.13068697862011397 valid 0.18248505982485685
LOSS train 0.13068697862011397 valid 0.1824121544326561
LOSS train 0.13068697862011397 valid 0.18228995821536234
LOSS train 0.13068697862011397 valid 0.18213902982018834
LOSS train 0.13068697862011397 valid 0.18212434238216782
LOSS train 0.13068697862011397 valid 0.18211547790893487
LOSS train 0.13068697862011397 valid 0.1820410420567964
LOSS train 0.13068697862011397 valid 0.18174548198779425
LOSS train 0.13068697862011397 valid 0.18167643027886907
LOSS train 0.13068697862011397 valid 0.18169415915306186
LOSS train 0.13068697862011397 valid 0.1817360248482018
LOSS train 0.13068697862011397 valid 0.1817383479613524
LOSS train 0.13068697862011397 valid 0.18169746784175314
LOSS train 0.13068697862011397 valid 0.1816737860855129
LOSS train 0.13068697862011397 valid 0.18168798171525183
LOSS train 0.13068697862011397 valid 0.1817193475262872
LOSS train 0.13068697862011397 valid 0.18155731542413586
LOSS train 0.13068697862011397 valid 0.1815503848435944
LOSS train 0.13068697862011397 valid 0.18156388935985826
LOSS train 0.13068697862011397 valid 0.18164307963685924
LOSS train 0.13068697862011397 valid 0.18172336973376194
LOSS train 0.13068697862011397 valid 0.18164247158612754
LOSS train 0.13068697862011397 valid 0.18172873586717278
LOSS train 0.13068697862011397 valid 0.1817601776663089
LOSS train 0.13068697862011397 valid 0.1817983444219449
LOSS train 0.13068697862011397 valid 0.18188917264342308
LOSS train 0.13068697862011397 valid 0.18183799335528847
LOSS train 0.13068697862011397 valid 0.18178541420508695
LOSS train 0.13068697862011397 valid 0.1818377552154434
LOSS train 0.13068697862011397 valid 0.18183475164206406
LOSS train 0.13068697862011397 valid 0.1817699815406174
LOSS train 0.13068697862011397 valid 0.18180003932683297
LOSS train 0.13068697862011397 valid 0.1817798828554464
LOSS train 0.13068697862011397 valid 0.18168296476269696
LOSS train 0.13068697862011397 valid 0.18168506301143794
LOSS train 0.13068697862011397 valid 0.18173836897457799
LOSS train 0.13068697862011397 valid 0.18173390497540354
LOSS train 0.13068697862011397 valid 0.18171151211628547
LOSS train 0.13068697862011397 valid 0.18189596072934305
LOSS train 0.13068697862011397 valid 0.18190849909357204
LOSS train 0.13068697862011397 valid 0.18182894502367292
LOSS train 0.13068697862011397 valid 0.18179325910308694
LOSS train 0.13068697862011397 valid 0.18185802861719103
LOSS train 0.13068697862011397 valid 0.1818893054183924
LOSS train 0.13068697862011397 valid 0.1820424358859705
LOSS train 0.13068697862011397 valid 0.1819759282283485
LOSS train 0.13068697862011397 valid 0.18214519465823784
LOSS train 0.13068697862011397 valid 0.18213152061708227
LOSS train 0.13068697862011397 valid 0.18209073731773778
LOSS train 0.13068697862011397 valid 0.18220401743863834
LOSS train 0.13068697862011397 valid 0.18220503756633172
LOSS train 0.13068697862011397 valid 0.18236461898849055
LOSS train 0.13068697862011397 valid 0.18241386074538624
LOSS train 0.13068697862011397 valid 0.18237741477787495
LOSS train 0.13068697862011397 valid 0.18243475729390118
LOSS train 0.13068697862011397 valid 0.18242888003587723
LOSS train 0.13068697862011397 valid 0.18231695480757248
LOSS train 0.13068697862011397 valid 0.1821977100727788
LOSS train 0.13068697862011397 valid 0.18225393267544182
LOSS train 0.13068697862011397 valid 0.18233210678229075
LOSS train 0.13068697862011397 valid 0.1822820729728955
LOSS train 0.13068697862011397 valid 0.18235511372664145
LOSS train 0.13068697862011397 valid 0.18234827561795888
LOSS train 0.13068697862011397 valid 0.18234462793585818
LOSS train 0.13068697862011397 valid 0.18236220401648576
LOSS train 0.13068697862011397 valid 0.18230516226852642
LOSS train 0.13068697862011397 valid 0.18215020957929998
LOSS train 0.13068697862011397 valid 0.18215661522066384
LOSS train 0.13068697862011397 valid 0.1821956416868955
LOSS train 0.13068697862011397 valid 0.18242339889497258
LOSS train 0.13068697862011397 valid 0.1825279555890871
LOSS train 0.13068697862011397 valid 0.18261480228060242
LOSS train 0.13068697862011397 valid 0.18247013841822787
LOSS train 0.13068697862011397 valid 0.18240371843179068
LOSS train 0.13068697862011397 valid 0.1823948928312449
LOSS train 0.13068697862011397 valid 0.18232640206813813
LOSS train 0.13068697862011397 valid 0.1822457226932558
LOSS train 0.13068697862011397 valid 0.1822343039834364
LOSS train 0.13068697862011397 valid 0.18224495324298276
LOSS train 0.13068697862011397 valid 0.18226960065674647
LOSS train 0.13068697862011397 valid 0.1823290375336795
LOSS train 0.13068697862011397 valid 0.18232949115754513
LOSS train 0.13068697862011397 valid 0.1823657316987922
LOSS train 0.13068697862011397 valid 0.182263537492166
LOSS train 0.13068697862011397 valid 0.1822707343815429
LOSS train 0.13068697862011397 valid 0.1822222280005614
LOSS train 0.13068697862011397 valid 0.18217219759057432
LOSS train 0.13068697862011397 valid 0.18222502314418718
LOSS train 0.13068697862011397 valid 0.1822239810359708
LOSS train 0.13068697862011397 valid 0.1822052064177754
LOSS train 0.13068697862011397 valid 0.18225917183373072
LOSS train 0.13068697862011397 valid 0.18228277262768458
LOSS train 0.13068697862011397 valid 0.1821883916448832
LOSS train 0.13068697862011397 valid 0.18223089906994416
LOSS train 0.13068697862011397 valid 0.1823023729288804
EPOCH 13:
  batch 1 loss: 0.11791171878576279
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.1198924146592617
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.116072711845239
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.12390383146703243
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.12679416090250015
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.12836066509286562
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.12235607632568904
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.12415594048798084
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.12321574572059843
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.12238856554031372
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.12075401436198842
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.12011863725880782
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.12083821055980828
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.12050985172390938
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.11903645942608515
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.12074079224839807
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.11974066320587606
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.12268849627839194
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.1225625617723716
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.12202765680849552
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.12246462738230116
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.12273733013055542
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.12308586777552315
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.1248005252952377
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.12412693560123443
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.12440124383339515
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.12476588840837832
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.12427150670971189
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.12393637104281063
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.1247975175579389
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.12532517506230262
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.12592482892796397
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.1251272252111724
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.12601586869534323
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.127516394002097
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.12681701882845825
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.1267384735716356
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.12653201210655665
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.12701826084118623
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.1270462481305003
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.12715598595578495
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.12732883560515584
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.12835513696421025
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.12782993201505055
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.12812569737434387
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.12755179599575375
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.1274728901842807
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.12767373118549585
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.12771200677570033
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.1277682116627693
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.1274323038318578
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.1276284228437222
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.12740837389005805
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.1272285208106041
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.12766228101470253
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.1276896327201809
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.12767096412809273
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.12718036385445758
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.12755124776040094
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.12734680424133937
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.1274425660977598
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.1273214535847787
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.12742775560371458
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.12722899939399213
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.1273210698595414
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.12711819645130273
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.1268701958122538
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.12708676891291842
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.12737230135910754
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.12770944067410062
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.12804148104828847
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.12830231711268425
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.12836023246588774
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.12841025055260272
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.1284401669104894
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.128331127527513
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.1283279177430388
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.128008533651248
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.12844911396880693
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.1286286639980972
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.12869246736352827
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.12834128264973804
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.1281575419637094
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.1285581300478606
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.12860176922643887
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.12889610066316848
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.12913061978145576
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.12960276066918264
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.12955194770285253
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.12991046003169485
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.130063313131149
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.13002256506487078
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.130121740202109
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.13003357848588457
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.13051262388103888
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.1305942425193886
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.1304066626681495
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.13024072790024233
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.1305725783711732
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.1304135563969612
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.1302533882236717
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.1301087745848824
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.1298441627939928
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.1299335168531308
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.1295404517934436
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.12957896264094226
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.12988896584399393
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.1296492859169289
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.12976292090131603
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.13000912991437044
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.12975597341318387
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.12971790228039026
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.1294490689065604
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.12978689392146311
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.12958490064610606
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.12945157694148607
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.12983714254238668
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.12983774539020101
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.12972478393246145
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.12966524995863438
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.1296323769348712
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.12966371535277757
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.12979750916725252
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.12976947018215734
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.1298345320224762
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.12990060283078086
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.12996317082502712
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.129710836103186
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.1296323292939238
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.12966914704212776
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.12953577748009268
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.12952193149337263
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.129823699873641
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.1298963291320338
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.12968095077408684
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.12962751556187868
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.12978261888679796
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.12975671439283137
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.1299645066154089
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.13000650006745543
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.13000475953445367
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.1298669959143014
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.12995769474264626
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.13011245734782684
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.13000464321210467
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.13010138288548548
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.13012851870992556
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.13008010593821873
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.1304372191529146
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.13049931094050407
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.1303757277150817
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.13048512410176427
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.13046736635413825
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.13015812983760586
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.1302262162008593
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.13022439239116815
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.13027688006686558
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.13024732819463633
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.13038250832062848
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.13049735873937607
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.1306457812001246
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.13056822111945093
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.13061015326186923
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.13082338833227392
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.13096037579305245
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.13086940855326423
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.1307351990285034
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.13062080980411597
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.13042035297705576
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.13069951538653934
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.13073341727082494
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.13073393750156082
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.13053304409188343
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.1304648304036979
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.13059908436877388
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.13041007510301741
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.13019152473931933
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.12990333139896393
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.13008348677078438
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.12991664455168778
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.12999120556353205
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.12996710390671268
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.12984109038863678
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.1297851823594259
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.12981368077767863
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.12975322847725243
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.1298237903551622
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.1297355229629481
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.12968874963187665
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.1297318994998932
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.12974877273225036
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.1296770138045152
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.12973207860721825
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.1294944832850363
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.12951921014449536
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.12964851215329706
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.12973132476134955
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.1298684709978224
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.12980441198726395
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.12994267713278532
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.1299518074339895
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.1300034931316824
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.13002722600263916
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.13006454702539771
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.13001076339221582
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.12987791877059104
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.12985134045570945
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.1296862123820644
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.12979876503134458
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.12990676271063942
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.12989780553991762
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.12993158168106708
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.13000943186417432
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.12979151106604905
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.1298737503761469
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.12976461652390384
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.12974506111875658
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.1298389859634255
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.12989166634131785
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.1298969917338003
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.1298796652196759
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.12990230687700952
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.12985469751694811
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.1299401359033904
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.12984501673115625
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.129869861742564
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.1299745858634621
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.12981003607835687
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.1297618337138251
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.1299920940204807
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.13022515419628714
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.13011866355122165
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.1301733178961942
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.1301511215030128
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.1301468959197085
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.13017429622932006
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.13013666050846567
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.13016897821877182
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.1301701705450792
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.1300576944525043
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.13016457293043493
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.13011827052009006
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.13017857335354566
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.13020580155072642
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.13018737243754522
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.13026754997246634
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.13025922901355302
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.13033464764274896
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.13038288209452686
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.1303446380496025
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.13052632211451512
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.13055134824817144
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.13058526460596695
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.1305593797775704
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.13051368395487467
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.13050741201732308
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.13042456897316276
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.13035609913888827
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.13041476439325045
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.13046441702888562
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.13034880212668715
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.13027226475586418
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.13023984993138693
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.13010966687491446
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.130106206322616
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.13014581340148038
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.1301202315851097
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.1301324525431021
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.13004621362043578
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.13006740380768422
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.13007058915286926
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.13003159297958894
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.13014270758235846
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.13001570619479583
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.1299105667797002
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.13010710863855438
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.1301047347674301
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.13013514879045726
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.13008136320925956
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.13009086927132948
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.13005590908149808
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.12998157575832192
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.12996262846376366
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.12992754867169218
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.13004750572798546
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.13015864049638068
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.1300806482338739
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.13007124190011787
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.1300245910934511
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.1300535451492359
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.13005540253677728
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.13017176349379428
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.13034448269182505
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.13037812494400408
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.1303445852661537
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.1303790045298032
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.13036134896756021
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.13032225596924757
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.1302779457192357
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.13025174709657827
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.13029206806540092
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.13029382266844344
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.13028359936900658
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.1302499422852538
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.1301953203365451
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.1301962260424701
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.13025022475261253
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.13021285014299602
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.13016035810450519
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.1300680099235427
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.1301339979005013
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.13009378887139833
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.13003361798799076
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.13001721708258246
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.13003391036911616
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.1301538344897047
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.13013469456028112
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.13011151781809405
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.13002640125706652
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.13012738316319883
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.13006937687177897
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.13001447221877413
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.13002406798833663
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.12993625636545963
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.12994881256268576
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.12986332978024803
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.12974910125911054
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.129745549976644
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.12978792238108655
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.12985047813166273
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.12988324866284057
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.12987435732530542
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.12980578264764123
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.12982455402345
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.1299005506866014
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.1299081146938815
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.12989194151732616
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.12990127799249965
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.12981879988075357
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.12981146872043609
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.12982648353248047
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.13007466054973546
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.13012532603115104
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.1301643592846948
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.13016781690328016
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.13013596074774086
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.13013087586642
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.13012648142617325
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.13005810888500133
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.12998520084789822
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.12996963619591503
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.1300615428481251
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.13016259034953104
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.13019202634860566
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.1301822337256351
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.13010009034965817
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.13018362128100142
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.1302544832562601
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.1301715114156516
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.1301236723860105
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.1300932399907931
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.13016755778074923
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.13015355557695893
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.13011832410408244
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.13014887344755538
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.13014162455995879
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.13021625568252818
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.1303283912534623
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.13028936495948937
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.13020871003737322
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.13025979930660475
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.13024684630574718
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.130268107350646
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.13035590289270177
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.1303901263078054
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.13038052582835896
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.13036209323124798
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.13043229977684045
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.13036533417597923
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.13035117297580368
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.13032893646811564
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.13026995044104092
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.13034370475431648
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.13034618277257928
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.13026756516524723
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.13021438702014443
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.13024122106243474
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.13019592785420492
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.13019441740448492
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.13014161785443623
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.13011935002663555
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.13012569572548477
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.13022554174786002
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.13023805187135784
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.13026159360438963
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.1301920194028303
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.13009122524513705
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.13012038717916863
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.13010610804372563
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.13006843123584985
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.13006398817547538
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.13003487606991582
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.12999133388546502
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.13003266824058968
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.13006469703014986
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.13001735688283525
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.13003046759369918
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.13007876612976485
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.13004087230468436
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.13003835160194374
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.13003297735667288
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.13009040301792252
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.13011455656757656
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.1301704812704941
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.13022836049996225
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.1303121987551164
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.13029650277156624
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.13026161254806953
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.1303125908131531
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.13027367668137663
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.13032902913464503
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.1303522890318909
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.13026906368509816
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.13025957414493808
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.13023711062529508
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.1301403523470874
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.13014615286987893
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.1301067317618388
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.1300175047559894
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.12994986841844958
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.12999267928008415
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.12995364886053182
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.12995473142308947
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.1300050951025453
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.12997300153833696
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.13006740041213846
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.13011275557481453
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.13008153600167466
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.1299729579401994
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.12994626543738624
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.12996493219112865
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.1299253809876841
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.129903943792961
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.12987868788207438
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.12988546961144115
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.12989428570920042
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.12986753714297974
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.1299215598763632
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.12990714694250932
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.12988531675603654
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.12986044356074936
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.12990048328266207
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.1298378139043486
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.1298398283445625
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.12979104052205662
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.12979761280707622
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.12979531498458766
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.129817961751764
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.12982239236564158
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.12988873116024163
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.12987741451395307
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.1298723179379325
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.1298074653577341
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.12982581156016937
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.12983667573300742
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.12986209005309277
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.1298839286990799
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.12997096246824816
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.1300213289762865
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.1300721860470924
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.1301089141304326
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.13021385677600816
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.13021385677600816 valid 0.2130051702260971
LOSS train 0.13021385677600816 valid 0.1875060275197029
LOSS train 0.13021385677600816 valid 0.18009276191393533
LOSS train 0.13021385677600816 valid 0.17157414555549622
LOSS train 0.13021385677600816 valid 0.16708756983280182
LOSS train 0.13021385677600816 valid 0.17695756504933038
LOSS train 0.13021385677600816 valid 0.1874197529894965
LOSS train 0.13021385677600816 valid 0.18535695411264896
LOSS train 0.13021385677600816 valid 0.1847682942946752
LOSS train 0.13021385677600816 valid 0.18500062376260756
LOSS train 0.13021385677600816 valid 0.18386240168051285
LOSS train 0.13021385677600816 valid 0.1844864971935749
LOSS train 0.13021385677600816 valid 0.18399451902279487
LOSS train 0.13021385677600816 valid 0.1834999876362937
LOSS train 0.13021385677600816 valid 0.18117786049842835
LOSS train 0.13021385677600816 valid 0.18102813698351383
LOSS train 0.13021385677600816 valid 0.18177925137912526
LOSS train 0.13021385677600816 valid 0.1806589580244488
LOSS train 0.13021385677600816 valid 0.18298891735704323
LOSS train 0.13021385677600816 valid 0.1827148973941803
LOSS train 0.13021385677600816 valid 0.18182135479790823
LOSS train 0.13021385677600816 valid 0.1804001432928172
LOSS train 0.13021385677600816 valid 0.180159836359646
LOSS train 0.13021385677600816 valid 0.18031585899492106
LOSS train 0.13021385677600816 valid 0.17904350221157073
LOSS train 0.13021385677600816 valid 0.17889881019408888
LOSS train 0.13021385677600816 valid 0.17917002075248295
LOSS train 0.13021385677600816 valid 0.17887208876865252
LOSS train 0.13021385677600816 valid 0.17831393202831006
LOSS train 0.13021385677600816 valid 0.17882396280765533
LOSS train 0.13021385677600816 valid 0.17964691452441678
LOSS train 0.13021385677600816 valid 0.17881720745936036
LOSS train 0.13021385677600816 valid 0.17902538857676767
LOSS train 0.13021385677600816 valid 0.17850566304781856
LOSS train 0.13021385677600816 valid 0.18008543465818677
LOSS train 0.13021385677600816 valid 0.17988952704601818
LOSS train 0.13021385677600816 valid 0.18059578578214389
LOSS train 0.13021385677600816 valid 0.1810227275678986
LOSS train 0.13021385677600816 valid 0.18066216623171782
LOSS train 0.13021385677600816 valid 0.18033385053277015
LOSS train 0.13021385677600816 valid 0.18086364211105718
LOSS train 0.13021385677600816 valid 0.1811151018454915
LOSS train 0.13021385677600816 valid 0.1811155292183854
LOSS train 0.13021385677600816 valid 0.1815708178010854
LOSS train 0.13021385677600816 valid 0.18163829214043087
LOSS train 0.13021385677600816 valid 0.18198263839535092
LOSS train 0.13021385677600816 valid 0.18261567202020199
LOSS train 0.13021385677600816 valid 0.18244056838254133
LOSS train 0.13021385677600816 valid 0.1830408913748605
LOSS train 0.13021385677600816 valid 0.1825340670347214
LOSS train 0.13021385677600816 valid 0.182719875200122
LOSS train 0.13021385677600816 valid 0.18221975375826543
LOSS train 0.13021385677600816 valid 0.18273572230114127
LOSS train 0.13021385677600816 valid 0.1828728966690876
LOSS train 0.13021385677600816 valid 0.18281997415152462
LOSS train 0.13021385677600816 valid 0.18207311337547644
LOSS train 0.13021385677600816 valid 0.1820424996447145
LOSS train 0.13021385677600816 valid 0.18188603273753462
LOSS train 0.13021385677600816 valid 0.18222626563856156
LOSS train 0.13021385677600816 valid 0.18209488640228907
LOSS train 0.13021385677600816 valid 0.18168572104368053
LOSS train 0.13021385677600816 valid 0.18219976271352462
LOSS train 0.13021385677600816 valid 0.18155760850225175
LOSS train 0.13021385677600816 valid 0.18245749222114682
LOSS train 0.13021385677600816 valid 0.18268852508985078
LOSS train 0.13021385677600816 valid 0.18246212353308996
LOSS train 0.13021385677600816 valid 0.18187518595759547
LOSS train 0.13021385677600816 valid 0.18188480989021413
LOSS train 0.13021385677600816 valid 0.18147577647713647
LOSS train 0.13021385677600816 valid 0.18201868555375508
LOSS train 0.13021385677600816 valid 0.18167598335675791
LOSS train 0.13021385677600816 valid 0.18188139216767418
LOSS train 0.13021385677600816 valid 0.18187836241232205
LOSS train 0.13021385677600816 valid 0.1818892945711677
LOSS train 0.13021385677600816 valid 0.18223003566265106
LOSS train 0.13021385677600816 valid 0.18262905117712522
LOSS train 0.13021385677600816 valid 0.18246613133263279
LOSS train 0.13021385677600816 valid 0.1822738821307818
LOSS train 0.13021385677600816 valid 0.18191558292394952
LOSS train 0.13021385677600816 valid 0.1812783855944872
LOSS train 0.13021385677600816 valid 0.18093931123062415
LOSS train 0.13021385677600816 valid 0.181387821712145
LOSS train 0.13021385677600816 valid 0.18116996733539076
LOSS train 0.13021385677600816 valid 0.18117201487932885
LOSS train 0.13021385677600816 valid 0.180973818898201
LOSS train 0.13021385677600816 valid 0.18077594059151272
LOSS train 0.13021385677600816 valid 0.18054793638059463
LOSS train 0.13021385677600816 valid 0.18025832470845093
LOSS train 0.13021385677600816 valid 0.18080233405815083
LOSS train 0.13021385677600816 valid 0.18078715668784248
LOSS train 0.13021385677600816 valid 0.1808907111267467
LOSS train 0.13021385677600816 valid 0.18084931924291278
LOSS train 0.13021385677600816 valid 0.18062880699352551
LOSS train 0.13021385677600816 valid 0.18082651828831814
LOSS train 0.13021385677600816 valid 0.18078032512413827
LOSS train 0.13021385677600816 valid 0.18085699699198207
LOSS train 0.13021385677600816 valid 0.1806972490758011
LOSS train 0.13021385677600816 valid 0.18089928538823613
LOSS train 0.13021385677600816 valid 0.18098927507496843
LOSS train 0.13021385677600816 valid 0.1810743933916092
LOSS train 0.13021385677600816 valid 0.18126405583749902
LOSS train 0.13021385677600816 valid 0.18147111304250418
LOSS train 0.13021385677600816 valid 0.181162881330379
LOSS train 0.13021385677600816 valid 0.18131331712580645
LOSS train 0.13021385677600816 valid 0.18139668717270807
LOSS train 0.13021385677600816 valid 0.1818388246421544
LOSS train 0.13021385677600816 valid 0.18173170939227132
LOSS train 0.13021385677600816 valid 0.18189116939902306
LOSS train 0.13021385677600816 valid 0.18227626861782248
LOSS train 0.13021385677600816 valid 0.18236607204784047
LOSS train 0.13021385677600816 valid 0.18231368266247414
LOSS train 0.13021385677600816 valid 0.1821620694494673
LOSS train 0.13021385677600816 valid 0.18221534129265135
LOSS train 0.13021385677600816 valid 0.1825121356207028
LOSS train 0.13021385677600816 valid 0.1827210739902828
LOSS train 0.13021385677600816 valid 0.18292779963592004
LOSS train 0.13021385677600816 valid 0.18286574689241555
LOSS train 0.13021385677600816 valid 0.18248089857525746
LOSS train 0.13021385677600816 valid 0.18213282124835903
LOSS train 0.13021385677600816 valid 0.18178968392312528
LOSS train 0.13021385677600816 valid 0.18162401660907368
LOSS train 0.13021385677600816 valid 0.1816590469880182
LOSS train 0.13021385677600816 valid 0.18144363382967507
LOSS train 0.13021385677600816 valid 0.18177457213882478
LOSS train 0.13021385677600816 valid 0.18161782109737395
LOSS train 0.13021385677600816 valid 0.18200095849377768
LOSS train 0.13021385677600816 valid 0.18184201055624355
LOSS train 0.13021385677600816 valid 0.18189237860497087
LOSS train 0.13021385677600816 valid 0.18204352721687436
LOSS train 0.13021385677600816 valid 0.1816761634670771
LOSS train 0.13021385677600816 valid 0.18137697568376557
LOSS train 0.13021385677600816 valid 0.1810540370643139
LOSS train 0.13021385677600816 valid 0.18096022856862923
LOSS train 0.13021385677600816 valid 0.18109363751180135
LOSS train 0.13021385677600816 valid 0.18097817621849202
LOSS train 0.13021385677600816 valid 0.1809781823745545
LOSS train 0.13021385677600816 valid 0.18066735271989864
LOSS train 0.13021385677600816 valid 0.18060457015383072
LOSS train 0.13021385677600816 valid 0.18054745362388144
LOSS train 0.13021385677600816 valid 0.18064615226217678
LOSS train 0.13021385677600816 valid 0.1804989739089993
LOSS train 0.13021385677600816 valid 0.18051408734959615
LOSS train 0.13021385677600816 valid 0.18041041430893479
LOSS train 0.13021385677600816 valid 0.18041605843851963
LOSS train 0.13021385677600816 valid 0.18029083085471187
LOSS train 0.13021385677600816 valid 0.18050239918983146
LOSS train 0.13021385677600816 valid 0.1804265648532076
LOSS train 0.13021385677600816 valid 0.18131908965674606
LOSS train 0.13021385677600816 valid 0.18145943388042834
LOSS train 0.13021385677600816 valid 0.18141156087319057
LOSS train 0.13021385677600816 valid 0.18164409499689443
LOSS train 0.13021385677600816 valid 0.18130380415210598
LOSS train 0.13021385677600816 valid 0.18124794852889442
LOSS train 0.13021385677600816 valid 0.18110953377825872
LOSS train 0.13021385677600816 valid 0.1810153965988467
LOSS train 0.13021385677600816 valid 0.18104391697889718
LOSS train 0.13021385677600816 valid 0.18113122833002904
LOSS train 0.13021385677600816 valid 0.18112968125298054
LOSS train 0.13021385677600816 valid 0.1812134453523084
LOSS train 0.13021385677600816 valid 0.18114819936454296
LOSS train 0.13021385677600816 valid 0.18100650488219647
LOSS train 0.13021385677600816 valid 0.18088292293710473
LOSS train 0.13021385677600816 valid 0.18060642160886636
LOSS train 0.13021385677600816 valid 0.18043084506218027
LOSS train 0.13021385677600816 valid 0.1802657113833861
LOSS train 0.13021385677600816 valid 0.18034589694566036
LOSS train 0.13021385677600816 valid 0.18068004109545382
LOSS train 0.13021385677600816 valid 0.18058776074931734
LOSS train 0.13021385677600816 valid 0.18076761737024996
LOSS train 0.13021385677600816 valid 0.1807740594534313
LOSS train 0.13021385677600816 valid 0.18079653460728495
LOSS train 0.13021385677600816 valid 0.18066179735022922
LOSS train 0.13021385677600816 valid 0.18064603777979152
LOSS train 0.13021385677600816 valid 0.18060131705012814
LOSS train 0.13021385677600816 valid 0.18028358336005892
LOSS train 0.13021385677600816 valid 0.18024206351996822
LOSS train 0.13021385677600816 valid 0.1803312416551477
LOSS train 0.13021385677600816 valid 0.18057365644346463
LOSS train 0.13021385677600816 valid 0.1804815947426764
LOSS train 0.13021385677600816 valid 0.18039515337182416
LOSS train 0.13021385677600816 valid 0.18053475075828437
LOSS train 0.13021385677600816 valid 0.18037612895880425
LOSS train 0.13021385677600816 valid 0.18044038804990997
LOSS train 0.13021385677600816 valid 0.18031391975186442
LOSS train 0.13021385677600816 valid 0.18017985027384115
LOSS train 0.13021385677600816 valid 0.18021435270546585
LOSS train 0.13021385677600816 valid 0.18006544137861638
LOSS train 0.13021385677600816 valid 0.18005572596287472
LOSS train 0.13021385677600816 valid 0.17992029220811903
LOSS train 0.13021385677600816 valid 0.18001129168428873
LOSS train 0.13021385677600816 valid 0.17989316750415332
LOSS train 0.13021385677600816 valid 0.17995534057263285
LOSS train 0.13021385677600816 valid 0.1796760508510733
LOSS train 0.13021385677600816 valid 0.17959314795959855
LOSS train 0.13021385677600816 valid 0.1793962944394503
LOSS train 0.13021385677600816 valid 0.1793623855907698
LOSS train 0.13021385677600816 valid 0.17956170897344648
LOSS train 0.13021385677600816 valid 0.1795137342855786
LOSS train 0.13021385677600816 valid 0.1796352098410453
LOSS train 0.13021385677600816 valid 0.17947077449411153
LOSS train 0.13021385677600816 valid 0.17927830859054975
LOSS train 0.13021385677600816 valid 0.17920745846511113
LOSS train 0.13021385677600816 valid 0.17920706582627274
LOSS train 0.13021385677600816 valid 0.1794005033067044
LOSS train 0.13021385677600816 valid 0.17917085344471584
LOSS train 0.13021385677600816 valid 0.1791928638749331
LOSS train 0.13021385677600816 valid 0.17912667971734264
LOSS train 0.13021385677600816 valid 0.17897056154190347
LOSS train 0.13021385677600816 valid 0.17896973762358204
LOSS train 0.13021385677600816 valid 0.17889236506252063
LOSS train 0.13021385677600816 valid 0.17889440310368607
LOSS train 0.13021385677600816 valid 0.17884904887738093
LOSS train 0.13021385677600816 valid 0.17881099694351635
LOSS train 0.13021385677600816 valid 0.17884204846537002
LOSS train 0.13021385677600816 valid 0.17871916921332826
LOSS train 0.13021385677600816 valid 0.17858633554230133
LOSS train 0.13021385677600816 valid 0.17851903001146932
LOSS train 0.13021385677600816 valid 0.17855480938739732
LOSS train 0.13021385677600816 valid 0.1786543900767962
LOSS train 0.13021385677600816 valid 0.1786202172325416
LOSS train 0.13021385677600816 valid 0.17853096002771843
LOSS train 0.13021385677600816 valid 0.17856312009531097
LOSS train 0.13021385677600816 valid 0.17873934756613633
LOSS train 0.13021385677600816 valid 0.17884631205483206
LOSS train 0.13021385677600816 valid 0.1790075566040145
LOSS train 0.13021385677600816 valid 0.1792679059492276
LOSS train 0.13021385677600816 valid 0.1793328893539139
LOSS train 0.13021385677600816 valid 0.17939686588943005
LOSS train 0.13021385677600816 valid 0.179402568031867
LOSS train 0.13021385677600816 valid 0.17945005715541218
LOSS train 0.13021385677600816 valid 0.17959809783862266
LOSS train 0.13021385677600816 valid 0.179613563745957
LOSS train 0.13021385677600816 valid 0.17971408517447665
LOSS train 0.13021385677600816 valid 0.17977361027628946
LOSS train 0.13021385677600816 valid 0.17993296692346006
LOSS train 0.13021385677600816 valid 0.17981388126263173
LOSS train 0.13021385677600816 valid 0.1797559855796617
LOSS train 0.13021385677600816 valid 0.17978661372756757
LOSS train 0.13021385677600816 valid 0.17958601720038816
LOSS train 0.13021385677600816 valid 0.17958588808154066
LOSS train 0.13021385677600816 valid 0.17977988420062047
LOSS train 0.13021385677600816 valid 0.17962342078897578
LOSS train 0.13021385677600816 valid 0.17983684639749212
LOSS train 0.13021385677600816 valid 0.18002801422090803
LOSS train 0.13021385677600816 valid 0.18008765684706823
LOSS train 0.13021385677600816 valid 0.1799369731509104
LOSS train 0.13021385677600816 valid 0.18009116669536118
LOSS train 0.13021385677600816 valid 0.18003472111999028
LOSS train 0.13021385677600816 valid 0.18004082325830995
LOSS train 0.13021385677600816 valid 0.1800721386373043
LOSS train 0.13021385677600816 valid 0.17990978397577406
LOSS train 0.13021385677600816 valid 0.18016749848094252
LOSS train 0.13021385677600816 valid 0.18010914234422412
LOSS train 0.13021385677600816 valid 0.1799969509771959
LOSS train 0.13021385677600816 valid 0.18004139933515997
LOSS train 0.13021385677600816 valid 0.18011204621871002
LOSS train 0.13021385677600816 valid 0.17992796812655862
LOSS train 0.13021385677600816 valid 0.18018379683295885
LOSS train 0.13021385677600816 valid 0.18019392398908793
LOSS train 0.13021385677600816 valid 0.18004468063322399
LOSS train 0.13021385677600816 valid 0.18020303493144413
LOSS train 0.13021385677600816 valid 0.18027186043958626
LOSS train 0.13021385677600816 valid 0.18031092466850243
LOSS train 0.13021385677600816 valid 0.18040262916210023
LOSS train 0.13021385677600816 valid 0.18035591961640232
LOSS train 0.13021385677600816 valid 0.18037899138223856
LOSS train 0.13021385677600816 valid 0.1804235332300154
LOSS train 0.13021385677600816 valid 0.18062180144462123
LOSS train 0.13021385677600816 valid 0.18071255877226258
LOSS train 0.13021385677600816 valid 0.18071205949893704
LOSS train 0.13021385677600816 valid 0.18080706627716436
LOSS train 0.13021385677600816 valid 0.1811142907039646
LOSS train 0.13021385677600816 valid 0.1813255276926708
LOSS train 0.13021385677600816 valid 0.18136702774323687
LOSS train 0.13021385677600816 valid 0.18135107704184272
LOSS train 0.13021385677600816 valid 0.18128054111224154
LOSS train 0.13021385677600816 valid 0.18116162429540167
LOSS train 0.13021385677600816 valid 0.1810120115552446
LOSS train 0.13021385677600816 valid 0.18099638934524256
LOSS train 0.13021385677600816 valid 0.18098611467118775
LOSS train 0.13021385677600816 valid 0.18091302603695317
LOSS train 0.13021385677600816 valid 0.18061602419466838
LOSS train 0.13021385677600816 valid 0.18054546502685379
LOSS train 0.13021385677600816 valid 0.18056406555566148
LOSS train 0.13021385677600816 valid 0.18060529130069833
LOSS train 0.13021385677600816 valid 0.18061384457078847
LOSS train 0.13021385677600816 valid 0.1805740127839693
LOSS train 0.13021385677600816 valid 0.18054932120463085
LOSS train 0.13021385677600816 valid 0.18056194770398024
LOSS train 0.13021385677600816 valid 0.18059646279133598
LOSS train 0.13021385677600816 valid 0.18043343904604206
LOSS train 0.13021385677600816 valid 0.18042498094680376
LOSS train 0.13021385677600816 valid 0.18043629654743565
LOSS train 0.13021385677600816 valid 0.1805147159413821
LOSS train 0.13021385677600816 valid 0.18059683449692646
LOSS train 0.13021385677600816 valid 0.18051556802379923
LOSS train 0.13021385677600816 valid 0.18060429742942355
LOSS train 0.13021385677600816 valid 0.18063522017742165
LOSS train 0.13021385677600816 valid 0.1806726304434215
LOSS train 0.13021385677600816 valid 0.18076527240375678
LOSS train 0.13021385677600816 valid 0.18071451760902754
LOSS train 0.13021385677600816 valid 0.18066164676026003
LOSS train 0.13021385677600816 valid 0.18071203024768986
LOSS train 0.13021385677600816 valid 0.1807068164558395
LOSS train 0.13021385677600816 valid 0.1806411097039942
LOSS train 0.13021385677600816 valid 0.1806737267065282
LOSS train 0.13021385677600816 valid 0.18065310939418378
LOSS train 0.13021385677600816 valid 0.18056048769745733
LOSS train 0.13021385677600816 valid 0.1805619381151153
LOSS train 0.13021385677600816 valid 0.18061420573822914
LOSS train 0.13021385677600816 valid 0.18060823879728746
LOSS train 0.13021385677600816 valid 0.1805868759416999
LOSS train 0.13021385677600816 valid 0.18077107399892503
LOSS train 0.13021385677600816 valid 0.18078222351184317
LOSS train 0.13021385677600816 valid 0.18070266253891445
LOSS train 0.13021385677600816 valid 0.1806682447042269
LOSS train 0.13021385677600816 valid 0.18073431369540066
LOSS train 0.13021385677600816 valid 0.18076660669168587
LOSS train 0.13021385677600816 valid 0.18092176231751247
LOSS train 0.13021385677600816 valid 0.1808557943208143
LOSS train 0.13021385677600816 valid 0.18102919212847113
LOSS train 0.13021385677600816 valid 0.18101545916118236
LOSS train 0.13021385677600816 valid 0.18097644972819663
LOSS train 0.13021385677600816 valid 0.18109092417598507
LOSS train 0.13021385677600816 valid 0.18109258090074246
LOSS train 0.13021385677600816 valid 0.18125335449364288
LOSS train 0.13021385677600816 valid 0.18130193293003496
LOSS train 0.13021385677600816 valid 0.18126330695046886
LOSS train 0.13021385677600816 valid 0.18131899505310145
LOSS train 0.13021385677600816 valid 0.1813116040870999
LOSS train 0.13021385677600816 valid 0.1811972320124822
LOSS train 0.13021385677600816 valid 0.1810776521507875
LOSS train 0.13021385677600816 valid 0.1811334501739379
LOSS train 0.13021385677600816 valid 0.18121205263598236
LOSS train 0.13021385677600816 valid 0.1811641735594664
LOSS train 0.13021385677600816 valid 0.1812378670798526
LOSS train 0.13021385677600816 valid 0.18123226082201174
LOSS train 0.13021385677600816 valid 0.18122889169839007
LOSS train 0.13021385677600816 valid 0.18124771045636287
LOSS train 0.13021385677600816 valid 0.18118867039242212
LOSS train 0.13021385677600816 valid 0.18103308785608438
LOSS train 0.13021385677600816 valid 0.1810398174787474
LOSS train 0.13021385677600816 valid 0.18107881214545699
LOSS train 0.13021385677600816 valid 0.18131187222464834
LOSS train 0.13021385677600816 valid 0.18141853802877925
LOSS train 0.13021385677600816 valid 0.18150658287495547
LOSS train 0.13021385677600816 valid 0.1813604060227658
LOSS train 0.13021385677600816 valid 0.1812931418889898
LOSS train 0.13021385677600816 valid 0.18128295923799362
LOSS train 0.13021385677600816 valid 0.18121356365936142
LOSS train 0.13021385677600816 valid 0.18113299433746907
LOSS train 0.13021385677600816 valid 0.18112426105124707
LOSS train 0.13021385677600816 valid 0.1811361212610523
LOSS train 0.13021385677600816 valid 0.18116058710183802
LOSS train 0.13021385677600816 valid 0.18122038029029336
LOSS train 0.13021385677600816 valid 0.18121846365543565
LOSS train 0.13021385677600816 valid 0.18125381137059182
LOSS train 0.13021385677600816 valid 0.1811509163810887
LOSS train 0.13021385677600816 valid 0.18115783242046998
LOSS train 0.13021385677600816 valid 0.18110888093296024
LOSS train 0.13021385677600816 valid 0.18105654432915586
LOSS train 0.13021385677600816 valid 0.18110910228849775
LOSS train 0.13021385677600816 valid 0.18110827849177288
LOSS train 0.13021385677600816 valid 0.18108898726711561
LOSS train 0.13021385677600816 valid 0.18114408358727416
LOSS train 0.13021385677600816 valid 0.1811674783016489
LOSS train 0.13021385677600816 valid 0.18107379089827758
LOSS train 0.13021385677600816 valid 0.18111607746180633
LOSS train 0.13021385677600816 valid 0.18118913005280302
EPOCH 14:
  batch 1 loss: 0.11611605435609818
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.11688404157757759
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.1153861532608668
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.12337281182408333
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.12619673013687133
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.12725640336672464
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.12158066779375076
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.12340434733778238
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.12282321602106094
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.12253330126404763
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.12086444483561949
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.12008361083765824
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.12094750484594932
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.12072887111987386
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.11938528468211491
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.12086508190259337
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.11987418418421465
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.12289642915129662
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.12279955219281347
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.12241280116140843
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.12288488625060945
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.1233414455570958
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.12360153671192087
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.12535306718200445
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.1245945417881012
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.12478382197710183
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.12527749439080557
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.12480682640203408
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.12434072998063318
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.12505626678466797
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.12551002252486446
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.1259183376096189
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.1250894692811099
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.12584994645679698
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.12748492232390812
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.12677262806230122
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.12675309624220873
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.12644810111899124
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.12693282541556236
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.12689626403152943
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.12703697637813846
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.12724451188530242
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.12828267798867338
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.1276914800771258
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.12783092475599714
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.12731380857851193
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.1272186836663713
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.1272710586587588
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.1272573550136722
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.12738878518342972
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.12700612477812112
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.12724746835346407
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.12707167624864937
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.12690453562471601
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.12729845372113316
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.12747217209211417
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.12765486930546008
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.12714550078942857
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.12752670605303879
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.12735170274972915
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.12747261905279317
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.12737960740923882
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.12753002688525214
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.12729319755453616
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.127369475708558
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.1272008687709317
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.12701731597754493
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.1272517557748977
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.12751289781021036
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.12772541056786266
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.1281276100748022
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.12836869671526882
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.12842903061680597
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.12846425467649022
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.128520318766435
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.12837828744791055
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.12838243944691374
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.12807771706810364
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.12847236468444898
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.12868620185181498
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.12871870131772242
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.12836239913978228
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.12818987891975656
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.12855881159859045
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.1286574126166456
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.12892989441752434
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.12919586631415905
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.12962008597837252
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.12951923680774283
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.12988011199567054
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.13009660936646408
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.13003860972821712
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.13016146813989968
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.1300589607117024
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.13052859792583868
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.13062731409445405
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.13042527683002433
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.13026252860317425
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.1306110391713152
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.13049954630434513
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.13031471218212998
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.13015334337365395
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.12995433098482853
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.1300489485550385
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.1296357485510054
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.12970633383067148
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.12996340605700127
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.1297130780639472
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.1298121707428486
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.13006461709737777
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.1297888933001338
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.1297224770033998
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.12944904180754602
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.12976748979928202
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.12957364268924879
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.1294737715145637
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.12984804490692595
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.1298568614963758
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.12971768215173432
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.12967392777403194
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.12966227900883384
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.12970656926026108
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.12985044947969235
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.12980582625154527
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.1298394879102707
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.12990507660877137
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.12993523947835908
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.12968944304157048
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.12962514629890753
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.12965131040949088
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.1295522570837545
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.12950572851255085
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.12983015498944692
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.12989532741815296
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.12964856298985306
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.1296024739961414
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.12975365663096852
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.12971377982825472
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.12997370985128898
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.12995082613612924
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.1299645536651848
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.12981228913429757
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.12989464053115646
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.1300195974504782
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.12991568250902769
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.13000214191740506
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.13004766181618177
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.1300051342192534
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.1304028775267953
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.1304450316230456
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.1303301811810361
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.13044832656650165
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.13042283048427183
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.13010685707067515
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.1301603058653493
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.13010889649964297
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.1301873834554557
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.13015634090274195
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.1302815076408896
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.13039341350086034
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.13050335835410942
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.13042199068967206
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.13043617278885988
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.130677923741864
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.13082390987511838
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.13072291223040547
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.13059429346979734
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.13050609370250077
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.130319726758102
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.1305871848236112
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.13062372694761432
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.13057636972083603
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.13036372835581014
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.1302699093846069
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.130395359652383
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.13020415117286824
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.13000086265600333
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.12971937819645646
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.1299105272030031
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.12974848461647828
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.12983182249642208
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.12977948428674058
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.12968415070752629
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.129618422249737
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.1296509545397114
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.12959487103326348
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.1296891531045424
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.129597599994629
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.12953641718972927
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.12956882574056325
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.12959575707687757
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.12950839971502623
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.12955331601627132
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.129315868941779
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.12934738107216665
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.12949674979460482
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.12957900505380582
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.1297381508229959
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.12967955991251384
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.12983656354248524
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.12986638689812144
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.1299343123146803
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.1299561011820591
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.1299770355370699
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.12990251706867684
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.1297674070211869
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.12974557428112352
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.12959800918514913
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.1297113594541139
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.1298571412052427
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.1298586022797354
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.12989948233062365
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.12997816580002297
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.1297757378735832
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.12987635076739068
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.12977869439594172
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.12977262867897887
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.12985474052369048
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.12991673492541597
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.12991940883750266
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.12990250910434248
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.12994776635959343
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.1298935140729485
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.12997253950951354
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.1298652853568395
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.12987482336770118
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.1299928447748596
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.12982761807609022
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.12978016402914014
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.13001974027441895
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.13028286400682482
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.1301919717084745
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.13025933779104584
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.13025812276153484
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.13024024773151316
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.1302387437199132
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.13018607706586016
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.13021601007139982
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.13020605244771208
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.13009297251701354
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.1301968456303925
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.13014521954719685
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.13020080585538604
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.13021923693232848
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.1301998070916351
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.13026794276344097
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.13026947042478723
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.1303123366087675
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.13036382874571176
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.13032814601063727
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.13053281774440134
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.13055125071061036
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.13056972013278442
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.1305542072266575
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.1305160644884203
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.13051584796630777
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.1304507454429619
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.13036777668221053
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.13042718350427032
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.13045184107927177
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.1303512468020578
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.13027160387230283
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.13022892239429198
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.13011565496862837
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.1301251491004566
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.130162210084666
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.13012523298964518
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.13014436115635863
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.13004427511231165
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.13005880029113204
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.13005928618221704
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.13000745887813323
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.13012133634243256
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.12999314551044555
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.12987987846136093
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.1300929442749939
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.13006393753987358
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.13007947735542016
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.1300423176004468
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.13005011414310763
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.1300128990273883
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.1299439180668787
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.12993497164531648
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.1299038452004463
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.13003514547619904
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.1301638436848884
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.13008955502655448
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.13007568487794036
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.1300197848760133
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.13004237236133936
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.13004882272678553
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.13015657701618868
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.1303530406372132
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.1303971264080531
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.13035316128852004
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.13037488886432066
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.13035666104719695
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.13030903868927252
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.13026672048512908
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.13022751097877822
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.13025782373656467
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.13025884996384185
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.13026143856371197
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.13024489807063028
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.13020144613062748
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.13022643473802828
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.130259400615863
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.13022371923381632
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.1301835385003522
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.13008910196442758
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.1301559528449724
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.13011086720209092
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.13004941555162589
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.13004617784528216
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.1300559524269331
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.13018272663879243
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.13016515232223067
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.13015669331235705
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.13007573428385683
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.1301661348436028
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.13012449201950774
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.13006377507070577
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.13008282548133804
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.1299920533956201
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.13000423007286513
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.12991280264101146
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.12980653325078684
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.12982733259204685
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.12987070458185346
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.12993531703497424
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.1299631406308301
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.1299498803150582
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.12987548403732768
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.12989942301176266
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.12997379881232532
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.130002023785242
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.12998660532232562
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.1299903078456602
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.1299109556066603
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.12989333935958497
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.12989397056378926
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.13015341634551683
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.13020843747492783
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.13026374186454123
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.1302681729197502
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.13024487206443197
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.13022581346182727
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.1302134906123767
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.13015185157771098
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.13007211248789516
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.13007610397376226
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.13017399120144546
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.13026321670911128
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.13029931357267213
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.13029202564921177
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.13019923399087419
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.13028839228450417
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.1303540771025852
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.1302649887242357
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.13020559274074103
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.13017719608429726
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.13026056212285606
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.13022785902680115
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.1301940354508358
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.13023108635863212
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.1302261152228371
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.13030059688422596
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.13039899819894976
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.13037474893619053
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.13029878022702965
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.13035390552163445
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.1303420316387889
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.13035405185523047
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.13043361452014687
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.1304642127752304
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.13046019453000515
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.13045045512108336
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.13050427134074863
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.130446488828961
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.13044725683958908
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.1304287500386163
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.13035368597554287
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.1304276590831286
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.13043024171687043
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.1303558156087801
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.13031379040813199
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.13033439788861484
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.13026996511850775
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.1302590520530556
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.13019125807361726
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.13019099254208757
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.13019394500143067
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.13030853084556324
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.13033697226552793
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.13034707615269892
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.13028175103468725
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.13018044636066375
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.13023265520457047
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.13021360061371834
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.13018781367689372
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.13018962551382118
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.1301628248905068
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.13011221069999723
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.1301635027226835
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.13020903258411973
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.13017032110118515
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.1301883056906283
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.13022969654012545
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.13019380871488878
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.13018313375551527
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.13016700358503927
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.1302267926953082
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.13024956014990519
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.1303073075838423
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.13035848178418286
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.1304489746868897
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.13044977786658193
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.13040657042031084
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.13044957414731775
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.13040912034256116
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.13046624527407938
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.1304958952829171
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.13040675029123364
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.13039473897584206
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.13037392284940272
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.13027811069653628
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.1302844576485263
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.13024163423715351
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.13015703978516283
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.13008188172135243
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.13012106605997772
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.1300723060655097
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.13006959323863784
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.13012699633493402
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.13008556926044926
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.13017310993318712
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.13024173211519452
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.13020322089121766
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.13009479684343642
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.130086002481932
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.13009571547613663
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.1300569781485726
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.1300339802729626
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.13002080930111645
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.1300183770361911
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.13001811484317607
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.1299906955402703
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.13005037752113172
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.13004035341713105
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.13002064499590132
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.12998606105997929
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.13002733652175
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.12996630489036737
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.12997168922870694
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.12992215746051664
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.12992472492419838
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.12992526003907492
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.12994789117418523
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.1299477705898368
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.13003114160636198
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.13001222179600577
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.1300041942788667
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.1299334140434121
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.12995693026560134
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.12997035871269882
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.12998156837895192
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.1300092882275326
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.13009252688950962
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.13013217599788454
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.13017026277932714
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.1302207430899776
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.13032607535310722
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.13032607535310722 valid 0.21695177257061005
LOSS train 0.13032607535310722 valid 0.1909351497888565
LOSS train 0.13032607535310722 valid 0.18318323294321695
LOSS train 0.13032607535310722 valid 0.17473455145955086
LOSS train 0.13032607535310722 valid 0.1701608866453171
LOSS train 0.13032607535310722 valid 0.1800528367360433
LOSS train 0.13032607535310722 valid 0.19060711349759782
LOSS train 0.13032607535310722 valid 0.18856235966086388
LOSS train 0.13032607535310722 valid 0.18801004853513506
LOSS train 0.13032607535310722 valid 0.18826062530279158
LOSS train 0.13032607535310722 valid 0.18706404214555566
LOSS train 0.13032607535310722 valid 0.18774663284420967
LOSS train 0.13032607535310722 valid 0.18724126196824586
LOSS train 0.13032607535310722 valid 0.18675163920436585
LOSS train 0.13032607535310722 valid 0.18432493209838868
LOSS train 0.13032607535310722 valid 0.18419180437922478
LOSS train 0.13032607535310722 valid 0.18496890103115754
LOSS train 0.13032607535310722 valid 0.1838652508126365
LOSS train 0.13032607535310722 valid 0.18624064169431986
LOSS train 0.13032607535310722 valid 0.18590703010559081
LOSS train 0.13032607535310722 valid 0.1849904436440695
LOSS train 0.13032607535310722 valid 0.1835358048027212
LOSS train 0.13032607535310722 valid 0.1833275627830754
LOSS train 0.13032607535310722 valid 0.18348464307685694
LOSS train 0.13032607535310722 valid 0.1821920907497406
LOSS train 0.13032607535310722 valid 0.18204982120257157
LOSS train 0.13032607535310722 valid 0.18231378164556292
LOSS train 0.13032607535310722 valid 0.18198899445789202
LOSS train 0.13032607535310722 valid 0.18144373133264738
LOSS train 0.13032607535310722 valid 0.18197548339764277
LOSS train 0.13032607535310722 valid 0.18279940562863503
LOSS train 0.13032607535310722 valid 0.18197087291628122
LOSS train 0.13032607535310722 valid 0.1821909768111778
LOSS train 0.13032607535310722 valid 0.18165268003940582
LOSS train 0.13032607535310722 valid 0.1832142059292112
LOSS train 0.13032607535310722 valid 0.1829987731244829
LOSS train 0.13032607535310722 valid 0.18371419085038676
LOSS train 0.13032607535310722 valid 0.18412074681959653
LOSS train 0.13032607535310722 valid 0.18376446687258208
LOSS train 0.13032607535310722 valid 0.18345183990895747
LOSS train 0.13032607535310722 valid 0.18401161708482883
LOSS train 0.13032607535310722 valid 0.18426514984596343
LOSS train 0.13032607535310722 valid 0.18428605067175488
LOSS train 0.13032607535310722 valid 0.184759740463712
LOSS train 0.13032607535310722 valid 0.18482787609100343
LOSS train 0.13032607535310722 valid 0.1851727262787197
LOSS train 0.13032607535310722 valid 0.18582203191645602
LOSS train 0.13032607535310722 valid 0.18565493977318206
LOSS train 0.13032607535310722 valid 0.18625883575604887
LOSS train 0.13032607535310722 valid 0.18575117588043213
LOSS train 0.13032607535310722 valid 0.18593760974266949
LOSS train 0.13032607535310722 valid 0.18542859359429434
LOSS train 0.13032607535310722 valid 0.18594830722179054
LOSS train 0.13032607535310722 valid 0.18608203171579926
LOSS train 0.13032607535310722 valid 0.18602143146774985
LOSS train 0.13032607535310722 valid 0.1852674266057355
LOSS train 0.13032607535310722 valid 0.185219102784207
LOSS train 0.13032607535310722 valid 0.18506118552438144
LOSS train 0.13032607535310722 valid 0.18540111184120178
LOSS train 0.13032607535310722 valid 0.18526205494999887
LOSS train 0.13032607535310722 valid 0.18484619797253218
LOSS train 0.13032607535310722 valid 0.18536977734296553
LOSS train 0.13032607535310722 valid 0.18472073593783
LOSS train 0.13032607535310722 valid 0.18564282241277397
LOSS train 0.13032607535310722 valid 0.18588668772807487
LOSS train 0.13032607535310722 valid 0.1856563843109391
LOSS train 0.13032607535310722 valid 0.18505972312457525
LOSS train 0.13032607535310722 valid 0.1850646810934824
LOSS train 0.13032607535310722 valid 0.18466251786204352
LOSS train 0.13032607535310722 valid 0.18521559366158077
LOSS train 0.13032607535310722 valid 0.1848652037516446
LOSS train 0.13032607535310722 valid 0.1850781235843897
LOSS train 0.13032607535310722 valid 0.18507187517538462
LOSS train 0.13032607535310722 valid 0.18508403506633397
LOSS train 0.13032607535310722 valid 0.18542397260665894
LOSS train 0.13032607535310722 valid 0.1858377403726703
LOSS train 0.13032607535310722 valid 0.18567706702591538
LOSS train 0.13032607535310722 valid 0.18548019192157647
LOSS train 0.13032607535310722 valid 0.18512476942961728
LOSS train 0.13032607535310722 valid 0.18448096476495265
LOSS train 0.13032607535310722 valid 0.18411909098978396
LOSS train 0.13032607535310722 valid 0.18457010170308555
LOSS train 0.13032607535310722 valid 0.18435179774301597
LOSS train 0.13032607535310722 valid 0.18435169383883476
LOSS train 0.13032607535310722 valid 0.18413632617277256
LOSS train 0.13032607535310722 valid 0.18393194276926128
LOSS train 0.13032607535310722 valid 0.18369549564246473
LOSS train 0.13032607535310722 valid 0.1833939545533874
LOSS train 0.13032607535310722 valid 0.1839456551530388
LOSS train 0.13032607535310722 valid 0.18393105185694164
LOSS train 0.13032607535310722 valid 0.18403030047704885
LOSS train 0.13032607535310722 valid 0.1839937846297803
LOSS train 0.13032607535310722 valid 0.1837728770189388
LOSS train 0.13032607535310722 valid 0.1839803450919212
LOSS train 0.13032607535310722 valid 0.1839356149497785
LOSS train 0.13032607535310722 valid 0.18401869665831327
LOSS train 0.13032607535310722 valid 0.18386109326918101
LOSS train 0.13032607535310722 valid 0.18407008644877648
LOSS train 0.13032607535310722 valid 0.18415463241663846
LOSS train 0.13032607535310722 valid 0.18424233183264732
LOSS train 0.13032607535310722 valid 0.18443490181228903
LOSS train 0.13032607535310722 valid 0.18464306186811597
LOSS train 0.13032607535310722 valid 0.18432841298070926
LOSS train 0.13032607535310722 valid 0.18447063848949397
LOSS train 0.13032607535310722 valid 0.18456054372446878
LOSS train 0.13032607535310722 valid 0.18500832608848247
LOSS train 0.13032607535310722 valid 0.1848954490133535
LOSS train 0.13032607535310722 valid 0.18505558826857144
LOSS train 0.13032607535310722 valid 0.18545039791032808
LOSS train 0.13032607535310722 valid 0.1855454982681708
LOSS train 0.13032607535310722 valid 0.1854848189128412
LOSS train 0.13032607535310722 valid 0.18533043629888976
LOSS train 0.13032607535310722 valid 0.1853847171353028
LOSS train 0.13032607535310722 valid 0.18568259111621924
LOSS train 0.13032607535310722 valid 0.18589175822942153
LOSS train 0.13032607535310722 valid 0.18609923920754728
LOSS train 0.13032607535310722 valid 0.18603832650388408
LOSS train 0.13032607535310722 valid 0.18564793972645777
LOSS train 0.13032607535310722 valid 0.18529362438105734
LOSS train 0.13032607535310722 valid 0.18494202146927516
LOSS train 0.13032607535310722 valid 0.18476755559937028
LOSS train 0.13032607535310722 valid 0.18480078155388596
LOSS train 0.13032607535310722 valid 0.1845825747018907
LOSS train 0.13032607535310722 valid 0.1849194288013443
LOSS train 0.13032607535310722 valid 0.1847679636478424
LOSS train 0.13032607535310722 valid 0.1851539635469043
LOSS train 0.13032607535310722 valid 0.18499021501991691
LOSS train 0.13032607535310722 valid 0.1850472801597789
LOSS train 0.13032607535310722 valid 0.18519820041896762
LOSS train 0.13032607535310722 valid 0.18482391375761767
LOSS train 0.13032607535310722 valid 0.1845194784967044
LOSS train 0.13032607535310722 valid 0.18419654419024786
LOSS train 0.13032607535310722 valid 0.1840987877738207
LOSS train 0.13032607535310722 valid 0.18422531136381093
LOSS train 0.13032607535310722 valid 0.18410868655752252
LOSS train 0.13032607535310722 valid 0.18410970566465573
LOSS train 0.13032607535310722 valid 0.18379149123699995
LOSS train 0.13032607535310722 valid 0.18372915749964508
LOSS train 0.13032607535310722 valid 0.1836721252837627
LOSS train 0.13032607535310722 valid 0.18377035483717918
LOSS train 0.13032607535310722 valid 0.18362313561828425
LOSS train 0.13032607535310722 valid 0.18364754224746999
LOSS train 0.13032607535310722 valid 0.18354659501489226
LOSS train 0.13032607535310722 valid 0.18354815410243142
LOSS train 0.13032607535310722 valid 0.18341444812971971
LOSS train 0.13032607535310722 valid 0.1836293925569482
LOSS train 0.13032607535310722 valid 0.18355110906013827
LOSS train 0.13032607535310722 valid 0.18445796610133067
LOSS train 0.13032607535310722 valid 0.18460643391481182
LOSS train 0.13032607535310722 valid 0.18456543614466986
LOSS train 0.13032607535310722 valid 0.18479859039483482
LOSS train 0.13032607535310722 valid 0.18445369287541039
LOSS train 0.13032607535310722 valid 0.18439744588206797
LOSS train 0.13032607535310722 valid 0.18425594686300723
LOSS train 0.13032607535310722 valid 0.18416058440362254
LOSS train 0.13032607535310722 valid 0.18419020030743036
LOSS train 0.13032607535310722 valid 0.18427431194265936
LOSS train 0.13032607535310722 valid 0.18427290908897978
LOSS train 0.13032607535310722 valid 0.18435903062235634
LOSS train 0.13032607535310722 valid 0.18429209822788833
LOSS train 0.13032607535310722 valid 0.18414821397073522
LOSS train 0.13032607535310722 valid 0.18402423084150127
LOSS train 0.13032607535310722 valid 0.18374478643291567
LOSS train 0.13032607535310722 valid 0.18356129472575536
LOSS train 0.13032607535310722 valid 0.1833938058578607
LOSS train 0.13032607535310722 valid 0.18347870747008957
LOSS train 0.13032607535310722 valid 0.18381381802216262
LOSS train 0.13032607535310722 valid 0.18371768074021452
LOSS train 0.13032607535310722 valid 0.18389896184382354
LOSS train 0.13032607535310722 valid 0.18390711151501712
LOSS train 0.13032607535310722 valid 0.1839337149384426
LOSS train 0.13032607535310722 valid 0.18379695661539255
LOSS train 0.13032607535310722 valid 0.18377774704053912
LOSS train 0.13032607535310722 valid 0.18372987005217323
LOSS train 0.13032607535310722 valid 0.18340293748038156
LOSS train 0.13032607535310722 valid 0.18336412302133712
LOSS train 0.13032607535310722 valid 0.18345632475648221
LOSS train 0.13032607535310722 valid 0.18370279401875614
LOSS train 0.13032607535310722 valid 0.1836105304556852
LOSS train 0.13032607535310722 valid 0.18352513760328293
LOSS train 0.13032607535310722 valid 0.18367045788475164
LOSS train 0.13032607535310722 valid 0.18351738400511688
LOSS train 0.13032607535310722 valid 0.18358191106814503
LOSS train 0.13032607535310722 valid 0.18345446163869422
LOSS train 0.13032607535310722 valid 0.18331259545442222
LOSS train 0.13032607535310722 valid 0.18334942647526342
LOSS train 0.13032607535310722 valid 0.1831983489626869
LOSS train 0.13032607535310722 valid 0.18318848993549955
LOSS train 0.13032607535310722 valid 0.1830527870743363
LOSS train 0.13032607535310722 valid 0.18314629667683652
LOSS train 0.13032607535310722 valid 0.18302487929141958
LOSS train 0.13032607535310722 valid 0.1830869383023431
LOSS train 0.13032607535310722 valid 0.18280452412644815
LOSS train 0.13032607535310722 valid 0.1827214265760687
LOSS train 0.13032607535310722 valid 0.18252227214666514
LOSS train 0.13032607535310722 valid 0.18248602078885448
LOSS train 0.13032607535310722 valid 0.1826871110098011
LOSS train 0.13032607535310722 valid 0.18263941897888375
LOSS train 0.13032607535310722 valid 0.1827616792677635
LOSS train 0.13032607535310722 valid 0.18258747488260268
LOSS train 0.13032607535310722 valid 0.18239182774996876
LOSS train 0.13032607535310722 valid 0.18232276025089886
LOSS train 0.13032607535310722 valid 0.18231736329095116
LOSS train 0.13032607535310722 valid 0.18251181737172836
LOSS train 0.13032607535310722 valid 0.1822743543037554
LOSS train 0.13032607535310722 valid 0.18229709572873068
LOSS train 0.13032607535310722 valid 0.1822295730240679
LOSS train 0.13032607535310722 valid 0.18206942045631316
LOSS train 0.13032607535310722 valid 0.18206538880270634
LOSS train 0.13032607535310722 valid 0.18198500651688804
LOSS train 0.13032607535310722 valid 0.18198896358363437
LOSS train 0.13032607535310722 valid 0.1819459691221984
LOSS train 0.13032607535310722 valid 0.1819088492594974
LOSS train 0.13032607535310722 valid 0.18194227338394273
LOSS train 0.13032607535310722 valid 0.18181812153306118
LOSS train 0.13032607535310722 valid 0.18168525908280303
LOSS train 0.13032607535310722 valid 0.18161903946630417
LOSS train 0.13032607535310722 valid 0.18165547612610214
LOSS train 0.13032607535310722 valid 0.18175382592362355
LOSS train 0.13032607535310722 valid 0.1817208470268683
LOSS train 0.13032607535310722 valid 0.18163261862631835
LOSS train 0.13032607535310722 valid 0.18166327926221196
LOSS train 0.13032607535310722 valid 0.18184209190676565
LOSS train 0.13032607535310722 valid 0.18194785102137498
LOSS train 0.13032607535310722 valid 0.18210876577430302
LOSS train 0.13032607535310722 valid 0.18237301628146552
LOSS train 0.13032607535310722 valid 0.18244214611956727
LOSS train 0.13032607535310722 valid 0.18250704236459314
LOSS train 0.13032607535310722 valid 0.18251473017394804
LOSS train 0.13032607535310722 valid 0.18256205184304197
LOSS train 0.13032607535310722 valid 0.18270867946860078
LOSS train 0.13032607535310722 valid 0.18272510530619784
LOSS train 0.13032607535310722 valid 0.18282911245403372
LOSS train 0.13032607535310722 valid 0.18288567471198547
LOSS train 0.13032607535310722 valid 0.18304996693387945
LOSS train 0.13032607535310722 valid 0.18292975463604522
LOSS train 0.13032607535310722 valid 0.1828731428596038
LOSS train 0.13032607535310722 valid 0.18290455113439
LOSS train 0.13032607535310722 valid 0.18270049247282819
LOSS train 0.13032607535310722 valid 0.18269818996389706
LOSS train 0.13032607535310722 valid 0.18289510207057494
LOSS train 0.13032607535310722 valid 0.18273632714817348
LOSS train 0.13032607535310722 valid 0.18295264716256301
LOSS train 0.13032607535310722 valid 0.1831459656479906
LOSS train 0.13032607535310722 valid 0.18320945215468504
LOSS train 0.13032607535310722 valid 0.1830587249582376
LOSS train 0.13032607535310722 valid 0.18321282582485723
LOSS train 0.13032607535310722 valid 0.18315522390748223
LOSS train 0.13032607535310722 valid 0.18315921556279363
LOSS train 0.13032607535310722 valid 0.18319369739294053
LOSS train 0.13032607535310722 valid 0.18303017401362795
LOSS train 0.13032607535310722 valid 0.18329031215537162
LOSS train 0.13032607535310722 valid 0.1832331472588151
LOSS train 0.13032607535310722 valid 0.18311558558246283
LOSS train 0.13032607535310722 valid 0.18316322033311808
LOSS train 0.13032607535310722 valid 0.18323627550853416
LOSS train 0.13032607535310722 valid 0.1830508885796432
LOSS train 0.13032607535310722 valid 0.18330880803193234
LOSS train 0.13032607535310722 valid 0.18331795332514642
LOSS train 0.13032607535310722 valid 0.18316456182644916
LOSS train 0.13032607535310722 valid 0.18332784026290266
LOSS train 0.13032607535310722 valid 0.18339785427310085
LOSS train 0.13032607535310722 valid 0.18343695658241388
LOSS train 0.13032607535310722 valid 0.18353104405105114
LOSS train 0.13032607535310722 valid 0.18348331513269892
LOSS train 0.13032607535310722 valid 0.18350537435004585
LOSS train 0.13032607535310722 valid 0.1835512372661619
LOSS train 0.13032607535310722 valid 0.18375037249122092
LOSS train 0.13032607535310722 valid 0.18384175501126782
LOSS train 0.13032607535310722 valid 0.18384107544466302
LOSS train 0.13032607535310722 valid 0.18393672139442274
LOSS train 0.13032607535310722 valid 0.18424840545391336
LOSS train 0.13032607535310722 valid 0.18446207898003714
LOSS train 0.13032607535310722 valid 0.18450282535848828
LOSS train 0.13032607535310722 valid 0.18448490110310642
LOSS train 0.13032607535310722 valid 0.18441082796324854
LOSS train 0.13032607535310722 valid 0.18429047606266794
LOSS train 0.13032607535310722 valid 0.18413982368844875
LOSS train 0.13032607535310722 valid 0.18412670744149062
LOSS train 0.13032607535310722 valid 0.18411642797291278
LOSS train 0.13032607535310722 valid 0.18404205991916384
LOSS train 0.13032607535310722 valid 0.18374210535317448
LOSS train 0.13032607535310722 valid 0.18367196901211047
LOSS train 0.13032607535310722 valid 0.18369133825558173
LOSS train 0.13032607535310722 valid 0.18373544260598065
LOSS train 0.13032607535310722 valid 0.18374484993673704
LOSS train 0.13032607535310722 valid 0.18370314204630536
LOSS train 0.13032607535310722 valid 0.1836777363334679
LOSS train 0.13032607535310722 valid 0.1836903213578112
LOSS train 0.13032607535310722 valid 0.1837250396866223
LOSS train 0.13032607535310722 valid 0.183562544331313
LOSS train 0.13032607535310722 valid 0.18355235863119773
LOSS train 0.13032607535310722 valid 0.1835643908218719
LOSS train 0.13032607535310722 valid 0.1836454895432709
LOSS train 0.13032607535310722 valid 0.18372746264025316
LOSS train 0.13032607535310722 valid 0.18364399220995806
LOSS train 0.13032607535310722 valid 0.1837338475817783
LOSS train 0.13032607535310722 valid 0.18376300701458984
LOSS train 0.13032607535310722 valid 0.1838000959227715
LOSS train 0.13032607535310722 valid 0.18389300249516963
LOSS train 0.13032607535310722 valid 0.18384125454976313
LOSS train 0.13032607535310722 valid 0.18379000073533183
LOSS train 0.13032607535310722 valid 0.1838423803251962
LOSS train 0.13032607535310722 valid 0.18383574478426262
LOSS train 0.13032607535310722 valid 0.18377005753458522
LOSS train 0.13032607535310722 valid 0.18380095339775865
LOSS train 0.13032607535310722 valid 0.1837787675012983
LOSS train 0.13032607535310722 valid 0.1836845592002977
LOSS train 0.13032607535310722 valid 0.1836841402267947
LOSS train 0.13032607535310722 valid 0.18373717985326243
LOSS train 0.13032607535310722 valid 0.18372968282442767
LOSS train 0.13032607535310722 valid 0.18370925584951273
LOSS train 0.13032607535310722 valid 0.18389620527196615
LOSS train 0.13032607535310722 valid 0.18390778274198247
LOSS train 0.13032607535310722 valid 0.18382617630182752
LOSS train 0.13032607535310722 valid 0.18379100692706019
LOSS train 0.13032607535310722 valid 0.18385745573213047
LOSS train 0.13032607535310722 valid 0.18389171654791953
LOSS train 0.13032607535310722 valid 0.18405151715192675
LOSS train 0.13032607535310722 valid 0.1839843103894964
LOSS train 0.13032607535310722 valid 0.18416160579503882
LOSS train 0.13032607535310722 valid 0.1841465013784281
LOSS train 0.13032607535310722 valid 0.18410544487252692
LOSS train 0.13032607535310722 valid 0.18421712073922894
LOSS train 0.13032607535310722 valid 0.18421960103970308
LOSS train 0.13032607535310722 valid 0.18438359754491437
LOSS train 0.13032607535310722 valid 0.18443236768883667
LOSS train 0.13032607535310722 valid 0.18439589897397815
LOSS train 0.13032607535310722 valid 0.18445227118520388
LOSS train 0.13032607535310722 valid 0.18444442222967292
LOSS train 0.13032607535310722 valid 0.1843282085846198
LOSS train 0.13032607535310722 valid 0.18420427936656647
LOSS train 0.13032607535310722 valid 0.1842610781659951
LOSS train 0.13032607535310722 valid 0.1843442962153586
LOSS train 0.13032607535310722 valid 0.1842948438738709
LOSS train 0.13032607535310722 valid 0.18436743959873206
LOSS train 0.13032607535310722 valid 0.18436031076692333
LOSS train 0.13032607535310722 valid 0.1843554823929389
LOSS train 0.13032607535310722 valid 0.18437414286674653
LOSS train 0.13032607535310722 valid 0.18431512110811824
LOSS train 0.13032607535310722 valid 0.18415766445335405
LOSS train 0.13032607535310722 valid 0.18416627437660568
LOSS train 0.13032607535310722 valid 0.18420464406468778
LOSS train 0.13032607535310722 valid 0.18444236027899868
LOSS train 0.13032607535310722 valid 0.1845510149995486
LOSS train 0.13032607535310722 valid 0.1846382876136744
LOSS train 0.13032607535310722 valid 0.18449001613285082
LOSS train 0.13032607535310722 valid 0.18442235383922342
LOSS train 0.13032607535310722 valid 0.18440986326482028
LOSS train 0.13032607535310722 valid 0.18433927167739186
LOSS train 0.13032607535310722 valid 0.18425740299486368
LOSS train 0.13032607535310722 valid 0.18424808595922182
LOSS train 0.13032607535310722 valid 0.18425962647618382
LOSS train 0.13032607535310722 valid 0.18428429487650677
LOSS train 0.13032607535310722 valid 0.1843456216891047
LOSS train 0.13032607535310722 valid 0.1843455048550046
LOSS train 0.13032607535310722 valid 0.18438036792001136
LOSS train 0.13032607535310722 valid 0.1842784689916246
LOSS train 0.13032607535310722 valid 0.18428646216900568
LOSS train 0.13032607535310722 valid 0.18423663955181838
LOSS train 0.13032607535310722 valid 0.18418460174197965
LOSS train 0.13032607535310722 valid 0.1842391877666692
LOSS train 0.13032607535310722 valid 0.18423562041245217
LOSS train 0.13032607535310722 valid 0.18421683957861676
LOSS train 0.13032607535310722 valid 0.1842723673949503
LOSS train 0.13032607535310722 valid 0.18429421395307682
LOSS train 0.13032607535310722 valid 0.18419992954188535
LOSS train 0.13032607535310722 valid 0.1842427434116278
LOSS train 0.13032607535310722 valid 0.18431620338179555
EPOCH 15:
  batch 1 loss: 0.11515188962221146
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.11532150581479073
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.1134315977493922
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.12242745980620384
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.12488944828510284
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.12639842182397842
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.12150022919688906
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.12345156352967024
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.12251868264542685
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.12196105495095252
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.1204544258388606
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.1196659995863835
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.12043496450552574
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.12045803080712046
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.11885509838660559
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.12053808802738786
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.11955647302024505
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.12264886208706433
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.12228490529876006
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.12187814749777318
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.1222741777698199
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.12280547043139284
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.12285802383785663
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.12471892094860475
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.12414220601320267
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.12447880237148358
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.1250111369623078
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.1246435557092939
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.12441795293627114
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.1250789274772008
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.12574645203928794
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.12615452287718654
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.1253897242925384
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.12615669452968767
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.12764996141195298
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.12703617310358417
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.12688201041640462
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.1264703791392477
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.12690395040389818
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.12694008015096186
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.12702551302386494
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.1271418939743723
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.12825594877087793
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.12782615626400168
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.12798321213987138
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.12742742021446643
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.12736029796143797
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.12745441465328136
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.12739866880738004
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.12745341300964355
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.1271129969288321
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.12721335085538718
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.12699345710142604
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.12679613036689935
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.12720146111466668
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.12731059520904506
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.12732514333829545
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.12684893698014063
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.12726630611439882
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.1270652547478676
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.12726849683972655
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.1271478044650247
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.12726317026785441
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.12702088372316211
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.12712774586219053
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.12692789338303334
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.12670513909699313
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.126991281285882
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.12722148292738458
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.12752843893000057
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.1279340118799411
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.1281644351159533
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.12821257655342963
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.12823858867223198
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.12829470624526343
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.12823480338250337
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.12822982039931533
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.1279419640508982
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.12834136782190467
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.12859051255509257
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.12867763371747218
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.12827624998441556
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.1281464271337153
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.128578930649729
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.12869817377889858
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.12896052479397418
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.129189345086443
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.12963380334390837
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.1295651685488358
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.12996857522262467
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.13012892523637185
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.1300988601439673
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.13017645117736631
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.13012829985707364
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.1306137804138033
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.1306685545326521
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.13047663137777565
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.1302501062045292
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.13058979523302328
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.1305017352849245
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.13032447679503129
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.13019545011076272
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.1299270902154515
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.13004626806538838
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.12967018293482915
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.12968654726754944
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.12991890666362282
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.12968124918363713
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.12978725449754558
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.13000553412870927
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.1297417117937191
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.1297506374706115
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.12948719972530298
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.12983878270575874
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.129620879046295
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.1294846482703398
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.12983718304297862
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.1298008280159053
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.1296978340554638
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.12968177466342848
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.12965223691926514
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.12970538919822114
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.1298472977629522
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.1298162302062396
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.12984607297182083
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.12988113168449628
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.12992528638267142
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.12967731180833653
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.12959349449983862
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.1295720871251363
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.129428748524826
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.12938400820800752
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.12968734298881732
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.12974451315491947
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.1295112234574777
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.12943366367150755
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.12960438887133216
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.12957545013531394
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.12979757324825947
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.1298009139086519
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.12981501052565608
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.12967257359078233
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.1297176604295944
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.12989211848212612
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.12977499386359906
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.1298484219467803
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.12988133323030407
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.12983205093926675
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.13018439125094639
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.1302718364695708
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.1301511146670935
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.13027067467766373
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.13023578239227432
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.12992494761363252
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.12997840099757718
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.1299411945331555
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.12999500898988384
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.12993449562146694
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.13002128669488355
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.13013789537362755
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.13028206241241894
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.13021044304341445
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.13021376902706053
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.13040533098505763
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.13054025064815175
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.13042859737593007
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.13027855651285833
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.1302352650091052
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.1300335246077656
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.13032418156371398
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.13035200004689176
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.13034407837792886
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.1301431802742054
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.13005518810502414
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.1301583652836936
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.12997039991684936
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.12980140223678222
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.1295240352327904
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.12971525718379953
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.12953036220537292
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.12962458928974951
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.1295899207202288
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.12949668746339818
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.12944207725155613
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.12946714956212688
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.12942943918288394
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.129485919314591
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.12938057813555637
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.1293292914433454
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.1293897284488929
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.12940899801503924
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.1293267256114632
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.12937316514667452
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.12911949130063205
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.12915472296568065
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.12929736321069757
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.12939208821596834
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.12955872375856747
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.12950881424561217
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.12965032652020456
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.12964554642563436
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.1296957121922238
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.12971560121169817
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.12974986046844839
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.12970294647100494
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.1295647262007866
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.12952035867073686
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.12936917871523362
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.129460651908765
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.1295841886174111
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.1295359585239989
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.12957340450781696
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.12966098940708268
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.1294595657665039
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.12956190809260967
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.12944056090243436
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.12942302374658496
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.12951107367599776
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.12957078868259578
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.12957199429246513
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.1295687867438092
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.12961227302481462
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.12955226415074994
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.1296118290629238
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.12949773566590414
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.12952396738450084
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.12962162189415372
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.12947266610960165
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.1294174008559452
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.1296395311213058
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.1298768711619047
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.1297715723129182
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.12982578613983203
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.12981426085417086
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.1298220881756316
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.12983985251541866
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.1298076694650489
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.1298399102412352
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.12985425034078096
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.1297372948999206
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.12988082013436866
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.12981962566533364
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.12986638774106532
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.12988987161976392
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.12988121314924592
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.12993091865768278
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.1299245381644863
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.12999264956001313
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.1300249155984825
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.1299839561879635
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.13018539604319043
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.1301888995877807
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.13024163313887335
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.1302214252843162
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.13019873830033282
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.130192320124479
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.1301134614860965
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.13004042237072952
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.1301078047301318
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.13013907854373638
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.1300517316065529
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.12998481529928346
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.1299614413382889
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.12983034215303083
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.12985650641175936
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.12987297148744864
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.12983328271447941
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.12984859843307467
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.1297574288672231
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.12977409776714113
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.12978360084370053
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.12973904634332833
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.1298542449817116
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.12973249820570876
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.12962186734784734
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.12983312390312768
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.12980004141799809
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.1298338967851169
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.12979013069556178
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.12979062737098762
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.12975174339746665
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.1296741431116635
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.12967673989256365
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.12962589058762705
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.12974830988729208
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.12986405774742574
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.1297845407956984
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.1297567899649342
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.1297225410756768
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.12975670248270035
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.12978214624616288
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.12990341626413882
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.1300895958121726
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.1301286879952262
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.1300905937613067
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.1301019879158687
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.13009557175616224
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.13004322917569403
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.13000567227502333
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.12997392778595288
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.1299922816282095
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.1299910242885154
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.12997951931489182
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.12995273021883086
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.12990889874149542
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.12988895243581602
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.12993433608003083
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.12988560670962582
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.12982882621990438
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.12973273937259952
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.1298068850226341
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.12976866361135855
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.12970534688272414
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.12968079586792144
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.12968536825880173
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.1298304778535532
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.12982137978358976
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.12980726565111358
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.12971929759721398
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.12982096553314476
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.12976239064586498
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.12970843064303725
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.12971610698168493
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.12963866690794626
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.12964782591049487
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.12955476336775382
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.1294440173455699
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.1294400187481831
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.12948965109137417
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.12957325516776605
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.129597458797099
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.1295918041099626
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.12952508258300502
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.1295387788402463
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.12959773796263024
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.12962750564994557
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.1296079326365751
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.12961225714203875
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.12953390717330585
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.12952175881056224
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.1295236493747605
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.12977550135195604
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.12982220587160428
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.12986153333859388
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.12986568061337955
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.12985050958188282
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.12983863884502597
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.12982295654799747
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.1297578426947908
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.1296771503346307
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.12966185912192718
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.12973865238018334
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.12982035124352567
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.12985154698039852
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.12983972860473983
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.1297650362333555
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.12984529843016498
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.12991270818357362
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.12982733591270315
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.1297834506465329
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.12976046820839357
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.12984058921590694
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.12981907807844728
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.1297706413686603
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.1298058830916065
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.1297950482230043
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.12986154892870125
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.12993835007933818
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.1299106921485769
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.12982108242608406
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.12988177789350083
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.1298622106432274
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.12987772589075022
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.12995842143016703
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.1299796846707662
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.1299696970096928
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.12996012893374465
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.13002071273389948
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.12996157197807584
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.12994858983315918
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.1299312648184969
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.12986940430252963
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.12993594019749147
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.12993716654212525
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.12986387728483645
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.12981748449678865
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.1298408386627217
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.12978315101840446
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.12977943052286353
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.12970764193779383
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.12970416251655734
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.12972248132739747
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.12984479857613415
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.12988145070783982
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.12990101089205924
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.12984948153748657
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.12974035266965103
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.12978617255412153
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.1297691005065029
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.12973621893674137
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.12975246245575664
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.12972185024945296
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.12967038761001662
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.12970891167031656
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.12973893229608183
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.12969293887612268
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.1297041435316388
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.1297527676375181
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.129714121572983
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.1297241172957711
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.12971152954365506
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.12977969789316932
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.1298138853027226
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.12986277470338173
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.12992567245859699
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.12999400075596684
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.12998473906902958
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.12994992016462617
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.12999605862223731
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.1299525388472137
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.13001730947837128
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.13004043286939934
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.12995129228342792
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.12994335525018988
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.12993317625101874
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.12983109048204802
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.12985414017070374
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.12981523421949873
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.1297326895184728
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.1296564345096433
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.12970383900915664
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.1296494214243635
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.12964827670580245
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.12969713185995405
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.1296539243610426
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.1297656407733576
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.12981473492539447
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.12977604836769843
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.1296699229364786
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.1296513244509697
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.12966962746616933
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.12962856775832393
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.1296066661821531
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.1295729833445302
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.1295779880680395
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.12958146820487998
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.12955178217066482
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.12960562871636025
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.12958727206180248
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.12956653515497843
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.12954215492285012
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.1295826639650406
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.12952398068002255
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.12952943577837314
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.12948180509137583
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.1294730887666606
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.12947805685600514
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.1295058103888316
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.12951076076181367
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.12958007467829663
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.12956611117158173
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.12955126401794936
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.12949301893008194
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.1295129506656065
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.12952606714861367
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.1295445621045618
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.1295638455026155
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.12964063263538048
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.12968429460772066
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.1297303609867045
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.12977034038609
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.12986794377724498
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.12986794377724498 valid 0.21678565442562103
LOSS train 0.12986794377724498 valid 0.19076799601316452
LOSS train 0.12986794377724498 valid 0.18329988420009613
LOSS train 0.12986794377724498 valid 0.17445727437734604
LOSS train 0.12986794377724498 valid 0.1699226826429367
LOSS train 0.12986794377724498 valid 0.17990249892075857
LOSS train 0.12986794377724498 valid 0.19067305752209254
LOSS train 0.12986794377724498 valid 0.18866481073200703
LOSS train 0.12986794377724498 valid 0.18806211319234636
LOSS train 0.12986794377724498 valid 0.18826541006565095
LOSS train 0.12986794377724498 valid 0.18702900951558893
LOSS train 0.12986794377724498 valid 0.18772337709863982
LOSS train 0.12986794377724498 valid 0.18720534443855286
LOSS train 0.12986794377724498 valid 0.18667911844594137
LOSS train 0.12986794377724498 valid 0.18431807657082874
LOSS train 0.12986794377724498 valid 0.1841404903680086
LOSS train 0.12986794377724498 valid 0.18491131943814895
LOSS train 0.12986794377724498 valid 0.18381672932042015
LOSS train 0.12986794377724498 valid 0.18620607805879494
LOSS train 0.12986794377724498 valid 0.18589730560779572
LOSS train 0.12986794377724498 valid 0.1849476531857536
LOSS train 0.12986794377724498 valid 0.18353898145935751
LOSS train 0.12986794377724498 valid 0.18332344682320303
LOSS train 0.12986794377724498 valid 0.18349839374423027
LOSS train 0.12986794377724498 valid 0.18219873905181885
LOSS train 0.12986794377724498 valid 0.18204976102480522
LOSS train 0.12986794377724498 valid 0.1823310520913866
LOSS train 0.12986794377724498 valid 0.18201501880373275
LOSS train 0.12986794377724498 valid 0.181470957295648
LOSS train 0.12986794377724498 valid 0.1820133551955223
LOSS train 0.12986794377724498 valid 0.18285074253236094
LOSS train 0.12986794377724498 valid 0.18200169736519456
LOSS train 0.12986794377724498 valid 0.18222356429605774
LOSS train 0.12986794377724498 valid 0.18169275013839498
LOSS train 0.12986794377724498 valid 0.18335893452167512
LOSS train 0.12986794377724498 valid 0.18314748174614376
LOSS train 0.12986794377724498 valid 0.1838510209644163
LOSS train 0.12986794377724498 valid 0.1842924630955646
LOSS train 0.12986794377724498 valid 0.18393450746169457
LOSS train 0.12986794377724498 valid 0.18360706940293312
LOSS train 0.12986794377724498 valid 0.1841510417984753
LOSS train 0.12986794377724498 valid 0.18439833252202897
LOSS train 0.12986794377724498 valid 0.18440412158189817
LOSS train 0.12986794377724498 valid 0.18489461188966577
LOSS train 0.12986794377724498 valid 0.18496909141540527
LOSS train 0.12986794377724498 valid 0.18532092027042224
LOSS train 0.12986794377724498 valid 0.18596431961719026
LOSS train 0.12986794377724498 valid 0.18578732262055078
LOSS train 0.12986794377724498 valid 0.18639981625031452
LOSS train 0.12986794377724498 valid 0.18589908689260481
LOSS train 0.12986794377724498 valid 0.1861065024254369
LOSS train 0.12986794377724498 valid 0.18558882348812544
LOSS train 0.12986794377724498 valid 0.1861098161283529
LOSS train 0.12986794377724498 valid 0.18624347862270144
LOSS train 0.12986794377724498 valid 0.18617917814038015
LOSS train 0.12986794377724498 valid 0.18542609975806304
LOSS train 0.12986794377724498 valid 0.18537575555475136
LOSS train 0.12986794377724498 valid 0.1852161999920319
LOSS train 0.12986794377724498 valid 0.18556873525603343
LOSS train 0.12986794377724498 valid 0.1854357863465945
LOSS train 0.12986794377724498 valid 0.1850225803304891
LOSS train 0.12986794377724498 valid 0.18555106150527154
LOSS train 0.12986794377724498 valid 0.18490006053258504
LOSS train 0.12986794377724498 valid 0.1858177080284804
LOSS train 0.12986794377724498 valid 0.186047976062848
LOSS train 0.12986794377724498 valid 0.18581336601213974
LOSS train 0.12986794377724498 valid 0.18521980185117295
LOSS train 0.12986794377724498 valid 0.1852403819999274
LOSS train 0.12986794377724498 valid 0.18483131411282913
LOSS train 0.12986794377724498 valid 0.18538529298135212
LOSS train 0.12986794377724498 valid 0.18503192151096506
LOSS train 0.12986794377724498 valid 0.18524352771540484
LOSS train 0.12986794377724498 valid 0.18522488233977802
LOSS train 0.12986794377724498 valid 0.18523310829658765
LOSS train 0.12986794377724498 valid 0.1855953226486842
LOSS train 0.12986794377724498 valid 0.18599924915715269
LOSS train 0.12986794377724498 valid 0.18583951213143088
LOSS train 0.12986794377724498 valid 0.1856401469080876
LOSS train 0.12986794377724498 valid 0.1852783104286918
LOSS train 0.12986794377724498 valid 0.1846289588138461
LOSS train 0.12986794377724498 valid 0.18427843223383397
LOSS train 0.12986794377724498 valid 0.1847319326749662
LOSS train 0.12986794377724498 valid 0.18451197068375277
LOSS train 0.12986794377724498 valid 0.1845207911516939
LOSS train 0.12986794377724498 valid 0.18431710215175853
LOSS train 0.12986794377724498 valid 0.18411354685938636
LOSS train 0.12986794377724498 valid 0.18388974529573288
LOSS train 0.12986794377724498 valid 0.1835896217010238
LOSS train 0.12986794377724498 valid 0.1841320472486903
LOSS train 0.12986794377724498 valid 0.18412054578463236
LOSS train 0.12986794377724498 valid 0.18422014978560772
LOSS train 0.12986794377724498 valid 0.18417803684006567
LOSS train 0.12986794377724498 valid 0.18395269654130422
LOSS train 0.12986794377724498 valid 0.18416029135597514
LOSS train 0.12986794377724498 valid 0.18410564927678358
LOSS train 0.12986794377724498 valid 0.18417974064747492
LOSS train 0.12986794377724498 valid 0.18401229596629584
LOSS train 0.12986794377724498 valid 0.1842150975550924
LOSS train 0.12986794377724498 valid 0.18429643593051218
LOSS train 0.12986794377724498 valid 0.18438957393169403
LOSS train 0.12986794377724498 valid 0.18458085839111027
LOSS train 0.12986794377724498 valid 0.18480177778823703
LOSS train 0.12986794377724498 valid 0.18448014268018667
LOSS train 0.12986794377724498 valid 0.18462530566522709
LOSS train 0.12986794377724498 valid 0.18471114195528485
LOSS train 0.12986794377724498 valid 0.18515730129097993
LOSS train 0.12986794377724498 valid 0.18504359204078388
LOSS train 0.12986794377724498 valid 0.18520699951935699
LOSS train 0.12986794377724498 valid 0.18560229364885103
LOSS train 0.12986794377724498 valid 0.18568270192904907
LOSS train 0.12986794377724498 valid 0.1856280647151105
LOSS train 0.12986794377724498 valid 0.1854666889246021
LOSS train 0.12986794377724498 valid 0.18552744388580322
LOSS train 0.12986794377724498 valid 0.18583246490411592
LOSS train 0.12986794377724498 valid 0.18604181408882142
LOSS train 0.12986794377724498 valid 0.18624725680926751
LOSS train 0.12986794377724498 valid 0.1861793198901364
LOSS train 0.12986794377724498 valid 0.18578662887468175
LOSS train 0.12986794377724498 valid 0.18543477819747284
LOSS train 0.12986794377724498 valid 0.18509062975645066
LOSS train 0.12986794377724498 valid 0.1849231573422093
LOSS train 0.12986794377724498 valid 0.18496047436702448
LOSS train 0.12986794377724498 valid 0.1847433272173734
LOSS train 0.12986794377724498 valid 0.18508171073852048
LOSS train 0.12986794377724498 valid 0.1849221488237381
LOSS train 0.12986794377724498 valid 0.18530803540396312
LOSS train 0.12986794377724498 valid 0.18514854795350802
LOSS train 0.12986794377724498 valid 0.18519671901594847
LOSS train 0.12986794377724498 valid 0.18534915807635285
LOSS train 0.12986794377724498 valid 0.1849723044496316
LOSS train 0.12986794377724498 valid 0.18466629178924415
LOSS train 0.12986794377724498 valid 0.18434044628432303
LOSS train 0.12986794377724498 valid 0.18423704343630856
LOSS train 0.12986794377724498 valid 0.18437657260627888
LOSS train 0.12986794377724498 valid 0.18425959061693262
LOSS train 0.12986794377724498 valid 0.1842635851353407
LOSS train 0.12986794377724498 valid 0.18394122430442894
LOSS train 0.12986794377724498 valid 0.1838739520829657
LOSS train 0.12986794377724498 valid 0.18381232012518875
LOSS train 0.12986794377724498 valid 0.18392126666648048
LOSS train 0.12986794377724498 valid 0.1837735038699833
LOSS train 0.12986794377724498 valid 0.18378962576389313
LOSS train 0.12986794377724498 valid 0.1836814723023168
LOSS train 0.12986794377724498 valid 0.18368419994496638
LOSS train 0.12986794377724498 valid 0.18355511909928815
LOSS train 0.12986794377724498 valid 0.18377010830461163
LOSS train 0.12986794377724498 valid 0.18369425316246188
LOSS train 0.12986794377724498 valid 0.18460084900662704
LOSS train 0.12986794377724498 valid 0.1847498884937107
LOSS train 0.12986794377724498 valid 0.18470599313577016
LOSS train 0.12986794377724498 valid 0.18494091523404152
LOSS train 0.12986794377724498 valid 0.18459469628961464
LOSS train 0.12986794377724498 valid 0.18453726834721035
LOSS train 0.12986794377724498 valid 0.1843960289057199
LOSS train 0.12986794377724498 valid 0.18429648145552605
LOSS train 0.12986794377724498 valid 0.18432492027297998
LOSS train 0.12986794377724498 valid 0.18441511880440317
LOSS train 0.12986794377724498 valid 0.1844122249491607
LOSS train 0.12986794377724498 valid 0.1844982254617619
LOSS train 0.12986794377724498 valid 0.18443134212866424
LOSS train 0.12986794377724498 valid 0.1842851622134262
LOSS train 0.12986794377724498 valid 0.18415975947806865
LOSS train 0.12986794377724498 valid 0.18387776997191774
LOSS train 0.12986794377724498 valid 0.18369966509138666
LOSS train 0.12986794377724498 valid 0.1835380545168212
LOSS train 0.12986794377724498 valid 0.18361795612846513
LOSS train 0.12986794377724498 valid 0.18395347402481263
LOSS train 0.12986794377724498 valid 0.18385746667072886
LOSS train 0.12986794377724498 valid 0.18403523543177272
LOSS train 0.12986794377724498 valid 0.18404021385838004
LOSS train 0.12986794377724498 valid 0.1840594644434968
LOSS train 0.12986794377724498 valid 0.18392051245237506
LOSS train 0.12986794377724498 valid 0.18390068704682278
LOSS train 0.12986794377724498 valid 0.18385518504970375
LOSS train 0.12986794377724498 valid 0.1835367786032813
LOSS train 0.12986794377724498 valid 0.18349647157910196
LOSS train 0.12986794377724498 valid 0.18359029250966627
LOSS train 0.12986794377724498 valid 0.1838406275832251
LOSS train 0.12986794377724498 valid 0.1837456935610851
LOSS train 0.12986794377724498 valid 0.1836591517759694
LOSS train 0.12986794377724498 valid 0.1838017687283827
LOSS train 0.12986794377724498 valid 0.1836453255388763
LOSS train 0.12986794377724498 valid 0.18370507437674727
LOSS train 0.12986794377724498 valid 0.18357518299118333
LOSS train 0.12986794377724498 valid 0.18343881038395135
LOSS train 0.12986794377724498 valid 0.18347926389786504
LOSS train 0.12986794377724498 valid 0.18332668381897524
LOSS train 0.12986794377724498 valid 0.18331637376166404
LOSS train 0.12986794377724498 valid 0.18317826234159015
LOSS train 0.12986794377724498 valid 0.18326754138657922
LOSS train 0.12986794377724498 valid 0.18314296592280502
LOSS train 0.12986794377724498 valid 0.18320398754440248
LOSS train 0.12986794377724498 valid 0.18292087108051222
LOSS train 0.12986794377724498 valid 0.1828379385250131
LOSS train 0.12986794377724498 valid 0.1826376240987044
LOSS train 0.12986794377724498 valid 0.1826058340315916
LOSS train 0.12986794377724498 valid 0.18281340773033006
LOSS train 0.12986794377724498 valid 0.18276294305770083
LOSS train 0.12986794377724498 valid 0.18288648495422535
LOSS train 0.12986794377724498 valid 0.18272019363939762
LOSS train 0.12986794377724498 valid 0.18252826092848137
LOSS train 0.12986794377724498 valid 0.1824582653618095
LOSS train 0.12986794377724498 valid 0.1824580483101859
LOSS train 0.12986794377724498 valid 0.18265714437938205
LOSS train 0.12986794377724498 valid 0.18241858024422716
LOSS train 0.12986794377724498 valid 0.18244317390965026
LOSS train 0.12986794377724498 valid 0.18237433557349128
LOSS train 0.12986794377724498 valid 0.1822183238199124
LOSS train 0.12986794377724498 valid 0.1822193891665582
LOSS train 0.12986794377724498 valid 0.1821417274219649
LOSS train 0.12986794377724498 valid 0.18214085846433142
LOSS train 0.12986794377724498 valid 0.1820969874707033
LOSS train 0.12986794377724498 valid 0.18205997179931319
LOSS train 0.12986794377724498 valid 0.18209196285945234
LOSS train 0.12986794377724498 valid 0.18196501385333927
LOSS train 0.12986794377724498 valid 0.18182936690195842
LOSS train 0.12986794377724498 valid 0.18175952184584834
LOSS train 0.12986794377724498 valid 0.18179876733263697
LOSS train 0.12986794377724498 valid 0.1819000746969763
LOSS train 0.12986794377724498 valid 0.18185941048643806
LOSS train 0.12986794377724498 valid 0.18177125235488512
LOSS train 0.12986794377724498 valid 0.1818032030452479
LOSS train 0.12986794377724498 valid 0.1819882139630382
LOSS train 0.12986794377724498 valid 0.18209393222683243
LOSS train 0.12986794377724498 valid 0.1822596545351876
LOSS train 0.12986794377724498 valid 0.1825215783530632
LOSS train 0.12986794377724498 valid 0.18259001106417652
LOSS train 0.12986794377724498 valid 0.1826557678480943
LOSS train 0.12986794377724498 valid 0.18266018392217212
LOSS train 0.12986794377724498 valid 0.1827072672869848
LOSS train 0.12986794377724498 valid 0.18285429006789153
LOSS train 0.12986794377724498 valid 0.1828707790580289
LOSS train 0.12986794377724498 valid 0.18297402816524833
LOSS train 0.12986794377724498 valid 0.18302971341161647
LOSS train 0.12986794377724498 valid 0.18319480736204918
LOSS train 0.12986794377724498 valid 0.18307429046954138
LOSS train 0.12986794377724498 valid 0.1830124916029379
LOSS train 0.12986794377724498 valid 0.18304205767246856
LOSS train 0.12986794377724498 valid 0.1828355407615087
LOSS train 0.12986794377724498 valid 0.18283233183125655
LOSS train 0.12986794377724498 valid 0.18303010473607487
LOSS train 0.12986794377724498 valid 0.18287157415112187
LOSS train 0.12986794377724498 valid 0.18308998362272363
LOSS train 0.12986794377724498 valid 0.18328333200245608
LOSS train 0.12986794377724498 valid 0.18334257450638985
LOSS train 0.12986794377724498 valid 0.18319021238059532
LOSS train 0.12986794377724498 valid 0.18334464276367837
LOSS train 0.12986794377724498 valid 0.1832837526115679
LOSS train 0.12986794377724498 valid 0.18328851898271875
LOSS train 0.12986794377724498 valid 0.18331788927316667
LOSS train 0.12986794377724498 valid 0.1831532939378009
LOSS train 0.12986794377724498 valid 0.18341312208582486
LOSS train 0.12986794377724498 valid 0.1833525790056221
LOSS train 0.12986794377724498 valid 0.18323868190444362
LOSS train 0.12986794377724498 valid 0.1832856386315589
LOSS train 0.12986794377724498 valid 0.1833558446378447
LOSS train 0.12986794377724498 valid 0.18316954333726534
LOSS train 0.12986794377724498 valid 0.18342779426611672
LOSS train 0.12986794377724498 valid 0.18343861786555138
LOSS train 0.12986794377724498 valid 0.18328628769287697
LOSS train 0.12986794377724498 valid 0.18344807721874265
LOSS train 0.12986794377724498 valid 0.18351967826144386
LOSS train 0.12986794377724498 valid 0.18356052505652715
LOSS train 0.12986794377724498 valid 0.18365360745652157
LOSS train 0.12986794377724498 valid 0.18361028500323026
LOSS train 0.12986794377724498 valid 0.1836322076002458
LOSS train 0.12986794377724498 valid 0.18367525321267517
LOSS train 0.12986794377724498 valid 0.18387296718003146
LOSS train 0.12986794377724498 valid 0.18396428532095205
LOSS train 0.12986794377724498 valid 0.1839647110413622
LOSS train 0.12986794377724498 valid 0.1840632543022782
LOSS train 0.12986794377724498 valid 0.18437841496265986
LOSS train 0.12986794377724498 valid 0.18459109236032534
LOSS train 0.12986794377724498 valid 0.18463267908044104
LOSS train 0.12986794377724498 valid 0.18461478997360575
LOSS train 0.12986794377724498 valid 0.18454772282553755
LOSS train 0.12986794377724498 valid 0.18443154218179655
LOSS train 0.12986794377724498 valid 0.18427817207231795
LOSS train 0.12986794377724498 valid 0.18426255919172774
LOSS train 0.12986794377724498 valid 0.18425010022308144
LOSS train 0.12986794377724498 valid 0.18417800406120002
LOSS train 0.12986794377724498 valid 0.18387550704762445
LOSS train 0.12986794377724498 valid 0.18380575469125707
LOSS train 0.12986794377724498 valid 0.18382498425182323
LOSS train 0.12986794377724498 valid 0.18386695873841905
LOSS train 0.12986794377724498 valid 0.18387748095226455
LOSS train 0.12986794377724498 valid 0.18383867179371338
LOSS train 0.12986794377724498 valid 0.18381525532135534
LOSS train 0.12986794377724498 valid 0.1838289976996534
LOSS train 0.12986794377724498 valid 0.18386449446452074
LOSS train 0.12986794377724498 valid 0.18369882067351817
LOSS train 0.12986794377724498 valid 0.18368806422062933
LOSS train 0.12986794377724498 valid 0.1836966666497875
LOSS train 0.12986794377724498 valid 0.18377395889081924
LOSS train 0.12986794377724498 valid 0.183859512790785
LOSS train 0.12986794377724498 valid 0.18377520967073538
LOSS train 0.12986794377724498 valid 0.18386436118320984
LOSS train 0.12986794377724498 valid 0.18389646641460042
LOSS train 0.12986794377724498 valid 0.18393368597911752
LOSS train 0.12986794377724498 valid 0.18402849776049454
LOSS train 0.12986794377724498 valid 0.18397698264284387
LOSS train 0.12986794377724498 valid 0.18392352431716508
LOSS train 0.12986794377724498 valid 0.18397307295118623
LOSS train 0.12986794377724498 valid 0.18396593667076608
LOSS train 0.12986794377724498 valid 0.183899259982539
LOSS train 0.12986794377724498 valid 0.18393655644814952
LOSS train 0.12986794377724498 valid 0.18391386133160575
LOSS train 0.12986794377724498 valid 0.18381883235430563
LOSS train 0.12986794377724498 valid 0.1838189827151669
LOSS train 0.12986794377724498 valid 0.18387223450887588
LOSS train 0.12986794377724498 valid 0.18386539138877506
LOSS train 0.12986794377724498 valid 0.18384508456652746
LOSS train 0.12986794377724498 valid 0.18403200545726112
LOSS train 0.12986794377724498 valid 0.1840428038719733
LOSS train 0.12986794377724498 valid 0.18396107689255758
LOSS train 0.12986794377724498 valid 0.18392859528996522
LOSS train 0.12986794377724498 valid 0.18399641319686305
LOSS train 0.12986794377724498 valid 0.1840297782430484
LOSS train 0.12986794377724498 valid 0.18418922606960733
LOSS train 0.12986794377724498 valid 0.184120199107565
LOSS train 0.12986794377724498 valid 0.18429751074481232
LOSS train 0.12986794377724498 valid 0.18428263685995747
LOSS train 0.12986794377724498 valid 0.18424034088665486
LOSS train 0.12986794377724498 valid 0.18435399586127865
LOSS train 0.12986794377724498 valid 0.18435394518650494
LOSS train 0.12986794377724498 valid 0.1845146325056904
LOSS train 0.12986794377724498 valid 0.1845670318977184
LOSS train 0.12986794377724498 valid 0.1845261522046313
LOSS train 0.12986794377724498 valid 0.184580582185538
LOSS train 0.12986794377724498 valid 0.18457351690440466
LOSS train 0.12986794377724498 valid 0.18445675554502405
LOSS train 0.12986794377724498 valid 0.18433282041854887
LOSS train 0.12986794377724498 valid 0.18438950210093735
LOSS train 0.12986794377724498 valid 0.18447009721648194
LOSS train 0.12986794377724498 valid 0.18442291575136469
LOSS train 0.12986794377724498 valid 0.18449628887520658
LOSS train 0.12986794377724498 valid 0.18448904123522053
LOSS train 0.12986794377724498 valid 0.18448195075230486
LOSS train 0.12986794377724498 valid 0.18450016308586858
LOSS train 0.12986794377724498 valid 0.184438992620391
LOSS train 0.12986794377724498 valid 0.1842800825615782
LOSS train 0.12986794377724498 valid 0.18428617482732612
LOSS train 0.12986794377724498 valid 0.1843253151451186
LOSS train 0.12986794377724498 valid 0.1845618338042567
LOSS train 0.12986794377724498 valid 0.18467265991629034
LOSS train 0.12986794377724498 valid 0.18476080039591458
LOSS train 0.12986794377724498 valid 0.1846137385847589
LOSS train 0.12986794377724498 valid 0.18454591049973307
LOSS train 0.12986794377724498 valid 0.1845345123748383
LOSS train 0.12986794377724498 valid 0.18446574038692884
LOSS train 0.12986794377724498 valid 0.18438438878015237
LOSS train 0.12986794377724498 valid 0.18437709411690859
LOSS train 0.12986794377724498 valid 0.1843897957175379
LOSS train 0.12986794377724498 valid 0.1844131570877665
LOSS train 0.12986794377724498 valid 0.18447367874249607
LOSS train 0.12986794377724498 valid 0.1844728783903162
LOSS train 0.12986794377724498 valid 0.18450880741455308
LOSS train 0.12986794377724498 valid 0.18440267012855194
LOSS train 0.12986794377724498 valid 0.18441083410905265
LOSS train 0.12986794377724498 valid 0.18436444751504394
LOSS train 0.12986794377724498 valid 0.18430991080741804
LOSS train 0.12986794377724498 valid 0.18436354557206616
LOSS train 0.12986794377724498 valid 0.18436329651716327
LOSS train 0.12986794377724498 valid 0.18434518745557948
LOSS train 0.12986794377724498 valid 0.18440130848998892
LOSS train 0.12986794377724498 valid 0.18442576804040559
LOSS train 0.12986794377724498 valid 0.18433129687968652
LOSS train 0.12986794377724498 valid 0.18437394700215562
LOSS train 0.12986794377724498 valid 0.18445078715641647
EPOCH 16:
  batch 1 loss: 0.11576186865568161
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.11830694228410721
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.11573280394077301
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.12388088554143906
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.1271277219057083
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.1280246302485466
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.12287949451378413
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.12507289089262486
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.12385083569420709
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.12330406159162521
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.12133841148831627
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.12085886237521966
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.12155602241937931
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.12110265610473496
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.11976801802714666
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.12144035054370761
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.12044190440107794
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.12329315021634102
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.12301971763372421
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.12248836122453213
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.12287658985172
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.1233735324984247
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.12353117666814638
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.12537363513062397
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.12474808663129806
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.12501599095188654
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.12541146328051886
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.12502367714686052
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.12457897231496613
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.12534399727980297
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.12587051718465744
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.1263608979061246
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.12562657237956018
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.12640130234991803
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.1278865505542074
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.1271760238127576
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.1270363552344812
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.1266627835207864
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.12705354411632586
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.12702550794929265
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.12702666568319973
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.12716581051548323
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.12821591056363527
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.12776401588185268
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.12801861117283503
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.12744357372107712
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.127403365804794
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.12748427968472242
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.12747432960539448
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.12759203046560288
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.1272374559559074
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.12731355443023717
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.12715672575077921
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.12708290721531268
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.12751758775927804
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.1276998139385666
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.12774776131437535
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.12725829756979284
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.1275584340348082
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.1273423220962286
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.12750280966035654
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.1272864179505456
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.12737372292885704
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.1270897869253531
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.12711413216132383
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.12686180007277112
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.12668926001929526
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.12697824255070267
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.1272101813684339
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.127472182150398
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.1278525140713638
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.12804034673091438
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.12810741068973933
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.12813146986268661
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.1282439026236534
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.12809519008978418
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.12803925718967016
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.12775224552322656
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.12811670320320734
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.12833238085731863
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.12840944445795482
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.1280305596386514
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.12784338545009313
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.12830481669377714
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.12841609216788236
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.12869450302664623
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.1289157289369353
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.12932794431055133
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.12930440425537945
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.12965269163250923
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.1298860571050382
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.12984138832468053
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.12993524031293008
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.12987754010456673
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.13031425358433474
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.13038039308351776
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.1301842642967234
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.1299637083192261
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.13024647293066738
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.13012799859046936
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.12993448271904842
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.12979380618415626
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.129533761262315
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.12967406392384034
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.12928467903818402
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.12934996145513822
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.12960121105207462
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.12940045742800943
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.1295143578440771
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.12974092614921656
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.12951279861164522
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.12948756871212805
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.12921533733606339
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.12955035079728094
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.1293170111334842
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.12921344257634262
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.12956951469437689
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.12959420264272367
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.1294769621571573
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.1294526057317853
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.12947090739799924
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.1295377251188286
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.1297057406083355
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.12965514144349483
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.12966306334733962
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.1297232641705445
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.1297468421614076
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.12948953622253612
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.1294123522301977
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.12941798120737075
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.1292993426322937
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.1292490322481502
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.1295470854169444
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.12962728672063173
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.12939496128647415
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.12934318661470623
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.1295027995522875
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.12947925919855852
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.12974065751266137
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.12974283562174865
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.12971749685123457
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.12956240788941653
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.12962609315877194
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.1297799106169906
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.1296386906299098
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.12974342708922412
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.12975066297111057
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.12971666027363893
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.1300764639405596
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.13015783642729123
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.13004268281507175
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.13014251915247818
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.13010282161968206
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.12977495141230622
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.1298378844414988
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.12977972516837794
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.12986295281132315
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.12982871109926247
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.12995460676322193
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.130069313198328
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.13020813354053853
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.13014683072213773
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.13016220680409413
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.1303622304666333
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.13050141560308862
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.13039501756429672
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.13028289739064827
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.1302093102020167
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.13002892755545103
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.13030612863161983
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.130342549486467
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.13029720144736212
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.13008091917444517
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.12998875915661626
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.1301160670178277
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.1299123397012326
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.12970581831736752
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.1294235565772887
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.12964491492043662
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.12947212677035067
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.12953492558628157
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.12951740511989857
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.12942631750321779
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.12938378253222807
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.12939765497639372
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.12931854650378227
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.12938278407495926
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.12929453292584164
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.12923640773567574
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.12930923312118178
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.1293295882057145
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.12924465261554965
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.12928519089141657
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.12902798426827206
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.1290636128340012
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.12918807125213194
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.12929147058332027
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.12944094610936713
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.12937414687332796
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.1295131267234683
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.1295655046158762
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.12961447840132337
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.12963666113579803
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.12966973593860281
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.12959562162073648
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.12945627612830365
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.12941168371030098
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.1292433705037603
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.12934115372206034
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.12946975997516086
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.12945223871565542
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.1294904848876989
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.12955572935337192
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.12935523514714198
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.12944958313953045
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.12935578026291397
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.12933196457025642
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.12941089005918677
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.1294795674820469
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.12946425059979613
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.129460840810478
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.12948560466368994
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.1294353291926897
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.1294868052604475
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.12938845657640033
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.12940256538248696
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.12953234227612156
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.129379247298889
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.1293158086106246
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.12953806869361711
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.1297714152248391
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.12967029091869964
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.12974001914902306
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.12975306968148956
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.12975304082353065
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.1297833221822472
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.12973268619318049
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.12976163775980973
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.1297815096054117
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.12966308587541184
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.1297691944847463
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.12971582860986064
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.12977457610667978
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.12979961968347675
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.12975908323210114
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.1298337887700011
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.1298272373825915
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.12988867953179345
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.12993334748419413
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.12988792991638184
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.1300685254344902
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.130105478186456
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.1301250112033173
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.13008894589472944
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.13005328149187798
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.13005853455979377
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.12998667646358913
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.12991639648297038
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.1299822827563783
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.13001591408481963
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.12990775633018137
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.1298415886081812
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.12981739651567584
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.12968116324169165
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.12969297353389128
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.12971052899956703
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.12967995657456502
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.12968046440562206
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.12958334624545725
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.12959707622174862
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.12961032502765585
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.12955918171278694
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.12968196730801473
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.12954330414424847
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.1294294432076541
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.12965905660952348
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.12963480806307673
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.1296488724702554
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.12960587623512446
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.12959930274103368
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.12954118041805526
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.12947265980831274
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.12945629901157252
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.12941328538450556
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.1295383568657072
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.12965537558172965
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.12957640428368639
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.12956830533221364
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.12951062132433624
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.1295434329273372
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.12955778972901838
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.12967173354572628
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.12987360894781738
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.12991509766501635
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.12988858445216034
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.1299242727659844
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.12991297209905053
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.12985665286147355
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.12980549555757773
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.12977671439448993
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.1297905369355433
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.12978232005574056
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.12979006408268076
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.12976438699192122
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.12972147000617668
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.12972082173317864
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.12975316395200426
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.1297083178704435
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.12968439008425742
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.12959734583573956
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.1296740091642383
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.12962598086167604
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.12957337086859602
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.12956283129988963
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.12956085874447748
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.12968927548750292
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.12967891744352666
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.1296587937611079
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.1295895644443162
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.12967599353287368
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.12961782747154296
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.12956542119106151
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.12957553583955617
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.1294793580877192
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.12949459603199592
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.1293991528040061
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.1293032053823865
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.1293090636637516
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.12936305136937865
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.12943886136918356
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.1294747618254938
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.12946301072566624
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.12940467641249792
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.12942946905236757
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.12949892956374295
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.1295306843350686
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.12951110444896652
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.12950917205514287
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.129431561075129
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.12940984100979916
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.1294234519130673
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.12969022640707897
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.1297147707584648
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.12975747588762018
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.12976913400318313
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.12974189068971342
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.12972519731401366
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.12971118116087613
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.12963895318245136
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.12955679687006133
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.12955237973874112
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.12964011811312626
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.1297263424769677
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.12974873389312103
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.12973583817062243
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.129644523468915
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.12972047382376106
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.1297864945157946
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.12970562868629656
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.1296515164896846
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.129631406196762
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.12970824895725067
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.1296847195418413
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.12963464686258153
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.12966803567458504
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.1296588373192347
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.1297219182270749
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.12982992530274
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.12979213107972934
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.12970937824732548
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.1297645402163508
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.12974060016373792
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.12974731845846124
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.12982523857670672
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.12985638012488684
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.12985078425721286
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.12984081594871272
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.1299106862849344
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.1298500843798265
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.12982756236666126
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.12981855368676773
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.12974969685779816
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.1298215143557628
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.1298208884545602
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.12974618713964114
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.12970316589573505
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.1297274277923335
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.12967857524522186
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.12967133294111047
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.1296141928778245
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.12958591553332555
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.1295955524190652
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.12969627879730616
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.12970212262672215
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.1297389872655084
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.12967219309072303
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.12957034635378673
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.12961146379610403
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.12958905948582747
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.12956027509644627
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.12955921187885383
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.12952797174157193
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.1294864834870949
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.12952660000191465
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.12957163804475172
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.1295251200351809
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.12953380782715518
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.12957261480829296
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.12952616734289016
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.12951382027893532
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.12949520628672737
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.12955449785567025
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.12956534946657555
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.12962358520514722
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.12967792390340782
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.12974641608217588
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.129737974142285
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.12969204430517398
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.12973983884569
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.12969884473298277
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.1297558092837379
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.1297984286895578
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.1297143725215966
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.1296962051529367
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.12967529777218315
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.1295887703871783
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.12959903714179435
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.12955822971449277
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.1294787468992191
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.1294059183708457
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.12945972576633683
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.12941860561086624
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.12942604027155089
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.1294850820186226
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.12945726061689442
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.12955572938016796
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.12962126701183668
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.12958825542910457
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.12947893300537097
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.12946498129855502
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.12947852798059684
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.12943980212395007
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.12942476239282444
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.12939525961137568
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.12939427931991856
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.12939886142386983
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.12936178204797258
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.12941424357372203
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.1294142125809113
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.12938364822003576
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.12935435002103876
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.1293956865515329
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.12933117700596783
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.12932503056027292
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.1292690157562822
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.1292674707150773
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.129271428159864
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.12928700785449498
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.12929233707373958
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.129380657232326
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.12936946280995573
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.12936482207599656
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.12930492012405498
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.12931987034670753
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.12932649310878527
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.12935179384033568
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.1293751803691433
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.12946407969754475
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.12951873864001556
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.12956916038343247
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.12960825779825258
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.12969945675803948
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.12969945675803948 valid 0.21560093760490417
LOSS train 0.12969945675803948 valid 0.1891859769821167
LOSS train 0.12969945675803948 valid 0.18160505096117655
LOSS train 0.12969945675803948 valid 0.17290227115154266
LOSS train 0.12969945675803948 valid 0.16837105751037598
LOSS train 0.12969945675803948 valid 0.17841613789399466
LOSS train 0.12969945675803948 valid 0.1889422791344779
LOSS train 0.12969945675803948 valid 0.1869508232921362
LOSS train 0.12969945675803948 valid 0.18640347487396663
LOSS train 0.12969945675803948 valid 0.18657779097557067
LOSS train 0.12969945675803948 valid 0.18532852828502655
LOSS train 0.12969945675803948 valid 0.18607005601127943
LOSS train 0.12969945675803948 valid 0.18553364391510302
LOSS train 0.12969945675803948 valid 0.18498016042368753
LOSS train 0.12969945675803948 valid 0.18256426751613616
LOSS train 0.12969945675803948 valid 0.18244211561977863
LOSS train 0.12969945675803948 valid 0.18321467234807856
LOSS train 0.12969945675803948 valid 0.18211627254883447
LOSS train 0.12969945675803948 valid 0.18448744008415624
LOSS train 0.12969945675803948 valid 0.1841720648109913
LOSS train 0.12969945675803948 valid 0.1832567205031713
LOSS train 0.12969945675803948 valid 0.18183340741829437
LOSS train 0.12969945675803948 valid 0.18163733702638876
LOSS train 0.12969945675803948 valid 0.18182965802649656
LOSS train 0.12969945675803948 valid 0.18052756369113923
LOSS train 0.12969945675803948 valid 0.180402788405235
LOSS train 0.12969945675803948 valid 0.18069463030055719
LOSS train 0.12969945675803948 valid 0.18035240045615605
LOSS train 0.12969945675803948 valid 0.17980869217165585
LOSS train 0.12969945675803948 valid 0.18035058230161666
LOSS train 0.12969945675803948 valid 0.18117018620814046
LOSS train 0.12969945675803948 valid 0.18035011226311326
LOSS train 0.12969945675803948 valid 0.18056845664978027
LOSS train 0.12969945675803948 valid 0.18002541407066233
LOSS train 0.12969945675803948 valid 0.18162067575114113
LOSS train 0.12969945675803948 valid 0.18140439896119964
LOSS train 0.12969945675803948 valid 0.18210992257337313
LOSS train 0.12969945675803948 valid 0.18250680714845657
LOSS train 0.12969945675803948 valid 0.18216030146831122
LOSS train 0.12969945675803948 valid 0.18184881322085858
LOSS train 0.12969945675803948 valid 0.18241203122022676
LOSS train 0.12969945675803948 valid 0.1826512494257518
LOSS train 0.12969945675803948 valid 0.1826631808003714
LOSS train 0.12969945675803948 valid 0.18315093795006926
LOSS train 0.12969945675803948 valid 0.18324497706360288
LOSS train 0.12969945675803948 valid 0.1835918892984805
LOSS train 0.12969945675803948 valid 0.18424057611759673
LOSS train 0.12969945675803948 valid 0.1840707433099548
LOSS train 0.12969945675803948 valid 0.18468025600423618
LOSS train 0.12969945675803948 valid 0.1841836977005005
LOSS train 0.12969945675803948 valid 0.18437665496386735
LOSS train 0.12969945675803948 valid 0.18386122985528067
LOSS train 0.12969945675803948 valid 0.18438464459383264
LOSS train 0.12969945675803948 valid 0.18451788745544576
LOSS train 0.12969945675803948 valid 0.18445641046220607
LOSS train 0.12969945675803948 valid 0.18371119696114743
LOSS train 0.12969945675803948 valid 0.18366097593516634
LOSS train 0.12969945675803948 valid 0.18351778002648517
LOSS train 0.12969945675803948 valid 0.18386134378990884
LOSS train 0.12969945675803948 valid 0.1837123232583205
LOSS train 0.12969945675803948 valid 0.18329622022441175
LOSS train 0.12969945675803948 valid 0.1838116960660104
LOSS train 0.12969945675803948 valid 0.1831721793564539
LOSS train 0.12969945675803948 valid 0.18408915936015546
LOSS train 0.12969945675803948 valid 0.184330998475735
LOSS train 0.12969945675803948 valid 0.18409562788226388
LOSS train 0.12969945675803948 valid 0.1835037234114177
LOSS train 0.12969945675803948 valid 0.1835085265338421
LOSS train 0.12969945675803948 valid 0.18311180087967197
LOSS train 0.12969945675803948 valid 0.1836790184889521
LOSS train 0.12969945675803948 valid 0.18332793741998538
LOSS train 0.12969945675803948 valid 0.18354043571485412
LOSS train 0.12969945675803948 valid 0.183527969130098
LOSS train 0.12969945675803948 valid 0.1835349973391842
LOSS train 0.12969945675803948 valid 0.18388525545597076
LOSS train 0.12969945675803948 valid 0.18429990485310555
LOSS train 0.12969945675803948 valid 0.1841466390080266
LOSS train 0.12969945675803948 valid 0.18394077053436866
LOSS train 0.12969945675803948 valid 0.18359292760679993
LOSS train 0.12969945675803948 valid 0.18294805735349656
LOSS train 0.12969945675803948 valid 0.18258194901325084
LOSS train 0.12969945675803948 valid 0.18303564227208857
LOSS train 0.12969945675803948 valid 0.1828218204788415
LOSS train 0.12969945675803948 valid 0.1828243721808706
LOSS train 0.12969945675803948 valid 0.18259700922405017
LOSS train 0.12969945675803948 valid 0.1823990112820337
LOSS train 0.12969945675803948 valid 0.18217510447419924
LOSS train 0.12969945675803948 valid 0.1818660543384877
LOSS train 0.12969945675803948 valid 0.18242126586062185
LOSS train 0.12969945675803948 valid 0.18240904659032822
LOSS train 0.12969945675803948 valid 0.1825110599562362
LOSS train 0.12969945675803948 valid 0.18246998038628828
LOSS train 0.12969945675803948 valid 0.1822482143999428
LOSS train 0.12969945675803948 valid 0.18245944047861912
LOSS train 0.12969945675803948 valid 0.18240805189860496
LOSS train 0.12969945675803948 valid 0.1824892652221024
LOSS train 0.12969945675803948 valid 0.18232845829934188
LOSS train 0.12969945675803948 valid 0.18253237100280062
LOSS train 0.12969945675803948 valid 0.1826058067757674
LOSS train 0.12969945675803948 valid 0.18269057974219322
LOSS train 0.12969945675803948 valid 0.18288188567846128
LOSS train 0.12969945675803948 valid 0.1830939700206121
LOSS train 0.12969945675803948 valid 0.1827714645746842
LOSS train 0.12969945675803948 valid 0.18291512332283533
LOSS train 0.12969945675803948 valid 0.183008350502877
LOSS train 0.12969945675803948 valid 0.18345749842108422
LOSS train 0.12969945675803948 valid 0.18334588241354327
LOSS train 0.12969945675803948 valid 0.18350642174482346
LOSS train 0.12969945675803948 valid 0.18390750078433152
LOSS train 0.12969945675803948 valid 0.18398725349794734
LOSS train 0.12969945675803948 valid 0.18392759011135446
LOSS train 0.12969945675803948 valid 0.18376315970505988
LOSS train 0.12969945675803948 valid 0.18381899648008093
LOSS train 0.12969945675803948 valid 0.18412289859955772
LOSS train 0.12969945675803948 valid 0.1843343102413675
LOSS train 0.12969945675803948 valid 0.1845411604334568
LOSS train 0.12969945675803948 valid 0.18447922692339644
LOSS train 0.12969945675803948 valid 0.18408311663542765
LOSS train 0.12969945675803948 valid 0.18372515733001613
LOSS train 0.12969945675803948 valid 0.18337155170738698
LOSS train 0.12969945675803948 valid 0.18319500921186338
LOSS train 0.12969945675803948 valid 0.18323110825702793
LOSS train 0.12969945675803948 valid 0.18301103888003806
LOSS train 0.12969945675803948 valid 0.18334706392019026
LOSS train 0.12969945675803948 valid 0.1831934677362442
LOSS train 0.12969945675803948 valid 0.18357029663664953
LOSS train 0.12969945675803948 valid 0.1834060932238271
LOSS train 0.12969945675803948 valid 0.18345743650570512
LOSS train 0.12969945675803948 valid 0.18360381844893905
LOSS train 0.12969945675803948 valid 0.18322425106397042
LOSS train 0.12969945675803948 valid 0.18291761436080203
LOSS train 0.12969945675803948 valid 0.1825987592791066
LOSS train 0.12969945675803948 valid 0.1824966303836134
LOSS train 0.12969945675803948 valid 0.1826280458871998
LOSS train 0.12969945675803948 valid 0.18251300123002795
LOSS train 0.12969945675803948 valid 0.18251547228325815
LOSS train 0.12969945675803948 valid 0.18218986418125402
LOSS train 0.12969945675803948 valid 0.18212433450895807
LOSS train 0.12969945675803948 valid 0.1820639752441173
LOSS train 0.12969945675803948 valid 0.1821604553077902
LOSS train 0.12969945675803948 valid 0.18201393966979168
LOSS train 0.12969945675803948 valid 0.18203286144515157
LOSS train 0.12969945675803948 valid 0.1819274852742682
LOSS train 0.12969945675803948 valid 0.18192917170623937
LOSS train 0.12969945675803948 valid 0.18179895631198226
LOSS train 0.12969945675803948 valid 0.18201238804892317
LOSS train 0.12969945675803948 valid 0.18193854847732854
LOSS train 0.12969945675803948 valid 0.18284479045384638
LOSS train 0.12969945675803948 valid 0.18299842380837306
LOSS train 0.12969945675803948 valid 0.18295766214529674
LOSS train 0.12969945675803948 valid 0.1831950077552669
LOSS train 0.12969945675803948 valid 0.18285296229939713
LOSS train 0.12969945675803948 valid 0.1827966541636224
LOSS train 0.12969945675803948 valid 0.18265739480009327
LOSS train 0.12969945675803948 valid 0.1825534321608082
LOSS train 0.12969945675803948 valid 0.1825797093602327
LOSS train 0.12969945675803948 valid 0.18266521773900196
LOSS train 0.12969945675803948 valid 0.1826615214725084
LOSS train 0.12969945675803948 valid 0.18275296238233457
LOSS train 0.12969945675803948 valid 0.18268549181520938
LOSS train 0.12969945675803948 valid 0.18253634944095376
LOSS train 0.12969945675803948 valid 0.18241135389716537
LOSS train 0.12969945675803948 valid 0.18213152154091677
LOSS train 0.12969945675803948 valid 0.18194329257055028
LOSS train 0.12969945675803948 valid 0.18178012921954645
LOSS train 0.12969945675803948 valid 0.18186669500477343
LOSS train 0.12969945675803948 valid 0.18220506251572136
LOSS train 0.12969945675803948 valid 0.18210619182458945
LOSS train 0.12969945675803948 valid 0.1822870697319155
LOSS train 0.12969945675803948 valid 0.1822945416850202
LOSS train 0.12969945675803948 valid 0.18231684614343252
LOSS train 0.12969945675803948 valid 0.18218089258948036
LOSS train 0.12969945675803948 valid 0.18215943984902663
LOSS train 0.12969945675803948 valid 0.1821135231133165
LOSS train 0.12969945675803948 valid 0.18178840041160582
LOSS train 0.12969945675803948 valid 0.18174850424243646
LOSS train 0.12969945675803948 valid 0.1818420147828463
LOSS train 0.12969945675803948 valid 0.18208962402651818
LOSS train 0.12969945675803948 valid 0.1819971455209082
LOSS train 0.12969945675803948 valid 0.18191206281383832
LOSS train 0.12969945675803948 valid 0.18205495293957094
LOSS train 0.12969945675803948 valid 0.18189889700203152
LOSS train 0.12969945675803948 valid 0.18196063566077603
LOSS train 0.12969945675803948 valid 0.18183461819653926
LOSS train 0.12969945675803948 valid 0.18169440788191718
LOSS train 0.12969945675803948 valid 0.18173618694787386
LOSS train 0.12969945675803948 valid 0.1815851850305649
LOSS train 0.12969945675803948 valid 0.18157342781728886
LOSS train 0.12969945675803948 valid 0.18143729439803533
LOSS train 0.12969945675803948 valid 0.1815296935407739
LOSS train 0.12969945675803948 valid 0.18140178419532577
LOSS train 0.12969945675803948 valid 0.18145993320892254
LOSS train 0.12969945675803948 valid 0.18117693267338017
LOSS train 0.12969945675803948 valid 0.1810995456330555
LOSS train 0.12969945675803948 valid 0.18089962127881173
LOSS train 0.12969945675803948 valid 0.18086382746696472
LOSS train 0.12969945675803948 valid 0.1810665402920718
LOSS train 0.12969945675803948 valid 0.18101898987184872
LOSS train 0.12969945675803948 valid 0.18114156826357147
LOSS train 0.12969945675803948 valid 0.18096711106598376
LOSS train 0.12969945675803948 valid 0.18077425332508276
LOSS train 0.12969945675803948 valid 0.18070800639320128
LOSS train 0.12969945675803948 valid 0.18070307307936287
LOSS train 0.12969945675803948 valid 0.18089558775810635
LOSS train 0.12969945675803948 valid 0.1806571529405873
LOSS train 0.12969945675803948 valid 0.18067776939822633
LOSS train 0.12969945675803948 valid 0.18060811749403027
LOSS train 0.12969945675803948 valid 0.18045162373723891
LOSS train 0.12969945675803948 valid 0.18044880289210086
LOSS train 0.12969945675803948 valid 0.1803676601676714
LOSS train 0.12969945675803948 valid 0.18037103278942018
LOSS train 0.12969945675803948 valid 0.18032661600495284
LOSS train 0.12969945675803948 valid 0.1802877122667474
LOSS train 0.12969945675803948 valid 0.1803204981244613
LOSS train 0.12969945675803948 valid 0.18019705553387486
LOSS train 0.12969945675803948 valid 0.1800635724707886
LOSS train 0.12969945675803948 valid 0.17999472774668224
LOSS train 0.12969945675803948 valid 0.18003153609573294
LOSS train 0.12969945675803948 valid 0.1801311822786723
LOSS train 0.12969945675803948 valid 0.18009276261383836
LOSS train 0.12969945675803948 valid 0.18000514764861283
LOSS train 0.12969945675803948 valid 0.18003626986666843
LOSS train 0.12969945675803948 valid 0.18021948054232406
LOSS train 0.12969945675803948 valid 0.18032013584992715
LOSS train 0.12969945675803948 valid 0.18047970334688823
LOSS train 0.12969945675803948 valid 0.18074492015669832
LOSS train 0.12969945675803948 valid 0.18081195928189198
LOSS train 0.12969945675803948 valid 0.1808767589952862
LOSS train 0.12969945675803948 valid 0.1808825142388781
LOSS train 0.12969945675803948 valid 0.1809280669559603
LOSS train 0.12969945675803948 valid 0.1810722943250235
LOSS train 0.12969945675803948 valid 0.18108978141741505
LOSS train 0.12969945675803948 valid 0.1811953303384167
LOSS train 0.12969945675803948 valid 0.18125067536647504
LOSS train 0.12969945675803948 valid 0.1814153734673845
LOSS train 0.12969945675803948 valid 0.18129852041602135
LOSS train 0.12969945675803948 valid 0.1812371046357014
LOSS train 0.12969945675803948 valid 0.18126767128705978
LOSS train 0.12969945675803948 valid 0.18106032440353137
LOSS train 0.12969945675803948 valid 0.1810575417553385
LOSS train 0.12969945675803948 valid 0.18125587726529704
LOSS train 0.12969945675803948 valid 0.18109979037164656
LOSS train 0.12969945675803948 valid 0.18132075710306442
LOSS train 0.12969945675803948 valid 0.18151233589551488
LOSS train 0.12969945675803948 valid 0.18157620697605367
LOSS train 0.12969945675803948 valid 0.18142532199863495
LOSS train 0.12969945675803948 valid 0.18158015608787537
LOSS train 0.12969945675803948 valid 0.18152061945969059
LOSS train 0.12969945675803948 valid 0.18152209380304957
LOSS train 0.12969945675803948 valid 0.18155368542671205
LOSS train 0.12969945675803948 valid 0.18138861151567967
LOSS train 0.12969945675803948 valid 0.18164608077633948
LOSS train 0.12969945675803948 valid 0.18158808156200076
LOSS train 0.12969945675803948 valid 0.18147074331448773
LOSS train 0.12969945675803948 valid 0.18151900271574656
LOSS train 0.12969945675803948 valid 0.1815890705329366
LOSS train 0.12969945675803948 valid 0.18140551349300355
LOSS train 0.12969945675803948 valid 0.18166305846715158
LOSS train 0.12969945675803948 valid 0.1816718446011709
LOSS train 0.12969945675803948 valid 0.1815179294691636
LOSS train 0.12969945675803948 valid 0.18168055092009547
LOSS train 0.12969945675803948 valid 0.1817515343427658
LOSS train 0.12969945675803948 valid 0.18179237162658923
LOSS train 0.12969945675803948 valid 0.18188478588832147
LOSS train 0.12969945675803948 valid 0.18184112147340234
LOSS train 0.12969945675803948 valid 0.18185976366127343
LOSS train 0.12969945675803948 valid 0.18190575845902332
LOSS train 0.12969945675803948 valid 0.18210247698337284
LOSS train 0.12969945675803948 valid 0.18219239843600746
LOSS train 0.12969945675803948 valid 0.1821921169757843
LOSS train 0.12969945675803948 valid 0.1822898662838109
LOSS train 0.12969945675803948 valid 0.18260237486923442
LOSS train 0.12969945675803948 valid 0.1828146599260442
LOSS train 0.12969945675803948 valid 0.182856530658085
LOSS train 0.12969945675803948 valid 0.1828364722295241
LOSS train 0.12969945675803948 valid 0.18276440357600432
LOSS train 0.12969945675803948 valid 0.1826476593525401
LOSS train 0.12969945675803948 valid 0.18249720495810612
LOSS train 0.12969945675803948 valid 0.18248452945849375
LOSS train 0.12969945675803948 valid 0.18247353269585542
LOSS train 0.12969945675803948 valid 0.1824035833844935
LOSS train 0.12969945675803948 valid 0.18210260821044022
LOSS train 0.12969945675803948 valid 0.1820328455984382
LOSS train 0.12969945675803948 valid 0.18205368385033707
LOSS train 0.12969945675803948 valid 0.18209837181003471
LOSS train 0.12969945675803948 valid 0.18210979819401996
LOSS train 0.12969945675803948 valid 0.18206899784479408
LOSS train 0.12969945675803948 valid 0.18204328492801222
LOSS train 0.12969945675803948 valid 0.1820569419169921
LOSS train 0.12969945675803948 valid 0.18209515812581983
LOSS train 0.12969945675803948 valid 0.1819311703244845
LOSS train 0.12969945675803948 valid 0.18191832071808103
LOSS train 0.12969945675803948 valid 0.18192869045933766
LOSS train 0.12969945675803948 valid 0.18200781467516405
LOSS train 0.12969945675803948 valid 0.1820892045305947
LOSS train 0.12969945675803948 valid 0.18200416513089393
LOSS train 0.12969945675803948 valid 0.18209524801383517
LOSS train 0.12969945675803948 valid 0.1821241483042304
LOSS train 0.12969945675803948 valid 0.18215936833500462
LOSS train 0.12969945675803948 valid 0.18225492594142756
LOSS train 0.12969945675803948 valid 0.18220201755975965
LOSS train 0.12969945675803948 valid 0.1821517375674074
LOSS train 0.12969945675803948 valid 0.18220112792434473
LOSS train 0.12969945675803948 valid 0.18219234855649502
LOSS train 0.12969945675803948 valid 0.1821265151510473
LOSS train 0.12969945675803948 valid 0.18215891122331027
LOSS train 0.12969945675803948 valid 0.18213534440105048
LOSS train 0.12969945675803948 valid 0.18203971358378987
LOSS train 0.12969945675803948 valid 0.1820384203471412
LOSS train 0.12969945675803948 valid 0.18208992949897243
LOSS train 0.12969945675803948 valid 0.18208260860569608
LOSS train 0.12969945675803948 valid 0.1820645246606989
LOSS train 0.12969945675803948 valid 0.1822517228821596
LOSS train 0.12969945675803948 valid 0.1822634767622325
LOSS train 0.12969945675803948 valid 0.1821825818173469
LOSS train 0.12969945675803948 valid 0.1821504114785149
LOSS train 0.12969945675803948 valid 0.1822165020679826
LOSS train 0.12969945675803948 valid 0.18225294775651685
LOSS train 0.12969945675803948 valid 0.18241507765547982
LOSS train 0.12969945675803948 valid 0.18234715124126524
LOSS train 0.12969945675803948 valid 0.1825240267167953
LOSS train 0.12969945675803948 valid 0.1825104357431764
LOSS train 0.12969945675803948 valid 0.18246789827896714
LOSS train 0.12969945675803948 valid 0.1825783913380202
LOSS train 0.12969945675803948 valid 0.1825805623943989
LOSS train 0.12969945675803948 valid 0.1827425537345234
LOSS train 0.12969945675803948 valid 0.18279184977486956
LOSS train 0.12969945675803948 valid 0.18275387051356276
LOSS train 0.12969945675803948 valid 0.18280792469583385
LOSS train 0.12969945675803948 valid 0.1827990554724679
LOSS train 0.12969945675803948 valid 0.18268171351211668
LOSS train 0.12969945675803948 valid 0.18255352069545222
LOSS train 0.12969945675803948 valid 0.1826104672776686
LOSS train 0.12969945675803948 valid 0.18269276259812767
LOSS train 0.12969945675803948 valid 0.18264501977767517
LOSS train 0.12969945675803948 valid 0.18271709719140614
LOSS train 0.12969945675803948 valid 0.1827094767364267
LOSS train 0.12969945675803948 valid 0.18270026114300864
LOSS train 0.12969945675803948 valid 0.18272014245786497
LOSS train 0.12969945675803948 valid 0.18266039747087395
LOSS train 0.12969945675803948 valid 0.18250362634047035
LOSS train 0.12969945675803948 valid 0.18251118405956274
LOSS train 0.12969945675803948 valid 0.18254982953777119
LOSS train 0.12969945675803948 valid 0.18278903650596393
LOSS train 0.12969945675803948 valid 0.18289900575427043
LOSS train 0.12969945675803948 valid 0.1829870250829727
LOSS train 0.12969945675803948 valid 0.18284004915945812
LOSS train 0.12969945675803948 valid 0.18277352866342014
LOSS train 0.12969945675803948 valid 0.18275920532238177
LOSS train 0.12969945675803948 valid 0.1826895408332348
LOSS train 0.12969945675803948 valid 0.18260739176955998
LOSS train 0.12969945675803948 valid 0.18260034776970066
LOSS train 0.12969945675803948 valid 0.18261207777363045
LOSS train 0.12969945675803948 valid 0.18263578749561715
LOSS train 0.12969945675803948 valid 0.1826958234251385
LOSS train 0.12969945675803948 valid 0.18269326498083185
LOSS train 0.12969945675803948 valid 0.182728840711237
LOSS train 0.12969945675803948 valid 0.18262610274403454
LOSS train 0.12969945675803948 valid 0.182634475524213
LOSS train 0.12969945675803948 valid 0.1825863705120153
LOSS train 0.12969945675803948 valid 0.18253288488226255
LOSS train 0.12969945675803948 valid 0.18258825226465641
LOSS train 0.12969945675803948 valid 0.18258460836880136
LOSS train 0.12969945675803948 valid 0.18256672776744262
LOSS train 0.12969945675803948 valid 0.18262215703725815
LOSS train 0.12969945675803948 valid 0.18264478520373179
LOSS train 0.12969945675803948 valid 0.1825519111362725
LOSS train 0.12969945675803948 valid 0.1825952518366925
LOSS train 0.12969945675803948 valid 0.1826710748034441
EPOCH 17:
  batch 1 loss: 0.11339041590690613
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.11531805247068405
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.11302911738554637
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.12182920426130295
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.12448561787605286
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.12553122639656067
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.12053596121924263
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.12299315631389618
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.12200101630555259
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.12179779782891273
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.12033720111305063
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.11968339855472247
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.12014452196084537
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.11983228847384453
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.11839773108561834
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.12047118181362748
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.1196958865312969
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.12263220672806104
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.1224757581949234
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.1218670304864645
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.12213551111164547
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.12248956310478123
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.12255273726971253
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.12446514548112948
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.1238847753405571
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.12413724769766514
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.12447886362119957
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.12413789890706539
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.1237853541970253
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.12456161106626193
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.12518458428882784
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.12563409027643502
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.12483214835325877
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.1255944813875591
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.12713421966348376
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.12641425803303719
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.12629144115222468
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.12589243073996745
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.12635884166528016
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.12628139592707158
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.12635479358638205
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.1263708681577728
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.12735559600730276
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.1268823849545284
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.1270660865637991
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.1265563968083133
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.1264665887710896
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.12662619818001986
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.1265420116940323
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.1266576910018921
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.12637197985952975
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.12649963036752665
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.1263324273925907
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.12619586854621215
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.12656151936812834
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.1267249042700444
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.12673600893794446
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.12618954505386024
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.12654535346112009
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.12639550206561884
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.1265157034895459
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.12639243672451667
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.1265711071235793
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.12637645960785449
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.12648875231926257
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.1262757404509819
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.12606632642781557
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.1263132104102303
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.12654376461885977
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.12681319436856678
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.12717461313160372
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.12737382161948416
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.12747574607803397
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.12749601095109372
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.1275507092475891
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.12739976475897588
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.12742070698892916
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.1271712608062304
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.12751193344593048
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.1277480937540531
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.12781805278342448
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.1274535710491785
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.12728216032306833
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.12770140481491885
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.12783806420424404
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.12810902427448784
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.12839046728679504
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.12883001675998623
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.12876030384154802
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.12914162692096498
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.1293116279028274
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.12932591334633206
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.12936560521202703
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.12928623983517606
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.12975151217297504
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.12983154171767333
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.12958288123619924
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.12936735647369405
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.12964071461347618
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.12956702135503292
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.12935736775398254
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.1292129854361216
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.12898750950410529
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.12914654583885118
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.12879262736865452
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.12882156838785927
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.12910412600107282
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.12888109104500878
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.12896801982450923
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.12920975495468487
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.1289673526813318
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.12894781905093364
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.12865179552968625
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.12899129733181836
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.12875327467918396
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.12867545824626397
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.12904041916386694
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.12905129828190398
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.12893185708202234
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.12894917614758014
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.12889901019078642
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.1289314877425061
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.12908295566231254
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.129055438443057
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.12910379499197006
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.12917690753700242
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.1291923920117964
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.12893677316606045
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.12888715768507283
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.12891999242397456
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.12879300669176888
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.12873082156434204
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.12903733148162527
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.129120482882457
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.1288948189329218
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.1288378443568945
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.12896329088367686
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.1289384834792303
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.12914591802538727
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.12916362785867283
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.129195991560077
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.12903237710116613
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.12908565466637378
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.12923894522504675
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.12911838652758761
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.12921994628563319
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.1292025988199273
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.1291537811425892
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.12950394547625677
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.12956684629122417
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.12945386801926506
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.12956937936771856
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.12955122937758765
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.12923903298842443
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.12930599537587936
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.12927936103481513
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.12935020086491944
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.12929500281056272
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.12941281019516712
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.12952618710696698
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.1296748456007205
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.12959636867414287
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.12961541436201224
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.12979950674059915
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.12993833061420557
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.12979310715054893
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.12962880070337993
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.12956880977643387
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.12940156759595026
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.12970595114371355
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.12973233918000382
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.1296994504193927
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.12948981956767208
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.1294227666810326
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.1295308044552803
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.12933357944712043
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.12913118824783693
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.12884230003430602
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.1290657064518449
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.1288936430381404
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.12894043359308613
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.12891305884817145
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.12880704218084044
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.12876042419963557
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.12879854259458748
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.12872254740326636
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.1288103653705694
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.1287202134807693
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.12867262667764431
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.12873373102200658
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.1287642131457154
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.1286647136633595
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.1287133420378433
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.12846541750369614
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.12851138489368635
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.12865490808474775
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.12875981976840703
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.1289357165194521
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.12887454590755493
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.12901742715388537
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.12904861513803254
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.12911615981766494
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.12913849586483292
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.12918290105082242
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.12913919560066084
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.1290298202662792
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.12899995213689436
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.12883028423843476
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.12893100696032128
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.12904855012893676
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.1290405983043508
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.1290873700998864
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.129162803362233
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.12895048009318727
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.12905331583217133
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.12896714069777065
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.1289616045863947
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.12906037284693586
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.12912888416688736
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.12912777960300445
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.12913608058815088
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.12918513348779162
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.12915606402495516
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.12923275792439068
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.12912260240978665
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.12915138715663843
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.1292578092230574
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.12910250299855283
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.1290482416358577
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.12927795768431996
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.12951847133569386
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.1294134349465884
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.1294741138432159
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.12948263274171415
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.12945262456827974
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.12946413839393753
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.12942526432909543
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.1294344698368501
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.12944534031292385
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.12934423936530948
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.12947002724122209
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.1294128375420393
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.12947071221017054
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.12952231649370466
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.12950529319291212
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.12958855936076583
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.1295655885266389
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.12963036655057822
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.12967499417353825
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.12965326261520385
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.12985507277141053
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.12987569725466153
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.12990508143138507
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.1298834184316669
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.12982750514558716
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.1298424948763568
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.12976514310340473
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.12970173306936442
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.12975968633379256
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.12980284450145868
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.12968399986567625
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.12961603860363705
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.12956974649723946
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.1294380109757185
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.12945690981622013
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.1295016770972345
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.12947144228197663
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.12946710379710838
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.12938092793009096
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.12939397659566668
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.12940031933388588
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.1293579728123458
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.12947381593478033
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.1293594360786633
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.12925731667063453
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.12947354544008124
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.12944032141555517
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.1294495418078179
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.12939647675949187
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.1293971402038421
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.12934556547751205
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.1292740746222912
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.12926998879273452
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.12922271718861353
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.12935897540627864
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.12947739233503808
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.12940976919301295
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.12939414971818528
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.12934618345284543
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.12939027026295663
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.129420865928799
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.12954668585874446
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.1297664704125489
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.12980041199294076
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.12976843002489058
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.12977415091685346
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.12975810362835122
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.1297049460145051
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.12966432208101886
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.12961765863001345
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.1296357678565076
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.12962106425339812
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.12962491891466746
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.12959494625561332
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.12955654852214407
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.12956161517041181
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.12959535776695133
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.12954071190740382
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.1294974609727227
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.12940735158420377
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.1294592684584032
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.1294197637396745
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.12935779407953682
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.1293531030321577
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.12937252985106573
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.12951166644881043
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.12950559956982308
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.12948736442991024
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.12939505321105071
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.12948887087404728
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.12945073132760057
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.12939943505092438
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.12942011412175447
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.12933448375559148
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.12936505065514492
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.12927268520911778
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.12917127284799512
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.12915788559106792
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.12920777182629767
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.12929183849782655
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.12931892320828856
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.12930952982012048
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.12924682847909383
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.12928415857210845
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.12936072051525116
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.12938001383805559
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.1293537412004584
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.12935690030720107
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.12926975804707996
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.12926591382745434
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.12927822447504816
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.1295248948211907
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.12956845354147625
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.12959922869624788
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.12961495779994606
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.12958535168729077
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.12957963248666493
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.12957057095635896
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.12948932413299993
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.12941158863050597
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.12941358552614168
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.1295148202599111
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.1296011403456288
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.12963616976377654
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.12962334733194028
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.12953397920459844
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.12961865740329945
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.12968625452931368
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.1295962433712064
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.129533389935063
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.12951183919678766
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.12960532359079102
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.12958098340461405
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.12952653736203582
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.12956664966393824
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.12956316874978321
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.12962666641139203
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.12972873246864133
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.12968215960151136
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.12959914342374415
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.12965866652219443
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.12963330931961536
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.12963915863960743
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.1297143483185832
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.12975758840640386
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.1297483498309838
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.1297374884511495
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.12980303954746988
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.1297454681434229
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.12973457347405584
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.12972554599675606
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.129656405154011
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.12971868644183984
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.1297226440316687
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.12964411977823678
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.12959499190045146
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.1296128408468355
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.12955181723095707
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.12954611524671392
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.1294818996427915
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.12946106142857494
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.1294648536309904
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.12957220367196257
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.12958735265405044
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.12961705327033995
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.12955427026808863
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.12945157865658216
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.12948915637542854
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.1294776843043796
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.12944784028455614
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.12944818321531848
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.12942678409058656
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.12936975427951766
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.12941100245507636
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.12944731009595187
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.12940280258728953
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.12941097420541894
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.12946305465975813
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.12941504827265635
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.12941647170156967
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.12940113320765415
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.12946134492827271
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.12948048116580627
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.12953663716353656
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.1296013791338507
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.12966998886818495
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.1296696024511358
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.12962556901303204
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.12967341835999546
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.12963162486751875
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.1296899919342825
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.12972247014396
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.12963794318003577
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.12962261719172294
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.12960511682664647
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.12951615757085908
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.12952381532979515
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.12947988609356859
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.12939810294371384
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.1293354445938454
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.12938783442268792
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.12934185990511818
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.12934461846844428
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.12939577008928022
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.1293676442627249
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.129455197551245
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.1295214458890856
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.1294850041094708
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.12938178086267246
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.1293611945753748
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.1293656452983415
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.12932612288456696
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.12929640276437418
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.12927273090358252
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.1292704923434204
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.12928048708380072
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.1292519283374684
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.12931045104882546
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.12930338838052644
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.1292790092031161
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.12924732140651563
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.12929226904777827
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.1292378529906273
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.12923671323231664
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.1291776309137816
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.12917870418740468
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.12918712513386812
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.12920360860931301
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.12920379059182274
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.12927375910398753
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.12926518013640237
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.1292497571477126
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.12918461231639525
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.129198271267373
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.129209162599297
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.12921766386779082
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.12924115905414557
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.1293251499788374
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.12937028086515887
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.1294121891260147
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.12944672498733376
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.12955697060767876
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.12955697060767876 valid 0.21377958357334137
LOSS train 0.12955697060767876 valid 0.18761064112186432
LOSS train 0.12955697060767876 valid 0.1801316092411677
LOSS train 0.12955697060767876 valid 0.17120276018977165
LOSS train 0.12955697060767876 valid 0.16676575541496277
LOSS train 0.12955697060767876 valid 0.1766244868437449
LOSS train 0.12955697060767876 valid 0.18695249727794103
LOSS train 0.12955697060767876 valid 0.18509730696678162
LOSS train 0.12955697060767876 valid 0.184555075234837
LOSS train 0.12955697060767876 valid 0.18467783629894258
LOSS train 0.12955697060767876 valid 0.18341985886747186
LOSS train 0.12955697060767876 valid 0.18411937604347864
LOSS train 0.12955697060767876 valid 0.18355643863861376
LOSS train 0.12955697060767876 valid 0.18301262706518173
LOSS train 0.12955697060767876 valid 0.18064671456813813
LOSS train 0.12955697060767876 valid 0.1805470548570156
LOSS train 0.12955697060767876 valid 0.18128317594528198
LOSS train 0.12955697060767876 valid 0.1802059221598837
LOSS train 0.12955697060767876 valid 0.18256059367405741
LOSS train 0.12955697060767876 valid 0.1822594590485096
LOSS train 0.12955697060767876 valid 0.18132146412418002
LOSS train 0.12955697060767876 valid 0.179892129518769
LOSS train 0.12955697060767876 valid 0.1797032971744952
LOSS train 0.12955697060767876 valid 0.17989377366999784
LOSS train 0.12955697060767876 valid 0.1786025047302246
LOSS train 0.12955697060767876 valid 0.17847828910900995
LOSS train 0.12955697060767876 valid 0.17879285856529517
LOSS train 0.12955697060767876 valid 0.17844313915286744
LOSS train 0.12955697060767876 valid 0.17791644869179563
LOSS train 0.12955697060767876 valid 0.1784489298860232
LOSS train 0.12955697060767876 valid 0.17927002810662793
LOSS train 0.12955697060767876 valid 0.17846693890169263
LOSS train 0.12955697060767876 valid 0.17866882185141245
LOSS train 0.12955697060767876 valid 0.17812957658487208
LOSS train 0.12955697060767876 valid 0.17971997261047362
LOSS train 0.12955697060767876 valid 0.17950090517600378
LOSS train 0.12955697060767876 valid 0.18020633547692685
LOSS train 0.12955697060767876 valid 0.18058663685070842
LOSS train 0.12955697060767876 valid 0.18026586832144323
LOSS train 0.12955697060767876 valid 0.17996301874518394
LOSS train 0.12955697060767876 valid 0.180521292293944
LOSS train 0.12955697060767876 valid 0.18073697920356477
LOSS train 0.12955697060767876 valid 0.18075403120628622
LOSS train 0.12955697060767876 valid 0.18125100806355476
LOSS train 0.12955697060767876 valid 0.1813424865404765
LOSS train 0.12955697060767876 valid 0.18170403656752213
LOSS train 0.12955697060767876 valid 0.18234698886566975
LOSS train 0.12955697060767876 valid 0.18219475572307905
LOSS train 0.12955697060767876 valid 0.18280307553252395
LOSS train 0.12955697060767876 valid 0.18232871890068053
LOSS train 0.12955697060767876 valid 0.18252827723821005
LOSS train 0.12955697060767876 valid 0.1820101379775084
LOSS train 0.12955697060767876 valid 0.18253125137877915
LOSS train 0.12955697060767876 valid 0.1826622494392925
LOSS train 0.12955697060767876 valid 0.18259052498774095
LOSS train 0.12955697060767876 valid 0.18187550136021205
LOSS train 0.12955697060767876 valid 0.18181461801654414
LOSS train 0.12955697060767876 valid 0.18168009021158876
LOSS train 0.12955697060767876 valid 0.1820243309109898
LOSS train 0.12955697060767876 valid 0.1818611721197764
LOSS train 0.12955697060767876 valid 0.18145608730980609
LOSS train 0.12955697060767876 valid 0.18196099107303926
LOSS train 0.12955697060767876 valid 0.181332818335957
LOSS train 0.12955697060767876 valid 0.18223229004070163
LOSS train 0.12955697060767876 valid 0.1824610065955382
LOSS train 0.12955697060767876 valid 0.18222177796291583
LOSS train 0.12955697060767876 valid 0.1816434989224619
LOSS train 0.12955697060767876 valid 0.18164783760028727
LOSS train 0.12955697060767876 valid 0.18126538633436395
LOSS train 0.12955697060767876 valid 0.1818302984748568
LOSS train 0.12955697060767876 valid 0.1814805290228884
LOSS train 0.12955697060767876 valid 0.1816816638327307
LOSS train 0.12955697060767876 valid 0.18166562486184787
LOSS train 0.12955697060767876 valid 0.18166129452151222
LOSS train 0.12955697060767876 valid 0.18201082150141398
LOSS train 0.12955697060767876 valid 0.1824193863492263
LOSS train 0.12955697060767876 valid 0.18227707720422126
LOSS train 0.12955697060767876 valid 0.18206836703496102
LOSS train 0.12955697060767876 valid 0.18173111013219326
LOSS train 0.12955697060767876 valid 0.18109591864049435
LOSS train 0.12955697060767876 valid 0.180739293495814
LOSS train 0.12955697060767876 valid 0.18118580249024602
LOSS train 0.12955697060767876 valid 0.18099009416189538
LOSS train 0.12955697060767876 valid 0.181008039131051
LOSS train 0.12955697060767876 valid 0.18078030067331652
LOSS train 0.12955697060767876 valid 0.18058711359667223
LOSS train 0.12955697060767876 valid 0.18036867695293207
LOSS train 0.12955697060767876 valid 0.1800691199234941
LOSS train 0.12955697060767876 valid 0.18060676987921254
LOSS train 0.12955697060767876 valid 0.18059049563275442
LOSS train 0.12955697060767876 valid 0.18068995203945662
LOSS train 0.12955697060767876 valid 0.1806735064348449
LOSS train 0.12955697060767876 valid 0.1804607275032228
LOSS train 0.12955697060767876 valid 0.18066396326460737
LOSS train 0.12955697060767876 valid 0.1806026080721303
LOSS train 0.12955697060767876 valid 0.18067450992142162
LOSS train 0.12955697060767876 valid 0.18051662764598414
LOSS train 0.12955697060767876 valid 0.18071179031109325
LOSS train 0.12955697060767876 valid 0.18077498388410818
LOSS train 0.12955697060767876 valid 0.18086860939860344
LOSS train 0.12955697060767876 valid 0.1810564795638075
LOSS train 0.12955697060767876 valid 0.18126416688456254
LOSS train 0.12955697060767876 valid 0.18094459060326362
LOSS train 0.12955697060767876 valid 0.18108843539196712
LOSS train 0.12955697060767876 valid 0.181181638723328
LOSS train 0.12955697060767876 valid 0.1816221725828243
LOSS train 0.12955697060767876 valid 0.18151075831640548
LOSS train 0.12955697060767876 valid 0.18166801319630058
LOSS train 0.12955697060767876 valid 0.18205949582091166
LOSS train 0.12955697060767876 valid 0.18212225437164306
LOSS train 0.12955697060767876 valid 0.18206613369890162
LOSS train 0.12955697060767876 valid 0.18189620013747895
LOSS train 0.12955697060767876 valid 0.18194612794745285
LOSS train 0.12955697060767876 valid 0.18225006419315673
LOSS train 0.12955697060767876 valid 0.18246516144793967
LOSS train 0.12955697060767876 valid 0.1826689886892664
LOSS train 0.12955697060767876 valid 0.18260267338691613
LOSS train 0.12955697060767876 valid 0.1822076547196356
LOSS train 0.12955697060767876 valid 0.18185881030659715
LOSS train 0.12955697060767876 valid 0.1815128605812788
LOSS train 0.12955697060767876 valid 0.18134077484450065
LOSS train 0.12955697060767876 valid 0.1813785133303189
LOSS train 0.12955697060767876 valid 0.18116477773926123
LOSS train 0.12955697060767876 valid 0.1814925913368502
LOSS train 0.12955697060767876 valid 0.1813360687494278
LOSS train 0.12955697060767876 valid 0.18170262849520122
LOSS train 0.12955697060767876 valid 0.18154380464647699
LOSS train 0.12955697060767876 valid 0.18159441673196852
LOSS train 0.12955697060767876 valid 0.1817358586677285
LOSS train 0.12955697060767876 valid 0.18135397560321367
LOSS train 0.12955697060767876 valid 0.18105153147955888
LOSS train 0.12955697060767876 valid 0.1807384735932856
LOSS train 0.12955697060767876 valid 0.18063013636528102
LOSS train 0.12955697060767876 valid 0.1807587793958721
LOSS train 0.12955697060767876 valid 0.18064605836515074
LOSS train 0.12955697060767876 valid 0.18065036482670727
LOSS train 0.12955697060767876 valid 0.18032579210987926
LOSS train 0.12955697060767876 valid 0.1802549104111782
LOSS train 0.12955697060767876 valid 0.18019125508747513
LOSS train 0.12955697060767876 valid 0.18028463231665748
LOSS train 0.12955697060767876 valid 0.1801417200489247
LOSS train 0.12955697060767876 valid 0.18015892782681425
LOSS train 0.12955697060767876 valid 0.18004895121484368
LOSS train 0.12955697060767876 valid 0.18005009088665247
LOSS train 0.12955697060767876 valid 0.1799288673647519
LOSS train 0.12955697060767876 valid 0.1801391603195504
LOSS train 0.12955697060767876 valid 0.18006607596160604
LOSS train 0.12955697060767876 valid 0.18096257554920944
LOSS train 0.12955697060767876 valid 0.18111624463692608
LOSS train 0.12955697060767876 valid 0.18107519259055455
LOSS train 0.12955697060767876 valid 0.18130809690383887
LOSS train 0.12955697060767876 valid 0.18097119121567198
LOSS train 0.12955697060767876 valid 0.1809137750879612
LOSS train 0.12955697060767876 valid 0.18077622295974136
LOSS train 0.12955697060767876 valid 0.18066673009626327
LOSS train 0.12955697060767876 valid 0.180690825654146
LOSS train 0.12955697060767876 valid 0.18078005598608854
LOSS train 0.12955697060767876 valid 0.18077203169276443
LOSS train 0.12955697060767876 valid 0.18086281072043772
LOSS train 0.12955697060767876 valid 0.18079187478870154
LOSS train 0.12955697060767876 valid 0.18064564142538153
LOSS train 0.12955697060767876 valid 0.18052487112121818
LOSS train 0.12955697060767876 valid 0.18024949190075412
LOSS train 0.12955697060767876 valid 0.1800639117818053
LOSS train 0.12955697060767876 valid 0.17990292103001565
LOSS train 0.12955697060767876 valid 0.17998414055769701
LOSS train 0.12955697060767876 valid 0.18032048089418581
LOSS train 0.12955697060767876 valid 0.18022332349348635
LOSS train 0.12955697060767876 valid 0.18039785713486417
LOSS train 0.12955697060767876 valid 0.18040673741522958
LOSS train 0.12955697060767876 valid 0.18042121688176316
LOSS train 0.12955697060767876 valid 0.18028835311185482
LOSS train 0.12955697060767876 valid 0.1802644839004285
LOSS train 0.12955697060767876 valid 0.1802210029343079
LOSS train 0.12955697060767876 valid 0.179901804625988
LOSS train 0.12955697060767876 valid 0.17986373836174607
LOSS train 0.12955697060767876 valid 0.17995895803143075
LOSS train 0.12955697060767876 valid 0.18020607258998947
LOSS train 0.12955697060767876 valid 0.1801146684661924
LOSS train 0.12955697060767876 valid 0.18003317295677132
LOSS train 0.12955697060767876 valid 0.1801728309435739
LOSS train 0.12955697060767876 valid 0.1800208124076272
LOSS train 0.12955697060767876 valid 0.18008143943352778
LOSS train 0.12955697060767876 valid 0.17995312481956638
LOSS train 0.12955697060767876 valid 0.17981473992805222
LOSS train 0.12955697060767876 valid 0.1798569010871072
LOSS train 0.12955697060767876 valid 0.17970730288621575
LOSS train 0.12955697060767876 valid 0.17969794757664204
LOSS train 0.12955697060767876 valid 0.17956365294084348
LOSS train 0.12955697060767876 valid 0.1796535821729585
LOSS train 0.12955697060767876 valid 0.17952452954977594
LOSS train 0.12955697060767876 valid 0.17958208751709512
LOSS train 0.12955697060767876 valid 0.17930398893016608
LOSS train 0.12955697060767876 valid 0.17922897041612065
LOSS train 0.12955697060767876 valid 0.17903060618883523
LOSS train 0.12955697060767876 valid 0.17899328025476058
LOSS train 0.12955697060767876 valid 0.1791932579451406
LOSS train 0.12955697060767876 valid 0.1791443523358215
LOSS train 0.12955697060767876 valid 0.17926831494773451
LOSS train 0.12955697060767876 valid 0.17909600164741277
LOSS train 0.12955697060767876 valid 0.17891035948078432
LOSS train 0.12955697060767876 valid 0.1788490718175279
LOSS train 0.12955697060767876 valid 0.17884712512123174
LOSS train 0.12955697060767876 valid 0.17904175712051346
LOSS train 0.12955697060767876 valid 0.178807309378938
LOSS train 0.12955697060767876 valid 0.17882731271166247
LOSS train 0.12955697060767876 valid 0.17875855207299265
LOSS train 0.12955697060767876 valid 0.17860505325146592
LOSS train 0.12955697060767876 valid 0.17860516638419274
LOSS train 0.12955697060767876 valid 0.17852704691744986
LOSS train 0.12955697060767876 valid 0.17852941730999833
LOSS train 0.12955697060767876 valid 0.17848789547833632
LOSS train 0.12955697060767876 valid 0.17844823335118137
LOSS train 0.12955697060767876 valid 0.17847915679634174
LOSS train 0.12955697060767876 valid 0.17835784232200577
LOSS train 0.12955697060767876 valid 0.17822420138313813
LOSS train 0.12955697060767876 valid 0.17815266051737394
LOSS train 0.12955697060767876 valid 0.178190168130015
LOSS train 0.12955697060767876 valid 0.17829092122512322
LOSS train 0.12955697060767876 valid 0.178247179124843
LOSS train 0.12955697060767876 valid 0.17816411171159055
LOSS train 0.12955697060767876 valid 0.17819512276364877
LOSS train 0.12955697060767876 valid 0.1783807598756033
LOSS train 0.12955697060767876 valid 0.17847803052115654
LOSS train 0.12955697060767876 valid 0.17863700724310344
LOSS train 0.12955697060767876 valid 0.17889686367284935
LOSS train 0.12955697060767876 valid 0.17896195973725046
LOSS train 0.12955697060767876 valid 0.17902659360123307
LOSS train 0.12955697060767876 valid 0.1790292502816067
LOSS train 0.12955697060767876 valid 0.17907477257692295
LOSS train 0.12955697060767876 valid 0.17921986946940938
LOSS train 0.12955697060767876 valid 0.17923991276143952
LOSS train 0.12955697060767876 valid 0.179344525057103
LOSS train 0.12955697060767876 valid 0.17939794862754324
LOSS train 0.12955697060767876 valid 0.17956064841214647
LOSS train 0.12955697060767876 valid 0.1794468808161505
LOSS train 0.12955697060767876 valid 0.179383777459211
LOSS train 0.12955697060767876 valid 0.1794120007891114
LOSS train 0.12955697060767876 valid 0.17920506003636197
LOSS train 0.12955697060767876 valid 0.1792014162056148
LOSS train 0.12955697060767876 valid 0.1793967047976755
LOSS train 0.12955697060767876 valid 0.17923964427645542
LOSS train 0.12955697060767876 valid 0.17945834335108352
LOSS train 0.12955697060767876 valid 0.17964538525728907
LOSS train 0.12955697060767876 valid 0.17970809702362334
LOSS train 0.12955697060767876 valid 0.1795596252309113
LOSS train 0.12955697060767876 valid 0.17971337116078326
LOSS train 0.12955697060767876 valid 0.17965095781631046
LOSS train 0.12955697060767876 valid 0.1796525874470611
LOSS train 0.12955697060767876 valid 0.1796774187386036
LOSS train 0.12955697060767876 valid 0.17951422000667488
LOSS train 0.12955697060767876 valid 0.1797690289538531
LOSS train 0.12955697060767876 valid 0.17971008518349982
LOSS train 0.12955697060767876 valid 0.1795935241549503
LOSS train 0.12955697060767876 valid 0.17964191509812485
LOSS train 0.12955697060767876 valid 0.1797071041946765
LOSS train 0.12955697060767876 valid 0.1795273984162724
LOSS train 0.12955697060767876 valid 0.17978012870795043
LOSS train 0.12955697060767876 valid 0.17978975720502235
LOSS train 0.12955697060767876 valid 0.1796348999039485
LOSS train 0.12955697060767876 valid 0.17979399799958043
LOSS train 0.12955697060767876 valid 0.17986527668269536
LOSS train 0.12955697060767876 valid 0.17990616658001346
LOSS train 0.12955697060767876 valid 0.17999694685479908
LOSS train 0.12955697060767876 valid 0.1799585337065301
LOSS train 0.12955697060767876 valid 0.17997337914163009
LOSS train 0.12955697060767876 valid 0.1800183460451244
LOSS train 0.12955697060767876 valid 0.1802083790691486
LOSS train 0.12955697060767876 valid 0.18029450763667826
LOSS train 0.12955697060767876 valid 0.1802962082679625
LOSS train 0.12955697060767876 valid 0.18039767419390132
LOSS train 0.12955697060767876 valid 0.18071046901647658
LOSS train 0.12955697060767876 valid 0.18091959315232742
LOSS train 0.12955697060767876 valid 0.1809624618694295
LOSS train 0.12955697060767876 valid 0.18094175839965992
LOSS train 0.12955697060767876 valid 0.1808767309277386
LOSS train 0.12955697060767876 valid 0.18076347188506317
LOSS train 0.12955697060767876 valid 0.18061588910951032
LOSS train 0.12955697060767876 valid 0.1806037617466783
LOSS train 0.12955697060767876 valid 0.18059159823294196
LOSS train 0.12955697060767876 valid 0.18052304016419576
LOSS train 0.12955697060767876 valid 0.18022501207095512
LOSS train 0.12955697060767876 valid 0.18015793505595346
LOSS train 0.12955697060767876 valid 0.18018002658557725
LOSS train 0.12955697060767876 valid 0.1802245964868027
LOSS train 0.12955697060767876 valid 0.18023691130997418
LOSS train 0.12955697060767876 valid 0.1801990673776703
LOSS train 0.12955697060767876 valid 0.18017479024517039
LOSS train 0.12955697060767876 valid 0.1801862472532942
LOSS train 0.12955697060767876 valid 0.18022351334321088
LOSS train 0.12955697060767876 valid 0.18006204368220163
LOSS train 0.12955697060767876 valid 0.18004943387047068
LOSS train 0.12955697060767876 valid 0.18005762905289288
LOSS train 0.12955697060767876 valid 0.18013259188151684
LOSS train 0.12955697060767876 valid 0.18021294451871162
LOSS train 0.12955697060767876 valid 0.18012974227501735
LOSS train 0.12955697060767876 valid 0.18021888491581586
LOSS train 0.12955697060767876 valid 0.18025116979975828
LOSS train 0.12955697060767876 valid 0.1802838784844979
LOSS train 0.12955697060767876 valid 0.18038035271068414
LOSS train 0.12955697060767876 valid 0.18032790742740282
LOSS train 0.12955697060767876 valid 0.18027818570567283
LOSS train 0.12955697060767876 valid 0.18032577828113788
LOSS train 0.12955697060767876 valid 0.18031579900630995
LOSS train 0.12955697060767876 valid 0.18025195420276924
LOSS train 0.12955697060767876 valid 0.1802843373049708
LOSS train 0.12955697060767876 valid 0.18026205691044805
LOSS train 0.12955697060767876 valid 0.1801637030121955
LOSS train 0.12955697060767876 valid 0.1801624632045675
LOSS train 0.12955697060767876 valid 0.1802127548042805
LOSS train 0.12955697060767876 valid 0.18020559289160265
LOSS train 0.12955697060767876 valid 0.1801891098372065
LOSS train 0.12955697060767876 valid 0.1803746477435953
LOSS train 0.12955697060767876 valid 0.18038528675960888
LOSS train 0.12955697060767876 valid 0.18030586675519034
LOSS train 0.12955697060767876 valid 0.18027823359434364
LOSS train 0.12955697060767876 valid 0.18034609656322642
LOSS train 0.12955697060767876 valid 0.18038230410443162
LOSS train 0.12955697060767876 valid 0.18054271251150059
LOSS train 0.12955697060767876 valid 0.18047516907099634
LOSS train 0.12955697060767876 valid 0.1806489848691355
LOSS train 0.12955697060767876 valid 0.18063744132100426
LOSS train 0.12955697060767876 valid 0.18059249395279692
LOSS train 0.12955697060767876 valid 0.18070116245912182
LOSS train 0.12955697060767876 valid 0.180702092899726
LOSS train 0.12955697060767876 valid 0.1808616041085837
LOSS train 0.12955697060767876 valid 0.18091087009108395
LOSS train 0.12955697060767876 valid 0.18087094338474477
LOSS train 0.12955697060767876 valid 0.18092338661564158
LOSS train 0.12955697060767876 valid 0.18091536672277883
LOSS train 0.12955697060767876 valid 0.180801105485762
LOSS train 0.12955697060767876 valid 0.1806732768260212
LOSS train 0.12955697060767876 valid 0.1807293893025444
LOSS train 0.12955697060767876 valid 0.18080802357750025
LOSS train 0.12955697060767876 valid 0.1807625452306733
LOSS train 0.12955697060767876 valid 0.18083360762379708
LOSS train 0.12955697060767876 valid 0.18082507757210237
LOSS train 0.12955697060767876 valid 0.180815616192726
LOSS train 0.12955697060767876 valid 0.18083500565417046
LOSS train 0.12955697060767876 valid 0.18077495591167142
LOSS train 0.12955697060767876 valid 0.18061945880176036
LOSS train 0.12955697060767876 valid 0.18062627635765494
LOSS train 0.12955697060767876 valid 0.1806644167175446
LOSS train 0.12955697060767876 valid 0.18090174301679052
LOSS train 0.12955697060767876 valid 0.18101160848054332
LOSS train 0.12955697060767876 valid 0.1810970081802393
LOSS train 0.12955697060767876 valid 0.18095434938023344
LOSS train 0.12955697060767876 valid 0.1808898015397376
LOSS train 0.12955697060767876 valid 0.18087356251512351
LOSS train 0.12955697060767876 valid 0.1808048710014139
LOSS train 0.12955697060767876 valid 0.1807253196665704
LOSS train 0.12955697060767876 valid 0.18071813862347466
LOSS train 0.12955697060767876 valid 0.1807289967160387
LOSS train 0.12955697060767876 valid 0.18074940336541942
LOSS train 0.12955697060767876 valid 0.18080695987167492
LOSS train 0.12955697060767876 valid 0.1808036266710986
LOSS train 0.12955697060767876 valid 0.1808376292011985
LOSS train 0.12955697060767876 valid 0.18073470946713532
LOSS train 0.12955697060767876 valid 0.18074205627763504
LOSS train 0.12955697060767876 valid 0.18069547665201954
LOSS train 0.12955697060767876 valid 0.18064322132730748
LOSS train 0.12955697060767876 valid 0.18069493282812735
LOSS train 0.12955697060767876 valid 0.18069390445619246
LOSS train 0.12955697060767876 valid 0.18067636023592817
LOSS train 0.12955697060767876 valid 0.1807313534290823
LOSS train 0.12955697060767876 valid 0.18075462461250727
LOSS train 0.12955697060767876 valid 0.18066373298944505
LOSS train 0.12955697060767876 valid 0.18070692943571054
LOSS train 0.12955697060767876 valid 0.1807813066332967
EPOCH 18:
  batch 1 loss: 0.11336466670036316
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.11821026727557182
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.11400396625200908
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.12277497351169586
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.12539141178131102
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.12665301313002905
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.12100476239408765
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.12284452468156815
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.12200241701470481
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.12126714512705802
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.1200901757587086
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.1191931211700042
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.12023998968876325
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.11983748897910118
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.11828416089216869
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.12049457058310509
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.1197025973130675
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.12255119904875755
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.12247731066063831
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.12212312333285809
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.12268391428958803
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.12325551123781638
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.12347485157458679
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.1253523345415791
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.12454010814428329
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.12473840467058696
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.12505456336118556
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.12468810086803776
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.12420621231712144
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.12484731699029604
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.12545039745107775
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.1258648515213281
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.12502940292611267
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.12574096440392382
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.12719548451048987
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.1264958226432403
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.12641996447298978
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.12602335175401286
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.1265033869407116
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.12650168128311634
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.126673714780226
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.12673176754088628
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.1277159629173057
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.1271869589320638
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.12729844964212841
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.12671773022283678
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.12650016425771915
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.12653209206958613
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.12653748143692406
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.12667268753051758
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.1262486731889201
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.1264317577274946
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.1261706127310699
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.1260216667972229
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.12636623070998626
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.1265870834301625
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.12662977777552187
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.12616832361652933
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.12653279771744194
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.12634697606166204
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.12649613350141245
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.12631863836319215
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.12640900082058376
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.1261592200025916
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.12622607350349427
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.12600771789297913
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.12583281522366538
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.1260897977387204
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.12629575789838599
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.12659749665430614
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.12695122394763247
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.12722067162394524
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.12734696922236927
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.12735581720197522
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.12741089741388956
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.1273048125991696
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.1272436852578993
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.12696269259620935
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.12736854155229616
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.12763113873079418
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.12768565826577905
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.12733924616037345
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.1271561654396804
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.1276031836335148
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.1277523160857313
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.1280347539415193
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.1282867396186138
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.12871803334829482
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.12868155497178602
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.12906598870952923
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.1292608394236355
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.12922591303029787
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.12933132605206582
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.1292477548122406
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.129683518409729
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.12972901513179144
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.12947020572178142
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.12927838833052285
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.12957833172998043
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.1294734013080597
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.12928978176695285
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.12913432286358348
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.1289263910894255
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.12902802551308504
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.1286197242992265
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.1286450926003591
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.12893095675194374
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.1286669105153393
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.1287389189141606
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.12896672155369412
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.1287210305800309
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.12869409112525837
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.12840932264792182
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.128795540254367
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.12857614224371702
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.12845277073311395
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.1288472435031182
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.12889538560125788
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.12877726492260685
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.12875328523417315
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.12872750466027535
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.12875297250317747
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.12889279594750908
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.12890917122844728
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.12895577263832092
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.1290322911171686
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.12905947500326503
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.12881438853219151
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.12874619847582291
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.12875374727524244
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.12858968074540145
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.1285712013416218
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.12888807271208083
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.12899904631411852
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.12877560793249695
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.1287218814934878
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.1288932240466132
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.1288490656601346
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.12907748465700972
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.12905457631817885
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.12905395744328804
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.12892087360083218
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.12896491993557324
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.12911298777908087
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.1289963189897866
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.12907204634114489
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.12907560392707385
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.12902531027793884
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.12939542801988205
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.1294699544707934
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.12936522402116005
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.1294797262863109
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.1294635290803473
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.12917744600540632
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.12923909156553207
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.12921993759198067
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.12928143722616184
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.1292389959945709
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.12935956061448692
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.1294888039585203
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.12960046062373226
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.12951582647216173
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.12949896576397257
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.12972124797723641
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.1298648169094866
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.12975023615073009
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.12959523708698992
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.1295265064885219
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.12933445148743117
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.1296004761229543
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.12963877997377463
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.12962956263055636
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.12943426670366628
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.12935913084396
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.12949129389865058
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.1292863657528704
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.12908850349275405
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.12879791269811353
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.12900523841381073
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.12884015440940857
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.12892311251624514
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.1288855662876433
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.12877603893071576
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.12872475328976693
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.12875356207022795
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.12867870338020787
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.12875786722503244
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.12866651421373196
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.12862857234068018
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.12870097572081968
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.12871249468694806
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.12863004995354763
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.12867326613689334
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.12843482806971393
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.12846678170638207
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.12861247189647082
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.12870414248729115
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.12886193955335953
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.12878722057270645
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.12893078424036503
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.12894812456114374
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.12900432029573045
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.12901985821465553
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.12904798809219808
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.12898073988716777
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.12885873312655005
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.12882062699195843
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.12867414514319256
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.12877369960814572
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.12888023853302003
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.1288549515873335
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.12890060752067925
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.12897906001185028
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.12875509958400905
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.1288533115109732
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.12877063170351363
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.12876362645406328
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.1288508891102371
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.12892114109100272
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.12892695008353752
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.12891764109490683
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.12894141096789558
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.12887559076061164
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.12897236811529314
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.1288752304514249
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.12889262153643422
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.12901877021080596
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.12884815984912085
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.12879760475528293
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.12903804983133854
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.1292986816593579
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.12918862106342768
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.1292755076493828
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.12927347443934178
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.1292836382985115
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.12930438973767272
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.12927386331281582
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.12929645089917824
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.1293047089519361
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.12917941597600777
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.12929498166216855
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.12923688801728989
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.12926650540941537
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.12929102915843002
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.12927533239126204
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.12933542848965987
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.1293224061005994
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.1293850676006367
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.12944852518867775
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.12943296280503272
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.12963703918860728
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.12964293700716797
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.1296705224002774
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.12964057382636182
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.1295917832676102
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.12959863696596585
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.12951821714987552
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.129439513350642
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.12952079195313473
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.12955136923835828
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.12944131256063324
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.12938668823424188
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.1293588919501341
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.12923947628587484
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.1292615812060968
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.1293085105156988
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.12928936527239696
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.1293062000021116
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.12921885963266225
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.12923518077090934
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.12924180704949087
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.1292005703674958
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.12931817243278246
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.1291898904762564
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.12907435885884544
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.12927789222179115
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.12925729284648
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.1292804036316254
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.12924073099769573
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.12925284092447586
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.12920407229683153
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.12913807320362286
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.12914056225610707
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.12910008440974732
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.12921179370921954
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.12934201518138805
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.12925970092260047
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.12922906394427022
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.1291623191076579
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.12916695303444203
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.1291866909546131
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.12931261045781717
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.12949472576786633
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.12951662231768882
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.12948075469772694
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.12950670016879165
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.12949187699893508
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.12943078580018658
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.12937361547679804
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.12932540799180667
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.1293461323279479
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.12933342514053875
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.12932803567880058
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.12929561185209373
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.12924865046974088
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.12923111165173692
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.12928181821146695
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.12923973421384763
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.12919250928757645
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.12911817652563895
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.1291669411195436
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.12914289354991454
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.12908292140442723
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.12906108725412635
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.12906229751450674
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.1291828891144523
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.12918700592751006
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.12917774020688338
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.12909468046176395
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.1291934320703149
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.1291460279038762
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.12909209688905604
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.1290961855290106
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.12900482400975846
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.12901612738004098
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.12892145327538068
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.12881563888686148
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.12880864582683255
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.12887095265928372
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.12894553923697183
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.12898702440034948
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.1289977563028953
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.12892702004841497
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.12894120747725407
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.1290069911684563
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.12902487985168895
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.12901420300986718
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.12901074194608356
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.12892688819616593
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.12891774313414797
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.1289173634258533
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.1291915336460398
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.12924952101255646
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.12929097160177175
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.12927985057450722
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.12924033523984038
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.12921849745580719
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.1292111216134381
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.1291484174593472
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.12906718113592694
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.12906149611344025
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.12914824159815907
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.12923514973847117
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.1292794258833605
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.12927955720626133
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.12917544542068846
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.12927463112258109
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.12933364481399845
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.12924446580635804
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.1291894023410148
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.12916364888157542
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.12924708378660746
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.12920669694084766
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.12916720658540726
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.1291970771877733
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.12918542498529284
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.1292507498035314
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.12934009736889732
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.12930275299927083
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.12921911524357022
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.12927546432477766
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.12926489597446816
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.1292833720830746
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.12936216044951887
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.12938665801286697
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.1293702930053498
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.12934868847975997
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.12941080205654973
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.1293501919405756
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.12932938307915864
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.12930900157671274
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.12924202613493535
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.12931386729600225
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.12932610197458416
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.12924850457287454
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.1292186335810108
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.12924279080034842
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.1291901327323975
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.12917742537272314
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.1291154077801949
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.12909868451030662
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.12911074452710394
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.12923529361192204
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.12926030438866107
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.12926791502705104
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.12921729618024946
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.12911883009696187
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.1291596138634574
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.1291434492047568
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.1291174860112369
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.1291170981050727
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.1290935165809458
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.1290346096748158
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.1290815741118818
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.12911795907550389
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.12907283966030395
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.1290786295616656
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.1291229268718584
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.12906914475786074
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.1290726711110371
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.12906455783368317
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.12912629577286036
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.12914363668270895
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.12918940395260778
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.12925925358950374
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.12933386482584935
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.12932919155207756
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.12928960083774402
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.12934601442057078
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.12931519638569583
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.12937242672236685
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.1294006342112453
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.12932042842708863
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.1293123050083248
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.12931038959937938
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.12920914175854603
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.12921866137682694
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.12918355530040843
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.12910311377340264
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.1290275673533595
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.12907598210874677
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.1290362554912766
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.129048006088574
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.12910617232185356
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.12907976136467922
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.12917062324053102
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.12922387089350132
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.12919488346808033
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.12908432710754572
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.12905905930833383
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.1290806723830381
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.12903467191560236
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.12902347565235456
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.12899817001108113
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.12900035888626335
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.12900522954327642
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.12898309273807795
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.12904068599787674
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.12903991552331134
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.12901165364517106
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.1289832457371403
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.1290348638508436
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.12897483409108967
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.12896845604617166
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.12892416769659126
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.12892893223059282
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.12892658654722655
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.12894211301181513
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.12894916675643983
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.12903880753918834
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.1290305750585947
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.1290276295533686
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.12897365982795123
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.1290062365668087
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.12900874242987684
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.1290372830411907
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.12904417913654376
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.12913506611799583
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.1291839198580683
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.12923225636811966
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.1292756038486578
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.12938537865372027
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.12938537865372027 valid 0.21470648050308228
LOSS train 0.12938537865372027 valid 0.18824230879545212
LOSS train 0.12938537865372027 valid 0.18065342803796133
LOSS train 0.12938537865372027 valid 0.17158732190728188
LOSS train 0.12938537865372027 valid 0.16718686521053314
LOSS train 0.12938537865372027 valid 0.1773156002163887
LOSS train 0.12938537865372027 valid 0.18770229177815573
LOSS train 0.12938537865372027 valid 0.18574289046227932
LOSS train 0.12938537865372027 valid 0.18525746795866224
LOSS train 0.12938537865372027 valid 0.18518959283828734
LOSS train 0.12938537865372027 valid 0.18389937417073685
LOSS train 0.12938537865372027 valid 0.18463439866900444
LOSS train 0.12938537865372027 valid 0.1841018727192512
LOSS train 0.12938537865372027 valid 0.1835427007504872
LOSS train 0.12938537865372027 valid 0.18116938869158428
LOSS train 0.12938537865372027 valid 0.18109190836548805
LOSS train 0.12938537865372027 valid 0.18183189981123982
LOSS train 0.12938537865372027 valid 0.18072663082016838
LOSS train 0.12938537865372027 valid 0.18309720409543892
LOSS train 0.12938537865372027 valid 0.18285277485847473
LOSS train 0.12938537865372027 valid 0.1818986740850267
LOSS train 0.12938537865372027 valid 0.18046039004217496
LOSS train 0.12938537865372027 valid 0.18024118892524554
LOSS train 0.12938537865372027 valid 0.1804461951057116
LOSS train 0.12938537865372027 valid 0.17911008715629578
LOSS train 0.12938537865372027 valid 0.1789980370264787
LOSS train 0.12938537865372027 valid 0.17930435251306603
LOSS train 0.12938537865372027 valid 0.17897045771990502
LOSS train 0.12938537865372027 valid 0.17844075324206515
LOSS train 0.12938537865372027 valid 0.17896808634201686
LOSS train 0.12938537865372027 valid 0.1798245354044822
LOSS train 0.12938537865372027 valid 0.17899940442293882
LOSS train 0.12938537865372027 valid 0.17922721125862814
LOSS train 0.12938537865372027 valid 0.17866921468692668
LOSS train 0.12938537865372027 valid 0.18033427042620523
LOSS train 0.12938537865372027 valid 0.18012619391083717
LOSS train 0.12938537865372027 valid 0.18086368449636409
LOSS train 0.12938537865372027 valid 0.18124300163043172
LOSS train 0.12938537865372027 valid 0.18093289587742242
LOSS train 0.12938537865372027 valid 0.18060951493680477
LOSS train 0.12938537865372027 valid 0.18118054256206606
LOSS train 0.12938537865372027 valid 0.1813988501117343
LOSS train 0.12938537865372027 valid 0.18140232216480168
LOSS train 0.12938537865372027 valid 0.18188752335580913
LOSS train 0.12938537865372027 valid 0.1819671779870987
LOSS train 0.12938537865372027 valid 0.18233142372058786
LOSS train 0.12938537865372027 valid 0.1829940343790866
LOSS train 0.12938537865372027 valid 0.18283350579440594
LOSS train 0.12938537865372027 valid 0.18342413221086776
LOSS train 0.12938537865372027 valid 0.18292227745056153
LOSS train 0.12938537865372027 valid 0.18310736061311236
LOSS train 0.12938537865372027 valid 0.18258352405749834
LOSS train 0.12938537865372027 valid 0.1831151691247832
LOSS train 0.12938537865372027 valid 0.1832523630173118
LOSS train 0.12938537865372027 valid 0.1831694941629063
LOSS train 0.12938537865372027 valid 0.18245755375495978
LOSS train 0.12938537865372027 valid 0.1823966890050654
LOSS train 0.12938537865372027 valid 0.1822606492145308
LOSS train 0.12938537865372027 valid 0.1826210514468662
LOSS train 0.12938537865372027 valid 0.18244894246260326
LOSS train 0.12938537865372027 valid 0.1820330417058507
LOSS train 0.12938537865372027 valid 0.18256496133342867
LOSS train 0.12938537865372027 valid 0.1819182825939996
LOSS train 0.12938537865372027 valid 0.18282991112209857
LOSS train 0.12938537865372027 valid 0.18305843885128315
LOSS train 0.12938537865372027 valid 0.18281313980167563
LOSS train 0.12938537865372027 valid 0.18221506832250908
LOSS train 0.12938537865372027 valid 0.18222603705876014
LOSS train 0.12938537865372027 valid 0.18183859275734943
LOSS train 0.12938537865372027 valid 0.1824088752269745
LOSS train 0.12938537865372027 valid 0.18205731897286967
LOSS train 0.12938537865372027 valid 0.1822587230967151
LOSS train 0.12938537865372027 valid 0.18225488250386224
LOSS train 0.12938537865372027 valid 0.18225631661511757
LOSS train 0.12938537865372027 valid 0.18261930346488953
LOSS train 0.12938537865372027 valid 0.1830386935096038
LOSS train 0.12938537865372027 valid 0.18287317067771763
LOSS train 0.12938537865372027 valid 0.18266182373731565
LOSS train 0.12938537865372027 valid 0.18232136021686507
LOSS train 0.12938537865372027 valid 0.1816665969789028
LOSS train 0.12938537865372027 valid 0.18131120245397828
LOSS train 0.12938537865372027 valid 0.18178032556685006
LOSS train 0.12938537865372027 valid 0.18156952204474483
LOSS train 0.12938537865372027 valid 0.18157335761047544
LOSS train 0.12938537865372027 valid 0.18133997671744403
LOSS train 0.12938537865372027 valid 0.1811276428228201
LOSS train 0.12938537865372027 valid 0.18090761118921742
LOSS train 0.12938537865372027 valid 0.18061660619621928
LOSS train 0.12938537865372027 valid 0.18115130652872363
LOSS train 0.12938537865372027 valid 0.18112505194213654
LOSS train 0.12938537865372027 valid 0.18121655105234502
LOSS train 0.12938537865372027 valid 0.18120180345747783
LOSS train 0.12938537865372027 valid 0.1809870917950907
LOSS train 0.12938537865372027 valid 0.18117744443898506
LOSS train 0.12938537865372027 valid 0.18111018695329364
LOSS train 0.12938537865372027 valid 0.18119102157652378
LOSS train 0.12938537865372027 valid 0.18104231111782113
LOSS train 0.12938537865372027 valid 0.18125364153969045
LOSS train 0.12938537865372027 valid 0.18132249092814898
LOSS train 0.12938537865372027 valid 0.1814132611453533
LOSS train 0.12938537865372027 valid 0.18159399100459447
LOSS train 0.12938537865372027 valid 0.1818057735468827
LOSS train 0.12938537865372027 valid 0.1814783811569214
LOSS train 0.12938537865372027 valid 0.18163606237906676
LOSS train 0.12938537865372027 valid 0.18173080881436665
LOSS train 0.12938537865372027 valid 0.18218065573359435
LOSS train 0.12938537865372027 valid 0.18207835677628206
LOSS train 0.12938537865372027 valid 0.1822321699173362
LOSS train 0.12938537865372027 valid 0.1826267785162007
LOSS train 0.12938537865372027 valid 0.18268930194052782
LOSS train 0.12938537865372027 valid 0.18263447257849547
LOSS train 0.12938537865372027 valid 0.1824635686352849
LOSS train 0.12938537865372027 valid 0.18250988713935412
LOSS train 0.12938537865372027 valid 0.1828133869066573
LOSS train 0.12938537865372027 valid 0.18304194199002308
LOSS train 0.12938537865372027 valid 0.1832506205758144
LOSS train 0.12938537865372027 valid 0.1831929547409726
LOSS train 0.12938537865372027 valid 0.1827950324800055
LOSS train 0.12938537865372027 valid 0.18244638663380086
LOSS train 0.12938537865372027 valid 0.18209516033530235
LOSS train 0.12938537865372027 valid 0.18193000776708618
LOSS train 0.12938537865372027 valid 0.18197602546605907
LOSS train 0.12938537865372027 valid 0.18175652368766507
LOSS train 0.12938537865372027 valid 0.18209005736054912
LOSS train 0.12938537865372027 valid 0.1819208596944809
LOSS train 0.12938537865372027 valid 0.1822906352934383
LOSS train 0.12938537865372027 valid 0.18213285623103614
LOSS train 0.12938537865372027 valid 0.18219267157837749
LOSS train 0.12938537865372027 valid 0.1823358117609985
LOSS train 0.12938537865372027 valid 0.18195138069299552
LOSS train 0.12938537865372027 valid 0.1816427434673746
LOSS train 0.12938537865372027 valid 0.18132311602433523
LOSS train 0.12938537865372027 valid 0.1812186613118738
LOSS train 0.12938537865372027 valid 0.1813525964519871
LOSS train 0.12938537865372027 valid 0.18123349823333598
LOSS train 0.12938537865372027 valid 0.18124061800977764
LOSS train 0.12938537865372027 valid 0.18091446509326461
LOSS train 0.12938537865372027 valid 0.1808444576850836
LOSS train 0.12938537865372027 valid 0.18077527565493
LOSS train 0.12938537865372027 valid 0.18087018792118345
LOSS train 0.12938537865372027 valid 0.18072133918180533
LOSS train 0.12938537865372027 valid 0.18074145724236126
LOSS train 0.12938537865372027 valid 0.18062079036152445
LOSS train 0.12938537865372027 valid 0.1806224114778969
LOSS train 0.12938537865372027 valid 0.18049830280501267
LOSS train 0.12938537865372027 valid 0.18070683413988922
LOSS train 0.12938537865372027 valid 0.18063750305548817
LOSS train 0.12938537865372027 valid 0.18155197225309708
LOSS train 0.12938537865372027 valid 0.1817055773615037
LOSS train 0.12938537865372027 valid 0.18165761590003968
LOSS train 0.12938537865372027 valid 0.18190113459991303
LOSS train 0.12938537865372027 valid 0.18155614551352828
LOSS train 0.12938537865372027 valid 0.18149823563940384
LOSS train 0.12938537865372027 valid 0.18135513094338504
LOSS train 0.12938537865372027 valid 0.18124299635810237
LOSS train 0.12938537865372027 valid 0.1812588713872127
LOSS train 0.12938537865372027 valid 0.18135034753258822
LOSS train 0.12938537865372027 valid 0.18134108647892747
LOSS train 0.12938537865372027 valid 0.18144246532857042
LOSS train 0.12938537865372027 valid 0.1813693975098431
LOSS train 0.12938537865372027 valid 0.18122361128374656
LOSS train 0.12938537865372027 valid 0.1810995791061425
LOSS train 0.12938537865372027 valid 0.18081794286066769
LOSS train 0.12938537865372027 valid 0.18063126731573081
LOSS train 0.12938537865372027 valid 0.18046853668761975
LOSS train 0.12938537865372027 valid 0.18055134410240564
LOSS train 0.12938537865372027 valid 0.18089938529594216
LOSS train 0.12938537865372027 valid 0.18080347226489157
LOSS train 0.12938537865372027 valid 0.1809744911610022
LOSS train 0.12938537865372027 valid 0.18097621842342265
LOSS train 0.12938537865372027 valid 0.18098408149348366
LOSS train 0.12938537865372027 valid 0.18085343278078145
LOSS train 0.12938537865372027 valid 0.18082748215667085
LOSS train 0.12938537865372027 valid 0.18078277354267822
LOSS train 0.12938537865372027 valid 0.18046149253845215
LOSS train 0.12938537865372027 valid 0.18041925445537677
LOSS train 0.12938537865372027 valid 0.18051687876383463
LOSS train 0.12938537865372027 valid 0.1807605063312509
LOSS train 0.12938537865372027 valid 0.18066945031035544
LOSS train 0.12938537865372027 valid 0.18058517550428707
LOSS train 0.12938537865372027 valid 0.18072954034278407
LOSS train 0.12938537865372027 valid 0.18057688997014537
LOSS train 0.12938537865372027 valid 0.180638872616278
LOSS train 0.12938537865372027 valid 0.18050817716056886
LOSS train 0.12938537865372027 valid 0.18037150405548713
LOSS train 0.12938537865372027 valid 0.1804128544785643
LOSS train 0.12938537865372027 valid 0.18025609221369188
LOSS train 0.12938537865372027 valid 0.18024566365366287
LOSS train 0.12938537865372027 valid 0.18010806903321908
LOSS train 0.12938537865372027 valid 0.18019823831947226
LOSS train 0.12938537865372027 valid 0.18005898446624816
LOSS train 0.12938537865372027 valid 0.18012179504148662
LOSS train 0.12938537865372027 valid 0.1798447113426238
LOSS train 0.12938537865372027 valid 0.17977032119158617
LOSS train 0.12938537865372027 valid 0.17956461211045582
LOSS train 0.12938537865372027 valid 0.17952418236099943
LOSS train 0.12938537865372027 valid 0.179731658494412
LOSS train 0.12938537865372027 valid 0.17968134296060811
LOSS train 0.12938537865372027 valid 0.17980916962851232
LOSS train 0.12938537865372027 valid 0.17963547855615616
LOSS train 0.12938537865372027 valid 0.1794494244441464
LOSS train 0.12938537865372027 valid 0.1793855814502971
LOSS train 0.12938537865372027 valid 0.1793858998367939
LOSS train 0.12938537865372027 valid 0.17958402290355927
LOSS train 0.12938537865372027 valid 0.17934483406020374
LOSS train 0.12938537865372027 valid 0.17936145355111188
LOSS train 0.12938537865372027 valid 0.17929005349315882
LOSS train 0.12938537865372027 valid 0.1791336089372635
LOSS train 0.12938537865372027 valid 0.17913450179487894
LOSS train 0.12938537865372027 valid 0.17905782007035756
LOSS train 0.12938537865372027 valid 0.17906066796508446
LOSS train 0.12938537865372027 valid 0.17901472220162176
LOSS train 0.12938537865372027 valid 0.17897654279296946
LOSS train 0.12938537865372027 valid 0.17900713721168376
LOSS train 0.12938537865372027 valid 0.17888501577599103
LOSS train 0.12938537865372027 valid 0.17874872670681388
LOSS train 0.12938537865372027 valid 0.17867116163128532
LOSS train 0.12938537865372027 valid 0.17870681555172718
LOSS train 0.12938537865372027 valid 0.17880729983930718
LOSS train 0.12938537865372027 valid 0.17876123548908668
LOSS train 0.12938537865372027 valid 0.17867598281456873
LOSS train 0.12938537865372027 valid 0.17871391276518503
LOSS train 0.12938537865372027 valid 0.17890242510579626
LOSS train 0.12938537865372027 valid 0.17900635694552744
LOSS train 0.12938537865372027 valid 0.17917249434524113
LOSS train 0.12938537865372027 valid 0.17943503826329138
LOSS train 0.12938537865372027 valid 0.17949575770006307
LOSS train 0.12938537865372027 valid 0.17956108240443364
LOSS train 0.12938537865372027 valid 0.1795611452457686
LOSS train 0.12938537865372027 valid 0.17960154252207797
LOSS train 0.12938537865372027 valid 0.17975412767170829
LOSS train 0.12938537865372027 valid 0.17977192683209633
LOSS train 0.12938537865372027 valid 0.17987461464855292
LOSS train 0.12938537865372027 valid 0.1799331634090497
LOSS train 0.12938537865372027 valid 0.18009635412946662
LOSS train 0.12938537865372027 valid 0.1799787899826543
LOSS train 0.12938537865372027 valid 0.17991145407851739
LOSS train 0.12938537865372027 valid 0.1799416367371543
LOSS train 0.12938537865372027 valid 0.17973300340784146
LOSS train 0.12938537865372027 valid 0.1797283198684454
LOSS train 0.12938537865372027 valid 0.17992783495499384
LOSS train 0.12938537865372027 valid 0.17976783991845185
LOSS train 0.12938537865372027 valid 0.17999043177675317
LOSS train 0.12938537865372027 valid 0.1801773407420174
LOSS train 0.12938537865372027 valid 0.18024305956704276
LOSS train 0.12938537865372027 valid 0.18009038439126518
LOSS train 0.12938537865372027 valid 0.18024522247102095
LOSS train 0.12938537865372027 valid 0.18018504869072668
LOSS train 0.12938537865372027 valid 0.18018752575878158
LOSS train 0.12938537865372027 valid 0.18021104431152343
LOSS train 0.12938537865372027 valid 0.18004166100367133
LOSS train 0.12938537865372027 valid 0.1802999862129726
LOSS train 0.12938537865372027 valid 0.180239111126176
LOSS train 0.12938537865372027 valid 0.18012324474223956
LOSS train 0.12938537865372027 valid 0.18016830741190443
LOSS train 0.12938537865372027 valid 0.1802292896900326
LOSS train 0.12938537865372027 valid 0.18004928071211285
LOSS train 0.12938537865372027 valid 0.18030774882135464
LOSS train 0.12938537865372027 valid 0.18031536433917675
LOSS train 0.12938537865372027 valid 0.18015784323215484
LOSS train 0.12938537865372027 valid 0.1803197314341863
LOSS train 0.12938537865372027 valid 0.18039219087089292
LOSS train 0.12938537865372027 valid 0.18043431883302477
LOSS train 0.12938537865372027 valid 0.18052827933746757
LOSS train 0.12938537865372027 valid 0.1804874927930112
LOSS train 0.12938537865372027 valid 0.18050462999066016
LOSS train 0.12938537865372027 valid 0.1805576259612144
LOSS train 0.12938537865372027 valid 0.18075556424793912
LOSS train 0.12938537865372027 valid 0.18084003723686956
LOSS train 0.12938537865372027 valid 0.18084030598402023
LOSS train 0.12938537865372027 valid 0.18094128265811948
LOSS train 0.12938537865372027 valid 0.1812606298857752
LOSS train 0.12938537865372027 valid 0.18146890647463745
LOSS train 0.12938537865372027 valid 0.1815096304146913
LOSS train 0.12938537865372027 valid 0.18148514677177777
LOSS train 0.12938537865372027 valid 0.18141995122035345
LOSS train 0.12938537865372027 valid 0.18130479489422877
LOSS train 0.12938537865372027 valid 0.18115638116066404
LOSS train 0.12938537865372027 valid 0.18114210547725787
LOSS train 0.12938537865372027 valid 0.18113333391291755
LOSS train 0.12938537865372027 valid 0.18106398315192118
LOSS train 0.12938537865372027 valid 0.18075892308079605
LOSS train 0.12938537865372027 valid 0.18068502211528617
LOSS train 0.12938537865372027 valid 0.18070780270746056
LOSS train 0.12938537865372027 valid 0.18074903132622702
LOSS train 0.12938537865372027 valid 0.1807609630839808
LOSS train 0.12938537865372027 valid 0.18072380862792609
LOSS train 0.12938537865372027 valid 0.18070169631391764
LOSS train 0.12938537865372027 valid 0.18071325881258427
LOSS train 0.12938537865372027 valid 0.18074826546784106
LOSS train 0.12938537865372027 valid 0.1805845984990654
LOSS train 0.12938537865372027 valid 0.18057267577068448
LOSS train 0.12938537865372027 valid 0.18058176679415913
LOSS train 0.12938537865372027 valid 0.1806606860185156
LOSS train 0.12938537865372027 valid 0.18073699701640566
LOSS train 0.12938537865372027 valid 0.18065442482160554
LOSS train 0.12938537865372027 valid 0.18074253933999676
LOSS train 0.12938537865372027 valid 0.18077611888215045
LOSS train 0.12938537865372027 valid 0.18080465071974788
LOSS train 0.12938537865372027 valid 0.18090450872977576
LOSS train 0.12938537865372027 valid 0.1808516999019737
LOSS train 0.12938537865372027 valid 0.180800897692213
LOSS train 0.12938537865372027 valid 0.18084909430038024
LOSS train 0.12938537865372027 valid 0.18083710620473875
LOSS train 0.12938537865372027 valid 0.18077549763390274
LOSS train 0.12938537865372027 valid 0.18080801497292676
LOSS train 0.12938537865372027 valid 0.1807878091688653
LOSS train 0.12938537865372027 valid 0.18068475048843916
LOSS train 0.12938537865372027 valid 0.18068375378171989
LOSS train 0.12938537865372027 valid 0.18073516656314173
LOSS train 0.12938537865372027 valid 0.1807286345498738
LOSS train 0.12938537865372027 valid 0.1807106264317647
LOSS train 0.12938537865372027 valid 0.18089640221466272
LOSS train 0.12938537865372027 valid 0.1809041573173681
LOSS train 0.12938537865372027 valid 0.18082165680234394
LOSS train 0.12938537865372027 valid 0.1807939147553112
LOSS train 0.12938537865372027 valid 0.1808610375653306
LOSS train 0.12938537865372027 valid 0.1809003161465597
LOSS train 0.12938537865372027 valid 0.18106297396567175
LOSS train 0.12938537865372027 valid 0.18099304800853133
LOSS train 0.12938537865372027 valid 0.18116773943477701
LOSS train 0.12938537865372027 valid 0.18116072556061774
LOSS train 0.12938537865372027 valid 0.18111419516266683
LOSS train 0.12938537865372027 valid 0.18122729548701533
LOSS train 0.12938537865372027 valid 0.1812284045953017
LOSS train 0.12938537865372027 valid 0.18139155401042634
LOSS train 0.12938537865372027 valid 0.18143959165713108
LOSS train 0.12938537865372027 valid 0.1813959287615811
LOSS train 0.12938537865372027 valid 0.18144773291055916
LOSS train 0.12938537865372027 valid 0.1814413473913164
LOSS train 0.12938537865372027 valid 0.18132591180030672
LOSS train 0.12938537865372027 valid 0.1812012777271041
LOSS train 0.12938537865372027 valid 0.18125810351099697
LOSS train 0.12938537865372027 valid 0.1813382409557611
LOSS train 0.12938537865372027 valid 0.18129251012161596
LOSS train 0.12938537865372027 valid 0.18136702859330744
LOSS train 0.12938537865372027 valid 0.18136293589184474
LOSS train 0.12938537865372027 valid 0.18135234488714377
LOSS train 0.12938537865372027 valid 0.18137458164607528
LOSS train 0.12938537865372027 valid 0.1813129809411133
LOSS train 0.12938537865372027 valid 0.18115638991779595
LOSS train 0.12938537865372027 valid 0.18116300666367102
LOSS train 0.12938537865372027 valid 0.18120217253793433
LOSS train 0.12938537865372027 valid 0.18144586448406064
LOSS train 0.12938537865372027 valid 0.18155659890693168
LOSS train 0.12938537865372027 valid 0.1816438598870542
LOSS train 0.12938537865372027 valid 0.18149944482859687
LOSS train 0.12938537865372027 valid 0.181434684358109
LOSS train 0.12938537865372027 valid 0.18141928345050373
LOSS train 0.12938537865372027 valid 0.18135136600051607
LOSS train 0.12938537865372027 valid 0.18127044249973406
LOSS train 0.12938537865372027 valid 0.18126586227762428
LOSS train 0.12938537865372027 valid 0.18127629443707616
LOSS train 0.12938537865372027 valid 0.181295490029168
LOSS train 0.12938537865372027 valid 0.18135083657754977
LOSS train 0.12938537865372027 valid 0.18134140604165164
LOSS train 0.12938537865372027 valid 0.18137498913692826
LOSS train 0.12938537865372027 valid 0.1812721767655298
LOSS train 0.12938537865372027 valid 0.1812804272828992
LOSS train 0.12938537865372027 valid 0.18123160414397715
LOSS train 0.12938537865372027 valid 0.1811781197868886
LOSS train 0.12938537865372027 valid 0.1812299745599868
LOSS train 0.12938537865372027 valid 0.18123087783654532
LOSS train 0.12938537865372027 valid 0.18121663693870818
LOSS train 0.12938537865372027 valid 0.1812692201300843
LOSS train 0.12938537865372027 valid 0.18130195332192334
LOSS train 0.12938537865372027 valid 0.1812106900384056
LOSS train 0.12938537865372027 valid 0.18125340779838356
LOSS train 0.12938537865372027 valid 0.18133252976224998
EPOCH 19:
  batch 1 loss: 0.11389385908842087
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.1157870702445507
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.11284520973761876
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.12122171558439732
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.1244540497660637
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.12680350616574287
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.12112015272889819
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.1231605764478445
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.12240481376647949
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.12146348804235459
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.11964344030076807
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.11867220203081767
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.1196367465532743
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.11936966116939272
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.11805442919333776
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.11996244871988893
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.11896176110295688
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.122069351375103
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.1217410933030279
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.12109872065484524
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.12157428158181054
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.12214514511552724
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.122332986606204
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.12437447936584552
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.12368645638227463
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.12388894105186829
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.124354039353353
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.12403346784412861
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.12354341843004885
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.1241737795372804
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.12477818540027065
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.12519168690778315
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.1244461303859046
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.12512553110718727
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.12661867418459483
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.12586390578912365
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.12581339699996485
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.1254508991382624
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.12607406289913717
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.12592181656509638
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.12602766966674386
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.12615079663339115
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.12724138536425525
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.12683091397312554
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.1269672554400232
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.1263735108077526
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.12628275948636075
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.12630170366416374
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.12631025454219508
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.12642229944467545
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.1260210908218926
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.12611109663087589
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.12589357933908138
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.12567291298398264
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.12604610350998965
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.12622724340430327
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.12637675200638018
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.12589216270837292
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.12619611002125983
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.1260332421710094
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.126127976008126
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.12604221112785802
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.12613627326393884
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.1259368589380756
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.12601237079271904
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.125831258229234
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.1256209371694878
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.12583871141952627
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.12611796912075818
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.12634549949850354
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.1266712139190083
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.1269001652383142
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.12696436435392458
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.1269653012623658
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.12701989928881327
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.12683704338575663
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.12681205570697784
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.12648103424371818
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.12688305087481874
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.12714482843875885
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.1272654452441651
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.1268627086608875
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.12673077454049902
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.1271813444438435
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.12728047107948975
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.12762591606655785
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.12796098987261453
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.12840041755275292
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.1283734700318133
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.12871752248870003
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.12893181384264768
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.12892041808885077
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.1290114664582796
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.12894664467015166
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.12945880529127624
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.12952817852298418
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.12930320433734618
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.129133902718218
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.12939127820609797
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.1292689446359873
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.12911863617672778
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.1289493917104076
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.12873285697791184
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.1288572039025334
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.12844424765734447
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.12844373812934137
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.12869567563322104
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.1284567005932331
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.12860660172930552
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.12881297780708834
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.1285485158766712
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.12853771321741597
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.12824678144096274
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.12860992237141258
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.12840404329092606
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.12829540233159886
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.12866627125658542
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.1286865696310997
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.1285631884546841
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.1285623123248418
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.12855478160637468
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.12862053721166047
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.12880021164087746
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.12875369846099807
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.12878379124403
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.12886541401819576
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.12888751824305753
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.1286601016181521
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.12857164586930311
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.12856878185501466
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.12846651963377728
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.1284402118374904
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.1287545067139138
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.12884047099236232
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.1286166634272646
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.12855091143180342
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.1287173346011308
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.12866998463869095
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.12890458428602425
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.1289006025663444
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.1289158376818853
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.12877060368027485
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.12884615997334462
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.1289469421737724
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.12887286847007687
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.12897011142683357
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.12900238261133634
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.12892735970986857
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.12930357206187792
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.12934988091389338
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.1292207296320934
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.1293413817490402
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.12928875428594017
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.1289681036073666
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.12901494834692248
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.12899343043756792
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.12906148968039044
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.12904530570288247
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.12914528304113532
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.12923854789696634
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.1293374613297652
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.1292458127402229
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.12924885452707852
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.12944526638744808
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.12958751081517247
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.12947195588824262
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.12934242474461743
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.12927333092583077
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.12907948089245508
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.12931267904008137
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.12938266020943548
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.12932320134064487
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.12912517105568352
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.1290554947209084
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.129189384494509
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.12899128495800224
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.1287760459686403
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.12851237928432024
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.12870380702797926
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.1285368895365132
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.12860584119077545
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.12857661298015616
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.12847970943144763
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.12842027409731047
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.12848893561073252
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.12840216955350292
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.12847347322472913
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.12840529360828248
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.12836460008349998
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.12843229649098295
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.12844196418349033
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.1283636518443624
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.12839748474909235
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.12814560432716743
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.1281776072887274
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.12830808082101297
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.12842960065694026
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.1285843580509677
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.12852582384144243
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.12864546965807677
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.1286805892242721
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.12873010481200595
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.12876952586356055
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.12881109333944088
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.12876994006517456
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.1286344799818923
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.12858406835852038
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.128449510030735
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.12853567084769882
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.12865310414206413
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.12864970211027923
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.12868798028607414
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.1287715678133875
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.12854739891312947
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.12863562384317087
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.12854300949860503
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.12852443730638874
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.12862249562909844
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.12868126624659315
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.12868296053599226
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.12871451544411042
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.12874486810854963
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.1287051526487141
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.12877903416353678
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.12869215574529436
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.1287185096107753
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.12882372133007133
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.12864916160571993
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.12861087202505253
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.12885845037906066
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.1291225195834131
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.1290449072638976
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.12910309654576585
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.12911801571902046
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.12912530515422213
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.1291427762263407
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.12909487964986247
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.12911569809212403
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.12913896617280887
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.12902599973604084
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.1291276525350527
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.12906929792082014
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.12912598292155522
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.12917867562443505
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.12917341738939286
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.12922303666307675
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.1292202471178553
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.12928865540532336
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.1293387199680968
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.12930918794870377
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.12949977049314643
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.1295156107535438
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.12955553629417194
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.12953450926762866
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.1294922157829883
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.12951446330407634
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.12945442101718851
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.12938092833803605
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.12945659510417334
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.12949079263668795
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.1293939878429033
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.12931773862765947
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.12927730200181894
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.12915010945024816
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.1291653391606403
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.1291907591311107
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.1291640962889132
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.1291818240129236
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.12909076171736736
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.1291050445150446
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.12912323664035305
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.1290711536181762
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.1291969047088326
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.12906036626574766
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.12895676791667937
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.12918105354343634
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.12915852948323914
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.12917375856702276
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.12911978259842882
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.12912766499710934
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.1290802048565654
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.12899989341484738
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.12898462800170846
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.1289396287847153
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.1290661078534628
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.12917658178643746
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.12909714270136496
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.12907521354241502
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.1289981502047047
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.12901546564595454
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.12903598078952214
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.1291425415404039
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.12931797793293975
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.1293562723158979
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.1293324584425506
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.12934604516202533
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.12933406577467518
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.1292880487522023
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.1292386723551463
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.12921619420250258
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.12923521273753968
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.12924030886975346
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.1292465510639814
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.12920466124227173
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.12916830829421028
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.12916645468547452
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.12921135814356882
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.12917931160175955
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.12913130399768022
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.129043986984799
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.12912822874125177
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.12909967008118445
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.12903190527956326
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.12900719950628128
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.12901139025177275
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.12914002500474453
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.12913005789653736
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.12911828684638132
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.12903800284414083
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.1291160497814417
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.12907896889518725
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.12901745583885205
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.12903900887390407
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.1289442104321939
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.128947507418119
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.12884964583658734
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.12874139239299554
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.1287460444358791
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.1287999480600415
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.12887973740245356
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.12892054512061382
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.12891633243086825
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.12886561171428576
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.12889281789699714
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.1289593917665197
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.12898532505191507
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.12896459220248088
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.12896068270742542
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.1288892808960954
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.12887116944526925
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.12887215301875146
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.12911712341228423
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.12916800812017118
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.12921583845258452
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.1292184875711151
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.12919535894872824
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.1291959925369502
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.1292021773627092
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.12912682911386464
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.12904239179832594
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.12903739492862654
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.12911856320517306
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.12920410489985354
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.12924652519283322
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.12923475333922346
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.1291454533387101
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.12922922559395558
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.12930271547683125
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.1292243478067406
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.12917738900416428
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.12914032662426667
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.12921956224747785
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.12918311591483345
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.12914508540224243
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.1291806114863043
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.1291809482652633
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.12925477021396647
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.129344138154841
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.1293224749082149
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.1292385580370555
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.129308373197873
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.12928724931853433
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.1293032714852379
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.12938130298320621
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.12940199130773544
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.1293812013290664
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.12935582757865718
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.12941919020795947
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.1293556695523866
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.1293434362466398
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.12932546583451623
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.1292553025624515
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.1293295977865747
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.12934167826703438
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.12926053228703413
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.12922161001075116
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.12924804407573792
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.12919505806543777
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.12920000863795417
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.1291210197676451
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.12910983217951585
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.12912269008859079
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.1292517234881719
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.12928494451766087
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.1293102166697949
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.12925312455480148
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.1291529090713794
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.12918732966759697
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.12916027442703867
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.12913721160963176
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.12913919281112285
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.12911442303983725
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.12905508675439187
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.12909861344217075
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.1291314129476194
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.12908860384096654
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.12910294239878362
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.1291529027124246
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.12910177179755675
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.12909668489563755
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.12908384463140274
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.12914027098240782
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.12916431839399709
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.1292208655408903
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.12928100750747934
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.1293648775631132
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.129359844080407
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.1293230336426833
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.12937516459488357
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.1293392569891044
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.12939107549020626
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.12942149963282862
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.12933797729212623
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.1293282464568345
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.12930496277178036
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.12920559533465076
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.12923094787279393
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.12918660579092592
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.12911290046704677
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.1290410365649434
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.12909746828668356
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.12906240630480978
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.1290766665621663
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.12912245860434898
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.12908888866846588
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.12918312112809321
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.1292516983005493
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.12921353553732237
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.12910581733140425
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.12908676789904183
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.12910148232769805
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.12905564221524005
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.12903568399208123
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.12901344204002674
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.12901410128963128
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.12901946862182276
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.12899571720519054
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.12905536269369935
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.1290422887572467
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.12901774822009934
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.12899203994982522
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.1290445458638457
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.12899193111765989
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.12899351316926763
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.12894227177232176
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.12894659992634205
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.12894991444550144
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.12897915724974013
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.12899448870313973
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.12908025050292846
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.12906589577229058
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.1290528452854394
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.12898733161116263
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.1290096316711399
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.1290170800461564
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.12904087219051538
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.1290495272654027
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.12913921948235768
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.1291839219073751
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.12923449190690162
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.12927225648154625
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.12937682533983963
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.12937682533983963 valid 0.21610036492347717
LOSS train 0.12937682533983963 valid 0.18975462764501572
LOSS train 0.12937682533983963 valid 0.18188919126987457
LOSS train 0.12937682533983963 valid 0.17248112335801125
LOSS train 0.12937682533983963 valid 0.1681204319000244
LOSS train 0.12937682533983963 valid 0.17840025325616202
LOSS train 0.12937682533983963 valid 0.1888239553996495
LOSS train 0.12937682533983963 valid 0.18683568947017193
LOSS train 0.12937682533983963 valid 0.18624822133117253
LOSS train 0.12937682533983963 valid 0.18614941835403442
LOSS train 0.12937682533983963 valid 0.18484050983732397
LOSS train 0.12937682533983963 valid 0.18557659784952799
LOSS train 0.12937682533983963 valid 0.1850842501108463
LOSS train 0.12937682533983963 valid 0.184585662824767
LOSS train 0.12937682533983963 valid 0.18223229746023814
LOSS train 0.12937682533983963 valid 0.18215749133378267
LOSS train 0.12937682533983963 valid 0.1828161539400325
LOSS train 0.12937682533983963 valid 0.18173171828190485
LOSS train 0.12937682533983963 valid 0.1840441462240721
LOSS train 0.12937682533983963 valid 0.1838676042854786
LOSS train 0.12937682533983963 valid 0.18286784418991633
LOSS train 0.12937682533983963 valid 0.18137326701120896
LOSS train 0.12937682533983963 valid 0.18113532856754636
LOSS train 0.12937682533983963 valid 0.18135124382873377
LOSS train 0.12937682533983963 valid 0.18003546059131623
LOSS train 0.12937682533983963 valid 0.17991465731309011
LOSS train 0.12937682533983963 valid 0.18024449823079286
LOSS train 0.12937682533983963 valid 0.17993817425199918
LOSS train 0.12937682533983963 valid 0.17941538784010658
LOSS train 0.12937682533983963 valid 0.1799357518553734
LOSS train 0.12937682533983963 valid 0.1808056648700468
LOSS train 0.12937682533983963 valid 0.1799657754600048
LOSS train 0.12937682533983963 valid 0.1802703711119565
LOSS train 0.12937682533983963 valid 0.1796936918707455
LOSS train 0.12937682533983963 valid 0.18139850241797312
LOSS train 0.12937682533983963 valid 0.18118563873900306
LOSS train 0.12937682533983963 valid 0.18193881576125687
LOSS train 0.12937682533983963 valid 0.18232768460323936
LOSS train 0.12937682533983963 valid 0.18201602804355133
LOSS train 0.12937682533983963 valid 0.1816829640418291
LOSS train 0.12937682533983963 valid 0.18225238272329655
LOSS train 0.12937682533983963 valid 0.18244127397026336
LOSS train 0.12937682533983963 valid 0.18243224017841872
LOSS train 0.12937682533983963 valid 0.18291364745660263
LOSS train 0.12937682533983963 valid 0.1829846180147595
LOSS train 0.12937682533983963 valid 0.18338837053464807
LOSS train 0.12937682533983963 valid 0.18402929667462695
LOSS train 0.12937682533983963 valid 0.1838770288353165
LOSS train 0.12937682533983963 valid 0.1844725970711027
LOSS train 0.12937682533983963 valid 0.183962319791317
LOSS train 0.12937682533983963 valid 0.18415044978553174
LOSS train 0.12937682533983963 valid 0.1836100507241029
LOSS train 0.12937682533983963 valid 0.18414212791424878
LOSS train 0.12937682533983963 valid 0.18427952627340952
LOSS train 0.12937682533983963 valid 0.18418676121668381
LOSS train 0.12937682533983963 valid 0.18347971034901484
LOSS train 0.12937682533983963 valid 0.18342177282299912
LOSS train 0.12937682533983963 valid 0.18328124747194094
LOSS train 0.12937682533983963 valid 0.1836467958607916
LOSS train 0.12937682533983963 valid 0.18346170435349146
LOSS train 0.12937682533983963 valid 0.18303628265857697
LOSS train 0.12937682533983963 valid 0.18357397135226958
LOSS train 0.12937682533983963 valid 0.1829228935733674
LOSS train 0.12937682533983963 valid 0.1838205570820719
LOSS train 0.12937682533983963 valid 0.1840328865326368
LOSS train 0.12937682533983963 valid 0.18376422718618857
LOSS train 0.12937682533983963 valid 0.18317903934130028
LOSS train 0.12937682533983963 valid 0.18318304397604046
LOSS train 0.12937682533983963 valid 0.18280798283176145
LOSS train 0.12937682533983963 valid 0.18336802188839232
LOSS train 0.12937682533983963 valid 0.1830080508346289
LOSS train 0.12937682533983963 valid 0.18318935359517732
LOSS train 0.12937682533983963 valid 0.1831950504485875
LOSS train 0.12937682533983963 valid 0.18318916917652697
LOSS train 0.12937682533983963 valid 0.18355728805065155
LOSS train 0.12937682533983963 valid 0.1839863830491116
LOSS train 0.12937682533983963 valid 0.18381033063708963
LOSS train 0.12937682533983963 valid 0.18359737155529168
LOSS train 0.12937682533983963 valid 0.18325806399689445
LOSS train 0.12937682533983963 valid 0.18259395342320203
LOSS train 0.12937682533983963 valid 0.1822509480479323
LOSS train 0.12937682533983963 valid 0.1827312945956137
LOSS train 0.12937682533983963 valid 0.18253116255783172
LOSS train 0.12937682533983963 valid 0.18253574289736293
LOSS train 0.12937682533983963 valid 0.18230252336053288
LOSS train 0.12937682533983963 valid 0.18208079369262206
LOSS train 0.12937682533983963 valid 0.18186435291821929
LOSS train 0.12937682533983963 valid 0.18157568827948786
LOSS train 0.12937682533983963 valid 0.18209644733520036
LOSS train 0.12937682533983963 valid 0.1820647367172771
LOSS train 0.12937682533983963 valid 0.18215505385791864
LOSS train 0.12937682533983963 valid 0.18213225609582404
LOSS train 0.12937682533983963 valid 0.18191972775484927
LOSS train 0.12937682533983963 valid 0.1820998898846038
LOSS train 0.12937682533983963 valid 0.18203503489494324
LOSS train 0.12937682533983963 valid 0.1821045548034211
LOSS train 0.12937682533983963 valid 0.1819609408218836
LOSS train 0.12937682533983963 valid 0.18218673172653937
LOSS train 0.12937682533983963 valid 0.1822524064719075
LOSS train 0.12937682533983963 valid 0.1823451779782772
LOSS train 0.12937682533983963 valid 0.1825287001262797
LOSS train 0.12937682533983963 valid 0.18274671511322843
LOSS train 0.12937682533983963 valid 0.1824197990512385
LOSS train 0.12937682533983963 valid 0.1825912301070415
LOSS train 0.12937682533983963 valid 0.18267549588566734
LOSS train 0.12937682533983963 valid 0.1831260343205254
LOSS train 0.12937682533983963 valid 0.18303130386031677
LOSS train 0.12937682533983963 valid 0.18319050581366927
LOSS train 0.12937682533983963 valid 0.18358362568627803
LOSS train 0.12937682533983963 valid 0.1836357994513078
LOSS train 0.12937682533983963 valid 0.18358771731187631
LOSS train 0.12937682533983963 valid 0.18340864884001867
LOSS train 0.12937682533983963 valid 0.18345896965634506
LOSS train 0.12937682533983963 valid 0.18375428818296968
LOSS train 0.12937682533983963 valid 0.18397984673147616
LOSS train 0.12937682533983963 valid 0.18419444933533669
LOSS train 0.12937682533983963 valid 0.18411556357501918
LOSS train 0.12937682533983963 valid 0.183709456506422
LOSS train 0.12937682533983963 valid 0.18336547610639525
LOSS train 0.12937682533983963 valid 0.18301711479822794
LOSS train 0.12937682533983963 valid 0.1828533067437243
LOSS train 0.12937682533983963 valid 0.182901993393898
LOSS train 0.12937682533983963 valid 0.18268194552359543
LOSS train 0.12937682533983963 valid 0.18301277211116207
LOSS train 0.12937682533983963 valid 0.18282917273044585
LOSS train 0.12937682533983963 valid 0.18319525948119542
LOSS train 0.12937682533983963 valid 0.1830562077404007
LOSS train 0.12937682533983963 valid 0.183121187845245
LOSS train 0.12937682533983963 valid 0.1832700292038363
LOSS train 0.12937682533983963 valid 0.1828780160500453
LOSS train 0.12937682533983963 valid 0.18256586460211804
LOSS train 0.12937682533983963 valid 0.1822396662424911
LOSS train 0.12937682533983963 valid 0.18213024074421788
LOSS train 0.12937682533983963 valid 0.18226316026342448
LOSS train 0.12937682533983963 valid 0.18213969767093657
LOSS train 0.12937682533983963 valid 0.18214101990794435
LOSS train 0.12937682533983963 valid 0.18181271002675495
LOSS train 0.12937682533983963 valid 0.18173972549645798
LOSS train 0.12937682533983963 valid 0.18166442912259548
LOSS train 0.12937682533983963 valid 0.18176140253032957
LOSS train 0.12937682533983963 valid 0.18161339117280134
LOSS train 0.12937682533983963 valid 0.18163583270260986
LOSS train 0.12937682533983963 valid 0.18151194128123196
LOSS train 0.12937682533983963 valid 0.18152184184226725
LOSS train 0.12937682533983963 valid 0.18140474115980082
LOSS train 0.12937682533983963 valid 0.18161206427094054
LOSS train 0.12937682533983963 valid 0.1815470188450651
LOSS train 0.12937682533983963 valid 0.18246657008657585
LOSS train 0.12937682533983963 valid 0.18261695528190408
LOSS train 0.12937682533983963 valid 0.18255825440088908
LOSS train 0.12937682533983963 valid 0.18281001947968212
LOSS train 0.12937682533983963 valid 0.18246476812974402
LOSS train 0.12937682533983963 valid 0.182401036320169
LOSS train 0.12937682533983963 valid 0.18225728178566153
LOSS train 0.12937682533983963 valid 0.18214061414041827
LOSS train 0.12937682533983963 valid 0.18214858810488993
LOSS train 0.12937682533983963 valid 0.182246319902171
LOSS train 0.12937682533983963 valid 0.18223388351594347
LOSS train 0.12937682533983963 valid 0.18234401200927278
LOSS train 0.12937682533983963 valid 0.18227260168641807
LOSS train 0.12937682533983963 valid 0.18212712792135913
LOSS train 0.12937682533983963 valid 0.18200315433887787
LOSS train 0.12937682533983963 valid 0.18171890244527827
LOSS train 0.12937682533983963 valid 0.18153168833473834
LOSS train 0.12937682533983963 valid 0.1813677744431929
LOSS train 0.12937682533983963 valid 0.1814491552222206
LOSS train 0.12937682533983963 valid 0.18180234007492752
LOSS train 0.12937682533983963 valid 0.18171199127322152
LOSS train 0.12937682533983963 valid 0.18188292899075345
LOSS train 0.12937682533983963 valid 0.18188242526615367
LOSS train 0.12937682533983963 valid 0.1818885198462079
LOSS train 0.12937682533983963 valid 0.18175720882623694
LOSS train 0.12937682533983963 valid 0.18173118082085096
LOSS train 0.12937682533983963 valid 0.1816902944240077
LOSS train 0.12937682533983963 valid 0.18137091636657715
LOSS train 0.12937682533983963 valid 0.18132559751922434
LOSS train 0.12937682533983963 valid 0.1814318628466062
LOSS train 0.12937682533983963 valid 0.18167411192749325
LOSS train 0.12937682533983963 valid 0.18157519724781954
LOSS train 0.12937682533983963 valid 0.18149355890022384
LOSS train 0.12937682533983963 valid 0.18163188723898724
LOSS train 0.12937682533983963 valid 0.18147672303430326
LOSS train 0.12937682533983963 valid 0.1815394926918009
LOSS train 0.12937682533983963 valid 0.18140264543826165
LOSS train 0.12937682533983963 valid 0.18127093331233876
LOSS train 0.12937682533983963 valid 0.181314050670593
LOSS train 0.12937682533983963 valid 0.18115523377841808
LOSS train 0.12937682533983963 valid 0.18114717590047957
LOSS train 0.12937682533983963 valid 0.18101224547656125
LOSS train 0.12937682533983963 valid 0.1810978517720574
LOSS train 0.12937682533983963 valid 0.1809629567786661
LOSS train 0.12937682533983963 valid 0.18102441991989812
LOSS train 0.12937682533983963 valid 0.18074804938210107
LOSS train 0.12937682533983963 valid 0.18067374226358748
LOSS train 0.12937682533983963 valid 0.18046505000346746
LOSS train 0.12937682533983963 valid 0.1804240595321266
LOSS train 0.12937682533983963 valid 0.1806424742725295
LOSS train 0.12937682533983963 valid 0.1805909408463372
LOSS train 0.12937682533983963 valid 0.18072019315245164
LOSS train 0.12937682533983963 valid 0.18054575234651565
LOSS train 0.12937682533983963 valid 0.18036590227440222
LOSS train 0.12937682533983963 valid 0.1803000319269624
LOSS train 0.12937682533983963 valid 0.18030588945438122
LOSS train 0.12937682533983963 valid 0.18050823163460283
LOSS train 0.12937682533983963 valid 0.18027224169998635
LOSS train 0.12937682533983963 valid 0.1802929328049271
LOSS train 0.12937682533983963 valid 0.18022159893731565
LOSS train 0.12937682533983963 valid 0.18006269426013416
LOSS train 0.12937682533983963 valid 0.18007009515636846
LOSS train 0.12937682533983963 valid 0.1799950320805822
LOSS train 0.12937682533983963 valid 0.17999483468408267
LOSS train 0.12937682533983963 valid 0.1799512785279526
LOSS train 0.12937682533983963 valid 0.17991327721747993
LOSS train 0.12937682533983963 valid 0.17993947731278767
LOSS train 0.12937682533983963 valid 0.17981703489325768
LOSS train 0.12937682533983963 valid 0.1796767152155991
LOSS train 0.12937682533983963 valid 0.17959835473018856
LOSS train 0.12937682533983963 valid 0.17963407079287624
LOSS train 0.12937682533983963 valid 0.1797369843068188
LOSS train 0.12937682533983963 valid 0.17970259562134744
LOSS train 0.12937682533983963 valid 0.17961816395300007
LOSS train 0.12937682533983963 valid 0.17965641886264355
LOSS train 0.12937682533983963 valid 0.17984957792566497
LOSS train 0.12937682533983963 valid 0.1799559310477759
LOSS train 0.12937682533983963 valid 0.18012672033574845
LOSS train 0.12937682533983963 valid 0.18038648848248795
LOSS train 0.12937682533983963 valid 0.18044151680847623
LOSS train 0.12937682533983963 valid 0.18050849208967729
LOSS train 0.12937682533983963 valid 0.1805073786770933
LOSS train 0.12937682533983963 valid 0.18054399367259896
LOSS train 0.12937682533983963 valid 0.1807081690985403
LOSS train 0.12937682533983963 valid 0.18072624697253623
LOSS train 0.12937682533983963 valid 0.18082737654063835
LOSS train 0.12937682533983963 valid 0.18089270744568262
LOSS train 0.12937682533983963 valid 0.18105307234094498
LOSS train 0.12937682533983963 valid 0.1809348977344521
LOSS train 0.12937682533983963 valid 0.1808611169017317
LOSS train 0.12937682533983963 valid 0.18088386477041646
LOSS train 0.12937682533983963 valid 0.18067432802840755
LOSS train 0.12937682533983963 valid 0.1806698938831687
LOSS train 0.12937682533983963 valid 0.18086814868004986
LOSS train 0.12937682533983963 valid 0.18070853039745458
LOSS train 0.12937682533983963 valid 0.1809300423404317
LOSS train 0.12937682533983963 valid 0.18111463391878566
LOSS train 0.12937682533983963 valid 0.18117837735584805
LOSS train 0.12937682533983963 valid 0.18102302959537117
LOSS train 0.12937682533983963 valid 0.1811790043405193
LOSS train 0.12937682533983963 valid 0.18111481006828048
LOSS train 0.12937682533983963 valid 0.18111891422166404
LOSS train 0.12937682533983963 valid 0.18113613986968993
LOSS train 0.12937682533983963 valid 0.1809661978506947
LOSS train 0.12937682533983963 valid 0.18122793402936724
LOSS train 0.12937682533983963 valid 0.18116382494745517
LOSS train 0.12937682533983963 valid 0.18105205934582733
LOSS train 0.12937682533983963 valid 0.18109513579630385
LOSS train 0.12937682533983963 valid 0.18115264328662306
LOSS train 0.12937682533983963 valid 0.18097179113427025
LOSS train 0.12937682533983963 valid 0.18123450621153958
LOSS train 0.12937682533983963 valid 0.1812425288569513
LOSS train 0.12937682533983963 valid 0.1810843913600995
LOSS train 0.12937682533983963 valid 0.18124519830919317
LOSS train 0.12937682533983963 valid 0.18131775255421645
LOSS train 0.12937682533983963 valid 0.1813613414311137
LOSS train 0.12937682533983963 valid 0.18145549517463555
LOSS train 0.12937682533983963 valid 0.18141837395587057
LOSS train 0.12937682533983963 valid 0.1814371494198204
LOSS train 0.12937682533983963 valid 0.1814928608440728
LOSS train 0.12937682533983963 valid 0.18169383738023132
LOSS train 0.12937682533983963 valid 0.18177505081249434
LOSS train 0.12937682533983963 valid 0.18177466580161342
LOSS train 0.12937682533983963 valid 0.1818777541612787
LOSS train 0.12937682533983963 valid 0.18219829778022625
LOSS train 0.12937682533983963 valid 0.18240625004628638
LOSS train 0.12937682533983963 valid 0.18244936202999448
LOSS train 0.12937682533983963 valid 0.1824263992092826
LOSS train 0.12937682533983963 valid 0.18236666453489359
LOSS train 0.12937682533983963 valid 0.18225186977145474
LOSS train 0.12937682533983963 valid 0.1821024225984546
LOSS train 0.12937682533983963 valid 0.18208580744522873
LOSS train 0.12937682533983963 valid 0.18207708538642953
LOSS train 0.12937682533983963 valid 0.18200666762331627
LOSS train 0.12937682533983963 valid 0.18169710634553687
LOSS train 0.12937682533983963 valid 0.18162457747615268
LOSS train 0.12937682533983963 valid 0.18164799391278919
LOSS train 0.12937682533983963 valid 0.18168769167703494
LOSS train 0.12937682533983963 valid 0.1817029045334646
LOSS train 0.12937682533983963 valid 0.18166616535664434
LOSS train 0.12937682533983963 valid 0.18164445105422702
LOSS train 0.12937682533983963 valid 0.18165536351166794
LOSS train 0.12937682533983963 valid 0.18169135577205953
LOSS train 0.12937682533983963 valid 0.18152612486143702
LOSS train 0.12937682533983963 valid 0.1815148537646826
LOSS train 0.12937682533983963 valid 0.18152121052705386
LOSS train 0.12937682533983963 valid 0.18159775068463924
LOSS train 0.12937682533983963 valid 0.1816761239857997
LOSS train 0.12937682533983963 valid 0.18159450960622445
LOSS train 0.12937682533983963 valid 0.18168293820186096
LOSS train 0.12937682533983963 valid 0.18171980404513793
LOSS train 0.12937682533983963 valid 0.18174930253256125
LOSS train 0.12937682533983963 valid 0.1818474262704452
LOSS train 0.12937682533983963 valid 0.18179111665368478
LOSS train 0.12937682533983963 valid 0.18173655692413943
LOSS train 0.12937682533983963 valid 0.18178407343799133
LOSS train 0.12937682533983963 valid 0.18177346250434456
LOSS train 0.12937682533983963 valid 0.18171369728983425
LOSS train 0.12937682533983963 valid 0.1817468454683918
LOSS train 0.12937682533983963 valid 0.1817283459274699
LOSS train 0.12937682533983963 valid 0.18162887066892988
LOSS train 0.12937682533983963 valid 0.18162950046336382
LOSS train 0.12937682533983963 valid 0.18168098736674554
LOSS train 0.12937682533983963 valid 0.18167561759806905
LOSS train 0.12937682533983963 valid 0.18165826069143337
LOSS train 0.12937682533983963 valid 0.18184198946141586
LOSS train 0.12937682533983963 valid 0.18184939404107203
LOSS train 0.12937682533983963 valid 0.18176731280391178
LOSS train 0.12937682533983963 valid 0.18174182526955876
LOSS train 0.12937682533983963 valid 0.18181025791525465
LOSS train 0.12937682533983963 valid 0.18184847115252004
LOSS train 0.12937682533983963 valid 0.1820104217688118
LOSS train 0.12937682533983963 valid 0.18193680078256874
LOSS train 0.12937682533983963 valid 0.18211063884221876
LOSS train 0.12937682533983963 valid 0.18210432023642967
LOSS train 0.12937682533983963 valid 0.18205742724816495
LOSS train 0.12937682533983963 valid 0.1821744327697857
LOSS train 0.12937682533983963 valid 0.18217340091100107
LOSS train 0.12937682533983963 valid 0.18233694453820862
LOSS train 0.12937682533983963 valid 0.18238418886512062
LOSS train 0.12937682533983963 valid 0.18233649489447112
LOSS train 0.12937682533983963 valid 0.18239052256280527
LOSS train 0.12937682533983963 valid 0.18238408450375904
LOSS train 0.12937682533983963 valid 0.1822679603973184
LOSS train 0.12937682533983963 valid 0.18214386820523853
LOSS train 0.12937682533983963 valid 0.18220199673651932
LOSS train 0.12937682533983963 valid 0.18227883738404263
LOSS train 0.12937682533983963 valid 0.18223701588698288
LOSS train 0.12937682533983963 valid 0.1823118032577137
LOSS train 0.12937682533983963 valid 0.18231026755330823
LOSS train 0.12937682533983963 valid 0.1822981677684911
LOSS train 0.12937682533983963 valid 0.1823227983352709
LOSS train 0.12937682533983963 valid 0.18225963635041434
LOSS train 0.12937682533983963 valid 0.18210874687978598
LOSS train 0.12937682533983963 valid 0.1821106813673736
LOSS train 0.12937682533983963 valid 0.18215006544273726
LOSS train 0.12937682533983963 valid 0.1823957764088761
LOSS train 0.12937682533983963 valid 0.1825056981565296
LOSS train 0.12937682533983963 valid 0.18259717620952282
LOSS train 0.12937682533983963 valid 0.1824548835584341
LOSS train 0.12937682533983963 valid 0.18239034834349976
LOSS train 0.12937682533983963 valid 0.1823781734081599
LOSS train 0.12937682533983963 valid 0.1823100130685738
LOSS train 0.12937682533983963 valid 0.18222873042366783
LOSS train 0.12937682533983963 valid 0.18222431925294752
LOSS train 0.12937682533983963 valid 0.18223366012569864
LOSS train 0.12937682533983963 valid 0.18225210338915135
LOSS train 0.12937682533983963 valid 0.1823064852978142
LOSS train 0.12937682533983963 valid 0.1822937841035342
LOSS train 0.12937682533983963 valid 0.18232660131210707
LOSS train 0.12937682533983963 valid 0.18221976818235894
LOSS train 0.12937682533983963 valid 0.1822255552437644
LOSS train 0.12937682533983963 valid 0.1821762644789285
LOSS train 0.12937682533983963 valid 0.18212492621588905
LOSS train 0.12937682533983963 valid 0.18217749747535142
LOSS train 0.12937682533983963 valid 0.18217943023797895
LOSS train 0.12937682533983963 valid 0.1821667137913979
LOSS train 0.12937682533983963 valid 0.18221919889727684
LOSS train 0.12937682533983963 valid 0.182251285558027
LOSS train 0.12937682533983963 valid 0.1821618650808971
LOSS train 0.12937682533983963 valid 0.1822060887782794
LOSS train 0.12937682533983963 valid 0.1822863303750834
EPOCH 20:
  batch 1 loss: 0.11564398556947708
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.11534366384148598
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.11114915460348129
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.12078144960105419
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.12393151670694351
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.1258138231933117
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.12031523670469012
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.12287476472556591
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.12181337177753448
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.12113702222704888
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.11951294202696193
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.11872344960769017
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.12002019584178925
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.11981887370347977
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.11860011667013168
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.12045861454680562
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.11963858893688988
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.122652694169018
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.12223164776438161
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.12165901176631451
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.12219010541836421
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.12281917069445956
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.12301305357528769
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.12466909208645423
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.12382268100976944
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.12417262649306884
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.1245371961483249
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.12410890284393515
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.12355999699954329
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.1243268072605133
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.12483110735493322
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.1251974869519472
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.12442788662332477
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.12532125939341152
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.12668647425515311
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.1260023373696539
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.12581210382081368
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.12555880824986257
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.12597232923293725
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.12596406284719705
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.12607546677676643
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.12621062869826952
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.1271540305988733
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.12659698707813566
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.12684855444563758
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.12632327591595444
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.1262748821618709
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.12638993406047425
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.12630168363756064
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.1264713054895401
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.12608573205915152
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.12621540819796231
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.12601599392463575
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.12587105662182527
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.12628145448186182
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.1264193266896265
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.1265325788081738
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.12604824492129788
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.12633504465980044
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.1261312370498975
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.12630299661980302
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.12627194954022283
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.12628494427790718
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.12603803316596895
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.12614817332762937
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.1258870174713207
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.12568738178085925
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.12595677321009777
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.1262141801956771
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.12652108041303498
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.12684505915557834
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.1271407932250036
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.12726349975556545
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.1272820289674643
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.12727918177843095
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.12713715737979664
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.12713477721849045
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.1268196563499096
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.12730043041932432
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.12754931645467876
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.127653398723514
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.12728953225220122
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.12710419611399432
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.1276007801116932
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.1277693063897245
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.1280359938567461
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.1283706739202313
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.12881278204308313
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.12874755748872008
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.12912609345383114
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.1293233783690484
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.12934107819329138
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.1293932112955278
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.12928001756997817
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.12978561661745372
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.12984453809137145
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.1295715005895526
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.12942477749014386
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.12974741927000008
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.12960850551724434
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.12941693477701433
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.12925232209119142
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.1290258138069829
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.12911212695046112
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.12869699987627212
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.12869591483811163
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.12896260083000236
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.12870747533937296
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.12885957429988668
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.12904451807791537
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.12879754233736176
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.12877685144277556
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.1284989796381081
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.12885873516400656
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.12865916043519973
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.1285070967699947
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.1288439607900432
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.1288549010405096
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.12871897978191615
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.1286823509261012
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.12866717533878058
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.12873523749533247
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.12889106335436426
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.1288818143187992
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.12889462846517563
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.12897951374687847
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.1289783175888024
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.12870852858759463
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.1286293257230012
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.128651960595296
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.12851509030993657
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.12850068667621323
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.12881569687585184
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.12888476708486898
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.12864276546019096
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.1285427361726761
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.12867481734630834
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.1286394448168036
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.1288646075794165
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.12888550832867623
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.12888401989818465
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.12874388086124205
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.12881409340388292
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.12893569955809248
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.12883927678239757
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.12893092489405855
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.12897638794110747
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.12890771293156855
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.1293022378579082
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.12938852737347284
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.12925830794288623
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.12937626041668027
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.1293683796634082
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.12906765478191437
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.12910008483356045
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.12908780579574597
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.1291785949639454
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.12913163576888131
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.12921058574000244
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.12933444730006158
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.1294627052193843
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.12938942967189682
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.12941332047153836
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.12959555159436492
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.12972804745941452
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.12960156713085003
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.129456461412821
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.1293777460232377
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.12918470575082938
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.12945697390857866
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.1295225147528258
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.12951377333076888
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.12930257131770856
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.12922751393982734
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.1293301313689777
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.12915710114281287
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.1289450356599975
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.12867580496528175
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.12886258085679742
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.1286726555062665
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.12876050009582582
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.1287449300289154
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.12864165614700057
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.1285885628717749
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.12862147902314727
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.12858647336402246
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.12867567138397756
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.12857444231656004
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.12853216032975565
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.12855779948203186
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.1285956854205481
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.12852942803874612
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.12856698290980542
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.12833572459435955
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.12836498973461297
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.12850877441161748
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.12861879990639422
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.12877572745536314
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.1287137089512456
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.12889701403677464
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.12893879213439885
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.12899973383634397
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.12899985175414624
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.12904072684400222
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.1289665962137827
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.1288406459741222
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.12879372269347095
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.12863357569305942
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.12871875428554544
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.1288066474099954
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.12879605142849881
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.12882763579628378
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.12891801075896187
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.12870492344008427
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.12878848428643028
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.12870612595644262
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.12869837207178916
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.1287989459179957
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.12886160210659514
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.1288578398525715
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.12884362224000612
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.12885972374194377
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.12879907784173306
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.12885849310883454
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.12875080506006878
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.12874932831103822
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.1288600591573421
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.12868948045529818
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.1286396104761086
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.12887355714388515
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.1291258233514699
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.12903583605356259
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.12908987198904348
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.12908777992567447
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.12908198925408912
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.1290957993054289
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.12904077746319872
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.1290756771106179
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.12906752835382476
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.12896272276217738
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.12907504296525385
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.1290231549973823
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.12906926756901016
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.12909567291985768
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.1290599431006276
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.12912478785568138
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.12912352941176186
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.12919781269926217
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.1292297469983139
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.12919558665156364
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.12938364007439745
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.12941083084377977
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.12942100086584393
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.12939788497925744
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.1293635848398302
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.12936403867206536
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.12929750384639674
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.1292396236008914
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.12930282320410127
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.12934006168865242
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.12923539147294802
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.1291652200956836
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.12912777392016617
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.1289962406806422
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.1290194460524703
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.1290421335488782
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.12900139796041818
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.12899159631733573
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.12889734639000272
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.1289189167872623
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.12891498188800918
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.128874442567501
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.12899820498385273
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.12886583878502358
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.12873776053840463
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.12895325897936372
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.12892038856602747
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.12894092569891497
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.12888919628862838
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.12890674588935716
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.12886174277690807
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.12879362617824094
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.1287761659693802
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.12874150656583444
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.1288772805479535
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.12896620448340068
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.12888366952278887
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.12884558475990263
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.12878896698200992
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.12884010323162737
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.12886353108481444
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.1289694558267724
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.12915574360015855
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.12919491169606748
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.12916108384980993
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.1291854004501491
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.12917487579162676
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.12912258018283235
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.12908605842486673
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.12906561255455018
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.12908255212330738
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.1290587480395835
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.1290454766636241
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.1290181340453656
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.1289766607714481
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.1289802356288324
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.1290038885239281
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.12896070472122012
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.12890495989218498
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.1288145336172273
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.12887496553912423
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.12884877450191057
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.12878347226320364
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.12875714978784514
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.12875416846502394
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.12886907848753507
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.12885213142689847
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.1288395842197556
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.12876077519502013
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.12885416224598883
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.12881146271559307
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.1287539954650106
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.1287713410915236
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.12868059999137013
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.12869664951012685
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.12859334478988968
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.12848868291676957
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.12848196301336695
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.1285243810672528
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.1286150052691951
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.12864997433751732
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.12864356865006757
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.12857626982637355
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.1286126869078168
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.12868917446527908
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.128720766864717
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.12868979290260404
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.12869025334804016
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.12861650604150288
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.12860359663472457
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.12860022087600573
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.12886015543637916
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.1289043347446286
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.12894776255585427
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.12895486350508703
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.12893527324144552
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.12891943057091848
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.12891833505582537
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.1288607210604031
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.12877105093428068
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.1287700925499965
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.12884765124710446
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.12892174524474076
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.12896601394829105
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.12895891836411516
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.12887297581169713
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.12895904011836573
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.12903066972197766
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.12894419265241677
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.1288966476296385
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.12886998426798638
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.12894650274959718
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.12892024167723565
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.12886850932960983
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.12889573770026638
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.12889025091636377
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.12895535903015967
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.1290477696887177
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.12902369602585873
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.1289401976844749
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.12899231772136815
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.12899029857769448
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.1289968285459935
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.12908288611368063
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.12912374546130498
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.12912506572505894
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.12911246626067224
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.12917249578805196
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.1291158228208333
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.12910332273887962
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.129089144818739
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.1290111301066988
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.12908994793580655
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.12909044418483973
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.12900577361707563
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.1289525081456634
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.12896436830207667
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.12891053001290745
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.12889299627901044
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.1288353800964661
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.12881623279027013
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.12883927173228288
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.1289474792949116
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.12896930953951052
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.1289878863888451
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.12892236139136132
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.1288150116350849
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.1288498315267527
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.12882761899988754
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.12880943726748229
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.1288076264602585
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.12878837045374794
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.1287296960134956
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.12876792716802937
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.1288145038816664
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.12877014345429802
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.12877360356088352
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.1288235215652807
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.12877032107930894
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.1287689583875784
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.12874936165600798
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.12880647283734628
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.12882020669035416
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.12886464509411136
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.1289313712752009
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.1289934916421771
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.12898567072350345
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.1289518901534628
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.12899944897483243
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.12896241950137274
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.12902473015909807
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.12903996659398645
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.1289514228525455
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.12894893595773094
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.12893718293484518
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.12884953994076576
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.12886809822896045
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.12882908311820476
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.12874897286330625
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.12866730965500653
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.1287062892608189
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.12866259871602612
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.1286773371400514
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.1287324508042654
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.12871088303368666
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.12881222080200092
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.1288665384054184
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.12882246519332607
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.12870521066129073
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.12867710260166362
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.12869331455217198
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.1286547482687963
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.12862628795896103
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.128604113468313
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.12860525044497478
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.1285994745323209
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.12857119089004979
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.1286306692886033
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.12862271047249138
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.12860741308993764
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.12857831634597608
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.12862238162650472
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.12857122735735071
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.1285669220373494
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.12851534335167852
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.12852310487314275
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.128536943109865
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.12855247531785716
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.1285580088565033
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.12864115889305655
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.12863164698935384
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.12861857161699952
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.12855488889031996
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.12858565032867522
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.12858981458089686
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.12861115226418163
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.1286300026374915
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.12871989104737583
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.12876266380871282
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.12881610095500945
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.12885541606480908
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.12896357918694867
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.12896357918694867 valid 0.21725475788116455
LOSS train 0.12896357918694867 valid 0.19048995524644852
LOSS train 0.12896357918694867 valid 0.1825679987668991
LOSS train 0.12896357918694867 valid 0.1732223555445671
LOSS train 0.12896357918694867 valid 0.1689082384109497
LOSS train 0.12896357918694867 valid 0.1791216159860293
LOSS train 0.12896357918694867 valid 0.18966450222900935
LOSS train 0.12896357918694867 valid 0.18768171034753323
LOSS train 0.12896357918694867 valid 0.18709570335017311
LOSS train 0.12896357918694867 valid 0.1870444104075432
LOSS train 0.12896357918694867 valid 0.18570671840147537
LOSS train 0.12896357918694867 valid 0.18651600182056427
LOSS train 0.12896357918694867 valid 0.18597032588261825
LOSS train 0.12896357918694867 valid 0.1854680318917547
LOSS train 0.12896357918694867 valid 0.18308805028597513
LOSS train 0.12896357918694867 valid 0.18301051296293736
LOSS train 0.12896357918694867 valid 0.18365015703089096
LOSS train 0.12896357918694867 valid 0.18255729393826592
LOSS train 0.12896357918694867 valid 0.18491753229969426
LOSS train 0.12896357918694867 valid 0.18474733009934424
LOSS train 0.12896357918694867 valid 0.18374524088132949
LOSS train 0.12896357918694867 valid 0.18222428519617428
LOSS train 0.12896357918694867 valid 0.18198355967583862
LOSS train 0.12896357918694867 valid 0.18220078758895397
LOSS train 0.12896357918694867 valid 0.180882568359375
LOSS train 0.12896357918694867 valid 0.18075230488410363
LOSS train 0.12896357918694867 valid 0.1811242903824206
LOSS train 0.12896357918694867 valid 0.1807898607637201
LOSS train 0.12896357918694867 valid 0.18026504383004946
LOSS train 0.12896357918694867 valid 0.18077857742706935
LOSS train 0.12896357918694867 valid 0.1816239183948886
LOSS train 0.12896357918694867 valid 0.18078457983210683
LOSS train 0.12896357918694867 valid 0.1810719542431109
LOSS train 0.12896357918694867 valid 0.18049901241765304
LOSS train 0.12896357918694867 valid 0.1821893538747515
LOSS train 0.12896357918694867 valid 0.1819620504975319
LOSS train 0.12896357918694867 valid 0.18271823187132138
LOSS train 0.12896357918694867 valid 0.18311457022240288
LOSS train 0.12896357918694867 valid 0.18279356872424102
LOSS train 0.12896357918694867 valid 0.18246398195624353
LOSS train 0.12896357918694867 valid 0.183026250179221
LOSS train 0.12896357918694867 valid 0.1832135979618345
LOSS train 0.12896357918694867 valid 0.18321205814217412
LOSS train 0.12896357918694867 valid 0.18370254168456251
LOSS train 0.12896357918694867 valid 0.1837766640716129
LOSS train 0.12896357918694867 valid 0.1841692869430003
LOSS train 0.12896357918694867 valid 0.18480009886812657
LOSS train 0.12896357918694867 valid 0.18465894367545843
LOSS train 0.12896357918694867 valid 0.18525413530213491
LOSS train 0.12896357918694867 valid 0.18475210309028625
LOSS train 0.12896357918694867 valid 0.18494253181943707
LOSS train 0.12896357918694867 valid 0.18438486468333465
LOSS train 0.12896357918694867 valid 0.1849222666812393
LOSS train 0.12896357918694867 valid 0.18504945519897673
LOSS train 0.12896357918694867 valid 0.18495363918217747
LOSS train 0.12896357918694867 valid 0.1842466156397547
LOSS train 0.12896357918694867 valid 0.18418788334779573
LOSS train 0.12896357918694867 valid 0.1840442231503026
LOSS train 0.12896357918694867 valid 0.1844037814160525
LOSS train 0.12896357918694867 valid 0.18421275317668914
LOSS train 0.12896357918694867 valid 0.1837843876393115
LOSS train 0.12896357918694867 valid 0.18432249801774178
LOSS train 0.12896357918694867 valid 0.1836642130972847
LOSS train 0.12896357918694867 valid 0.18456248007714748
LOSS train 0.12896357918694867 valid 0.18478250755713535
LOSS train 0.12896357918694867 valid 0.18448615593440604
LOSS train 0.12896357918694867 valid 0.18389416586107282
LOSS train 0.12896357918694867 valid 0.18389707233975916
LOSS train 0.12896357918694867 valid 0.1835271126550177
LOSS train 0.12896357918694867 valid 0.18409723745925086
LOSS train 0.12896357918694867 valid 0.1837289081492894
LOSS train 0.12896357918694867 valid 0.1839090900288688
LOSS train 0.12896357918694867 valid 0.1839063143076962
LOSS train 0.12896357918694867 valid 0.18390013921905207
LOSS train 0.12896357918694867 valid 0.1842591998974482
LOSS train 0.12896357918694867 valid 0.18468951629964928
LOSS train 0.12896357918694867 valid 0.1845113333169516
LOSS train 0.12896357918694867 valid 0.18429340918858847
LOSS train 0.12896357918694867 valid 0.18395810489413106
LOSS train 0.12896357918694867 valid 0.18329618517309426
LOSS train 0.12896357918694867 valid 0.18294443429252247
LOSS train 0.12896357918694867 valid 0.18341808675265894
LOSS train 0.12896357918694867 valid 0.18322432651577225
LOSS train 0.12896357918694867 valid 0.18323079283748353
LOSS train 0.12896357918694867 valid 0.18298811439205617
LOSS train 0.12896357918694867 valid 0.18276916011128314
LOSS train 0.12896357918694867 valid 0.18254319278673195
LOSS train 0.12896357918694867 valid 0.1822530942207033
LOSS train 0.12896357918694867 valid 0.18276177915964234
LOSS train 0.12896357918694867 valid 0.18273037903838688
LOSS train 0.12896357918694867 valid 0.18281643223631514
LOSS train 0.12896357918694867 valid 0.18278636378438576
LOSS train 0.12896357918694867 valid 0.18256789590081862
LOSS train 0.12896357918694867 valid 0.1827508570983055
LOSS train 0.12896357918694867 valid 0.1826882674505836
LOSS train 0.12896357918694867 valid 0.18275015677014986
LOSS train 0.12896357918694867 valid 0.1826022975530821
LOSS train 0.12896357918694867 valid 0.18283091652758268
LOSS train 0.12896357918694867 valid 0.18289101439895053
LOSS train 0.12896357918694867 valid 0.18298783138394356
LOSS train 0.12896357918694867 valid 0.18317786124673222
LOSS train 0.12896357918694867 valid 0.18340164160027222
LOSS train 0.12896357918694867 valid 0.18306899924301406
LOSS train 0.12896357918694867 valid 0.18323629363798177
LOSS train 0.12896357918694867 valid 0.1833241166103454
LOSS train 0.12896357918694867 valid 0.18377745966866332
LOSS train 0.12896357918694867 valid 0.1836817864781228
LOSS train 0.12896357918694867 valid 0.1838427006646439
LOSS train 0.12896357918694867 valid 0.1842393188848408
LOSS train 0.12896357918694867 valid 0.18429155349731446
LOSS train 0.12896357918694867 valid 0.18423936630154517
LOSS train 0.12896357918694867 valid 0.1840559955952423
LOSS train 0.12896357918694867 valid 0.18410918551736175
LOSS train 0.12896357918694867 valid 0.18440687002842887
LOSS train 0.12896357918694867 valid 0.1846309475276781
LOSS train 0.12896357918694867 valid 0.1848506439348747
LOSS train 0.12896357918694867 valid 0.18477006727813655
LOSS train 0.12896357918694867 valid 0.18436087604801535
LOSS train 0.12896357918694867 valid 0.1840134112774825
LOSS train 0.12896357918694867 valid 0.18366308187445005
LOSS train 0.12896357918694867 valid 0.18349453437426858
LOSS train 0.12896357918694867 valid 0.18354067988083012
LOSS train 0.12896357918694867 valid 0.18332009150729917
LOSS train 0.12896357918694867 valid 0.18365210090433398
LOSS train 0.12896357918694867 valid 0.18347511029243468
LOSS train 0.12896357918694867 valid 0.18383924899593232
LOSS train 0.12896357918694867 valid 0.18367979341135252
LOSS train 0.12896357918694867 valid 0.18373935506679118
LOSS train 0.12896357918694867 valid 0.18388970485029293
LOSS train 0.12896357918694867 valid 0.18349458552323855
LOSS train 0.12896357918694867 valid 0.18318334790586516
LOSS train 0.12896357918694867 valid 0.18285974171577077
LOSS train 0.12896357918694867 valid 0.18274281179546414
LOSS train 0.12896357918694867 valid 0.18287449040964468
LOSS train 0.12896357918694867 valid 0.18275068744465156
LOSS train 0.12896357918694867 valid 0.18275070091819062
LOSS train 0.12896357918694867 valid 0.18242070470413152
LOSS train 0.12896357918694867 valid 0.18234557431677115
LOSS train 0.12896357918694867 valid 0.18227576351851868
LOSS train 0.12896357918694867 valid 0.18236999000821794
LOSS train 0.12896357918694867 valid 0.18222267027442338
LOSS train 0.12896357918694867 valid 0.18224902576963667
LOSS train 0.12896357918694867 valid 0.18212480645079712
LOSS train 0.12896357918694867 valid 0.18213216422332656
LOSS train 0.12896357918694867 valid 0.18201293493139334
LOSS train 0.12896357918694867 valid 0.18222254851501282
LOSS train 0.12896357918694867 valid 0.18215646701199667
LOSS train 0.12896357918694867 valid 0.18308282170344042
LOSS train 0.12896357918694867 valid 0.1832350697493393
LOSS train 0.12896357918694867 valid 0.18317782322565715
LOSS train 0.12896357918694867 valid 0.18342783711603936
LOSS train 0.12896357918694867 valid 0.18308006540725105
LOSS train 0.12896357918694867 valid 0.18301489890790454
LOSS train 0.12896357918694867 valid 0.18286635465436168
LOSS train 0.12896357918694867 valid 0.18274984628923477
LOSS train 0.12896357918694867 valid 0.1827591941333734
LOSS train 0.12896357918694867 valid 0.1828574765070229
LOSS train 0.12896357918694867 valid 0.18284292622834822
LOSS train 0.12896357918694867 valid 0.1829516293492707
LOSS train 0.12896357918694867 valid 0.18287906227633358
LOSS train 0.12896357918694867 valid 0.18273264095649955
LOSS train 0.12896357918694867 valid 0.18261127284279577
LOSS train 0.12896357918694867 valid 0.1823256216897555
LOSS train 0.12896357918694867 valid 0.18213538789167638
LOSS train 0.12896357918694867 valid 0.18197198157960717
LOSS train 0.12896357918694867 valid 0.1820538984723838
LOSS train 0.12896357918694867 valid 0.18240732846859686
LOSS train 0.12896357918694867 valid 0.18231523675577982
LOSS train 0.12896357918694867 valid 0.18248668498188786
LOSS train 0.12896357918694867 valid 0.18248821689802058
LOSS train 0.12896357918694867 valid 0.18250045624741337
LOSS train 0.12896357918694867 valid 0.18236852160026862
LOSS train 0.12896357918694867 valid 0.18234194462009937
LOSS train 0.12896357918694867 valid 0.18229989227892338
LOSS train 0.12896357918694867 valid 0.18197722579751696
LOSS train 0.12896357918694867 valid 0.18193392210047354
LOSS train 0.12896357918694867 valid 0.18204293463189722
LOSS train 0.12896357918694867 valid 0.18228821757804142
LOSS train 0.12896357918694867 valid 0.18218959618214123
LOSS train 0.12896357918694867 valid 0.1821072897149457
LOSS train 0.12896357918694867 valid 0.1822441843497819
LOSS train 0.12896357918694867 valid 0.18209055360856946
LOSS train 0.12896357918694867 valid 0.1821522331628643
LOSS train 0.12896357918694867 valid 0.18201371258043725
LOSS train 0.12896357918694867 valid 0.18188088955105963
LOSS train 0.12896357918694867 valid 0.18192313291052337
LOSS train 0.12896357918694867 valid 0.1817704619252108
LOSS train 0.12896357918694867 valid 0.18176546447137568
LOSS train 0.12896357918694867 valid 0.18163027145244456
LOSS train 0.12896357918694867 valid 0.18171506578985014
LOSS train 0.12896357918694867 valid 0.1815798416031593
LOSS train 0.12896357918694867 valid 0.18164117401465774
LOSS train 0.12896357918694867 valid 0.18136408932777268
LOSS train 0.12896357918694867 valid 0.18129027166317419
LOSS train 0.12896357918694867 valid 0.18107939469508635
LOSS train 0.12896357918694867 valid 0.18103643993333896
LOSS train 0.12896357918694867 valid 0.18125642435199718
LOSS train 0.12896357918694867 valid 0.18120357928553013
LOSS train 0.12896357918694867 valid 0.1813359998129121
LOSS train 0.12896357918694867 valid 0.1811562332510948
LOSS train 0.12896357918694867 valid 0.18097328872823004
LOSS train 0.12896357918694867 valid 0.18090636379057817
LOSS train 0.12896357918694867 valid 0.18091193832493768
LOSS train 0.12896357918694867 valid 0.18111384250954085
LOSS train 0.12896357918694867 valid 0.18087815467904253
LOSS train 0.12896357918694867 valid 0.180899384965017
LOSS train 0.12896357918694867 valid 0.18082781971076836
LOSS train 0.12896357918694867 valid 0.18067000376490447
LOSS train 0.12896357918694867 valid 0.18067860396569996
LOSS train 0.12896357918694867 valid 0.18060286108936582
LOSS train 0.12896357918694867 valid 0.18059846617599234
LOSS train 0.12896357918694867 valid 0.18056167451278218
LOSS train 0.12896357918694867 valid 0.18052405944452599
LOSS train 0.12896357918694867 valid 0.18055007864381664
LOSS train 0.12896357918694867 valid 0.18042615402576534
LOSS train 0.12896357918694867 valid 0.18028681559695137
LOSS train 0.12896357918694867 valid 0.18021040035557637
LOSS train 0.12896357918694867 valid 0.18024779124817716
LOSS train 0.12896357918694867 valid 0.180350636170335
LOSS train 0.12896357918694867 valid 0.1802984545854005
LOSS train 0.12896357918694867 valid 0.1802126145605588
LOSS train 0.12896357918694867 valid 0.18024682965096053
LOSS train 0.12896357918694867 valid 0.1804432798126888
LOSS train 0.12896357918694867 valid 0.18054865912667342
LOSS train 0.12896357918694867 valid 0.1807220153013865
LOSS train 0.12896357918694867 valid 0.18098234312724223
LOSS train 0.12896357918694867 valid 0.18104017929621205
LOSS train 0.12896357918694867 valid 0.18110782358991473
LOSS train 0.12896357918694867 valid 0.18110971191824785
LOSS train 0.12896357918694867 valid 0.181146029026612
LOSS train 0.12896357918694867 valid 0.18131080927786888
LOSS train 0.12896357918694867 valid 0.18132830693803983
LOSS train 0.12896357918694867 valid 0.18143334895244484
LOSS train 0.12896357918694867 valid 0.18149867118933263
LOSS train 0.12896357918694867 valid 0.18165970419315583
LOSS train 0.12896357918694867 valid 0.18154112704224507
LOSS train 0.12896357918694867 valid 0.18146867226447738
LOSS train 0.12896357918694867 valid 0.18149084779394775
LOSS train 0.12896357918694867 valid 0.1812776267279142
LOSS train 0.12896357918694867 valid 0.18127031307667493
LOSS train 0.12896357918694867 valid 0.1814704822430472
LOSS train 0.12896357918694867 valid 0.1813096418237883
LOSS train 0.12896357918694867 valid 0.18152994610399867
LOSS train 0.12896357918694867 valid 0.18171593394191538
LOSS train 0.12896357918694867 valid 0.18178036547437007
LOSS train 0.12896357918694867 valid 0.18162631697771026
LOSS train 0.12896357918694867 valid 0.18177791307812277
LOSS train 0.12896357918694867 valid 0.18171343850272317
LOSS train 0.12896357918694867 valid 0.181715941991672
LOSS train 0.12896357918694867 valid 0.18173510605096818
LOSS train 0.12896357918694867 valid 0.181566103758565
LOSS train 0.12896357918694867 valid 0.18182709328238927
LOSS train 0.12896357918694867 valid 0.18176514764312698
LOSS train 0.12896357918694867 valid 0.18165040667366794
LOSS train 0.12896357918694867 valid 0.1816947401154275
LOSS train 0.12896357918694867 valid 0.1817533090361394
LOSS train 0.12896357918694867 valid 0.18157069906882276
LOSS train 0.12896357918694867 valid 0.18183378684659338
LOSS train 0.12896357918694867 valid 0.18184253153423546
LOSS train 0.12896357918694867 valid 0.1816824090595429
LOSS train 0.12896357918694867 valid 0.18184717519073193
LOSS train 0.12896357918694867 valid 0.1819245125387461
LOSS train 0.12896357918694867 valid 0.18197002768969808
LOSS train 0.12896357918694867 valid 0.18206489018418573
LOSS train 0.12896357918694867 valid 0.18202721330354799
LOSS train 0.12896357918694867 valid 0.18204792807424874
LOSS train 0.12896357918694867 valid 0.182101609499267
LOSS train 0.12896357918694867 valid 0.18230118157703484
LOSS train 0.12896357918694867 valid 0.1823827000023264
LOSS train 0.12896357918694867 valid 0.18238138617188843
LOSS train 0.12896357918694867 valid 0.18248475461208513
LOSS train 0.12896357918694867 valid 0.18280612836208412
LOSS train 0.12896357918694867 valid 0.18301438428325095
LOSS train 0.12896357918694867 valid 0.18305807712956937
LOSS train 0.12896357918694867 valid 0.18303598338907415
LOSS train 0.12896357918694867 valid 0.18297456501834636
LOSS train 0.12896357918694867 valid 0.18286022753706907
LOSS train 0.12896357918694867 valid 0.18271375222386216
LOSS train 0.12896357918694867 valid 0.1826970364671454
LOSS train 0.12896357918694867 valid 0.18268677278288772
LOSS train 0.12896357918694867 valid 0.1826154677681227
LOSS train 0.12896357918694867 valid 0.182305893576737
LOSS train 0.12896357918694867 valid 0.18223569465820866
LOSS train 0.12896357918694867 valid 0.1822590008378029
LOSS train 0.12896357918694867 valid 0.1822988461506994
LOSS train 0.12896357918694867 valid 0.18231505353550811
LOSS train 0.12896357918694867 valid 0.18227759476115063
LOSS train 0.12896357918694867 valid 0.1822551672036449
LOSS train 0.12896357918694867 valid 0.18226191886156076
LOSS train 0.12896357918694867 valid 0.18229917960947958
LOSS train 0.12896357918694867 valid 0.18213443703872642
LOSS train 0.12896357918694867 valid 0.18212286786061443
LOSS train 0.12896357918694867 valid 0.18212734335721964
LOSS train 0.12896357918694867 valid 0.18220357596874237
LOSS train 0.12896357918694867 valid 0.18228417468273034
LOSS train 0.12896357918694867 valid 0.18220138474292047
LOSS train 0.12896357918694867 valid 0.18229142127414344
LOSS train 0.12896357918694867 valid 0.18232841159673346
LOSS train 0.12896357918694867 valid 0.18235819689605548
LOSS train 0.12896357918694867 valid 0.18245650122563045
LOSS train 0.12896357918694867 valid 0.18239933598873226
LOSS train 0.12896357918694867 valid 0.18234535464585222
LOSS train 0.12896357918694867 valid 0.18239359311734882
LOSS train 0.12896357918694867 valid 0.18238328730589465
LOSS train 0.12896357918694867 valid 0.18232442721968792
LOSS train 0.12896357918694867 valid 0.18235740460017147
LOSS train 0.12896357918694867 valid 0.18233822485134735
LOSS train 0.12896357918694867 valid 0.18223997481264076
LOSS train 0.12896357918694867 valid 0.18223872800089394
LOSS train 0.12896357918694867 valid 0.18228917371842168
LOSS train 0.12896357918694867 valid 0.1822799460968404
LOSS train 0.12896357918694867 valid 0.18226330913603306
LOSS train 0.12896357918694867 valid 0.18244691212146807
LOSS train 0.12896357918694867 valid 0.18245496012412818
LOSS train 0.12896357918694867 valid 0.18237299578530447
LOSS train 0.12896357918694867 valid 0.1823496372922312
LOSS train 0.12896357918694867 valid 0.1824199173258682
LOSS train 0.12896357918694867 valid 0.18245821211885357
LOSS train 0.12896357918694867 valid 0.18262321158636327
LOSS train 0.12896357918694867 valid 0.18255016105249525
LOSS train 0.12896357918694867 valid 0.18272644026071483
LOSS train 0.12896357918694867 valid 0.18271977037931822
LOSS train 0.12896357918694867 valid 0.18267226053096192
LOSS train 0.12896357918694867 valid 0.18278612362013924
LOSS train 0.12896357918694867 valid 0.18278548167302058
LOSS train 0.12896357918694867 valid 0.18295010339263026
LOSS train 0.12896357918694867 valid 0.18299783611334064
LOSS train 0.12896357918694867 valid 0.18295202395174562
LOSS train 0.12896357918694867 valid 0.18300585625381818
LOSS train 0.12896357918694867 valid 0.18299910290674729
LOSS train 0.12896357918694867 valid 0.18288406864754023
LOSS train 0.12896357918694867 valid 0.18275525515158492
LOSS train 0.12896357918694867 valid 0.18281397939444305
LOSS train 0.12896357918694867 valid 0.18289214542169058
LOSS train 0.12896357918694867 valid 0.1828510688756829
LOSS train 0.12896357918694867 valid 0.1829271503679809
LOSS train 0.12896357918694867 valid 0.18292483156795672
LOSS train 0.12896357918694867 valid 0.18291156743405132
LOSS train 0.12896357918694867 valid 0.18293506518050281
LOSS train 0.12896357918694867 valid 0.18287103395251667
LOSS train 0.12896357918694867 valid 0.18272120992808746
LOSS train 0.12896357918694867 valid 0.1827247989805121
LOSS train 0.12896357918694867 valid 0.1827653301092348
LOSS train 0.12896357918694867 valid 0.18301315020856468
LOSS train 0.12896357918694867 valid 0.18312680941560994
LOSS train 0.12896357918694867 valid 0.1832176049406818
LOSS train 0.12896357918694867 valid 0.18307422200266155
LOSS train 0.12896357918694867 valid 0.1830098703931803
LOSS train 0.12896357918694867 valid 0.18299656307970555
LOSS train 0.12896357918694867 valid 0.1829271937268121
LOSS train 0.12896357918694867 valid 0.18284563809378535
LOSS train 0.12896357918694867 valid 0.1828413204797967
LOSS train 0.12896357918694867 valid 0.18285075944476356
LOSS train 0.12896357918694867 valid 0.1828698368564164
LOSS train 0.12896357918694867 valid 0.18292514589470876
LOSS train 0.12896357918694867 valid 0.18291464627961093
LOSS train 0.12896357918694867 valid 0.18294677651729904
LOSS train 0.12896357918694867 valid 0.18284014283611788
LOSS train 0.12896357918694867 valid 0.1828446842335725
LOSS train 0.12896357918694867 valid 0.18279639209310214
LOSS train 0.12896357918694867 valid 0.18274396620794017
LOSS train 0.12896357918694867 valid 0.18279682688456214
LOSS train 0.12896357918694867 valid 0.1827963465419354
LOSS train 0.12896357918694867 valid 0.18278467442308152
LOSS train 0.12896357918694867 valid 0.18283873041198678
LOSS train 0.12896357918694867 valid 0.18287154042818507
LOSS train 0.12896357918694867 valid 0.18278228883359998
LOSS train 0.12896357918694867 valid 0.18282644859636607
LOSS train 0.12896357918694867 valid 0.18290534498407265
EPOCH 21:
  batch 1 loss: 0.11626235395669937
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.11724186316132545
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.11406618853410085
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.12260538712143898
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.12564599514007568
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.12644010533889136
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.12107664772442409
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.12358760461211205
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.12218191888597277
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.12176642045378686
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.11978001757101579
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.11877439108987649
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.11930066289810035
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.11953005141445569
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.11812092711528142
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.12004372896626592
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.11900976694682065
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.12214965828590924
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.1217589499919038
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.12131756953895093
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.12164445327860969
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.12199611250649799
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.12207593282927638
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.12388656412561734
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.12315887987613677
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.1235447944356845
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.12411445103309772
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.12363641895353794
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.1230552080890228
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.12392799680431683
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.12449764892939598
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.12493361230008304
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.12404034422202544
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.12487239806967623
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.12625433781317302
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.12562035210430622
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.12539960182196386
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.1250844797805736
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.12557908052053207
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.1255601292476058
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.125640650529687
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.12586348769920214
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.12694808372924493
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.12650435976684093
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.12665004283189774
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.12620934260928113
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.126185657179102
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.1263252276306351
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.12627060909052285
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.12645087793469428
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.12601571237924053
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.12613429057483488
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.1259749597817097
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.1258476119902399
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.12619146921417929
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.12634128172482764
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.12645769929676726
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.125992801040411
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.1263924563082598
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.12616012406845886
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.12630657595200617
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.12618366989397234
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.12626245854392884
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.1260319771245122
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.12609373285220218
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.12583920641830473
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.12562733010124805
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.12588239143438198
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.1260760727783908
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.1263430834880897
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.12665168342875763
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.12686514471554095
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.1269438604581846
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.12699034820134575
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.1270343847076098
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.12694124416693262
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.12691301649267023
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.12660858885217935
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.12694679775947257
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.12723945071920753
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.12727953134863465
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.12686231305322995
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.12668352448437586
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.12711093051447755
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.12729329913854598
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.1275977063490901
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.12784320535673493
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.12831044222482227
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.1282604659876127
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.12865023298396006
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.1288555336522532
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.12879640544238297
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.1288867006378789
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.1287978654529186
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.12925518453121185
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.12930585707848272
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.1290792601624715
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.1288831855113409
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.12919274085398877
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.1290692887455225
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.1288745251327458
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.12870853847148372
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.12844533458786103
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.1285622130649594
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.1281620523759297
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.1281815048939777
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.1284326618519899
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.12816242370064612
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.1282625155301269
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.1284901454367421
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.1282159027618331
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.12821238255128264
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.12795078391786172
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.1283281136369496
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.12816218027602072
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.12803872167293368
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.12837854377989077
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.12835364125795284
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.12821454705060029
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.12821243995179732
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.12818073789196566
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.1282061281629273
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.12834047635154025
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.1282941687611803
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.12835353213548661
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.12841208947319832
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.12845510743030414
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.12817998783430085
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.1281043400251588
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.1280823104083538
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.12795276922806528
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.1279080601578409
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.12821960953393377
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.12830238778199723
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.12807848674279673
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.12801285608507254
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.12817181358589744
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.12812670493039532
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.12836610295360895
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.128378676623106
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.1283770241635911
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.12821792674736238
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.12830119710285348
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.1284675139726864
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.12836385749537368
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.12844594037287857
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.12847449513925177
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.12842623884412083
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.12879218916164947
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.1288659015794595
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.1287409138205825
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.12884255371203548
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.12881341498661664
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.12850140949541872
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.12852575697245136
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.1285201687938892
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.12858861148547215
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.12854504335341574
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.12864783326597334
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.12875010180287064
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.128872082331536
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.12875830653456993
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.1287874961100473
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.1290029671224879
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.1291556214292844
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.12906825151967716
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.1289333847022342
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.12882032906193108
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.1286424235303021
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.12891017733251348
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.12896720611909676
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.1289369402409986
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.1287355103416939
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.12865973443820558
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.12876459079129354
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.12858043119988657
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.12837526429507692
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.12808535687541694
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.12828079993997873
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.12811775588326985
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.12819905702580406
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.12814992232309594
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.12805143169692304
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.12798704417503398
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.128013525218577
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.12795078894624146
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.1280421546515934
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.1279421015147199
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.1279013385533025
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.1279564873168343
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.12798416060614962
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.12791035925814262
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.1279625040647897
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.1277170280359455
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.12773555830503122
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.12788397970856452
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.1280234723829376
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.12816895898243394
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.12811223001935373
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.12827079318463802
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.12829338644274432
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.1283484404482464
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.12836369842731307
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.1283978448048526
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.12833329341033609
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.12818424016671273
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.1281497062764306
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.12799649094589627
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.12809073258387416
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.12820071546094758
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.12821196482221098
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.12824340660195305
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.1283557746961643
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.1281376187210885
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.12820202243882556
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.1280996329323561
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.1280885799895234
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.12818971409573468
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.1282440042590986
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.12824502881955016
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.1282794154944463
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.12829260605278317
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.1282354148914996
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.1282890151653971
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.12819148924615648
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.12820769285997458
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.1283416906905069
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.12817900055986747
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.1281414485198962
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.12836600344466126
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.12860858121088573
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.128521544955157
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.12858314111892757
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.12859950669937664
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.12858294911207035
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.12860705795050678
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.12855155407627927
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.12859736752359807
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.12860741251183355
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.12848962613691886
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.12861239050681167
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.12855752717611219
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.1286166208516423
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.1286598765825639
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.12864359866599648
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.12870300803484955
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.12868467579607057
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.1287565131281172
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.1288069248738059
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.12877961891889572
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.12895483496892024
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.1289881230110214
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.12900777149341794
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.1289948513658028
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.12895574774227891
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.12896871601697057
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.12889356624175602
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.12883214563015818
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.1289212599856973
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.12896082427066105
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.12885158189297635
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.12877964896668914
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.12873594692338103
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.1286144975967931
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.12863865401947272
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.12867691253024832
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.12865698775660234
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.1286494178947673
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.128560338183185
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.12857238222603445
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.12859837251725673
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.128555356990546
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.12868193438747427
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.1285548157935595
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.1284297532655976
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.128645660949574
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.12862626086611179
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.12865403329404138
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.1286060959901861
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.12861646469682456
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.1285641498041747
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.128500621395965
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.12848202823644814
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.1284464629445697
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.12856502248006954
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.12868078237959554
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.1285981922459104
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.12857533944770694
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.12850685255234628
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.12853370840693343
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.12854982607860335
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.12865960906731755
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.12884014120179232
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.12887881176710939
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.12883995140002946
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.12885354484456615
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.12883498226151321
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.12877682468935148
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.12873372891575197
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.12870259235302609
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.12872749571982411
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.1287317911027283
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.128739727310615
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.12870433007514007
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.12866648281695414
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.12866606633939776
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.1287130473927877
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.1286759820899793
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.1286245079102254
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.1285427465073524
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.12860821973280892
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.1285725334802499
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.12851618363643988
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.1284894803242319
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.1284940475982333
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.12861266369117966
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.1285812351558863
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.12858169514146991
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.1284965822782636
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.12859345101751388
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.12855595057812808
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.12849864456224144
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.12852331082518256
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.1284390571354716
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.1284507060280213
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.1283552578758974
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.1282523820569756
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.12825898015190187
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.12829673061526656
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.12838270485852704
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.12841424384235975
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.12840080541749316
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.12833193667210616
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.12835645419127212
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.12844170921329243
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.12846934389589088
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.12846714569924494
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.12846996117184853
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.12838426621350568
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.12837400611709146
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.1283795409275989
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.12863629363607942
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.12868750612346494
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.12872558576596338
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.1287120312884234
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.12868138755849332
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.12867069572897397
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.12866050465267964
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.12858812231303626
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.12850529023579188
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.12849588704924297
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.12858518864959478
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.12866979347747717
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.12870613426643576
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.12870756593388571
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.1286205363072706
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.12869117206552116
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.1287538859181564
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.12866897889332518
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.12862200691468187
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.12859225429986654
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.1286620120234911
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.12861502548223028
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.12857741101102516
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.12862056175323383
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.12862055451492144
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.1287010272695843
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.12880471369008656
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.12877229548647476
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.12867761987286644
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.12873174234059942
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.12872520157246178
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.12872593736041327
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.1288019714349094
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.12883439183235168
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.12881207838654518
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.12880545815042854
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.12887045445423279
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.128811365912489
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.12880197451695016
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.1287900216075692
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.12872325550979344
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.12879727002690416
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.12881575392869613
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.128737583408108
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.12868364612750438
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.12871108181023783
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.12865610558962084
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.1286436347314815
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.12857101565370194
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.12855748877958265
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.12856963356690748
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.12869234815353656
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.12871637361757646
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.1287280220396911
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.128661016021112
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.1285595065475411
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.12859220671743604
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.1285765424258727
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.12854583710432052
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.12854432613772346
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.12851525896891433
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.12846316448955913
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.12849918081618772
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.1285297601679225
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.12848959477811023
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.12849672299784584
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.12855371675801044
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.1284999383071524
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.1285118766129017
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.12849413433141663
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.12855576034816144
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.12857045083516447
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.12862737662190402
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.12868355245116245
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.12875344974991793
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.12875801172974013
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.1287312928818915
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.12878067579858343
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.12875353275310425
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.12881181826217725
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.12884594154018927
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.1287641028915455
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.1287555475982855
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.12874342939432928
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.1286384331438463
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.1286545352229469
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.1286168620164428
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.12854136005565003
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.12846120783062867
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.12850915673437363
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.1284660127841764
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.12847369379314486
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.12853976768282702
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.12851519317462526
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.12860551825083724
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.1286684982222878
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.12862574805816016
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.12851536255287418
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.12848894281143491
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.12851012371429782
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.12846388971239195
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.12844756245613098
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.1284391025400108
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.1284388800852754
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.12844237700238356
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.12841221775184541
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.1284631064793627
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.12845706845113058
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.1284331258634726
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.12841161365255285
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.12845717878204532
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.1284034892456158
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.12840950713874486
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.12836152138290824
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.1283617055925884
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.1283612397358767
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.12837144237938927
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.12837936432143443
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.12846485855786696
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.12844590671408979
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.12843315080775844
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.12837425501892427
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.12840170676595178
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.1284270579135546
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.12845256042071165
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.12848062883480202
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.12857342110230371
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.12862492865845085
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.12868195058817558
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.12871737721239684
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.12881325349464254
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.12881325349464254 valid 0.21585340797901154
LOSS train 0.12881325349464254 valid 0.1883198618888855
LOSS train 0.12881325349464254 valid 0.18070566654205322
LOSS train 0.12881325349464254 valid 0.171395942568779
LOSS train 0.12881325349464254 valid 0.16704483032226564
LOSS train 0.12881325349464254 valid 0.1771478752295176
LOSS train 0.12881325349464254 valid 0.18762597015925817
LOSS train 0.12881325349464254 valid 0.1857871674001217
LOSS train 0.12881325349464254 valid 0.18516628609763253
LOSS train 0.12881325349464254 valid 0.18502287864685057
LOSS train 0.12881325349464254 valid 0.18377372080629523
LOSS train 0.12881325349464254 valid 0.18449512993295988
LOSS train 0.12881325349464254 valid 0.1838890268252446
LOSS train 0.12881325349464254 valid 0.18337807059288025
LOSS train 0.12881325349464254 valid 0.181002480785052
LOSS train 0.12881325349464254 valid 0.18100041337311268
LOSS train 0.12881325349464254 valid 0.18164041813682108
LOSS train 0.12881325349464254 valid 0.18055016050736108
LOSS train 0.12881325349464254 valid 0.18294623651002584
LOSS train 0.12881325349464254 valid 0.1827169544994831
LOSS train 0.12881325349464254 valid 0.18175449328763144
LOSS train 0.12881325349464254 valid 0.18030076948079196
LOSS train 0.12881325349464254 valid 0.18007675849873087
LOSS train 0.12881325349464254 valid 0.18031122287114462
LOSS train 0.12881325349464254 valid 0.17902147829532622
LOSS train 0.12881325349464254 valid 0.17893295677808616
LOSS train 0.12881325349464254 valid 0.1792339108608387
LOSS train 0.12881325349464254 valid 0.17888449664626802
LOSS train 0.12881325349464254 valid 0.1783501361978465
LOSS train 0.12881325349464254 valid 0.17887394825617473
LOSS train 0.12881325349464254 valid 0.1797143168026401
LOSS train 0.12881325349464254 valid 0.17890530358999968
LOSS train 0.12881325349464254 valid 0.17915717173706402
LOSS train 0.12881325349464254 valid 0.17860283921746647
LOSS train 0.12881325349464254 valid 0.1801908735718046
LOSS train 0.12881325349464254 valid 0.17995553008384174
LOSS train 0.12881325349464254 valid 0.18071634100901113
LOSS train 0.12881325349464254 valid 0.1810851873535859
LOSS train 0.12881325349464254 valid 0.1807863536553505
LOSS train 0.12881325349464254 valid 0.1804858662188053
LOSS train 0.12881325349464254 valid 0.18105832287451115
LOSS train 0.12881325349464254 valid 0.18124821108011974
LOSS train 0.12881325349464254 valid 0.18125498987907587
LOSS train 0.12881325349464254 valid 0.18176078593189066
LOSS train 0.12881325349464254 valid 0.18183769285678864
LOSS train 0.12881325349464254 valid 0.18223631835502127
LOSS train 0.12881325349464254 valid 0.18286686882059625
LOSS train 0.12881325349464254 valid 0.18269195097188154
LOSS train 0.12881325349464254 valid 0.18329680239667698
LOSS train 0.12881325349464254 valid 0.18279891818761826
LOSS train 0.12881325349464254 valid 0.18298724527452506
LOSS train 0.12881325349464254 valid 0.18243165084948906
LOSS train 0.12881325349464254 valid 0.1829662022163283
LOSS train 0.12881325349464254 valid 0.18308975961473253
LOSS train 0.12881325349464254 valid 0.18299785852432252
LOSS train 0.12881325349464254 valid 0.1822994276881218
LOSS train 0.12881325349464254 valid 0.18223901537426732
LOSS train 0.12881325349464254 valid 0.18210772168019723
LOSS train 0.12881325349464254 valid 0.18246090917264002
LOSS train 0.12881325349464254 valid 0.1822622259457906
LOSS train 0.12881325349464254 valid 0.1818401293187845
LOSS train 0.12881325349464254 valid 0.18237327207480708
LOSS train 0.12881325349464254 valid 0.18172943710334719
LOSS train 0.12881325349464254 valid 0.18262177961878479
LOSS train 0.12881325349464254 valid 0.18285639194341807
LOSS train 0.12881325349464254 valid 0.18255424634976822
LOSS train 0.12881325349464254 valid 0.1819517281073243
LOSS train 0.12881325349464254 valid 0.18193717585767016
LOSS train 0.12881325349464254 valid 0.1815793319888737
LOSS train 0.12881325349464254 valid 0.18216051352875573
LOSS train 0.12881325349464254 valid 0.18180028863356146
LOSS train 0.12881325349464254 valid 0.18198195099830627
LOSS train 0.12881325349464254 valid 0.18197792033626609
LOSS train 0.12881325349464254 valid 0.18196976668125875
LOSS train 0.12881325349464254 valid 0.18232568542162578
LOSS train 0.12881325349464254 valid 0.18274614724673724
LOSS train 0.12881325349464254 valid 0.18258348223450896
LOSS train 0.12881325349464254 valid 0.18236297980332986
LOSS train 0.12881325349464254 valid 0.18202913750576066
LOSS train 0.12881325349464254 valid 0.1813744580373168
LOSS train 0.12881325349464254 valid 0.18102910194867922
LOSS train 0.12881325349464254 valid 0.18147990798077931
LOSS train 0.12881325349464254 valid 0.18129103801336632
LOSS train 0.12881325349464254 valid 0.18130843571963765
LOSS train 0.12881325349464254 valid 0.1810568350202897
LOSS train 0.12881325349464254 valid 0.18085753016693648
LOSS train 0.12881325349464254 valid 0.18064081394809417
LOSS train 0.12881325349464254 valid 0.18034553798762235
LOSS train 0.12881325349464254 valid 0.180857496984889
LOSS train 0.12881325349464254 valid 0.1808363096581565
LOSS train 0.12881325349464254 valid 0.18093012076812787
LOSS train 0.12881325349464254 valid 0.18088160212273183
LOSS train 0.12881325349464254 valid 0.18066112921443037
LOSS train 0.12881325349464254 valid 0.18085309498487634
LOSS train 0.12881325349464254 valid 0.18079451181386647
LOSS train 0.12881325349464254 valid 0.1808563736267388
LOSS train 0.12881325349464254 valid 0.18070270290079804
LOSS train 0.12881325349464254 valid 0.1809252848734661
LOSS train 0.12881325349464254 valid 0.1809789395392543
LOSS train 0.12881325349464254 valid 0.1810732214152813
LOSS train 0.12881325349464254 valid 0.18126536905765533
LOSS train 0.12881325349464254 valid 0.18149037454642503
LOSS train 0.12881325349464254 valid 0.18115412378774107
LOSS train 0.12881325349464254 valid 0.18130220659077168
LOSS train 0.12881325349464254 valid 0.18139451657022748
LOSS train 0.12881325349464254 valid 0.1818503996111312
LOSS train 0.12881325349464254 valid 0.18174771170749843
LOSS train 0.12881325349464254 valid 0.18191108190351063
LOSS train 0.12881325349464254 valid 0.18230874658724583
LOSS train 0.12881325349464254 valid 0.18235979026014154
LOSS train 0.12881325349464254 valid 0.18231453954636515
LOSS train 0.12881325349464254 valid 0.182133170509977
LOSS train 0.12881325349464254 valid 0.18218574996015666
LOSS train 0.12881325349464254 valid 0.18249526319273732
LOSS train 0.12881325349464254 valid 0.18271468748217043
LOSS train 0.12881325349464254 valid 0.18293169038049106
LOSS train 0.12881325349464254 valid 0.18286661587209782
LOSS train 0.12881325349464254 valid 0.1824579101245282
LOSS train 0.12881325349464254 valid 0.1821079356830661
LOSS train 0.12881325349464254 valid 0.18176078585286934
LOSS train 0.12881325349464254 valid 0.18159441945474009
LOSS train 0.12881325349464254 valid 0.18164081163093693
LOSS train 0.12881325349464254 valid 0.18142508588186124
LOSS train 0.12881325349464254 valid 0.18175465180989234
LOSS train 0.12881325349464254 valid 0.18158367443084716
LOSS train 0.12881325349464254 valid 0.1819391418544073
LOSS train 0.12881325349464254 valid 0.1817842886203856
LOSS train 0.12881325349464254 valid 0.18183207942638546
LOSS train 0.12881325349464254 valid 0.1819723811029464
LOSS train 0.12881325349464254 valid 0.18158382578538015
LOSS train 0.12881325349464254 valid 0.18127642822174625
LOSS train 0.12881325349464254 valid 0.18096191876313902
LOSS train 0.12881325349464254 valid 0.1808482266234276
LOSS train 0.12881325349464254 valid 0.18096946113145174
LOSS train 0.12881325349464254 valid 0.1808446059624354
LOSS train 0.12881325349464254 valid 0.18084930562797716
LOSS train 0.12881325349464254 valid 0.18052591720636743
LOSS train 0.12881325349464254 valid 0.18045137660658878
LOSS train 0.12881325349464254 valid 0.18038477014294632
LOSS train 0.12881325349464254 valid 0.1804804489016533
LOSS train 0.12881325349464254 valid 0.1803385872156062
LOSS train 0.12881325349464254 valid 0.18035269990353517
LOSS train 0.12881325349464254 valid 0.18023393829385717
LOSS train 0.12881325349464254 valid 0.18023805071910223
LOSS train 0.12881325349464254 valid 0.18011844363705865
LOSS train 0.12881325349464254 valid 0.18032549199176162
LOSS train 0.12881325349464254 valid 0.18026269627671662
LOSS train 0.12881325349464254 valid 0.18117840356520704
LOSS train 0.12881325349464254 valid 0.18133539521454164
LOSS train 0.12881325349464254 valid 0.1812883828083674
LOSS train 0.12881325349464254 valid 0.18153204665278758
LOSS train 0.12881325349464254 valid 0.18119116441199654
LOSS train 0.12881325349464254 valid 0.1811276684009951
LOSS train 0.12881325349464254 valid 0.18098613029563582
LOSS train 0.12881325349464254 valid 0.18087189120631064
LOSS train 0.12881325349464254 valid 0.1808831281004808
LOSS train 0.12881325349464254 valid 0.1809764389589334
LOSS train 0.12881325349464254 valid 0.18096122579484047
LOSS train 0.12881325349464254 valid 0.1810625675339369
LOSS train 0.12881325349464254 valid 0.1809914568439126
LOSS train 0.12881325349464254 valid 0.18084432027354744
LOSS train 0.12881325349464254 valid 0.18072493503123155
LOSS train 0.12881325349464254 valid 0.18044490179766906
LOSS train 0.12881325349464254 valid 0.18025344846452154
LOSS train 0.12881325349464254 valid 0.18009281032013172
LOSS train 0.12881325349464254 valid 0.18017466034156732
LOSS train 0.12881325349464254 valid 0.18052255705802026
LOSS train 0.12881325349464254 valid 0.1804302761419898
LOSS train 0.12881325349464254 valid 0.18060286960305547
LOSS train 0.12881325349464254 valid 0.18060711815076716
LOSS train 0.12881325349464254 valid 0.18061765366130406
LOSS train 0.12881325349464254 valid 0.1804855614900589
LOSS train 0.12881325349464254 valid 0.1804647593828984
LOSS train 0.12881325349464254 valid 0.18042091228838625
LOSS train 0.12881325349464254 valid 0.18009846061468124
LOSS train 0.12881325349464254 valid 0.1800607074983418
LOSS train 0.12881325349464254 valid 0.1801674297384623
LOSS train 0.12881325349464254 valid 0.18041488415237222
LOSS train 0.12881325349464254 valid 0.18031937121179517
LOSS train 0.12881325349464254 valid 0.18023757425447304
LOSS train 0.12881325349464254 valid 0.18037677910281807
LOSS train 0.12881325349464254 valid 0.18022761710888738
LOSS train 0.12881325349464254 valid 0.18028693426339354
LOSS train 0.12881325349464254 valid 0.18015271388804135
LOSS train 0.12881325349464254 valid 0.1800168627419987
LOSS train 0.12881325349464254 valid 0.18006487883707528
LOSS train 0.12881325349464254 valid 0.17991479470289964
LOSS train 0.12881325349464254 valid 0.17991024759063062
LOSS train 0.12881325349464254 valid 0.17977502827764189
LOSS train 0.12881325349464254 valid 0.17986018065559237
LOSS train 0.12881325349464254 valid 0.17972771476232569
LOSS train 0.12881325349464254 valid 0.17978716800765446
LOSS train 0.12881325349464254 valid 0.17950729882933314
LOSS train 0.12881325349464254 valid 0.17943680858642785
LOSS train 0.12881325349464254 valid 0.17923035869995754
LOSS train 0.12881325349464254 valid 0.1791901026985475
LOSS train 0.12881325349464254 valid 0.17939245515517172
LOSS train 0.12881325349464254 valid 0.1793403144963462
LOSS train 0.12881325349464254 valid 0.17947431616297918
LOSS train 0.12881325349464254 valid 0.17929335411638023
LOSS train 0.12881325349464254 valid 0.1791107838797332
LOSS train 0.12881325349464254 valid 0.1790483657750163
LOSS train 0.12881325349464254 valid 0.17905077760295915
LOSS train 0.12881325349464254 valid 0.1792463250823465
LOSS train 0.12881325349464254 valid 0.17900897856892609
LOSS train 0.12881325349464254 valid 0.17902297653185512
LOSS train 0.12881325349464254 valid 0.17895210829477956
LOSS train 0.12881325349464254 valid 0.1787920048675285
LOSS train 0.12881325349464254 valid 0.17879950604512931
LOSS train 0.12881325349464254 valid 0.17871992655453228
LOSS train 0.12881325349464254 valid 0.17871515788314465
LOSS train 0.12881325349464254 valid 0.17867524271725482
LOSS train 0.12881325349464254 valid 0.17863512091653447
LOSS train 0.12881325349464254 valid 0.1786643012870695
LOSS train 0.12881325349464254 valid 0.17854297538829406
LOSS train 0.12881325349464254 valid 0.1784074255782697
LOSS train 0.12881325349464254 valid 0.1783337569387827
LOSS train 0.12881325349464254 valid 0.1783695537767826
LOSS train 0.12881325349464254 valid 0.17847374377593603
LOSS train 0.12881325349464254 valid 0.17841430899094451
LOSS train 0.12881325349464254 valid 0.17833035493193708
LOSS train 0.12881325349464254 valid 0.17836490766824903
LOSS train 0.12881325349464254 valid 0.17856081688751554
LOSS train 0.12881325349464254 valid 0.17865918140991457
LOSS train 0.12881325349464254 valid 0.17882981926202773
LOSS train 0.12881325349464254 valid 0.1790890718389929
LOSS train 0.12881325349464254 valid 0.17915195440011927
LOSS train 0.12881325349464254 valid 0.17921878662156432
LOSS train 0.12881325349464254 valid 0.17921905164244914
LOSS train 0.12881325349464254 valid 0.17925899200465367
LOSS train 0.12881325349464254 valid 0.17940643310417861
LOSS train 0.12881325349464254 valid 0.17942378146509672
LOSS train 0.12881325349464254 valid 0.17953364899613827
LOSS train 0.12881325349464254 valid 0.17959574377562246
LOSS train 0.12881325349464254 valid 0.17975639989401432
LOSS train 0.12881325349464254 valid 0.17964089500828315
LOSS train 0.12881325349464254 valid 0.17957388094080148
LOSS train 0.12881325349464254 valid 0.179595962323311
LOSS train 0.12881325349464254 valid 0.17938560531104458
LOSS train 0.12881325349464254 valid 0.17937778361762563
LOSS train 0.12881325349464254 valid 0.1795754142383817
LOSS train 0.12881325349464254 valid 0.17941705251405063
LOSS train 0.12881325349464254 valid 0.1796395989295877
LOSS train 0.12881325349464254 valid 0.1798250310916881
LOSS train 0.12881325349464254 valid 0.1798885675413268
LOSS train 0.12881325349464254 valid 0.17973648662853048
LOSS train 0.12881325349464254 valid 0.17988513020972008
LOSS train 0.12881325349464254 valid 0.17982153609515197
LOSS train 0.12881325349464254 valid 0.17982028792780566
LOSS train 0.12881325349464254 valid 0.17983841326832772
LOSS train 0.12881325349464254 valid 0.17967025428060515
LOSS train 0.12881325349464254 valid 0.17992605922359323
LOSS train 0.12881325349464254 valid 0.17986344917136218
LOSS train 0.12881325349464254 valid 0.17974639968491915
LOSS train 0.12881325349464254 valid 0.17979309146895128
LOSS train 0.12881325349464254 valid 0.17985779748414643
LOSS train 0.12881325349464254 valid 0.17967535343731425
LOSS train 0.12881325349464254 valid 0.1799313038183275
LOSS train 0.12881325349464254 valid 0.17994133774263057
LOSS train 0.12881325349464254 valid 0.1797822349633162
LOSS train 0.12881325349464254 valid 0.17994213792213534
LOSS train 0.12881325349464254 valid 0.1800195546038733
LOSS train 0.12881325349464254 valid 0.18006374624632157
LOSS train 0.12881325349464254 valid 0.180156709090101
LOSS train 0.12881325349464254 valid 0.18011866580202895
LOSS train 0.12881325349464254 valid 0.1801328543937744
LOSS train 0.12881325349464254 valid 0.18018448612328325
LOSS train 0.12881325349464254 valid 0.1803808989309108
LOSS train 0.12881325349464254 valid 0.18046450512449094
LOSS train 0.12881325349464254 valid 0.1804659725615272
LOSS train 0.12881325349464254 valid 0.1805697824957186
LOSS train 0.12881325349464254 valid 0.1808897520086783
LOSS train 0.12881325349464254 valid 0.1810992093477057
LOSS train 0.12881325349464254 valid 0.18114489433865477
LOSS train 0.12881325349464254 valid 0.1811215986717831
LOSS train 0.12881325349464254 valid 0.18105839260354423
LOSS train 0.12881325349464254 valid 0.18094809006375095
LOSS train 0.12881325349464254 valid 0.18080117947442068
LOSS train 0.12881325349464254 valid 0.18078689408024579
LOSS train 0.12881325349464254 valid 0.18077490832656623
LOSS train 0.12881325349464254 valid 0.18070542698334968
LOSS train 0.12881325349464254 valid 0.1804013802861491
LOSS train 0.12881325349464254 valid 0.1803338546634984
LOSS train 0.12881325349464254 valid 0.18035839188476682
LOSS train 0.12881325349464254 valid 0.18040141431908857
LOSS train 0.12881325349464254 valid 0.18041802036178695
LOSS train 0.12881325349464254 valid 0.18037933625202976
LOSS train 0.12881325349464254 valid 0.1803560810060137
LOSS train 0.12881325349464254 valid 0.180363188977885
LOSS train 0.12881325349464254 valid 0.18040018158740012
LOSS train 0.12881325349464254 valid 0.18023484750711632
LOSS train 0.12881325349464254 valid 0.18022436071952727
LOSS train 0.12881325349464254 valid 0.18022930922158342
LOSS train 0.12881325349464254 valid 0.1803051335673754
LOSS train 0.12881325349464254 valid 0.18038740683410126
LOSS train 0.12881325349464254 valid 0.18030445675390797
LOSS train 0.12881325349464254 valid 0.1803944384309178
LOSS train 0.12881325349464254 valid 0.18042975029089306
LOSS train 0.12881325349464254 valid 0.18046014220220188
LOSS train 0.12881325349464254 valid 0.18055983483791352
LOSS train 0.12881325349464254 valid 0.1804996093246232
LOSS train 0.12881325349464254 valid 0.1804492211598434
LOSS train 0.12881325349464254 valid 0.18049658943127483
LOSS train 0.12881325349464254 valid 0.1804881561057348
LOSS train 0.12881325349464254 valid 0.18042954050126622
LOSS train 0.12881325349464254 valid 0.18046311196548487
LOSS train 0.12881325349464254 valid 0.18044243223892362
LOSS train 0.12881325349464254 valid 0.1803418817361454
LOSS train 0.12881325349464254 valid 0.18033767677240772
LOSS train 0.12881325349464254 valid 0.1803858202791983
LOSS train 0.12881325349464254 valid 0.1803779651881031
LOSS train 0.12881325349464254 valid 0.18036274354045206
LOSS train 0.12881325349464254 valid 0.18054791694631972
LOSS train 0.12881325349464254 valid 0.1805578004687455
LOSS train 0.12881325349464254 valid 0.18047852804736486
LOSS train 0.12881325349464254 valid 0.18045518231354182
LOSS train 0.12881325349464254 valid 0.1805226150841367
LOSS train 0.12881325349464254 valid 0.18056119697273904
LOSS train 0.12881325349464254 valid 0.1807265072874141
LOSS train 0.12881325349464254 valid 0.18065599733963608
LOSS train 0.12881325349464254 valid 0.18083375413841177
LOSS train 0.12881325349464254 valid 0.18082534887405657
LOSS train 0.12881325349464254 valid 0.1807771322804708
LOSS train 0.12881325349464254 valid 0.1808858327107665
LOSS train 0.12881325349464254 valid 0.1808882935689046
LOSS train 0.12881325349464254 valid 0.18104798602728756
LOSS train 0.12881325349464254 valid 0.18109752720831365
LOSS train 0.12881325349464254 valid 0.18105201527658032
LOSS train 0.12881325349464254 valid 0.18110663554769882
LOSS train 0.12881325349464254 valid 0.18109826644261678
LOSS train 0.12881325349464254 valid 0.18098317703451636
LOSS train 0.12881325349464254 valid 0.18084989703563323
LOSS train 0.12881325349464254 valid 0.18090926419507275
LOSS train 0.12881325349464254 valid 0.18098696584473112
LOSS train 0.12881325349464254 valid 0.1809462457450468
LOSS train 0.12881325349464254 valid 0.1810199838309061
LOSS train 0.12881325349464254 valid 0.1810149621627451
LOSS train 0.12881325349464254 valid 0.18100266445141572
LOSS train 0.12881325349464254 valid 0.18102526915284384
LOSS train 0.12881325349464254 valid 0.1809645846924361
LOSS train 0.12881325349464254 valid 0.18081294314777396
LOSS train 0.12881325349464254 valid 0.18081855839281752
LOSS train 0.12881325349464254 valid 0.1808592569810656
LOSS train 0.12881325349464254 valid 0.18110178016819234
LOSS train 0.12881325349464254 valid 0.18121595996013587
LOSS train 0.12881325349464254 valid 0.18130651232651893
LOSS train 0.12881325349464254 valid 0.18116458593424872
LOSS train 0.12881325349464254 valid 0.18110105690771136
LOSS train 0.12881325349464254 valid 0.18108712563712823
LOSS train 0.12881325349464254 valid 0.18101859905890055
LOSS train 0.12881325349464254 valid 0.1809375203318066
LOSS train 0.12881325349464254 valid 0.18093232260170308
LOSS train 0.12881325349464254 valid 0.18093942844496233
LOSS train 0.12881325349464254 valid 0.18095762741431004
LOSS train 0.12881325349464254 valid 0.18101561581584769
LOSS train 0.12881325349464254 valid 0.1810095442479916
LOSS train 0.12881325349464254 valid 0.18104299389514603
LOSS train 0.12881325349464254 valid 0.18093728723639216
LOSS train 0.12881325349464254 valid 0.18094053739956825
LOSS train 0.12881325349464254 valid 0.1808966060479482
LOSS train 0.12881325349464254 valid 0.18084377165976653
LOSS train 0.12881325349464254 valid 0.18089414848971763
LOSS train 0.12881325349464254 valid 0.1808908378536051
LOSS train 0.12881325349464254 valid 0.18088036304810545
LOSS train 0.12881325349464254 valid 0.18093662123157553
LOSS train 0.12881325349464254 valid 0.18096101752232985
LOSS train 0.12881325349464254 valid 0.18087201705583109
LOSS train 0.12881325349464254 valid 0.180918429573269
LOSS train 0.12881325349464254 valid 0.1809967091170753
EPOCH 22:
  batch 1 loss: 0.11794818192720413
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.11739857494831085
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.1136034553249677
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.12177339009940624
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.12574794739484788
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.12726804489890733
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.12112338947398323
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.12325285281985998
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.12208302567402522
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.12148431316018105
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.1194502826441418
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.1187497799595197
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.11967310424034412
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.11947580160839218
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.1180208499232928
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.11945214355364442
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.11848328876144745
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.12148173857066366
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.12122671776696255
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.12086929306387902
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.12125404321012043
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.12174842438914558
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.12180553635825282
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.12355821828047435
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.12284357130527496
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.12334382132841991
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.12400449094948945
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.12373262643814087
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.12306360386568925
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.12382559478282928
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.12444640647980475
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.12492631422355771
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.12410790224870046
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.12493248327690012
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.126384693809918
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.12569033520089257
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.12561025248991475
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.12530101855334483
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.1257642780741056
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.12581747118383646
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.12594154422603002
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.1260926668487844
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.12703380768382272
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.1264749260788614
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.1267248358991411
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.12623547714041627
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.1261799858605608
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.12627974804490805
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.12624370428372403
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.12637536570429803
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.12596656834962322
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.12608294604489437
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.12580861137160715
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.12569266599085596
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.1259958490729332
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.1261543579665678
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.12617204024603493
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.12571581620080718
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.12598099812107572
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.12567948835591475
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.12580891112323667
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.12572778533062628
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.1258188501473457
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.12561380863189697
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.12566046921106486
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.1254447033685265
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.12524247514222986
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.12549405797001192
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.12570351113875708
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.12596254018800598
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.12634267259231755
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.1265701133136948
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.12671154625203512
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.12673155812395587
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.12678023169438044
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.12665521480927341
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.12663368996861693
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.1263159621411409
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.12674339389121986
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.1270069914869964
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.12701789252919915
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.1266348697245121
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.12644077550215893
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.1269071717702207
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.1270769327878952
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.12739831659682962
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.12763971843938718
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.12814766659655355
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.1280640209658762
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.12843878981139925
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.128632046855413
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.12861378118395805
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.12867827809626056
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.12862699264858632
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.12908231592492053
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.12913583676951626
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.12893456005558526
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.12878760178478396
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.12907169066896343
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.12894404366612433
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.12878759677457338
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.1286208208139036
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.128424865745225
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.1284663054662255
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.12807045018389113
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.12813649305476332
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.12839367936983287
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.12817130382690164
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.12827720534090603
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.1285288586535237
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.12826531085076634
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.12829279201105237
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.12801252931883905
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.1283387218258883
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.12813347416079562
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.12798131090299836
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.12837791073526073
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.12839068181939045
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.12826174729261078
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.12825323585420847
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.1282489010987203
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.1282861388486917
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.12848013875687994
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.12843143345127184
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.12846991461515428
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.1285333509837824
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.12860517655535947
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.1283528430503793
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.12830277006755503
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.12834206968545914
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.12822258239256518
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.12820081824831892
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.12853034507287175
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.12859610840678215
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.12836041897535325
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.12827957136666074
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.12842127115186983
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.1283745261422102
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.12859553323804046
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.12859227125133788
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.12863170403115293
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.12848631533938396
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.1285505862711193
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.12870971113443375
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.1286012671117125
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.12871617007337205
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.1287441001254685
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.12871177887191643
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.12909013082917103
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.12912801067034405
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.12898698064270397
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.12910131737589836
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.12905819139449426
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.1287489135737543
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.12879988512685223
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.1287866609218793
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.12887651440064618
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.12885821412635756
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.1289595383330711
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.12907692287117242
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.129188921973572
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.12911671781797468
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.12914957933447843
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.12934726235888352
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.1294990170634154
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.12937333376472254
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.1292358547627569
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.12914810303066457
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.12893976399002696
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.12923160743187456
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.12931149496495375
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.1292623421827028
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.12907057384707335
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.12898209417003326
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.1291054357801165
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.1289291437715292
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.12871322481982453
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.12843792333026952
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.12863034459465708
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.12842791796558434
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.1285251795175326
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.1284938572527288
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.1283930146791896
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.12836783419808614
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.12840753221834028
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.12835624317328134
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.12844333817614592
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.12833234271470537
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.1282851913618663
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.12836654703868064
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.12836353660253955
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.12827156367711723
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.12832090992075174
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.1280964400196813
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.12812717816768548
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.12828026309001203
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.12838285064636754
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.12853581330390892
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.12847884426164866
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.12865489542484285
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.12868771060782286
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.12875242073937218
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.12877419957973688
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.12879046651662565
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.12873311773305987
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.12859660542561013
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.1285596594646357
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.128394190616046
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.1284698137755029
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.12857984783393997
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.1285619647087644
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.12859689714914224
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.12868965032374916
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.1284723313194569
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.12856164616207744
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.12846415821048948
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.12844993682226277
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.12853423041214637
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.12860083967855532
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.12860487780787727
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.12860672553470232
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.12863216079301662
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.1285855256923111
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.12864579945536597
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.12854892522096634
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.12855914970282958
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.1286631209800422
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.12850436898307843
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.12845370632201825
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.1286889973865903
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.1289330900554017
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.12884841910723982
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.12891505658626556
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.12891335970061457
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.12889891311209253
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.12892200109564653
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.1288748789059965
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.12890588103973566
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.12892309934773705
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.12880751204987367
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.1289098021647742
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.12887068443800792
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.12893363109831948
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.1289702644724338
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.1289519947098226
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.12899442710648706
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.1289837064470357
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.12906181028172856
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.1290903823023819
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.1290507575571537
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.12924636630304306
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.12928280089464453
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.12930990587699084
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.12927590967632654
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.12924675777846692
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.12926068378146738
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.12919839883244918
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.12913755483405534
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.1292053923643694
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.12926383482722137
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.12914464993837693
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.1290678848860828
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.1290348618939349
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.12890811397157836
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.12891267630851494
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.1289356940502959
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.12892626585920205
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.12891350055474843
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.12880528596364876
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.1288289270191281
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.1288334250285177
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.12878373333746018
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.12890899358760743
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.1287790753328017
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.12865001101385462
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.12886980837345988
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.12883784062487985
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.1288522148250247
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.12882343599552748
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.1288391502574086
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.12879409816023293
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.12874107081310968
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.12875186667741398
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.12871768716460383
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.12884754546378788
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.1289499994185004
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.12886873718338146
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.12883886229246855
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.12878829359389507
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.12880646319224917
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.1288155180584524
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.12893703880987756
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.1291332171130099
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.12917699604010097
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.12914672178232064
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.12917402515036835
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.12915528595748574
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.12910194877510103
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.12904902926356498
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.12902141369879247
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.12904974092973823
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.12905475978326325
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.1290590334007449
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.1290213889932554
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.12898805173205546
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.12897742050243358
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.12901349266208345
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.12896951365393478
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.1289149433326181
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.12882727487914025
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.12888121501904976
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.128847651470166
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.12879080358690348
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.12876472693340035
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.12877089683971707
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.12893070667227613
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.1289204371276338
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.12892830423401586
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.12884185741127097
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.12893866393715142
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.1289026836322104
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.1288375465534859
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.12883830723286413
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.12875166266328758
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.12876503304793285
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.1286656326540043
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.1285531805879479
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.12855555569162455
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.12860100879404682
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.12867593203078617
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.1287102488367579
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.12871095863541207
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.12864700892755576
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.12867566932015076
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.12874452131452846
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.1287622593476304
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.12874562775257434
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.1287431659827218
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.12866589000618914
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.12866256828693784
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.12867814477238138
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.1289290075152241
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.12897208876060676
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.12901717056195403
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.12902175097361854
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.12898193351450682
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.1289657563271715
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.12895067749095374
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.12887491183328764
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.12880114653280803
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.12878977078572976
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.1288702390791679
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.12898303388958274
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.12902493372896298
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.12902670286490883
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.12893456411077064
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.12902072616735427
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.12907566885625185
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.12898690392546003
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.1289382685182823
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.12891089984337048
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.1289794934265192
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.12893711845326358
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.128896093835215
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.12892858190079257
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.12891406787793494
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.12898641652161158
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.1290679015826596
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.12902938822905222
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.12895453811900034
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.12900848005900808
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.1289840128513113
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.12899796890429455
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.12907630270894835
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.12910598621765773
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.12909218377651685
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.1290779522069253
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.12914071200543611
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.12907363977627265
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.12906480800164374
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.12906360055205077
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.12899267556467606
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.12906399559881296
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.1290689551193888
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.1289879898359249
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.12893486926283862
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.12894958038200705
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.12889550977684172
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.12889206419030927
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.12882669566151422
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.1288024847731566
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.1288097881502947
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.12891452020133726
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.1289277422586976
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.1289528189014785
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.1288799216989616
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.1287736193246445
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.12879417834794102
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.12877893812002095
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.12874563347548246
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.12875143306659642
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.1287268393520099
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.12866477492651335
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.12870877794921398
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.12875494752769118
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.12870334204414796
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.12870329993728924
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.12876214169184952
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.12870760733821865
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.12871239208957044
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.12869451195001602
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.12874547325219343
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.1287570702719342
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.12880733045906836
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.12886161105819496
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.12892461014696613
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.12891512868001306
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.12887502752208824
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.12891467312141933
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.12888047757248083
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.12893840728986858
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.12896549262071108
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.12888583003699638
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.1288779632205952
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.12886033184388104
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.1287655852770022
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.1287738297317849
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.12872847664022
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.12865120462534868
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.1285767026765402
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.1286236888917029
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.12858146108273003
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.12859241733237173
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.12863790329700242
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.1286194387694885
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.1287027976962678
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.12874629686094258
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.12871015059961577
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.12859057114037403
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.128570028309795
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.12858666667119176
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.1285346293172955
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.12852201341708142
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.12850352757618771
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.1285048582748081
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.1285063926915684
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.12847414294411938
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.12853324195436602
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.12852819942618265
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.1284960027370188
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.12847161552395894
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.12851766132609507
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.12846301523989137
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.12846137798239482
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.1284129004393305
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.1284246495516415
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.12841827513678591
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.1284412838128196
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.12844652413088253
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.1285324625832879
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.1285175726205563
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.12849913402037186
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.12844117194993449
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.12847393205196694
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.12848563892866977
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.12850667360386622
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.1285229649801581
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.12861081814536682
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.12865573129674265
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.12869859321954402
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.1287256991787321
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.12884045275464906
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.12884045275464906 valid 0.219881072640419
LOSS train 0.12884045275464906 valid 0.1919683739542961
LOSS train 0.12884045275464906 valid 0.18399287263552347
LOSS train 0.12884045275464906 valid 0.17449986189603806
LOSS train 0.12884045275464906 valid 0.17008877098560332
LOSS train 0.12884045275464906 valid 0.1802443489432335
LOSS train 0.12884045275464906 valid 0.19090413408620016
LOSS train 0.12884045275464906 valid 0.18894475139677525
LOSS train 0.12884045275464906 valid 0.18835779858960044
LOSS train 0.12884045275464906 valid 0.1882776901125908
LOSS train 0.12884045275464906 valid 0.18690351058136334
LOSS train 0.12884045275464906 valid 0.1877093923588594
LOSS train 0.12884045275464906 valid 0.18713627411768988
LOSS train 0.12884045275464906 valid 0.18661793640681676
LOSS train 0.12884045275464906 valid 0.18424088756243387
LOSS train 0.12884045275464906 valid 0.184217750094831
LOSS train 0.12884045275464906 valid 0.18488222272957072
LOSS train 0.12884045275464906 valid 0.18376503802008098
LOSS train 0.12884045275464906 valid 0.18615944840406118
LOSS train 0.12884045275464906 valid 0.1859777234494686
LOSS train 0.12884045275464906 valid 0.18497636985211147
LOSS train 0.12884045275464906 valid 0.18343938209793784
LOSS train 0.12884045275464906 valid 0.18318480253219604
LOSS train 0.12884045275464906 valid 0.18340160821874937
LOSS train 0.12884045275464906 valid 0.1820947289466858
LOSS train 0.12884045275464906 valid 0.1819595224582232
LOSS train 0.12884045275464906 valid 0.18235382989600854
LOSS train 0.12884045275464906 valid 0.18200149067810603
LOSS train 0.12884045275464906 valid 0.18146561445861026
LOSS train 0.12884045275464906 valid 0.18197540541489918
LOSS train 0.12884045275464906 valid 0.18281463894151873
LOSS train 0.12884045275464906 valid 0.18196688685566187
LOSS train 0.12884045275464906 valid 0.18225561533913467
LOSS train 0.12884045275464906 valid 0.1816695363206022
LOSS train 0.12884045275464906 valid 0.1833451564822878
LOSS train 0.12884045275464906 valid 0.1831075068977144
LOSS train 0.12884045275464906 valid 0.18386939892897736
LOSS train 0.12884045275464906 valid 0.18427249946092306
LOSS train 0.12884045275464906 valid 0.18395143747329712
LOSS train 0.12884045275464906 valid 0.18362633623182772
LOSS train 0.12884045275464906 valid 0.18418318651071408
LOSS train 0.12884045275464906 valid 0.1843753517383621
LOSS train 0.12884045275464906 valid 0.18438556686390278
LOSS train 0.12884045275464906 valid 0.18488729000091553
LOSS train 0.12884045275464906 valid 0.18497173521253799
LOSS train 0.12884045275464906 valid 0.18538054337967996
LOSS train 0.12884045275464906 valid 0.18600028816689837
LOSS train 0.12884045275464906 valid 0.18584282447894415
LOSS train 0.12884045275464906 valid 0.18644524228816128
LOSS train 0.12884045275464906 valid 0.18594585627317428
LOSS train 0.12884045275464906 valid 0.18615401872232848
LOSS train 0.12884045275464906 valid 0.18558467638034087
LOSS train 0.12884045275464906 valid 0.18612244219150184
LOSS train 0.12884045275464906 valid 0.18624484180300324
LOSS train 0.12884045275464906 valid 0.18614401763135738
LOSS train 0.12884045275464906 valid 0.18543950574738638
LOSS train 0.12884045275464906 valid 0.1853948230283302
LOSS train 0.12884045275464906 valid 0.18525470536330652
LOSS train 0.12884045275464906 valid 0.18561998388524784
LOSS train 0.12884045275464906 valid 0.18541937495271366
LOSS train 0.12884045275464906 valid 0.18498741969710492
LOSS train 0.12884045275464906 valid 0.18552628087420617
LOSS train 0.12884045275464906 valid 0.18486263945935263
LOSS train 0.12884045275464906 valid 0.1857579795178026
LOSS train 0.12884045275464906 valid 0.18597902426352866
LOSS train 0.12884045275464906 valid 0.1856699599461122
LOSS train 0.12884045275464906 valid 0.18507804198941188
LOSS train 0.12884045275464906 valid 0.18507446151445894
LOSS train 0.12884045275464906 valid 0.18470558934453604
LOSS train 0.12884045275464906 valid 0.18528016741786685
LOSS train 0.12884045275464906 valid 0.1849036388833758
LOSS train 0.12884045275464906 valid 0.18508771537906593
LOSS train 0.12884045275464906 valid 0.1850844139922155
LOSS train 0.12884045275464906 valid 0.18508476361229614
LOSS train 0.12884045275464906 valid 0.1854616556564967
LOSS train 0.12884045275464906 valid 0.18589564314798304
LOSS train 0.12884045275464906 valid 0.18571757863868366
LOSS train 0.12884045275464906 valid 0.18549663038590017
LOSS train 0.12884045275464906 valid 0.18516046468970143
LOSS train 0.12884045275464906 valid 0.18449513651430607
LOSS train 0.12884045275464906 valid 0.18414576443625086
LOSS train 0.12884045275464906 valid 0.1846118384018177
LOSS train 0.12884045275464906 valid 0.18441995511572046
LOSS train 0.12884045275464906 valid 0.18442689591930025
LOSS train 0.12884045275464906 valid 0.18418127736624548
LOSS train 0.12884045275464906 valid 0.18396254403646603
LOSS train 0.12884045275464906 valid 0.183736446088758
LOSS train 0.12884045275464906 valid 0.1834492122923786
LOSS train 0.12884045275464906 valid 0.18395336541566956
LOSS train 0.12884045275464906 valid 0.18392367329862383
LOSS train 0.12884045275464906 valid 0.18401182811338823
LOSS train 0.12884045275464906 valid 0.1839530850234239
LOSS train 0.12884045275464906 valid 0.18373647668669302
LOSS train 0.12884045275464906 valid 0.1839207906989341
LOSS train 0.12884045275464906 valid 0.18385477332692396
LOSS train 0.12884045275464906 valid 0.1839036598491172
LOSS train 0.12884045275464906 valid 0.18375052603864178
LOSS train 0.12884045275464906 valid 0.1839860306710613
LOSS train 0.12884045275464906 valid 0.1840371432328465
LOSS train 0.12884045275464906 valid 0.18414522111415862
LOSS train 0.12884045275464906 valid 0.18434399749973032
LOSS train 0.12884045275464906 valid 0.18457482609094358
LOSS train 0.12884045275464906 valid 0.1842380351233251
LOSS train 0.12884045275464906 valid 0.18439900903747633
LOSS train 0.12884045275464906 valid 0.18448701146103086
LOSS train 0.12884045275464906 valid 0.18494726197337205
LOSS train 0.12884045275464906 valid 0.1848503709117943
LOSS train 0.12884045275464906 valid 0.1850111830841612
LOSS train 0.12884045275464906 valid 0.18541180868761256
LOSS train 0.12884045275464906 valid 0.18545495650985025
LOSS train 0.12884045275464906 valid 0.1854075208172068
LOSS train 0.12884045275464906 valid 0.185218311447118
LOSS train 0.12884045275464906 valid 0.18527674833230212
LOSS train 0.12884045275464906 valid 0.1855837104090473
LOSS train 0.12884045275464906 valid 0.18581076730852542
LOSS train 0.12884045275464906 valid 0.18603591531001287
LOSS train 0.12884045275464906 valid 0.18595072041209945
LOSS train 0.12884045275464906 valid 0.18553288823972314
LOSS train 0.12884045275464906 valid 0.18518268899256443
LOSS train 0.12884045275464906 valid 0.18482974991202356
LOSS train 0.12884045275464906 valid 0.1846572377464988
LOSS train 0.12884045275464906 valid 0.18470279652564253
LOSS train 0.12884045275464906 valid 0.1844798448851438
LOSS train 0.12884045275464906 valid 0.18481170890792722
LOSS train 0.12884045275464906 valid 0.1846337420940399
LOSS train 0.12884045275464906 valid 0.18499561907753112
LOSS train 0.12884045275464906 valid 0.18483199941830372
LOSS train 0.12884045275464906 valid 0.18488195503596216
LOSS train 0.12884045275464906 valid 0.18503173989380978
LOSS train 0.12884045275464906 valid 0.184632459626748
LOSS train 0.12884045275464906 valid 0.18432058723828265
LOSS train 0.12884045275464906 valid 0.1839968612925573
LOSS train 0.12884045275464906 valid 0.1838773299652831
LOSS train 0.12884045275464906 valid 0.18400859999567715
LOSS train 0.12884045275464906 valid 0.18388299423235435
LOSS train 0.12884045275464906 valid 0.1838868812603109
LOSS train 0.12884045275464906 valid 0.18355465000563295
LOSS train 0.12884045275464906 valid 0.1834746320610461
LOSS train 0.12884045275464906 valid 0.18340600662523035
LOSS train 0.12884045275464906 valid 0.18350018241575786
LOSS train 0.12884045275464906 valid 0.18335168623755163
LOSS train 0.12884045275464906 valid 0.18337273366854223
LOSS train 0.12884045275464906 valid 0.1832451483794859
LOSS train 0.12884045275464906 valid 0.18324989753050935
LOSS train 0.12884045275464906 valid 0.1831297155084281
LOSS train 0.12884045275464906 valid 0.18333574864145827
LOSS train 0.12884045275464906 valid 0.18327053945486238
LOSS train 0.12884045275464906 valid 0.18420021588335167
LOSS train 0.12884045275464906 valid 0.18435641523175592
LOSS train 0.12884045275464906 valid 0.18430308878421783
LOSS train 0.12884045275464906 valid 0.18455346530636416
LOSS train 0.12884045275464906 valid 0.18420599871560148
LOSS train 0.12884045275464906 valid 0.184142228629854
LOSS train 0.12884045275464906 valid 0.18399476521200947
LOSS train 0.12884045275464906 valid 0.1838764266621682
LOSS train 0.12884045275464906 valid 0.18388222196163276
LOSS train 0.12884045275464906 valid 0.18398238595124264
LOSS train 0.12884045275464906 valid 0.1839665845979618
LOSS train 0.12884045275464906 valid 0.18407352952837194
LOSS train 0.12884045275464906 valid 0.18400081545114516
LOSS train 0.12884045275464906 valid 0.1838504753497817
LOSS train 0.12884045275464906 valid 0.18373034259787313
LOSS train 0.12884045275464906 valid 0.18344534686737995
LOSS train 0.12884045275464906 valid 0.18325156382307772
LOSS train 0.12884045275464906 valid 0.18308932934746597
LOSS train 0.12884045275464906 valid 0.1831713235342359
LOSS train 0.12884045275464906 valid 0.18352941651187257
LOSS train 0.12884045275464906 valid 0.18343552352771872
LOSS train 0.12884045275464906 valid 0.18360911486064188
LOSS train 0.12884045275464906 valid 0.18361369283760295
LOSS train 0.12884045275464906 valid 0.18362671624847324
LOSS train 0.12884045275464906 valid 0.18349686706828516
LOSS train 0.12884045275464906 valid 0.1834702813900964
LOSS train 0.12884045275464906 valid 0.1834262820153401
LOSS train 0.12884045275464906 valid 0.18310049176216125
LOSS train 0.12884045275464906 valid 0.18305899168957362
LOSS train 0.12884045275464906 valid 0.18317065636316934
LOSS train 0.12884045275464906 valid 0.1834216003170174
LOSS train 0.12884045275464906 valid 0.1833207630578366
LOSS train 0.12884045275464906 valid 0.18323852337068983
LOSS train 0.12884045275464906 valid 0.18337487095956645
LOSS train 0.12884045275464906 valid 0.18321980347672662
LOSS train 0.12884045275464906 valid 0.1832825517882415
LOSS train 0.12884045275464906 valid 0.18314199222494726
LOSS train 0.12884045275464906 valid 0.18300841152668
LOSS train 0.12884045275464906 valid 0.18305073646447992
LOSS train 0.12884045275464906 valid 0.18290022661022962
LOSS train 0.12884045275464906 valid 0.18289637367459052
LOSS train 0.12884045275464906 valid 0.18276187596182344
LOSS train 0.12884045275464906 valid 0.18284423272860678
LOSS train 0.12884045275464906 valid 0.18270910628803114
LOSS train 0.12884045275464906 valid 0.18277076515369117
LOSS train 0.12884045275464906 valid 0.18249187799933042
LOSS train 0.12884045275464906 valid 0.18242119682818345
LOSS train 0.12884045275464906 valid 0.1822090091613623
LOSS train 0.12884045275464906 valid 0.18216755963405784
LOSS train 0.12884045275464906 valid 0.18239048463741536
LOSS train 0.12884045275464906 valid 0.1823340736404814
LOSS train 0.12884045275464906 valid 0.18247099737426145
LOSS train 0.12884045275464906 valid 0.18228596121072768
LOSS train 0.12884045275464906 valid 0.18210005789846923
LOSS train 0.12884045275464906 valid 0.18203380040012965
LOSS train 0.12884045275464906 valid 0.18203760492684218
LOSS train 0.12884045275464906 valid 0.18224293402597017
LOSS train 0.12884045275464906 valid 0.18200577381180555
LOSS train 0.12884045275464906 valid 0.18202639580930322
LOSS train 0.12884045275464906 valid 0.18195325985622868
LOSS train 0.12884045275464906 valid 0.1817940570987188
LOSS train 0.12884045275464906 valid 0.18180755393927178
LOSS train 0.12884045275464906 valid 0.18173133248374576
LOSS train 0.12884045275464906 valid 0.18172581140746438
LOSS train 0.12884045275464906 valid 0.18169260756024774
LOSS train 0.12884045275464906 valid 0.18165384897603676
LOSS train 0.12884045275464906 valid 0.1816812386718866
LOSS train 0.12884045275464906 valid 0.1815545743981073
LOSS train 0.12884045275464906 valid 0.18141605829199156
LOSS train 0.12884045275464906 valid 0.1813405423944447
LOSS train 0.12884045275464906 valid 0.18138070178961535
LOSS train 0.12884045275464906 valid 0.18148497359393395
LOSS train 0.12884045275464906 valid 0.181423023681749
LOSS train 0.12884045275464906 valid 0.1813378830975537
LOSS train 0.12884045275464906 valid 0.1813711142083546
LOSS train 0.12884045275464906 valid 0.18157238757129207
LOSS train 0.12884045275464906 valid 0.1816780028332557
LOSS train 0.12884045275464906 valid 0.1818581055932575
LOSS train 0.12884045275464906 valid 0.1821175874334521
LOSS train 0.12884045275464906 valid 0.1821763860759231
LOSS train 0.12884045275464906 valid 0.182243578724171
LOSS train 0.12884045275464906 valid 0.1822447437126043
LOSS train 0.12884045275464906 valid 0.18228175432785698
LOSS train 0.12884045275464906 valid 0.18244352846434622
LOSS train 0.12884045275464906 valid 0.18246186897158623
LOSS train 0.12884045275464906 valid 0.1825692065283976
LOSS train 0.12884045275464906 valid 0.18263433071283194
LOSS train 0.12884045275464906 valid 0.1827945539291869
LOSS train 0.12884045275464906 valid 0.1826750645571846
LOSS train 0.12884045275464906 valid 0.18260163232244017
LOSS train 0.12884045275464906 valid 0.18262424338765504
LOSS train 0.12884045275464906 valid 0.18240814977111178
LOSS train 0.12884045275464906 valid 0.18239939014116924
LOSS train 0.12884045275464906 valid 0.1826004030422551
LOSS train 0.12884045275464906 valid 0.18243872220358573
LOSS train 0.12884045275464906 valid 0.18266081779336732
LOSS train 0.12884045275464906 valid 0.18284588545316555
LOSS train 0.12884045275464906 valid 0.1829075624748152
LOSS train 0.12884045275464906 valid 0.1827522631946618
LOSS train 0.12884045275464906 valid 0.18290433498770603
LOSS train 0.12884045275464906 valid 0.1828376510811429
LOSS train 0.12884045275464906 valid 0.18283923640547986
LOSS train 0.12884045275464906 valid 0.18285669898986817
LOSS train 0.12884045275464906 valid 0.18268715504156166
LOSS train 0.12884045275464906 valid 0.18294661675417234
LOSS train 0.12884045275464906 valid 0.1828834515197475
LOSS train 0.12884045275464906 valid 0.18276523904302927
LOSS train 0.12884045275464906 valid 0.182809048482016
LOSS train 0.12884045275464906 valid 0.18287056119879708
LOSS train 0.12884045275464906 valid 0.1826860961515152
LOSS train 0.12884045275464906 valid 0.18294861288957817
LOSS train 0.12884045275464906 valid 0.1829582058094643
LOSS train 0.12884045275464906 valid 0.18279577780228395
LOSS train 0.12884045275464906 valid 0.18296159958017283
LOSS train 0.12884045275464906 valid 0.18304760441297793
LOSS train 0.12884045275464906 valid 0.18309315245867683
LOSS train 0.12884045275464906 valid 0.18318690675677676
LOSS train 0.12884045275464906 valid 0.18315012893586788
LOSS train 0.12884045275464906 valid 0.18316892411251715
LOSS train 0.12884045275464906 valid 0.18322084565064434
LOSS train 0.12884045275464906 valid 0.1834204568902948
LOSS train 0.12884045275464906 valid 0.1835015539675397
LOSS train 0.12884045275464906 valid 0.18349987500243717
LOSS train 0.12884045275464906 valid 0.1836018848793093
LOSS train 0.12884045275464906 valid 0.1839267305262825
LOSS train 0.12884045275464906 valid 0.1841352723893665
LOSS train 0.12884045275464906 valid 0.1841782034531127
LOSS train 0.12884045275464906 valid 0.184155047156594
LOSS train 0.12884045275464906 valid 0.1840941048618676
LOSS train 0.12884045275464906 valid 0.18398218814431544
LOSS train 0.12884045275464906 valid 0.1838372198583411
LOSS train 0.12884045275464906 valid 0.1838200145190762
LOSS train 0.12884045275464906 valid 0.18380814407552992
LOSS train 0.12884045275464906 valid 0.1837360239007719
LOSS train 0.12884045275464906 valid 0.18342444002099917
LOSS train 0.12884045275464906 valid 0.18335454463432196
LOSS train 0.12884045275464906 valid 0.18337929193717492
LOSS train 0.12884045275464906 valid 0.18341879319203527
LOSS train 0.12884045275464906 valid 0.18343962913939169
LOSS train 0.12884045275464906 valid 0.1834011088524546
LOSS train 0.12884045275464906 valid 0.18337742065907353
LOSS train 0.12884045275464906 valid 0.18338331995228996
LOSS train 0.12884045275464906 valid 0.18341968298472208
LOSS train 0.12884045275464906 valid 0.18325250728945552
LOSS train 0.12884045275464906 valid 0.18324058908611945
LOSS train 0.12884045275464906 valid 0.18324329098845504
LOSS train 0.12884045275464906 valid 0.18331942071213203
LOSS train 0.12884045275464906 valid 0.18340112902855468
LOSS train 0.12884045275464906 valid 0.183317918959703
LOSS train 0.12884045275464906 valid 0.1834098914205426
LOSS train 0.12884045275464906 valid 0.1834461477728898
LOSS train 0.12884045275464906 valid 0.18347544053625503
LOSS train 0.12884045275464906 valid 0.1835736365367969
LOSS train 0.12884045275464906 valid 0.18351476337525535
LOSS train 0.12884045275464906 valid 0.1834602417387315
LOSS train 0.12884045275464906 valid 0.18350618352295936
LOSS train 0.12884045275464906 valid 0.1834954712784996
LOSS train 0.12884045275464906 valid 0.18343826314953507
LOSS train 0.12884045275464906 valid 0.1834717320120023
LOSS train 0.12884045275464906 valid 0.18345222864846064
LOSS train 0.12884045275464906 valid 0.18335414578678547
LOSS train 0.12884045275464906 valid 0.18335041989689893
LOSS train 0.12884045275464906 valid 0.18340033184616797
LOSS train 0.12884045275464906 valid 0.18339134606613608
LOSS train 0.12884045275464906 valid 0.1833751960299336
LOSS train 0.12884045275464906 valid 0.1835602931083201
LOSS train 0.12884045275464906 valid 0.18356948163194262
LOSS train 0.12884045275464906 valid 0.1834885508531616
LOSS train 0.12884045275464906 valid 0.18346759594411036
LOSS train 0.12884045275464906 valid 0.18353713092273718
LOSS train 0.12884045275464906 valid 0.1835766479888427
LOSS train 0.12884045275464906 valid 0.1837443932890892
LOSS train 0.12884045275464906 valid 0.18367067764047534
LOSS train 0.12884045275464906 valid 0.18385050891437263
LOSS train 0.12884045275464906 valid 0.18384350278640385
LOSS train 0.12884045275464906 valid 0.1837941788529095
LOSS train 0.12884045275464906 valid 0.18390533036011972
LOSS train 0.12884045275464906 valid 0.18390452405581106
LOSS train 0.12884045275464906 valid 0.18407083373585362
LOSS train 0.12884045275464906 valid 0.18411994611542523
LOSS train 0.12884045275464906 valid 0.18407357786214207
LOSS train 0.12884045275464906 valid 0.18412945781888207
LOSS train 0.12884045275464906 valid 0.18412263657559047
LOSS train 0.12884045275464906 valid 0.18400788993723802
LOSS train 0.12884045275464906 valid 0.18387699941823998
LOSS train 0.12884045275464906 valid 0.18393628713157442
LOSS train 0.12884045275464906 valid 0.18401286445424228
LOSS train 0.12884045275464906 valid 0.18397355766883536
LOSS train 0.12884045275464906 valid 0.18405092332423442
LOSS train 0.12884045275464906 valid 0.184049315794816
LOSS train 0.12884045275464906 valid 0.18403615240426457
LOSS train 0.12884045275464906 valid 0.1840583644477667
LOSS train 0.12884045275464906 valid 0.18399526154732002
LOSS train 0.12884045275464906 valid 0.1838451966293746
LOSS train 0.12884045275464906 valid 0.18384947063542947
LOSS train 0.12884045275464906 valid 0.18389249028252444
LOSS train 0.12884045275464906 valid 0.18414183262042527
LOSS train 0.12884045275464906 valid 0.18426021594500197
LOSS train 0.12884045275464906 valid 0.18435100287285155
LOSS train 0.12884045275464906 valid 0.18420666179241296
LOSS train 0.12884045275464906 valid 0.1841436188411096
LOSS train 0.12884045275464906 valid 0.18413045688566984
LOSS train 0.12884045275464906 valid 0.18406030171683857
LOSS train 0.12884045275464906 valid 0.1839785979386748
LOSS train 0.12884045275464906 valid 0.18397297957827421
LOSS train 0.12884045275464906 valid 0.1839803989776133
LOSS train 0.12884045275464906 valid 0.183998636206833
LOSS train 0.12884045275464906 valid 0.1840554749671842
LOSS train 0.12884045275464906 valid 0.18404866761287278
LOSS train 0.12884045275464906 valid 0.1840823482857699
LOSS train 0.12884045275464906 valid 0.18397465283727513
LOSS train 0.12884045275464906 valid 0.18397747662987218
LOSS train 0.12884045275464906 valid 0.1839329519826505
LOSS train 0.12884045275464906 valid 0.1838793818764079
LOSS train 0.12884045275464906 valid 0.1839324812821591
LOSS train 0.12884045275464906 valid 0.18393129617304185
LOSS train 0.12884045275464906 valid 0.18391912051855208
LOSS train 0.12884045275464906 valid 0.1839754750263201
LOSS train 0.12884045275464906 valid 0.18399910321408283
LOSS train 0.12884045275464906 valid 0.1839091708168022
LOSS train 0.12884045275464906 valid 0.18395403122691356
LOSS train 0.12884045275464906 valid 0.18403404226228798
EPOCH 23:
  batch 1 loss: 0.11454488337039948
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.11526597291231155
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.11245190103848775
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.12170692533254623
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.1247897207736969
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.12688682973384857
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.12145177594252995
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.1234088335186243
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.12215081519550747
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.12188672795891761
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.12001136622645638
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.11891513752440612
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.1198258910041589
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.11964459930147443
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.11836258719364802
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.12009990448132157
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.11893086705137701
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.12180610125263532
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.12154475285818703
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.12090999893844127
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.12123101346549534
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.12175411155278032
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.1215938851237297
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.12328093467901151
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.12251003831624985
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.12288647517561913
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.12312868816985025
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.12296216030205999
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.12263410358593382
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.12328009853760401
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.12397808461419997
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.12437798222526908
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.12349119353474992
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.12402485245290924
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.12542495876550674
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.1247414354648855
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.12458024194111696
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.12433568662718723
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.12483876332258567
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.12482777051627636
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.12497265491543746
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.12520657515241987
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.1262126627356507
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.12570910748432984
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.12594598217142952
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.12540371181524318
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.1254008230059705
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.12557598995044827
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.12558662420024677
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.125672839730978
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.1252337361083311
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.1253529145167424
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.12519111048500492
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.12502150789455133
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.12535807246511632
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.12549302274627344
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.12569052843671097
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.12526041667523055
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.12553592998597582
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.12534440432985625
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.12544405387073268
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.12537342358020045
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.1254725619440987
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.12524828012101352
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.12532415917286507
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.12509688193147833
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.1249371015313846
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.12523531826103435
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.1254671032446018
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.1256968406694276
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.12611557690190597
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.12632854779561362
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.12643755360008918
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.12648634632696976
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.12647734820842743
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.12636974826455116
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.12632049523390732
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.12603896178114107
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.12637625964759272
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.1266524146310985
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.12674343779131217
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.1263716880504678
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.12614100812429405
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.1265578456223011
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.12673893886453966
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.1270274187243262
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.12723944516017519
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.12769288739020174
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.1276514656590612
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.12802639810575378
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.12826304260518526
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.12824995532307937
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.12831741226937182
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.12823311017548783
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.12869167782758412
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.12873661409442624
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.12854696211126662
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.12837819891924762
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.1287048150493641
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.1285420325398445
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.12839331204938417
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.12825162548060512
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.12801973384271548
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.12810348927115017
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.12770356308846248
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.12778617708750492
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.12803898250388207
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.12780683080631275
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.12798460518274832
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.12817861932245167
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.1279395665000151
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.12793710767956717
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.1276933102481133
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.12805626672088055
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.12791153885748074
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.12779105294110446
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.12815192577421156
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.12816772780428498
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.12802150870571619
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.12803297427793345
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.12800462704059506
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.1280548594525603
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.1281463020458454
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.12808166516404
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.12813051295280456
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.12815022456740577
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.1281911212158954
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.12794976448640227
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.1278772751490275
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.12787958864982313
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.12771631453100962
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.1276404976167462
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.1280379912682942
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.1281278254603272
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.12789257687551003
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.12783175553469098
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.12796655797610318
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.1279121154676313
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.1280992182253076
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.1281265149159091
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.1281595472114306
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.1279931468757945
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.12803538023800284
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.1281626275740564
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.12806524454519666
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.12818844305121735
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.12823147286160463
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.12818415654269424
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.12856499340710223
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.1286348416407903
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.12854544621034963
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.12865488172361725
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.1286282933810178
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.1283185612168405
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.1283678866682514
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.12837012078708562
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.12846011051516623
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.12842029955568193
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.12852003027058248
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.12861651536077262
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.1287622935838581
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.1287086601426572
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.12872127256144775
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.12895607257761607
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.12911308931581902
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.12899742311382867
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.1288511740590284
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.12875725834497384
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.1285767194170218
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.12885401884422581
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.12892708622398433
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.12889034781864908
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.12869512500790503
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.1286185984553277
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.12874889778239387
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.12857116060331464
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.12836154478754702
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.12809232258227435
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.12829913902382611
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.1281243453009261
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.12821621653618734
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.1281648478419571
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.12808012901270976
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.12802062175520088
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.1280750726525848
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.1280091742994965
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.12806757145068226
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.12796910249806465
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.12792354246611318
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.1279562830140716
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.12796604453893232
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.12789117773839584
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.12796170766335077
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.12773750783856383
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.12777394545383944
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.12790220702181057
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.12800913549921838
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.12815521619837694
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.12807754936380003
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.12826613556593658
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.1283008742021091
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.12837241534696947
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.1283944768491637
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.12844773458645625
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.12834945124823874
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.12822283978022417
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.12818624946662194
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.1280225277878344
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.12814439765431665
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.12826724048881305
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.12824972528275722
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.12829984299276234
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.12837880986257338
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.12815611412592023
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.12824998026670412
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.12814781711333328
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.12814525078793276
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.12822114074722343
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.12828942364481485
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.1282891801812432
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.1283080780128548
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.1283215954899788
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.12828819092880986
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.12835775901164329
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.12825405610932245
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.12826400599648466
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.12837533134195772
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.1282032223320321
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.12815155677790205
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.12838614119783692
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.12862222744917973
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.1285292038629795
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.12859133397560774
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.128589852179727
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.12859360207902623
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.1286282251446934
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.12861348355369728
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.12864044665538965
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.1286592805859434
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.12852631074686846
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.12866660563035626
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.1286267683464141
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.12867708936517622
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.12873939580482538
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.12872460140865677
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.12878479163457707
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.12878671973219769
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.12886949397263028
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.12891622734476763
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.12886403223872184
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.1290791005905406
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.12911137746321777
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.1291367462560122
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.12911374588298985
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.12906113711057926
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.12907533935504034
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.12901435804042372
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.12893370062459347
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.1290049069924244
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.12904587516990992
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.12893294617248222
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.1288651555264724
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.12880870498745162
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.12867959089238534
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.12870609167049515
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.1287379997285237
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.12871069821079126
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.1287078305300492
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.1286307979084302
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.1286282622427852
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.1286532682851232
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.1286052452016841
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.12872601567934722
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.1285988874733448
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.12845623742450368
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.12866719158879225
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.1286399659266971
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.1286593338997244
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.1286197671112621
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.12861827324543681
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.12857776079525726
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.12850518965869084
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.12850182229659582
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.12845267250504291
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.12858509546832034
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.12869348538505448
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.1286036720830389
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.12857608292769226
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.12850763378052563
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.12855373189367097
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.1285739599429455
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.12867851830916863
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.12886328166255365
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.12888869481021856
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.12884865910825083
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.12888232434822902
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.1288740692867173
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.12882214449596086
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.12877002510438396
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.12872592575848102
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.1287565224144942
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.1287475726906432
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.12875218441088995
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.12870484029286003
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.12867528686269386
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.12867044163198252
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.1287141979371686
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.12867509249549408
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.1286242017853993
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.12852719195427434
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.12860823679008668
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.12856563488737896
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.12849208017507682
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.12847755047356246
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.12848462670568436
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.12864057493360737
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.1286389469536321
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.1286265626081131
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.12854416930002852
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.12864528968930244
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.12860564232986665
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.1285530512932665
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.12856574324262401
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.1284772907556207
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.12849099134023373
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.12838887736384122
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.1282831028969645
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.12828172457109138
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.12834359809620402
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.1284194148399613
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.1284478797923402
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.12843759232256785
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.12838274553731396
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.1284067410819545
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.12847547050732286
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.12848632760523332
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.1284636906417258
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.12846497812214688
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.1283884473518636
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.1283863963887972
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.1283938339815811
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.12865327477280858
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.12870557232605126
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.1287380029382401
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.12873677369477093
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.12869817522392107
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.1286766261978864
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.1286597997378344
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.1285819265681898
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.12849472248128482
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.12850276601535301
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.12859850755723362
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.12868879256353163
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.12872399551804456
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.12871254176740915
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.12861635512933972
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.12869549818149134
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.12875471715737322
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.1286764920671006
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.1286308711187707
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.12859863451478223
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.12867406607497464
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.1286405363522942
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.12859018689410373
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.12862984033888333
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.12863263852202175
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.128710436946852
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.12882384744918216
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.12879191611598179
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.1287043553349134
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.12875923466168324
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.1287433134532103
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.12876015326292842
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.12884152712509594
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.12887257130940755
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.1288543158468414
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.12884469990388783
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.12890965692580691
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.12884702380147017
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.12885456871437398
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.12883981714254916
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.12877098623058558
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.1288358059471021
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.12883601147526255
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.12875284928006012
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.12871969701635405
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.1287340139657336
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.1286890336609993
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.1286825640725599
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.12861654691589183
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.12859007630430525
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.12860275483785236
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.12870672578347547
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.12872318675751007
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.1287421362875383
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.12868378751657225
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.1285742718904685
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.1286008737457158
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.12858063006834278
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.12854709250852467
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.12854440954335014
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.12852323620547704
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.12846186263463633
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.12849400430401364
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.1285302699164108
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.12847611232857986
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.12849111410191955
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.12854036241404565
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.12848048045101843
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.12848525154517917
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.12846822830012244
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.1285295202170761
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.12854953550397627
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.1286086687359257
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.12866831888635474
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.12874866416677833
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.1287465850226313
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.1287038763494868
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.12875465762430274
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.12872054206118697
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.1287742043865265
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.1287962367572773
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.12871026096262267
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.1286998437014672
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.12868928471032312
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.1285949749740916
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.1286049922644673
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.128559683684573
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.12849293864551403
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.12842435805603516
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.1284783039665554
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.12844718406321826
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.12845693762665808
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.12851951302196574
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.12849232167348093
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.1285879475602863
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.128648046191006
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.12861566740560204
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.12850043535707753
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.1284864667295055
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.12849880344405465
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.12846450298620027
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.12844396007518466
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.1284162478001268
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.1284207511818811
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.12842772597021052
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.1284019244163895
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.12845953765125678
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.128461640319872
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.1284296558631791
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.1284026763920245
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.1284494296177826
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.12838097735746018
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.12837878828006694
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.1283299201301166
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.12834025577952465
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.12833645227375384
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.12835902826429454
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.12837127713011762
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.12845834213430468
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.12844222582020107
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.1284332974164775
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.12835894982062973
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.12837656693340377
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.12837855639637158
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.12840293597254118
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.1284220323955732
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.12850896240426943
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.12854541682485324
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.1285915437213918
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.1286252760140506
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.12872520733182713
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.12872520733182713 valid 0.2169746309518814
LOSS train 0.12872520733182713 valid 0.18849162012338638
LOSS train 0.12872520733182713 valid 0.18063807487487793
LOSS train 0.12872520733182713 valid 0.17124361172318459
LOSS train 0.12872520733182713 valid 0.16691688001155852
LOSS train 0.12872520733182713 valid 0.1768206631143888
LOSS train 0.12872520733182713 valid 0.18736886765275682
LOSS train 0.12872520733182713 valid 0.18543937057256699
LOSS train 0.12872520733182713 valid 0.18491162028577593
LOSS train 0.12872520733182713 valid 0.18474827706813812
LOSS train 0.12872520733182713 valid 0.18324592303146015
LOSS train 0.12872520733182713 valid 0.1841543142994245
LOSS train 0.12872520733182713 valid 0.1835487141058995
LOSS train 0.12872520733182713 valid 0.18302137617553985
LOSS train 0.12872520733182713 valid 0.18063946664333344
LOSS train 0.12872520733182713 valid 0.18062706105411053
LOSS train 0.12872520733182713 valid 0.18126298311878652
LOSS train 0.12872520733182713 valid 0.18012244254350662
LOSS train 0.12872520733182713 valid 0.1825581404723619
LOSS train 0.12872520733182713 valid 0.1823177807033062
LOSS train 0.12872520733182713 valid 0.18132371987615312
LOSS train 0.12872520733182713 valid 0.17982868647033518
LOSS train 0.12872520733182713 valid 0.1795631828515426
LOSS train 0.12872520733182713 valid 0.1797876680890719
LOSS train 0.12872520733182713 valid 0.17848463833332062
LOSS train 0.12872520733182713 valid 0.1783583966585306
LOSS train 0.12872520733182713 valid 0.1787730720308092
LOSS train 0.12872520733182713 valid 0.17841600999236107
LOSS train 0.12872520733182713 valid 0.17786664746958633
LOSS train 0.12872520733182713 valid 0.17836125542720158
LOSS train 0.12872520733182713 valid 0.17917354741404135
LOSS train 0.12872520733182713 valid 0.17834493797272444
LOSS train 0.12872520733182713 valid 0.17862424164107352
LOSS train 0.12872520733182713 valid 0.17805289082667408
LOSS train 0.12872520733182713 valid 0.17968325912952424
LOSS train 0.12872520733182713 valid 0.17943635375963318
LOSS train 0.12872520733182713 valid 0.1801962187966785
LOSS train 0.12872520733182713 valid 0.1805751696228981
LOSS train 0.12872520733182713 valid 0.18027037802415016
LOSS train 0.12872520733182713 valid 0.17994836196303368
LOSS train 0.12872520733182713 valid 0.18049377239331965
LOSS train 0.12872520733182713 valid 0.18068548469316392
LOSS train 0.12872520733182713 valid 0.18069731218870297
LOSS train 0.12872520733182713 valid 0.18118753081018274
LOSS train 0.12872520733182713 valid 0.18128094441360898
LOSS train 0.12872520733182713 valid 0.18168186882267828
LOSS train 0.12872520733182713 valid 0.18230095125259238
LOSS train 0.12872520733182713 valid 0.1821415120114883
LOSS train 0.12872520733182713 valid 0.1827108847851656
LOSS train 0.12872520733182713 valid 0.18222509443759918
LOSS train 0.12872520733182713 valid 0.182423175257795
LOSS train 0.12872520733182713 valid 0.1818566881120205
LOSS train 0.12872520733182713 valid 0.18238995744372313
LOSS train 0.12872520733182713 valid 0.1825085229895733
LOSS train 0.12872520733182713 valid 0.18241156773133713
LOSS train 0.12872520733182713 valid 0.18171910329588822
LOSS train 0.12872520733182713 valid 0.18167417927792198
LOSS train 0.12872520733182713 valid 0.18153431754687738
LOSS train 0.12872520733182713 valid 0.18189932810047926
LOSS train 0.12872520733182713 valid 0.18169684633612632
LOSS train 0.12872520733182713 valid 0.1812698758039318
LOSS train 0.12872520733182713 valid 0.1818022237670037
LOSS train 0.12872520733182713 valid 0.1811431771706021
LOSS train 0.12872520733182713 valid 0.18203151691704988
LOSS train 0.12872520733182713 valid 0.18225712065513316
LOSS train 0.12872520733182713 valid 0.18195608845262817
LOSS train 0.12872520733182713 valid 0.1813713065279064
LOSS train 0.12872520733182713 valid 0.18136150039294185
LOSS train 0.12872520733182713 valid 0.18100025645200757
LOSS train 0.12872520733182713 valid 0.18157797924109867
LOSS train 0.12872520733182713 valid 0.18120348117720914
LOSS train 0.12872520733182713 valid 0.18138333120279843
LOSS train 0.12872520733182713 valid 0.18137606978416443
LOSS train 0.12872520733182713 valid 0.1813732442018148
LOSS train 0.12872520733182713 valid 0.1817427537838618
LOSS train 0.12872520733182713 valid 0.18216167488380483
LOSS train 0.12872520733182713 valid 0.18198365069829026
LOSS train 0.12872520733182713 valid 0.1817666429739732
LOSS train 0.12872520733182713 valid 0.18144620068465608
LOSS train 0.12872520733182713 valid 0.18078955840319394
LOSS train 0.12872520733182713 valid 0.180442856049832
LOSS train 0.12872520733182713 valid 0.18089749645896075
LOSS train 0.12872520733182713 valid 0.18071001570626913
LOSS train 0.12872520733182713 valid 0.18071234119789942
LOSS train 0.12872520733182713 valid 0.18045053622301888
LOSS train 0.12872520733182713 valid 0.18024327519328096
LOSS train 0.12872520733182713 valid 0.1800185762260152
LOSS train 0.12872520733182713 valid 0.17973650500855662
LOSS train 0.12872520733182713 valid 0.1802393922979912
LOSS train 0.12872520733182713 valid 0.1802071506778399
LOSS train 0.12872520733182713 valid 0.180291915169129
LOSS train 0.12872520733182713 valid 0.18023577575450359
LOSS train 0.12872520733182713 valid 0.18002256670946715
LOSS train 0.12872520733182713 valid 0.1802036465799555
LOSS train 0.12872520733182713 valid 0.18013893632512343
LOSS train 0.12872520733182713 valid 0.1801886335015297
LOSS train 0.12872520733182713 valid 0.1800348285547237
LOSS train 0.12872520733182713 valid 0.1802760105656118
LOSS train 0.12872520733182713 valid 0.18032335738341013
LOSS train 0.12872520733182713 valid 0.18043045088648796
LOSS train 0.12872520733182713 valid 0.18062182331439292
LOSS train 0.12872520733182713 valid 0.18084947297386095
LOSS train 0.12872520733182713 valid 0.18050772269952645
LOSS train 0.12872520733182713 valid 0.1806627016227979
LOSS train 0.12872520733182713 valid 0.18076070476145972
LOSS train 0.12872520733182713 valid 0.1812230684566048
LOSS train 0.12872520733182713 valid 0.181120934051888
LOSS train 0.12872520733182713 valid 0.1812719831036197
LOSS train 0.12872520733182713 valid 0.1816749533108615
LOSS train 0.12872520733182713 valid 0.18171652257442475
LOSS train 0.12872520733182713 valid 0.1816699212198859
LOSS train 0.12872520733182713 valid 0.1814833472349814
LOSS train 0.12872520733182713 valid 0.1815390792568173
LOSS train 0.12872520733182713 valid 0.18184863344619148
LOSS train 0.12872520733182713 valid 0.1820710177006929
LOSS train 0.12872520733182713 valid 0.1822943602656496
LOSS train 0.12872520733182713 valid 0.1822158513415573
LOSS train 0.12872520733182713 valid 0.18180888575517526
LOSS train 0.12872520733182713 valid 0.18146335302280778
LOSS train 0.12872520733182713 valid 0.18111411333084107
LOSS train 0.12872520733182713 valid 0.18094245584543087
LOSS train 0.12872520733182713 valid 0.1809875271359428
LOSS train 0.12872520733182713 valid 0.18076731894559006
LOSS train 0.12872520733182713 valid 0.18109393035692553
LOSS train 0.12872520733182713 valid 0.1809264158010483
LOSS train 0.12872520733182713 valid 0.18128022598841834
LOSS train 0.12872520733182713 valid 0.18112363238034285
LOSS train 0.12872520733182713 valid 0.18117154599167407
LOSS train 0.12872520733182713 valid 0.1813148122656253
LOSS train 0.12872520733182713 valid 0.18091934667183804
LOSS train 0.12872520733182713 valid 0.18061315467339437
LOSS train 0.12872520733182713 valid 0.18029813570055095
LOSS train 0.12872520733182713 valid 0.18017953891951338
LOSS train 0.12872520733182713 valid 0.18030525065624892
LOSS train 0.12872520733182713 valid 0.1801754097143809
LOSS train 0.12872520733182713 valid 0.18018129007781253
LOSS train 0.12872520733182713 valid 0.17985082988756418
LOSS train 0.12872520733182713 valid 0.1797725015144417
LOSS train 0.12872520733182713 valid 0.17970820714672692
LOSS train 0.12872520733182713 valid 0.17979693029608046
LOSS train 0.12872520733182713 valid 0.17964805673200188
LOSS train 0.12872520733182713 valid 0.17967251436391346
LOSS train 0.12872520733182713 valid 0.17954203653168846
LOSS train 0.12872520733182713 valid 0.17953893387069306
LOSS train 0.12872520733182713 valid 0.1794132013773096
LOSS train 0.12872520733182713 valid 0.17961339234081034
LOSS train 0.12872520733182713 valid 0.1795446468251092
LOSS train 0.12872520733182713 valid 0.18046468224477125
LOSS train 0.12872520733182713 valid 0.18062364845067863
LOSS train 0.12872520733182713 valid 0.1805801577369372
LOSS train 0.12872520733182713 valid 0.18083080836873971
LOSS train 0.12872520733182713 valid 0.18048292006316938
LOSS train 0.12872520733182713 valid 0.1804241802957323
LOSS train 0.12872520733182713 valid 0.18027442490512674
LOSS train 0.12872520733182713 valid 0.18015538617487878
LOSS train 0.12872520733182713 valid 0.18016134173824236
LOSS train 0.12872520733182713 valid 0.18025352886527968
LOSS train 0.12872520733182713 valid 0.18023447447185276
LOSS train 0.12872520733182713 valid 0.18034058275087825
LOSS train 0.12872520733182713 valid 0.18026489969342946
LOSS train 0.12872520733182713 valid 0.18011458856718882
LOSS train 0.12872520733182713 valid 0.17999998930795694
LOSS train 0.12872520733182713 valid 0.17971954133612977
LOSS train 0.12872520733182713 valid 0.17952578805568742
LOSS train 0.12872520733182713 valid 0.17936363409865985
LOSS train 0.12872520733182713 valid 0.17944910467030054
LOSS train 0.12872520733182713 valid 0.17980666155229785
LOSS train 0.12872520733182713 valid 0.1797094584575721
LOSS train 0.12872520733182713 valid 0.1798760218204126
LOSS train 0.12872520733182713 valid 0.17988171445972778
LOSS train 0.12872520733182713 valid 0.17989903270152577
LOSS train 0.12872520733182713 valid 0.17977493828119234
LOSS train 0.12872520733182713 valid 0.1797471062813191
LOSS train 0.12872520733182713 valid 0.17969700599881425
LOSS train 0.12872520733182713 valid 0.1793736891235624
LOSS train 0.12872520733182713 valid 0.1793336926705458
LOSS train 0.12872520733182713 valid 0.17944209058742738
LOSS train 0.12872520733182713 valid 0.17969950974992152
LOSS train 0.12872520733182713 valid 0.1796043054851074
LOSS train 0.12872520733182713 valid 0.17952127448386615
LOSS train 0.12872520733182713 valid 0.17966167826349563
LOSS train 0.12872520733182713 valid 0.17951128476268643
LOSS train 0.12872520733182713 valid 0.17957249886351204
LOSS train 0.12872520733182713 valid 0.1794340334346761
LOSS train 0.12872520733182713 valid 0.1793010325045199
LOSS train 0.12872520733182713 valid 0.17934286065639987
LOSS train 0.12872520733182713 valid 0.1791945952463915
LOSS train 0.12872520733182713 valid 0.17918845472183634
LOSS train 0.12872520733182713 valid 0.17905285607570062
LOSS train 0.12872520733182713 valid 0.17913689958421808
LOSS train 0.12872520733182713 valid 0.17899491470209591
LOSS train 0.12872520733182713 valid 0.17905770343107483
LOSS train 0.12872520733182713 valid 0.1787813301994393
LOSS train 0.12872520733182713 valid 0.17871483737967678
LOSS train 0.12872520733182713 valid 0.1785044422516456
LOSS train 0.12872520733182713 valid 0.17846070100762407
LOSS train 0.12872520733182713 valid 0.17867719249676933
LOSS train 0.12872520733182713 valid 0.17862110848378654
LOSS train 0.12872520733182713 valid 0.17875739445338895
LOSS train 0.12872520733182713 valid 0.17857164286077024
LOSS train 0.12872520733182713 valid 0.1783828619700759
LOSS train 0.12872520733182713 valid 0.1783178803501743
LOSS train 0.12872520733182713 valid 0.17831871309891123
LOSS train 0.12872520733182713 valid 0.17852065894825786
LOSS train 0.12872520733182713 valid 0.17828355046307168
LOSS train 0.12872520733182713 valid 0.17830152238167604
LOSS train 0.12872520733182713 valid 0.1782264224285089
LOSS train 0.12872520733182713 valid 0.17806999084468073
LOSS train 0.12872520733182713 valid 0.17807951619465384
LOSS train 0.12872520733182713 valid 0.17800667867774056
LOSS train 0.12872520733182713 valid 0.17800517422610548
LOSS train 0.12872520733182713 valid 0.1779737352340851
LOSS train 0.12872520733182713 valid 0.1779326106740835
LOSS train 0.12872520733182713 valid 0.17796226528203377
LOSS train 0.12872520733182713 valid 0.17783683746360068
LOSS train 0.12872520733182713 valid 0.17770131560111488
LOSS train 0.12872520733182713 valid 0.17762556301283947
LOSS train 0.12872520733182713 valid 0.17766661326819605
LOSS train 0.12872520733182713 valid 0.177769906308553
LOSS train 0.12872520733182713 valid 0.17770493159239942
LOSS train 0.12872520733182713 valid 0.17762227801445923
LOSS train 0.12872520733182713 valid 0.17765263004882917
LOSS train 0.12872520733182713 valid 0.17785436730213763
LOSS train 0.12872520733182713 valid 0.1779582322841244
LOSS train 0.12872520733182713 valid 0.17813778261343638
LOSS train 0.12872520733182713 valid 0.1783965628231521
LOSS train 0.12872520733182713 valid 0.17845784075197144
LOSS train 0.12872520733182713 valid 0.17852071646535606
LOSS train 0.12872520733182713 valid 0.17852202721558283
LOSS train 0.12872520733182713 valid 0.1785582591658053
LOSS train 0.12872520733182713 valid 0.17871767824346368
LOSS train 0.12872520733182713 valid 0.17873231225229544
LOSS train 0.12872520733182713 valid 0.1788394020094892
LOSS train 0.12872520733182713 valid 0.17890235603365123
LOSS train 0.12872520733182713 valid 0.17906545857165723
LOSS train 0.12872520733182713 valid 0.17894809112205343
LOSS train 0.12872520733182713 valid 0.17887818297756372
LOSS train 0.12872520733182713 valid 0.17890469091279165
LOSS train 0.12872520733182713 valid 0.17868824839342587
LOSS train 0.12872520733182713 valid 0.17867755219340326
LOSS train 0.12872520733182713 valid 0.1788768370121841
LOSS train 0.12872520733182713 valid 0.17871751445384065
LOSS train 0.12872520733182713 valid 0.17893820768030583
LOSS train 0.12872520733182713 valid 0.17912312987886492
LOSS train 0.12872520733182713 valid 0.17918733072524168
LOSS train 0.12872520733182713 valid 0.17903402459815265
LOSS train 0.12872520733182713 valid 0.1791811418919428
LOSS train 0.12872520733182713 valid 0.17911713762629417
LOSS train 0.12872520733182713 valid 0.17911580875216718
LOSS train 0.12872520733182713 valid 0.1791348156929016
LOSS train 0.12872520733182713 valid 0.17896707385659694
LOSS train 0.12872520733182713 valid 0.17922197524753827
LOSS train 0.12872520733182713 valid 0.17916021204513052
LOSS train 0.12872520733182713 valid 0.1790387483797674
LOSS train 0.12872520733182713 valid 0.17908304789487053
LOSS train 0.12872520733182713 valid 0.17914300208212808
LOSS train 0.12872520733182713 valid 0.17896153409657312
LOSS train 0.12872520733182713 valid 0.17921946989011395
LOSS train 0.12872520733182713 valid 0.1792281463339522
LOSS train 0.12872520733182713 valid 0.1790650551708845
LOSS train 0.12872520733182713 valid 0.1792334269632325
LOSS train 0.12872520733182713 valid 0.17932396040845464
LOSS train 0.12872520733182713 valid 0.17936734821179973
LOSS train 0.12872520733182713 valid 0.1794601112717029
LOSS train 0.12872520733182713 valid 0.17942428268351646
LOSS train 0.12872520733182713 valid 0.17944477268851788
LOSS train 0.12872520733182713 valid 0.17949595573019891
LOSS train 0.12872520733182713 valid 0.1796923615038395
LOSS train 0.12872520733182713 valid 0.1797718717041512
LOSS train 0.12872520733182713 valid 0.1797722585775234
LOSS train 0.12872520733182713 valid 0.1798698166419659
LOSS train 0.12872520733182713 valid 0.18019148871740875
LOSS train 0.12872520733182713 valid 0.18039784937987835
LOSS train 0.12872520733182713 valid 0.18043875754097083
LOSS train 0.12872520733182713 valid 0.1804151221297004
LOSS train 0.12872520733182713 valid 0.18035318923817165
LOSS train 0.12872520733182713 valid 0.1802448979461236
LOSS train 0.12872520733182713 valid 0.18010545329009886
LOSS train 0.12872520733182713 valid 0.18008869031851438
LOSS train 0.12872520733182713 valid 0.18007485302431242
LOSS train 0.12872520733182713 valid 0.18000438204863742
LOSS train 0.12872520733182713 valid 0.17969779450948356
LOSS train 0.12872520733182713 valid 0.17962603015634282
LOSS train 0.12872520733182713 valid 0.17965053439035383
LOSS train 0.12872520733182713 valid 0.1796884512430743
LOSS train 0.12872520733182713 valid 0.17970970327724944
LOSS train 0.12872520733182713 valid 0.1796704324332264
LOSS train 0.12872520733182713 valid 0.17964508226658735
LOSS train 0.12872520733182713 valid 0.1796488897765384
LOSS train 0.12872520733182713 valid 0.17968310192227363
LOSS train 0.12872520733182713 valid 0.17951820148123088
LOSS train 0.12872520733182713 valid 0.17950398982693888
LOSS train 0.12872520733182713 valid 0.17950654088331977
LOSS train 0.12872520733182713 valid 0.179583090832647
LOSS train 0.12872520733182713 valid 0.17966083745330066
LOSS train 0.12872520733182713 valid 0.17957799320386067
LOSS train 0.12872520733182713 valid 0.1796702892340795
LOSS train 0.12872520733182713 valid 0.17970453495247252
LOSS train 0.12872520733182713 valid 0.17972983228024034
LOSS train 0.12872520733182713 valid 0.17982920654118062
LOSS train 0.12872520733182713 valid 0.17977094028974291
LOSS train 0.12872520733182713 valid 0.17971962312890205
LOSS train 0.12872520733182713 valid 0.17976395158779504
LOSS train 0.12872520733182713 valid 0.179751244496162
LOSS train 0.12872520733182713 valid 0.1796943476209875
LOSS train 0.12872520733182713 valid 0.17972529341095414
LOSS train 0.12872520733182713 valid 0.1797051504254341
LOSS train 0.12872520733182713 valid 0.1796087126017778
LOSS train 0.12872520733182713 valid 0.17960050111353204
LOSS train 0.12872520733182713 valid 0.17964797863556492
LOSS train 0.12872520733182713 valid 0.1796361404168644
LOSS train 0.12872520733182713 valid 0.17962010240612122
LOSS train 0.12872520733182713 valid 0.1798027703127922
LOSS train 0.12872520733182713 valid 0.17980927729587645
LOSS train 0.12872520733182713 valid 0.17972718053866946
LOSS train 0.12872520733182713 valid 0.17971007457545288
LOSS train 0.12872520733182713 valid 0.1797781353258184
LOSS train 0.12872520733182713 valid 0.17981834571410274
LOSS train 0.12872520733182713 valid 0.1799859274003573
LOSS train 0.12872520733182713 valid 0.17991516918409617
LOSS train 0.12872520733182713 valid 0.18009348726625382
LOSS train 0.12872520733182713 valid 0.18008784535601272
LOSS train 0.12872520733182713 valid 0.1800376781415275
LOSS train 0.12872520733182713 valid 0.1801440429954249
LOSS train 0.12872520733182713 valid 0.1801451573234338
LOSS train 0.12872520733182713 valid 0.18031042190325772
LOSS train 0.12872520733182713 valid 0.18035730124886976
LOSS train 0.12872520733182713 valid 0.18031254731027818
LOSS train 0.12872520733182713 valid 0.18036615135187797
LOSS train 0.12872520733182713 valid 0.18035931580445982
LOSS train 0.12872520733182713 valid 0.18024600895960166
LOSS train 0.12872520733182713 valid 0.1801142535250948
LOSS train 0.12872520733182713 valid 0.18017258271351233
LOSS train 0.12872520733182713 valid 0.18024901624122067
LOSS train 0.12872520733182713 valid 0.18021124385630907
LOSS train 0.12872520733182713 valid 0.1802875868133491
LOSS train 0.12872520733182713 valid 0.1802877763656551
LOSS train 0.12872520733182713 valid 0.18027527482640107
LOSS train 0.12872520733182713 valid 0.180296281028462
LOSS train 0.12872520733182713 valid 0.18023509736008503
LOSS train 0.12872520733182713 valid 0.1800860930616555
LOSS train 0.12872520733182713 valid 0.18009402426449875
LOSS train 0.12872520733182713 valid 0.18013614253208868
LOSS train 0.12872520733182713 valid 0.18038484101118737
LOSS train 0.12872520733182713 valid 0.1805040034910907
LOSS train 0.12872520733182713 valid 0.18059001356966234
LOSS train 0.12872520733182713 valid 0.1804462653888062
LOSS train 0.12872520733182713 valid 0.18038372042449727
LOSS train 0.12872520733182713 valid 0.1803693429147958
LOSS train 0.12872520733182713 valid 0.1802984947179045
LOSS train 0.12872520733182713 valid 0.18021752392379647
LOSS train 0.12872520733182713 valid 0.18021172498860819
LOSS train 0.12872520733182713 valid 0.18021902258352246
LOSS train 0.12872520733182713 valid 0.1802359732909728
LOSS train 0.12872520733182713 valid 0.18029269981132426
LOSS train 0.12872520733182713 valid 0.1802860661043545
LOSS train 0.12872520733182713 valid 0.18031740173989652
LOSS train 0.12872520733182713 valid 0.1802142204810454
LOSS train 0.12872520733182713 valid 0.18021713913234164
LOSS train 0.12872520733182713 valid 0.18017385347435871
LOSS train 0.12872520733182713 valid 0.18011886411839245
LOSS train 0.12872520733182713 valid 0.18017241755984106
LOSS train 0.12872520733182713 valid 0.18016876278255267
LOSS train 0.12872520733182713 valid 0.180159312539867
LOSS train 0.12872520733182713 valid 0.1802155630025145
LOSS train 0.12872520733182713 valid 0.18023960188519758
LOSS train 0.12872520733182713 valid 0.18015106089352262
LOSS train 0.12872520733182713 valid 0.18019510694253055
LOSS train 0.12872520733182713 valid 0.18027314933415675
EPOCH 24:
  batch 1 loss: 0.11515005677938461
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.1168590597808361
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.11394551396369934
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.12236113473773003
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.12516030371189119
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.12719538311163583
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.12167296345744814
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.12346261087805033
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.12222467031743792
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.12114447951316834
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.11947210945866325
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.11859140731394291
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.11947383502354988
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.11898078343697957
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.11756784617900848
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.11947371903806925
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.11868937751826118
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.1218763209051556
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.12144032826549128
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.12088439390063285
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.12136662290209815
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.12177959219975905
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.12188441597897073
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.1234642347941796
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.12272509098052979
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.12323434765522297
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.12358649792494597
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.12310158115412508
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.12265580866871209
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.12352170223991076
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.12401805457568937
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.12443513167090714
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.12375997255245845
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.12451486881164943
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.12596221885510853
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.12530734410716426
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.12512865743121585
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.12483975526533629
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.1253572228627327
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.12539126873016357
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.1255436706833723
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.12572816546474183
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.1267391363548678
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.12634304369037802
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.12643613980876076
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.12579793968926306
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.12569705317629146
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.1258012168109417
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.12575407463068866
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.12585279151797293
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.12549149055106967
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.12557215243577957
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.12535499907889455
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.12524406708501004
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.1255650285970081
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.1256884720974735
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.12577849901036212
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.12527997098092375
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.12572117478160535
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.12551236401001611
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.1256796309693915
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.12560750183559233
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.12560212257362546
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.12539875193033367
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.12541918972363839
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.12517495995218103
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.125007839456423
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.1252220975344672
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.12543912221124207
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.1257522575557232
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.12617381641142805
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.1264417665079236
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.12648088729953114
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.12651199998485074
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.1265644147992134
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.12649360759870001
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.12646107791693179
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.1261225723876403
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.12654583240988887
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.12680362658575178
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.12688435053015934
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.12654552427006932
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.12639223200729094
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.12679338472939672
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.12694927068317638
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.12728112589481266
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.12752485994634957
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.1279495529491793
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.1278896006975281
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.12833142876625062
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.1285045115829824
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.1284517504112876
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.12849625232078696
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.12843623194605747
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.1288963670009061
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.1289975370746106
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.12881534339226397
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.12859379043992686
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.12890209257602692
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.12880595788359642
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.1285943705404159
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.1284011916494837
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.12812498045488468
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.12817952201630062
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.12775889840864
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.12780905226772687
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.128062142738115
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.1278116369826926
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.12798618484254276
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.12824574607339773
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.12800610823942735
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.12802014027589134
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.1277319388711347
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.1281185150800044
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.12795607907616574
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.12783670104269323
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.12814579725774944
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.1281416705351765
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.12801982424840205
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.1280167892575264
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.12800136541039492
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.12807841660057911
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.12821102542121235
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.12817534431815147
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.12818292236328124
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.12822972746595504
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.1282613706870342
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.12800922169117257
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.12794541833243628
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.12794956650871497
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.1277993451661736
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.1277504260228439
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.12807035104448633
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.128114598177707
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.12788743912069886
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.1278396303053288
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.12801579483886705
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.12798889656213747
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.1282230306336348
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.1282211784273386
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.12823987868449366
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.12809057476025232
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.1281823894673294
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.12831006390560004
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.12817176143670905
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.1282711235729799
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.1282998696983266
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.1282836165077783
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.1286960790721362
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.12873738691210745
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.12858353940066913
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.1286935690594347
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.1286519738583783
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.12833563654453722
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.12836328206523773
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.12834719262826136
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.12839370140224504
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.12836498998199838
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.12846679099888172
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.1285571411717683
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.12868277464779268
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.12861015845412088
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.12860760197866183
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.12885553176264938
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.12900990794102352
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.12888527791722712
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.12875735282362577
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.12868772411630267
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.12848751617253884
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.12874715626239777
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.1288341442395372
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.1288148072916408
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.12863563194957084
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.1285605511959942
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.128681263923645
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.1284940857016905
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.1282787428653173
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.12797535725691345
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.12816448230816666
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.1280107224567069
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.12807495468064567
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.12805946936810408
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.1279648200817447
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.12791248455481685
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.12793062647452225
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.12782200069356991
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.12791779800691708
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.12783439183964374
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.12777938244361725
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.12783213716588523
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.12785276072343607
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.12775980941175172
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.1278018653778832
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.12756259658748342
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.12763347553137022
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.12777482569977947
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.12786374014192425
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.1279982899249804
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.12794972971156615
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.12807757630944253
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.12811151621353567
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.1281714266774678
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.128204879649167
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.12827233845988908
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.12821033262624973
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.1280731479592115
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.12803173925421665
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.12786874887891686
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.12797344803952715
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.12809239090198563
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.12808995835165277
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.12811356364696655
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.1282094881078447
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.12799755992176376
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.12810049847114918
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.12800383854105515
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.12798847405729205
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.12808344872036112
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.12814853558121206
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.12815353541888974
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.12817773590395354
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.12819436484494726
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.1281648306635463
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.12824307012903904
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.12814022345675363
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.1281672434271437
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.12826987355947495
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.12808724156205067
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.12803713294095867
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.12829183074443237
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.1285530391322586
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.12844609629748197
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.12850280285137405
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.12848901914225686
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.12845063523409214
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.1284833361978753
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.12844482394336146
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.12847904394529447
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.12849644739882218
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.12837571411704024
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.12849837105674863
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.12843236508817713
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.12848791507659135
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.1285229117960715
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.1285093298067852
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.1285704575721326
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.12856679651056707
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.1286385241114805
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.12869236869625297
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.12863425287604333
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.1288219355195167
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.12885981050157358
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.1288818601857532
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.1288648567155121
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.12884322226631875
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.1288525060226675
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.12876820317850038
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.12867675557039504
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.1287509637046965
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.12879966465899578
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.12868176828855757
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.1286145059093264
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.12858128692147397
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.1284686831870314
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.12848603818214166
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.1285107415589623
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.12848872719297694
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.12849309626243896
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.12841099018722663
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.12843333576564436
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.12844947755776648
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.12841128300437155
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.1285340761963701
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.12841096895672108
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.12830329797484658
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.12850161042550337
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.1284663228028948
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.1284850268162412
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.1284445756011539
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.12845376719321525
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.1284105745392762
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.12833320262267234
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.1283368503299703
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.12829805440990857
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.1284354285451404
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.1285303705274225
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.12844520217969443
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.1284245936355243
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.12836594046915278
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.12841539642420308
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.12844123209977068
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.12853324451573092
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.128725984829888
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.12875284399000966
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.12871504653308352
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.12874715382585655
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.12873166852226162
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.12869694616470562
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.12864940279602605
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.12861284134288628
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.1286419202075448
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.12864092537602842
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.12864139732649618
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.12861213597812152
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.12857811167591907
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.12857371972861634
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.12861745467209273
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.12857654168234242
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.12852665779563602
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.12843353195536522
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.1284926299401035
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.1284483551310423
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.12839173589842007
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.12837054656379543
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.12838065061304305
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.12852680263451383
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.12850279476176302
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.12849604792939792
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.12842389056021145
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.12851039508823306
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.12846744578111208
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.12839590880719032
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.12840610592634685
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.12831639142040116
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.12832845025337658
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.12822820269074175
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.1281089531627999
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.12809351043457665
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.12813578933296232
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.12821077557675767
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.1282541083110063
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.12823928634535117
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.12817581416339846
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.12822249578590877
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.12829514423857874
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.12833201124643287
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.12831879614987785
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.12831081073047848
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.12824116194494353
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.12823442054145476
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.128244282737855
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.12848876224972353
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.12853232039133244
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.12857978990258173
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.12857276983019233
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.12854184632356455
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.12853190204080311
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.12851382331687145
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.12844755504824712
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.1283594631084374
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.1283648713913738
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.12846146217039364
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.12854554981162122
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.12859104786299716
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.12858380887290122
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.12849942400130662
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.12859799316366846
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.12865344278888996
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.12857437885073233
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.12851777395440472
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.1284880346290953
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.12857943085840393
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.12854258404126181
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.1284869221745284
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.1285236828335344
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.12850791000114764
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.12858187088524614
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.12868072716114315
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.12863906602145533
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.1285572233030925
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.12861118168442076
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.12859320786771594
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.12859234990846696
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.12866322447192222
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.12868970761696497
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.12866467936955234
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.12864077771057816
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.12870615870548935
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.1286507390616437
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.12863228681840394
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.12862821193191948
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.12856070432051314
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.1286266407985289
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.1286309758434072
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.12855151102140353
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.12850754501520043
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.1285329255677317
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.12847019845293356
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.12847414852184624
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.1284076641767453
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.12839897396162037
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.12840092763760869
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.12850825382401318
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.12853434201575778
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.12853412250929241
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.1284716362603987
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.12837297050868834
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.12839341743956859
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.12837691677916319
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.1283472468331456
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.1283538157951802
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.1283290250161987
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.1282796979357232
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.1283313056804461
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.1283758837683701
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.1283288610355901
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.12833510915902088
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.1283771462336767
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.1283278205502587
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.1283239879986135
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.12830897152351348
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.12836603662993723
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.12838126399687358
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.1284464034949236
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.12849842455372754
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.1285720114202167
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.12856399871819882
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.12854176829234834
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.1285899639094075
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.1285533468992937
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.12861795404886123
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.1286419331321219
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.12855919122907286
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.1285417177099383
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.12853024731664098
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.12843791042135355
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.12846383480333332
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.12842584537150703
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.12835124513624988
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.12827481305183366
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.12832828191067946
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.12829631015106482
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.12832413162975884
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.12838145870766882
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.1283563353549475
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.12845769186222225
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.12851332247393626
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.12846314531142852
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.12835294099902236
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.12834235587241974
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.12836163710642293
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.12831621827785247
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.1282908533348725
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.12826799688575505
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.12827212174287003
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.12827868318611196
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.12825121640559012
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.1283155505578699
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.12831128667417774
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.12828119575977326
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.1282540157007801
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.12830610115048105
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.1282434801984307
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.12824559283939227
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.128195071547896
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.12820097748516945
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.12820472504690908
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.1282281996641617
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.1282279300858512
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.12830013632774354
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.12827476490604128
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.12827118224911876
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.12820839730632486
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.12823201176421395
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.1282298856845466
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.12824078367887137
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.12825006288047522
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.1283409375156093
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.128371177133975
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.12842021955454602
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.1284507583913783
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.12856585065187034
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.12856585065187034 valid 0.22050461173057556
LOSS train 0.12856585065187034 valid 0.19167374819517136
LOSS train 0.12856585065187034 valid 0.18380247056484222
LOSS train 0.12856585065187034 valid 0.17438533902168274
LOSS train 0.12856585065187034 valid 0.1699727326631546
LOSS train 0.12856585065187034 valid 0.18003334353367487
LOSS train 0.12856585065187034 valid 0.1906005037682397
LOSS train 0.12856585065187034 valid 0.18874353170394897
LOSS train 0.12856585065187034 valid 0.18822036683559418
LOSS train 0.12856585065187034 valid 0.18808392584323883
LOSS train 0.12856585065187034 valid 0.18671791390939194
LOSS train 0.12856585065187034 valid 0.18747038145860037
LOSS train 0.12856585065187034 valid 0.18694595534067887
LOSS train 0.12856585065187034 valid 0.18641748811517442
LOSS train 0.12856585065187034 valid 0.184003088871638
LOSS train 0.12856585065187034 valid 0.1840045116841793
LOSS train 0.12856585065187034 valid 0.18468276016852436
LOSS train 0.12856585065187034 valid 0.18358142756753498
LOSS train 0.12856585065187034 valid 0.18601751405941813
LOSS train 0.12856585065187034 valid 0.18582338467240334
LOSS train 0.12856585065187034 valid 0.18482350877353124
LOSS train 0.12856585065187034 valid 0.1833461116660725
LOSS train 0.12856585065187034 valid 0.18310006610725238
LOSS train 0.12856585065187034 valid 0.18331224409242472
LOSS train 0.12856585065187034 valid 0.18203134715557098
LOSS train 0.12856585065187034 valid 0.1819223492191388
LOSS train 0.12856585065187034 valid 0.1822362580785045
LOSS train 0.12856585065187034 valid 0.18188775703310966
LOSS train 0.12856585065187034 valid 0.1813593609579678
LOSS train 0.12856585065187034 valid 0.1818794533610344
LOSS train 0.12856585065187034 valid 0.18271429740613507
LOSS train 0.12856585065187034 valid 0.18189326766878366
LOSS train 0.12856585065187034 valid 0.18216877362944864
LOSS train 0.12856585065187034 valid 0.18159477544181488
LOSS train 0.12856585065187034 valid 0.18315984989915576
LOSS train 0.12856585065187034 valid 0.182890175945229
LOSS train 0.12856585065187034 valid 0.1836634608539375
LOSS train 0.12856585065187034 valid 0.18399259134342796
LOSS train 0.12856585065187034 valid 0.18369210950839213
LOSS train 0.12856585065187034 valid 0.18339507691562176
LOSS train 0.12856585065187034 valid 0.18396299241519556
LOSS train 0.12856585065187034 valid 0.18413859144562766
LOSS train 0.12856585065187034 valid 0.1841710341531177
LOSS train 0.12856585065187034 valid 0.1846840615299615
LOSS train 0.12856585065187034 valid 0.18474496669239468
LOSS train 0.12856585065187034 valid 0.1851597680993702
LOSS train 0.12856585065187034 valid 0.1858126936440772
LOSS train 0.12856585065187034 valid 0.18566671231140694
LOSS train 0.12856585065187034 valid 0.18627765744316335
LOSS train 0.12856585065187034 valid 0.18578885316848756
LOSS train 0.12856585065187034 valid 0.18597536607115878
LOSS train 0.12856585065187034 valid 0.1854164325273954
LOSS train 0.12856585065187034 valid 0.18594337156358756
LOSS train 0.12856585065187034 valid 0.18605992363558876
LOSS train 0.12856585065187034 valid 0.1859721690416336
LOSS train 0.12856585065187034 valid 0.18527407571673393
LOSS train 0.12856585065187034 valid 0.18520478773535343
LOSS train 0.12856585065187034 valid 0.18507694761300908
LOSS train 0.12856585065187034 valid 0.1854545607910318
LOSS train 0.12856585065187034 valid 0.18523633753259977
LOSS train 0.12856585065187034 valid 0.18480853125697277
LOSS train 0.12856585065187034 valid 0.18535294600071445
LOSS train 0.12856585065187034 valid 0.18469972624665215
LOSS train 0.12856585065187034 valid 0.18559358431957662
LOSS train 0.12856585065187034 valid 0.18582447217060968
LOSS train 0.12856585065187034 valid 0.18551983300483588
LOSS train 0.12856585065187034 valid 0.18493387369967218
LOSS train 0.12856585065187034 valid 0.1849005496238961
LOSS train 0.12856585065187034 valid 0.18453982817953912
LOSS train 0.12856585065187034 valid 0.18513855189085007
LOSS train 0.12856585065187034 valid 0.18477352719071885
LOSS train 0.12856585065187034 valid 0.1849510692473915
LOSS train 0.12856585065187034 valid 0.1849481863518284
LOSS train 0.12856585065187034 valid 0.18494262868488157
LOSS train 0.12856585065187034 valid 0.18531625111897787
LOSS train 0.12856585065187034 valid 0.18575027486995646
LOSS train 0.12856585065187034 valid 0.18557577357663738
LOSS train 0.12856585065187034 valid 0.18535068325507334
LOSS train 0.12856585065187034 valid 0.1850149605093123
LOSS train 0.12856585065187034 valid 0.1843512499704957
LOSS train 0.12856585065187034 valid 0.1840059174063765
LOSS train 0.12856585065187034 valid 0.1844676633070155
LOSS train 0.12856585065187034 valid 0.1842809500823538
LOSS train 0.12856585065187034 valid 0.18428984497274672
LOSS train 0.12856585065187034 valid 0.1840258547488381
LOSS train 0.12856585065187034 valid 0.18381691412177198
LOSS train 0.12856585065187034 valid 0.18359421330621872
LOSS train 0.12856585065187034 valid 0.18330535478889942
LOSS train 0.12856585065187034 valid 0.18381797195820326
LOSS train 0.12856585065187034 valid 0.18379896018240188
LOSS train 0.12856585065187034 valid 0.18388547592765683
LOSS train 0.12856585065187034 valid 0.18382974892206813
LOSS train 0.12856585065187034 valid 0.18360665424536632
LOSS train 0.12856585065187034 valid 0.1837944928952988
LOSS train 0.12856585065187034 valid 0.18372822184311716
LOSS train 0.12856585065187034 valid 0.18378563007960716
LOSS train 0.12856585065187034 valid 0.18363342656917178
LOSS train 0.12856585065187034 valid 0.18387407794290658
LOSS train 0.12856585065187034 valid 0.18392685447076354
LOSS train 0.12856585065187034 valid 0.184027056992054
LOSS train 0.12856585065187034 valid 0.18422041833400726
LOSS train 0.12856585065187034 valid 0.18444308974579268
LOSS train 0.12856585065187034 valid 0.18410598797705566
LOSS train 0.12856585065187034 valid 0.18425693993385023
LOSS train 0.12856585065187034 valid 0.18435051469575792
LOSS train 0.12856585065187034 valid 0.18480948471235778
LOSS train 0.12856585065187034 valid 0.18470643168297884
LOSS train 0.12856585065187034 valid 0.18487134327491125
LOSS train 0.12856585065187034 valid 0.18527702276312977
LOSS train 0.12856585065187034 valid 0.18532014204697175
LOSS train 0.12856585065187034 valid 0.1852831559943723
LOSS train 0.12856585065187034 valid 0.18509849347174168
LOSS train 0.12856585065187034 valid 0.18515292469379122
LOSS train 0.12856585065187034 valid 0.1854681434077129
LOSS train 0.12856585065187034 valid 0.18568977882032808
LOSS train 0.12856585065187034 valid 0.18591541385856167
LOSS train 0.12856585065187034 valid 0.1858356649804319
LOSS train 0.12856585065187034 valid 0.18541809196694423
LOSS train 0.12856585065187034 valid 0.18506671989164433
LOSS train 0.12856585065187034 valid 0.18471499383449555
LOSS train 0.12856585065187034 valid 0.18454601153854497
LOSS train 0.12856585065187034 valid 0.18459573094962073
LOSS train 0.12856585065187034 valid 0.18437577953668144
LOSS train 0.12856585065187034 valid 0.18469701070458658
LOSS train 0.12856585065187034 valid 0.1845236015319824
LOSS train 0.12856585065187034 valid 0.18487788618557036
LOSS train 0.12856585065187034 valid 0.18471851651593457
LOSS train 0.12856585065187034 valid 0.1847683525411412
LOSS train 0.12856585065187034 valid 0.18490984261960022
LOSS train 0.12856585065187034 valid 0.1845119282603264
LOSS train 0.12856585065187034 valid 0.1842006186720069
LOSS train 0.12856585065187034 valid 0.1838811474541823
LOSS train 0.12856585065187034 valid 0.18376217804905168
LOSS train 0.12856585065187034 valid 0.18387725437755015
LOSS train 0.12856585065187034 valid 0.18375035987959967
LOSS train 0.12856585065187034 valid 0.1837642210371354
LOSS train 0.12856585065187034 valid 0.18343225306403027
LOSS train 0.12856585065187034 valid 0.18335513608611148
LOSS train 0.12856585065187034 valid 0.1832884296024446
LOSS train 0.12856585065187034 valid 0.1833766082567828
LOSS train 0.12856585065187034 valid 0.18322918696183685
LOSS train 0.12856585065187034 valid 0.1832463297415787
LOSS train 0.12856585065187034 valid 0.18312087780112155
LOSS train 0.12856585065187034 valid 0.18311810865998268
LOSS train 0.12856585065187034 valid 0.1829948546557591
LOSS train 0.12856585065187034 valid 0.18320187691548098
LOSS train 0.12856585065187034 valid 0.18314083630130404
LOSS train 0.12856585065187034 valid 0.18407041106272387
LOSS train 0.12856585065187034 valid 0.18423575872942904
LOSS train 0.12856585065187034 valid 0.18418955792983374
LOSS train 0.12856585065187034 valid 0.184439207050974
LOSS train 0.12856585065187034 valid 0.1840944237222797
LOSS train 0.12856585065187034 valid 0.1840360184120976
LOSS train 0.12856585065187034 valid 0.1838947566492217
LOSS train 0.12856585065187034 valid 0.18377414893719457
LOSS train 0.12856585065187034 valid 0.18377936803377593
LOSS train 0.12856585065187034 valid 0.18387541630465515
LOSS train 0.12856585065187034 valid 0.18385561980024168
LOSS train 0.12856585065187034 valid 0.1839573896905911
LOSS train 0.12856585065187034 valid 0.18388489857316018
LOSS train 0.12856585065187034 valid 0.18373216966294353
LOSS train 0.12856585065187034 valid 0.18361062032205086
LOSS train 0.12856585065187034 valid 0.1833273500569759
LOSS train 0.12856585065187034 valid 0.18313316010483882
LOSS train 0.12856585065187034 valid 0.18297138927560863
LOSS train 0.12856585065187034 valid 0.18305908682116542
LOSS train 0.12856585065187034 valid 0.18341371854265293
LOSS train 0.12856585065187034 valid 0.18332318615700519
LOSS train 0.12856585065187034 valid 0.18349341095904625
LOSS train 0.12856585065187034 valid 0.18349821471116123
LOSS train 0.12856585065187034 valid 0.18350882819521497
LOSS train 0.12856585065187034 valid 0.18337975927563602
LOSS train 0.12856585065187034 valid 0.18335560997786549
LOSS train 0.12856585065187034 valid 0.18331293444866423
LOSS train 0.12856585065187034 valid 0.18298216308866228
LOSS train 0.12856585065187034 valid 0.182942272587256
LOSS train 0.12856585065187034 valid 0.18305347607297412
LOSS train 0.12856585065187034 valid 0.18331071357713657
LOSS train 0.12856585065187034 valid 0.18321511956899525
LOSS train 0.12856585065187034 valid 0.18313475292589929
LOSS train 0.12856585065187034 valid 0.18327879065966737
LOSS train 0.12856585065187034 valid 0.18312675866124395
LOSS train 0.12856585065187034 valid 0.18318803103569428
LOSS train 0.12856585065187034 valid 0.18305220430635888
LOSS train 0.12856585065187034 valid 0.18291125120343388
LOSS train 0.12856585065187034 valid 0.1829591800448715
LOSS train 0.12856585065187034 valid 0.18281007211476086
LOSS train 0.12856585065187034 valid 0.1828048272811352
LOSS train 0.12856585065187034 valid 0.1826705608576063
LOSS train 0.12856585065187034 valid 0.18275265897575177
LOSS train 0.12856585065187034 valid 0.1826171974861185
LOSS train 0.12856585065187034 valid 0.18267337054324648
LOSS train 0.12856585065187034 valid 0.1823949116653729
LOSS train 0.12856585065187034 valid 0.18232326539828606
LOSS train 0.12856585065187034 valid 0.18211365877053676
LOSS train 0.12856585065187034 valid 0.18207349719441668
LOSS train 0.12856585065187034 valid 0.18228534015302125
LOSS train 0.12856585065187034 valid 0.18223244358192792
LOSS train 0.12856585065187034 valid 0.1823707771810455
LOSS train 0.12856585065187034 valid 0.1821800610423088
LOSS train 0.12856585065187034 valid 0.18199737696208765
LOSS train 0.12856585065187034 valid 0.18193617035256754
LOSS train 0.12856585065187034 valid 0.18193599904699279
LOSS train 0.12856585065187034 valid 0.18213890492916107
LOSS train 0.12856585065187034 valid 0.18189676018749795
LOSS train 0.12856585065187034 valid 0.18191408235760567
LOSS train 0.12856585065187034 valid 0.18184520923285094
LOSS train 0.12856585065187034 valid 0.18167924587256634
LOSS train 0.12856585065187034 valid 0.1816908666534287
LOSS train 0.12856585065187034 valid 0.18161518098342985
LOSS train 0.12856585065187034 valid 0.18160925297093053
LOSS train 0.12856585065187034 valid 0.1815801702158631
LOSS train 0.12856585065187034 valid 0.18154046324217263
LOSS train 0.12856585065187034 valid 0.18157245197864336
LOSS train 0.12856585065187034 valid 0.1814477963503017
LOSS train 0.12856585065187034 valid 0.1813109673007771
LOSS train 0.12856585065187034 valid 0.18123414123662607
LOSS train 0.12856585065187034 valid 0.18126866529020694
LOSS train 0.12856585065187034 valid 0.18137483149086503
LOSS train 0.12856585065187034 valid 0.18131244805726138
LOSS train 0.12856585065187034 valid 0.18123031060350428
LOSS train 0.12856585065187034 valid 0.18126802374650766
LOSS train 0.12856585065187034 valid 0.18146862891490148
LOSS train 0.12856585065187034 valid 0.1815675473106759
LOSS train 0.12856585065187034 valid 0.1817403717835744
LOSS train 0.12856585065187034 valid 0.18200075046151085
LOSS train 0.12856585065187034 valid 0.18206549636067798
LOSS train 0.12856585065187034 valid 0.18213106789871267
LOSS train 0.12856585065187034 valid 0.18213206692293743
LOSS train 0.12856585065187034 valid 0.18216982218234434
LOSS train 0.12856585065187034 valid 0.18231726434581724
LOSS train 0.12856585065187034 valid 0.18233359065549126
LOSS train 0.12856585065187034 valid 0.18244462885570117
LOSS train 0.12856585065187034 valid 0.18250512822061524
LOSS train 0.12856585065187034 valid 0.18266666861290629
LOSS train 0.12856585065187034 valid 0.18254957177628905
LOSS train 0.12856585065187034 valid 0.1824788422901419
LOSS train 0.12856585065187034 valid 0.18250079046026998
LOSS train 0.12856585065187034 valid 0.18228665193753263
LOSS train 0.12856585065187034 valid 0.18227448022613923
LOSS train 0.12856585065187034 valid 0.18247619135251184
LOSS train 0.12856585065187034 valid 0.1823148695397968
LOSS train 0.12856585065187034 valid 0.18253826150678312
LOSS train 0.12856585065187034 valid 0.18272111222880785
LOSS train 0.12856585065187034 valid 0.18278634323149312
LOSS train 0.12856585065187034 valid 0.18263098615698697
LOSS train 0.12856585065187034 valid 0.18278095994883703
LOSS train 0.12856585065187034 valid 0.18271453630539677
LOSS train 0.12856585065187034 valid 0.1827149275675356
LOSS train 0.12856585065187034 valid 0.18273157167434692
LOSS train 0.12856585065187034 valid 0.18256258192765284
LOSS train 0.12856585065187034 valid 0.18282328161691863
LOSS train 0.12856585065187034 valid 0.18276258849579355
LOSS train 0.12856585065187034 valid 0.18264311024053828
LOSS train 0.12856585065187034 valid 0.18269064817942826
LOSS train 0.12856585065187034 valid 0.18275733245536685
LOSS train 0.12856585065187034 valid 0.1825728830428439
LOSS train 0.12856585065187034 valid 0.18282958657242532
LOSS train 0.12856585065187034 valid 0.18283942698511838
LOSS train 0.12856585065187034 valid 0.18267577628676707
LOSS train 0.12856585065187034 valid 0.1828382704335611
LOSS train 0.12856585065187034 valid 0.18292748711946358
LOSS train 0.12856585065187034 valid 0.18297317442785196
LOSS train 0.12856585065187034 valid 0.1830666068602692
LOSS train 0.12856585065187034 valid 0.18302775739498858
LOSS train 0.12856585065187034 valid 0.18304212340959033
LOSS train 0.12856585065187034 valid 0.18309663275207919
LOSS train 0.12856585065187034 valid 0.18329587000519482
LOSS train 0.12856585065187034 valid 0.18337816559470718
LOSS train 0.12856585065187034 valid 0.18337896206864604
LOSS train 0.12856585065187034 valid 0.1834805901969931
LOSS train 0.12856585065187034 valid 0.18380702303393798
LOSS train 0.12856585065187034 valid 0.18401822027487633
LOSS train 0.12856585065187034 valid 0.1840624236603723
LOSS train 0.12856585065187034 valid 0.1840374268726869
LOSS train 0.12856585065187034 valid 0.18397589195249736
LOSS train 0.12856585065187034 valid 0.18386539838374308
LOSS train 0.12856585065187034 valid 0.18371782693073904
LOSS train 0.12856585065187034 valid 0.18370349977606087
LOSS train 0.12856585065187034 valid 0.18369229042104313
LOSS train 0.12856585065187034 valid 0.18362238950152415
LOSS train 0.12856585065187034 valid 0.18331183530126058
LOSS train 0.12856585065187034 valid 0.183244345940044
LOSS train 0.12856585065187034 valid 0.18327002594588507
LOSS train 0.12856585065187034 valid 0.18331253094631328
LOSS train 0.12856585065187034 valid 0.1833310104854457
LOSS train 0.12856585065187034 valid 0.1832917206482605
LOSS train 0.12856585065187034 valid 0.183269909893473
LOSS train 0.12856585065187034 valid 0.18327773767152872
LOSS train 0.12856585065187034 valid 0.18331184870210188
LOSS train 0.12856585065187034 valid 0.18314365090168627
LOSS train 0.12856585065187034 valid 0.18313213641921136
LOSS train 0.12856585065187034 valid 0.1831367408456249
LOSS train 0.12856585065187034 valid 0.18321489193001572
LOSS train 0.12856585065187034 valid 0.18329489316980718
LOSS train 0.12856585065187034 valid 0.1832112436358993
LOSS train 0.12856585065187034 valid 0.18329992940530232
LOSS train 0.12856585065187034 valid 0.18333600381476767
LOSS train 0.12856585065187034 valid 0.18336186366137056
LOSS train 0.12856585065187034 valid 0.18346254676580429
LOSS train 0.12856585065187034 valid 0.18339729353637
LOSS train 0.12856585065187034 valid 0.18334626218933142
LOSS train 0.12856585065187034 valid 0.18339192513192054
LOSS train 0.12856585065187034 valid 0.18338289239296787
LOSS train 0.12856585065187034 valid 0.1833290045378638
LOSS train 0.12856585065187034 valid 0.18336083705908332
LOSS train 0.12856585065187034 valid 0.18333992774595267
LOSS train 0.12856585065187034 valid 0.18323820524595
LOSS train 0.12856585065187034 valid 0.18323108148806302
LOSS train 0.12856585065187034 valid 0.1832817701082076
LOSS train 0.12856585065187034 valid 0.18327403638715528
LOSS train 0.12856585065187034 valid 0.18325813807165012
LOSS train 0.12856585065187034 valid 0.1834465168630734
LOSS train 0.12856585065187034 valid 0.1834566648693601
LOSS train 0.12856585065187034 valid 0.18337589876046256
LOSS train 0.12856585065187034 valid 0.18335708678711818
LOSS train 0.12856585065187034 valid 0.18342391244056472
LOSS train 0.12856585065187034 valid 0.18346403412661463
LOSS train 0.12856585065187034 valid 0.1836330239488787
LOSS train 0.12856585065187034 valid 0.18355816951952875
LOSS train 0.12856585065187034 valid 0.18373534756471807
LOSS train 0.12856585065187034 valid 0.18372926088223546
LOSS train 0.12856585065187034 valid 0.183676241490494
LOSS train 0.12856585065187034 valid 0.18378422082758245
LOSS train 0.12856585065187034 valid 0.18378434671805455
LOSS train 0.12856585065187034 valid 0.18394679132780414
LOSS train 0.12856585065187034 valid 0.1839973524167268
LOSS train 0.12856585065187034 valid 0.1839493209178128
LOSS train 0.12856585065187034 valid 0.18400452646078672
LOSS train 0.12856585065187034 valid 0.18399874595078555
LOSS train 0.12856585065187034 valid 0.18388374298360774
LOSS train 0.12856585065187034 valid 0.1837475988789495
LOSS train 0.12856585065187034 valid 0.18380908605393706
LOSS train 0.12856585065187034 valid 0.18388609262462147
LOSS train 0.12856585065187034 valid 0.18384551632760177
LOSS train 0.12856585065187034 valid 0.18392075620414244
LOSS train 0.12856585065187034 valid 0.18391638529937415
LOSS train 0.12856585065187034 valid 0.1839031392829658
LOSS train 0.12856585065187034 valid 0.18392561846426456
LOSS train 0.12856585065187034 valid 0.1838657973005491
LOSS train 0.12856585065187034 valid 0.18371327068449114
LOSS train 0.12856585065187034 valid 0.1837181071178955
LOSS train 0.12856585065187034 valid 0.18376047274982965
LOSS train 0.12856585065187034 valid 0.18400654755532742
LOSS train 0.12856585065187034 valid 0.18412604798441348
LOSS train 0.12856585065187034 valid 0.1842157959593514
LOSS train 0.12856585065187034 valid 0.18407343190582068
LOSS train 0.12856585065187034 valid 0.184011684880517
LOSS train 0.12856585065187034 valid 0.18399711589417006
LOSS train 0.12856585065187034 valid 0.18392879247665406
LOSS train 0.12856585065187034 valid 0.18384702087637367
LOSS train 0.12856585065187034 valid 0.1838404428721829
LOSS train 0.12856585065187034 valid 0.18384628441110887
LOSS train 0.12856585065187034 valid 0.18386305355082797
LOSS train 0.12856585065187034 valid 0.18392086213743183
LOSS train 0.12856585065187034 valid 0.18391446275322626
LOSS train 0.12856585065187034 valid 0.18394917216287607
LOSS train 0.12856585065187034 valid 0.18384310340914647
LOSS train 0.12856585065187034 valid 0.18384717471254236
LOSS train 0.12856585065187034 valid 0.18380563693741958
LOSS train 0.12856585065187034 valid 0.18375355331356175
LOSS train 0.12856585065187034 valid 0.18380524057545056
LOSS train 0.12856585065187034 valid 0.1838001123226707
LOSS train 0.12856585065187034 valid 0.18379187968733546
LOSS train 0.12856585065187034 valid 0.18384751938793759
LOSS train 0.12856585065187034 valid 0.18387338421383842
LOSS train 0.12856585065187034 valid 0.18378304326729164
LOSS train 0.12856585065187034 valid 0.18382955678617177
LOSS train 0.12856585065187034 valid 0.18390949910410698
EPOCH 25:
  batch 1 loss: 0.11488150805234909
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.11586572602391243
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.11222492655118306
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.12266404554247856
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.12597562074661256
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.127015749613444
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.12171060911246709
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.12329190596938133
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.12238216648499171
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.12137148156762123
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.1194935217499733
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.11890073493123055
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.1196991319839771
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.11961445159145764
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.11823115994532903
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.12033006874844432
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.11921242054770975
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.12204541431532966
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.12140671595146782
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.1209606785327196
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.12138457560823077
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.12209265983917496
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.12210772899181946
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.12364931994428237
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.12291942000389099
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.12320400315981644
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.12363772315007669
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.12335385914359774
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.12280492844252751
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.12334986726442973
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.1241265304626957
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.12449226528406143
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.12374489438353163
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.12430891635663369
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.12592457511595317
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.12523940516014895
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.12500737406112053
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.12467964032762929
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.12510003837255332
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.12514128014445305
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.1253033251297183
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.12550362980081922
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.1265940842933433
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.12612862038341435
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.1262972205877304
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.125847266258105
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.12560329348482985
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.12570396127800146
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.12566512838310126
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.12574818715453148
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.12535583300917758
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.12551385078292626
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.12518947937016217
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.1250215254172131
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.1253786340355873
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.12561344395258597
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.1256830926265633
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.12523299853863387
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.12562970097287227
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.1254880069444577
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.1255682774010252
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.12540522325904138
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.12547319973745044
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.12533252022694796
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.1254774495959282
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.12523997484734564
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.12505767105230645
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.12531868193079443
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.12562504972236743
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.12590591545615878
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.12625935904576746
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.12644539504415458
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.12652782360984854
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.12649239237244064
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.12658701241016387
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.12645107272424197
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.12642549413752247
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.12607574816315603
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.12643880076423475
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.12669087694957853
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.12673854211598268
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.126385953095628
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.12620651084615522
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.1265941234748988
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.12677668447003645
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.12704980815219324
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.12734496482144828
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.1277950300242413
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.12774028302578444
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.1281824557317628
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.12835707785664024
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.12831184834889744
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.1284061026188635
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.12831562234366195
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.12875936878354927
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.128822126891464
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.12865883672667533
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.1284989505550083
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.12879431586373935
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.1286959894746542
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.12845652771763283
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.12830314427322032
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.12807453727548562
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.12816558126360178
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.12773333752439137
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.12778952935675406
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.12801838568716406
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.12775946429206264
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.12792969672778332
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.1281604483046315
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.12790286829611203
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.1278936952751662
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.12760501374713087
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.12800230569483942
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.12780622736267422
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.127674898196911
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.12803718116548327
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.1280410470346273
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.12793808161210612
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.1279369122038285
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.1278977395335505
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.1279514215764452
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.12809889902913474
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.12807068613267714
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.1280834333896637
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.12814160791181384
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.1281600260124432
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.12794145435327664
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.1278475352613501
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.12782264861922998
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.1277088214650409
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.12769468196413733
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.12804490731174784
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.12813316786022327
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.1279191227974715
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.12786560160491398
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.12803123788024387
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.12801046739669814
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.12822306879775988
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.12823401960943426
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.1282529389710291
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.12807836929257488
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.1281570709877081
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.12828142402900589
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.1281347824581738
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.1282222818021905
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.12824154163704438
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.12820525477464134
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.12860658834844627
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.1286532375216484
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.12849872090563869
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.12861500543199086
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.12856630333303626
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.12823486134603426
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.12826375211438826
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.12826445488593516
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.1283389441906267
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.1283302048716364
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.12841628360673316
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.12849709782749413
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.1286169645023642
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.12857037359549675
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.1285823245355688
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.12875479432504353
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.12890838836178636
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.1287858356463622
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.12864953471336535
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.12857712765357324
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.1283964781013466
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.1286856670590008
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.128738464929207
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.12870285893942035
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.1284967893465406
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.12842022965865574
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.12854122310876848
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.12835308600386436
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.12814055096968419
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.12786279459682742
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.12805291469203695
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.12788058093024623
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.12793767044722046
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.12790793675806497
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.12783263986065088
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.12777219797768022
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.12777322607265937
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.12771500370675518
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.12780673797914688
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.12773366014849633
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.12769015436923062
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.12774267961320124
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.1277633087407232
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.12767296098172665
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.12773864650664551
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.12750226190102468
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.1275104536459996
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.12765879929065704
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.12776561671404668
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.1279635126091013
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.12790618410062551
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.12806526251137257
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.12807353456222004
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.1281500613630408
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.1281774013940924
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.12825341529998124
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.12815961205377813
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.12800693164751367
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.12799366063253892
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.1278320674497921
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.12791754344576284
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.12803821702088627
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.12802280997636759
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.12805666974833552
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.128116545997595
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.12789051415764283
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.12798993566701578
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.12790011041970165
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.12787630989254894
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.1279426269028165
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.1280010371855949
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.12799858674407005
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.12802374099983888
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.12803836603153934
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.12799872033665533
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.12805786885188095
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.12796365135245852
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.1279829102939209
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.12810757519915242
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.12793858208807937
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.1278964680290118
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.12811147567370665
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.1283464143093014
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.12824797636732974
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.12830418092498452
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.12829366759357289
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.12828830303029812
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.12832411706952726
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.12828150366429036
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.12831793193306243
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.12833046582702812
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.12821585092072685
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.12834987727561928
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.12828422584070648
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.1283500176153065
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.12841147752326043
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.1283905666093437
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.12845993623500918
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.12845998743043738
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.12851807739465468
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.12857885700631813
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.12855039316415787
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.12874818619978856
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.12878195551179705
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.1287908316718731
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.12877965947305123
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.1287568881815555
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.12876704370137304
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.12869626237260692
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.1286423340901848
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.12873184358751452
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.12880272739208662
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.12869286771249955
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.1286257250902762
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.12858367603654644
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.1284605927596038
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.12849096411241676
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.12850543797800415
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.12847454371523767
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.12848089407406635
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.12840529791595323
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.12842842177660377
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.12844145289756276
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.1283644296174102
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.12847681095202765
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.1283535216085232
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.1282344635508277
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.1284450105242971
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.12839605836769305
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.1284176386999998
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.12836160225039314
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.12836622734154973
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.12832678000697886
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.12827849596844498
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.12827952830088013
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.12824764832014768
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.12838698045203562
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.1285122111439705
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.1284271264481212
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.1283887480903003
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.1283219000099027
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.12835745151186811
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.12838501467839958
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.1284980836949528
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.12869504895124825
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.1287412948298211
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.12870533347634946
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.12872370939097694
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.12871621631913716
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.12866406280222356
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.12860908329387175
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.12856952312091988
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.12858600540711634
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.12858223858357265
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.1285690760405937
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.12853363026423673
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.12849689712289905
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.12848398464372734
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.12852193308381382
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.12848356131512623
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.12844853095927286
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.12836300286073837
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.12843396308625243
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.12838491563422558
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.128316046902166
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.1282979765088315
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.12830950132911167
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.12844533282273177
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.12843463499369306
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.12843151744337952
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.12836189722192698
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.12845482714474202
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.1284193155178771
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.12835764732234967
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.12836786924685487
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.12829003124325364
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.1283063911474668
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.12820914067739358
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.12811423817632395
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.12812508853924712
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.12817489097636522
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.12824308702891524
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.1282814295225273
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.12827108854270844
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.12819462636480103
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.128206040397911
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.12826344817876817
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.12829022670519494
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.1282773006829027
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.12826618488282848
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.1281922497624493
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.12818626768010505
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.12820227970737866
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.12845012508550582
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.1284974095989942
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.12854053866291462
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.12853662071452623
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.12850989496087753
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.12850219758511963
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.12849727410009537
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.12843728193580933
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.12834569515926497
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.1283455051460497
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.12842891045676713
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.12851387982074666
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.12856359693542713
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.12856814237967343
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.12848426110707642
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.12856912189254574
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.1286207449377915
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.1285373331319987
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.12848440023760002
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.12846760175234723
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.12857111312901776
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.12853296930303915
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.12848897376558283
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.12852248435151087
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.1285142340376729
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.12859861024881253
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.12868842478517606
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.12865306848917551
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.12857178401302646
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.1286316463809129
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.12861513302371066
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.12863362095151445
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.12871225433234862
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.12874561643600463
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.12874151718743304
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.1287247345681532
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.12880103526607392
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.12872961039672112
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.12871683888921612
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.12871316566163787
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.12864068901429626
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.12871771461425188
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.1287249944677266
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.1286302980470967
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.1285887713220762
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.12861522364138941
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.12854966838104823
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.12854494110401002
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.12846947765121094
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.12845003400998348
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.1284548477364742
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.12856427614242977
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.12859126269287868
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.12859664806459523
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.1285371206926577
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.12843459015078748
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.12846016887594108
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.12843885976718483
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.1283997093886137
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.12841343002723637
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.12838257854776597
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.12833442254264657
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.12837124584556217
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.12840576541644555
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.12836076911960917
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.12836921517107938
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.12841092344081284
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.1283701467339159
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.12836772341553757
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.128358926542484
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.1284065233561599
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.12842414620136233
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.12848889496591356
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.12855205201959036
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.12863040543519533
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.1286271171246787
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.12860174339852834
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.128651275466194
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.12861129657498427
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.12865703298119638
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.12868367783478085
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.12860364008950284
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.1285871896576488
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.1285699866273824
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.1284748868483333
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.12848749226373588
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.12844704892312256
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.12837682709549414
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.12830484344862228
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.12835770290509177
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.12831121738310214
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.12832416370338987
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.12838117204915545
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.12834323094836597
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.1284410678567963
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.1284938402627371
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.12844609761755216
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.12833803610405237
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.12831690934571352
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.12833837005827162
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.1282991870597207
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.12827542605841402
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.1282611715666077
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.12826554150728697
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.1282738148664947
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.12825016687380386
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.12830354033836297
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.12829841238618694
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.128277598089642
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.12824910485955934
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.1283050186734284
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.12823903820504154
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.1282340826442063
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.12817927397541948
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.12818244114322097
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.12818214432611508
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.12819355760404116
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.12820914915249498
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.12829921279588472
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.128271116529654
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.12826592141351142
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.12819622157716853
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.12821198636987086
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.12821423811938173
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.12822394741323373
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.1282389708006918
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.12832927477792797
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.1283779523964884
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.12842827651728975
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.12846418509690877
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.1285704850077124
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.1285704850077124 valid 0.22369571030139923
LOSS train 0.1285704850077124 valid 0.19397757202386856
LOSS train 0.1285704850077124 valid 0.18610181907812753
LOSS train 0.1285704850077124 valid 0.1765889674425125
LOSS train 0.1285704850077124 valid 0.17216619551181794
LOSS train 0.1285704850077124 valid 0.18242298314968744
LOSS train 0.1285704850077124 valid 0.19322864711284637
LOSS train 0.1285704850077124 valid 0.19137964770197868
LOSS train 0.1285704850077124 valid 0.1907398137781355
LOSS train 0.1285704850077124 valid 0.19059364050626754
LOSS train 0.1285704850077124 valid 0.1891227662563324
LOSS train 0.1285704850077124 valid 0.18984835470716158
LOSS train 0.1285704850077124 valid 0.18928210666546455
LOSS train 0.1285704850077124 valid 0.18865225464105606
LOSS train 0.1285704850077124 valid 0.1862237294514974
LOSS train 0.1285704850077124 valid 0.18619778379797935
LOSS train 0.1285704850077124 valid 0.18692906025577993
LOSS train 0.1285704850077124 valid 0.1858311171332995
LOSS train 0.1285704850077124 valid 0.18829564514913058
LOSS train 0.1285704850077124 valid 0.18805721029639244
LOSS train 0.1285704850077124 valid 0.1870611813806352
LOSS train 0.1285704850077124 valid 0.185624936087565
LOSS train 0.1285704850077124 valid 0.18538690002068228
LOSS train 0.1285704850077124 valid 0.18557309359312057
LOSS train 0.1285704850077124 valid 0.18430206298828125
LOSS train 0.1285704850077124 valid 0.18419632143699205
LOSS train 0.1285704850077124 valid 0.18450968629784054
LOSS train 0.1285704850077124 valid 0.1841246671974659
LOSS train 0.1285704850077124 valid 0.18360740408815188
LOSS train 0.1285704850077124 valid 0.184140378733476
LOSS train 0.1285704850077124 valid 0.184973394678485
LOSS train 0.1285704850077124 valid 0.18414889043197036
LOSS train 0.1285704850077124 valid 0.18439707656701407
LOSS train 0.1285704850077124 valid 0.18383259194738724
LOSS train 0.1285704850077124 valid 0.1854028386729104
LOSS train 0.1285704850077124 valid 0.1851072824663586
LOSS train 0.1285704850077124 valid 0.18587139812675682
LOSS train 0.1285704850077124 valid 0.18621545324200078
LOSS train 0.1285704850077124 valid 0.18590535338108355
LOSS train 0.1285704850077124 valid 0.1856069505214691
LOSS train 0.1285704850077124 valid 0.18617398382686987
LOSS train 0.1285704850077124 valid 0.18634034054619925
LOSS train 0.1285704850077124 valid 0.18636972578459007
LOSS train 0.1285704850077124 valid 0.18688848072832281
LOSS train 0.1285704850077124 valid 0.18696192966567146
LOSS train 0.1285704850077124 valid 0.18736916424139685
LOSS train 0.1285704850077124 valid 0.18801933431879003
LOSS train 0.1285704850077124 valid 0.18785868491977453
LOSS train 0.1285704850077124 valid 0.18848446710985534
LOSS train 0.1285704850077124 valid 0.18799316614866257
LOSS train 0.1285704850077124 valid 0.1881845689871732
LOSS train 0.1285704850077124 valid 0.18762009046398676
LOSS train 0.1285704850077124 valid 0.1881463794775729
LOSS train 0.1285704850077124 valid 0.1882701634808823
LOSS train 0.1285704850077124 valid 0.18819476962089537
LOSS train 0.1285704850077124 valid 0.18749801149325712
LOSS train 0.1285704850077124 valid 0.1874217377942905
LOSS train 0.1285704850077124 valid 0.18729469488407002
LOSS train 0.1285704850077124 valid 0.18767392003940322
LOSS train 0.1285704850077124 valid 0.18744942049185434
LOSS train 0.1285704850077124 valid 0.187018484610026
LOSS train 0.1285704850077124 valid 0.18756021102589945
LOSS train 0.1285704850077124 valid 0.18690687914689383
LOSS train 0.1285704850077124 valid 0.18781178048811853
LOSS train 0.1285704850077124 valid 0.18803654886209048
LOSS train 0.1285704850077124 valid 0.1877282512458888
LOSS train 0.1285704850077124 valid 0.1871403353427773
LOSS train 0.1285704850077124 valid 0.18709570936420383
LOSS train 0.1285704850077124 valid 0.1867392125768938
LOSS train 0.1285704850077124 valid 0.18734380645411355
LOSS train 0.1285704850077124 valid 0.18697317091512008
LOSS train 0.1285704850077124 valid 0.1871583422438966
LOSS train 0.1285704850077124 valid 0.18714729216817308
LOSS train 0.1285704850077124 valid 0.18714668037923607
LOSS train 0.1285704850077124 valid 0.18752841889858246
LOSS train 0.1285704850077124 valid 0.18795803542199888
LOSS train 0.1285704850077124 valid 0.18779752393821617
LOSS train 0.1285704850077124 valid 0.18756843492006645
LOSS train 0.1285704850077124 valid 0.18723443791836122
LOSS train 0.1285704850077124 valid 0.18656952846795322
LOSS train 0.1285704850077124 valid 0.1862239000605948
LOSS train 0.1285704850077124 valid 0.18669424533117107
LOSS train 0.1285704850077124 valid 0.18650871012584272
LOSS train 0.1285704850077124 valid 0.1865272702915328
LOSS train 0.1285704850077124 valid 0.18626227361314437
LOSS train 0.1285704850077124 valid 0.18605269803557284
LOSS train 0.1285704850077124 valid 0.18583596472082467
LOSS train 0.1285704850077124 valid 0.18554030037061733
LOSS train 0.1285704850077124 valid 0.18606443301345524
LOSS train 0.1285704850077124 valid 0.18604989730649524
LOSS train 0.1285704850077124 valid 0.18614504510884758
LOSS train 0.1285704850077124 valid 0.18608307141972624
LOSS train 0.1285704850077124 valid 0.18585597026732661
LOSS train 0.1285704850077124 valid 0.1860531803141249
LOSS train 0.1285704850077124 valid 0.18598468727187106
LOSS train 0.1285704850077124 valid 0.18604633677750826
LOSS train 0.1285704850077124 valid 0.18588036314113854
LOSS train 0.1285704850077124 valid 0.18612123630484756
LOSS train 0.1285704850077124 valid 0.1861733551880326
LOSS train 0.1285704850077124 valid 0.1862742157280445
LOSS train 0.1285704850077124 valid 0.1864725127078519
LOSS train 0.1285704850077124 valid 0.18670897419546165
LOSS train 0.1285704850077124 valid 0.18636455582183542
LOSS train 0.1285704850077124 valid 0.18651951390963334
LOSS train 0.1285704850077124 valid 0.18661836073512122
LOSS train 0.1285704850077124 valid 0.18708622680520112
LOSS train 0.1285704850077124 valid 0.18698085022863942
LOSS train 0.1285704850077124 valid 0.18714281364723487
LOSS train 0.1285704850077124 valid 0.18755324417297994
LOSS train 0.1285704850077124 valid 0.18759837137027222
LOSS train 0.1285704850077124 valid 0.1875556625492938
LOSS train 0.1285704850077124 valid 0.18736588702137982
LOSS train 0.1285704850077124 valid 0.1874203906122562
LOSS train 0.1285704850077124 valid 0.18774915068295964
LOSS train 0.1285704850077124 valid 0.18796364133772642
LOSS train 0.1285704850077124 valid 0.1881874673839273
LOSS train 0.1285704850077124 valid 0.18811504886700556
LOSS train 0.1285704850077124 valid 0.18769286231974425
LOSS train 0.1285704850077124 valid 0.1873379272072255
LOSS train 0.1285704850077124 valid 0.1869913173218568
LOSS train 0.1285704850077124 valid 0.18681958071456467
LOSS train 0.1285704850077124 valid 0.1868698723980638
LOSS train 0.1285704850077124 valid 0.18665383925767448
LOSS train 0.1285704850077124 valid 0.18697615712881088
LOSS train 0.1285704850077124 valid 0.18680291891098022
LOSS train 0.1285704850077124 valid 0.18716076276605093
LOSS train 0.1285704850077124 valid 0.1870035469297349
LOSS train 0.1285704850077124 valid 0.1870533882174641
LOSS train 0.1285704850077124 valid 0.1871936208055925
LOSS train 0.1285704850077124 valid 0.18679015728143544
LOSS train 0.1285704850077124 valid 0.1864781678860424
LOSS train 0.1285704850077124 valid 0.18615552749146114
LOSS train 0.1285704850077124 valid 0.18602939465440305
LOSS train 0.1285704850077124 valid 0.1861519729023549
LOSS train 0.1285704850077124 valid 0.18602500524785784
LOSS train 0.1285704850077124 valid 0.1860403983908541
LOSS train 0.1285704850077124 valid 0.18570291430410676
LOSS train 0.1285704850077124 valid 0.18562362278285233
LOSS train 0.1285704850077124 valid 0.18555722766214136
LOSS train 0.1285704850077124 valid 0.18564665551696505
LOSS train 0.1285704850077124 valid 0.18550262700581382
LOSS train 0.1285704850077124 valid 0.1855193671416229
LOSS train 0.1285704850077124 valid 0.18539636879117338
LOSS train 0.1285704850077124 valid 0.1853900487638182
LOSS train 0.1285704850077124 valid 0.1852648029039646
LOSS train 0.1285704850077124 valid 0.1854759227127245
LOSS train 0.1285704850077124 valid 0.18541436323097774
LOSS train 0.1285704850077124 valid 0.18634431154744044
LOSS train 0.1285704850077124 valid 0.1865145246054502
LOSS train 0.1285704850077124 valid 0.18647307972113292
LOSS train 0.1285704850077124 valid 0.1867213011380063
LOSS train 0.1285704850077124 valid 0.1863748938041298
LOSS train 0.1285704850077124 valid 0.18632000363340565
LOSS train 0.1285704850077124 valid 0.18617964671416717
LOSS train 0.1285704850077124 valid 0.18605430020440009
LOSS train 0.1285704850077124 valid 0.18605862328639397
LOSS train 0.1285704850077124 valid 0.1861544334964388
LOSS train 0.1285704850077124 valid 0.18613576134548912
LOSS train 0.1285704850077124 valid 0.1862331506966045
LOSS train 0.1285704850077124 valid 0.18616016125306487
LOSS train 0.1285704850077124 valid 0.18600369823275145
LOSS train 0.1285704850077124 valid 0.18588049590219685
LOSS train 0.1285704850077124 valid 0.18559834105105488
LOSS train 0.1285704850077124 valid 0.18539995809153811
LOSS train 0.1285704850077124 valid 0.1852349857489268
LOSS train 0.1285704850077124 valid 0.18532501286770925
LOSS train 0.1285704850077124 valid 0.1856775736202023
LOSS train 0.1285704850077124 valid 0.18558989323320843
LOSS train 0.1285704850077124 valid 0.18575922395350666
LOSS train 0.1285704850077124 valid 0.1857640595997081
LOSS train 0.1285704850077124 valid 0.18577533637919622
LOSS train 0.1285704850077124 valid 0.18563937586407328
LOSS train 0.1285704850077124 valid 0.18561689495351275
LOSS train 0.1285704850077124 valid 0.18557091571133713
LOSS train 0.1285704850077124 valid 0.18523978063038418
LOSS train 0.1285704850077124 valid 0.18519882222806866
LOSS train 0.1285704850077124 valid 0.18531192662352222
LOSS train 0.1285704850077124 valid 0.18557975767703538
LOSS train 0.1285704850077124 valid 0.1854798921992659
LOSS train 0.1285704850077124 valid 0.1853986431327131
LOSS train 0.1285704850077124 valid 0.1855429751767638
LOSS train 0.1285704850077124 valid 0.1853892009828117
LOSS train 0.1285704850077124 valid 0.1854552402685249
LOSS train 0.1285704850077124 valid 0.18532335490960142
LOSS train 0.1285704850077124 valid 0.1851833499766685
LOSS train 0.1285704850077124 valid 0.18523401166162184
LOSS train 0.1285704850077124 valid 0.18508585594235893
LOSS train 0.1285704850077124 valid 0.18507853744828956
LOSS train 0.1285704850077124 valid 0.18494236035637124
LOSS train 0.1285704850077124 valid 0.18502634512750726
LOSS train 0.1285704850077124 valid 0.18489366652765823
LOSS train 0.1285704850077124 valid 0.18493982955502966
LOSS train 0.1285704850077124 valid 0.18465410010802313
LOSS train 0.1285704850077124 valid 0.1845823256778963
LOSS train 0.1285704850077124 valid 0.18437311481206844
LOSS train 0.1285704850077124 valid 0.18433531122852345
LOSS train 0.1285704850077124 valid 0.18454296319617838
LOSS train 0.1285704850077124 valid 0.18449059076080418
LOSS train 0.1285704850077124 valid 0.1846334730560456
LOSS train 0.1285704850077124 valid 0.1844438711553812
LOSS train 0.1285704850077124 valid 0.1842607892123028
LOSS train 0.1285704850077124 valid 0.18419886594361598
LOSS train 0.1285704850077124 valid 0.18419602850975075
LOSS train 0.1285704850077124 valid 0.18440092486493728
LOSS train 0.1285704850077124 valid 0.1841553624810242
LOSS train 0.1285704850077124 valid 0.18417473650962404
LOSS train 0.1285704850077124 valid 0.18410534048138033
LOSS train 0.1285704850077124 valid 0.1839410625398159
LOSS train 0.1285704850077124 valid 0.18395257701999262
LOSS train 0.1285704850077124 valid 0.18387527827705655
LOSS train 0.1285704850077124 valid 0.18387055665395835
LOSS train 0.1285704850077124 valid 0.18383955737611032
LOSS train 0.1285704850077124 valid 0.18380129386281743
LOSS train 0.1285704850077124 valid 0.18383644173078448
LOSS train 0.1285704850077124 valid 0.18371143756910813
LOSS train 0.1285704850077124 valid 0.1835719816110752
LOSS train 0.1285704850077124 valid 0.1834942530239782
LOSS train 0.1285704850077124 valid 0.18353059074474037
LOSS train 0.1285704850077124 valid 0.18363768386242052
LOSS train 0.1285704850077124 valid 0.183576664802703
LOSS train 0.1285704850077124 valid 0.1834972603288711
LOSS train 0.1285704850077124 valid 0.18353257200739406
LOSS train 0.1285704850077124 valid 0.1837388752153636
LOSS train 0.1285704850077124 valid 0.1838321828815554
LOSS train 0.1285704850077124 valid 0.1840056542555491
LOSS train 0.1285704850077124 valid 0.1842688711079876
LOSS train 0.1285704850077124 valid 0.18433519068793577
LOSS train 0.1285704850077124 valid 0.18439976936369612
LOSS train 0.1285704850077124 valid 0.18440097290615848
LOSS train 0.1285704850077124 valid 0.18444084265957708
LOSS train 0.1285704850077124 valid 0.18458486990237133
LOSS train 0.1285704850077124 valid 0.18460132252296496
LOSS train 0.1285704850077124 valid 0.18470865564796546
LOSS train 0.1285704850077124 valid 0.18476590539655116
LOSS train 0.1285704850077124 valid 0.1849292737372378
LOSS train 0.1285704850077124 valid 0.1848121683223773
LOSS train 0.1285704850077124 valid 0.18474450273604331
LOSS train 0.1285704850077124 valid 0.18476569671340348
LOSS train 0.1285704850077124 valid 0.18454778880254993
LOSS train 0.1285704850077124 valid 0.18453550909956296
LOSS train 0.1285704850077124 valid 0.18473457851103234
LOSS train 0.1285704850077124 valid 0.18457339765611758
LOSS train 0.1285704850077124 valid 0.18479796189339562
LOSS train 0.1285704850077124 valid 0.18498333445826515
LOSS train 0.1285704850077124 valid 0.18504797518253327
LOSS train 0.1285704850077124 valid 0.184889533049692
LOSS train 0.1285704850077124 valid 0.18504151605401445
LOSS train 0.1285704850077124 valid 0.18497069053832563
LOSS train 0.1285704850077124 valid 0.18496956954519433
LOSS train 0.1285704850077124 valid 0.18498999494314194
LOSS train 0.1285704850077124 valid 0.18482261070929676
LOSS train 0.1285704850077124 valid 0.18508318864873477
LOSS train 0.1285704850077124 valid 0.18502183523573895
LOSS train 0.1285704850077124 valid 0.18490108542554962
LOSS train 0.1285704850077124 valid 0.18495272158407697
LOSS train 0.1285704850077124 valid 0.18502118246397004
LOSS train 0.1285704850077124 valid 0.18483596170458813
LOSS train 0.1285704850077124 valid 0.18508770542089328
LOSS train 0.1285704850077124 valid 0.18509923796165864
LOSS train 0.1285704850077124 valid 0.18493496443216617
LOSS train 0.1285704850077124 valid 0.18509763781823418
LOSS train 0.1285704850077124 valid 0.18518625978977626
LOSS train 0.1285704850077124 valid 0.1852323835000339
LOSS train 0.1285704850077124 valid 0.1853244754514008
LOSS train 0.1285704850077124 valid 0.18528827490671626
LOSS train 0.1285704850077124 valid 0.185298503565609
LOSS train 0.1285704850077124 valid 0.18535194559936666
LOSS train 0.1285704850077124 valid 0.18555043837917384
LOSS train 0.1285704850077124 valid 0.18563209706744296
LOSS train 0.1285704850077124 valid 0.18563359026555662
LOSS train 0.1285704850077124 valid 0.18573247754045957
LOSS train 0.1285704850077124 valid 0.18606070335954428
LOSS train 0.1285704850077124 valid 0.18627230435500652
LOSS train 0.1285704850077124 valid 0.1863175938381766
LOSS train 0.1285704850077124 valid 0.1862920787117698
LOSS train 0.1285704850077124 valid 0.1862313606492851
LOSS train 0.1285704850077124 valid 0.18612420010222425
LOSS train 0.1285704850077124 valid 0.18597510258714073
LOSS train 0.1285704850077124 valid 0.1859630377275542
LOSS train 0.1285704850077124 valid 0.18595146558114461
LOSS train 0.1285704850077124 valid 0.1858814420224933
LOSS train 0.1285704850077124 valid 0.1855689419816572
LOSS train 0.1285704850077124 valid 0.1855057417398628
LOSS train 0.1285704850077124 valid 0.18553229064588816
LOSS train 0.1285704850077124 valid 0.18557726291188023
LOSS train 0.1285704850077124 valid 0.18559383361281215
LOSS train 0.1285704850077124 valid 0.18555324413012128
LOSS train 0.1285704850077124 valid 0.185529425740242
LOSS train 0.1285704850077124 valid 0.18553912758208477
LOSS train 0.1285704850077124 valid 0.18557617854455422
LOSS train 0.1285704850077124 valid 0.18540566371068923
LOSS train 0.1285704850077124 valid 0.18539217721721898
LOSS train 0.1285704850077124 valid 0.18539498878623845
LOSS train 0.1285704850077124 valid 0.18547274648737747
LOSS train 0.1285704850077124 valid 0.18555469189660023
LOSS train 0.1285704850077124 valid 0.18546747370950273
LOSS train 0.1285704850077124 valid 0.18555852866132652
LOSS train 0.1285704850077124 valid 0.1855934201770981
LOSS train 0.1285704850077124 valid 0.18562089753988195
LOSS train 0.1285704850077124 valid 0.18572169462839763
LOSS train 0.1285704850077124 valid 0.18565457118706052
LOSS train 0.1285704850077124 valid 0.1856042229576616
LOSS train 0.1285704850077124 valid 0.18564863252167654
LOSS train 0.1285704850077124 valid 0.18564047184037535
LOSS train 0.1285704850077124 valid 0.18558611635301933
LOSS train 0.1285704850077124 valid 0.18562016899094863
LOSS train 0.1285704850077124 valid 0.1855961601295378
LOSS train 0.1285704850077124 valid 0.1854915836608255
LOSS train 0.1285704850077124 valid 0.18548320499052892
LOSS train 0.1285704850077124 valid 0.18553454779809522
LOSS train 0.1285704850077124 valid 0.1855248182628699
LOSS train 0.1285704850077124 valid 0.18551055069726247
LOSS train 0.1285704850077124 valid 0.18570122594079272
LOSS train 0.1285704850077124 valid 0.1857140390736282
LOSS train 0.1285704850077124 valid 0.1856342181326851
LOSS train 0.1285704850077124 valid 0.18561626914181287
LOSS train 0.1285704850077124 valid 0.1856827038319705
LOSS train 0.1285704850077124 valid 0.185722568453108
LOSS train 0.1285704850077124 valid 0.1858930386906507
LOSS train 0.1285704850077124 valid 0.1858189580962062
LOSS train 0.1285704850077124 valid 0.1860006268243552
LOSS train 0.1285704850077124 valid 0.1859928122403459
LOSS train 0.1285704850077124 valid 0.18593820080668563
LOSS train 0.1285704850077124 valid 0.1860465399093098
LOSS train 0.1285704850077124 valid 0.18604585675092844
LOSS train 0.1285704850077124 valid 0.18620766045674225
LOSS train 0.1285704850077124 valid 0.18626110779765184
LOSS train 0.1285704850077124 valid 0.18621288548882414
LOSS train 0.1285704850077124 valid 0.18626830842357275
LOSS train 0.1285704850077124 valid 0.18626284545118157
LOSS train 0.1285704850077124 valid 0.18614728820828153
LOSS train 0.1285704850077124 valid 0.18600523619946227
LOSS train 0.1285704850077124 valid 0.1860685020506203
LOSS train 0.1285704850077124 valid 0.18614603226591728
LOSS train 0.1285704850077124 valid 0.18610653539202107
LOSS train 0.1285704850077124 valid 0.1861812233747471
LOSS train 0.1285704850077124 valid 0.18617722109092802
LOSS train 0.1285704850077124 valid 0.1861652290150964
LOSS train 0.1285704850077124 valid 0.1861862787718618
LOSS train 0.1285704850077124 valid 0.18612865244641025
LOSS train 0.1285704850077124 valid 0.1859719672423304
LOSS train 0.1285704850077124 valid 0.18597696791266838
LOSS train 0.1285704850077124 valid 0.1860204579705052
LOSS train 0.1285704850077124 valid 0.1862636207667894
LOSS train 0.1285704850077124 valid 0.18638302010038624
LOSS train 0.1285704850077124 valid 0.18647227475064337
LOSS train 0.1285704850077124 valid 0.18632950781917024
LOSS train 0.1285704850077124 valid 0.18626497343353843
LOSS train 0.1285704850077124 valid 0.18624955513928204
LOSS train 0.1285704850077124 valid 0.1861794461097036
LOSS train 0.1285704850077124 valid 0.18609728020989996
LOSS train 0.1285704850077124 valid 0.18609073292464018
LOSS train 0.1285704850077124 valid 0.1860959800461534
LOSS train 0.1285704850077124 valid 0.18611377700742354
LOSS train 0.1285704850077124 valid 0.1861722468910083
LOSS train 0.1285704850077124 valid 0.18616865679956554
LOSS train 0.1285704850077124 valid 0.1862057221202957
LOSS train 0.1285704850077124 valid 0.18609814083609502
LOSS train 0.1285704850077124 valid 0.1861034281406562
LOSS train 0.1285704850077124 valid 0.18606355177859465
LOSS train 0.1285704850077124 valid 0.18600899984136512
LOSS train 0.1285704850077124 valid 0.1860603153787924
LOSS train 0.1285704850077124 valid 0.18605315664747202
LOSS train 0.1285704850077124 valid 0.18604660075116944
LOSS train 0.1285704850077124 valid 0.18610116749593655
LOSS train 0.1285704850077124 valid 0.18612865627300543
LOSS train 0.1285704850077124 valid 0.18603682071376562
LOSS train 0.1285704850077124 valid 0.1860842757007998
LOSS train 0.1285704850077124 valid 0.1861644207139002
EPOCH 26:
  batch 1 loss: 0.11333326995372772
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.11629034206271172
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.11311395466327667
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.12175712361931801
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.12441837787628174
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.12672320504983267
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.12153423790420805
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.12336405273526907
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.12226796895265579
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.1218803770840168
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.11982648616487329
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.11870794370770454
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.1195640483727822
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.11978445148893765
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.11827622751394908
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.11998236179351807
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.11887134962222155
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.12179453422625859
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.12164238722700822
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.12100505381822586
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.12151033253896804
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.12200146168470383
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.12191128439229468
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.12386195082217455
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.12318040996789932
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.1234581691141312
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.12415513865373752
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.12381576427391597
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.1233093137371129
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.12394489844640096
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.12469973871784826
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.12492185086011887
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.12408473658742326
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.12483672863420318
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.12609173506498336
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.12537018499440616
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.12527825784038854
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.12483350971811696
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.12540530967406738
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.12552973181009291
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.12564741511170457
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.12575982014338175
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.1267928224663402
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.1262822843749415
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.1264632279674212
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.12594130619064622
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.12584907973700382
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.12596542838340005
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.1258553207224729
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.12595835074782372
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.12550776802441654
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.12560707431000012
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.12531350623324233
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.12520622765576397
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.12551689283414322
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.12559612574321882
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.12565595721989348
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.1251438288339253
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.12552725788900407
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.1253134749829769
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.1254956766230161
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.12543463226287596
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.12550286830417692
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.12528343498706818
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.12535285078562222
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.12515217359318878
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.12494909774456452
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.12513112616451347
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.1253779227103012
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.12570991782205446
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.1260563925328389
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.1263024712809258
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.12646419514123708
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.12647948001284856
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.12656703462203345
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.12643258303011717
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.12644774757035368
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.12617833749988142
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.12662816226859636
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.12692314693704249
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.12697899608332433
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.12665003047483722
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.12651026159165854
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.1269376292115166
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.1271022552953047
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.12737087298964345
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.12763489251849294
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.12812714295631106
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.12808747489130898
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.12844734920395745
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.12866021889251666
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.12857522880253586
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.12864747883812075
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.12857513795507716
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.12897934192105343
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.1290501200904449
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.12880472700620435
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.12863181143694993
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.12893925343800072
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.1288019683212042
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.12861017595128257
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.12844784360598116
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.12826524949768214
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.12836926688368505
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.12792985361246836
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.1279781974570931
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.12824781482743325
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.12799045112397936
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.1281396410334001
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.12836571471257643
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.12811666525698998
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.12808000263092773
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.12777640004601099
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.12814015267710938
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.12791399560544803
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.12778029044897393
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.12812073038429275
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.12810863333485895
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.1279771942175737
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.12798132331420978
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.12795991856943478
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.12805661882777683
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.12816937019427618
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.12810239129729808
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.12814376455545426
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.1281827206294688
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.12822399147617536
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.127999069460202
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.1279293883794038
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.12793646838802558
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.12778435334677005
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.1277393689661315
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.12805723807865516
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.12813032996743473
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.12789062129126655
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.12784210028236404
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.12803881561016514
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.12798376268018846
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.1281797776012112
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.12817691708249704
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.12821160526351727
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.12804158231322194
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.12807689773869682
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.12821160267210668
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.12809841838376276
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.12817118388332732
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.12820065508083422
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.12813093261541547
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.12848332084265332
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.12849852363268535
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.12837904755840238
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.12847766608588004
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.12844173400815018
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.12812466498512726
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.12815153882388144
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.12814186776104647
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.12820529515386386
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.1281976720105998
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.12832750590904704
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.12845059423707425
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.12855857670862483
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.12847921367228768
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.12851252854784573
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.12872986400091066
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.12887694397659014
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.1287690210773284
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.1286251293327994
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.12852247831012523
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.12832691132493274
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.12858571598635
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.12865502204288515
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.12862505681466224
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.1284185622554983
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.12832570178755398
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.1284357340846743
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.12823307446458124
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.1280296088780387
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.12773680557193381
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.12793415813972164
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.1277755399545034
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.1278441581278216
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.1278027828659985
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.12771246578198314
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.12764824801327093
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.1276842518029986
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.12760818853814115
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.12769993565936777
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.12761954788831956
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.12756867178533443
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.12763035854226665
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.12764456782353487
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.12754043249879032
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.1275758745988416
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.12732625656674817
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.12735983076003882
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.12750425414011188
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.12763641627155584
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.12777753810238357
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.12768966098077333
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.1278225139901042
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.12785312707596158
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.1279007357609744
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.12795650257999674
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.12800941473859198
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.12793897666582246
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.12780983861788964
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.12777640886496808
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.1276024804593852
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.12770822412517083
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.12781129454572995
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.12780682753188916
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.12782228572891569
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.12790104204640143
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.12767627567192105
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.12777750862892284
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.12767835885838227
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.12767295745385956
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.12776084264757437
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.12781110446747035
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.12782008708877998
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.1278195147870353
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.12785206480069203
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.12782098496574992
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.1278889544773847
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.12779712253146702
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.12781543251687447
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.12792533833024786
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.12775947699290618
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.1276805458110493
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.1279206229292828
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.12819023933503534
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.12810908810331903
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.12816722505594016
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.1281597130955794
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.1281508833804029
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.12817333209312567
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.1281178593006818
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.12812711674125254
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.1281368214964368
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.12803102334340413
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.12813937942269432
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.12809282067147168
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.12815576178546795
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.128179276087245
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.12817448833767248
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.12822349684509804
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.12822706514765858
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.12831620379321038
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.12836217192042784
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.1283269304037094
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.12854177561176724
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.12856822679676708
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.12858091748279074
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.12855948575251686
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.12852465016584771
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.12854490053723566
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.12849149592075831
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.12842715398747792
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.12852573596142433
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.12859253900555465
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.12847876440291203
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.12840558124976303
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.1283716125406693
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.12825899032142127
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.12827103590627886
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.12829747036995745
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.12828069649050744
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.12826146702490634
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.12818047185480372
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.12821714339984788
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.12821185140702118
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.12816333833753185
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.1283025834095347
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.12817514940661234
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.12806921983307057
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.1282676958033572
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.1282453174410314
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.12826635886868126
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.12823370590432143
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.1282458954091583
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.12821034670830622
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.1281332123416958
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.1281236659985549
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.12808051482367683
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.1282016379007122
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.1282892499405604
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.1282157998415236
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.12817602669302788
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.1281178622767587
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.12814128961542556
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.12816830540133506
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.12827785336093545
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.12847073185464222
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.12849758385496884
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.12845355805199024
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.12848516601465038
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.12847848103604334
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.12842094270674975
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.12837414981728812
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.12833912154038749
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.12834624035215852
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.12835147981809464
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.12835143649145322
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.12832507868542484
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.12829249132363524
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.12828636839011914
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.12831025604607615
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.12826280484532382
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.12822108949658167
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.1281272968457591
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.12820113055000734
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.1281671847861547
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.12810816418248624
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.12808920632881723
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.12810629847503843
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.12824181717219232
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.12823077220740003
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.12822172174569946
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.12814615781310956
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.1282479126704857
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.12820628638208098
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.12814764294379988
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.12815962584210625
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.12808044171995586
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.12808593208973224
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.12797639582968928
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.12785703887086397
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.12785352225892427
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.12790776014690342
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.12797745285612164
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.12801498390756705
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.12800760719790516
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.1279480088356737
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.12797157299108133
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.12803381545774972
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.12805739455368548
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.1280435659765136
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.12803805579800578
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.12795636704010246
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.12794915862819728
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.12796503030135134
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.12819927116060814
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.12824416199682753
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.1282650473159413
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.12827516573926676
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.12824271347998195
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.12822286157683613
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.12820573521499662
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.1281429098166504
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.12806247740983964
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.12805586913202563
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.12815446385436438
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.12824022824804776
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.12828584078509928
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.1282823879953841
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.1281992139483101
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.12828723633406208
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.1283443155776522
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.12824698321749572
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.12819311432540417
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.1281637030129948
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.12824437894873855
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.12820891106177953
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.1281643934998211
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.1282077455969706
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.12820251475347846
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.12828382226443097
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.12836661536003585
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.12833102933840376
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.12824392600639448
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.12829959071228766
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.1282792075346875
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.12829439569734058
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.1283742633653197
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.12840838245550792
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.12838466778556082
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.12837652447052914
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.12844738884577675
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.12839297258287746
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.1283809152676871
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.12837719833209446
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.12830463964589603
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.1283757489748474
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.12838869914412498
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.12830634626088205
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.1282651258024527
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.12829429804508694
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.12823402607026174
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.12824163264128113
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.12817616711060206
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.12816213985995564
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.128168380230057
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.12828272273977295
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.1283079402398337
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.12831869276264046
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.1282665358122551
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.12816433940711489
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.1281804077177491
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.12817320879315375
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.1281436835974455
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.12813455115381323
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.12810755928802253
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.1280613575065994
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.12810747364148645
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.12815030837132607
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.1281072597565322
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.1281162011242616
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.12815864800530322
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.128116692482405
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.12812068738588472
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.1281072329321917
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.12815438885494923
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.1281719474938245
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.1282245856732274
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.12828568358378237
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.12835458990258092
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.12835171227332212
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.12831849442643412
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.1283742867783601
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.12834348852435748
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.12839705498796178
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.12841673265983708
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.12833016288266397
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.1283219446832279
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.1283012638723149
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.12821749840893656
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.12823975915023816
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.12819665088971086
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.1281224716029245
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.12805287342431934
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.12809226919492822
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.12804897429628503
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.12806239029001143
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.12812191622185817
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.12810434284566463
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.12820402570410605
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.12825594566233917
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.12821992920481995
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.1281078571373075
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.12809499994936316
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.12811560808868905
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.12806935985972978
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.12804541893252938
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.12801869012214043
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.1280271571673704
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.12802646332524817
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.12800173636450862
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.128057896941235
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.12805665194789126
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.128031478855345
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.1279991278197707
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.12804969883136516
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.1279936180538426
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.1279880911571339
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.1279312423297337
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.12794802533952812
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.1279568282858809
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.12798093597701543
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.12798980219301834
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.1280839298730311
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.12806620102965652
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.1280578181824901
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.12799679590932236
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.12801801138718066
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.1280228705335689
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.12805033070654828
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.1280642209746087
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.1281639194418477
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.12820448764542272
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.12826040651887022
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.12830614896526762
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.12841735169354637
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.12841735169354637 valid 0.22126437723636627
LOSS train 0.12841735169354637 valid 0.19245512038469315
LOSS train 0.12841735169354637 valid 0.18453355133533478
LOSS train 0.12841735169354637 valid 0.17489362880587578
LOSS train 0.12841735169354637 valid 0.17062066197395326
LOSS train 0.12841735169354637 valid 0.18087738752365112
LOSS train 0.12841735169354637 valid 0.1916308147566659
LOSS train 0.12841735169354637 valid 0.18984093144536018
LOSS train 0.12841735169354637 valid 0.1891779965824551
LOSS train 0.12841735169354637 valid 0.1889852374792099
LOSS train 0.12841735169354637 valid 0.18750748173757034
LOSS train 0.12841735169354637 valid 0.18824298307299614
LOSS train 0.12841735169354637 valid 0.18764339158168206
LOSS train 0.12841735169354637 valid 0.18706376318420684
LOSS train 0.12841735169354637 valid 0.184654035170873
LOSS train 0.12841735169354637 valid 0.1846421891823411
LOSS train 0.12841735169354637 valid 0.18535497521652894
LOSS train 0.12841735169354637 valid 0.1842472180724144
LOSS train 0.12841735169354637 valid 0.18673038874801837
LOSS train 0.12841735169354637 valid 0.1865201212465763
LOSS train 0.12841735169354637 valid 0.18549395955744244
LOSS train 0.12841735169354637 valid 0.18401090462099423
LOSS train 0.12841735169354637 valid 0.18372766414414282
LOSS train 0.12841735169354637 valid 0.18391957816978297
LOSS train 0.12841735169354637 valid 0.18263467669487
LOSS train 0.12841735169354637 valid 0.182508141375505
LOSS train 0.12841735169354637 valid 0.18286453739360528
LOSS train 0.12841735169354637 valid 0.18250087329319545
LOSS train 0.12841735169354637 valid 0.1819828830916306
LOSS train 0.12841735169354637 valid 0.18252371698617936
LOSS train 0.12841735169354637 valid 0.18334063503050035
LOSS train 0.12841735169354637 valid 0.18251062650233507
LOSS train 0.12841735169354637 valid 0.18275808113994021
LOSS train 0.12841735169354637 valid 0.18217861257931767
LOSS train 0.12841735169354637 valid 0.1838510228054864
LOSS train 0.12841735169354637 valid 0.18358475301000807
LOSS train 0.12841735169354637 valid 0.18435470276587718
LOSS train 0.12841735169354637 valid 0.18474867665453962
LOSS train 0.12841735169354637 valid 0.18446346467886215
LOSS train 0.12841735169354637 valid 0.1841658651828766
LOSS train 0.12841735169354637 valid 0.18472696877107386
LOSS train 0.12841735169354637 valid 0.1848763365830694
LOSS train 0.12841735169354637 valid 0.18490341375040453
LOSS train 0.12841735169354637 valid 0.18542347035624765
LOSS train 0.12841735169354637 valid 0.18548997342586518
LOSS train 0.12841735169354637 valid 0.18590589612722397
LOSS train 0.12841735169354637 valid 0.1865352344639758
LOSS train 0.12841735169354637 valid 0.18637834023684263
LOSS train 0.12841735169354637 valid 0.1869898474946314
LOSS train 0.12841735169354637 valid 0.18648022443056106
LOSS train 0.12841735169354637 valid 0.18667537207696952
LOSS train 0.12841735169354637 valid 0.18609768668046364
LOSS train 0.12841735169354637 valid 0.18662598925941395
LOSS train 0.12841735169354637 valid 0.18674996219299458
LOSS train 0.12841735169354637 valid 0.1866481827063994
LOSS train 0.12841735169354637 valid 0.18595828541687556
LOSS train 0.12841735169354637 valid 0.18587819081649445
LOSS train 0.12841735169354637 valid 0.18574455817197932
LOSS train 0.12841735169354637 valid 0.18611621578871193
LOSS train 0.12841735169354637 valid 0.18589703664183616
LOSS train 0.12841735169354637 valid 0.18547272902043138
LOSS train 0.12841735169354637 valid 0.18600949236462194
LOSS train 0.12841735169354637 valid 0.18535420984502823
LOSS train 0.12841735169354637 valid 0.18625702033750713
LOSS train 0.12841735169354637 valid 0.18647587780769054
LOSS train 0.12841735169354637 valid 0.18614906704787051
LOSS train 0.12841735169354637 valid 0.1855589627329983
LOSS train 0.12841735169354637 valid 0.18554416496087522
LOSS train 0.12841735169354637 valid 0.18518881888493247
LOSS train 0.12841735169354637 valid 0.1857828919376646
LOSS train 0.12841735169354637 valid 0.18539967755196798
LOSS train 0.12841735169354637 valid 0.18557862668401665
LOSS train 0.12841735169354637 valid 0.185574690977188
LOSS train 0.12841735169354637 valid 0.18557171543707718
LOSS train 0.12841735169354637 valid 0.1859535664319992
LOSS train 0.12841735169354637 valid 0.18638577469085393
LOSS train 0.12841735169354637 valid 0.1862192873830919
LOSS train 0.12841735169354637 valid 0.1859845106418316
LOSS train 0.12841735169354637 valid 0.18565452683575545
LOSS train 0.12841735169354637 valid 0.18499058149755002
LOSS train 0.12841735169354637 valid 0.18464875478803375
LOSS train 0.12841735169354637 valid 0.18511829616093053
LOSS train 0.12841735169354637 valid 0.18493761086320304
LOSS train 0.12841735169354637 valid 0.18494648007409914
LOSS train 0.12841735169354637 valid 0.18469240963459016
LOSS train 0.12841735169354637 valid 0.18447988722906555
LOSS train 0.12841735169354637 valid 0.18426083662044043
LOSS train 0.12841735169354637 valid 0.18396587771448222
LOSS train 0.12841735169354637 valid 0.1844714545132069
LOSS train 0.12841735169354637 valid 0.1844469850262006
LOSS train 0.12841735169354637 valid 0.18454177039010183
LOSS train 0.12841735169354637 valid 0.18447645008563995
LOSS train 0.12841735169354637 valid 0.184250503297775
LOSS train 0.12841735169354637 valid 0.18443720597536006
LOSS train 0.12841735169354637 valid 0.18436391337921745
LOSS train 0.12841735169354637 valid 0.1844094729361435
LOSS train 0.12841735169354637 valid 0.18425024047340313
LOSS train 0.12841735169354637 valid 0.18449451150942822
LOSS train 0.12841735169354637 valid 0.18453912391807092
LOSS train 0.12841735169354637 valid 0.1846441099047661
LOSS train 0.12841735169354637 valid 0.18483950168189436
LOSS train 0.12841735169354637 valid 0.18507464186233633
LOSS train 0.12841735169354637 valid 0.18472786594941779
LOSS train 0.12841735169354637 valid 0.18488168358229673
LOSS train 0.12841735169354637 valid 0.18498044652598244
LOSS train 0.12841735169354637 valid 0.18544334173202515
LOSS train 0.12841735169354637 valid 0.18534481817873838
LOSS train 0.12841735169354637 valid 0.18550672271737345
LOSS train 0.12841735169354637 valid 0.185911469776696
LOSS train 0.12841735169354637 valid 0.18594960841265593
LOSS train 0.12841735169354637 valid 0.18591121778831826
LOSS train 0.12841735169354637 valid 0.18571383239967482
LOSS train 0.12841735169354637 valid 0.1857622366035934
LOSS train 0.12841735169354637 valid 0.1860813893247069
LOSS train 0.12841735169354637 valid 0.18630688605101212
LOSS train 0.12841735169354637 valid 0.18652914181865496
LOSS train 0.12841735169354637 valid 0.18645779266316667
LOSS train 0.12841735169354637 valid 0.18603666606595962
LOSS train 0.12841735169354637 valid 0.18568692575482762
LOSS train 0.12841735169354637 valid 0.18534000056485336
LOSS train 0.12841735169354637 valid 0.18517025356942957
LOSS train 0.12841735169354637 valid 0.1852197858398078
LOSS train 0.12841735169354637 valid 0.18500293439965906
LOSS train 0.12841735169354637 valid 0.18533051134117187
LOSS train 0.12841735169354637 valid 0.18514647102355958
LOSS train 0.12841735169354637 valid 0.1854947561782504
LOSS train 0.12841735169354637 valid 0.18534133366243108
LOSS train 0.12841735169354637 valid 0.18538812967017293
LOSS train 0.12841735169354637 valid 0.185528000311334
LOSS train 0.12841735169354637 valid 0.18512777663194216
LOSS train 0.12841735169354637 valid 0.18481733849030416
LOSS train 0.12841735169354637 valid 0.18449295820160347
LOSS train 0.12841735169354637 valid 0.18436964752530693
LOSS train 0.12841735169354637 valid 0.18450547143149731
LOSS train 0.12841735169354637 valid 0.18437356540450342
LOSS train 0.12841735169354637 valid 0.18438536794308355
LOSS train 0.12841735169354637 valid 0.1840507773152233
LOSS train 0.12841735169354637 valid 0.18397030417901883
LOSS train 0.12841735169354637 valid 0.1839000744570931
LOSS train 0.12841735169354637 valid 0.18399779083473342
LOSS train 0.12841735169354637 valid 0.18385419619421586
LOSS train 0.12841735169354637 valid 0.1838715802615797
LOSS train 0.12841735169354637 valid 0.1837436385921665
LOSS train 0.12841735169354637 valid 0.18374060415145424
LOSS train 0.12841735169354637 valid 0.18362004983014074
LOSS train 0.12841735169354637 valid 0.1838274851237258
LOSS train 0.12841735169354637 valid 0.18377006722956288
LOSS train 0.12841735169354637 valid 0.18469836562871933
LOSS train 0.12841735169354637 valid 0.18486191742372193
LOSS train 0.12841735169354637 valid 0.18481522788604102
LOSS train 0.12841735169354637 valid 0.185065670716052
LOSS train 0.12841735169354637 valid 0.18472040474022688
LOSS train 0.12841735169354637 valid 0.18465844176563562
LOSS train 0.12841735169354637 valid 0.18451543994151154
LOSS train 0.12841735169354637 valid 0.18439121255951543
LOSS train 0.12841735169354637 valid 0.18438965225448975
LOSS train 0.12841735169354637 valid 0.18448945927392146
LOSS train 0.12841735169354637 valid 0.1844654872636252
LOSS train 0.12841735169354637 valid 0.18456427621766455
LOSS train 0.12841735169354637 valid 0.18449159748852254
LOSS train 0.12841735169354637 valid 0.18433843488278595
LOSS train 0.12841735169354637 valid 0.18421691766491644
LOSS train 0.12841735169354637 valid 0.183932828665511
LOSS train 0.12841735169354637 valid 0.1837351337620398
LOSS train 0.12841735169354637 valid 0.18356633068937245
LOSS train 0.12841735169354637 valid 0.18364852325743938
LOSS train 0.12841735169354637 valid 0.18400851695123546
LOSS train 0.12841735169354637 valid 0.18392044030839488
LOSS train 0.12841735169354637 valid 0.18408544458581147
LOSS train 0.12841735169354637 valid 0.18408957316594965
LOSS train 0.12841735169354637 valid 0.18409849598742367
LOSS train 0.12841735169354637 valid 0.1839647689011208
LOSS train 0.12841735169354637 valid 0.1839422353602558
LOSS train 0.12841735169354637 valid 0.18390191971570596
LOSS train 0.12841735169354637 valid 0.1835771236249379
LOSS train 0.12841735169354637 valid 0.18353576746515252
LOSS train 0.12841735169354637 valid 0.1836472540083578
LOSS train 0.12841735169354637 valid 0.18390856273053738
LOSS train 0.12841735169354637 valid 0.18380854474432642
LOSS train 0.12841735169354637 valid 0.18372824249996078
LOSS train 0.12841735169354637 valid 0.18386863608386636
LOSS train 0.12841735169354637 valid 0.18371816659039195
LOSS train 0.12841735169354637 valid 0.1837780341261723
LOSS train 0.12841735169354637 valid 0.18364050911496516
LOSS train 0.12841735169354637 valid 0.1835040153683843
LOSS train 0.12841735169354637 valid 0.1835516361139154
LOSS train 0.12841735169354637 valid 0.1834045051731528
LOSS train 0.12841735169354637 valid 0.18339525694225697
LOSS train 0.12841735169354637 valid 0.18326321055018713
LOSS train 0.12841735169354637 valid 0.18334196539301623
LOSS train 0.12841735169354637 valid 0.1832050020314012
LOSS train 0.12841735169354637 valid 0.18325497241069874
LOSS train 0.12841735169354637 valid 0.18297500869770741
LOSS train 0.12841735169354637 valid 0.18290606824700364
LOSS train 0.12841735169354637 valid 0.18269576330979664
LOSS train 0.12841735169354637 valid 0.18265694250561754
LOSS train 0.12841735169354637 valid 0.182869553565979
LOSS train 0.12841735169354637 valid 0.18281370777674397
LOSS train 0.12841735169354637 valid 0.18295632145512644
LOSS train 0.12841735169354637 valid 0.18276991039514542
LOSS train 0.12841735169354637 valid 0.18258514251578506
LOSS train 0.12841735169354637 valid 0.1825240911676152
LOSS train 0.12841735169354637 valid 0.18252616937230962
LOSS train 0.12841735169354637 valid 0.1827342673113533
LOSS train 0.12841735169354637 valid 0.18249186960662284
LOSS train 0.12841735169354637 valid 0.18250808372948935
LOSS train 0.12841735169354637 valid 0.18243642957602146
LOSS train 0.12841735169354637 valid 0.1822797738445493
LOSS train 0.12841735169354637 valid 0.18228974503478365
LOSS train 0.12841735169354637 valid 0.18221218578872225
LOSS train 0.12841735169354637 valid 0.1822087838186472
LOSS train 0.12841735169354637 valid 0.18217581095841695
LOSS train 0.12841735169354637 valid 0.18213917678790473
LOSS train 0.12841735169354637 valid 0.18216801817729095
LOSS train 0.12841735169354637 valid 0.18204389935316043
LOSS train 0.12841735169354637 valid 0.1819052123637111
LOSS train 0.12841735169354637 valid 0.18182483809884242
LOSS train 0.12841735169354637 valid 0.1818619514003806
LOSS train 0.12841735169354637 valid 0.18197039286839908
LOSS train 0.12841735169354637 valid 0.18190243718298998
LOSS train 0.12841735169354637 valid 0.18182133616904866
LOSS train 0.12841735169354637 valid 0.18185443236484183
LOSS train 0.12841735169354637 valid 0.18206278699128617
LOSS train 0.12841735169354637 valid 0.18215905336130941
LOSS train 0.12841735169354637 valid 0.18233106911182403
LOSS train 0.12841735169354637 valid 0.18258985461650695
LOSS train 0.12841735169354637 valid 0.18265062813454264
LOSS train 0.12841735169354637 valid 0.18271605350207865
LOSS train 0.12841735169354637 valid 0.18271367401535335
LOSS train 0.12841735169354637 valid 0.1827490380924681
LOSS train 0.12841735169354637 valid 0.18289809593390594
LOSS train 0.12841735169354637 valid 0.18291470106562663
LOSS train 0.12841735169354637 valid 0.18302372782271306
LOSS train 0.12841735169354637 valid 0.1830804154404208
LOSS train 0.12841735169354637 valid 0.18324128674699905
LOSS train 0.12841735169354637 valid 0.18312460840758632
LOSS train 0.12841735169354637 valid 0.1830522501644706
LOSS train 0.12841735169354637 valid 0.18307497014268106
LOSS train 0.12841735169354637 valid 0.18285706152975809
LOSS train 0.12841735169354637 valid 0.1828453814610839
LOSS train 0.12841735169354637 valid 0.18304209244201786
LOSS train 0.12841735169354637 valid 0.18287762494619228
LOSS train 0.12841735169354637 valid 0.1830998044690968
LOSS train 0.12841735169354637 valid 0.18328144511238473
LOSS train 0.12841735169354637 valid 0.18334518713610512
LOSS train 0.12841735169354637 valid 0.18318690801781368
LOSS train 0.12841735169354637 valid 0.18333666774666743
LOSS train 0.12841735169354637 valid 0.18326699150906456
LOSS train 0.12841735169354637 valid 0.1832655610808407
LOSS train 0.12841735169354637 valid 0.18328368973731995
LOSS train 0.12841735169354637 valid 0.18311463355780597
LOSS train 0.12841735169354637 valid 0.1833741201294793
LOSS train 0.12841735169354637 valid 0.1833127046762248
LOSS train 0.12841735169354637 valid 0.183193766284646
LOSS train 0.12841735169354637 valid 0.18324127057019401
LOSS train 0.12841735169354637 valid 0.18330531741958112
LOSS train 0.12841735169354637 valid 0.18312196253802526
LOSS train 0.12841735169354637 valid 0.18338160944539447
LOSS train 0.12841735169354637 valid 0.18339175313826234
LOSS train 0.12841735169354637 valid 0.1832291197891419
LOSS train 0.12841735169354637 valid 0.18338975641462538
LOSS train 0.12841735169354637 valid 0.1834755592787539
LOSS train 0.12841735169354637 valid 0.18352166957048408
LOSS train 0.12841735169354637 valid 0.18361604377401597
LOSS train 0.12841735169354637 valid 0.1835792086596759
LOSS train 0.12841735169354637 valid 0.1835923271631836
LOSS train 0.12841735169354637 valid 0.18364658097872574
LOSS train 0.12841735169354637 valid 0.18384360299626393
LOSS train 0.12841735169354637 valid 0.18392164041340128
LOSS train 0.12841735169354637 valid 0.18392189161645042
LOSS train 0.12841735169354637 valid 0.18402198600373146
LOSS train 0.12841735169354637 valid 0.18434976123492508
LOSS train 0.12841735169354637 valid 0.184558724909475
LOSS train 0.12841735169354637 valid 0.18460268831818644
LOSS train 0.12841735169354637 valid 0.18457762777805328
LOSS train 0.12841735169354637 valid 0.1845178091223689
LOSS train 0.12841735169354637 valid 0.184406232640201
LOSS train 0.12841735169354637 valid 0.1842585586601024
LOSS train 0.12841735169354637 valid 0.18424332953695755
LOSS train 0.12841735169354637 valid 0.1842316997902734
LOSS train 0.12841735169354637 valid 0.18416100910125677
LOSS train 0.12841735169354637 valid 0.1838488904032724
LOSS train 0.12841735169354637 valid 0.18378250298251533
LOSS train 0.12841735169354637 valid 0.18380748729785562
LOSS train 0.12841735169354637 valid 0.18385044239592133
LOSS train 0.12841735169354637 valid 0.18386704977471513
LOSS train 0.12841735169354637 valid 0.1838302774556007
LOSS train 0.12841735169354637 valid 0.1838084060129606
LOSS train 0.12841735169354637 valid 0.18381658095905112
LOSS train 0.12841735169354637 valid 0.18385097217457047
LOSS train 0.12841735169354637 valid 0.18368138226958894
LOSS train 0.12841735169354637 valid 0.18366925063708875
LOSS train 0.12841735169354637 valid 0.1836709246141105
LOSS train 0.12841735169354637 valid 0.18374668287278031
LOSS train 0.12841735169354637 valid 0.1838259500214609
LOSS train 0.12841735169354637 valid 0.18374178958805026
LOSS train 0.12841735169354637 valid 0.18383368506074352
LOSS train 0.12841735169354637 valid 0.1838719557545729
LOSS train 0.12841735169354637 valid 0.18390210538504514
LOSS train 0.12841735169354637 valid 0.18400244000057378
LOSS train 0.12841735169354637 valid 0.1839413257482836
LOSS train 0.12841735169354637 valid 0.18389036212831933
LOSS train 0.12841735169354637 valid 0.18393417565834405
LOSS train 0.12841735169354637 valid 0.18392542217809119
LOSS train 0.12841735169354637 valid 0.18387186539466263
LOSS train 0.12841735169354637 valid 0.1839060704741213
LOSS train 0.12841735169354637 valid 0.18388416749161307
LOSS train 0.12841735169354637 valid 0.18377838310386455
LOSS train 0.12841735169354637 valid 0.18377183944372683
LOSS train 0.12841735169354637 valid 0.18382102041955917
LOSS train 0.12841735169354637 valid 0.18381225116479052
LOSS train 0.12841735169354637 valid 0.18379881908782783
LOSS train 0.12841735169354637 valid 0.18398554258929273
LOSS train 0.12841735169354637 valid 0.18399524733803835
LOSS train 0.12841735169354637 valid 0.1839128934911319
LOSS train 0.12841735169354637 valid 0.18389748661672767
LOSS train 0.12841735169354637 valid 0.18396255507924203
LOSS train 0.12841735169354637 valid 0.18400488674359503
LOSS train 0.12841735169354637 valid 0.18417180717271697
LOSS train 0.12841735169354637 valid 0.1840965466806665
LOSS train 0.12841735169354637 valid 0.18427502744572927
LOSS train 0.12841735169354637 valid 0.18426885647895913
LOSS train 0.12841735169354637 valid 0.18421238515491456
LOSS train 0.12841735169354637 valid 0.18432060562442115
LOSS train 0.12841735169354637 valid 0.18431944239598055
LOSS train 0.12841735169354637 valid 0.1844816361239717
LOSS train 0.12841735169354637 valid 0.18453463103155113
LOSS train 0.12841735169354637 valid 0.18448667861975548
LOSS train 0.12841735169354637 valid 0.18454107658026067
LOSS train 0.12841735169354637 valid 0.1845341388249036
LOSS train 0.12841735169354637 valid 0.18442091766711088
LOSS train 0.12841735169354637 valid 0.18428239888365727
LOSS train 0.12841735169354637 valid 0.18434539546927176
LOSS train 0.12841735169354637 valid 0.1844223217737532
LOSS train 0.12841735169354637 valid 0.1843846895134271
LOSS train 0.12841735169354637 valid 0.18445926046531116
LOSS train 0.12841735169354637 valid 0.18445929046996623
LOSS train 0.12841735169354637 valid 0.1844468794586743
LOSS train 0.12841735169354637 valid 0.18446935249552965
LOSS train 0.12841735169354637 valid 0.18441035920644508
LOSS train 0.12841735169354637 valid 0.18425617393248242
LOSS train 0.12841735169354637 valid 0.18426087500843388
LOSS train 0.12841735169354637 valid 0.18430369855624246
LOSS train 0.12841735169354637 valid 0.18455045338893353
LOSS train 0.12841735169354637 valid 0.18467025020416233
LOSS train 0.12841735169354637 valid 0.1847577357955406
LOSS train 0.12841735169354637 valid 0.18461501866750826
LOSS train 0.12841735169354637 valid 0.18455154819135694
LOSS train 0.12841735169354637 valid 0.1845371376264061
LOSS train 0.12841735169354637 valid 0.18446841867906708
LOSS train 0.12841735169354637 valid 0.18438507830909853
LOSS train 0.12841735169354637 valid 0.18438184301538224
LOSS train 0.12841735169354637 valid 0.1843880259830283
LOSS train 0.12841735169354637 valid 0.1844044047602489
LOSS train 0.12841735169354637 valid 0.1844622246186498
LOSS train 0.12841735169354637 valid 0.18445607628464028
LOSS train 0.12841735169354637 valid 0.18449022608394383
LOSS train 0.12841735169354637 valid 0.18438307863956724
LOSS train 0.12841735169354637 valid 0.18438773178871629
LOSS train 0.12841735169354637 valid 0.1843461812577314
LOSS train 0.12841735169354637 valid 0.18429192481691486
LOSS train 0.12841735169354637 valid 0.18434181169334038
LOSS train 0.12841735169354637 valid 0.18433927568111866
LOSS train 0.12841735169354637 valid 0.18433322411562716
LOSS train 0.12841735169354637 valid 0.18438590677225428
LOSS train 0.12841735169354637 valid 0.18441109328729208
LOSS train 0.12841735169354637 valid 0.18432111211872232
LOSS train 0.12841735169354637 valid 0.18436716426082927
LOSS train 0.12841735169354637 valid 0.1844486858907754
EPOCH 27:
  batch 1 loss: 0.11138834059238434
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.11158881708979607
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.10914047807455063
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.11811542697250843
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.12095395177602768
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.12328729405999184
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.11785211094788142
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.12056805565953255
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.119103385342492
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.11887563690543175
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.11698646233840422
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.11614177065591018
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.11717453656288293
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.11753267156226295
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.11634577612082163
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.11847665347158909
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.11748087274677613
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.12078000646498468
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.12068110273072594
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.11997683085501194
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.12043498988662447
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.12104818800633604
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.12117586155300555
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.1228804827357332
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.12229032814502716
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.12248437049297187
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.12302429918889646
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.12266246655157634
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.12212545861457956
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.12294525454441706
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.12352768307731997
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.12396387150511146
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.12329459641918991
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.12404098580865298
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.12544986392770494
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.12478396834598647
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.12478125598785039
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.12446498753208864
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.12488474219273298
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.12494867220520973
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.12499692781669337
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.12521767864624658
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.1262699840373771
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.12579302980818532
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.1259694101081954
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.1254674027795377
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.1253635970518944
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.12541296503817043
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.12542090656197802
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.1254968075454235
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.1251049820406764
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.12523661740124226
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.12502531327728955
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.12495663555132018
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.12535113055597652
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.12548199043210065
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.1255704667745975
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.1251663939449294
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.12552245024402262
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.1254213166733583
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.12553272310827598
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.1254275345033215
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.12547169389232757
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.1252054130891338
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.12527227527820148
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.12508551313570052
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.1249361966750515
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.12517578669768922
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.12542770856964416
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.1256467379629612
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.12599875673022068
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.12622953092472422
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.12633391102291133
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.1263546948698727
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.1263403535882632
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.12615621021311535
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.12614672263334323
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.1258603209295334
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.12617021374687365
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.12645364394411446
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.12655872373301305
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.12620466320616444
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.12607964846384095
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.1264704523519391
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.1266089776859564
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.12695320444398148
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.12723504769048471
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.12767698979851874
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.12758708016925982
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.12796918087535433
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.12818813029226367
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.12818005567659502
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.12823824312097282
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.12816650000341395
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.12858439221193915
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.12866400081353882
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.12840552551230205
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.12821769258197474
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.12857580184936523
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.12846480712294578
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.12830702849838993
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.1281453403918182
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.12787231774006075
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.12795023218943521
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.12747343934717634
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.1274818335220499
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.12775136098683437
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.1275259845510677
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.1276995819916419
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.12789433300495148
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.12765164767299686
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.1276168456035001
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.12732678414446064
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.12768626003934627
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.1275000508064809
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.12738192665936618
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.12778034061193466
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.12777209843871956
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.12763870231995061
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.12762254395832617
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.1276048053025214
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.12765247503020724
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.12778971974200348
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.12778911505255006
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.12780169242620468
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.1278574870574096
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.12789427899704206
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.12764163414249197
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.1275881383885709
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.1275788511794347
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.12744298139373764
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.12742052447389474
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.12775713177327824
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.1278774811149533
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.1276584741142061
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.12763567564680295
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.1277778422745475
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.12773067838903787
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.12794540061367501
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.12794506124087743
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.12797791716900278
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.12781361967954838
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.12786671878783018
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.1280007317351798
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.1278748059581066
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.12796296470173418
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.1279811231153352
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.1279076389162927
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.12830811298933606
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.1283646117647489
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.1282530068759097
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.12834736175442996
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.1283036760644975
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.12800121336401282
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.12802579489446456
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.12801042734048304
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.12808261830715617
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.12805341566099396
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.1281456673201525
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.12826078147627412
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.12838061787882207
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.12832641532575642
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.12831750242439516
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.12849674660076454
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.128651057815913
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.128553818911314
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.12842135644423033
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.12835737437541997
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.12816427114623538
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.12844596202759181
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.12853110119787572
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.1285370373015487
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.12834904971667108
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.12827563050320778
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.12838664919137954
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.1282201440242881
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.12799955563525023
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.12771535819668448
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.12789167476266455
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.1277336812267701
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.12782169512128302
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.12779896988809764
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.12769535566613974
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.12761896087423616
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.12760937544139656
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.1275472135473323
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.12765288663739188
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.1275622981422125
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.12749807339496713
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.12757199900714974
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.12758593713737906
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.12747609440702945
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.127545855707764
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.1273137567968098
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.12734917535995827
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.1275123628806703
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.1276250289857085
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.12778450544885914
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.1277223485124171
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.1278821275755763
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.12788970638715214
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.12795224767362717
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.12795308845236972
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.12799186489599593
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.12789655167882036
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.127775131137047
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.1277580071186674
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.1276168031617999
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.1276976653691114
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.12779954402219681
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.12782712047698938
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.12784502852075505
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.12794062215397614
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.12771378903188438
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.1277876968993697
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.12768935187961217
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.12767163506163978
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.12776685113070207
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.1278322293810104
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.12784005948765712
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.12785022624882098
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.12788333116216702
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.12781781696551583
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.12787705104398941
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.12776206463575362
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.12776194997461496
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.12788718147818737
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.12770401166850015
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.12765730507509157
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.1278704346522041
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.12813280200287377
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.1280488396898426
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.1281188621541461
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.1281215925501962
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.1281156416269059
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.12814431997426487
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.1280850338659206
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.12810039607917562
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.12811001804832634
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.1279922302191456
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.12811080140691575
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.12806674160740592
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.1281405613746172
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.1281760929793608
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.12816043943166733
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.128215335824383
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.1282092410362201
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.12828193637993066
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.12829854500940047
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.12826501017808914
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.12847180302399563
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.1285081954584235
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.1285244360153854
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.12849558356005375
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.12847375916499718
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.12848619127180427
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.12841570890483225
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.12835280293988627
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.12844085253228552
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.12848634797220046
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.1283915272674798
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.128326331613628
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.12830158008595383
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.12817878380530712
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.12819776672799632
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.12822060586702555
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.12819088719199212
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.12818313036947998
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.1280952048899959
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.12810060696469414
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.12812356254931306
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.12808604423394976
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.12821241498211802
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.1280904659172044
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.12797232161868702
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.12818025473667227
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.1281468294265038
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.12817482923646625
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.12812616595978377
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.12815218416175672
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.12811299660027664
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.1280481740039714
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.12803307420478693
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.1279919897534058
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.12813968028415712
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.12822850040726727
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.12814004519363728
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.12811178913236493
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.1280464554998289
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.12808863995917913
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.12810294092008748
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.1282144734151151
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.12840649783102726
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.12846359162002194
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.1284222410391953
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.1284483433071826
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.12844256479732116
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.1283940181836186
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.12833195057601035
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.1282942625383536
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.12831799854273812
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.12830556534378734
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.12830664050264326
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.12827593941045434
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.12823708240614562
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.1282234824463433
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.1282434787734712
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.12819123585019018
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.12815245096255273
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.12806687016160256
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.12814679667593198
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.1281067106920557
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.1280335764439342
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.12801036084438586
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.12800889466963117
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.12814439076317263
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.12812565732453524
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.12812363857743125
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.12804528795252773
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.12813066709786652
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.12808014791331188
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.12802126142919434
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.12803486926083224
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.12795171856788207
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.12794754860492852
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.1278570437449619
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.12774745987825073
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.1277401962022229
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.1277746520596797
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.1278659239411354
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.12789375868448677
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.12789245088955006
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.12783526656058458
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.1278520368574979
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.12791637949534315
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.12795014495384835
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.12793648545837544
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.1279384897437674
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.12785692972331625
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.1278529003919924
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.12786917665924144
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.12810386741283344
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.12815503374219983
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.1282137390009539
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.12820639076872148
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.1281733818007687
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.12815139223673847
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.12812954245199418
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.1280613687412445
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.12798380411096982
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.12797572540167051
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.12806411843154242
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.1281475148060842
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.1282023902624677
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.1281875591672642
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.12809296301827672
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.12818800549463732
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.12824807659457516
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.12816675329025742
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.12810967541817161
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.12808907954564056
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.1281792780188895
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.12815092582019563
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.1281053097406914
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.1281406748172355
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.12813331416802978
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.12820278394368428
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.1283097013466708
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.128277894987808
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.12819524256361498
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.12823873929016674
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.1282210637244486
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.1282286396694567
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.12832121447445874
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.12835382695992786
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.12834293633065325
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.12831931723286683
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.1283761099887588
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.12831112555668034
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.12829682925029806
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.12829524501571504
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.12822906949913315
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.12829876300093399
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.1283021195946882
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.12821268890198176
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.12816241094022218
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.12819724173816907
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.1281461160169127
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.12815160043824914
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.12808699470299942
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.1280782677404716
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.1280843311426591
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.12819493840668947
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.12821924501112875
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.12822576395318477
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.12816480026025362
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.12805646791190886
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.12808484735725514
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.12806148001723422
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.12803506955504418
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.12805307439140548
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.1280207325384688
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.12796546675948883
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.12801247560373036
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.12804999316548124
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.12801023040499007
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.12802375157430074
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.12806614415318357
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.1280177166744666
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.12802390608845687
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.12801289514903605
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.12806732598149662
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.12809233349402938
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.12814783783638534
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.12820750772953032
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.12828382667010793
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.12827618938269947
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.12824312793581108
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.12829506518163658
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.12825509834857213
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.12831526725705616
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.12833814029868745
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.1282626054036138
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.1282615247621851
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.12824405796387617
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.1281469324035264
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.1281663589441246
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.12813288194484243
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.12806324586098566
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.12798943502265353
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.12804900078651799
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.1280156771686894
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.12802440337578638
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.1280765159201512
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.12804518713690768
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.12813494287841365
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.12820223227859362
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.12815857154729704
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.12804671415978522
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.1280198679051616
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.12803699708038987
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.1280032104867346
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.12798490052365827
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.12797414514798303
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.12797896068417625
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.12798173007275493
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.1279593572453898
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.12802345039589064
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.12800714504678956
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.12797182998723455
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.12794082441641857
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.1279876825226619
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.12793028254316868
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.1279309317961926
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.12788403112809738
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.12788171696950468
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.12789761596226848
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.1279167488347495
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.12793146814006606
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.12801618844918583
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.12799519866123116
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.12797436811578222
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.12790641825863402
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.12792902070515114
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.12793508409171975
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.12793961157742487
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.1279573022553446
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.1280388024627653
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.1280895053768463
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.12814987412792572
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.1281775967267415
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.12828725936301685
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.12828725936301685 valid 0.2194073349237442
LOSS train 0.12828725936301685 valid 0.19043835997581482
LOSS train 0.12828725936301685 valid 0.18234329422314963
LOSS train 0.12828725936301685 valid 0.17284520715475082
LOSS train 0.12828725936301685 valid 0.16854821145534515
LOSS train 0.12828725936301685 valid 0.1786859929561615
LOSS train 0.12828725936301685 valid 0.18941871183259146
LOSS train 0.12828725936301685 valid 0.18757936172187328
LOSS train 0.12828725936301685 valid 0.1869581358300315
LOSS train 0.12828725936301685 valid 0.1867200404405594
LOSS train 0.12828725936301685 valid 0.18523608283563095
LOSS train 0.12828725936301685 valid 0.18601395189762115
LOSS train 0.12828725936301685 valid 0.1854059065763767
LOSS train 0.12828725936301685 valid 0.184820117694991
LOSS train 0.12828725936301685 valid 0.18237091501553854
LOSS train 0.12828725936301685 valid 0.1823614565655589
LOSS train 0.12828725936301685 valid 0.1830365596448674
LOSS train 0.12828725936301685 valid 0.181908727520042
LOSS train 0.12828725936301685 valid 0.18439958519057223
LOSS train 0.12828725936301685 valid 0.18417526558041572
LOSS train 0.12828725936301685 valid 0.18315152114345915
LOSS train 0.12828725936301685 valid 0.18166035278276962
LOSS train 0.12828725936301685 valid 0.1813824915367624
LOSS train 0.12828725936301685 valid 0.1816014846165975
LOSS train 0.12828725936301685 valid 0.1803138154745102
LOSS train 0.12828725936301685 valid 0.1801993354008748
LOSS train 0.12828725936301685 valid 0.18055556273018872
LOSS train 0.12828725936301685 valid 0.1801768716956888
LOSS train 0.12828725936301685 valid 0.17963687870009193
LOSS train 0.12828725936301685 valid 0.18015849143266677
LOSS train 0.12828725936301685 valid 0.1809497163180382
LOSS train 0.12828725936301685 valid 0.1801426550373435
LOSS train 0.12828725936301685 valid 0.1803992476427194
LOSS train 0.12828725936301685 valid 0.1798148939714712
LOSS train 0.12828725936301685 valid 0.18141963013580867
LOSS train 0.12828725936301685 valid 0.1811533263987965
LOSS train 0.12828725936301685 valid 0.18193947584242434
LOSS train 0.12828725936301685 valid 0.1822914594882413
LOSS train 0.12828725936301685 valid 0.18200496030159485
LOSS train 0.12828725936301685 valid 0.18171360045671464
LOSS train 0.12828725936301685 valid 0.18227565324887995
LOSS train 0.12828725936301685 valid 0.18243171452056794
LOSS train 0.12828725936301685 valid 0.18246463946131772
LOSS train 0.12828725936301685 valid 0.18297887458042664
LOSS train 0.12828725936301685 valid 0.18304863737689125
LOSS train 0.12828725936301685 valid 0.1834627998911816
LOSS train 0.12828725936301685 valid 0.18409542263822354
LOSS train 0.12828725936301685 valid 0.1839449486384789
LOSS train 0.12828725936301685 valid 0.1845429193000404
LOSS train 0.12828725936301685 valid 0.18403541892766953
LOSS train 0.12828725936301685 valid 0.18421016779600405
LOSS train 0.12828725936301685 valid 0.18363453046633646
LOSS train 0.12828725936301685 valid 0.1841722993918185
LOSS train 0.12828725936301685 valid 0.18430508066106727
LOSS train 0.12828725936301685 valid 0.18420630232854324
LOSS train 0.12828725936301685 valid 0.18351295803274428
LOSS train 0.12828725936301685 valid 0.183439316456778
LOSS train 0.12828725936301685 valid 0.18330946025149575
LOSS train 0.12828725936301685 valid 0.18368807889647404
LOSS train 0.12828725936301685 valid 0.1834627702832222
LOSS train 0.12828725936301685 valid 0.18303743664358482
LOSS train 0.12828725936301685 valid 0.18357979386083542
LOSS train 0.12828725936301685 valid 0.18293118287646581
LOSS train 0.12828725936301685 valid 0.18383034085854888
LOSS train 0.12828725936301685 valid 0.18406448799830216
LOSS train 0.12828725936301685 valid 0.18374110419641843
LOSS train 0.12828725936301685 valid 0.18314964402077802
LOSS train 0.12828725936301685 valid 0.1831033120260519
LOSS train 0.12828725936301685 valid 0.18275259262409763
LOSS train 0.12828725936301685 valid 0.18335894026926586
LOSS train 0.12828725936301685 valid 0.18297869596682803
LOSS train 0.12828725936301685 valid 0.18315866341193518
LOSS train 0.12828725936301685 valid 0.18316036340308517
LOSS train 0.12828725936301685 valid 0.18315873496435783
LOSS train 0.12828725936301685 valid 0.18353463153044383
LOSS train 0.12828725936301685 valid 0.18396789580583572
LOSS train 0.12828725936301685 valid 0.1837948388480521
LOSS train 0.12828725936301685 valid 0.18356108378905517
LOSS train 0.12828725936301685 valid 0.18323495399348344
LOSS train 0.12828725936301685 valid 0.18256857581436634
LOSS train 0.12828725936301685 valid 0.18222344988658104
LOSS train 0.12828725936301685 valid 0.18269118666648865
LOSS train 0.12828725936301685 valid 0.18251181833715324
LOSS train 0.12828725936301685 valid 0.18251139528694607
LOSS train 0.12828725936301685 valid 0.18223627577809726
LOSS train 0.12828725936301685 valid 0.1820338266186936
LOSS train 0.12828725936301685 valid 0.18181315578263382
LOSS train 0.12828725936301685 valid 0.18151609514924613
LOSS train 0.12828725936301685 valid 0.18203126915385215
LOSS train 0.12828725936301685 valid 0.18200805054770575
LOSS train 0.12828725936301685 valid 0.18209714450678982
LOSS train 0.12828725936301685 valid 0.1820373901206514
LOSS train 0.12828725936301685 valid 0.18180747378257014
LOSS train 0.12828725936301685 valid 0.18199610646734846
LOSS train 0.12828725936301685 valid 0.18192554775037265
LOSS train 0.12828725936301685 valid 0.1819774693188568
LOSS train 0.12828725936301685 valid 0.18182269860174238
LOSS train 0.12828725936301685 valid 0.18207333647474952
LOSS train 0.12828725936301685 valid 0.18211779826217228
LOSS train 0.12828725936301685 valid 0.18221443742513657
LOSS train 0.12828725936301685 valid 0.18241288844901737
LOSS train 0.12828725936301685 valid 0.18264394794024674
LOSS train 0.12828725936301685 valid 0.18229439869089034
LOSS train 0.12828725936301685 valid 0.1824524553062824
LOSS train 0.12828725936301685 valid 0.18255428813752675
LOSS train 0.12828725936301685 valid 0.18301995787418113
LOSS train 0.12828725936301685 valid 0.18292100616147586
LOSS train 0.12828725936301685 valid 0.18308053662379584
LOSS train 0.12828725936301685 valid 0.18348756555570375
LOSS train 0.12828725936301685 valid 0.1835285785523328
LOSS train 0.12828725936301685 valid 0.18348946616993295
LOSS train 0.12828725936301685 valid 0.183295075116413
LOSS train 0.12828725936301685 valid 0.18333836625107622
LOSS train 0.12828725936301685 valid 0.18365668714569325
LOSS train 0.12828725936301685 valid 0.18388595412606779
LOSS train 0.12828725936301685 valid 0.18411400387513227
LOSS train 0.12828725936301685 valid 0.18404053202551654
LOSS train 0.12828725936301685 valid 0.1836209516909163
LOSS train 0.12828725936301685 valid 0.18326969407185786
LOSS train 0.12828725936301685 valid 0.18291672045985857
LOSS train 0.12828725936301685 valid 0.18274418726440303
LOSS train 0.12828725936301685 valid 0.18279628553351418
LOSS train 0.12828725936301685 valid 0.1825761997360524
LOSS train 0.12828725936301685 valid 0.18290017016472354
LOSS train 0.12828725936301685 valid 0.1827199193239212
LOSS train 0.12828725936301685 valid 0.18306224935111545
LOSS train 0.12828725936301685 valid 0.18290376076548118
LOSS train 0.12828725936301685 valid 0.18295015196781605
LOSS train 0.12828725936301685 valid 0.18308691022007964
LOSS train 0.12828725936301685 valid 0.18268621552449008
LOSS train 0.12828725936301685 valid 0.18237438581827034
LOSS train 0.12828725936301685 valid 0.18205512597253828
LOSS train 0.12828725936301685 valid 0.1819389288810859
LOSS train 0.12828725936301685 valid 0.18205855464312568
LOSS train 0.12828725936301685 valid 0.18192662696043652
LOSS train 0.12828725936301685 valid 0.18193480449126048
LOSS train 0.12828725936301685 valid 0.18160301871108311
LOSS train 0.12828725936301685 valid 0.18152593929266583
LOSS train 0.12828725936301685 valid 0.18145784960805084
LOSS train 0.12828725936301685 valid 0.18154623114636967
LOSS train 0.12828725936301685 valid 0.18140376089735233
LOSS train 0.12828725936301685 valid 0.1814226152401575
LOSS train 0.12828725936301685 valid 0.181297274319442
LOSS train 0.12828725936301685 valid 0.18129537678841087
LOSS train 0.12828725936301685 valid 0.18117219641290863
LOSS train 0.12828725936301685 valid 0.18137615143436275
LOSS train 0.12828725936301685 valid 0.18132166412411904
LOSS train 0.12828725936301685 valid 0.18225234928163322
LOSS train 0.12828725936301685 valid 0.18241587781266078
LOSS train 0.12828725936301685 valid 0.1823701637983322
LOSS train 0.12828725936301685 valid 0.1826227813564389
LOSS train 0.12828725936301685 valid 0.18227885192946383
LOSS train 0.12828725936301685 valid 0.1822154934889351
LOSS train 0.12828725936301685 valid 0.18206909473066207
LOSS train 0.12828725936301685 valid 0.18194544728725187
LOSS train 0.12828725936301685 valid 0.18194359703323779
LOSS train 0.12828725936301685 valid 0.1820386284665697
LOSS train 0.12828725936301685 valid 0.1820138362766821
LOSS train 0.12828725936301685 valid 0.1821170706044203
LOSS train 0.12828725936301685 valid 0.1820453037507832
LOSS train 0.12828725936301685 valid 0.1818931089795154
LOSS train 0.12828725936301685 valid 0.1817702038420571
LOSS train 0.12828725936301685 valid 0.1814865520212548
LOSS train 0.12828725936301685 valid 0.18128549661941645
LOSS train 0.12828725936301685 valid 0.18111955866669163
LOSS train 0.12828725936301685 valid 0.18120490140225515
LOSS train 0.12828725936301685 valid 0.18156912155494004
LOSS train 0.12828725936301685 valid 0.18147850178536915
LOSS train 0.12828725936301685 valid 0.18164618048794876
LOSS train 0.12828725936301685 valid 0.18164824992418288
LOSS train 0.12828725936301685 valid 0.18166323128150919
LOSS train 0.12828725936301685 valid 0.1815318093570166
LOSS train 0.12828725936301685 valid 0.181505896021865
LOSS train 0.12828725936301685 valid 0.1814627740746257
LOSS train 0.12828725936301685 valid 0.18113531308514733
LOSS train 0.12828725936301685 valid 0.18109292901036414
LOSS train 0.12828725936301685 valid 0.1812032746561503
LOSS train 0.12828725936301685 valid 0.18146419935346988
LOSS train 0.12828725936301685 valid 0.18136646061636216
LOSS train 0.12828725936301685 valid 0.18128715571429993
LOSS train 0.12828725936301685 valid 0.1814296864672919
LOSS train 0.12828725936301685 valid 0.18127970896906906
LOSS train 0.12828725936301685 valid 0.1813394092634076
LOSS train 0.12828725936301685 valid 0.1812031398648801
LOSS train 0.12828725936301685 valid 0.18106606715434306
LOSS train 0.12828725936301685 valid 0.1811111689575257
LOSS train 0.12828725936301685 valid 0.1809614848166226
LOSS train 0.12828725936301685 valid 0.18095142093110592
LOSS train 0.12828725936301685 valid 0.1808207401680568
LOSS train 0.12828725936301685 valid 0.18090416624357825
LOSS train 0.12828725936301685 valid 0.1807633807677873
LOSS train 0.12828725936301685 valid 0.18081838893704116
LOSS train 0.12828725936301685 valid 0.18053841521394068
LOSS train 0.12828725936301685 valid 0.18047429590495592
LOSS train 0.12828725936301685 valid 0.18026265509617634
LOSS train 0.12828725936301685 valid 0.18021974644186545
LOSS train 0.12828725936301685 valid 0.18042752880432883
LOSS train 0.12828725936301685 valid 0.18037197972186889
LOSS train 0.12828725936301685 valid 0.1805120775448018
LOSS train 0.12828725936301685 valid 0.18031899884343147
LOSS train 0.12828725936301685 valid 0.1801310504846905
LOSS train 0.12828725936301685 valid 0.18007001775031042
LOSS train 0.12828725936301685 valid 0.18007058015304245
LOSS train 0.12828725936301685 valid 0.18027417953400052
LOSS train 0.12828725936301685 valid 0.18003229062731674
LOSS train 0.12828725936301685 valid 0.18004785116436411
LOSS train 0.12828725936301685 valid 0.1799739887892912
LOSS train 0.12828725936301685 valid 0.17981348451800072
LOSS train 0.12828725936301685 valid 0.17982283769326918
LOSS train 0.12828725936301685 valid 0.17974256184839066
LOSS train 0.12828725936301685 valid 0.17974032278027013
LOSS train 0.12828725936301685 valid 0.17970665769194658
LOSS train 0.12828725936301685 valid 0.17966759001984842
LOSS train 0.12828725936301685 valid 0.17969582578010648
LOSS train 0.12828725936301685 valid 0.17957202092159627
LOSS train 0.12828725936301685 valid 0.17943570700784525
LOSS train 0.12828725936301685 valid 0.1793564157689222
LOSS train 0.12828725936301685 valid 0.17939325093949607
LOSS train 0.12828725936301685 valid 0.1795016876230501
LOSS train 0.12828725936301685 valid 0.179434039037336
LOSS train 0.12828725936301685 valid 0.17935186186257532
LOSS train 0.12828725936301685 valid 0.17938364491806374
LOSS train 0.12828725936301685 valid 0.17958905985537132
LOSS train 0.12828725936301685 valid 0.17968453759593622
LOSS train 0.12828725936301685 valid 0.17985321521759035
LOSS train 0.12828725936301685 valid 0.1801142172343963
LOSS train 0.12828725936301685 valid 0.18017385886891824
LOSS train 0.12828725936301685 valid 0.18023888655660444
LOSS train 0.12828725936301685 valid 0.18023569283266774
LOSS train 0.12828725936301685 valid 0.1802700623869896
LOSS train 0.12828725936301685 valid 0.18041871836433163
LOSS train 0.12828725936301685 valid 0.18043074596287875
LOSS train 0.12828725936301685 valid 0.18054108180969058
LOSS train 0.12828725936301685 valid 0.18059874988264507
LOSS train 0.12828725936301685 valid 0.18075928922663342
LOSS train 0.12828725936301685 valid 0.1806443760455665
LOSS train 0.12828725936301685 valid 0.1805726479628921
LOSS train 0.12828725936301685 valid 0.18059046278480723
LOSS train 0.12828725936301685 valid 0.1803722232580185
LOSS train 0.12828725936301685 valid 0.18035993358741204
LOSS train 0.12828725936301685 valid 0.1805574448153191
LOSS train 0.12828725936301685 valid 0.18039573094815262
LOSS train 0.12828725936301685 valid 0.18062025517102623
LOSS train 0.12828725936301685 valid 0.18080211125436377
LOSS train 0.12828725936301685 valid 0.18086724086683623
LOSS train 0.12828725936301685 valid 0.18070967998204193
LOSS train 0.12828725936301685 valid 0.18085787007924517
LOSS train 0.12828725936301685 valid 0.18079067424180045
LOSS train 0.12828725936301685 valid 0.18078823285888
LOSS train 0.12828725936301685 valid 0.18080462402105332
LOSS train 0.12828725936301685 valid 0.18063436027067117
LOSS train 0.12828725936301685 valid 0.18089307401151883
LOSS train 0.12828725936301685 valid 0.18083207662633285
LOSS train 0.12828725936301685 valid 0.1807113862999781
LOSS train 0.12828725936301685 valid 0.18075944161882587
LOSS train 0.12828725936301685 valid 0.1808242694241926
LOSS train 0.12828725936301685 valid 0.18064305940026903
LOSS train 0.12828725936301685 valid 0.18089883052563482
LOSS train 0.12828725936301685 valid 0.1809073092279287
LOSS train 0.12828725936301685 valid 0.18074287365262326
LOSS train 0.12828725936301685 valid 0.1809061657765816
LOSS train 0.12828725936301685 valid 0.180992049740926
LOSS train 0.12828725936301685 valid 0.1810373650870849
LOSS train 0.12828725936301685 valid 0.1811332575638186
LOSS train 0.12828725936301685 valid 0.1810959018626303
LOSS train 0.12828725936301685 valid 0.18110977214081844
LOSS train 0.12828725936301685 valid 0.18116737349649495
LOSS train 0.12828725936301685 valid 0.18136512910696997
LOSS train 0.12828725936301685 valid 0.1814428079305528
LOSS train 0.12828725936301685 valid 0.18144287180017543
LOSS train 0.12828725936301685 valid 0.18154146305089508
LOSS train 0.12828725936301685 valid 0.1818652161232689
LOSS train 0.12828725936301685 valid 0.18207308038687095
LOSS train 0.12828725936301685 valid 0.18211667337556825
LOSS train 0.12828725936301685 valid 0.18209279976107856
LOSS train 0.12828725936301685 valid 0.18202918384602104
LOSS train 0.12828725936301685 valid 0.18191677622416388
LOSS train 0.12828725936301685 valid 0.181772764370167
LOSS train 0.12828725936301685 valid 0.18175843683835854
LOSS train 0.12828725936301685 valid 0.18174739299075945
LOSS train 0.12828725936301685 valid 0.18167641345292224
LOSS train 0.12828725936301685 valid 0.18136575207748312
LOSS train 0.12828725936301685 valid 0.18129798156528507
LOSS train 0.12828725936301685 valid 0.1813221176902593
LOSS train 0.12828725936301685 valid 0.18136532319742338
LOSS train 0.12828725936301685 valid 0.18138255832808
LOSS train 0.12828725936301685 valid 0.1813431421379179
LOSS train 0.12828725936301685 valid 0.18132090928136474
LOSS train 0.12828725936301685 valid 0.1813264558523584
LOSS train 0.12828725936301685 valid 0.1813622351093539
LOSS train 0.12828725936301685 valid 0.18119571998049713
LOSS train 0.12828725936301685 valid 0.18118166834218044
LOSS train 0.12828725936301685 valid 0.18118550297851854
LOSS train 0.12828725936301685 valid 0.18126264211981474
LOSS train 0.12828725936301685 valid 0.18134025484323502
LOSS train 0.12828725936301685 valid 0.18125678525885214
LOSS train 0.12828725936301685 valid 0.18134948834526016
LOSS train 0.12828725936301685 valid 0.1813866067802746
LOSS train 0.12828725936301685 valid 0.18141625380535986
LOSS train 0.12828725936301685 valid 0.18151744909584522
LOSS train 0.12828725936301685 valid 0.18145068220026866
LOSS train 0.12828725936301685 valid 0.18140134454661647
LOSS train 0.12828725936301685 valid 0.1814459697297304
LOSS train 0.12828725936301685 valid 0.1814374973458287
LOSS train 0.12828725936301685 valid 0.1813839113370317
LOSS train 0.12828725936301685 valid 0.18141506892306353
LOSS train 0.12828725936301685 valid 0.18139315911049952
LOSS train 0.12828725936301685 valid 0.18128690924253557
LOSS train 0.12828725936301685 valid 0.1812783716228402
LOSS train 0.12828725936301685 valid 0.1813252897272187
LOSS train 0.12828725936301685 valid 0.1813161033190715
LOSS train 0.12828725936301685 valid 0.1813024470869165
LOSS train 0.12828725936301685 valid 0.18148919494864277
LOSS train 0.12828725936301685 valid 0.18149835889221758
LOSS train 0.12828725936301685 valid 0.1814154817234902
LOSS train 0.12828725936301685 valid 0.1813994315747596
LOSS train 0.12828725936301685 valid 0.18146279489486375
LOSS train 0.12828725936301685 valid 0.1815050389359957
LOSS train 0.12828725936301685 valid 0.18167402912923908
LOSS train 0.12828725936301685 valid 0.18159998378250747
LOSS train 0.12828725936301685 valid 0.18177735246137666
LOSS train 0.12828725936301685 valid 0.18177131607388117
LOSS train 0.12828725936301685 valid 0.18171510587074438
LOSS train 0.12828725936301685 valid 0.18182190330584108
LOSS train 0.12828725936301685 valid 0.18182355429117497
LOSS train 0.12828725936301685 valid 0.18198734140194997
LOSS train 0.12828725936301685 valid 0.1820381856338329
LOSS train 0.12828725936301685 valid 0.18199175171463228
LOSS train 0.12828725936301685 valid 0.182046291925081
LOSS train 0.12828725936301685 valid 0.18203953190734892
LOSS train 0.12828725936301685 valid 0.18192589663396788
LOSS train 0.12828725936301685 valid 0.181786518072687
LOSS train 0.12828725936301685 valid 0.18184917218781807
LOSS train 0.12828725936301685 valid 0.18192847146691676
LOSS train 0.12828725936301685 valid 0.1818917441946357
LOSS train 0.12828725936301685 valid 0.18196557127382784
LOSS train 0.12828725936301685 valid 0.18196408271524248
LOSS train 0.12828725936301685 valid 0.18195162155836292
LOSS train 0.12828725936301685 valid 0.1819742961750973
LOSS train 0.12828725936301685 valid 0.18191424334312187
LOSS train 0.12828725936301685 valid 0.18176152259985373
LOSS train 0.12828725936301685 valid 0.18176748370479423
LOSS train 0.12828725936301685 valid 0.18180969424828147
LOSS train 0.12828725936301685 valid 0.18205851080372584
LOSS train 0.12828725936301685 valid 0.18217832582152407
LOSS train 0.12828725936301685 valid 0.1822666396424605
LOSS train 0.12828725936301685 valid 0.18212367639332064
LOSS train 0.12828725936301685 valid 0.18206136949874205
LOSS train 0.12828725936301685 valid 0.18204640631262417
LOSS train 0.12828725936301685 valid 0.18197655071105276
LOSS train 0.12828725936301685 valid 0.181891986893283
LOSS train 0.12828725936301685 valid 0.1818896951788867
LOSS train 0.12828725936301685 valid 0.18189563348931564
LOSS train 0.12828725936301685 valid 0.18191238010754693
LOSS train 0.12828725936301685 valid 0.18196901857433184
LOSS train 0.12828725936301685 valid 0.18196241781534103
LOSS train 0.12828725936301685 valid 0.18199430312178716
LOSS train 0.12828725936301685 valid 0.18188962667098257
LOSS train 0.12828725936301685 valid 0.18189342209970719
LOSS train 0.12828725936301685 valid 0.1818501117121842
LOSS train 0.12828725936301685 valid 0.18179840156989086
LOSS train 0.12828725936301685 valid 0.18184959762909794
LOSS train 0.12828725936301685 valid 0.18184127498510455
LOSS train 0.12828725936301685 valid 0.18183508775309548
LOSS train 0.12828725936301685 valid 0.18188676174781093
LOSS train 0.12828725936301685 valid 0.18191135013119772
LOSS train 0.12828725936301685 valid 0.18182311263899711
LOSS train 0.12828725936301685 valid 0.18186916830018163
LOSS train 0.12828725936301685 valid 0.18194980267182922
EPOCH 28:
  batch 1 loss: 0.11842261254787445
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.1181354969739914
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.11339948077996571
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.12283267825841904
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.12496061623096466
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.12682651231686273
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.12142559885978699
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.12327820621430874
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.12202671666940053
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.12143544554710388
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.11946256865154613
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.11861435820659001
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.1192257828437365
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.11893036589026451
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.11750154246886571
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.11953546991571784
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.11853067839846891
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.12150122225284576
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.12115991546919472
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.12052668705582618
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.12100136705807277
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.12164803594350815
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.12159695638262707
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.12343681727846463
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.12277927428483963
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.12316450791863295
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.1235776651236746
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.12328992172011308
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.12271823554203429
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.12346657713254293
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.12411652361193011
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.12443273514509201
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.12354396373936624
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.12416560509625603
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.12551422119140626
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.12491155084636477
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.12479223451904349
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.12446065325486033
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.12496632566818824
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.12493585646152497
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.12507042688567463
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.12530407877195449
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.1262361951345621
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.12572066401216117
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.1258524571855863
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.12525582588885142
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.12515234883795393
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.12515975317607322
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.12516101160827947
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.1252728047966957
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.12493297136297413
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.1250582500719107
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.12487006215554364
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.12475916784670618
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.12512587918476625
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.12533984133707626
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.12549441072501635
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.1250471948035832
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.12534655428538888
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.12518616865078608
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.12523157479333097
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.12516099334724487
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.12520315344371494
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.12497791042551398
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.12505785272671627
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.1248253353617408
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.1246585861515643
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.12494957972975339
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.1252232625864554
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.12554096451827457
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.12595348794695357
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.12616513296961784
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.12621037825329662
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.12621864314014847
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.1262352752685547
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.12608951487039266
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.1260744466797098
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.12580179958007273
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.1262638184088695
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.12651556320488452
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.12657789683636325
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.12623805261966659
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.12608802803309568
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.12652766473946117
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.12662337895701914
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.12698410156854364
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.12724301407391997
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.12764494646001945
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.12754189030507976
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.1278789300057623
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.128093713587457
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.12802774888341842
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.12808900022058076
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.12805579975247383
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.12851879839834415
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.1285194446487973
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.12831801997939335
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.12814005973691844
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.12845823055866992
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.12838050819933414
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.1282324570387897
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.12806514088137477
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.12783365139683472
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.12795568959644207
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.12754373805863517
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.1275786726542239
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.1278350435406248
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.12759327998867742
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.1277053323360758
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.12794357348572125
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.1277075918676617
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.12766143287132894
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.1273763126372236
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.12772359705546446
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.12752934337958047
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.1274000746679717
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.12778477523571405
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.1278065398840581
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.1276731755928833
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.12764271708826225
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.12761958023487044
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.12767896074496332
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.1277893573893764
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.12774855835783866
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.12778053855895996
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.12782113774428291
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.12785374062267815
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.127616744954139
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.1275380772444629
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.12751944191180742
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.1273986391211284
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.12736559913239695
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.12768595654489404
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.12772505624747987
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.12747317872665548
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.12742426158750758
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.12757451190565625
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.12752490053358284
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.1277795280162379
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.12778575383126736
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.1278224875423925
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.12765136434578558
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.1277070922868235
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.12786134601467186
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.12775020434938628
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.12782356112378918
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.1278234651299561
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.1277916831644
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.12821860136401733
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.1282751247783502
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.1281516422203045
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.1282826937048843
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.12828102615339304
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.12798409879981698
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.12801851457165134
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.1280087290857083
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.12809878587722778
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.12807961477886273
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.12816593229020917
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.12827974166721107
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.12839510990595965
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.1283342178772997
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.1283377726202362
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.12853489934307774
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.12871769892446921
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.12858373959021396
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.12845999113992304
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.12836509756743908
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.12818701216984077
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.12848313867169267
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.12853997470865472
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.12852573061231956
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.12831032125419276
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.12823277335057312
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.12837428706032888
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.12819400717588988
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.12797961522967127
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.127707684600956
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.1278780466101689
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.12771083008911874
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.12779096336983845
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.1277741802917732
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.1276851789466019
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.12763283470564563
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.1276678340257825
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.12757836422452362
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.12766340785326166
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.1276012868719532
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.12752921146059792
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.12758168985969143
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.1276247087885572
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.1275283508002758
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.1275690546925204
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.12733145085038597
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.12738097436152973
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.12753546568660104
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.12762686787976832
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.1277956645371336
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.12773559234998932
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.1278787388280034
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.1279015341282484
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.12793940368412746
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.12796517586326364
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.12801924125090533
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.12793986197651885
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.12780573034604775
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.12776934146737132
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.1276188462327879
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.1276968950361156
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.12779392859055883
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.12779122884098387
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.1278163795974457
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.12791913709310299
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.12771332403209723
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.12780173968437106
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.1276845989610862
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.1276795545824662
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.12777044073840893
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.12783664733715797
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.12784797613593665
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.12787365984188487
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.12789427713902146
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.12786147242303386
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.12792527066942835
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.12783210986190371
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.12784783443254708
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.1279622194919292
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.1277910879110558
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.1277269787728526
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.1279697892782481
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.12820734099779293
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.12812218368695727
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.12818006617956407
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.12817808110107723
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.12816859055706795
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.1281979350152157
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.1281547366566799
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.12817114297331883
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.1281941271227274
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.12807300897936028
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.12819373360313321
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.12812711360040774
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.1281920604622413
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.12823583478810358
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.1282253181447788
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.12827579004735482
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.12827499898580405
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.12833855619593973
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.1283624179391976
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.12832030773162842
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.12853781623669355
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.1285682570130106
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.12860308192935385
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.1285823548403312
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.12854427140133054
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.12857234256807715
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.12849238827998535
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.1284023996933486
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.12847908940094319
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.12851584937710028
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.12839870560214894
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.12833624927483442
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.12828711301314966
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.12817411996762862
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.12821526704531797
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.12824877696041775
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.1282354122523065
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.12823711816610686
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.12814725578163635
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.12817652404860214
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.1281828641396607
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.12816321110243306
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.12829315242094871
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.1281665820002991
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.12806642079895192
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.1282652806814598
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.12825267401520526
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.1282588432429077
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.1282190741657356
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.12822269911744766
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.12818060364587452
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.12810928213363842
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.12810348529803037
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.12805869831690486
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.1282012212956161
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.12830757232500123
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.1282293524476294
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.12819526911092302
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.1281308129326695
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.12816644005220512
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.1281968141902763
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.1283094467314547
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.12847938500471895
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.1285214271320372
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.1284883737816649
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.1285144566633814
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.12850997802685407
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.12846531786474605
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.1284125146259831
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.12838115893304347
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.12839564891451616
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.1283876034647029
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.1283999340909936
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.12837171351144971
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.12833161634988471
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.12831606124253833
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.12835699817353818
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.12830027217021236
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.12825066844622293
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.12815299312914571
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.1282275188295021
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.12819326826586172
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.1281168965485911
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.12808923193129004
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.1280812123701686
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.12822392146704317
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.1282039045676448
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.12819642585309796
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.12812825364752622
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.1282138535287231
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.1281756445579811
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.12811678421238196
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.12812219763134286
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.1280373679359018
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.12803980263379905
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.12792933752840283
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.12782070824494784
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.12781084138082294
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.12784317942013493
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.127925024339647
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.12795625793609736
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.12795055285096169
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.12789651180978293
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.1279257287209976
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.1279835413418599
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.1280100187951965
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.12799714728968786
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.12800203834676885
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.1279161110915969
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.1278966574546169
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.1278967105311033
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.1281509534047361
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.12820886853137448
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.12824612855911255
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.12824822664260865
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.12821503487453295
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.12820458124453465
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.1281861225678318
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.1281201625109402
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.12803967837776456
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.12804188938052566
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.12813249679112976
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.12823304753465625
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.12826150036969428
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.12825118469100602
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.12816225546883064
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.1282401569291991
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.1282975781688144
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.12821755564478446
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.12815595085008277
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.12812747360514143
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.12821038369892052
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.1281696504428367
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.12812673814267247
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.12816725750900296
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.12815264885217115
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.12822416542498877
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.12832460038201965
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.12829154061511927
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.12821540766068407
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.12827930217884942
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.12827164096938026
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.1282874465028658
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.12836566836359029
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.12838599155346553
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.12836995384001987
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.1283529186991861
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.12841524538539706
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.12835040930314554
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.12833348032283157
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.1283274255595182
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.12825314302719076
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.12832227442656732
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.1283276135024304
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.12825419891190218
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.1282130868941391
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.128237925951179
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.12817954345001387
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.12819072753574057
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.1281181714664667
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.1280993271018843
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.1281018483129387
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.128214781274474
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.12823780177889135
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.12823697654884073
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.1281770242547447
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.12807140882204401
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.12809899065587388
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.12807371336639972
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.12803780337795614
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.1280431317309489
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.128018124483118
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.12796936149041055
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.12801210855198378
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.1280493864306697
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.12800623292993443
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.1280116976332606
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.12805975319854185
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.12800623148606868
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.12800405577915472
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.12798979871174423
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.12805049164781293
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.12806345031711727
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.12812375345667779
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.12818353747028902
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.12826227744181568
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.12825064613021536
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.12822266871278937
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.12827050810355276
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.1282412839609952
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.12830044338015084
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.1283294366257733
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.12824613579832916
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.1282430542281495
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.12822170743170908
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.12812145739933695
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.12813512642433827
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.12809284148024064
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.12802050517989205
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.12795026738976323
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.12800601377149745
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.1279610677242831
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.12797067680892987
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.1280307020940539
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.12799316341507025
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.12808309028933354
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.12814318620572102
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.1280900220892745
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.12797625797348847
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.12795668991113252
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.12796878100049738
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.1279238205217668
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.12789875189889366
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.12787759762156653
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.12787829907087797
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.1278820187354569
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.12785408045821542
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.12791427456042065
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.12790169069058113
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.12787361174821854
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.12785290847398226
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.1279053241508988
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.12784882953953797
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.1278569975186812
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.1278131549666216
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.12781441762324489
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.12782876605995375
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.12784904191132196
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.12785620339750464
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.12793924139569635
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.12792713005992698
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.12791705171847756
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.12785804868709963
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.1278816939267362
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.12788163998755075
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.12790644614929295
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.12793241950186723
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.128032338558736
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.12807610905818592
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.12813262525708116
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.1281684708095407
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.1282768676183739
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.1282768676183739 valid 0.2180778682231903
LOSS train 0.1282768676183739 valid 0.18904855847358704
LOSS train 0.1282768676183739 valid 0.181568905711174
LOSS train 0.1282768676183739 valid 0.17222543060779572
LOSS train 0.1282768676183739 valid 0.16798281967639922
LOSS train 0.1282768676183739 valid 0.17792901893456778
LOSS train 0.1282768676183739 valid 0.18840638654572622
LOSS train 0.1282768676183739 valid 0.18669413402676582
LOSS train 0.1282768676183739 valid 0.1861305899090237
LOSS train 0.1282768676183739 valid 0.18586580455303192
LOSS train 0.1282768676183739 valid 0.18451243097131903
LOSS train 0.1282768676183739 valid 0.18513318275411925
LOSS train 0.1282768676183739 valid 0.18446946717225587
LOSS train 0.1282768676183739 valid 0.18384461956364767
LOSS train 0.1282768676183739 valid 0.18147004942099254
LOSS train 0.1282768676183739 valid 0.18148524966090918
LOSS train 0.1282768676183739 valid 0.18211147890371435
LOSS train 0.1282768676183739 valid 0.1809493766890632
LOSS train 0.1282768676183739 valid 0.18343960611443771
LOSS train 0.1282768676183739 valid 0.1831298850476742
LOSS train 0.1282768676183739 valid 0.18215155672459377
LOSS train 0.1282768676183739 valid 0.18074346672404895
LOSS train 0.1282768676183739 valid 0.18046547602052274
LOSS train 0.1282768676183739 valid 0.18069684070845446
LOSS train 0.1282768676183739 valid 0.17942604780197144
LOSS train 0.1282768676183739 valid 0.1793529104727965
LOSS train 0.1282768676183739 valid 0.1797025418943829
LOSS train 0.1282768676183739 valid 0.17930531554988452
LOSS train 0.1282768676183739 valid 0.17877060123558702
LOSS train 0.1282768676183739 valid 0.17932039101918537
LOSS train 0.1282768676183739 valid 0.18012319553282954
LOSS train 0.1282768676183739 valid 0.17931324802339077
LOSS train 0.1282768676183739 valid 0.1794689383470651
LOSS train 0.1282768676183739 valid 0.17891701354699976
LOSS train 0.1282768676183739 valid 0.1804011127778462
LOSS train 0.1282768676183739 valid 0.180130987531609
LOSS train 0.1282768676183739 valid 0.1808973872983778
LOSS train 0.1282768676183739 valid 0.18121796375826785
LOSS train 0.1282768676183739 valid 0.1809324962206376
LOSS train 0.1282768676183739 valid 0.18067840710282326
LOSS train 0.1282768676183739 valid 0.1812415551848528
LOSS train 0.1282768676183739 valid 0.18140457357679093
LOSS train 0.1282768676183739 valid 0.18145058945167897
LOSS train 0.1282768676183739 valid 0.18196843259713866
LOSS train 0.1282768676183739 valid 0.18201559219095442
LOSS train 0.1282768676183739 valid 0.1824118886952815
LOSS train 0.1282768676183739 valid 0.18307076045807372
LOSS train 0.1282768676183739 valid 0.1828963023920854
LOSS train 0.1282768676183739 valid 0.1835095021797686
LOSS train 0.1282768676183739 valid 0.18302063643932343
LOSS train 0.1282768676183739 valid 0.1831943448267731
LOSS train 0.1282768676183739 valid 0.1826433467750366
LOSS train 0.1282768676183739 valid 0.183172418823782
LOSS train 0.1282768676183739 valid 0.18330577236634713
LOSS train 0.1282768676183739 valid 0.18320935043421657
LOSS train 0.1282768676183739 valid 0.18253701206828868
LOSS train 0.1282768676183739 valid 0.18245446211413333
LOSS train 0.1282768676183739 valid 0.18233714689468516
LOSS train 0.1282768676183739 valid 0.1827223768173638
LOSS train 0.1282768676183739 valid 0.18249511395891507
LOSS train 0.1282768676183739 valid 0.18208980120596338
LOSS train 0.1282768676183739 valid 0.18263933038519276
LOSS train 0.1282768676183739 valid 0.18200959076010992
LOSS train 0.1282768676183739 valid 0.18290208955295384
LOSS train 0.1282768676183739 valid 0.18314881806190197
LOSS train 0.1282768676183739 valid 0.18283297144102328
LOSS train 0.1282768676183739 valid 0.18222786041338052
LOSS train 0.1282768676183739 valid 0.182161044986809
LOSS train 0.1282768676183739 valid 0.1817893591048061
LOSS train 0.1282768676183739 valid 0.1824122450181416
LOSS train 0.1282768676183739 valid 0.18205026673599028
LOSS train 0.1282768676183739 valid 0.18223322327766153
LOSS train 0.1282768676183739 valid 0.18223829873620648
LOSS train 0.1282768676183739 valid 0.18223570105997292
LOSS train 0.1282768676183739 valid 0.1826071685552597
LOSS train 0.1282768676183739 valid 0.18303261011054642
LOSS train 0.1282768676183739 valid 0.1828743778652959
LOSS train 0.1282768676183739 valid 0.18265387072012976
LOSS train 0.1282768676183739 valid 0.18232161938389646
LOSS train 0.1282768676183739 valid 0.1816707167774439
LOSS train 0.1282768676183739 valid 0.18134225353046698
LOSS train 0.1282768676183739 valid 0.18181105521393987
LOSS train 0.1282768676183739 valid 0.18163762986660004
LOSS train 0.1282768676183739 valid 0.18164865974159466
LOSS train 0.1282768676183739 valid 0.18137782198541305
LOSS train 0.1282768676183739 valid 0.1811877898005552
LOSS train 0.1282768676183739 valid 0.18097684558095603
LOSS train 0.1282768676183739 valid 0.1806710096584125
LOSS train 0.1282768676183739 valid 0.18118344632427344
LOSS train 0.1282768676183739 valid 0.18117065363460116
LOSS train 0.1282768676183739 valid 0.1812658462223116
LOSS train 0.1282768676183739 valid 0.18121532334581666
LOSS train 0.1282768676183739 valid 0.18098762214824718
LOSS train 0.1282768676183739 valid 0.1811789225073571
LOSS train 0.1282768676183739 valid 0.181102276005243
LOSS train 0.1282768676183739 valid 0.18116508967553577
LOSS train 0.1282768676183739 valid 0.18099550088656316
LOSS train 0.1282768676183739 valid 0.18124255902913153
LOSS train 0.1282768676183739 valid 0.1812932247465307
LOSS train 0.1282768676183739 valid 0.1813908949494362
LOSS train 0.1282768676183739 valid 0.18158900663994326
LOSS train 0.1282768676183739 valid 0.18182309074144737
LOSS train 0.1282768676183739 valid 0.1814825160121455
LOSS train 0.1282768676183739 valid 0.1816333605406376
LOSS train 0.1282768676183739 valid 0.1817417696827934
LOSS train 0.1282768676183739 valid 0.18219543079722603
LOSS train 0.1282768676183739 valid 0.18209613497569183
LOSS train 0.1282768676183739 valid 0.18226174154767283
LOSS train 0.1282768676183739 valid 0.1826711581387651
LOSS train 0.1282768676183739 valid 0.18271457783200523
LOSS train 0.1282768676183739 valid 0.18268168361874315
LOSS train 0.1282768676183739 valid 0.1824935202353767
LOSS train 0.1282768676183739 valid 0.18253620615047692
LOSS train 0.1282768676183739 valid 0.182867891825082
LOSS train 0.1282768676183739 valid 0.18309598850167316
LOSS train 0.1282768676183739 valid 0.18331270266709657
LOSS train 0.1282768676183739 valid 0.18324426784474626
LOSS train 0.1282768676183739 valid 0.18282587194846847
LOSS train 0.1282768676183739 valid 0.1824809418005102
LOSS train 0.1282768676183739 valid 0.18213180924455324
LOSS train 0.1282768676183739 valid 0.18196371801135955
LOSS train 0.1282768676183739 valid 0.18201983683421963
LOSS train 0.1282768676183739 valid 0.18180840311011648
LOSS train 0.1282768676183739 valid 0.18210684003368502
LOSS train 0.1282768676183739 valid 0.18193948936462404
LOSS train 0.1282768676183739 valid 0.18228546778361002
LOSS train 0.1282768676183739 valid 0.18213235123420324
LOSS train 0.1282768676183739 valid 0.18217966053634882
LOSS train 0.1282768676183739 valid 0.18231710052305414
LOSS train 0.1282768676183739 valid 0.18192366808652877
LOSS train 0.1282768676183739 valid 0.18162013658585438
LOSS train 0.1282768676183739 valid 0.18130545024618958
LOSS train 0.1282768676183739 valid 0.1811791147504534
LOSS train 0.1282768676183739 valid 0.18129255700467237
LOSS train 0.1282768676183739 valid 0.18116224584756074
LOSS train 0.1282768676183739 valid 0.18118192441761494
LOSS train 0.1282768676183739 valid 0.18085280909155407
LOSS train 0.1282768676183739 valid 0.18077222570992899
LOSS train 0.1282768676183739 valid 0.18070753561935837
LOSS train 0.1282768676183739 valid 0.18077774037207875
LOSS train 0.1282768676183739 valid 0.1806401607203991
LOSS train 0.1282768676183739 valid 0.18065329221352724
LOSS train 0.1282768676183739 valid 0.18053027997900556
LOSS train 0.1282768676183739 valid 0.1805256876266665
LOSS train 0.1282768676183739 valid 0.18040844140381648
LOSS train 0.1282768676183739 valid 0.18061056171786294
LOSS train 0.1282768676183739 valid 0.18055689517332582
LOSS train 0.1282768676183739 valid 0.1814742708528364
LOSS train 0.1282768676183739 valid 0.18164619023367862
LOSS train 0.1282768676183739 valid 0.181611414651076
LOSS train 0.1282768676183739 valid 0.18185743413224126
LOSS train 0.1282768676183739 valid 0.18152171353760518
LOSS train 0.1282768676183739 valid 0.18145448904411465
LOSS train 0.1282768676183739 valid 0.1813163704105786
LOSS train 0.1282768676183739 valid 0.1811919338280155
LOSS train 0.1282768676183739 valid 0.1811941544023844
LOSS train 0.1282768676183739 valid 0.18129164474025652
LOSS train 0.1282768676183739 valid 0.18126971293476563
LOSS train 0.1282768676183739 valid 0.18136736685000127
LOSS train 0.1282768676183739 valid 0.18129481738433242
LOSS train 0.1282768676183739 valid 0.18114303061680764
LOSS train 0.1282768676183739 valid 0.18101957863495674
LOSS train 0.1282768676183739 valid 0.18074328520912333
LOSS train 0.1282768676183739 valid 0.18054832499928591
LOSS train 0.1282768676183739 valid 0.18038477446093704
LOSS train 0.1282768676183739 valid 0.18045597191316537
LOSS train 0.1282768676183739 valid 0.18081616293527408
LOSS train 0.1282768676183739 valid 0.18072462418959254
LOSS train 0.1282768676183739 valid 0.1808897482572928
LOSS train 0.1282768676183739 valid 0.18089135753757812
LOSS train 0.1282768676183739 valid 0.18090605003791943
LOSS train 0.1282768676183739 valid 0.1807768842162088
LOSS train 0.1282768676183739 valid 0.18075215006839332
LOSS train 0.1282768676183739 valid 0.18070877848685474
LOSS train 0.1282768676183739 valid 0.18038059937102455
LOSS train 0.1282768676183739 valid 0.18034268378026105
LOSS train 0.1282768676183739 valid 0.1804490845839856
LOSS train 0.1282768676183739 valid 0.18070595154768965
LOSS train 0.1282768676183739 valid 0.18061556300803935
LOSS train 0.1282768676183739 valid 0.18053653981122705
LOSS train 0.1282768676183739 valid 0.18067822967609648
LOSS train 0.1282768676183739 valid 0.18051184005625956
LOSS train 0.1282768676183739 valid 0.18055881018358502
LOSS train 0.1282768676183739 valid 0.18041951351029717
LOSS train 0.1282768676183739 valid 0.18027853542888486
LOSS train 0.1282768676183739 valid 0.1803236945422106
LOSS train 0.1282768676183739 valid 0.18016903074508045
LOSS train 0.1282768676183739 valid 0.18016131598740182
LOSS train 0.1282768676183739 valid 0.18003035730943479
LOSS train 0.1282768676183739 valid 0.18011015634003438
LOSS train 0.1282768676183739 valid 0.17997519028280418
LOSS train 0.1282768676183739 valid 0.18001732024519393
LOSS train 0.1282768676183739 valid 0.17973673115894584
LOSS train 0.1282768676183739 valid 0.1796751357614994
LOSS train 0.1282768676183739 valid 0.17946971444747387
LOSS train 0.1282768676183739 valid 0.17942964038526527
LOSS train 0.1282768676183739 valid 0.17962757929176243
LOSS train 0.1282768676183739 valid 0.1795730290163045
LOSS train 0.1282768676183739 valid 0.1797110761454956
LOSS train 0.1282768676183739 valid 0.1795193748548627
LOSS train 0.1282768676183739 valid 0.17933255956688923
LOSS train 0.1282768676183739 valid 0.17927445002859183
LOSS train 0.1282768676183739 valid 0.1792762206472787
LOSS train 0.1282768676183739 valid 0.17947671843656138
LOSS train 0.1282768676183739 valid 0.17923822035876716
LOSS train 0.1282768676183739 valid 0.17925035248248322
LOSS train 0.1282768676183739 valid 0.17917984122959313
LOSS train 0.1282768676183739 valid 0.17901526001067117
LOSS train 0.1282768676183739 valid 0.17902405820252223
LOSS train 0.1282768676183739 valid 0.17894693126990682
LOSS train 0.1282768676183739 valid 0.17894076654837596
LOSS train 0.1282768676183739 valid 0.17889766820337413
LOSS train 0.1282768676183739 valid 0.1788599007356335
LOSS train 0.1282768676183739 valid 0.17889467720812727
LOSS train 0.1282768676183739 valid 0.17876988528079765
LOSS train 0.1282768676183739 valid 0.17863759981399333
LOSS train 0.1282768676183739 valid 0.17855834250213914
LOSS train 0.1282768676183739 valid 0.17859473814247945
LOSS train 0.1282768676183739 valid 0.17869768727179533
LOSS train 0.1282768676183739 valid 0.17863134697756983
LOSS train 0.1282768676183739 valid 0.17855558760160775
LOSS train 0.1282768676183739 valid 0.17859101674712455
LOSS train 0.1282768676183739 valid 0.1787939929307309
LOSS train 0.1282768676183739 valid 0.17888693847427412
LOSS train 0.1282768676183739 valid 0.17905737357007132
LOSS train 0.1282768676183739 valid 0.17931391882289827
LOSS train 0.1282768676183739 valid 0.1793773212031121
LOSS train 0.1282768676183739 valid 0.17944371148028918
LOSS train 0.1282768676183739 valid 0.17943040945254038
LOSS train 0.1282768676183739 valid 0.17946901408874472
LOSS train 0.1282768676183739 valid 0.17961078440343148
LOSS train 0.1282768676183739 valid 0.17962630982671318
LOSS train 0.1282768676183739 valid 0.1797275235276877
LOSS train 0.1282768676183739 valid 0.17977965887413067
LOSS train 0.1282768676183739 valid 0.17993915432945212
LOSS train 0.1282768676183739 valid 0.17982641995837123
LOSS train 0.1282768676183739 valid 0.17975927125426788
LOSS train 0.1282768676183739 valid 0.17977876832880893
LOSS train 0.1282768676183739 valid 0.1795651876839135
LOSS train 0.1282768676183739 valid 0.17955118358756106
LOSS train 0.1282768676183739 valid 0.1797504132899506
LOSS train 0.1282768676183739 valid 0.1795935854623633
LOSS train 0.1282768676183739 valid 0.17981476747572667
LOSS train 0.1282768676183739 valid 0.17999615083585996
LOSS train 0.1282768676183739 valid 0.18005531080523315
LOSS train 0.1282768676183739 valid 0.17990034658128654
LOSS train 0.1282768676183739 valid 0.18005143066770152
LOSS train 0.1282768676183739 valid 0.1799857897323466
LOSS train 0.1282768676183739 valid 0.17998138749336143
LOSS train 0.1282768676183739 valid 0.1799903624355793
LOSS train 0.1282768676183739 valid 0.17982548537601037
LOSS train 0.1282768676183739 valid 0.18008176080646024
LOSS train 0.1282768676183739 valid 0.1800162038018581
LOSS train 0.1282768676183739 valid 0.17989790489471805
LOSS train 0.1282768676183739 valid 0.17994999140501022
LOSS train 0.1282768676183739 valid 0.18001857356284745
LOSS train 0.1282768676183739 valid 0.17983714444743984
LOSS train 0.1282768676183739 valid 0.18008452691426574
LOSS train 0.1282768676183739 valid 0.18009383271674853
LOSS train 0.1282768676183739 valid 0.17993258372522317
LOSS train 0.1282768676183739 valid 0.180088820100059
LOSS train 0.1282768676183739 valid 0.18016382372220055
LOSS train 0.1282768676183739 valid 0.180207064084907
LOSS train 0.1282768676183739 valid 0.1803021305946238
LOSS train 0.1282768676183739 valid 0.18026780944387868
LOSS train 0.1282768676183739 valid 0.1802773783473592
LOSS train 0.1282768676183739 valid 0.1803349420316657
LOSS train 0.1282768676183739 valid 0.18053026211017104
LOSS train 0.1282768676183739 valid 0.18060998272386183
LOSS train 0.1282768676183739 valid 0.1806133436108077
LOSS train 0.1282768676183739 valid 0.1807098867873424
LOSS train 0.1282768676183739 valid 0.18103592456592357
LOSS train 0.1282768676183739 valid 0.18124375650625088
LOSS train 0.1282768676183739 valid 0.1812896506357802
LOSS train 0.1282768676183739 valid 0.1812638804858381
LOSS train 0.1282768676183739 valid 0.18120323874704217
LOSS train 0.1282768676183739 valid 0.1810953174095722
LOSS train 0.1282768676183739 valid 0.1809521834466526
LOSS train 0.1282768676183739 valid 0.18094202739897594
LOSS train 0.1282768676183739 valid 0.1809327421709895
LOSS train 0.1282768676183739 valid 0.1808652540277756
LOSS train 0.1282768676183739 valid 0.18055851205337978
LOSS train 0.1282768676183739 valid 0.1804954771597478
LOSS train 0.1282768676183739 valid 0.18052334739813503
LOSS train 0.1282768676183739 valid 0.18056999714228145
LOSS train 0.1282768676183739 valid 0.18058381378129645
LOSS train 0.1282768676183739 valid 0.1805447372981065
LOSS train 0.1282768676183739 valid 0.18052733542087177
LOSS train 0.1282768676183739 valid 0.1805337661020071
LOSS train 0.1282768676183739 valid 0.18056897723983073
LOSS train 0.1282768676183739 valid 0.1804031490306674
LOSS train 0.1282768676183739 valid 0.18039045342537638
LOSS train 0.1282768676183739 valid 0.1803971125963605
LOSS train 0.1282768676183739 valid 0.1804761325775766
LOSS train 0.1282768676183739 valid 0.18055448443707772
LOSS train 0.1282768676183739 valid 0.18047318242590976
LOSS train 0.1282768676183739 valid 0.18055655471102558
LOSS train 0.1282768676183739 valid 0.18059504629681575
LOSS train 0.1282768676183739 valid 0.18062135385330705
LOSS train 0.1282768676183739 valid 0.18072262905538083
LOSS train 0.1282768676183739 valid 0.18065315086481182
LOSS train 0.1282768676183739 valid 0.18060742710895886
LOSS train 0.1282768676183739 valid 0.1806482142790316
LOSS train 0.1282768676183739 valid 0.1806366584557844
LOSS train 0.1282768676183739 valid 0.18058405809226583
LOSS train 0.1282768676183739 valid 0.18061528639758334
LOSS train 0.1282768676183739 valid 0.18059304130873385
LOSS train 0.1282768676183739 valid 0.18048365697167912
LOSS train 0.1282768676183739 valid 0.18047452631216604
LOSS train 0.1282768676183739 valid 0.18052309552027332
LOSS train 0.1282768676183739 valid 0.18051367698662535
LOSS train 0.1282768676183739 valid 0.18049918580800295
LOSS train 0.1282768676183739 valid 0.18068687420207472
LOSS train 0.1282768676183739 valid 0.18069915433123612
LOSS train 0.1282768676183739 valid 0.18062033348140263
LOSS train 0.1282768676183739 valid 0.18059860610792153
LOSS train 0.1282768676183739 valid 0.1806613344828413
LOSS train 0.1282768676183739 valid 0.18070524080463177
LOSS train 0.1282768676183739 valid 0.18087488247029088
LOSS train 0.1282768676183739 valid 0.1807997835567221
LOSS train 0.1282768676183739 valid 0.1809744259046617
LOSS train 0.1282768676183739 valid 0.18096960375305288
LOSS train 0.1282768676183739 valid 0.1809143717674648
LOSS train 0.1282768676183739 valid 0.18101655312434392
LOSS train 0.1282768676183739 valid 0.18101998473589237
LOSS train 0.1282768676183739 valid 0.18118027791929392
LOSS train 0.1282768676183739 valid 0.18123439544475772
LOSS train 0.1282768676183739 valid 0.1811871418911146
LOSS train 0.1282768676183739 valid 0.18123701434457917
LOSS train 0.1282768676183739 valid 0.18123243724306423
LOSS train 0.1282768676183739 valid 0.18112079211682353
LOSS train 0.1282768676183739 valid 0.18098030474979476
LOSS train 0.1282768676183739 valid 0.18104494926897255
LOSS train 0.1282768676183739 valid 0.18112304559100173
LOSS train 0.1282768676183739 valid 0.18108405210633777
LOSS train 0.1282768676183739 valid 0.18115530023351312
LOSS train 0.1282768676183739 valid 0.18114732787322574
LOSS train 0.1282768676183739 valid 0.18113749348641148
LOSS train 0.1282768676183739 valid 0.18116035501425948
LOSS train 0.1282768676183739 valid 0.18110200615928454
LOSS train 0.1282768676183739 valid 0.18094880203784736
LOSS train 0.1282768676183739 valid 0.18095335754299025
LOSS train 0.1282768676183739 valid 0.18099825108103432
LOSS train 0.1282768676183739 valid 0.18124470870595338
LOSS train 0.1282768676183739 valid 0.18136477844006774
LOSS train 0.1282768676183739 valid 0.1814478540394692
LOSS train 0.1282768676183739 valid 0.18130764650766032
LOSS train 0.1282768676183739 valid 0.18124603122558403
LOSS train 0.1282768676183739 valid 0.18123074889951585
LOSS train 0.1282768676183739 valid 0.18116393989750318
LOSS train 0.1282768676183739 valid 0.18108300467756738
LOSS train 0.1282768676183739 valid 0.1810783445200121
LOSS train 0.1282768676183739 valid 0.18108251486131896
LOSS train 0.1282768676183739 valid 0.1810982395489674
LOSS train 0.1282768676183739 valid 0.18114685200049843
LOSS train 0.1282768676183739 valid 0.18114298353955316
LOSS train 0.1282768676183739 valid 0.18117875251032056
LOSS train 0.1282768676183739 valid 0.181077022646559
LOSS train 0.1282768676183739 valid 0.1810827951120799
LOSS train 0.1282768676183739 valid 0.18104369009120597
LOSS train 0.1282768676183739 valid 0.18099184295064524
LOSS train 0.1282768676183739 valid 0.18103983214135327
LOSS train 0.1282768676183739 valid 0.18103075329183546
LOSS train 0.1282768676183739 valid 0.18102636407282982
LOSS train 0.1282768676183739 valid 0.1810793227324747
LOSS train 0.1282768676183739 valid 0.1811047174199357
LOSS train 0.1282768676183739 valid 0.18101726718023622
LOSS train 0.1282768676183739 valid 0.18106380038206343
LOSS train 0.1282768676183739 valid 0.18114358176626164
EPOCH 29:
  batch 1 loss: 0.11527501791715622
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.11501472443342209
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.11047950387001038
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.12057606503367424
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.12360240519046783
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.1256675124168396
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.12021560966968536
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.12243259139358997
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.12101570930745867
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.12047050446271897
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.11868817968802019
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.11768671373526256
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.11853686433572036
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.11852201925856727
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.11691653430461883
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.11879701819270849
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.11799842645140256
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.12108413792318767
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.12070152712495703
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.12004376128315926
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.12049445155121032
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.12126427076079628
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.12117408442756404
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.12294884615888198
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.12214322566986084
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.12243812531232834
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.12305891624203434
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.12262229568191937
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.12209105054880011
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.12288508986433347
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.12356850480841051
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.12400912749581039
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.12328815211852391
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.12397370290230303
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.12532616875001362
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.1247596304035849
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.12452313243537336
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.12425463352548449
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.12490954364721592
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.12484890315681696
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.1249920971146444
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.12513993414384977
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.12621605552213136
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.125708812170408
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.12587088462379242
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.12537467657871867
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.12520640787292034
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.12536124683295688
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.12529888186527757
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.12546936318278312
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.1250405714792364
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.12524337522112405
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.12500173331431622
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.12484097342800211
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.12524294257164
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.12538745813071728
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.12543518998120962
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.12496758111078164
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.12531300634145737
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.12508707083761691
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.12514699983303665
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.12515934088057087
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.1251634197339179
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.1248944221297279
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.12496603280305862
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.12479750699166095
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.12455234632118424
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.12482463360271033
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.12514394251764685
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.12540325171181133
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.12575269154679608
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.12595731940948302
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.12608686487560403
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.12613731068936554
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.12611505379279456
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.12598504331943236
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.12594811521567306
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.12567525777297142
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.12609857641443423
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.1263260141015053
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.12644358733553945
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.12603284255033587
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.12589002858443432
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.12637863733938762
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.1265454691999099
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.1268578890797704
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.1270939519350556
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.12754222547466104
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.12744898901561671
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.12791478410363197
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.12813640238492044
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.12811050960875076
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.12822876013414833
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.12816835963662634
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.1285881843221815
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.1286863995871196
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.1284595601951953
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.12824687878696286
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.12856614800414654
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.12850089482963084
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.12832935560162706
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.12815347467275226
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.12788961654149214
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.12800236863012499
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.12759362501757485
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.12764113326117676
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.12786046575720064
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.12763076772292456
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.12777660226603166
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.12798595320094716
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.12776243646402616
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.12773538014984556
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.127418476132165
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.1278056942841463
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.12756831088791723
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.12744292854492006
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.1278204303394016
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.12784247106667293
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.12776164560257888
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.12773643595476944
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.12774268014371887
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.12778791748597973
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.12791611011919937
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.12791457123333408
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.1279355962276459
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.12803353735851863
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.12806725232150612
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.12782575679011643
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.1277737859492154
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.1277964680813826
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.1276866670214493
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.12764910930259663
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.12795583539663402
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.12806029712308697
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.1278360175865668
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.12780079985147014
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.12797227224511823
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.12795234876482384
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.12815427335260585
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.12814833069486278
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.12816827458904145
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.12801223793919658
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.12806494711162328
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.1282145566203528
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.12808822455077334
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.12815260866733447
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.1281193869859994
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.12804794623642354
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.12843719294807254
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.12848748048146566
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.1283473843671628
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.1284566589778191
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.12839578335581261
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.1280938826694891
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.12813864883876616
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.12815646822444904
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.12820978543371153
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.12820956530638888
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.128314234503785
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.12843179753981532
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.1285620555281639
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.12850390429849978
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.12852918525780638
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.12872668382961575
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.12887660093379744
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.12875298518373304
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.12862110106709473
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.12854093508351416
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.12834956426415922
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.12863485247773282
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.12869318025676826
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.1286893156465403
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.12849918696921686
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.1284052467979919
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.1284940409234592
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.12834176840260625
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.12811727228306108
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.1278175904174869
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.12801185067139526
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.1278419442060921
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.12790642946299927
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.12789578428798978
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.12782541800880692
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.12774567399173975
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.1277965527531263
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.12772924985776665
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.12780965175380044
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.12772691340998132
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.1276612812801013
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.1276999553175349
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.12771516488328655
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.12762122915592045
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.1276757391700473
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.12743126343666894
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.1274539398459288
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.12759034225375068
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.12772424674124888
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.12788161518748362
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.12784030007657093
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.12798656783998014
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.12800941456906237
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.12807359125944648
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.12807723094383483
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.12811802050062254
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.12806067066948587
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.12794670024977148
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.1279294583676518
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.1277739922516048
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.1278825951606463
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.12798986019832748
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.12800395654699814
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.12805448609562414
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.1281250416183136
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.1279034248997118
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.12800046503543855
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.12791054651003192
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.12790343416039296
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.12798681592039013
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.12804154646859323
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.12803018611263145
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.12804289597057109
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.12806356990256826
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.1279932952114285
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.1280670548523111
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.12796591056717765
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.1279701154712027
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.12807180663562556
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.12790050730109215
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.12784609569472516
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.12809489116720532
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.12833659699210873
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.12824857549677635
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.12832856657934802
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.12833272430122408
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.1283163141697011
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.12835871137804905
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.12833849761933716
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.12837758463095217
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.1283745047749336
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.12824521179621418
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.12837129290296823
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.12831747849002356
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.12837578672072525
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.12842570095643646
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.1284119026089201
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.1284791293243567
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.12846186618331956
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.12852599399705086
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.12859326038494645
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.12856060329079627
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.12874202409469748
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.1287755427202062
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.12879713589377084
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.12877430919352478
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.12874399391459485
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.12875387255917303
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.12867216951420335
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.12863096773855445
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.12871239584615332
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.12874733026211077
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.1286422239752108
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.12857145474839757
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.12854241293646085
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.12843309430349054
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.12844313931352688
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.12847423102510602
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.12844298280468594
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.1284152968550351
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.1283213054435847
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.12832882837564857
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.12834183544580346
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.1283009601483012
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.12842751681913822
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.12830060345195507
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.12819239638068458
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.12839371797399243
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.12836628044124976
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.12838452668498745
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.1283462508498127
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.1283441330173186
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.12830990059732117
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.12822901806298723
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.12821604271975506
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.12817781224427088
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.12830031142945875
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.12840650970493997
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.12832511120349271
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.12828987795445654
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.12823604372133432
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.12825038530703248
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.12825712981502624
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.12836449220776558
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.1285382692826079
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.12856637975391078
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.12852963211677843
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.1285507893189788
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.12854023576284498
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.12849198109251542
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.1284448439909463
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.12839733496308325
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.1284294137328962
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.1284186966371852
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.12843404548002943
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.12839508887478396
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.1283589709489072
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.12835424354458166
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.1283947083286043
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.12834080693299893
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.12828763598473714
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.12819016818558016
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.12826597927755098
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.1282366762558619
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.1281739510952855
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.12814941273847963
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.12813960510113884
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.12826140217860288
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.12823896904473048
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.12824606342510608
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.1281698474829847
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.12825243358965963
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.12819873358230352
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.128143813137127
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.1281527275040792
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.1280660764026789
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.12806188427484952
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.12795165702243524
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.12784948405049262
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.12785358274946126
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.12790288803334657
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.12798614585489937
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.12802446088852118
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.1280240122767457
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.12796329100568732
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.127995110245165
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.12805191572032756
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.12808042094998418
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.12806289353427264
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.1280647769305833
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.12799612696455642
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.1279819007962942
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.1279935649981247
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.1282328360479826
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.12827648576511932
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.12832445592814407
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.1283302363926086
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.12830027845295178
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.12829276071244083
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.1282715585254047
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.128205648252479
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.1281242543033191
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.12812003327740562
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.12821794466369532
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.12831153625638222
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.12835617498152674
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.1283391915366683
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.1282387108364132
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.12832364619446068
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.1283821884397022
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.1283034939827361
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.12826500253544915
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.1282449640601956
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.12832763525580174
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.12827650798335877
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.12823306062964948
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.12827779603739306
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.12828017984990214
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.12835192187037064
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.12843601128248416
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.12840618802926082
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.12832414848176207
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.12838684904045172
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.12835909031651993
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.12837004028360582
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.1284406254277191
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.1284739520351092
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.12845280154192068
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.12844999737306362
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.12851770409437083
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.12845478173453448
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.1284436430389944
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.12844590089843655
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.12837892823428385
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.1284556282056841
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.1284622572711669
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.12837661344122578
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.1283217361631171
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.12834022042055154
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.12828334334508046
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.12829360333307238
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.1282210625326022
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.12818866057316666
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.1281959231258655
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.12828926594202755
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.12830372415672098
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.1283273738019074
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.1282553718698145
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.12814992692518595
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.12817621942441068
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.12815735225838826
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.1281308567710221
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.12812950629322903
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.12809542709247984
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.12803564827066496
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.12807227861099313
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.12810414413243165
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.1280638678137011
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.12806837398018825
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.12810454084811843
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.12804845647663243
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.12805534926492992
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.12805063505894945
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.12809159811043624
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.1281026726087993
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.12816046176541254
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.12821322390473033
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.12828514938099453
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.12827242508852224
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.12824215408432427
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.12828139736714966
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.12824417157542137
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.12831260915181983
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.128352929030267
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.1282705466128692
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.12825688826939408
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.128229778110981
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.1281332365224059
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.12814724344140752
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.12810513673123913
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.12802777496668008
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.1279517569514208
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.1280025831162238
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.1279582499905869
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.1279735024889402
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.12803060925363946
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.12801668794675805
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.12810454982409783
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.12815827584784964
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.128111288121574
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.1279991554915226
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.12798498552292586
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.12800717007478618
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.12796266012871427
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.1279480772988522
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.1279359348453917
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.1279373425111342
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.12793637985873116
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.12790839725759473
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.12796991642764105
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.1279597703890174
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.1279250388012992
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.12789267294951395
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.12794401288955612
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.12787715870286717
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.12787800459478396
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.1278179557277606
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.12781863207030192
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.12782656929620767
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.127845813597524
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.12784923754291597
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.127929887742452
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.1279144396363008
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.12790213772035264
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.1278311853854239
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.12785192665740333
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.12785739241748728
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.127891400373289
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.12791495532499392
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.12799355492759973
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.1280307098428832
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.1280821309444752
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.12809939027591877
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.1281924539450872
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.1281924539450872 valid 0.21897007524967194
LOSS train 0.1281924539450872 valid 0.18969450145959854
LOSS train 0.1281924539450872 valid 0.1823377162218094
LOSS train 0.1281924539450872 valid 0.17297443002462387
LOSS train 0.1281924539450872 valid 0.16858862042427064
LOSS train 0.1281924539450872 valid 0.17857509354750314
LOSS train 0.1281924539450872 valid 0.18918725848197937
LOSS train 0.1281924539450872 valid 0.18739834055304527
LOSS train 0.1281924539450872 valid 0.18685761756367153
LOSS train 0.1281924539450872 valid 0.18659650087356566
LOSS train 0.1281924539450872 valid 0.18504104289141568
LOSS train 0.1281924539450872 valid 0.1857509637872378
LOSS train 0.1281924539450872 valid 0.18509646218556625
LOSS train 0.1281924539450872 valid 0.18447995292288916
LOSS train 0.1281924539450872 valid 0.18209290007750192
LOSS train 0.1281924539450872 valid 0.18207508232444525
LOSS train 0.1281924539450872 valid 0.18273813847233267
LOSS train 0.1281924539450872 valid 0.1815873020225101
LOSS train 0.1281924539450872 valid 0.18400536869701586
LOSS train 0.1281924539450872 valid 0.18370457068085672
LOSS train 0.1281924539450872 valid 0.18270928944860185
LOSS train 0.1281924539450872 valid 0.18130117519335312
LOSS train 0.1281924539450872 valid 0.18102299778357797
LOSS train 0.1281924539450872 valid 0.1812541609009107
LOSS train 0.1281924539450872 valid 0.17997233271598817
LOSS train 0.1281924539450872 valid 0.1798795759677887
LOSS train 0.1281924539450872 valid 0.1802087762841472
LOSS train 0.1281924539450872 valid 0.17980932178241865
LOSS train 0.1281924539450872 valid 0.17927582973036274
LOSS train 0.1281924539450872 valid 0.17982037663459777
LOSS train 0.1281924539450872 valid 0.18060965115024197
LOSS train 0.1281924539450872 valid 0.17979797441512346
LOSS train 0.1281924539450872 valid 0.17997385787241388
LOSS train 0.1281924539450872 valid 0.1794086309040294
LOSS train 0.1281924539450872 valid 0.18096582889556884
LOSS train 0.1281924539450872 valid 0.18067478761076927
LOSS train 0.1281924539450872 valid 0.1814632899052388
LOSS train 0.1281924539450872 valid 0.18179716836465032
LOSS train 0.1281924539450872 valid 0.1815140297015508
LOSS train 0.1281924539450872 valid 0.18124375604093074
LOSS train 0.1281924539450872 valid 0.1817923086445506
LOSS train 0.1281924539450872 valid 0.18195696671803793
LOSS train 0.1281924539450872 valid 0.1820008269576139
LOSS train 0.1281924539450872 valid 0.18249990994280035
LOSS train 0.1281924539450872 valid 0.18256573014789157
LOSS train 0.1281924539450872 valid 0.1829581613773885
LOSS train 0.1281924539450872 valid 0.18361426477736614
LOSS train 0.1281924539450872 valid 0.18344622757285833
LOSS train 0.1281924539450872 valid 0.18405171681423577
LOSS train 0.1281924539450872 valid 0.18355888187885283
LOSS train 0.1281924539450872 valid 0.1837453713604048
LOSS train 0.1281924539450872 valid 0.18318119301245764
LOSS train 0.1281924539450872 valid 0.18370771408081055
LOSS train 0.1281924539450872 valid 0.18383743200037214
LOSS train 0.1281924539450872 valid 0.18374268575148148
LOSS train 0.1281924539450872 valid 0.18307540459292276
LOSS train 0.1281924539450872 valid 0.18295943501748538
LOSS train 0.1281924539450872 valid 0.1828289643443864
LOSS train 0.1281924539450872 valid 0.18323028668508692
LOSS train 0.1281924539450872 valid 0.18300294776757559
LOSS train 0.1281924539450872 valid 0.18259083907135198
LOSS train 0.1281924539450872 valid 0.1831413062830125
LOSS train 0.1281924539450872 valid 0.18250043453678252
LOSS train 0.1281924539450872 valid 0.1834021566901356
LOSS train 0.1281924539450872 valid 0.1836454159938372
LOSS train 0.1281924539450872 valid 0.18335052105513486
LOSS train 0.1281924539450872 valid 0.1827574665866681
LOSS train 0.1281924539450872 valid 0.18269970662453594
LOSS train 0.1281924539450872 valid 0.18233130073201828
LOSS train 0.1281924539450872 valid 0.18293508908578326
LOSS train 0.1281924539450872 valid 0.18256346608551455
LOSS train 0.1281924539450872 valid 0.18276031149758232
LOSS train 0.1281924539450872 valid 0.18275750418231912
LOSS train 0.1281924539450872 valid 0.18276352838084503
LOSS train 0.1281924539450872 valid 0.18314340571562449
LOSS train 0.1281924539450872 valid 0.18356571719050407
LOSS train 0.1281924539450872 valid 0.18341092204118703
LOSS train 0.1281924539450872 valid 0.18319134834485176
LOSS train 0.1281924539450872 valid 0.1828710255743582
LOSS train 0.1281924539450872 valid 0.18222669549286366
LOSS train 0.1281924539450872 valid 0.18189010447190132
LOSS train 0.1281924539450872 valid 0.1823692227282175
LOSS train 0.1281924539450872 valid 0.18219983559775066
LOSS train 0.1281924539450872 valid 0.1822084193783147
LOSS train 0.1281924539450872 valid 0.18193633556365968
LOSS train 0.1281924539450872 valid 0.18173124104045157
LOSS train 0.1281924539450872 valid 0.1815214823374803
LOSS train 0.1281924539450872 valid 0.18121448142284696
LOSS train 0.1281924539450872 valid 0.18172696601139027
LOSS train 0.1281924539450872 valid 0.181700004140536
LOSS train 0.1281924539450872 valid 0.18179209530353546
LOSS train 0.1281924539450872 valid 0.18173771400166594
LOSS train 0.1281924539450872 valid 0.1815136900191666
LOSS train 0.1281924539450872 valid 0.18169805581899398
LOSS train 0.1281924539450872 valid 0.18160105269206198
LOSS train 0.1281924539450872 valid 0.18166614913692078
LOSS train 0.1281924539450872 valid 0.18149480346551874
LOSS train 0.1281924539450872 valid 0.18174755177935775
LOSS train 0.1281924539450872 valid 0.1817920496969512
LOSS train 0.1281924539450872 valid 0.1818929359316826
LOSS train 0.1281924539450872 valid 0.18209250890972592
LOSS train 0.1281924539450872 valid 0.1823300463603992
LOSS train 0.1281924539450872 valid 0.1819871642346521
LOSS train 0.1281924539450872 valid 0.18214077072647902
LOSS train 0.1281924539450872 valid 0.1822564932562056
LOSS train 0.1281924539450872 valid 0.1827144185608288
LOSS train 0.1281924539450872 valid 0.1826109017167136
LOSS train 0.1281924539450872 valid 0.18276501460759728
LOSS train 0.1281924539450872 valid 0.18317467534760817
LOSS train 0.1281924539450872 valid 0.18321901925585488
LOSS train 0.1281924539450872 valid 0.18318326274553934
LOSS train 0.1281924539450872 valid 0.1829941545480064
LOSS train 0.1281924539450872 valid 0.18303334726169046
LOSS train 0.1281924539450872 valid 0.1833625452821715
LOSS train 0.1281924539450872 valid 0.18359121343363885
LOSS train 0.1281924539450872 valid 0.18381025886227345
LOSS train 0.1281924539450872 valid 0.1837452159732835
LOSS train 0.1281924539450872 valid 0.18332528171397872
LOSS train 0.1281924539450872 valid 0.18298155219615006
LOSS train 0.1281924539450872 valid 0.18263723974426588
LOSS train 0.1281924539450872 valid 0.18246373718927714
LOSS train 0.1281924539450872 valid 0.1825210877129289
LOSS train 0.1281924539450872 valid 0.1823104893773552
LOSS train 0.1281924539450872 valid 0.18261828513876086
LOSS train 0.1281924539450872 valid 0.18245147681236268
LOSS train 0.1281924539450872 valid 0.18280505601848876
LOSS train 0.1281924539450872 valid 0.18265168251484398
LOSS train 0.1281924539450872 valid 0.18270832719281316
LOSS train 0.1281924539450872 valid 0.1828469957492148
LOSS train 0.1281924539450872 valid 0.18244963712417162
LOSS train 0.1281924539450872 valid 0.18214495809933612
LOSS train 0.1281924539450872 valid 0.1818275587125258
LOSS train 0.1281924539450872 valid 0.18170425531111264
LOSS train 0.1281924539450872 valid 0.18182470971968637
LOSS train 0.1281924539450872 valid 0.18169265378404548
LOSS train 0.1281924539450872 valid 0.18171629463048541
LOSS train 0.1281924539450872 valid 0.18138055633889497
LOSS train 0.1281924539450872 valid 0.18129924382420554
LOSS train 0.1281924539450872 valid 0.1812337902810076
LOSS train 0.1281924539450872 valid 0.18131217679807118
LOSS train 0.1281924539450872 valid 0.1811692121815174
LOSS train 0.1281924539450872 valid 0.18119192774027165
LOSS train 0.1281924539450872 valid 0.18106139331430823
LOSS train 0.1281924539450872 valid 0.18105528658876816
LOSS train 0.1281924539450872 valid 0.18093498883576228
LOSS train 0.1281924539450872 valid 0.18113637603309057
LOSS train 0.1281924539450872 valid 0.18107435776262867
LOSS train 0.1281924539450872 valid 0.1819964349269867
LOSS train 0.1281924539450872 valid 0.18216765646966512
LOSS train 0.1281924539450872 valid 0.18213866382837296
LOSS train 0.1281924539450872 valid 0.18238648861054554
LOSS train 0.1281924539450872 valid 0.18204419756014095
LOSS train 0.1281924539450872 valid 0.18198151808548596
LOSS train 0.1281924539450872 valid 0.1818364696069197
LOSS train 0.1281924539450872 valid 0.1817096561193466
LOSS train 0.1281924539450872 valid 0.1817058023925011
LOSS train 0.1281924539450872 valid 0.18179710854770273
LOSS train 0.1281924539450872 valid 0.1817771175616904
LOSS train 0.1281924539450872 valid 0.18187401832649544
LOSS train 0.1281924539450872 valid 0.18180044870823622
LOSS train 0.1281924539450872 valid 0.1816451730009932
LOSS train 0.1281924539450872 valid 0.18152683200659575
LOSS train 0.1281924539450872 valid 0.18125074772747016
LOSS train 0.1281924539450872 valid 0.1810517829911011
LOSS train 0.1281924539450872 valid 0.18088613188627994
LOSS train 0.1281924539450872 valid 0.1809620594224298
LOSS train 0.1281924539450872 valid 0.18132749235558654
LOSS train 0.1281924539450872 valid 0.1812372501229956
LOSS train 0.1281924539450872 valid 0.18139829473382624
LOSS train 0.1281924539450872 valid 0.1814004569369204
LOSS train 0.1281924539450872 valid 0.18141673784158382
LOSS train 0.1281924539450872 valid 0.1812913281799749
LOSS train 0.1281924539450872 valid 0.1812626810776705
LOSS train 0.1281924539450872 valid 0.18120822498853179
LOSS train 0.1281924539450872 valid 0.1808799601452691
LOSS train 0.1281924539450872 valid 0.1808402987874367
LOSS train 0.1281924539450872 valid 0.18095157998429853
LOSS train 0.1281924539450872 valid 0.18121389827031767
LOSS train 0.1281924539450872 valid 0.18112456473891295
LOSS train 0.1281924539450872 valid 0.18104532625940112
LOSS train 0.1281924539450872 valid 0.1811900955537406
LOSS train 0.1281924539450872 valid 0.18102454672475438
LOSS train 0.1281924539450872 valid 0.18107856875234615
LOSS train 0.1281924539450872 valid 0.18093840551117193
LOSS train 0.1281924539450872 valid 0.18079773678972916
LOSS train 0.1281924539450872 valid 0.1808473883617309
LOSS train 0.1281924539450872 valid 0.18069277201744324
LOSS train 0.1281924539450872 valid 0.18068106868799697
LOSS train 0.1281924539450872 valid 0.18054661367620742
LOSS train 0.1281924539450872 valid 0.18062844903845537
LOSS train 0.1281924539450872 valid 0.18048787296442462
LOSS train 0.1281924539450872 valid 0.1805359891926249
LOSS train 0.1281924539450872 valid 0.18025511456894752
LOSS train 0.1281924539450872 valid 0.18019129888913066
LOSS train 0.1281924539450872 valid 0.17998512769356753
LOSS train 0.1281924539450872 valid 0.17994477037264375
LOSS train 0.1281924539450872 valid 0.18014928759052062
LOSS train 0.1281924539450872 valid 0.18009529608969738
LOSS train 0.1281924539450872 valid 0.18023444749602122
LOSS train 0.1281924539450872 valid 0.18004262790083886
LOSS train 0.1281924539450872 valid 0.17985168966784407
LOSS train 0.1281924539450872 valid 0.17979231904638876
LOSS train 0.1281924539450872 valid 0.17978966456328707
LOSS train 0.1281924539450872 valid 0.17999336159988946
LOSS train 0.1281924539450872 valid 0.17975389499489855
LOSS train 0.1281924539450872 valid 0.1797692484936668
LOSS train 0.1281924539450872 valid 0.17969384955035317
LOSS train 0.1281924539450872 valid 0.1795397625089838
LOSS train 0.1281924539450872 valid 0.17954517530197162
LOSS train 0.1281924539450872 valid 0.17947184642155964
LOSS train 0.1281924539450872 valid 0.17947583910413262
LOSS train 0.1281924539450872 valid 0.1794359045770933
LOSS train 0.1281924539450872 valid 0.17939801961883134
LOSS train 0.1281924539450872 valid 0.17943278630481702
LOSS train 0.1281924539450872 valid 0.1793054691580839
LOSS train 0.1281924539450872 valid 0.17917054146528244
LOSS train 0.1281924539450872 valid 0.1790865435166293
LOSS train 0.1281924539450872 valid 0.17912189532584005
LOSS train 0.1281924539450872 valid 0.17922483962964794
LOSS train 0.1281924539450872 valid 0.17915744415738366
LOSS train 0.1281924539450872 valid 0.17908506634817944
LOSS train 0.1281924539450872 valid 0.17911703830903714
LOSS train 0.1281924539450872 valid 0.17932160392470423
LOSS train 0.1281924539450872 valid 0.17941652617550322
LOSS train 0.1281924539450872 valid 0.17958744353718228
LOSS train 0.1281924539450872 valid 0.17984443731540073
LOSS train 0.1281924539450872 valid 0.1799088000988645
LOSS train 0.1281924539450872 valid 0.1799702794666876
LOSS train 0.1281924539450872 valid 0.17996084729136338
LOSS train 0.1281924539450872 valid 0.17999808179295582
LOSS train 0.1281924539450872 valid 0.18014392940512983
LOSS train 0.1281924539450872 valid 0.18015623015576396
LOSS train 0.1281924539450872 valid 0.18025698950873936
LOSS train 0.1281924539450872 valid 0.18030844520554584
LOSS train 0.1281924539450872 valid 0.18047255334701945
LOSS train 0.1281924539450872 valid 0.18035705671724628
LOSS train 0.1281924539450872 valid 0.18029124582115608
LOSS train 0.1281924539450872 valid 0.18032151567084448
LOSS train 0.1281924539450872 valid 0.180102575461236
LOSS train 0.1281924539450872 valid 0.18008711108316977
LOSS train 0.1281924539450872 valid 0.18028388515547597
LOSS train 0.1281924539450872 valid 0.1801255990651028
LOSS train 0.1281924539450872 valid 0.18034563044952268
LOSS train 0.1281924539450872 valid 0.18052813453508204
LOSS train 0.1281924539450872 valid 0.18058956806757012
LOSS train 0.1281924539450872 valid 0.18043101869705247
LOSS train 0.1281924539450872 valid 0.1805853709156214
LOSS train 0.1281924539450872 valid 0.1805180001763567
LOSS train 0.1281924539450872 valid 0.18051255251987872
LOSS train 0.1281924539450872 valid 0.18052671486139296
LOSS train 0.1281924539450872 valid 0.18036159831451704
LOSS train 0.1281924539450872 valid 0.18061545574002796
LOSS train 0.1281924539450872 valid 0.18055034107841522
LOSS train 0.1281924539450872 valid 0.18042764842040895
LOSS train 0.1281924539450872 valid 0.18047921669249442
LOSS train 0.1281924539450872 valid 0.1805460171890445
LOSS train 0.1281924539450872 valid 0.18036849541654845
LOSS train 0.1281924539450872 valid 0.18061893496864526
LOSS train 0.1281924539450872 valid 0.18062645847042555
LOSS train 0.1281924539450872 valid 0.1804633694772537
LOSS train 0.1281924539450872 valid 0.1806239724387611
LOSS train 0.1281924539450872 valid 0.18070493109581123
LOSS train 0.1281924539450872 valid 0.1807463226889476
LOSS train 0.1281924539450872 valid 0.18084186778375597
LOSS train 0.1281924539450872 valid 0.18080907419042766
LOSS train 0.1281924539450872 valid 0.1808229462992876
LOSS train 0.1281924539450872 valid 0.1808803995673576
LOSS train 0.1281924539450872 valid 0.1810741083168272
LOSS train 0.1281924539450872 valid 0.18115257801398024
LOSS train 0.1281924539450872 valid 0.18115894943475724
LOSS train 0.1281924539450872 valid 0.1812510510212381
LOSS train 0.1281924539450872 valid 0.18157762223306825
LOSS train 0.1281924539450872 valid 0.18178522221116356
LOSS train 0.1281924539450872 valid 0.18182894892066065
LOSS train 0.1281924539450872 valid 0.1818015667525205
LOSS train 0.1281924539450872 valid 0.18174249948798746
LOSS train 0.1281924539450872 valid 0.1816375790843034
LOSS train 0.1281924539450872 valid 0.18149734315254706
LOSS train 0.1281924539450872 valid 0.18148506782601811
LOSS train 0.1281924539450872 valid 0.18147513797240597
LOSS train 0.1281924539450872 valid 0.181408229247531
LOSS train 0.1281924539450872 valid 0.18109893170020258
LOSS train 0.1281924539450872 valid 0.1810331757940589
LOSS train 0.1281924539450872 valid 0.18106204591376682
LOSS train 0.1281924539450872 valid 0.18110415051903642
LOSS train 0.1281924539450872 valid 0.18112100233564843
LOSS train 0.1281924539450872 valid 0.18108313195796794
LOSS train 0.1281924539450872 valid 0.18106208151827255
LOSS train 0.1281924539450872 valid 0.181068376789456
LOSS train 0.1281924539450872 valid 0.1811019831176462
LOSS train 0.1281924539450872 valid 0.1809350177901717
LOSS train 0.1281924539450872 valid 0.1809173318723293
LOSS train 0.1281924539450872 valid 0.18092213240499788
LOSS train 0.1281924539450872 valid 0.18100110796235858
LOSS train 0.1281924539450872 valid 0.18107532828541126
LOSS train 0.1281924539450872 valid 0.18099036868158225
LOSS train 0.1281924539450872 valid 0.18107711019540074
LOSS train 0.1281924539450872 valid 0.18111302263584714
LOSS train 0.1281924539450872 valid 0.1811338737357819
LOSS train 0.1281924539450872 valid 0.18123658274610838
LOSS train 0.1281924539450872 valid 0.18117476484902278
LOSS train 0.1281924539450872 valid 0.18112817411588517
LOSS train 0.1281924539450872 valid 0.18117148505382413
LOSS train 0.1281924539450872 valid 0.18115635147612347
LOSS train 0.1281924539450872 valid 0.18110357498536345
LOSS train 0.1281924539450872 valid 0.1811352431579353
LOSS train 0.1281924539450872 valid 0.18111191452131986
LOSS train 0.1281924539450872 valid 0.18100205745983433
LOSS train 0.1281924539450872 valid 0.1809886619689781
LOSS train 0.1281924539450872 valid 0.1810367101623166
LOSS train 0.1281924539450872 valid 0.18102662083228685
LOSS train 0.1281924539450872 valid 0.18101293947070074
LOSS train 0.1281924539450872 valid 0.1811997229679705
LOSS train 0.1281924539450872 valid 0.18120987457074936
LOSS train 0.1281924539450872 valid 0.1811292168639955
LOSS train 0.1281924539450872 valid 0.1811118108939521
LOSS train 0.1281924539450872 valid 0.18117509204130444
LOSS train 0.1281924539450872 valid 0.18122109175665574
LOSS train 0.1281924539450872 valid 0.1813922436633454
LOSS train 0.1281924539450872 valid 0.18131889826618136
LOSS train 0.1281924539450872 valid 0.1814931167806049
LOSS train 0.1281924539450872 valid 0.18148939556771923
LOSS train 0.1281924539450872 valid 0.18143254790697305
LOSS train 0.1281924539450872 valid 0.18153686209777256
LOSS train 0.1281924539450872 valid 0.18153825324315292
LOSS train 0.1281924539450872 valid 0.18170236206493495
LOSS train 0.1281924539450872 valid 0.181754376545594
LOSS train 0.1281924539450872 valid 0.18170929790996923
LOSS train 0.1281924539450872 valid 0.18175704410373258
LOSS train 0.1281924539450872 valid 0.18175274485891516
LOSS train 0.1281924539450872 valid 0.18164198532745557
LOSS train 0.1281924539450872 valid 0.18150352529553046
LOSS train 0.1281924539450872 valid 0.18156670687255916
LOSS train 0.1281924539450872 valid 0.1816461233500235
LOSS train 0.1281924539450872 valid 0.1816076981042748
LOSS train 0.1281924539450872 valid 0.18167810363783723
LOSS train 0.1281924539450872 valid 0.18167572156788686
LOSS train 0.1281924539450872 valid 0.1816668864008943
LOSS train 0.1281924539450872 valid 0.18168839201287182
LOSS train 0.1281924539450872 valid 0.18163151044179413
LOSS train 0.1281924539450872 valid 0.18147524662556186
LOSS train 0.1281924539450872 valid 0.18148071628216414
LOSS train 0.1281924539450872 valid 0.1815263657198008
LOSS train 0.1281924539450872 valid 0.18177387347921384
LOSS train 0.1281924539450872 valid 0.18189439903134885
LOSS train 0.1281924539450872 valid 0.18197549143106262
LOSS train 0.1281924539450872 valid 0.18183357536964526
LOSS train 0.1281924539450872 valid 0.18176929841096373
LOSS train 0.1281924539450872 valid 0.18175228687275446
LOSS train 0.1281924539450872 valid 0.18168246388435363
LOSS train 0.1281924539450872 valid 0.18160133959560992
LOSS train 0.1281924539450872 valid 0.18159819229251958
LOSS train 0.1281924539450872 valid 0.18160466693616115
LOSS train 0.1281924539450872 valid 0.18161970520086881
LOSS train 0.1281924539450872 valid 0.18166527475269748
LOSS train 0.1281924539450872 valid 0.18166120316875115
LOSS train 0.1281924539450872 valid 0.1816971629691057
LOSS train 0.1281924539450872 valid 0.1815957337950861
LOSS train 0.1281924539450872 valid 0.18160285657494846
LOSS train 0.1281924539450872 valid 0.1815637420862913
LOSS train 0.1281924539450872 valid 0.1815093900193138
LOSS train 0.1281924539450872 valid 0.1815600149045333
LOSS train 0.1281924539450872 valid 0.18155435770339545
LOSS train 0.1281924539450872 valid 0.1815515486421166
LOSS train 0.1281924539450872 valid 0.18160460027929853
LOSS train 0.1281924539450872 valid 0.18163055047148563
LOSS train 0.1281924539450872 valid 0.18154446265028348
LOSS train 0.1281924539450872 valid 0.1815894536835992
LOSS train 0.1281924539450872 valid 0.18166952517620594
EPOCH 30:
  batch 1 loss: 0.11669688671827316
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.11391358450055122
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.1112247680624326
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.12025954388082027
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.1234045997262001
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.12523761267463365
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.11967328935861588
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.12151175457984209
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.12089522265725666
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.12035631164908409
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.11857098273255608
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.11770453428228696
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.11852171787848839
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.11865032251392092
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.11709428230921427
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.11881786305457354
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.11761107164270737
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.12074549330605401
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.12042282992287685
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.11993315890431404
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.120202117732593
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.12079270251772621
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.1206616853242335
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.12254461615035932
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.12198725283145904
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.12220109311433938
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.12269013844154499
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.12226758524775505
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.12179651702272482
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.12259760349988938
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.12312822812987913
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.12346768565475941
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.12271243514436664
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.12341939789407394
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.12485683134623936
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.12418994700743093
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.12410362970990103
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.12377962666122537
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.12427575924457648
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.12427084185183049
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.12441360659715606
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.12457361675444104
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.125662115770717
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.12522703934122215
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.12544822444518408
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.12495133053997289
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.1248076577135857
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.1249474777529637
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.1249653268225339
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.1251678568124771
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.12477296415497274
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.12491939560725139
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.12476211533231556
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.12463039932427583
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.1250153972343965
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.1251819668603795
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.12527099591598176
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.12478980088028414
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.12510552067877884
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.12493179614345233
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.1251051145010307
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.12506036364263104
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.12514961262543997
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.12489189626649022
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.12498012597744282
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.12476127414089261
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.12460467830967548
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.12484564292518531
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.12500319910654123
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.12530572212168148
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.12570711414159183
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.12594261589563555
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.12610458664289892
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.12612793987264503
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.126136973798275
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.12597252938308215
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.12591935365231005
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.12555450764604104
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.125973351488385
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.12626947956159712
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.12640560252798927
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.1260065689864682
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.1258880931390337
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.1262596550264529
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.1264436983886887
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.1267853045706139
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.12700404012682795
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.1274340356784788
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.12739141193333636
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.12781740269727176
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.12801677553535817
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.12798855947735516
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.12806628315999943
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.1279884141651874
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.12842551697241633
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.1285169196780771
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.12830475379818493
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.1281281850319736
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.1284947537563064
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.12840521298348903
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.12821352430204352
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.12804737526412105
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.12779597142367688
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.12789008694772536
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.12748390578088306
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.1275475337257925
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.12783347613343568
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.12757898232451192
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.12768934530402543
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.12787871157581157
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.12760580760670137
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.1275738193653524
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.12732747177370882
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.12767752475644412
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.1274746654474217
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.12736001077654033
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.12775151539816815
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.1278225545282081
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.12771766883235017
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.1277201451982061
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.12769483891893024
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.12776487528300676
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.12786826018880054
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.12783300035422848
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.127847323179245
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.12792143651417323
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.12793261434618883
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.12767374672694132
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.1275940387285957
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.12756280199839518
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.127470824611551
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.12745916104000626
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.12775025628787234
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.1278323073551726
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.1276172117502601
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.127552249468863
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.1276831008548284
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.1276110460360845
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.12782939110728478
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.12785202903406961
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.1278454321073302
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.12768501756896436
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.12774716619845036
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.12783766123983595
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.12772820617618233
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.12782467192370597
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.12783577151241757
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.12775413681929176
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.12814635338399233
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.12818339695533118
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.12805728268939137
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.12814859879252158
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.12812157924853118
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.12781069103580017
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.12784061965442473
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.12784810407230487
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.12790107703322817
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.12788487428539916
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.1279848001007014
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.12808178993873298
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.1281822498263039
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.12811281913776457
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.12811616817317859
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.12831702087892266
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.12847303517840125
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.128362387553396
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.12820545316277865
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.12812700601560728
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.12793938158708212
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.12823203859083793
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.12829485871115623
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.12828511367876863
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.12808748890209748
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.12802016396803417
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.12811229233230864
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.12794913368468935
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.12774050648266314
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.12746843051039772
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.12765232561020878
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.12747053760621282
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.12755460320915307
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.12752617072764333
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.1274155698757354
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.12734325906342786
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.12736593819147832
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.1273022928946121
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.12738124902713746
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.1273106131306354
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.12726419296844926
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.1273373682247965
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.12736081101819482
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.12725277726228038
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.12731701328655598
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.12705658072816958
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.1270988739071748
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.12727524336351423
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.12738836042318247
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.12753152730639536
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.1274827984933877
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.12763733427971602
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.127677881487863
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.12774845976198074
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.12775832719256724
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.12780840311418562
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.1277227998506732
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.12758330523389058
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.12755224349850042
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.12739581224293664
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.12749380860745052
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.12760042927804446
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.1276058716725964
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.12763560173224728
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.1277251821098753
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.12750812640814024
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.1275917618080627
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.1274915982244743
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.1274929572291638
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.12757044623887867
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.12761375393089094
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.12761793180622838
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.1276414229521924
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.12764625127116838
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.12761535702665824
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.12766183277459017
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.12758316900995043
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.12761754239291218
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.12772489092948677
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.12757246794277116
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.12751303984088147
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.127719887961512
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.12794572205254526
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.12786506704086886
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.12793214097938824
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.12794331954712543
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.12793771247280405
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.12798763303307154
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.12795805415523706
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.12799168866472083
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.128013097529132
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.12788279699161648
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.12803633378130766
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.12798529829491267
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.12805841657727834
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.12811892226216245
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.12808907850056278
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.1281843526152576
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.12818566546869664
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.1282913316221487
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.12835316446770625
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.1283469401896
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.12855043179129225
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.12857406009875594
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.12860561356596326
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.12859788240762207
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.12853214217751635
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.12853507962427102
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.12845461757614454
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.12838002949837565
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.12848522698211853
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.12853180872133144
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.12845337436573714
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.1284159786771727
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.12837744515545016
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.12825818475561612
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.1282880923376893
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.12831672356653034
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.1283122951004389
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.1283126836225613
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.12822315462459863
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.12822451089267378
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.12825478816824207
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.12823744008646293
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.1283778305346276
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.12824933222719354
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.12815227205103094
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.12837496243309285
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.12833157170980847
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.12834454895995503
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.12827657083029387
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.12828187554010323
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.12822751581562797
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.12816094485580498
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.12816234175181643
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.1281419623590691
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.12827923026001245
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.12839169346994453
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.12831560945469328
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.12828798783529136
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.1282302796531301
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.12828137417291774
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.12829034317195212
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.12839330921638503
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.12856683333375754
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.12860002084856942
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.12857196060782772
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.12860317324363701
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.12859099348285785
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.1285400539136573
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.1284856641023454
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.12845177700122198
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.1284912146405128
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.12848810698615004
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.1284853079826525
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.1284392132472835
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.12840075062923745
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.12837652713547346
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.12840424543580325
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.12836051872604853
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.12830981414487833
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.12821203451002797
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.12826833239130653
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.1282440206179252
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.12817581128864625
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.12815122408377136
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.12817948462944181
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.12832194927466822
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.12830398203540674
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.12830583088146816
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.1282248965531681
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.12831541579216718
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.12827971809749664
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.12822852033820953
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.12825149800570756
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.12816400908761555
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.12817073235144982
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.1280657468032252
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.1279552216650149
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.12793468697587165
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.12798093580909775
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.12804251100980873
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.1280653367319856
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.1280576989323978
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.1280013242283383
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.1280230976596564
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.12808745285468315
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.12810947630731834
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.12809281630402858
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.1280848210203577
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.12800392704111987
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.1279979235766565
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.12801329763141894
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.12825344810098932
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.1283002829195459
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.12834453857828712
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.1283445747650188
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.12830369521169305
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.12829974939998356
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.12828370806728973
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.12821651455068314
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.1281225777098111
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.12810954211698977
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.12820704719475048
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.1282859212658223
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.1283269321051358
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.12832366049709454
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.12823419965636196
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.12831751307269104
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.12837533779733673
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.12829172914240686
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.12824129534678327
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.12820419782664308
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.12830079731773275
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.12826660724957129
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.128223811470709
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.12825612958571683
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.12825196902038621
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.12831429574811165
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.1284096014442975
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.12837135660535276
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.12828463319991085
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.1283421622453674
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.1283265243094134
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.12833488508859206
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.12841920935135473
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.12844550043344496
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.12842647982959418
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.1284067819502689
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.12847864140988027
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.128412234378207
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.12840640589986976
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.12839061757085204
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.12832679236746583
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.12840651979489986
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.12840523147800317
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.1283154672616488
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.12826563249983935
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.12829986262768123
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.1282424315541368
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.12824881408162472
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.1281795451274285
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.12814822335682258
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.12816541919446722
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.12826188883101972
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.12828651119761056
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.12830473120453992
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.12823934168225587
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.12814268396618383
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.12817412511862103
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.12815734828101064
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.1281318107061088
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.12812655608627266
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.1280958354473114
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.12803190245891624
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.12808075386772652
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.12812309664340668
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.12807945696077325
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.1280886565474679
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.12813425303308987
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.12808545551337938
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.12808995474039053
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.12806914868218475
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.12811851713046865
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.1281355935674314
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.12819783020638614
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.12825727033686926
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.12833776029471594
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.12832696759086146
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.12829143445243676
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.12834205151244393
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.12830832435616424
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.12837360351569863
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.12839838609946846
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.12831688876622677
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.12830637875100914
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.12829786728410159
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.12820681354179628
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.12822581190764207
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.12818733672393817
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.12810646471652118
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.1280322488830533
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.128084070842255
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.12804323091620096
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.12804821001120584
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.12810360332993867
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.12807866843609975
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.12817429739195818
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.1282207995005276
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.12817191830985078
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.12805351993421216
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.1280333896252242
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.12805661187420628
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.12801456837430258
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.1280079913361347
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.1279874760321937
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.12799007512545318
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.1279947180551531
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.12796132676553407
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.12803835355277574
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.1280286318242948
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.12800352359811465
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.12797202088946513
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.12801508359874772
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.12796003667526687
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.1279607555184858
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.1279128643346357
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.12791684529695072
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.12792800981510652
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.12794833830415422
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.1279535828430149
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.12803649774388126
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.12802727517233495
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.1280207225319111
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.12796070597277348
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.12798546220914558
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.12798850746244514
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.12800401338883735
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.12802327564684177
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.12811502831804955
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.12815204200777672
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.12820929122414995
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.12824741510512216
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.12834943630495818
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.12834943630495818 valid 0.21531322598457336
LOSS train 0.12834943630495818 valid 0.1861230581998825
LOSS train 0.12834943630495818 valid 0.17884994546572366
LOSS train 0.12834943630495818 valid 0.16954874619841576
LOSS train 0.12834943630495818 valid 0.16525411009788513
LOSS train 0.12834943630495818 valid 0.17505613714456558
LOSS train 0.12834943630495818 valid 0.18559111654758453
LOSS train 0.12834943630495818 valid 0.18388846516609192
LOSS train 0.12834943630495818 valid 0.1833815508418613
LOSS train 0.12834943630495818 valid 0.18310029655694962
LOSS train 0.12834943630495818 valid 0.18159771778366782
LOSS train 0.12834943630495818 valid 0.18224932253360748
LOSS train 0.12834943630495818 valid 0.1816079467535019
LOSS train 0.12834943630495818 valid 0.1809775403567723
LOSS train 0.12834943630495818 valid 0.17861854135990143
LOSS train 0.12834943630495818 valid 0.17862325813621283
LOSS train 0.12834943630495818 valid 0.17924480753786423
LOSS train 0.12834943630495818 valid 0.17808696627616882
LOSS train 0.12834943630495818 valid 0.1805145301316914
LOSS train 0.12834943630495818 valid 0.18024624288082122
LOSS train 0.12834943630495818 valid 0.1792543771721068
LOSS train 0.12834943630495818 valid 0.17785545167597858
LOSS train 0.12834943630495818 valid 0.17757801452408667
LOSS train 0.12834943630495818 valid 0.17782178396979967
LOSS train 0.12834943630495818 valid 0.17655984282493592
LOSS train 0.12834943630495818 valid 0.17647773829790261
LOSS train 0.12834943630495818 valid 0.17684509522385067
LOSS train 0.12834943630495818 valid 0.17644537080611503
LOSS train 0.12834943630495818 valid 0.1759061546161257
LOSS train 0.12834943630495818 valid 0.17644119560718535
LOSS train 0.12834943630495818 valid 0.17719487845897675
LOSS train 0.12834943630495818 valid 0.17641093861311674
LOSS train 0.12834943630495818 valid 0.1765811967127251
LOSS train 0.12834943630495818 valid 0.1760274316458141
LOSS train 0.12834943630495818 valid 0.17757348375661033
LOSS train 0.12834943630495818 valid 0.17729152366518974
LOSS train 0.12834943630495818 valid 0.17806595483341733
LOSS train 0.12834943630495818 valid 0.17839702610906802
LOSS train 0.12834943630495818 valid 0.17813576528659233
LOSS train 0.12834943630495818 valid 0.1778780434280634
LOSS train 0.12834943630495818 valid 0.17841808788660096
LOSS train 0.12834943630495818 valid 0.17857373221999123
LOSS train 0.12834943630495818 valid 0.1786082889451537
LOSS train 0.12834943630495818 valid 0.17911219055002386
LOSS train 0.12834943630495818 valid 0.17918513284789192
LOSS train 0.12834943630495818 valid 0.17956945494465207
LOSS train 0.12834943630495818 valid 0.18021010274582722
LOSS train 0.12834943630495818 valid 0.1800542213022709
LOSS train 0.12834943630495818 valid 0.18064555799474522
LOSS train 0.12834943630495818 valid 0.1801605960726738
LOSS train 0.12834943630495818 valid 0.180336132645607
LOSS train 0.12834943630495818 valid 0.17977716057346418
LOSS train 0.12834943630495818 valid 0.18029315994595582
LOSS train 0.12834943630495818 valid 0.18042297219788586
LOSS train 0.12834943630495818 valid 0.18032574111765082
LOSS train 0.12834943630495818 valid 0.17966580390930176
LOSS train 0.12834943630495818 valid 0.17955364469896284
LOSS train 0.12834943630495818 valid 0.17943489346010932
LOSS train 0.12834943630495818 valid 0.17982581990250088
LOSS train 0.12834943630495818 valid 0.17959524169564248
LOSS train 0.12834943630495818 valid 0.17918809979665476
LOSS train 0.12834943630495818 valid 0.1797250985137878
LOSS train 0.12834943630495818 valid 0.17909992616327983
LOSS train 0.12834943630495818 valid 0.17997909313999116
LOSS train 0.12834943630495818 valid 0.18022517195114723
LOSS train 0.12834943630495818 valid 0.17992985338875742
LOSS train 0.12834943630495818 valid 0.1793429515699842
LOSS train 0.12834943630495818 valid 0.1792745517895502
LOSS train 0.12834943630495818 valid 0.1789068108883457
LOSS train 0.12834943630495818 valid 0.17951737684862953
LOSS train 0.12834943630495818 valid 0.1791509062471524
LOSS train 0.12834943630495818 valid 0.17934496576587358
LOSS train 0.12834943630495818 valid 0.17934062403358825
LOSS train 0.12834943630495818 valid 0.179337772565919
LOSS train 0.12834943630495818 valid 0.17971335371335348
LOSS train 0.12834943630495818 valid 0.18012867162102147
LOSS train 0.12834943630495818 valid 0.17996953040748448
LOSS train 0.12834943630495818 valid 0.1797502142114517
LOSS train 0.12834943630495818 valid 0.17943859251239633
LOSS train 0.12834943630495818 valid 0.1788006793707609
LOSS train 0.12834943630495818 valid 0.17846237141409038
LOSS train 0.12834943630495818 valid 0.17893350178875575
LOSS train 0.12834943630495818 valid 0.17876854000321354
LOSS train 0.12834943630495818 valid 0.17876829384338289
LOSS train 0.12834943630495818 valid 0.17849420221412884
LOSS train 0.12834943630495818 valid 0.17829916660868844
LOSS train 0.12834943630495818 valid 0.17809273645110513
LOSS train 0.12834943630495818 valid 0.17778825776820834
LOSS train 0.12834943630495818 valid 0.1782993706089727
LOSS train 0.12834943630495818 valid 0.17827615920040343
LOSS train 0.12834943630495818 valid 0.17836208936277326
LOSS train 0.12834943630495818 valid 0.17830805610055508
LOSS train 0.12834943630495818 valid 0.17808305816624753
LOSS train 0.12834943630495818 valid 0.1782659017976294
LOSS train 0.12834943630495818 valid 0.17816690833945023
LOSS train 0.12834943630495818 valid 0.17822955641895533
LOSS train 0.12834943630495818 valid 0.17805560593752517
LOSS train 0.12834943630495818 valid 0.17830590645269473
LOSS train 0.12834943630495818 valid 0.1783457827387434
LOSS train 0.12834943630495818 valid 0.17843953311443328
LOSS train 0.12834943630495818 valid 0.17864099336732733
LOSS train 0.12834943630495818 valid 0.17888116646631091
LOSS train 0.12834943630495818 valid 0.17853892095459317
LOSS train 0.12834943630495818 valid 0.1786908440005321
LOSS train 0.12834943630495818 valid 0.17881016191982088
LOSS train 0.12834943630495818 valid 0.17926216069257483
LOSS train 0.12834943630495818 valid 0.17916075564990533
LOSS train 0.12834943630495818 valid 0.17931684092790992
LOSS train 0.12834943630495818 valid 0.17972412445676436
LOSS train 0.12834943630495818 valid 0.1797605648636818
LOSS train 0.12834943630495818 valid 0.17972443796492912
LOSS train 0.12834943630495818 valid 0.17953280106719052
LOSS train 0.12834943630495818 valid 0.17956856592039092
LOSS train 0.12834943630495818 valid 0.1798995699276004
LOSS train 0.12834943630495818 valid 0.1801309418419133
LOSS train 0.12834943630495818 valid 0.18035093553621193
LOSS train 0.12834943630495818 valid 0.18028258920734763
LOSS train 0.12834943630495818 valid 0.17986553149708248
LOSS train 0.12834943630495818 valid 0.17952258909950738
LOSS train 0.12834943630495818 valid 0.17917949656645457
LOSS train 0.12834943630495818 valid 0.17900797392218565
LOSS train 0.12834943630495818 valid 0.1790628107111962
LOSS train 0.12834943630495818 valid 0.17885302794658073
LOSS train 0.12834943630495818 valid 0.17915779135880933
LOSS train 0.12834943630495818 valid 0.17899086809158324
LOSS train 0.12834943630495818 valid 0.1793321640718551
LOSS train 0.12834943630495818 valid 0.17918102314153056
LOSS train 0.12834943630495818 valid 0.17922576260752976
LOSS train 0.12834943630495818 valid 0.1793585026911063
LOSS train 0.12834943630495818 valid 0.1789645980183895
LOSS train 0.12834943630495818 valid 0.17866336412102213
LOSS train 0.12834943630495818 valid 0.1783522101062717
LOSS train 0.12834943630495818 valid 0.1782293029521641
LOSS train 0.12834943630495818 valid 0.17835111949426025
LOSS train 0.12834943630495818 valid 0.17822151128892547
LOSS train 0.12834943630495818 valid 0.17824060272644549
LOSS train 0.12834943630495818 valid 0.17791083661744195
LOSS train 0.12834943630495818 valid 0.17782875334007153
LOSS train 0.12834943630495818 valid 0.17776397106458816
LOSS train 0.12834943630495818 valid 0.17784078866243364
LOSS train 0.12834943630495818 valid 0.17770093721700897
LOSS train 0.12834943630495818 valid 0.17771438446263194
LOSS train 0.12834943630495818 valid 0.17758714954753022
LOSS train 0.12834943630495818 valid 0.1775822267971105
LOSS train 0.12834943630495818 valid 0.1774673598593679
LOSS train 0.12834943630495818 valid 0.1776626457090247
LOSS train 0.12834943630495818 valid 0.1775987900438763
LOSS train 0.12834943630495818 valid 0.17850721285149856
LOSS train 0.12834943630495818 valid 0.17867815004499166
LOSS train 0.12834943630495818 valid 0.1786462238430977
LOSS train 0.12834943630495818 valid 0.17889315353718815
LOSS train 0.12834943630495818 valid 0.17855810785764142
LOSS train 0.12834943630495818 valid 0.1784926052966149
LOSS train 0.12834943630495818 valid 0.17835098553400536
LOSS train 0.12834943630495818 valid 0.1782233803502975
LOSS train 0.12834943630495818 valid 0.1782208258907
LOSS train 0.12834943630495818 valid 0.17830928192017184
LOSS train 0.12834943630495818 valid 0.1782879280515864
LOSS train 0.12834943630495818 valid 0.17838561965984367
LOSS train 0.12834943630495818 valid 0.17831143820658327
LOSS train 0.12834943630495818 valid 0.17815508568509025
LOSS train 0.12834943630495818 valid 0.17803652235019354
LOSS train 0.12834943630495818 valid 0.17776293988608144
LOSS train 0.12834943630495818 valid 0.1775639335193285
LOSS train 0.12834943630495818 valid 0.1774017847848661
LOSS train 0.12834943630495818 valid 0.17747653042336545
LOSS train 0.12834943630495818 valid 0.177843007588101
LOSS train 0.12834943630495818 valid 0.17775221151255427
LOSS train 0.12834943630495818 valid 0.17791353818580244
LOSS train 0.12834943630495818 valid 0.17791618967757505
LOSS train 0.12834943630495818 valid 0.17793343691100852
LOSS train 0.12834943630495818 valid 0.17780848375933114
LOSS train 0.12834943630495818 valid 0.1777809704142499
LOSS train 0.12834943630495818 valid 0.17772999594266387
LOSS train 0.12834943630495818 valid 0.17740602872201375
LOSS train 0.12834943630495818 valid 0.17736523695798082
LOSS train 0.12834943630495818 valid 0.1774773755568569
LOSS train 0.12834943630495818 valid 0.17773852350839067
LOSS train 0.12834943630495818 valid 0.17765152375291846
LOSS train 0.12834943630495818 valid 0.17757239188585017
LOSS train 0.12834943630495818 valid 0.1777136436565805
LOSS train 0.12834943630495818 valid 0.1775485232554294
LOSS train 0.12834943630495818 valid 0.17759824446478828
LOSS train 0.12834943630495818 valid 0.17746065739218309
LOSS train 0.12834943630495818 valid 0.1773230520857347
LOSS train 0.12834943630495818 valid 0.1773736462077146
LOSS train 0.12834943630495818 valid 0.17722140559378793
LOSS train 0.12834943630495818 valid 0.17720776982605457
LOSS train 0.12834943630495818 valid 0.17707756954053092
LOSS train 0.12834943630495818 valid 0.17715956086390897
LOSS train 0.12834943630495818 valid 0.17701952311537028
LOSS train 0.12834943630495818 valid 0.1770653510854269
LOSS train 0.12834943630495818 valid 0.1767889455038031
LOSS train 0.12834943630495818 valid 0.17673210838099115
LOSS train 0.12834943630495818 valid 0.17652845329199082
LOSS train 0.12834943630495818 valid 0.17648895290129038
LOSS train 0.12834943630495818 valid 0.17668913350190002
LOSS train 0.12834943630495818 valid 0.17663483577545244
LOSS train 0.12834943630495818 valid 0.17677099860493262
LOSS train 0.12834943630495818 valid 0.17657705403864385
LOSS train 0.12834943630495818 valid 0.17638678856156953
LOSS train 0.12834943630495818 valid 0.17632995383574232
LOSS train 0.12834943630495818 valid 0.17632887508775213
LOSS train 0.12834943630495818 valid 0.17652957156008364
LOSS train 0.12834943630495818 valid 0.17629445900277393
LOSS train 0.12834943630495818 valid 0.17630801805593435
LOSS train 0.12834943630495818 valid 0.17623306695677807
LOSS train 0.12834943630495818 valid 0.17608145860811839
LOSS train 0.12834943630495818 valid 0.17608783240808826
LOSS train 0.12834943630495818 valid 0.17601418516465595
LOSS train 0.12834943630495818 valid 0.17601677443461394
LOSS train 0.12834943630495818 valid 0.17597560226073805
LOSS train 0.12834943630495818 valid 0.1759368355425311
LOSS train 0.12834943630495818 valid 0.175968830134267
LOSS train 0.12834943630495818 valid 0.17584340135718501
LOSS train 0.12834943630495818 valid 0.17571127387108626
LOSS train 0.12834943630495818 valid 0.17562998110248196
LOSS train 0.12834943630495818 valid 0.17566436911941669
LOSS train 0.12834943630495818 valid 0.17576800987600735
LOSS train 0.12834943630495818 valid 0.17569830905307424
LOSS train 0.12834943630495818 valid 0.17562556442092447
LOSS train 0.12834943630495818 valid 0.1756572215943723
LOSS train 0.12834943630495818 valid 0.17585976548793605
LOSS train 0.12834943630495818 valid 0.1759510511931564
LOSS train 0.12834943630495818 valid 0.1761165170537101
LOSS train 0.12834943630495818 valid 0.17636939975540195
LOSS train 0.12834943630495818 valid 0.17642984115867363
LOSS train 0.12834943630495818 valid 0.17649177474933758
LOSS train 0.12834943630495818 valid 0.17647977231891915
LOSS train 0.12834943630495818 valid 0.17651556853366934
LOSS train 0.12834943630495818 valid 0.17665719534411575
LOSS train 0.12834943630495818 valid 0.1766684382896999
LOSS train 0.12834943630495818 valid 0.17677100444044677
LOSS train 0.12834943630495818 valid 0.1768202885475933
LOSS train 0.12834943630495818 valid 0.17697930335998535
LOSS train 0.12834943630495818 valid 0.1768670734965195
LOSS train 0.12834943630495818 valid 0.17679954530820566
LOSS train 0.12834943630495818 valid 0.17682422503453343
LOSS train 0.12834943630495818 valid 0.17660630098446642
LOSS train 0.12834943630495818 valid 0.17659289352595806
LOSS train 0.12834943630495818 valid 0.1767878202234561
LOSS train 0.12834943630495818 valid 0.17663354882277732
LOSS train 0.12834943630495818 valid 0.17685117814766527
LOSS train 0.12834943630495818 valid 0.17703143085856907
LOSS train 0.12834943630495818 valid 0.17709356984313654
LOSS train 0.12834943630495818 valid 0.17693603662698248
LOSS train 0.12834943630495818 valid 0.1770869632843535
LOSS train 0.12834943630495818 valid 0.1770207260645205
LOSS train 0.12834943630495818 valid 0.17701438260844435
LOSS train 0.12834943630495818 valid 0.17702646631002425
LOSS train 0.12834943630495818 valid 0.1768622539670344
LOSS train 0.12834943630495818 valid 0.17711267330580288
LOSS train 0.12834943630495818 valid 0.17704768767469956
LOSS train 0.12834943630495818 valid 0.17692731948584084
LOSS train 0.12834943630495818 valid 0.17697914768667783
LOSS train 0.12834943630495818 valid 0.1770444554858841
LOSS train 0.12834943630495818 valid 0.17686828646214556
LOSS train 0.12834943630495818 valid 0.1771147032809812
LOSS train 0.12834943630495818 valid 0.17712058477880413
LOSS train 0.12834943630495818 valid 0.17695968586664934
LOSS train 0.12834943630495818 valid 0.17711735845068863
LOSS train 0.12834943630495818 valid 0.17719512842311205
LOSS train 0.12834943630495818 valid 0.17723662437368254
LOSS train 0.12834943630495818 valid 0.17733084980511304
LOSS train 0.12834943630495818 valid 0.17729936641342234
LOSS train 0.12834943630495818 valid 0.1773134750082977
LOSS train 0.12834943630495818 valid 0.17737191318349446
LOSS train 0.12834943630495818 valid 0.17756343866462138
LOSS train 0.12834943630495818 valid 0.17763903144345408
LOSS train 0.12834943630495818 valid 0.1776459292681129
LOSS train 0.12834943630495818 valid 0.17773715915275237
LOSS train 0.12834943630495818 valid 0.17805967699078953
LOSS train 0.12834943630495818 valid 0.17826434729736804
LOSS train 0.12834943630495818 valid 0.1783081561218213
LOSS train 0.12834943630495818 valid 0.17828217273408717
LOSS train 0.12834943630495818 valid 0.17822179937924165
LOSS train 0.12834943630495818 valid 0.1781181269174018
LOSS train 0.12834943630495818 valid 0.17798131275520052
LOSS train 0.12834943630495818 valid 0.1779701294223895
LOSS train 0.12834943630495818 valid 0.17796016170510223
LOSS train 0.12834943630495818 valid 0.1778939694590416
LOSS train 0.12834943630495818 valid 0.17758914130799314
LOSS train 0.12834943630495818 valid 0.17752408165181904
LOSS train 0.12834943630495818 valid 0.17755325867886274
LOSS train 0.12834943630495818 valid 0.17759593045502378
LOSS train 0.12834943630495818 valid 0.17761132735889273
LOSS train 0.12834943630495818 valid 0.1775748124832891
LOSS train 0.12834943630495818 valid 0.1775550603763097
LOSS train 0.12834943630495818 valid 0.1775605721354072
LOSS train 0.12834943630495818 valid 0.1775939195834357
LOSS train 0.12834943630495818 valid 0.17742937219511604
LOSS train 0.12834943630495818 valid 0.1774127381305172
LOSS train 0.12834943630495818 valid 0.1774186896789603
LOSS train 0.12834943630495818 valid 0.17749550386148244
LOSS train 0.12834943630495818 valid 0.17756871212336978
LOSS train 0.12834943630495818 valid 0.1774844064603786
LOSS train 0.12834943630495818 valid 0.17756821345600615
LOSS train 0.12834943630495818 valid 0.17760407704634953
LOSS train 0.12834943630495818 valid 0.17762602885630618
LOSS train 0.12834943630495818 valid 0.17772877077261606
LOSS train 0.12834943630495818 valid 0.17766208966507072
LOSS train 0.12834943630495818 valid 0.17761796821426873
LOSS train 0.12834943630495818 valid 0.1776583886677676
LOSS train 0.12834943630495818 valid 0.17764286858666883
LOSS train 0.12834943630495818 valid 0.17758825875696588
LOSS train 0.12834943630495818 valid 0.17761990817543727
LOSS train 0.12834943630495818 valid 0.1775963073444677
LOSS train 0.12834943630495818 valid 0.1774868286855809
LOSS train 0.12834943630495818 valid 0.1774726042276833
LOSS train 0.12834943630495818 valid 0.1775176996665616
LOSS train 0.12834943630495818 valid 0.17750817350444303
LOSS train 0.12834943630495818 valid 0.17749498025155985
LOSS train 0.12834943630495818 valid 0.1776802327019719
LOSS train 0.12834943630495818 valid 0.1776895179964934
LOSS train 0.12834943630495818 valid 0.17761152154869503
LOSS train 0.12834943630495818 valid 0.17759686750890333
LOSS train 0.12834943630495818 valid 0.17765724311115613
LOSS train 0.12834943630495818 valid 0.17770175692045465
LOSS train 0.12834943630495818 valid 0.17787126851119217
LOSS train 0.12834943630495818 valid 0.17779879732988774
LOSS train 0.12834943630495818 valid 0.17796977486380164
LOSS train 0.12834943630495818 valid 0.1779678315849778
LOSS train 0.12834943630495818 valid 0.17791007722125335
LOSS train 0.12834943630495818 valid 0.1780111155944106
LOSS train 0.12834943630495818 valid 0.17801354046051318
LOSS train 0.12834943630495818 valid 0.17817508447572497
LOSS train 0.12834943630495818 valid 0.17822701199156793
LOSS train 0.12834943630495818 valid 0.1781819090701458
LOSS train 0.12834943630495818 valid 0.17823017177973113
LOSS train 0.12834943630495818 valid 0.1782241173765876
LOSS train 0.12834943630495818 valid 0.1781136918464096
LOSS train 0.12834943630495818 valid 0.17797438961913786
LOSS train 0.12834943630495818 valid 0.17803681576932157
LOSS train 0.12834943630495818 valid 0.17811426270507766
LOSS train 0.12834943630495818 valid 0.17807750661871327
LOSS train 0.12834943630495818 valid 0.17814781264002835
LOSS train 0.12834943630495818 valid 0.17814369420092607
LOSS train 0.12834943630495818 valid 0.17813488761701526
LOSS train 0.12834943630495818 valid 0.17815720406429605
LOSS train 0.12834943630495818 valid 0.17810038963661473
LOSS train 0.12834943630495818 valid 0.17794629490095848
LOSS train 0.12834943630495818 valid 0.17795263226443564
LOSS train 0.12834943630495818 valid 0.17799812272929588
LOSS train 0.12834943630495818 valid 0.1782438635046399
LOSS train 0.12834943630495818 valid 0.17836347874523936
LOSS train 0.12834943630495818 valid 0.17844452739106437
LOSS train 0.12834943630495818 valid 0.17830564884699732
LOSS train 0.12834943630495818 valid 0.17824310423998996
LOSS train 0.12834943630495818 valid 0.1782260952159805
LOSS train 0.12834943630495818 valid 0.17815806882722038
LOSS train 0.12834943630495818 valid 0.1780764867692252
LOSS train 0.12834943630495818 valid 0.17807459877803922
LOSS train 0.12834943630495818 valid 0.17808039575055368
LOSS train 0.12834943630495818 valid 0.1780957868796284
LOSS train 0.12834943630495818 valid 0.17813890051673836
LOSS train 0.12834943630495818 valid 0.17813390196206863
LOSS train 0.12834943630495818 valid 0.1781690774761996
LOSS train 0.12834943630495818 valid 0.17806921025228234
LOSS train 0.12834943630495818 valid 0.17807498063715718
LOSS train 0.12834943630495818 valid 0.17803555801510812
LOSS train 0.12834943630495818 valid 0.17798285450466453
LOSS train 0.12834943630495818 valid 0.17803332205470754
LOSS train 0.12834943630495818 valid 0.17802455709328666
LOSS train 0.12834943630495818 valid 0.17802157248933237
LOSS train 0.12834943630495818 valid 0.17807321144293434
LOSS train 0.12834943630495818 valid 0.17809921234357554
LOSS train 0.12834943630495818 valid 0.17801489175502871
LOSS train 0.12834943630495818 valid 0.17805924384004396
LOSS train 0.12834943630495818 valid 0.17813818568621226
bichrom_seq(
  (conv1d): Conv1d(4, 256, kernel_size=(24,), stride=(1,))
  (relu): ReLU()
  (batchNorm1d): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (maxPool1d): MaxPool1d(kernel_size=15, stride=15, padding=0, dilation=1, ceil_mode=True)
  (lstm): LSTM(256, 32, batch_first=True)
  (tanh): Tanh()
  (model_dense_repeat): Sequential(
    (0): Linear(in_features=32, out_features=512, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.5, inplace=False)
    (3): Linear(in_features=512, out_features=512, bias=True)
    (4): ReLU()
    (5): Dropout(p=0.5, inplace=False)
    (6): Linear(in_features=512, out_features=512, bias=True)
    (7): ReLU()
    (8): Dropout(p=0.5, inplace=False)
  )
  (linear): Linear(in_features=512, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)
bimodal_network(
  (base_model): bichrom_seq(
    (conv1d): Conv1d(4, 256, kernel_size=(24,), stride=(1,))
    (relu): ReLU()
    (batchNorm1d): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (maxPool1d): MaxPool1d(kernel_size=15, stride=15, padding=0, dilation=1, ceil_mode=True)
    (lstm): LSTM(256, 32, batch_first=True)
    (tanh): Tanh()
    (model_dense_repeat): Sequential(
      (0): Linear(in_features=32, out_features=512, bias=True)
      (1): ReLU()
      (2): Dropout(p=0.5, inplace=False)
      (3): Linear(in_features=512, out_features=512, bias=True)
      (4): ReLU()
      (5): Dropout(p=0.5, inplace=False)
      (6): Linear(in_features=512, out_features=512, bias=True)
      (7): ReLU()
      (8): Dropout(p=0.5, inplace=False)
    )
    (linear): Linear(in_features=512, out_features=1, bias=True)
    (sigmoid): Sigmoid()
  )
  (linear): Linear(in_features=512, out_features=1, bias=True)
  (tanh): Tanh()
  (model): bichrom_chrom(
    (_reshape): _reshape()
    (conv1d): Conv1d(12, 15, kernel_size=(1,), stride=(1,), padding=valid)
    (relu): ReLU()
    (lstm): LSTM(15, 5, batch_first=True)
    (relu2): ReLU()
    (linear): Linear(in_features=5, out_features=1, bias=True)
    (tanh): Tanh()
  )
  (linear2): Linear(in_features=2, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)
0.4908637873754153
0.6993355481727574
Elapsed: 13hrs 34min 16sec
