Training seq
DEVICE = cpu
####################
Total Parameters = 605185
Total Trainable Parameters = 605185
bichrom_seq(
  (conv1d): Conv1d(4, 256, kernel_size=(24,), stride=(1,))
  (relu): ReLU()
  (batchNorm1d): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (maxPool1d): MaxPool1d(kernel_size=15, stride=15, padding=0, dilation=1, ceil_mode=True)
  (lstm): LSTM(256, 32, batch_first=True)
  (tanh): Tanh()
  (model_dense_repeat): Sequential(
    (0): Linear(in_features=32, out_features=512, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.5, inplace=False)
    (3): Linear(in_features=512, out_features=512, bias=True)
    (4): ReLU()
    (5): Dropout(p=0.5, inplace=False)
    (6): Linear(in_features=512, out_features=512, bias=True)
    (7): ReLU()
    (8): Dropout(p=0.5, inplace=False)
  )
  (linear): Linear(in_features=512, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)
####################
EPOCH 1:
  batch 1 loss: 0.6931483745574951
  batch 2 loss: 0.694069504737854
  batch 3 loss: 0.6947539250055949
  batch 4 loss: 0.6947050541639328
  batch 5 loss: 0.6941787481307984
  batch 6 loss: 0.6939417918523153
  batch 7 loss: 0.6938325847898211
  batch 8 loss: 0.6938738971948624
  batch 9 loss: 0.6934881342781914
  batch 10 loss: 0.6934234499931335
  batch 11 loss: 0.6927264603701505
  batch 12 loss: 0.6920359234015147
  batch 13 loss: 0.6909180696193988
  batch 14 loss: 0.6897823086806706
  batch 15 loss: 0.6884895881017049
  batch 16 loss: 0.6870285272598267
  batch 17 loss: 0.6860635175424463
  batch 18 loss: 0.6854706340365939
  batch 19 loss: 0.6847526462454545
  batch 20 loss: 0.6830220490694046
  batch 21 loss: 0.6825976031167167
  batch 22 loss: 0.682373130863363
  batch 23 loss: 0.6817817273347274
  batch 24 loss: 0.6814072678486506
  batch 25 loss: 0.6812256598472595
  batch 26 loss: 0.679749573652561
  batch 27 loss: 0.6793135470814176
  batch 28 loss: 0.6780196215425219
  batch 29 loss: 0.6786324217401701
  batch 30 loss: 0.6780501385529836
  batch 31 loss: 0.6774648562554391
  batch 32 loss: 0.677092993631959
  batch 33 loss: 0.6767267458366625
  batch 34 loss: 0.6761741234975702
  batch 35 loss: 0.6761451857430595
  batch 36 loss: 0.6755132526159286
  batch 37 loss: 0.6749172838958534
  batch 38 loss: 0.6743735373020172
  batch 39 loss: 0.6738185897851602
  batch 40 loss: 0.6731837689876556
  batch 41 loss: 0.6722527538857809
  batch 42 loss: 0.6720749948705945
  batch 43 loss: 0.672086869561395
  batch 44 loss: 0.6712010231885043
  batch 45 loss: 0.6708850979804992
  batch 46 loss: 0.6705045363177424
  batch 47 loss: 0.6707059614201809
  batch 48 loss: 0.6701945289969444
  batch 49 loss: 0.6695802309075181
  batch 50 loss: 0.669350938796997
  batch 51 loss: 0.6691634409567889
  batch 52 loss: 0.668879237312537
  batch 53 loss: 0.6686785176115216
  batch 54 loss: 0.667929455086037
  batch 55 loss: 0.6673982587727634
  batch 56 loss: 0.6667452145900045
  batch 57 loss: 0.6667614736055073
  batch 58 loss: 0.6668755155185173
  batch 59 loss: 0.6661718184665099
  batch 60 loss: 0.6659451931715011
  batch 61 loss: 0.6654194642285831
  batch 62 loss: 0.6652769561736814
  batch 63 loss: 0.6646025994467357
  batch 64 loss: 0.6642411537468433
  batch 65 loss: 0.6638760401652409
  batch 66 loss: 0.6636508802572886
  batch 67 loss: 0.6634673209332708
  batch 68 loss: 0.663211670868537
  batch 69 loss: 0.6631004369777181
  batch 70 loss: 0.6625116305691855
  batch 71 loss: 0.6619459894341482
  batch 72 loss: 0.6616486600703664
  batch 73 loss: 0.6614507167306665
  batch 74 loss: 0.660896902148788
  batch 75 loss: 0.660636568069458
  batch 76 loss: 0.660524908649294
  batch 77 loss: 0.6601419332739594
  batch 78 loss: 0.6596912741661072
  batch 79 loss: 0.6591856570183476
  batch 80 loss: 0.6586432918906212
  batch 81 loss: 0.657985089737692
  batch 82 loss: 0.6579750958012371
  batch 83 loss: 0.6575617510152151
  batch 84 loss: 0.6570576408079692
  batch 85 loss: 0.6563866075347452
  batch 86 loss: 0.656526539907899
  batch 87 loss: 0.6560972127421149
  batch 88 loss: 0.6554141254587607
  batch 89 loss: 0.6548741942041376
  batch 90 loss: 0.6542221691873339
  batch 91 loss: 0.6536596915224097
  batch 92 loss: 0.6529664066822632
  batch 93 loss: 0.6524668194914377
  batch 94 loss: 0.6518903387353775
  batch 95 loss: 0.6513212749832555
  batch 96 loss: 0.6505057116349539
  batch 97 loss: 0.6494970997584235
  batch 98 loss: 0.6491984560781595
  batch 99 loss: 0.6488239801291263
  batch 100 loss: 0.6483651399612427
  batch 101 loss: 0.6475994362689481
  batch 102 loss: 0.6470046656973222
  batch 103 loss: 0.6466291350068398
  batch 104 loss: 0.6459565901985536
  batch 105 loss: 0.6451282926968166
  batch 106 loss: 0.6443420777905662
  batch 107 loss: 0.6432515939819479
  batch 108 loss: 0.6425816510562543
  batch 109 loss: 0.6420184089503157
  batch 110 loss: 0.6412707220424305
  batch 111 loss: 0.6406319420616906
  batch 112 loss: 0.639622147061995
  batch 113 loss: 0.6391191461444956
  batch 114 loss: 0.6388201828588519
  batch 115 loss: 0.6382474199585293
  batch 116 loss: 0.6376516541530346
  batch 117 loss: 0.6368487028994112
  batch 118 loss: 0.6363232585333162
  batch 119 loss: 0.6356243160592407
  batch 120 loss: 0.6347984994451205
  batch 121 loss: 0.6336969743090227
  batch 122 loss: 0.6329033780293386
  batch 123 loss: 0.6320390211857432
  batch 124 loss: 0.6315030676703299
  batch 125 loss: 0.6307119398117066
  batch 126 loss: 0.6296832745983487
  batch 127 loss: 0.6291631511815889
  batch 128 loss: 0.6282138880342245
  batch 129 loss: 0.6273897257886192
  batch 130 loss: 0.626491400370231
  batch 131 loss: 0.6258062260751506
  batch 132 loss: 0.6249116419842748
  batch 133 loss: 0.6243369395571544
  batch 134 loss: 0.6234078754240008
  batch 135 loss: 0.6225436493202492
  batch 136 loss: 0.6215729073566549
  batch 137 loss: 0.6206881260349802
  batch 138 loss: 0.6199883676093557
  batch 139 loss: 0.6193720226665195
  batch 140 loss: 0.6182230417217527
  batch 141 loss: 0.6174746603830487
  batch 142 loss: 0.6167013170853467
  batch 143 loss: 0.6155456487949078
  batch 144 loss: 0.6146371213512288
  batch 145 loss: 0.6137849349400093
  batch 146 loss: 0.6131430086615968
  batch 147 loss: 0.6125958750442583
  batch 148 loss: 0.6119594861929482
  batch 149 loss: 0.6111120967656974
  batch 150 loss: 0.6101372802257538
  batch 151 loss: 0.6095689728560038
  batch 152 loss: 0.6089878227365645
  batch 153 loss: 0.6080430424680897
  batch 154 loss: 0.6071941223624465
  batch 155 loss: 0.606341673866395
  batch 156 loss: 0.60582085335866
  batch 157 loss: 0.6053082236818447
  batch 158 loss: 0.6043420977230314
  batch 159 loss: 0.60352460993161
  batch 160 loss: 0.6028840411454439
  batch 161 loss: 0.6021787961077246
  batch 162 loss: 0.6013224273920059
  batch 163 loss: 0.6004312091444167
  batch 164 loss: 0.5996637851363276
  batch 165 loss: 0.5987040521520557
  batch 166 loss: 0.5980357023247753
  batch 167 loss: 0.5971299543709099
  batch 168 loss: 0.5964073787132899
  batch 169 loss: 0.5955562256497039
  batch 170 loss: 0.5948742985725403
  batch 171 loss: 0.5941637322219492
  batch 172 loss: 0.5936305713514949
  batch 173 loss: 0.5930145730862039
  batch 174 loss: 0.5922792300410654
  batch 175 loss: 0.5916366934776306
  batch 176 loss: 0.5906778304752979
  batch 177 loss: 0.5897755388822933
  batch 178 loss: 0.589306234141414
  batch 179 loss: 0.5888195245958573
  batch 180 loss: 0.5879987039499812
  batch 181 loss: 0.5875211547422146
  batch 182 loss: 0.5865825648818698
  batch 183 loss: 0.5858795152661579
  batch 184 loss: 0.5852610616256362
  batch 185 loss: 0.5848166325607815
  batch 186 loss: 0.5842200766007105
  batch 187 loss: 0.5834204591213064
  batch 188 loss: 0.5825408584576972
  batch 189 loss: 0.5815452425568192
  batch 190 loss: 0.5807645862039766
  batch 191 loss: 0.5798696819400289
  batch 192 loss: 0.5791433222281436
  batch 193 loss: 0.5787034919533705
  batch 194 loss: 0.578258546818163
  batch 195 loss: 0.5777042326254722
  batch 196 loss: 0.5768078716129673
  batch 197 loss: 0.5760149079833539
  batch 198 loss: 0.5756613354791295
  batch 199 loss: 0.5750250407499284
  batch 200 loss: 0.5744269283115864
  batch 201 loss: 0.5737684919169886
  batch 202 loss: 0.5730597295678488
  batch 203 loss: 0.5724029527802773
  batch 204 loss: 0.5715522611258077
  batch 205 loss: 0.5709613539823671
  batch 206 loss: 0.5702518330326358
  batch 207 loss: 0.5695989382439766
  batch 208 loss: 0.5686572155413719
  batch 209 loss: 0.5679441346791372
  batch 210 loss: 0.5673094969420206
  batch 211 loss: 0.5668105348874043
  batch 212 loss: 0.5662803499484962
  batch 213 loss: 0.5655706708140217
  batch 214 loss: 0.5648338312857619
  batch 215 loss: 0.5640941082045089
  batch 216 loss: 0.5634398416236595
  batch 217 loss: 0.5627988453284936
  batch 218 loss: 0.5621766614804574
  batch 219 loss: 0.5616265268630634
  batch 220 loss: 0.5609061800620773
  batch 221 loss: 0.5604118091218612
  batch 222 loss: 0.5599678169499647
  batch 223 loss: 0.559401824468989
  batch 224 loss: 0.5588010685252291
  batch 225 loss: 0.558045850859748
  batch 226 loss: 0.5575916382063807
  batch 227 loss: 0.556886721812681
  batch 228 loss: 0.556258208396142
  batch 229 loss: 0.5555966784079522
  batch 230 loss: 0.5551092631143072
  batch 231 loss: 0.5545039827173407
  batch 232 loss: 0.5537231713533401
  batch 233 loss: 0.5531831456612109
  batch 234 loss: 0.5528436021672355
  batch 235 loss: 0.552335614853717
  batch 236 loss: 0.5517190725116407
  batch 237 loss: 0.5512872720569498
  batch 238 loss: 0.5506999696753606
  batch 239 loss: 0.550050293925417
  batch 240 loss: 0.5495710466057062
  batch 241 loss: 0.5491781493192887
  batch 242 loss: 0.5484287996183742
  batch 243 loss: 0.5480076532059737
  batch 244 loss: 0.5475388252344288
  batch 245 loss: 0.5471126033335316
  batch 246 loss: 0.546489736777011
  batch 247 loss: 0.5459568093904117
  batch 248 loss: 0.5455178217061104
  batch 249 loss: 0.5448421749723963
  batch 250 loss: 0.5443359092473984
  batch 251 loss: 0.5437998354909904
  batch 252 loss: 0.5431970269197509
  batch 253 loss: 0.5425582438589556
  batch 254 loss: 0.5420436976462837
  batch 255 loss: 0.5415284053952086
  batch 256 loss: 0.5410821745172143
  batch 257 loss: 0.5405129165269058
  batch 258 loss: 0.5400262485409892
  batch 259 loss: 0.5395616489717859
  batch 260 loss: 0.5390319973230362
  batch 261 loss: 0.53839663733011
  batch 262 loss: 0.5377282184043913
  batch 263 loss: 0.5372134828975446
  batch 264 loss: 0.536642388073784
  batch 265 loss: 0.5362746192599243
  batch 266 loss: 0.5358228314864008
  batch 267 loss: 0.5355781889363621
  batch 268 loss: 0.5349846774740006
  batch 269 loss: 0.5344616849404729
  batch 270 loss: 0.5340358916256163
  batch 271 loss: 0.5335449854606192
  batch 272 loss: 0.5331708620138028
  batch 273 loss: 0.5327047690367087
  batch 274 loss: 0.5322539636688511
  batch 275 loss: 0.5317491923679005
  batch 276 loss: 0.5313143013180166
  batch 277 loss: 0.530821809269461
  batch 278 loss: 0.5303307685920661
  batch 279 loss: 0.5299373691013637
  batch 280 loss: 0.5294191263616085
  batch 281 loss: 0.5287870761550618
  batch 282 loss: 0.5282811231858341
  batch 283 loss: 0.5277531064862497
  batch 284 loss: 0.5274319212201616
  batch 285 loss: 0.527008590049911
  batch 286 loss: 0.5265493186620566
  batch 287 loss: 0.5261997915934187
  batch 288 loss: 0.525613030522234
  batch 289 loss: 0.5252597896079291
  batch 290 loss: 0.5245527209906743
  batch 291 loss: 0.524029028272301
  batch 292 loss: 0.5236245264133362
  batch 293 loss: 0.5230840516782052
  batch 294 loss: 0.5226446203634042
  batch 295 loss: 0.5222237554647154
  batch 296 loss: 0.5218377778860362
  batch 297 loss: 0.5214027268717988
  batch 298 loss: 0.521011442246053
  batch 299 loss: 0.5205437803148825
  batch 300 loss: 0.5201397208372752
  batch 301 loss: 0.5197052205123774
  batch 302 loss: 0.519343314877409
  batch 303 loss: 0.5189069234105227
  batch 304 loss: 0.5184664396863234
  batch 305 loss: 0.5178914972993194
  batch 306 loss: 0.5175793797167298
  batch 307 loss: 0.517128010040774
  batch 308 loss: 0.5168180040918388
  batch 309 loss: 0.5164242052144603
  batch 310 loss: 0.5159965598775494
  batch 311 loss: 0.5156672462773093
  batch 312 loss: 0.5153843089938164
  batch 313 loss: 0.5151185863695967
  batch 314 loss: 0.5147178269875278
  batch 315 loss: 0.5143008163997105
  batch 316 loss: 0.5138050420563433
  batch 317 loss: 0.5135270063809416
  batch 318 loss: 0.5131753281977192
  batch 319 loss: 0.5126694172714198
  batch 320 loss: 0.5122435977682471
  batch 321 loss: 0.5119106987740764
  batch 322 loss: 0.5115777063443794
  batch 323 loss: 0.5111567446321895
  batch 324 loss: 0.51067103187979
  batch 325 loss: 0.5102473600094135
  batch 326 loss: 0.5098610494765767
  batch 327 loss: 0.5095641140908641
  batch 328 loss: 0.5090579898437348
  batch 329 loss: 0.5088525888224141
  batch 330 loss: 0.5084706130352887
  batch 331 loss: 0.5081245383287124
  batch 332 loss: 0.5077349444828838
  batch 333 loss: 0.5073545362140324
  batch 334 loss: 0.5068994882756365
  batch 335 loss: 0.5063582028026011
  batch 336 loss: 0.5058811655534166
  batch 337 loss: 0.5054843594660038
  batch 338 loss: 0.5051408954978694
  batch 339 loss: 0.5047982156628352
  batch 340 loss: 0.5044900190304308
  batch 341 loss: 0.5040897627380236
  batch 342 loss: 0.5036729016499213
  batch 343 loss: 0.5033653812922819
  batch 344 loss: 0.5029765261813651
  batch 345 loss: 0.5027378669683484
  batch 346 loss: 0.5022649789993474
  batch 347 loss: 0.5019759978100614
  batch 348 loss: 0.5016888556466705
  batch 349 loss: 0.501254661206188
  batch 350 loss: 0.5010091987677983
  batch 351 loss: 0.5006334038711341
  batch 352 loss: 0.5004087525673888
  batch 353 loss: 0.5000841786435913
  batch 354 loss: 0.49995957045568584
  batch 355 loss: 0.4995895455420857
  batch 356 loss: 0.4992650629261906
  batch 357 loss: 0.49886090464952615
  batch 358 loss: 0.49860572207240417
  batch 359 loss: 0.49834200266676026
  batch 360 loss: 0.4979930614431699
  batch 361 loss: 0.4976478913319078
  batch 362 loss: 0.49731078134715884
  batch 363 loss: 0.496957647077965
  batch 364 loss: 0.4965587921522476
  batch 365 loss: 0.4962607208996603
  batch 366 loss: 0.4959484325732038
  batch 367 loss: 0.4955830752687168
  batch 368 loss: 0.4952231456080209
  batch 369 loss: 0.49496291759537486
  batch 370 loss: 0.4947524136788136
  batch 371 loss: 0.4944831068625026
  batch 372 loss: 0.4941938814296517
  batch 373 loss: 0.49381938360971034
  batch 374 loss: 0.49337728863412683
  batch 375 loss: 0.4930522092183431
  batch 376 loss: 0.4929507114785783
  batch 377 loss: 0.49265681717060605
  batch 378 loss: 0.4922438984510129
  batch 379 loss: 0.4920424131730616
  batch 380 loss: 0.4917756799804537
  batch 381 loss: 0.49136198568218964
  batch 382 loss: 0.49101660841422556
  batch 383 loss: 0.4907218360247251
  batch 384 loss: 0.4903663228421162
  batch 385 loss: 0.490231232441865
  batch 386 loss: 0.48988175847678606
  batch 387 loss: 0.48961626466854596
  batch 388 loss: 0.48942943670086025
  batch 389 loss: 0.48921060730681626
  batch 390 loss: 0.4889398993589939
  batch 391 loss: 0.4886978798357727
  batch 392 loss: 0.48842646455278205
  batch 393 loss: 0.4881145598475866
  batch 394 loss: 0.4879110076554536
  batch 395 loss: 0.4875548904455161
  batch 396 loss: 0.4872713616550571
  batch 397 loss: 0.4870399406334615
  batch 398 loss: 0.48670830072170523
  batch 399 loss: 0.4864928750764756
  batch 400 loss: 0.4862344980984926
  batch 401 loss: 0.4858459961830529
  batch 402 loss: 0.4856146551779847
  batch 403 loss: 0.4853652203940872
  batch 404 loss: 0.4851420496654983
  batch 405 loss: 0.48492907381352085
  batch 406 loss: 0.48477335127410043
  batch 407 loss: 0.4844631844129258
  batch 408 loss: 0.4842775414387385
  batch 409 loss: 0.48402242584741495
  batch 410 loss: 0.4837671916659285
  batch 411 loss: 0.4835510961620767
  batch 412 loss: 0.48325562679651873
  batch 413 loss: 0.4830107841884253
  batch 414 loss: 0.48272755454127914
  batch 415 loss: 0.48243729802499336
  batch 416 loss: 0.48220187260840947
  batch 417 loss: 0.4819056361699276
  batch 418 loss: 0.48162811937514677
  batch 419 loss: 0.48145932499719407
  batch 420 loss: 0.4812688109420595
  batch 421 loss: 0.480956391243357
  batch 422 loss: 0.48086278288850287
  batch 423 loss: 0.4806170425516494
  batch 424 loss: 0.4803559995203648
  batch 425 loss: 0.4801560246243196
  batch 426 loss: 0.4799088194196773
  batch 427 loss: 0.47961071342997585
  batch 428 loss: 0.47924708637678737
  batch 429 loss: 0.4790289859115939
  batch 430 loss: 0.4787149342686631
  batch 431 loss: 0.47860091558188683
  batch 432 loss: 0.47844878301300386
  batch 433 loss: 0.47815480718161163
  batch 434 loss: 0.4780132900322637
  batch 435 loss: 0.47767797400211465
  batch 436 loss: 0.47746249643761085
  batch 437 loss: 0.4772397129704805
  batch 438 loss: 0.47709269576693236
  batch 439 loss: 0.47684114024144914
  batch 440 loss: 0.4766134998337789
  batch 441 loss: 0.47632303117624486
  batch 442 loss: 0.4760485701431516
  batch 443 loss: 0.47580136355374253
  batch 444 loss: 0.4754630758821427
  batch 445 loss: 0.475299178951242
  batch 446 loss: 0.47506041390478876
  batch 447 loss: 0.47474241523401317
  batch 448 loss: 0.47457278187253643
  batch 449 loss: 0.47440748988652814
  batch 450 loss: 0.4741527945465512
  batch 451 loss: 0.47394301529205557
  batch 452 loss: 0.4737662677611925
  batch 453 loss: 0.47359509650971454
  batch 454 loss: 0.47347339469168154
  batch 455 loss: 0.47324201739751376
  batch 456 loss: 0.4730571420177033
  batch 457 loss: 0.4728769345669569
  batch 458 loss: 0.4726286946295651
  batch 459 loss: 0.47249428320814063
  batch 460 loss: 0.47233218220265016
  batch 461 loss: 0.47209912195898707
  batch 462 loss: 0.47194218022740764
  batch 463 loss: 0.47165615357796525
  batch 464 loss: 0.471483590916313
  batch 465 loss: 0.4712282887069128
  batch 466 loss: 0.4708977360837961
  batch 467 loss: 0.4708043338144617
  batch 468 loss: 0.47053520164938056
  batch 469 loss: 0.47042941869194826
  batch 470 loss: 0.47020564890922384
  batch 471 loss: 0.47002049420036834
  batch 472 loss: 0.46975754914900003
LOSS train 0.46975754914900003 valid 0.2925529479980469
LOSS train 0.46975754914900003 valid 0.2933821678161621
LOSS train 0.46975754914900003 valid 0.29280004898707074
LOSS train 0.46975754914900003 valid 0.2927192449569702
LOSS train 0.46975754914900003 valid 0.28938854336738584
LOSS train 0.46975754914900003 valid 0.2893264989058177
LOSS train 0.46975754914900003 valid 0.2984567710331508
LOSS train 0.46975754914900003 valid 0.2967948690056801
LOSS train 0.46975754914900003 valid 0.29452261328697205
LOSS train 0.46975754914900003 valid 0.2972999632358551
LOSS train 0.46975754914900003 valid 0.29548497091640125
LOSS train 0.46975754914900003 valid 0.29746756454308826
LOSS train 0.46975754914900003 valid 0.29761608518086946
LOSS train 0.46975754914900003 valid 0.2977635732718876
LOSS train 0.46975754914900003 valid 0.2941781004269918
LOSS train 0.46975754914900003 valid 0.2953008599579334
LOSS train 0.46975754914900003 valid 0.29729548797887917
LOSS train 0.46975754914900003 valid 0.29798979063828784
LOSS train 0.46975754914900003 valid 0.3000023380706185
LOSS train 0.46975754914900003 valid 0.2991573378443718
LOSS train 0.46975754914900003 valid 0.29790882695288884
LOSS train 0.46975754914900003 valid 0.2970682436769659
LOSS train 0.46975754914900003 valid 0.2961106766825137
LOSS train 0.46975754914900003 valid 0.2962701842188835
LOSS train 0.46975754914900003 valid 0.29498118996620176
LOSS train 0.46975754914900003 valid 0.2943692413660196
LOSS train 0.46975754914900003 valid 0.29406794133009734
LOSS train 0.46975754914900003 valid 0.2940565294453076
LOSS train 0.46975754914900003 valid 0.29427508752921533
LOSS train 0.46975754914900003 valid 0.2955380101998647
LOSS train 0.46975754914900003 valid 0.29671322241906195
LOSS train 0.46975754914900003 valid 0.29644236993044615
LOSS train 0.46975754914900003 valid 0.29703154708399915
LOSS train 0.46975754914900003 valid 0.29685967810013714
LOSS train 0.46975754914900003 valid 0.297985965864999
LOSS train 0.46975754914900003 valid 0.2975439089867804
LOSS train 0.46975754914900003 valid 0.29731119162327535
LOSS train 0.46975754914900003 valid 0.2984428915538286
LOSS train 0.46975754914900003 valid 0.29745043699557966
LOSS train 0.46975754914900003 valid 0.2976275958120823
LOSS train 0.46975754914900003 valid 0.29825156490977217
LOSS train 0.46975754914900003 valid 0.29829320311546326
LOSS train 0.46975754914900003 valid 0.29837945103645325
LOSS train 0.46975754914900003 valid 0.298552388494665
LOSS train 0.46975754914900003 valid 0.297997436258528
LOSS train 0.46975754914900003 valid 0.29879285719083704
LOSS train 0.46975754914900003 valid 0.29886543623944545
LOSS train 0.46975754914900003 valid 0.29924482852220535
LOSS train 0.46975754914900003 valid 0.2999808642329002
LOSS train 0.46975754914900003 valid 0.29956257820129395
LOSS train 0.46975754914900003 valid 0.299790095464856
LOSS train 0.46975754914900003 valid 0.29987870214077145
LOSS train 0.46975754914900003 valid 0.2998741381573227
LOSS train 0.46975754914900003 valid 0.29966491571179144
LOSS train 0.46975754914900003 valid 0.2998819009824233
LOSS train 0.46975754914900003 valid 0.2996250263282231
LOSS train 0.46975754914900003 valid 0.2994059637973183
LOSS train 0.46975754914900003 valid 0.2991777124076054
LOSS train 0.46975754914900003 valid 0.2991952956733057
LOSS train 0.46975754914900003 valid 0.29915560930967333
LOSS train 0.46975754914900003 valid 0.2988238422597041
LOSS train 0.46975754914900003 valid 0.299448819891099
LOSS train 0.46975754914900003 valid 0.2996496137172457
LOSS train 0.46975754914900003 valid 0.3003515121527016
LOSS train 0.46975754914900003 valid 0.30108660963865425
LOSS train 0.46975754914900003 valid 0.3012848175836332
LOSS train 0.46975754914900003 valid 0.30096341736281096
LOSS train 0.46975754914900003 valid 0.30098381419392195
LOSS train 0.46975754914900003 valid 0.30061377217804175
LOSS train 0.46975754914900003 valid 0.30064379700592586
LOSS train 0.46975754914900003 valid 0.30013923326008757
LOSS train 0.46975754914900003 valid 0.3000896813140975
LOSS train 0.46975754914900003 valid 0.30002786280357674
LOSS train 0.46975754914900003 valid 0.299838573546023
LOSS train 0.46975754914900003 valid 0.2997560302416484
LOSS train 0.46975754914900003 valid 0.29999025558170517
LOSS train 0.46975754914900003 valid 0.30007255580518155
LOSS train 0.46975754914900003 valid 0.30021913731709504
LOSS train 0.46975754914900003 valid 0.3003438737573503
LOSS train 0.46975754914900003 valid 0.29981215447187426
LOSS train 0.46975754914900003 valid 0.2990784810649024
LOSS train 0.46975754914900003 valid 0.29939460718050237
LOSS train 0.46975754914900003 valid 0.299102131860802
LOSS train 0.46975754914900003 valid 0.2990484670514152
LOSS train 0.46975754914900003 valid 0.29861103646895465
LOSS train 0.46975754914900003 valid 0.29784421951964846
LOSS train 0.46975754914900003 valid 0.29789600375740005
LOSS train 0.46975754914900003 valid 0.2975889486684041
LOSS train 0.46975754914900003 valid 0.29797217735413756
LOSS train 0.46975754914900003 valid 0.2982082524233394
LOSS train 0.46975754914900003 valid 0.29825926760395804
LOSS train 0.46975754914900003 valid 0.29818168298705766
LOSS train 0.46975754914900003 valid 0.29808471135554776
LOSS train 0.46975754914900003 valid 0.2982250979923187
LOSS train 0.46975754914900003 valid 0.2979792191794044
LOSS train 0.46975754914900003 valid 0.29805029726897675
LOSS train 0.46975754914900003 valid 0.29803388189409197
LOSS train 0.46975754914900003 valid 0.29795872298430426
LOSS train 0.46975754914900003 valid 0.29811920707273964
LOSS train 0.46975754914900003 valid 0.29843820914626124
LOSS train 0.46975754914900003 valid 0.2986600350330372
LOSS train 0.46975754914900003 valid 0.29880549378839194
LOSS train 0.46975754914900003 valid 0.2987208072710963
LOSS train 0.46975754914900003 valid 0.29846603093812096
LOSS train 0.46975754914900003 valid 0.2985756909563428
LOSS train 0.46975754914900003 valid 0.2988406291828965
LOSS train 0.46975754914900003 valid 0.2985974838521993
LOSS train 0.46975754914900003 valid 0.298692403982083
LOSS train 0.46975754914900003 valid 0.2989956661375291
LOSS train 0.46975754914900003 valid 0.29940092983570965
LOSS train 0.46975754914900003 valid 0.2991113000893378
LOSS train 0.46975754914900003 valid 0.29889867547899485
LOSS train 0.46975754914900003 valid 0.29888893690256946
LOSS train 0.46975754914900003 valid 0.2987927767006974
LOSS train 0.46975754914900003 valid 0.2989164141209229
LOSS train 0.46975754914900003 valid 0.2990384031215618
LOSS train 0.46975754914900003 valid 0.2992006852331325
LOSS train 0.46975754914900003 valid 0.2991974913215233
LOSS train 0.46975754914900003 valid 0.2992265448600304
LOSS train 0.46975754914900003 valid 0.29893868751823904
LOSS train 0.46975754914900003 valid 0.2988337831310004
LOSS train 0.46975754914900003 valid 0.29875788324680486
LOSS train 0.46975754914900003 valid 0.2989372299696372
LOSS train 0.46975754914900003 valid 0.2990578611291224
LOSS train 0.46975754914900003 valid 0.29913005673885346
LOSS train 0.46975754914900003 valid 0.29915089444035575
LOSS train 0.46975754914900003 valid 0.29911800385929466
LOSS train 0.46975754914900003 valid 0.2992199418367818
LOSS train 0.46975754914900003 valid 0.2992751775092857
LOSS train 0.46975754914900003 valid 0.2992764662091549
LOSS train 0.46975754914900003 valid 0.2993188749515373
LOSS train 0.46975754914900003 valid 0.298998938929854
LOSS train 0.46975754914900003 valid 0.2989403006053509
LOSS train 0.46975754914900003 valid 0.2990399064190352
LOSS train 0.46975754914900003 valid 0.29907855446691867
LOSS train 0.46975754914900003 valid 0.2990288126337178
LOSS train 0.46975754914900003 valid 0.2989296209420601
LOSS train 0.46975754914900003 valid 0.29886331474003586
LOSS train 0.46975754914900003 valid 0.2987429629341304
LOSS train 0.46975754914900003 valid 0.29895159570234164
LOSS train 0.46975754914900003 valid 0.2991903917797914
LOSS train 0.46975754914900003 valid 0.2994794166633781
LOSS train 0.46975754914900003 valid 0.29943477180037464
LOSS train 0.46975754914900003 valid 0.29952134823219645
LOSS train 0.46975754914900003 valid 0.29920368328176694
LOSS train 0.46975754914900003 valid 0.2995008766038777
LOSS train 0.46975754914900003 valid 0.29926616956993024
LOSS train 0.46975754914900003 valid 0.2999652820463116
LOSS train 0.46975754914900003 valid 0.3002376079359311
LOSS train 0.46975754914900003 valid 0.30020494272311526
LOSS train 0.46975754914900003 valid 0.30013604501620034
LOSS train 0.46975754914900003 valid 0.2999010453686902
LOSS train 0.46975754914900003 valid 0.30007072747532837
LOSS train 0.46975754914900003 valid 0.3001105734086656
LOSS train 0.46975754914900003 valid 0.30013329319415555
LOSS train 0.46975754914900003 valid 0.30042552002347434
LOSS train 0.46975754914900003 valid 0.3003232886267316
LOSS train 0.46975754914900003 valid 0.30040469435574135
LOSS train 0.46975754914900003 valid 0.30052175827371247
LOSS train 0.46975754914900003 valid 0.3003617304377258
LOSS train 0.46975754914900003 valid 0.300363581095423
LOSS train 0.46975754914900003 valid 0.30028772896822586
LOSS train 0.46975754914900003 valid 0.3001502477135395
LOSS train 0.46975754914900003 valid 0.2999938398417903
LOSS train 0.46975754914900003 valid 0.29994623218521926
LOSS train 0.46975754914900003 valid 0.30009584855961513
LOSS train 0.46975754914900003 valid 0.3001811721189293
LOSS train 0.46975754914900003 valid 0.30008120896915597
LOSS train 0.46975754914900003 valid 0.30015051267908877
LOSS train 0.46975754914900003 valid 0.3002983790110139
LOSS train 0.46975754914900003 valid 0.3003081186995869
LOSS train 0.46975754914900003 valid 0.3001609183847904
LOSS train 0.46975754914900003 valid 0.30037562514660676
LOSS train 0.46975754914900003 valid 0.30039028472256385
LOSS train 0.46975754914900003 valid 0.30025755873748233
LOSS train 0.46975754914900003 valid 0.3002431918443604
LOSS train 0.46975754914900003 valid 0.3002057014043722
LOSS train 0.46975754914900003 valid 0.3003038870317213
LOSS train 0.46975754914900003 valid 0.300269254842284
LOSS train 0.46975754914900003 valid 0.30042284006873765
LOSS train 0.46975754914900003 valid 0.3005819181546322
LOSS train 0.46975754914900003 valid 0.30082690183605465
LOSS train 0.46975754914900003 valid 0.30077171008117864
LOSS train 0.46975754914900003 valid 0.30092824177573557
LOSS train 0.46975754914900003 valid 0.3006926696848225
LOSS train 0.46975754914900003 valid 0.30072551236678197
LOSS train 0.46975754914900003 valid 0.3005810195908827
LOSS train 0.46975754914900003 valid 0.30080945258761976
LOSS train 0.46975754914900003 valid 0.30077881953388297
LOSS train 0.46975754914900003 valid 0.3008095833031755
LOSS train 0.46975754914900003 valid 0.300744329369505
LOSS train 0.46975754914900003 valid 0.30081702885217965
LOSS train 0.46975754914900003 valid 0.3008117902000951
LOSS train 0.46975754914900003 valid 0.3007201319656421
LOSS train 0.46975754914900003 valid 0.30057694354118447
LOSS train 0.46975754914900003 valid 0.300670182902594
LOSS train 0.46975754914900003 valid 0.3006992573816764
LOSS train 0.46975754914900003 valid 0.3006135474401291
LOSS train 0.46975754914900003 valid 0.3006114499203524
LOSS train 0.46975754914900003 valid 0.3005234795063734
LOSS train 0.46975754914900003 valid 0.30042687006554203
LOSS train 0.46975754914900003 valid 0.3004250595032579
LOSS train 0.46975754914900003 valid 0.3003117964038708
LOSS train 0.46975754914900003 valid 0.3002943880856037
LOSS train 0.46975754914900003 valid 0.30017564522057044
LOSS train 0.46975754914900003 valid 0.30019028130376224
LOSS train 0.46975754914900003 valid 0.30010526170189256
LOSS train 0.46975754914900003 valid 0.3000198540349419
LOSS train 0.46975754914900003 valid 0.2998018067276649
LOSS train 0.46975754914900003 valid 0.2998199130098025
LOSS train 0.46975754914900003 valid 0.29987000225562055
LOSS train 0.46975754914900003 valid 0.29968090980963885
LOSS train 0.46975754914900003 valid 0.29974192905593927
LOSS train 0.46975754914900003 valid 0.29964357078353937
LOSS train 0.46975754914900003 valid 0.299517043593318
LOSS train 0.46975754914900003 valid 0.29953309293422437
LOSS train 0.46975754914900003 valid 0.2996245517571401
LOSS train 0.46975754914900003 valid 0.2996451035141945
LOSS train 0.46975754914900003 valid 0.2996059765690538
LOSS train 0.46975754914900003 valid 0.29966863237998703
LOSS train 0.46975754914900003 valid 0.29963204096075635
LOSS train 0.46975754914900003 valid 0.2995925707457302
LOSS train 0.46975754914900003 valid 0.299712172204069
LOSS train 0.46975754914900003 valid 0.2998318539799324
LOSS train 0.46975754914900003 valid 0.29982545038064323
LOSS train 0.46975754914900003 valid 0.299860628634955
LOSS train 0.46975754914900003 valid 0.3001767494736264
LOSS train 0.46975754914900003 valid 0.3002223259393583
LOSS train 0.46975754914900003 valid 0.3002712725942312
LOSS train 0.46975754914900003 valid 0.30032325069541516
LOSS train 0.46975754914900003 valid 0.3003238816927006
LOSS train 0.46975754914900003 valid 0.3003207652604786
LOSS train 0.46975754914900003 valid 0.3002568327945701
LOSS train 0.46975754914900003 valid 0.3002767517653286
LOSS train 0.46975754914900003 valid 0.30037416352870616
LOSS train 0.46975754914900003 valid 0.3003408357627311
LOSS train 0.46975754914900003 valid 0.3003457308695789
LOSS train 0.46975754914900003 valid 0.30025178513356615
LOSS train 0.46975754914900003 valid 0.30012782853268183
LOSS train 0.46975754914900003 valid 0.30001493686189257
LOSS train 0.46975754914900003 valid 0.3000118028202492
LOSS train 0.46975754914900003 valid 0.2997826487441693
LOSS train 0.46975754914900003 valid 0.2998822061000047
LOSS train 0.46975754914900003 valid 0.29995938127890964
LOSS train 0.46975754914900003 valid 0.3000709421172434
LOSS train 0.46975754914900003 valid 0.30006572113531393
LOSS train 0.46975754914900003 valid 0.30007966200591096
LOSS train 0.46975754914900003 valid 0.30005866511454504
LOSS train 0.46975754914900003 valid 0.2999886965536209
LOSS train 0.46975754914900003 valid 0.3000775198340416
LOSS train 0.46975754914900003 valid 0.30008830320075214
LOSS train 0.46975754914900003 valid 0.30019956218108296
LOSS train 0.46975754914900003 valid 0.30012067757811944
LOSS train 0.46975754914900003 valid 0.30011546746717666
LOSS train 0.46975754914900003 valid 0.30012204547723137
LOSS train 0.46975754914900003 valid 0.30005781137151644
LOSS train 0.46975754914900003 valid 0.30002122891320326
LOSS train 0.46975754914900003 valid 0.30010222805331843
LOSS train 0.46975754914900003 valid 0.30004743046512017
LOSS train 0.46975754914900003 valid 0.2999908552146875
LOSS train 0.46975754914900003 valid 0.3000433534612144
LOSS train 0.46975754914900003 valid 0.3000090045442108
LOSS train 0.46975754914900003 valid 0.3000113811669694
LOSS train 0.46975754914900003 valid 0.3000577027260354
LOSS train 0.46975754914900003 valid 0.3000257212598369
LOSS train 0.46975754914900003 valid 0.3000205799489093
LOSS train 0.46975754914900003 valid 0.3001551975806554
LOSS train 0.46975754914900003 valid 0.3002826972350256
LOSS train 0.46975754914900003 valid 0.3004679345840858
LOSS train 0.46975754914900003 valid 0.3004265300101704
LOSS train 0.46975754914900003 valid 0.30052027012809174
LOSS train 0.46975754914900003 valid 0.3006686935858691
LOSS train 0.46975754914900003 valid 0.30065300563971203
LOSS train 0.46975754914900003 valid 0.30065665111272005
LOSS train 0.46975754914900003 valid 0.3006514330885627
LOSS train 0.46975754914900003 valid 0.3005478818887386
LOSS train 0.46975754914900003 valid 0.3004387457340633
LOSS train 0.46975754914900003 valid 0.3004055559742365
LOSS train 0.46975754914900003 valid 0.3004924238903121
LOSS train 0.46975754914900003 valid 0.300424179275121
LOSS train 0.46975754914900003 valid 0.3003058347095374
LOSS train 0.46975754914900003 valid 0.300204373156348
LOSS train 0.46975754914900003 valid 0.3002263420778113
LOSS train 0.46975754914900003 valid 0.3003666841123306
LOSS train 0.46975754914900003 valid 0.300440202313557
LOSS train 0.46975754914900003 valid 0.3003719557936375
LOSS train 0.46975754914900003 valid 0.3003326852251013
LOSS train 0.46975754914900003 valid 0.30035731315405834
LOSS train 0.46975754914900003 valid 0.30033762835507577
LOSS train 0.46975754914900003 valid 0.3003429002289114
LOSS train 0.46975754914900003 valid 0.30032112697760266
LOSS train 0.46975754914900003 valid 0.30038045392665147
LOSS train 0.46975754914900003 valid 0.3003913400624799
LOSS train 0.46975754914900003 valid 0.3004332081902595
LOSS train 0.46975754914900003 valid 0.3004659511780335
LOSS train 0.46975754914900003 valid 0.3004630244764927
LOSS train 0.46975754914900003 valid 0.3005409323506885
LOSS train 0.46975754914900003 valid 0.3005813675738821
LOSS train 0.46975754914900003 valid 0.3006609500351558
LOSS train 0.46975754914900003 valid 0.30059886232018473
LOSS train 0.46975754914900003 valid 0.3006200589907922
LOSS train 0.46975754914900003 valid 0.30065249572723907
LOSS train 0.46975754914900003 valid 0.30068060013130554
LOSS train 0.46975754914900003 valid 0.30066438367296205
LOSS train 0.46975754914900003 valid 0.3005770901676084
LOSS train 0.46975754914900003 valid 0.3004964086062768
LOSS train 0.46975754914900003 valid 0.3004721557958118
LOSS train 0.46975754914900003 valid 0.30042536242248175
LOSS train 0.46975754914900003 valid 0.3004886158365262
LOSS train 0.46975754914900003 valid 0.3005509278947307
LOSS train 0.46975754914900003 valid 0.300516932004901
LOSS train 0.46975754914900003 valid 0.3005073322222019
LOSS train 0.46975754914900003 valid 0.3005523998230791
LOSS train 0.46975754914900003 valid 0.30065643858567925
LOSS train 0.46975754914900003 valid 0.30069119301107194
LOSS train 0.46975754914900003 valid 0.3006312946635711
LOSS train 0.46975754914900003 valid 0.3007008332277322
LOSS train 0.46975754914900003 valid 0.30079035062647463
LOSS train 0.46975754914900003 valid 0.300881031305065
LOSS train 0.46975754914900003 valid 0.3008029934484512
LOSS train 0.46975754914900003 valid 0.30087732753463997
LOSS train 0.46975754914900003 valid 0.30078836623000804
LOSS train 0.46975754914900003 valid 0.3007205169728666
LOSS train 0.46975754914900003 valid 0.30070909218471725
LOSS train 0.46975754914900003 valid 0.300762054507549
LOSS train 0.46975754914900003 valid 0.3008524422173851
LOSS train 0.46975754914900003 valid 0.3008138114979508
LOSS train 0.46975754914900003 valid 0.300786846340066
LOSS train 0.46975754914900003 valid 0.3008627878405765
LOSS train 0.46975754914900003 valid 0.3007901981472969
LOSS train 0.46975754914900003 valid 0.3006526273633058
LOSS train 0.46975754914900003 valid 0.3005790610360094
LOSS train 0.46975754914900003 valid 0.3005885808943986
LOSS train 0.46975754914900003 valid 0.30076264920170437
LOSS train 0.46975754914900003 valid 0.3007801470027041
LOSS train 0.46975754914900003 valid 0.30076430467445225
LOSS train 0.46975754914900003 valid 0.30075885956825066
LOSS train 0.46975754914900003 valid 0.3007313280680476
LOSS train 0.46975754914900003 valid 0.30073474787153676
LOSS train 0.46975754914900003 valid 0.30073292925953865
LOSS train 0.46975754914900003 valid 0.3006640123394577
LOSS train 0.46975754914900003 valid 0.3006489325249404
LOSS train 0.46975754914900003 valid 0.3006232760998668
LOSS train 0.46975754914900003 valid 0.30076390829717004
LOSS train 0.46975754914900003 valid 0.3008236000071401
LOSS train 0.46975754914900003 valid 0.3007465992445891
LOSS train 0.46975754914900003 valid 0.30061388243344056
LOSS train 0.46975754914900003 valid 0.30056878243540897
LOSS train 0.46975754914900003 valid 0.30068376698774046
LOSS train 0.46975754914900003 valid 0.3006263184121677
LOSS train 0.46975754914900003 valid 0.3005343369524024
LOSS train 0.46975754914900003 valid 0.30053069649941544
LOSS train 0.46975754914900003 valid 0.30057233770406955
LOSS train 0.46975754914900003 valid 0.30066774983358924
LOSS train 0.46975754914900003 valid 0.30069861063654996
LOSS train 0.46975754914900003 valid 0.3007452181132322
LOSS train 0.46975754914900003 valid 0.30067186424879133
LOSS train 0.46975754914900003 valid 0.30063624417815127
LOSS train 0.46975754914900003 valid 0.3007016101836494
LOSS train 0.46975754914900003 valid 0.3007089289940066
LOSS train 0.46975754914900003 valid 0.30070015627096236
LOSS train 0.46975754914900003 valid 0.300794387349437
LOSS train 0.46975754914900003 valid 0.3006147665918366
LOSS train 0.46975754914900003 valid 0.3006033432352674
LOSS train 0.46975754914900003 valid 0.30063517395764183
LOSS train 0.46975754914900003 valid 0.30062558359461405
LOSS train 0.46975754914900003 valid 0.3005187918282335
LOSS train 0.46975754914900003 valid 0.30042460998115333
LOSS train 0.46975754914900003 valid 0.300513768260718
EPOCH 2:
  batch 1 loss: 0.3730611205101013
  batch 2 loss: 0.3686714619398117
  batch 3 loss: 0.36637069781621295
  batch 4 loss: 0.36918625235557556
  batch 5 loss: 0.3727988123893738
  batch 6 loss: 0.37025855481624603
  batch 7 loss: 0.37367397972515654
  batch 8 loss: 0.37404753267765045
  batch 9 loss: 0.37618181109428406
  batch 10 loss: 0.37526598274707795
  batch 11 loss: 0.37451312758705835
  batch 12 loss: 0.37248272200425464
  batch 13 loss: 0.37059407509290254
  batch 14 loss: 0.37241438031196594
  batch 15 loss: 0.37376516660054526
  batch 16 loss: 0.37336851470172405
  batch 17 loss: 0.3694586771375993
  batch 18 loss: 0.3685656620396508
  batch 19 loss: 0.3688755192254719
  batch 20 loss: 0.36815200597047804
  batch 21 loss: 0.368443774325507
  batch 22 loss: 0.36883277513764123
  batch 23 loss: 0.36883011330728943
  batch 24 loss: 0.36738592262069386
  batch 25 loss: 0.3684846043586731
  batch 26 loss: 0.36715546594216275
  batch 27 loss: 0.36751193470425075
  batch 28 loss: 0.3666008359619549
  batch 29 loss: 0.36660703091785823
  batch 30 loss: 0.3652410715818405
  batch 31 loss: 0.3653682710662965
  batch 32 loss: 0.3642569798976183
  batch 33 loss: 0.3641138058720213
  batch 34 loss: 0.3641909632612677
  batch 35 loss: 0.3645395074571882
  batch 36 loss: 0.36406446165508694
  batch 37 loss: 0.36553069949150085
  batch 38 loss: 0.3661811579214899
  batch 39 loss: 0.3656943570344876
  batch 40 loss: 0.36647152006626127
  batch 41 loss: 0.36652436489012186
  batch 42 loss: 0.367189123516991
  batch 43 loss: 0.367515247921611
  batch 44 loss: 0.3679027347402139
  batch 45 loss: 0.3680335594548119
  batch 46 loss: 0.3678415385277375
  batch 47 loss: 0.36777701910505906
  batch 48 loss: 0.3669539447873831
  batch 49 loss: 0.366592390196664
  batch 50 loss: 0.3664900600910187
  batch 51 loss: 0.36601680867812214
  batch 52 loss: 0.3665179375272531
  batch 53 loss: 0.3660090290150552
  batch 54 loss: 0.366434242438387
  batch 55 loss: 0.36580291173674845
  batch 56 loss: 0.3657027706503868
  batch 57 loss: 0.364989229461603
  batch 58 loss: 0.36595928823125773
  batch 59 loss: 0.36601832054429134
  batch 60 loss: 0.3657911295692126
  batch 61 loss: 0.3660029368322404
  batch 62 loss: 0.36604071672885646
  batch 63 loss: 0.3657805725695595
  batch 64 loss: 0.3660970190539956
  batch 65 loss: 0.3657001183583186
  batch 66 loss: 0.36523366742061847
  batch 67 loss: 0.3658408148964839
  batch 68 loss: 0.3665299941511715
  batch 69 loss: 0.36632052884585614
  batch 70 loss: 0.36634966773646216
  batch 71 loss: 0.36595723200851765
  batch 72 loss: 0.36585644177264637
  batch 73 loss: 0.3654322036325115
  batch 74 loss: 0.36552545025541977
  batch 75 loss: 0.36527533531188966
  batch 76 loss: 0.36543670610377665
  batch 77 loss: 0.3654242479181909
  batch 78 loss: 0.36520564326873195
  batch 79 loss: 0.36576645283759396
  batch 80 loss: 0.3655211519449949
  batch 81 loss: 0.36581208198158827
  batch 82 loss: 0.3661497471536078
  batch 83 loss: 0.3659214345087488
  batch 84 loss: 0.3655166586949712
  batch 85 loss: 0.3651285010225633
  batch 86 loss: 0.3655928706013879
  batch 87 loss: 0.36518262206822977
  batch 88 loss: 0.36481617391109467
  batch 89 loss: 0.3644566388612383
  batch 90 loss: 0.3642316391070684
  batch 91 loss: 0.36424387975053474
  batch 92 loss: 0.3641255951446036
  batch 93 loss: 0.36413471288578486
  batch 94 loss: 0.3643705553196846
  batch 95 loss: 0.3640382280475215
  batch 96 loss: 0.3643486388027668
  batch 97 loss: 0.3645607091716884
  batch 98 loss: 0.36464405455151383
  batch 99 loss: 0.3646470485913633
  batch 100 loss: 0.3645042166113853
  batch 101 loss: 0.36418137013322055
  batch 102 loss: 0.36438923080762226
  batch 103 loss: 0.36483902578215
  batch 104 loss: 0.36492303357674527
  batch 105 loss: 0.3643844343367077
  batch 106 loss: 0.36480990024107807
  batch 107 loss: 0.3643894975430497
  batch 108 loss: 0.36446269380825536
  batch 109 loss: 0.36459432337262215
  batch 110 loss: 0.36479189964857966
  batch 111 loss: 0.36483806344839903
  batch 112 loss: 0.36502005053418024
  batch 113 loss: 0.36548472558502604
  batch 114 loss: 0.36575649809419064
  batch 115 loss: 0.36600180475608163
  batch 116 loss: 0.366308876923446
  batch 117 loss: 0.3663506197114276
  batch 118 loss: 0.36631082831803013
  batch 119 loss: 0.366543401189211
  batch 120 loss: 0.3664307949443658
  batch 121 loss: 0.36645318678587924
  batch 122 loss: 0.3663846144422156
  batch 123 loss: 0.3662249915968112
  batch 124 loss: 0.3666751327053193
  batch 125 loss: 0.3665603642463684
  batch 126 loss: 0.36654123048933723
  batch 127 loss: 0.36712906773634785
  batch 128 loss: 0.3669244146440178
  batch 129 loss: 0.36705260013425073
  batch 130 loss: 0.36711497467297777
  batch 131 loss: 0.3672221168761945
  batch 132 loss: 0.36701882472544006
  batch 133 loss: 0.3671754887677673
  batch 134 loss: 0.36711727371856345
  batch 135 loss: 0.36710095206896465
  batch 136 loss: 0.3669671464492293
  batch 137 loss: 0.36693834127300845
  batch 138 loss: 0.36694273309431213
  batch 139 loss: 0.3673408788313969
  batch 140 loss: 0.36713647778545105
  batch 141 loss: 0.36716542429957827
  batch 142 loss: 0.3670316764586408
  batch 143 loss: 0.36670358718692003
  batch 144 loss: 0.3665993557208114
  batch 145 loss: 0.36649220256969844
  batch 146 loss: 0.36639281107138283
  batch 147 loss: 0.36672379471817795
  batch 148 loss: 0.3667010467600178
  batch 149 loss: 0.36653177010132965
  batch 150 loss: 0.36635432918866473
  batch 151 loss: 0.3663026891796794
  batch 152 loss: 0.3664897901839332
  batch 153 loss: 0.36632226534139095
  batch 154 loss: 0.36632040143013
  batch 155 loss: 0.36640302981099776
  batch 156 loss: 0.3662287617723147
  batch 157 loss: 0.3664365014073196
  batch 158 loss: 0.36646912841102747
  batch 159 loss: 0.3664600800418254
  batch 160 loss: 0.366266524605453
  batch 161 loss: 0.3662791163284586
  batch 162 loss: 0.3662130928333895
  batch 163 loss: 0.36608630729599234
  batch 164 loss: 0.3662786322032533
  batch 165 loss: 0.3660817821820577
  batch 166 loss: 0.3660999859671995
  batch 167 loss: 0.3658538325104171
  batch 168 loss: 0.3657499063937437
  batch 169 loss: 0.36545851332901497
  batch 170 loss: 0.3653194026035421
  batch 171 loss: 0.36520700095689784
  batch 172 loss: 0.3651234329093334
  batch 173 loss: 0.365093124395161
  batch 174 loss: 0.36519767343997955
  batch 175 loss: 0.3652459682737078
  batch 176 loss: 0.36497927524826745
  batch 177 loss: 0.36496221261509393
  batch 178 loss: 0.36505602936396436
  batch 179 loss: 0.3650998319993472
  batch 180 loss: 0.3648379973239369
  batch 181 loss: 0.36478361882557525
  batch 182 loss: 0.3645479954206027
  batch 183 loss: 0.3642527672762428
  batch 184 loss: 0.36415875686899474
  batch 185 loss: 0.36416449530704603
  batch 186 loss: 0.36434600766628017
  batch 187 loss: 0.36421369916615004
  batch 188 loss: 0.36398644777054484
  batch 189 loss: 0.36373241647841437
  batch 190 loss: 0.36360111801247846
  batch 191 loss: 0.3634615521780483
  batch 192 loss: 0.3634601798839867
  batch 193 loss: 0.3633855067077696
  batch 194 loss: 0.3636738671777175
  batch 195 loss: 0.36386425449297977
  batch 196 loss: 0.3635598373960476
  batch 197 loss: 0.36334780448584386
  batch 198 loss: 0.3635263635654642
  batch 199 loss: 0.36349731429138377
  batch 200 loss: 0.36340967983007433
  batch 201 loss: 0.36345462819830104
  batch 202 loss: 0.3633597656936929
  batch 203 loss: 0.36331424660283357
  batch 204 loss: 0.362901658550197
  batch 205 loss: 0.3629735612287754
  batch 206 loss: 0.36287012580529
  batch 207 loss: 0.3628297643960962
  batch 208 loss: 0.36250790681403416
  batch 209 loss: 0.3622819440501729
  batch 210 loss: 0.36222763870443614
  batch 211 loss: 0.3622006627055706
  batch 212 loss: 0.3620697945918677
  batch 213 loss: 0.3620819863579083
  batch 214 loss: 0.361952108618255
  batch 215 loss: 0.3616957444091176
  batch 216 loss: 0.3615736668860471
  batch 217 loss: 0.3615645335017261
  batch 218 loss: 0.36159395050565035
  batch 219 loss: 0.3615460855775772
  batch 220 loss: 0.36147528439760207
  batch 221 loss: 0.36148039342591126
  batch 222 loss: 0.36150450843411525
  batch 223 loss: 0.361490203126129
  batch 224 loss: 0.36150786599942614
  batch 225 loss: 0.3613818731572893
  batch 226 loss: 0.36151645985324826
  batch 227 loss: 0.3613219242789147
  batch 228 loss: 0.36127587956817525
  batch 229 loss: 0.36113534470833025
  batch 230 loss: 0.3611403279978296
  batch 231 loss: 0.36111670848611116
  batch 232 loss: 0.3608560210158085
  batch 233 loss: 0.36079386196422986
  batch 234 loss: 0.3610035850960984
  batch 235 loss: 0.36102239616373755
  batch 236 loss: 0.3609263734039614
  batch 237 loss: 0.360954412679632
  batch 238 loss: 0.3607961257465747
  batch 239 loss: 0.36063813888877005
  batch 240 loss: 0.3607324302196503
  batch 241 loss: 0.36091869834547713
  batch 242 loss: 0.36069456481736556
  batch 243 loss: 0.36071936375319713
  batch 244 loss: 0.36066981600444825
  batch 245 loss: 0.3606784220860929
  batch 246 loss: 0.36059376997191733
  batch 247 loss: 0.3606486868279183
  batch 248 loss: 0.3608429113463048
  batch 249 loss: 0.36076654547668363
  batch 250 loss: 0.36071397721767423
  batch 251 loss: 0.36075610824790133
  batch 252 loss: 0.36062162676973947
  batch 253 loss: 0.3603914969052251
  batch 254 loss: 0.3603394519625686
  batch 255 loss: 0.36034715222377406
  batch 256 loss: 0.36029949807561934
  batch 257 loss: 0.36019546396537516
  batch 258 loss: 0.36020711306915726
  batch 259 loss: 0.3602070375759169
  batch 260 loss: 0.3601221692103606
  batch 261 loss: 0.35999228123047344
  batch 262 loss: 0.3597380447251196
  batch 263 loss: 0.35969832426241594
  batch 264 loss: 0.3595291847983996
  batch 265 loss: 0.359447991285684
  batch 266 loss: 0.3593783077216686
  batch 267 loss: 0.359483675228969
  batch 268 loss: 0.35935461209781133
  batch 269 loss: 0.3592651277005008
  batch 270 loss: 0.3593266898835147
  batch 271 loss: 0.3592596248727003
  batch 272 loss: 0.35927120201727925
  batch 273 loss: 0.3592897559696938
  batch 274 loss: 0.35930107664452854
  batch 275 loss: 0.3592291323705153
  batch 276 loss: 0.35921349106491474
  batch 277 loss: 0.3592138061239401
  batch 278 loss: 0.35908092890711996
  batch 279 loss: 0.35911636647357736
  batch 280 loss: 0.3589211351105145
  batch 281 loss: 0.35884322644976957
  batch 282 loss: 0.3587440436190747
  batch 283 loss: 0.3586204144642968
  batch 284 loss: 0.3586878619143661
  batch 285 loss: 0.35855266096299154
  batch 286 loss: 0.35849561526641976
  batch 287 loss: 0.35844259405385326
  batch 288 loss: 0.3583312482676572
  batch 289 loss: 0.3584348124203797
  batch 290 loss: 0.35824157585357796
  batch 291 loss: 0.3581668602437088
  batch 292 loss: 0.35817269531831347
  batch 293 loss: 0.3580637854927636
  batch 294 loss: 0.3579644134052757
  batch 295 loss: 0.3579592317847882
  batch 296 loss: 0.3580250853823649
  batch 297 loss: 0.35795632687080586
  batch 298 loss: 0.35792716057508583
  batch 299 loss: 0.3578839178468073
  batch 300 loss: 0.3578547190626462
  batch 301 loss: 0.357740104693511
  batch 302 loss: 0.35771374522850213
  batch 303 loss: 0.3576453429440854
  batch 304 loss: 0.35758844204247
  batch 305 loss: 0.35734078981837286
  batch 306 loss: 0.3574671072313209
  batch 307 loss: 0.3573811408557022
  batch 308 loss: 0.3574699532095488
  batch 309 loss: 0.3573303294027507
  batch 310 loss: 0.3572683357423352
  batch 311 loss: 0.35728350923758995
  batch 312 loss: 0.3572963456121775
  batch 313 loss: 0.35729564483554216
  batch 314 loss: 0.3572208572914646
  batch 315 loss: 0.35710843867725794
  batch 316 loss: 0.35701385479939135
  batch 317 loss: 0.35702515861213396
  batch 318 loss: 0.35702394904955376
  batch 319 loss: 0.3568932348657925
  batch 320 loss: 0.3568123372271657
  batch 321 loss: 0.3567605915470658
  batch 322 loss: 0.3567087135699965
  batch 323 loss: 0.3566778502412625
  batch 324 loss: 0.35644100578846755
  batch 325 loss: 0.3563008804504688
  batch 326 loss: 0.3562461358089388
  batch 327 loss: 0.3561918460265577
  batch 328 loss: 0.3559758737683296
  batch 329 loss: 0.35606655994809505
  batch 330 loss: 0.35596632307226006
  batch 331 loss: 0.3558934252607858
  batch 332 loss: 0.3557918862585562
  batch 333 loss: 0.3558083074586885
  batch 334 loss: 0.35567432659828735
  batch 335 loss: 0.355458441332205
  batch 336 loss: 0.35524531542545273
  batch 337 loss: 0.35510673579547103
  batch 338 loss: 0.3550192517112698
  batch 339 loss: 0.35504044561611164
  batch 340 loss: 0.35500805974006655
  batch 341 loss: 0.35486724579439133
  batch 342 loss: 0.35470630794937846
  batch 343 loss: 0.3546963834554044
  batch 344 loss: 0.35462515870499056
  batch 345 loss: 0.35462612812069877
  batch 346 loss: 0.3544365867024901
  batch 347 loss: 0.3544222817297284
  batch 348 loss: 0.35445093132298566
  batch 349 loss: 0.35427998164321767
  batch 350 loss: 0.3542935021434511
  batch 351 loss: 0.3542129998533135
  batch 352 loss: 0.3543051402507858
  batch 353 loss: 0.3542208126178882
  batch 354 loss: 0.3543368462620482
  batch 355 loss: 0.35427634741218994
  batch 356 loss: 0.3541964274946223
  batch 357 loss: 0.35404890717244614
  batch 358 loss: 0.3540355888658396
  batch 359 loss: 0.35398224220302443
  batch 360 loss: 0.35388626770840753
  batch 361 loss: 0.353813787081235
  batch 362 loss: 0.35368920146431054
  batch 363 loss: 0.35354879917192067
  batch 364 loss: 0.3533853215503169
  batch 365 loss: 0.3534053888222943
  batch 366 loss: 0.3533583992654509
  batch 367 loss: 0.35321858720168753
  batch 368 loss: 0.35310602811691555
  batch 369 loss: 0.35312996703757826
  batch 370 loss: 0.35315391832106824
  batch 371 loss: 0.3531481298314272
  batch 372 loss: 0.353083208123202
  batch 373 loss: 0.35293538816492936
  batch 374 loss: 0.35276289252355136
  batch 375 loss: 0.3526465624173482
  batch 376 loss: 0.3527650855323102
  batch 377 loss: 0.35268181396416076
  batch 378 loss: 0.35252737249962235
  batch 379 loss: 0.3526086180537231
  batch 380 loss: 0.35259793745843987
  batch 381 loss: 0.3524708330944141
  batch 382 loss: 0.352344168418365
  batch 383 loss: 0.3523094630739396
  batch 384 loss: 0.3521820013411343
  batch 385 loss: 0.3522440895631716
  batch 386 loss: 0.3521091398334256
  batch 387 loss: 0.35211781229467665
  batch 388 loss: 0.35215334164113116
  batch 389 loss: 0.35208764511392787
  batch 390 loss: 0.35203720048452036
  batch 391 loss: 0.3520365941250111
  batch 392 loss: 0.35199310615354656
  batch 393 loss: 0.35197123915487877
  batch 394 loss: 0.351958931021884
  batch 395 loss: 0.35184114243410813
  batch 396 loss: 0.3518389201525486
  batch 397 loss: 0.3518689241006933
  batch 398 loss: 0.3517726423153326
  batch 399 loss: 0.3518048113300687
  batch 400 loss: 0.3517638352513313
  batch 401 loss: 0.3517268030274836
  batch 402 loss: 0.3517707440983597
  batch 403 loss: 0.3517920867385107
  batch 404 loss: 0.3517699651788957
  batch 405 loss: 0.3517869253953298
  batch 406 loss: 0.3518600099045655
  batch 407 loss: 0.35175617416309196
  batch 408 loss: 0.3518373852997434
  batch 409 loss: 0.3518007900429238
  batch 410 loss: 0.3518007540121311
  batch 411 loss: 0.3517637520375913
  batch 412 loss: 0.35166280470716144
  batch 413 loss: 0.35166111169946684
  batch 414 loss: 0.3516649590166295
  batch 415 loss: 0.35162711919072165
  batch 416 loss: 0.3516495210620073
  batch 417 loss: 0.35163412948878264
  batch 418 loss: 0.3515670839964488
  batch 419 loss: 0.3515666636476084
  batch 420 loss: 0.35154713420640854
  batch 421 loss: 0.35148412449342903
  batch 422 loss: 0.35155143041463827
  batch 423 loss: 0.35157119135766446
  batch 424 loss: 0.35153702808157455
  batch 425 loss: 0.35153354132876674
  batch 426 loss: 0.35147421013021696
  batch 427 loss: 0.35139151632925386
  batch 428 loss: 0.3512898578404266
  batch 429 loss: 0.35122409817222117
  batch 430 loss: 0.35113096895606016
  batch 431 loss: 0.35118700034933686
  batch 432 loss: 0.3512514611637151
  batch 433 loss: 0.351187035682295
  batch 434 loss: 0.35123229239668163
  batch 435 loss: 0.3511152757310319
  batch 436 loss: 0.35108931656550924
  batch 437 loss: 0.351120041150523
  batch 438 loss: 0.3511728091871357
  batch 439 loss: 0.35110953361950054
  batch 440 loss: 0.3511299799789082
  batch 441 loss: 0.3509863263625407
  batch 442 loss: 0.35093202863343703
  batch 443 loss: 0.3508641363939367
  batch 444 loss: 0.35072619924405674
  batch 445 loss: 0.3507487987534384
  batch 446 loss: 0.3506725424474665
  batch 447 loss: 0.35057166632123177
  batch 448 loss: 0.3505747579038143
  batch 449 loss: 0.3505876588662111
  batch 450 loss: 0.3505199974113041
  batch 451 loss: 0.35049877528869394
  batch 452 loss: 0.3505209174040145
  batch 453 loss: 0.35051609348777113
  batch 454 loss: 0.3505736983951493
  batch 455 loss: 0.3505551667003841
  batch 456 loss: 0.3505690055981017
  batch 457 loss: 0.350557033942617
  batch 458 loss: 0.3504605266594991
  batch 459 loss: 0.3504828123745056
  batch 460 loss: 0.35050101649502047
  batch 461 loss: 0.35042750609153783
  batch 462 loss: 0.3504231043837287
  batch 463 loss: 0.3503044035856976
  batch 464 loss: 0.35031334965907296
  batch 465 loss: 0.3502566372835508
  batch 466 loss: 0.35009491795378184
  batch 467 loss: 0.3501387657777145
  batch 468 loss: 0.3500727127250443
  batch 469 loss: 0.3501664199300412
  batch 470 loss: 0.35011825104977223
  batch 471 loss: 0.3500741114029206
  batch 472 loss: 0.3499430159769826
LOSS train 0.3499430159769826 valid 0.26758426427841187
LOSS train 0.3499430159769826 valid 0.25653593242168427
LOSS train 0.3499430159769826 valid 0.25600781043370563
LOSS train 0.3499430159769826 valid 0.2549124211072922
LOSS train 0.3499430159769826 valid 0.2496454209089279
LOSS train 0.3499430159769826 valid 0.24987319856882095
LOSS train 0.3499430159769826 valid 0.25803094463688986
LOSS train 0.3499430159769826 valid 0.25558064691722393
LOSS train 0.3499430159769826 valid 0.2536618991030587
LOSS train 0.3499430159769826 valid 0.256268148124218
LOSS train 0.3499430159769826 valid 0.25339617512442847
LOSS train 0.3499430159769826 valid 0.25613893816868466
LOSS train 0.3499430159769826 valid 0.25586917996406555
LOSS train 0.3499430159769826 valid 0.2557696487222399
LOSS train 0.3499430159769826 valid 0.2519056876500448
LOSS train 0.3499430159769826 valid 0.25235626101493835
LOSS train 0.3499430159769826 valid 0.2544008896631353
LOSS train 0.3499430159769826 valid 0.25462330877780914
LOSS train 0.3499430159769826 valid 0.25697599900396245
LOSS train 0.3499430159769826 valid 0.2556592747569084
LOSS train 0.3499430159769826 valid 0.2547279362167631
LOSS train 0.3499430159769826 valid 0.25403710319237277
LOSS train 0.3499430159769826 valid 0.25365574204403424
LOSS train 0.3499430159769826 valid 0.2531408332288265
LOSS train 0.3499430159769826 valid 0.2517669051885605
LOSS train 0.3499430159769826 valid 0.2518141493201256
LOSS train 0.3499430159769826 valid 0.2516068137354321
LOSS train 0.3499430159769826 valid 0.25155007040926386
LOSS train 0.3499430159769826 valid 0.2515775843940932
LOSS train 0.3499430159769826 valid 0.25276371588309604
LOSS train 0.3499430159769826 valid 0.25428184914973473
LOSS train 0.3499430159769826 valid 0.25420538941398263
LOSS train 0.3499430159769826 valid 0.2553862933859681
LOSS train 0.3499430159769826 valid 0.2550719840561642
LOSS train 0.3499430159769826 valid 0.2562337709324701
LOSS train 0.3499430159769826 valid 0.2558833389646477
LOSS train 0.3499430159769826 valid 0.2558300974401268
LOSS train 0.3499430159769826 valid 0.25687399781063985
LOSS train 0.3499430159769826 valid 0.2558169731727013
LOSS train 0.3499430159769826 valid 0.25606240779161454
LOSS train 0.3499430159769826 valid 0.2568457083004277
LOSS train 0.3499430159769826 valid 0.256857611593746
LOSS train 0.3499430159769826 valid 0.2567412846310194
LOSS train 0.3499430159769826 valid 0.2571534216403961
LOSS train 0.3499430159769826 valid 0.25683150192101795
LOSS train 0.3499430159769826 valid 0.25747188893349277
LOSS train 0.3499430159769826 valid 0.2576982382130116
LOSS train 0.3499430159769826 valid 0.2577716326341033
LOSS train 0.3499430159769826 valid 0.2583926079832778
LOSS train 0.3499430159769826 valid 0.258091701567173
LOSS train 0.3499430159769826 valid 0.25872353216012317
LOSS train 0.3499430159769826 valid 0.2584930993616581
LOSS train 0.3499430159769826 valid 0.25854616485676674
LOSS train 0.3499430159769826 valid 0.25845627183163605
LOSS train 0.3499430159769826 valid 0.258569599552588
LOSS train 0.3499430159769826 valid 0.2582372791532959
LOSS train 0.3499430159769826 valid 0.257882097834035
LOSS train 0.3499430159769826 valid 0.25746968773932294
LOSS train 0.3499430159769826 valid 0.25742743151672814
LOSS train 0.3499430159769826 valid 0.25716351295510925
LOSS train 0.3499430159769826 valid 0.25707591092977367
LOSS train 0.3499430159769826 valid 0.25767297922603544
LOSS train 0.3499430159769826 valid 0.2577635846913807
LOSS train 0.3499430159769826 valid 0.25850184517912567
LOSS train 0.3499430159769826 valid 0.25914163429003495
LOSS train 0.3499430159769826 valid 0.25912481568979495
LOSS train 0.3499430159769826 valid 0.25872966335780584
LOSS train 0.3499430159769826 valid 0.25897692231570973
LOSS train 0.3499430159769826 valid 0.2584424306085144
LOSS train 0.3499430159769826 valid 0.2585079135639327
LOSS train 0.3499430159769826 valid 0.25820102930908473
LOSS train 0.3499430159769826 valid 0.2583804852846596
LOSS train 0.3499430159769826 valid 0.2584046584694353
LOSS train 0.3499430159769826 valid 0.2582822790419733
LOSS train 0.3499430159769826 valid 0.2583093529939651
LOSS train 0.3499430159769826 valid 0.25864607507460996
LOSS train 0.3499430159769826 valid 0.2585544135276373
LOSS train 0.3499430159769826 valid 0.2586773551809482
LOSS train 0.3499430159769826 valid 0.25873956065389175
LOSS train 0.3499430159769826 valid 0.25816842559725045
LOSS train 0.3499430159769826 valid 0.25742337530777776
LOSS train 0.3499430159769826 valid 0.2576486430153614
LOSS train 0.3499430159769826 valid 0.2574274423251669
LOSS train 0.3499430159769826 valid 0.2573250686483724
LOSS train 0.3499430159769826 valid 0.2570312401827644
LOSS train 0.3499430159769826 valid 0.256328240903311
LOSS train 0.3499430159769826 valid 0.256397012492706
LOSS train 0.3499430159769826 valid 0.2559881237420169
LOSS train 0.3499430159769826 valid 0.2561999921048625
LOSS train 0.3499430159769826 valid 0.2564347462521659
LOSS train 0.3499430159769826 valid 0.2565480696631002
LOSS train 0.3499430159769826 valid 0.25645303823377774
LOSS train 0.3499430159769826 valid 0.25625601779389123
LOSS train 0.3499430159769826 valid 0.25650467723608017
LOSS train 0.3499430159769826 valid 0.25628146591939427
LOSS train 0.3499430159769826 valid 0.25646694159756106
LOSS train 0.3499430159769826 valid 0.256468839252118
LOSS train 0.3499430159769826 valid 0.2564638293519312
LOSS train 0.3499430159769826 valid 0.25667564646162166
LOSS train 0.3499430159769826 valid 0.25696472615003585
LOSS train 0.3499430159769826 valid 0.25723314609858067
LOSS train 0.3499430159769826 valid 0.2573688424685422
LOSS train 0.3499430159769826 valid 0.2572970887989674
LOSS train 0.3499430159769826 valid 0.2571566923019978
LOSS train 0.3499430159769826 valid 0.25721230719770705
LOSS train 0.3499430159769826 valid 0.25749994041222446
LOSS train 0.3499430159769826 valid 0.25734644640828963
LOSS train 0.3499430159769826 valid 0.2574256406062179
LOSS train 0.3499430159769826 valid 0.2578375696315678
LOSS train 0.3499430159769826 valid 0.25831497623161837
LOSS train 0.3499430159769826 valid 0.25800295805071927
LOSS train 0.3499430159769826 valid 0.2578168172123177
LOSS train 0.3499430159769826 valid 0.2577659689483389
LOSS train 0.3499430159769826 valid 0.25770401941579685
LOSS train 0.3499430159769826 valid 0.2578794778689094
LOSS train 0.3499430159769826 valid 0.25808827000959167
LOSS train 0.3499430159769826 valid 0.2582420664720046
LOSS train 0.3499430159769826 valid 0.25816807037187833
LOSS train 0.3499430159769826 valid 0.2582154430511619
LOSS train 0.3499430159769826 valid 0.2579533410569032
LOSS train 0.3499430159769826 valid 0.2579337415123774
LOSS train 0.3499430159769826 valid 0.2579424171174159
LOSS train 0.3499430159769826 valid 0.25801802335715873
LOSS train 0.3499430159769826 valid 0.25816681644608896
LOSS train 0.3499430159769826 valid 0.2582820882797241
LOSS train 0.3499430159769826 valid 0.2584218233823776
LOSS train 0.3499430159769826 valid 0.25838089951380033
LOSS train 0.3499430159769826 valid 0.2585729092825204
LOSS train 0.3499430159769826 valid 0.2586243817972582
LOSS train 0.3499430159769826 valid 0.2586858593500577
LOSS train 0.3499430159769826 valid 0.25861631299703175
LOSS train 0.3499430159769826 valid 0.25827044327602244
LOSS train 0.3499430159769826 valid 0.25821339243784885
LOSS train 0.3499430159769826 valid 0.25827461581176786
LOSS train 0.3499430159769826 valid 0.2583330599246202
LOSS train 0.3499430159769826 valid 0.2582718768321416
LOSS train 0.3499430159769826 valid 0.2581705671809886
LOSS train 0.3499430159769826 valid 0.2580348686247632
LOSS train 0.3499430159769826 valid 0.2580069900631047
LOSS train 0.3499430159769826 valid 0.25803001214350973
LOSS train 0.3499430159769826 valid 0.2582994391943546
LOSS train 0.3499430159769826 valid 0.25850552393936777
LOSS train 0.3499430159769826 valid 0.25848954863898405
LOSS train 0.3499430159769826 valid 0.25854828570865923
LOSS train 0.3499430159769826 valid 0.2582254991449159
LOSS train 0.3499430159769826 valid 0.2585469864002646
LOSS train 0.3499430159769826 valid 0.2583926292706509
LOSS train 0.3499430159769826 valid 0.2591117043551561
LOSS train 0.3499430159769826 valid 0.25934483330921837
LOSS train 0.3499430159769826 valid 0.25927023371060687
LOSS train 0.3499430159769826 valid 0.2592428064109474
LOSS train 0.3499430159769826 valid 0.25905152469089154
LOSS train 0.3499430159769826 valid 0.25920454602615506
LOSS train 0.3499430159769826 valid 0.2593018973802591
LOSS train 0.3499430159769826 valid 0.25926182500777706
LOSS train 0.3499430159769826 valid 0.25956832063503754
LOSS train 0.3499430159769826 valid 0.25948320053944923
LOSS train 0.3499430159769826 valid 0.25959978677049467
LOSS train 0.3499430159769826 valid 0.2596332434588258
LOSS train 0.3499430159769826 valid 0.2594966759905219
LOSS train 0.3499430159769826 valid 0.2594575409933647
LOSS train 0.3499430159769826 valid 0.25925630349435924
LOSS train 0.3499430159769826 valid 0.25906565438018986
LOSS train 0.3499430159769826 valid 0.2588522557805224
LOSS train 0.3499430159769826 valid 0.2588298237685001
LOSS train 0.3499430159769826 valid 0.2590015997369605
LOSS train 0.3499430159769826 valid 0.2591322183965923
LOSS train 0.3499430159769826 valid 0.2590369140463216
LOSS train 0.3499430159769826 valid 0.2591829798983399
LOSS train 0.3499430159769826 valid 0.2593164447475882
LOSS train 0.3499430159769826 valid 0.259298698072545
LOSS train 0.3499430159769826 valid 0.25911193011805067
LOSS train 0.3499430159769826 valid 0.25923037718486236
LOSS train 0.3499430159769826 valid 0.2592491750744568
LOSS train 0.3499430159769826 valid 0.2591024204662868
LOSS train 0.3499430159769826 valid 0.25911665826358576
LOSS train 0.3499430159769826 valid 0.25904093867978134
LOSS train 0.3499430159769826 valid 0.2591539350955674
LOSS train 0.3499430159769826 valid 0.25912539044904975
LOSS train 0.3499430159769826 valid 0.2593004696071148
LOSS train 0.3499430159769826 valid 0.2593795251286491
LOSS train 0.3499430159769826 valid 0.2595797091886237
LOSS train 0.3499430159769826 valid 0.25947580205612497
LOSS train 0.3499430159769826 valid 0.25960261625764164
LOSS train 0.3499430159769826 valid 0.25933872855998374
LOSS train 0.3499430159769826 valid 0.2593454922879896
LOSS train 0.3499430159769826 valid 0.25911629263729974
LOSS train 0.3499430159769826 valid 0.25924262626374023
LOSS train 0.3499430159769826 valid 0.25922988252665
LOSS train 0.3499430159769826 valid 0.25928985398066673
LOSS train 0.3499430159769826 valid 0.25924546213050165
LOSS train 0.3499430159769826 valid 0.2592962756752968
LOSS train 0.3499430159769826 valid 0.2593198336776674
LOSS train 0.3499430159769826 valid 0.2592215352912539
LOSS train 0.3499430159769826 valid 0.2590846972587781
LOSS train 0.3499430159769826 valid 0.25917595853002706
LOSS train 0.3499430159769826 valid 0.25919421977803186
LOSS train 0.3499430159769826 valid 0.2591374387042691
LOSS train 0.3499430159769826 valid 0.2590971261113133
LOSS train 0.3499430159769826 valid 0.25899781174957753
LOSS train 0.3499430159769826 valid 0.25887335629308994
LOSS train 0.3499430159769826 valid 0.25882921456405433
LOSS train 0.3499430159769826 valid 0.2587128417392082
LOSS train 0.3499430159769826 valid 0.2586719154873315
LOSS train 0.3499430159769826 valid 0.2584966615932744
LOSS train 0.3499430159769826 valid 0.2585114642833043
LOSS train 0.3499430159769826 valid 0.2583583559534976
LOSS train 0.3499430159769826 valid 0.25824903854383874
LOSS train 0.3499430159769826 valid 0.25808828070973666
LOSS train 0.3499430159769826 valid 0.2581028890042078
LOSS train 0.3499430159769826 valid 0.25816133095754834
LOSS train 0.3499430159769826 valid 0.25796879827976227
LOSS train 0.3499430159769826 valid 0.25805563853939933
LOSS train 0.3499430159769826 valid 0.25794583377994107
LOSS train 0.3499430159769826 valid 0.2577999402617299
LOSS train 0.3499430159769826 valid 0.2578209395071975
LOSS train 0.3499430159769826 valid 0.25792346015778556
LOSS train 0.3499430159769826 valid 0.25797876312371787
LOSS train 0.3499430159769826 valid 0.258004116345214
LOSS train 0.3499430159769826 valid 0.2580919335511598
LOSS train 0.3499430159769826 valid 0.2580079387081155
LOSS train 0.3499430159769826 valid 0.25796236312604165
LOSS train 0.3499430159769826 valid 0.25807249652965186
LOSS train 0.3499430159769826 valid 0.25820711920303957
LOSS train 0.3499430159769826 valid 0.2581940579414368
LOSS train 0.3499430159769826 valid 0.25827192047000985
LOSS train 0.3499430159769826 valid 0.25853269281366326
LOSS train 0.3499430159769826 valid 0.258630029072887
LOSS train 0.3499430159769826 valid 0.2587375251747115
LOSS train 0.3499430159769826 valid 0.25878405713516733
LOSS train 0.3499430159769826 valid 0.2587415116044866
LOSS train 0.3499430159769826 valid 0.2587385742186472
LOSS train 0.3499430159769826 valid 0.258682904900911
LOSS train 0.3499430159769826 valid 0.2587400540454775
LOSS train 0.3499430159769826 valid 0.25886628773618253
LOSS train 0.3499430159769826 valid 0.258820536656905
LOSS train 0.3499430159769826 valid 0.2588651202902009
LOSS train 0.3499430159769826 valid 0.2587828396999535
LOSS train 0.3499430159769826 valid 0.2586922860045812
LOSS train 0.3499430159769826 valid 0.2586236021791895
LOSS train 0.3499430159769826 valid 0.258645196858778
LOSS train 0.3499430159769826 valid 0.2584364282444489
LOSS train 0.3499430159769826 valid 0.2585474512704606
LOSS train 0.3499430159769826 valid 0.2586753717699989
LOSS train 0.3499430159769826 valid 0.2588202764793318
LOSS train 0.3499430159769826 valid 0.25880591522871965
LOSS train 0.3499430159769826 valid 0.258870851173092
LOSS train 0.3499430159769826 valid 0.25883190514099214
LOSS train 0.3499430159769826 valid 0.2587522985226658
LOSS train 0.3499430159769826 valid 0.25878178560733794
LOSS train 0.3499430159769826 valid 0.25881593956415394
LOSS train 0.3499430159769826 valid 0.2589160538618527
LOSS train 0.3499430159769826 valid 0.25885715108850726
LOSS train 0.3499430159769826 valid 0.25885150886661423
LOSS train 0.3499430159769826 valid 0.2589093899609996
LOSS train 0.3499430159769826 valid 0.25888802408007905
LOSS train 0.3499430159769826 valid 0.2588357875551231
LOSS train 0.3499430159769826 valid 0.25890894765539685
LOSS train 0.3499430159769826 valid 0.25886462450487735
LOSS train 0.3499430159769826 valid 0.2587792147237521
LOSS train 0.3499430159769826 valid 0.258863820369673
LOSS train 0.3499430159769826 valid 0.25883380949269724
LOSS train 0.3499430159769826 valid 0.2588711125429139
LOSS train 0.3499430159769826 valid 0.2589332850254846
LOSS train 0.3499430159769826 valid 0.25891314446926117
LOSS train 0.3499430159769826 valid 0.2589143858032119
LOSS train 0.3499430159769826 valid 0.25906162466226
LOSS train 0.3499430159769826 valid 0.2591970120904161
LOSS train 0.3499430159769826 valid 0.25942921223028886
LOSS train 0.3499430159769826 valid 0.2594338776336776
LOSS train 0.3499430159769826 valid 0.25949276059975923
LOSS train 0.3499430159769826 valid 0.2596520423779593
LOSS train 0.3499430159769826 valid 0.2596716238356335
LOSS train 0.3499430159769826 valid 0.25963037129301225
LOSS train 0.3499430159769826 valid 0.259622070572593
LOSS train 0.3499430159769826 valid 0.2595299809523251
LOSS train 0.3499430159769826 valid 0.2594104497441316
LOSS train 0.3499430159769826 valid 0.25929983967928577
LOSS train 0.3499430159769826 valid 0.25936234680982473
LOSS train 0.3499430159769826 valid 0.25927312315574713
LOSS train 0.3499430159769826 valid 0.2591504641805255
LOSS train 0.3499430159769826 valid 0.2590399706300269
LOSS train 0.3499430159769826 valid 0.25904252919839044
LOSS train 0.3499430159769826 valid 0.2591313547126844
LOSS train 0.3499430159769826 valid 0.25918581815142383
LOSS train 0.3499430159769826 valid 0.2591434621831754
LOSS train 0.3499430159769826 valid 0.25910818441819644
LOSS train 0.3499430159769826 valid 0.2591657405719161
LOSS train 0.3499430159769826 valid 0.259154788350564
LOSS train 0.3499430159769826 valid 0.2592090553250806
LOSS train 0.3499430159769826 valid 0.2591902695570615
LOSS train 0.3499430159769826 valid 0.25922656477722406
LOSS train 0.3499430159769826 valid 0.2592353567531898
LOSS train 0.3499430159769826 valid 0.2592937505366851
LOSS train 0.3499430159769826 valid 0.25936579997256654
LOSS train 0.3499430159769826 valid 0.2593628698708238
LOSS train 0.3499430159769826 valid 0.25943250306929
LOSS train 0.3499430159769826 valid 0.2594237574594933
LOSS train 0.3499430159769826 valid 0.2594947203943961
LOSS train 0.3499430159769826 valid 0.2594485703110695
LOSS train 0.3499430159769826 valid 0.2594514637690446
LOSS train 0.3499430159769826 valid 0.25946189038800876
LOSS train 0.3499430159769826 valid 0.2594791256358521
LOSS train 0.3499430159769826 valid 0.2593994821470819
LOSS train 0.3499430159769826 valid 0.25934213932420386
LOSS train 0.3499430159769826 valid 0.25929539056580053
LOSS train 0.3499430159769826 valid 0.2592612985970531
LOSS train 0.3499430159769826 valid 0.2592144822822763
LOSS train 0.3499430159769826 valid 0.2592826249819357
LOSS train 0.3499430159769826 valid 0.2593389018408714
LOSS train 0.3499430159769826 valid 0.2593137814971795
LOSS train 0.3499430159769826 valid 0.259339792558398
LOSS train 0.3499430159769826 valid 0.25937792235098706
LOSS train 0.3499430159769826 valid 0.2595050107616528
LOSS train 0.3499430159769826 valid 0.25953402429346056
LOSS train 0.3499430159769826 valid 0.25949531105122986
LOSS train 0.3499430159769826 valid 0.2595536804725695
LOSS train 0.3499430159769826 valid 0.25971689764058814
LOSS train 0.3499430159769826 valid 0.25977942384896235
LOSS train 0.3499430159769826 valid 0.2596951820421964
LOSS train 0.3499430159769826 valid 0.25978825698574753
LOSS train 0.3499430159769826 valid 0.25974702423218615
LOSS train 0.3499430159769826 valid 0.25968352024769265
LOSS train 0.3499430159769826 valid 0.25964898818436966
LOSS train 0.3499430159769826 valid 0.2596957840369298
LOSS train 0.3499430159769826 valid 0.2597749182965858
LOSS train 0.3499430159769826 valid 0.25975002681078896
LOSS train 0.3499430159769826 valid 0.25971956442042093
LOSS train 0.3499430159769826 valid 0.25980300825417585
LOSS train 0.3499430159769826 valid 0.25971753037337103
LOSS train 0.3499430159769826 valid 0.25957097765902376
LOSS train 0.3499430159769826 valid 0.25953417868319767
LOSS train 0.3499430159769826 valid 0.2595523641900615
LOSS train 0.3499430159769826 valid 0.25973460020240907
LOSS train 0.3499430159769826 valid 0.25972219790095713
LOSS train 0.3499430159769826 valid 0.25967700437953073
LOSS train 0.3499430159769826 valid 0.259645069167239
LOSS train 0.3499430159769826 valid 0.2596169399351058
LOSS train 0.3499430159769826 valid 0.25961558424090214
LOSS train 0.3499430159769826 valid 0.2596157361917636
LOSS train 0.3499430159769826 valid 0.2595454763369826
LOSS train 0.3499430159769826 valid 0.2595117547445827
LOSS train 0.3499430159769826 valid 0.259498636202979
LOSS train 0.3499430159769826 valid 0.25962360484828784
LOSS train 0.3499430159769826 valid 0.2596455950235975
LOSS train 0.3499430159769826 valid 0.25959578091870844
LOSS train 0.3499430159769826 valid 0.2594612799906937
LOSS train 0.3499430159769826 valid 0.25938265419554435
LOSS train 0.3499430159769826 valid 0.25948942561545824
LOSS train 0.3499430159769826 valid 0.2594225146089281
LOSS train 0.3499430159769826 valid 0.2593469093328188
LOSS train 0.3499430159769826 valid 0.25932402976534585
LOSS train 0.3499430159769826 valid 0.2593865058577432
LOSS train 0.3499430159769826 valid 0.2594984205765913
LOSS train 0.3499430159769826 valid 0.25956264499207615
LOSS train 0.3499430159769826 valid 0.259621777896131
LOSS train 0.3499430159769826 valid 0.25955329622541157
LOSS train 0.3499430159769826 valid 0.25951822281050285
LOSS train 0.3499430159769826 valid 0.2595795615884919
LOSS train 0.3499430159769826 valid 0.2595690275645918
LOSS train 0.3499430159769826 valid 0.2595406719109358
LOSS train 0.3499430159769826 valid 0.2596205046081411
LOSS train 0.3499430159769826 valid 0.2594473045062756
LOSS train 0.3499430159769826 valid 0.2594613853719208
LOSS train 0.3499430159769826 valid 0.2595082590841267
LOSS train 0.3499430159769826 valid 0.259478513171764
LOSS train 0.3499430159769826 valid 0.25937816874207853
LOSS train 0.3499430159769826 valid 0.25930652257217013
LOSS train 0.3499430159769826 valid 0.2594060441504326
Training bichrom
DEVICE = cpu
####################
Total Parameters = 606342
Total Trainable Parameters = 1157
bimodal_network(
  (base_model): bichrom_seq(
    (conv1d): Conv1d(4, 256, kernel_size=(24,), stride=(1,))
    (relu): ReLU()
    (batchNorm1d): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (maxPool1d): MaxPool1d(kernel_size=15, stride=15, padding=0, dilation=1, ceil_mode=True)
    (lstm): LSTM(256, 32, batch_first=True)
    (tanh): Tanh()
    (model_dense_repeat): Sequential(
      (0): Linear(in_features=32, out_features=512, bias=True)
      (1): ReLU()
      (2): Dropout(p=0.5, inplace=False)
      (3): Linear(in_features=512, out_features=512, bias=True)
      (4): ReLU()
      (5): Dropout(p=0.5, inplace=False)
      (6): Linear(in_features=512, out_features=512, bias=True)
      (7): ReLU()
      (8): Dropout(p=0.5, inplace=False)
    )
    (linear): Linear(in_features=512, out_features=1, bias=True)
    (sigmoid): Sigmoid()
  )
  (linear): Linear(in_features=512, out_features=1, bias=True)
  (tanh): Tanh()
  (model): bichrom_chrom(
    (_reshape): _reshape()
    (conv1d): Conv1d(12, 15, kernel_size=(1,), stride=(1,), padding=valid)
    (relu): ReLU()
    (lstm): LSTM(15, 5, batch_first=True)
    (relu2): ReLU()
    (linear): Linear(in_features=5, out_features=1, bias=True)
    (tanh): Tanh()
  )
  (linear2): Linear(in_features=2, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)
base_model.conv1d.weight False
base_model.conv1d.bias False
base_model.batchNorm1d.weight False
base_model.batchNorm1d.bias False
base_model.lstm.weight_ih_l0 False
base_model.lstm.weight_hh_l0 False
base_model.lstm.bias_ih_l0 False
base_model.lstm.bias_hh_l0 False
base_model.model_dense_repeat.0.weight False
base_model.model_dense_repeat.0.bias False
base_model.model_dense_repeat.3.weight False
base_model.model_dense_repeat.3.bias False
base_model.model_dense_repeat.6.weight False
base_model.model_dense_repeat.6.bias False
base_model.linear.weight False
base_model.linear.bias False
linear.weight True
linear.bias True
model.conv1d.weight True
model.conv1d.bias True
model.lstm.weight_ih_l0 True
model.lstm.weight_hh_l0 True
model.lstm.bias_ih_l0 True
model.lstm.bias_hh_l0 True
model.linear.weight True
model.linear.bias True
linear2.weight True
linear2.bias True
####################
EPOCH 1:
  batch 1 loss: 0.7090902924537659
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.704029768705368
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.705651581287384
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.708421066403389
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.7060149431228637
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.7043796181678772
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.7034147296633039
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.7020582035183907
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.7010207441118028
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.6987444758415222
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.6978968273509633
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.6970975150664648
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.6949080641453083
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.6926661900111607
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.690831716855367
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.6896364763379097
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.6875511863652397
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.685481071472168
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.6834424671373869
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.6817994892597199
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.6791195500464666
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.6765614937652241
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.6738710144291753
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.6712301994363467
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.6686239004135132
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.6656653353801141
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.6628418719327008
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.6599616365773338
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.6567591490416691
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.6540675699710846
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.6510304923980467
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.6480082850903273
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.6443596969951283
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.6414153155158547
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.6386967199189323
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.6355527655945884
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.6323924547917134
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.6288627480205736
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.625657530931326
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.6222000703215599
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.6189807195489
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.6159274166538602
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.6130087181579235
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.6097316877408461
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.6066758175690968
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.6032872685919637
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.5998840915395859
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.5968690005441507
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.593740429197039
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.5908819508552551
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.5878928391372457
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.5847657741262362
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.5819561413998874
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.5789198809199863
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.5761452067982067
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.5731459552688258
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.5703987243928408
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.567636017141671
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.5650542923959635
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.5623860304554303
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.5596616463583024
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.5571506403146251
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.55460134716261
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.5519588463939726
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.5495775300722856
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.5471026920007936
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.5446925768211707
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.5423952016760322
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.540151793455732
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.538176851613181
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.5358381065684306
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.5334350595043765
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.531340333696914
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.5291781357011279
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.5273229455947877
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.5252668473281359
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.5229870971147116
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.5208620681212499
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.5188019049318531
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.5169238358736038
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.5150177684831031
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.5132268278337107
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.51135166689574
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.5094132246006102
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.5075944062541513
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.5056595732999403
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.5040401086725038
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.5022523643618281
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.5000813318772263
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.49845907820595636
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.49687927928599684
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.49506253878707474
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.4934965378494673
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.4923680776611288
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.49095843904896785
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.4891590628152092
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.4873523328107657
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.4860684543239827
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.4846103215458417
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.48327094197273257
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.48161567408259554
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.48007869136099723
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.4789092355561488
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.4775233228619282
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.4759095061393011
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.47455936698418744
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.47332415068260975
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.4720552900323161
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.47081527770112414
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.4695627139373259
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.4681111270779962
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.46663947137338774
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.46513043119844083
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.46389363104836984
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.46253897469976674
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.4613136125021967
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.46013957363927466
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.45878119564662545
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.45756137646546885
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.4563584141433239
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.4552165380686768
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.4540571284587266
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.4532322411130114
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.4520915936558477
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.4511243088245392
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.45027545165447963
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.4492871890387197
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.4484471695031971
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.4474916532058124
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.446552165884238
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.4453267486950823
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.4443228247039246
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.44353634522373514
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.44255834906848507
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.4417189249285945
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.44093772243050966
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.4402683035735666
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.43950358767440356
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.4389302008014789
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.4380947885768754
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.4373555646297779
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.43621831040986825
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.43559664717087376
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.43511473532352185
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.4342546892577204
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.4334831084698847
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.43249405788726547
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.4315748468444154
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.43088247271992214
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.4301233148574829
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.4293403070888772
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.42842356644962964
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.4275059686377158
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.4267124545264554
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.4261248869280661
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.4252989804133391
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.42449932227468795
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.4238395443822764
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.4235797933062667
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.4227386565878987
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.4220031222201282
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.4212250887979696
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.4205952233697739
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.42002329793645116
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.4192254713087371
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.41844644783491114
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.41774906173437654
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.4171546009324846
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.41665722526742155
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.4160699460436316
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.41539576318528915
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.4146143565344256
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.41390612221866674
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.4132529216251154
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.41267001237188067
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.4119307024573738
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.4112456373575717
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.410565479585294
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.41004549041806654
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.4094053359495269
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.4089477964527699
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.408478516798753
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.4077547298428791
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.4071811769643556
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.40655412303434835
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.4059385718196951
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.40536078635384054
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.40479545105010906
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.4041421314080556
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.4034551722438712
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.4030836628681702
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.40249553772931296
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.4021827005660596
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.4015816706664783
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.4010553447099832
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.400690173008004
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.40019602642446606
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.39991753300031024
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.3994354770411199
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.39902950018644334
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.3983998114789896
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.39776801944959284
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.39715860453732493
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.3965680828573657
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.3960786943028613
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.3955587656463234
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.39503833853104264
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.39433464121360046
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.3937534448917973
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.3933996928589685
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.39297547385590903
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.3927267112259595
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.39245471330315856
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.3918341213019095
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.39148712601772573
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.39094160373012227
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.39047713048996463
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.3900617888761223
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.38983514782500595
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.38966296111995524
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.38925636151797094
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.38886109370369093
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.38841559119823266
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.3880229282325932
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.3875436880853441
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.38716649705857303
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.3866233584114108
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.38609427540448676
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.38573450722028074
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.3853740921486979
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.3849178364266565
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.3844820177503701
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.38415773475118975
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.3838333076136744
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.383377702439085
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.3831100563629199
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.3828436561274629
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.3825356285361683
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.38226084330091914
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.3818347794314226
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.38153079943538204
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.38107847637874037
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.38073606611279304
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.38047164898426805
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.3802050394671304
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.38004387539576706
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.379665342902365
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.37926295099239193
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.37894444245411213
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.3786650723218918
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.37842476379823875
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.37808900823195773
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.3777614469113557
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.3775070485636944
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.3772261381149292
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.3768706031842157
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.3765548524457657
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.37619540199290874
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.3758718736153312
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.37554967644122933
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.375241092452601
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.37483858971195366
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.3745286555344614
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.3741056365948735
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.3737791946474111
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.3736756752084072
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.3733241284607948
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.37301874016203096
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.3726147196106751
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.3723715170666024
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.3720486367540606
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.3717870321124792
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.3715547678016481
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.37124327075307384
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.37082112485712226
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.3706016701416693
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.37028176130370544
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.3700170314354862
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.36977828820119196
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.369474380250488
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.36920110992689575
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.3688906281129688
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.3685529559534767
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.3681362979638744
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.36791616458641857
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.36766100638396254
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.367354233401993
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.3671131985675957
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.36684848476446213
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.36664936008124516
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.3663152593517631
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.3661715812879066
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.36600207194126505
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.3657601270951381
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.3655493459459079
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.3652808287659207
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.3650862462191469
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.3647470257226253
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.3644035351914307
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.3640896667043368
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.3639323233171951
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.36362571382759423
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.36335652958441883
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.36308935588519825
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.36293226617281554
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.3626074044922598
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.362351507045547
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.36207093424224235
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.3617933937453915
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.36149792565453437
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.361232933603299
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.36103047210818684
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.36074023534314703
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.3604395336406246
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.3602788435088264
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.3601013009872618
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.35997276041033893
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.3597712307793539
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.3594883081494454
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.3592791981995106
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.35896187287253384
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.35865602350753284
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.3584603003857675
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.35825706641247246
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.3580550884283506
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.3578363926681273
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.3575715089063032
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.35745461812106577
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.3571476303154212
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.35710955397649247
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.3569863575462848
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.3567585137235113
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.3564917581217425
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.35625966937242154
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.3560783002803575
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.35600401638519197
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.35566174961869723
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.3553573516963502
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.3551027176566532
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.3549204654991627
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.3546751932926542
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.35461226275615526
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.3544119096445273
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.35415409665641395
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.3539036345222722
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.35360995249424365
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.3534897703688152
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.3532993395695056
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.35305390287094607
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.35272230501685825
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.35255208590261616
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.3524316741068932
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.3522684611696697
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.35208489080775257
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.35203815887511614
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.3517991245378939
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.35154974389643895
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.3513751255983081
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.35113737223706204
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.35085033708148533
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.3506090047451928
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.3503562711714381
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.350152036785751
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.3499173136366593
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.3497442700275003
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.34949308422093833
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.34928816427002163
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.34917785381169425
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.34906596938769024
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.3488715721948727
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.34863731265068054
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.34840483739170974
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.3482971694769872
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.34817271866900396
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.3480459645589193
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.34796135753710217
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.347872093833726
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.3477433204808563
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.34749798732256826
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.3472742889272539
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.34704299525326004
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.3468951651563195
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.3468222280395249
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.34659182544176775
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.3464359401108383
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.3462618145454733
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.346192050118779
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.3460237788631744
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.3459223114279671
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.34575220468716744
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.3455934621336515
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.3454719021612284
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.34533575397108046
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.3452290618026317
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.345099834626234
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.34493447862791293
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.3447907293773718
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.34467183360502346
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.3444137685654456
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.3443562914058566
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.34415544628948347
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.3440232858506601
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.34391073690306756
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.3438228172904784
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.34362312382386057
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.3434824423469933
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.3434197524475524
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.3432931538086896
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.3431149546630808
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.3429932341706462
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.3428308597228823
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.34273903256336463
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.34262769999429044
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.3425515679561574
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.3424359742058329
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.34227107848542243
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.3421248067840398
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.34194591031833127
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.34187573027354723
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.3416855415063245
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.34170675111354953
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.34161220452090574
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.3414348627024509
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.34128061270798155
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.34117747555760775
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.34100975127707067
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.34086433405479727
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.34071755224717
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.3404533159691137
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.3403104423437008
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.3402073367773242
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.34006112900183155
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.33984029867770216
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.3397836957453033
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.339638535352959
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.3396331952594289
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.3395756535931092
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.33941889214053
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.3391730852577844
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.33904146430167287
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.33898592752123635
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.33894197568634515
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.3388706198126145
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.3388753233326448
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.3388134680437238
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.33873053282632953
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.3385873586939485
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.3384896697742598
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.3384107617466381
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.3382014657722579
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.3380180456884156
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.33798026244065404
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.3378295742189121
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.3377158751112249
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.3375970167445613
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.3375442829636628
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.3374517650325137
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.3373592361547541
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.33725478242035783
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.3371744910983936
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.3370594625682479
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.33700207033843704
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.3368370942047296
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.33679264934797737
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.3366663045460178
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.3365930246245196
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.33648391872070077
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.3364179879745357
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.3363713486425912
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.3363297980199469
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.33623436628000497
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.33613810015810747
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.33613810015810747 valid 0.3301905393600464
LOSS train 0.33613810015810747 valid 0.31697554886341095
LOSS train 0.33613810015810747 valid 0.31882400314013165
LOSS train 0.33613810015810747 valid 0.3191772848367691
LOSS train 0.33613810015810747 valid 0.31588069796562196
LOSS train 0.33613810015810747 valid 0.3184772729873657
LOSS train 0.33613810015810747 valid 0.32531270384788513
LOSS train 0.33613810015810747 valid 0.32376642897725105
LOSS train 0.33613810015810747 valid 0.32246754897965324
LOSS train 0.33613810015810747 valid 0.3258454352617264
LOSS train 0.33613810015810747 valid 0.32301954518664966
LOSS train 0.33613810015810747 valid 0.3253991901874542
LOSS train 0.33613810015810747 valid 0.32550249191430897
LOSS train 0.33613810015810747 valid 0.32564073375293184
LOSS train 0.33613810015810747 valid 0.3222217957178752
LOSS train 0.33613810015810747 valid 0.32286519184708595
LOSS train 0.33613810015810747 valid 0.3245277404785156
LOSS train 0.33613810015810747 valid 0.32552990482913124
LOSS train 0.33613810015810747 valid 0.3271093227361378
LOSS train 0.33613810015810747 valid 0.3260822996497154
LOSS train 0.33613810015810747 valid 0.32482478448322843
LOSS train 0.33613810015810747 valid 0.3232799524610693
LOSS train 0.33613810015810747 valid 0.3229710343091384
LOSS train 0.33613810015810747 valid 0.3223862697680791
LOSS train 0.33613810015810747 valid 0.32131756663322447
LOSS train 0.33613810015810747 valid 0.32131583530169266
LOSS train 0.33613810015810747 valid 0.3208485985243762
LOSS train 0.33613810015810747 valid 0.3210609161428043
LOSS train 0.33613810015810747 valid 0.32097088160185977
LOSS train 0.33613810015810747 valid 0.32269500295321146
LOSS train 0.33613810015810747 valid 0.32385693730846526
LOSS train 0.33613810015810747 valid 0.32339688390493393
LOSS train 0.33613810015810747 valid 0.32470576961835224
LOSS train 0.33613810015810747 valid 0.32447892778060017
LOSS train 0.33613810015810747 valid 0.32537720799446107
LOSS train 0.33613810015810747 valid 0.32509032305743957
LOSS train 0.33613810015810747 valid 0.3250934554112924
LOSS train 0.33613810015810747 valid 0.326222845598271
LOSS train 0.33613810015810747 valid 0.3250770232616327
LOSS train 0.33613810015810747 valid 0.32538404166698454
LOSS train 0.33613810015810747 valid 0.32663150022669535
LOSS train 0.33613810015810747 valid 0.32645557678881143
LOSS train 0.33613810015810747 valid 0.3266194920207179
LOSS train 0.33613810015810747 valid 0.32726575095545163
LOSS train 0.33613810015810747 valid 0.3264392932256063
LOSS train 0.33613810015810747 valid 0.32714907306691876
LOSS train 0.33613810015810747 valid 0.32727394459095405
LOSS train 0.33613810015810747 valid 0.3276165413359801
LOSS train 0.33613810015810747 valid 0.3282800967596015
LOSS train 0.33613810015810747 valid 0.3281721806526184
LOSS train 0.33613810015810747 valid 0.32871075179062637
LOSS train 0.33613810015810747 valid 0.32857231681163496
LOSS train 0.33613810015810747 valid 0.3283792717276879
LOSS train 0.33613810015810747 valid 0.328254039088885
LOSS train 0.33613810015810747 valid 0.32830706888979133
LOSS train 0.33613810015810747 valid 0.32803578515137943
LOSS train 0.33613810015810747 valid 0.32758617296553494
LOSS train 0.33613810015810747 valid 0.32704621347887763
LOSS train 0.33613810015810747 valid 0.32704245943134114
LOSS train 0.33613810015810747 valid 0.32675695021947226
LOSS train 0.33613810015810747 valid 0.32666008198847535
LOSS train 0.33613810015810747 valid 0.3269615505010851
LOSS train 0.33613810015810747 valid 0.3269927260421571
LOSS train 0.33613810015810747 valid 0.327812971547246
LOSS train 0.33613810015810747 valid 0.3282390365233788
LOSS train 0.33613810015810747 valid 0.32856929392525647
LOSS train 0.33613810015810747 valid 0.3281391872398889
LOSS train 0.33613810015810747 valid 0.3284785782589632
LOSS train 0.33613810015810747 valid 0.3278875843338344
LOSS train 0.33613810015810747 valid 0.3278762412922723
LOSS train 0.33613810015810747 valid 0.3275796724876887
LOSS train 0.33613810015810747 valid 0.32785522068540257
LOSS train 0.33613810015810747 valid 0.32794568799946405
LOSS train 0.33613810015810747 valid 0.3278714156634099
LOSS train 0.33613810015810747 valid 0.3278481078147888
LOSS train 0.33613810015810747 valid 0.3281550203499041
LOSS train 0.33613810015810747 valid 0.32823091164811863
LOSS train 0.33613810015810747 valid 0.32823114555615646
LOSS train 0.33613810015810747 valid 0.32828416854520387
LOSS train 0.33613810015810747 valid 0.32774737253785136
LOSS train 0.33613810015810747 valid 0.32695596674342214
LOSS train 0.33613810015810747 valid 0.3271925747394562
LOSS train 0.33613810015810747 valid 0.3270377516746521
LOSS train 0.33613810015810747 valid 0.3268576023124513
LOSS train 0.33613810015810747 valid 0.3267229357186486
LOSS train 0.33613810015810747 valid 0.32589966232000395
LOSS train 0.33613810015810747 valid 0.3259341213209876
LOSS train 0.33613810015810747 valid 0.3255590302023021
LOSS train 0.33613810015810747 valid 0.32586983348546406
LOSS train 0.33613810015810747 valid 0.3260588063134087
LOSS train 0.33613810015810747 valid 0.32613832478994853
LOSS train 0.33613810015810747 valid 0.32598850228216336
LOSS train 0.33613810015810747 valid 0.32581663003531836
LOSS train 0.33613810015810747 valid 0.3260586791216059
LOSS train 0.33613810015810747 valid 0.325724815067492
LOSS train 0.33613810015810747 valid 0.3260063153381149
LOSS train 0.33613810015810747 valid 0.32611083707858607
LOSS train 0.33613810015810747 valid 0.3260448651046169
LOSS train 0.33613810015810747 valid 0.32630987691156793
LOSS train 0.33613810015810747 valid 0.32668453246355056
LOSS train 0.33613810015810747 valid 0.3268525187331851
LOSS train 0.33613810015810747 valid 0.32708150615879134
LOSS train 0.33613810015810747 valid 0.3270419945994627
LOSS train 0.33613810015810747 valid 0.32680965014375174
LOSS train 0.33613810015810747 valid 0.32695269187291465
LOSS train 0.33613810015810747 valid 0.32731477831894495
LOSS train 0.33613810015810747 valid 0.3270725153874014
LOSS train 0.33613810015810747 valid 0.3271649540574462
LOSS train 0.33613810015810747 valid 0.3275255377686352
LOSS train 0.33613810015810747 valid 0.32804624140262606
LOSS train 0.33613810015810747 valid 0.32771734477163433
LOSS train 0.33613810015810747 valid 0.32756357373935835
LOSS train 0.33613810015810747 valid 0.32748115590188354
LOSS train 0.33613810015810747 valid 0.327416157513334
LOSS train 0.33613810015810747 valid 0.32760776307271877
LOSS train 0.33613810015810747 valid 0.32763824611902237
LOSS train 0.33613810015810747 valid 0.3277624195966965
LOSS train 0.33613810015810747 valid 0.3275944224353564
LOSS train 0.33613810015810747 valid 0.3276446619454552
LOSS train 0.33613810015810747 valid 0.3273378245532513
LOSS train 0.33613810015810747 valid 0.327225719847955
LOSS train 0.33613810015810747 valid 0.3271622862972197
LOSS train 0.33613810015810747 valid 0.3272426666283026
LOSS train 0.33613810015810747 valid 0.3273273925146749
LOSS train 0.33613810015810747 valid 0.3274480228424072
LOSS train 0.33613810015810747 valid 0.32757953164123355
LOSS train 0.33613810015810747 valid 0.32761567995304197
LOSS train 0.33613810015810747 valid 0.3278892885427922
LOSS train 0.33613810015810747 valid 0.327894412269888
LOSS train 0.33613810015810747 valid 0.3278972511108105
LOSS train 0.33613810015810747 valid 0.3278961511513659
LOSS train 0.33613810015810747 valid 0.32759633953824185
LOSS train 0.33613810015810747 valid 0.32751605766160147
LOSS train 0.33613810015810747 valid 0.32765733283847126
LOSS train 0.33613810015810747 valid 0.32772720654805504
LOSS train 0.33613810015810747 valid 0.3276598970679676
LOSS train 0.33613810015810747 valid 0.32754317361072904
LOSS train 0.33613810015810747 valid 0.3273943764144096
LOSS train 0.33613810015810747 valid 0.32728719325374356
LOSS train 0.33613810015810747 valid 0.3273792722395488
LOSS train 0.33613810015810747 valid 0.3275654599176231
LOSS train 0.33613810015810747 valid 0.3277607052259042
LOSS train 0.33613810015810747 valid 0.32773954843307707
LOSS train 0.33613810015810747 valid 0.3277301546186209
LOSS train 0.33613810015810747 valid 0.3274236843503755
LOSS train 0.33613810015810747 valid 0.3277701357864354
LOSS train 0.33613810015810747 valid 0.3275956736535442
LOSS train 0.33613810015810747 valid 0.3283001335488783
LOSS train 0.33613810015810747 valid 0.3285074914061783
LOSS train 0.33613810015810747 valid 0.3284551501274109
LOSS train 0.33613810015810747 valid 0.3283203506706566
LOSS train 0.33613810015810747 valid 0.32822457446079506
LOSS train 0.33613810015810747 valid 0.3283582643356199
LOSS train 0.33613810015810747 valid 0.3285214730671474
LOSS train 0.33613810015810747 valid 0.32848433679149996
LOSS train 0.33613810015810747 valid 0.32888705818316877
LOSS train 0.33613810015810747 valid 0.3287757631320103
LOSS train 0.33613810015810747 valid 0.32890143236027486
LOSS train 0.33613810015810747 valid 0.32897734342131224
LOSS train 0.33613810015810747 valid 0.3287901172414422
LOSS train 0.33613810015810747 valid 0.32883632571800897
LOSS train 0.33613810015810747 valid 0.32861890984170233
LOSS train 0.33613810015810747 valid 0.32845321425631
LOSS train 0.33613810015810747 valid 0.32826313231049514
LOSS train 0.33613810015810747 valid 0.3283481993458488
LOSS train 0.33613810015810747 valid 0.3285051384963185
LOSS train 0.33613810015810747 valid 0.3286171650101325
LOSS train 0.33613810015810747 valid 0.3285235996757235
LOSS train 0.33613810015810747 valid 0.32855823935841666
LOSS train 0.33613810015810747 valid 0.32875825640033274
LOSS train 0.33613810015810747 valid 0.32874290374984516
LOSS train 0.33613810015810747 valid 0.32859684786824295
LOSS train 0.33613810015810747 valid 0.3287522806597583
LOSS train 0.33613810015810747 valid 0.32879257972898157
LOSS train 0.33613810015810747 valid 0.328601678950446
LOSS train 0.33613810015810747 valid 0.32862876186316664
LOSS train 0.33613810015810747 valid 0.3285811908837766
LOSS train 0.33613810015810747 valid 0.3285925755005204
LOSS train 0.33613810015810747 valid 0.3285975964042728
LOSS train 0.33613810015810747 valid 0.32873795678218204
LOSS train 0.33613810015810747 valid 0.3288136468737165
LOSS train 0.33613810015810747 valid 0.32904042041563725
LOSS train 0.33613810015810747 valid 0.32892966417015573
LOSS train 0.33613810015810747 valid 0.329084875629
LOSS train 0.33613810015810747 valid 0.328823041271519
LOSS train 0.33613810015810747 valid 0.328881131865645
LOSS train 0.33613810015810747 valid 0.32864434451342905
LOSS train 0.33613810015810747 valid 0.3286797217232116
LOSS train 0.33613810015810747 valid 0.3287106474871358
LOSS train 0.33613810015810747 valid 0.3287932957473554
LOSS train 0.33613810015810747 valid 0.32871920322872583
LOSS train 0.33613810015810747 valid 0.32876659960796434
LOSS train 0.33613810015810747 valid 0.3288326569171767
LOSS train 0.33613810015810747 valid 0.32866493650932904
LOSS train 0.33613810015810747 valid 0.32852295759396677
LOSS train 0.33613810015810747 valid 0.32861489224798823
LOSS train 0.33613810015810747 valid 0.3286706336258632
LOSS train 0.33613810015810747 valid 0.32864876558082273
LOSS train 0.33613810015810747 valid 0.3286475852206724
LOSS train 0.33613810015810747 valid 0.3285509830713272
LOSS train 0.33613810015810747 valid 0.3284581884815918
LOSS train 0.33613810015810747 valid 0.3284212390680124
LOSS train 0.33613810015810747 valid 0.32826639176002276
LOSS train 0.33613810015810747 valid 0.32827531634008184
LOSS train 0.33613810015810747 valid 0.32814729911525076
LOSS train 0.33613810015810747 valid 0.328198226910193
LOSS train 0.33613810015810747 valid 0.32812075410488145
LOSS train 0.33613810015810747 valid 0.32797636564534444
LOSS train 0.33613810015810747 valid 0.32779083326102443
LOSS train 0.33613810015810747 valid 0.3278553023224785
LOSS train 0.33613810015810747 valid 0.327960678892678
LOSS train 0.33613810015810747 valid 0.32777406556426353
LOSS train 0.33613810015810747 valid 0.3277718419200378
LOSS train 0.33613810015810747 valid 0.32767002521274247
LOSS train 0.33613810015810747 valid 0.3275126010872597
LOSS train 0.33613810015810747 valid 0.32756018776584556
LOSS train 0.33613810015810747 valid 0.3276334301941955
LOSS train 0.33613810015810747 valid 0.32764414185230883
LOSS train 0.33613810015810747 valid 0.32761052550246184
LOSS train 0.33613810015810747 valid 0.327695959400047
LOSS train 0.33613810015810747 valid 0.3276305523663085
LOSS train 0.33613810015810747 valid 0.32764272901925956
LOSS train 0.33613810015810747 valid 0.3277974997400703
LOSS train 0.33613810015810747 valid 0.3279970431966441
LOSS train 0.33613810015810747 valid 0.3279884877469805
LOSS train 0.33613810015810747 valid 0.3280884318910869
LOSS train 0.33613810015810747 valid 0.3283333689105668
LOSS train 0.33613810015810747 valid 0.3283912420534251
LOSS train 0.33613810015810747 valid 0.3285013229045285
LOSS train 0.33613810015810747 valid 0.32856141264023986
LOSS train 0.33613810015810747 valid 0.32847001026203104
LOSS train 0.33613810015810747 valid 0.32844263071130064
LOSS train 0.33613810015810747 valid 0.3283684121948455
LOSS train 0.33613810015810747 valid 0.3284278026758096
LOSS train 0.33613810015810747 valid 0.32855624226813623
LOSS train 0.33613810015810747 valid 0.3285534552345842
LOSS train 0.33613810015810747 valid 0.3285982345981437
LOSS train 0.33613810015810747 valid 0.3284932715552194
LOSS train 0.33613810015810747 valid 0.3283803709880079
LOSS train 0.33613810015810747 valid 0.32830113420883816
LOSS train 0.33613810015810747 valid 0.328299282250068
LOSS train 0.33613810015810747 valid 0.3280981381323712
LOSS train 0.33613810015810747 valid 0.32815093145448976
LOSS train 0.33613810015810747 valid 0.3282653038374713
LOSS train 0.33613810015810747 valid 0.32843529910457375
LOSS train 0.33613810015810747 valid 0.32843773478899546
LOSS train 0.33613810015810747 valid 0.3285138667353734
LOSS train 0.33613810015810747 valid 0.3284508435716552
LOSS train 0.33613810015810747 valid 0.32839589987892703
LOSS train 0.33613810015810747 valid 0.32843135893344877
LOSS train 0.33613810015810747 valid 0.32848316276690875
LOSS train 0.33613810015810747 valid 0.3285742203394572
LOSS train 0.33613810015810747 valid 0.32855554696599487
LOSS train 0.33613810015810747 valid 0.32849245852842107
LOSS train 0.33613810015810747 valid 0.32857092665691
LOSS train 0.33613810015810747 valid 0.3285593467298895
LOSS train 0.33613810015810747 valid 0.3284856246835063
LOSS train 0.33613810015810747 valid 0.3285988844180292
LOSS train 0.33613810015810747 valid 0.32854212423549195
LOSS train 0.33613810015810747 valid 0.32849834847908754
LOSS train 0.33613810015810747 valid 0.3285250674033987
LOSS train 0.33613810015810747 valid 0.3284687496551121
LOSS train 0.33613810015810747 valid 0.32845203198860806
LOSS train 0.33613810015810747 valid 0.3284943465030555
LOSS train 0.33613810015810747 valid 0.3284676529326529
LOSS train 0.33613810015810747 valid 0.32848651864026723
LOSS train 0.33613810015810747 valid 0.32865222909030845
LOSS train 0.33613810015810747 valid 0.3287511827118361
LOSS train 0.33613810015810747 valid 0.3289423107436155
LOSS train 0.33613810015810747 valid 0.32892731991079116
LOSS train 0.33613810015810747 valid 0.3289748854742719
LOSS train 0.33613810015810747 valid 0.3291454349151429
LOSS train 0.33613810015810747 valid 0.3291824112881671
LOSS train 0.33613810015810747 valid 0.32913270179372633
LOSS train 0.33613810015810747 valid 0.32910638895901767
LOSS train 0.33613810015810747 valid 0.3290394566197326
LOSS train 0.33613810015810747 valid 0.3289138336904643
LOSS train 0.33613810015810747 valid 0.3288045101028552
LOSS train 0.33613810015810747 valid 0.3288961958714284
LOSS train 0.33613810015810747 valid 0.32885167662586484
LOSS train 0.33613810015810747 valid 0.32873116025296817
LOSS train 0.33613810015810747 valid 0.32865396797234286
LOSS train 0.33613810015810747 valid 0.3286374592528326
LOSS train 0.33613810015810747 valid 0.32874480362089586
LOSS train 0.33613810015810747 valid 0.3288306588666481
LOSS train 0.33613810015810747 valid 0.32880754664644496
LOSS train 0.33613810015810747 valid 0.3287562912764865
LOSS train 0.33613810015810747 valid 0.3288423479017284
LOSS train 0.33613810015810747 valid 0.3288503509904274
LOSS train 0.33613810015810747 valid 0.32888016228018135
LOSS train 0.33613810015810747 valid 0.32885441634663193
LOSS train 0.33613810015810747 valid 0.3289616937710814
LOSS train 0.33613810015810747 valid 0.3290187902621442
LOSS train 0.33613810015810747 valid 0.32908344724956823
LOSS train 0.33613810015810747 valid 0.32911471835637496
LOSS train 0.33613810015810747 valid 0.32915105197477984
LOSS train 0.33613810015810747 valid 0.3292402993348311
LOSS train 0.33613810015810747 valid 0.3292081282042817
LOSS train 0.33613810015810747 valid 0.3292593498493118
LOSS train 0.33613810015810747 valid 0.3292029991745949
LOSS train 0.33613810015810747 valid 0.329227427509536
LOSS train 0.33613810015810747 valid 0.3292604322662417
LOSS train 0.33613810015810747 valid 0.32930838550278063
LOSS train 0.33613810015810747 valid 0.32921707551730306
LOSS train 0.33613810015810747 valid 0.32917148881271235
LOSS train 0.33613810015810747 valid 0.32910601668108524
LOSS train 0.33613810015810747 valid 0.32908215895537835
LOSS train 0.33613810015810747 valid 0.3290290269371751
LOSS train 0.33613810015810747 valid 0.32910594035506635
LOSS train 0.33613810015810747 valid 0.32921407799566943
LOSS train 0.33613810015810747 valid 0.3292148583763282
LOSS train 0.33613810015810747 valid 0.3292258669359562
LOSS train 0.33613810015810747 valid 0.3292434140325735
LOSS train 0.33613810015810747 valid 0.3294287072435306
LOSS train 0.33613810015810747 valid 0.32946960159710476
LOSS train 0.33613810015810747 valid 0.329387114300758
LOSS train 0.33613810015810747 valid 0.32941479553183545
LOSS train 0.33613810015810747 valid 0.3295634350499267
LOSS train 0.33613810015810747 valid 0.3296233177371907
LOSS train 0.33613810015810747 valid 0.3295317587442696
LOSS train 0.33613810015810747 valid 0.32961403905788317
LOSS train 0.33613810015810747 valid 0.3295705588708013
LOSS train 0.33613810015810747 valid 0.32945788700145096
LOSS train 0.33613810015810747 valid 0.3294213361762188
LOSS train 0.33613810015810747 valid 0.32946414534862223
LOSS train 0.33613810015810747 valid 0.3295664716900492
LOSS train 0.33613810015810747 valid 0.32953926228237446
LOSS train 0.33613810015810747 valid 0.32952886574515483
LOSS train 0.33613810015810747 valid 0.3296486746214081
LOSS train 0.33613810015810747 valid 0.32954049796769114
LOSS train 0.33613810015810747 valid 0.3293931568857406
LOSS train 0.33613810015810747 valid 0.32939962701625136
LOSS train 0.33613810015810747 valid 0.3293912829579534
LOSS train 0.33613810015810747 valid 0.32960800576709703
LOSS train 0.33613810015810747 valid 0.32959630934160145
LOSS train 0.33613810015810747 valid 0.3295362270658925
LOSS train 0.33613810015810747 valid 0.3295075842994611
LOSS train 0.33613810015810747 valid 0.3294544701392834
LOSS train 0.33613810015810747 valid 0.329467438139395
LOSS train 0.33613810015810747 valid 0.32945316872176
LOSS train 0.33613810015810747 valid 0.3293496249707913
LOSS train 0.33613810015810747 valid 0.3293212484895137
LOSS train 0.33613810015810747 valid 0.3292975163320758
LOSS train 0.33613810015810747 valid 0.3293945684509222
LOSS train 0.33613810015810747 valid 0.32944493907085365
LOSS train 0.33613810015810747 valid 0.3293758207835214
LOSS train 0.33613810015810747 valid 0.3292409138652021
LOSS train 0.33613810015810747 valid 0.3291899202198818
LOSS train 0.33613810015810747 valid 0.3292993287473148
LOSS train 0.33613810015810747 valid 0.3292102702174868
LOSS train 0.33613810015810747 valid 0.32914672490538355
LOSS train 0.33613810015810747 valid 0.32909239091995085
LOSS train 0.33613810015810747 valid 0.32921535156603915
LOSS train 0.33613810015810747 valid 0.32934445168002174
LOSS train 0.33613810015810747 valid 0.32937772274017335
LOSS train 0.33613810015810747 valid 0.3294593175308088
LOSS train 0.33613810015810747 valid 0.3294064857545687
LOSS train 0.33613810015810747 valid 0.3293869169897207
LOSS train 0.33613810015810747 valid 0.32946395957038266
LOSS train 0.33613810015810747 valid 0.3294804441432158
LOSS train 0.33613810015810747 valid 0.3294668558396791
LOSS train 0.33613810015810747 valid 0.3295565669872484
LOSS train 0.33613810015810747 valid 0.3293813640256887
LOSS train 0.33613810015810747 valid 0.32941921538376545
LOSS train 0.33613810015810747 valid 0.3294896574869548
LOSS train 0.33613810015810747 valid 0.3294482407022695
LOSS train 0.33613810015810747 valid 0.3293248543297562
LOSS train 0.33613810015810747 valid 0.3292571603442016
LOSS train 0.33613810015810747 valid 0.32938896616299945
EPOCH 2:
  batch 1 loss: 0.2608778476715088
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.28323251008987427
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.29207061727841693
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.2861400172114372
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.2856933295726776
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.28548115491867065
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.2814917138644627
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.2813148684799671
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.2791756722662184
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.2749961629509926
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.27404377270828595
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.2757353000342846
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.2728163038308804
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.2736074190054621
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.2714229981104533
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.27203490026295185
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.2732587912503411
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.2745165344741609
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.27564242795894023
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.2749119609594345
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.2744719286759694
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.27454597028819
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.2763161166854527
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.2779447336991628
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.27846742391586304
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.2794305028823706
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.2791712228898649
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.278812190251691
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.27760903896956607
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.27794356147448224
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.27875332101698846
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.27972008753567934
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.2777454988522963
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.27892939045148735
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.2807498701981136
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.2810780819919374
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.2812349474107897
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.28069352005657394
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.28066801184262985
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.28010725826025007
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.2802160142398462
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.2804198144447236
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.28137842097947763
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.28108991411599243
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.2817900856335958
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.28150189635546313
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.28113965912068145
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.2813537133236726
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.2812360160204829
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.28142432391643524
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.2814484238624573
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.28098228573799133
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.2811729907989502
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.2807939024987044
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.2809086642482064
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.28052776040775435
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.2808944828677596
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.2808676629230894
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.281061944314989
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.2811892156799634
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.28100339701918303
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.28106641865545706
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.28103544456618174
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.2809125897474587
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.28115771871346695
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.2812266896168391
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.2812295943053801
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.2813505455851555
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.28154096698415454
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.282097242133958
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.28205244599933355
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.2817722203003036
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.2820758823662588
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.28213754215755976
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.28245888312657674
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.2825342706943813
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.28226812009687546
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.28206444474366993
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.28197061713737775
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.2820480354130268
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.2821112764470371
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.28210951060783573
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.2821291922086693
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.28195920018922715
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.2819183353115531
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.2815616640240647
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.2818858952357851
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.2818156433376399
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.28123841828174806
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.28136501610279085
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.2814856119208283
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.2812415082817492
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.28137724373930245
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.28208753720242924
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.2823386506030434
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.2819561117018263
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.2815780329335596
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.28185167148405194
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.28180906477600637
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.2820169559121132
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.28177114170376616
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.2815086879566604
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.2817167469020029
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.28178170867837393
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.28147688039711544
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.28156854704303563
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.28174009431745406
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.28179520630726107
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.2818447295405449
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.2818196568976749
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.28155482983266983
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.2813483847837363
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.28093106701838233
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.28095375824915736
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.2806710501079974
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.2806022697738532
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.2806458955901301
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.2803844954755347
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.28023470562546193
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.28012949811915555
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.2800639243411624
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.2799963463769584
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.28043730726571586
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.28031293867576507
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.28040297520160673
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.28052984213545207
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.2805009717781713
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.28065111523028463
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.2807227496721948
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.2807606388743107
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.28050776747346834
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.2804797111135541
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.28066365342391164
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.2806203994733184
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.2806652075714535
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.2807675337090212
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.2809940509117433
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.2811339339916257
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.2814477343353436
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.281562368784632
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.28161626356713315
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.28130894794430533
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.28147888954702793
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.2818996606187688
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.28182537247394696
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.2818479833945836
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.28164214037713553
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.2814220109501401
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.28155744395800086
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.2816285238663356
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.2816437678226572
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.28147060502516597
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.28126555466963576
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.2811593504308106
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.28129164403484713
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.28121984807344585
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.281128110020024
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.281143825454048
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.2816068486972425
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.2814615985378623
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.28141609574697035
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.28128410158333955
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.2813365197986182
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.28140939572235435
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.28126312277533794
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.28113699695432043
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.28104808462594083
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.28110562247179804
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.2811901812722697
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.2812834958819782
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.2812934559688233
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.2811066104575645
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.280950597428173
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.2809256027484762
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.2809394994803837
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.28074180631136353
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.2806443859123241
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.28050862194112175
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.2805656372002383
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.2804754870633284
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.28050298675977064
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.28056083889780464
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.2803334833982864
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.2803270939089682
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.28030038564591797
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.28020393039270114
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.28013800817696166
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.2801487353095349
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.2800222738552346
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.27987433271972756
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.27997734479567143
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.2798570663823436
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.2800575679293568
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.27990702585768457
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.27992657644626423
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.2800947918119479
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.2801203360866169
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.28030755927767415
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.28029853068104943
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.28032470636069773
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.2801511060094359
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.2799505229191025
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.2797773777704521
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.2796378967662652
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.27960887078831836
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.2794929630692723
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.27940514302196134
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.27910919365688014
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.27895134238249947
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.2790441144789968
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.27908883395635686
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.27927563402731465
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.2794244935115178
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.27919382344339494
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.2793072136335595
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.27918059806580897
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.27912727204336
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.2791603377105993
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.2793518360619131
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.27955616834488783
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.27951977573908293
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.27948956997007934
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.27942436652867786
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.27945859424237696
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.27935989472601147
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.2793752407341932
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.2792370345624008
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.2790721418956916
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.27906496107057716
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.27912624653266827
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.27902453099236346
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.27892842831025866
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.27897558390070953
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.2790271295314161
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.2789554346749123
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.27904468313869785
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.27912247551895897
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.2791202975546612
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.27919397275567553
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.2791097606842717
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.27912444574951634
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.27902108424704924
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.27903911820900296
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.2790424936741102
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.2791317374730597
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.27930362153101745
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.27926771921908805
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.27920420113350114
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.27923753444688865
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.279292241871357
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.27937712843912055
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.27934732530561707
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.2793866021948841
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.27942205208727694
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.27944539721105616
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.2794300027308054
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.2794008186007288
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.27930612440488134
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.27929583588852386
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.2792777127371385
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.2792815815557465
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.2791705608709168
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.27916076739704654
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.27900223806500435
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.2789536564979913
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.2791160544506589
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.2790604652313704
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.2790540921599118
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.27893164620966715
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.2789703999404554
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.27893608346636445
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.27895348617697463
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.27901997911187754
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.2789738197831342
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.2787971032749523
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.2788650957138642
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.2788111845078451
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.2788169851406015
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.27882253589595946
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.2787778075252261
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.2787448203860653
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.278694545334958
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.278586411276049
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.27843464737836743
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.27846978353826624
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.27847172439723583
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.2784024419373336
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.2783775212139719
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.27835666261948516
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.2784058348372065
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.27833645516859296
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.2784507512944202
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.27850350783343203
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.2785361106602513
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.2785404233104092
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.27851947532916393
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.27857701229527343
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.2784666819560448
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.27837117559535046
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.278299141228199
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.2783827092560423
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.278307681249467
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.2782827328534016
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.27827487836935016
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.2783257100425783
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.2782309269593432
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.2782012819273076
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.27813689807405717
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.27808674957759943
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.27800808279745043
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.27795242458294445
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.27797309872813714
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.27791579101032343
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.27783084010622305
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.2778721749782562
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.2778905895126017
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.27797899665516634
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.27799373049781007
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.2779258221481287
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.27790422085672617
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.2778044433515763
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.2777140118783305
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.27770577629468757
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.27768083749178013
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.2776739061337251
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.27760939377765714
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.27755258148051914
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.2776552368318889
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.2775343310959796
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.27771320121757914
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.27778015094401254
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.2777360995042037
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.27766760042658795
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.2776313589629299
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.27764143023028304
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.2777240573029433
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.2775574936268945
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.2774375276805381
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.27735252511360653
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.2773365769754438
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.2772623140179167
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.2774095403259261
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.2774008267040503
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.2773463590460461
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.27727688442969667
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.27715555552630067
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.2772500000958484
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.277237249045879
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.2771431471949662
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.27699635203395573
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.27700256363109305
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.27705642771483824
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.2770707863501719
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.27705721704468217
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.2771747287310345
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.27710030952029013
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.27703907104934294
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.27705009552187093
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.27700528805468405
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.2768858802401357
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.27679384539001867
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.2766919406902724
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.2766468267959668
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.2765840655499762
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.2765881455107911
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.2765234001184422
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.2764922395876383
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.27655993321019673
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.27659409204472696
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.2765635156953657
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.2764837960348618
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.27639537276599996
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.276423870518444
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.2764430930987399
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.2764539084037145
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.2765139612032378
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.2765703559000865
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.27660148582918936
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.27651933028232456
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.27644950174971633
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.2763676573754609
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.2763748079226279
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.2764501742843548
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.276387703527386
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.27637026712491913
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.27634974789125316
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.2764263677504636
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.2764053438434896
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.2764211657114691
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.27641125382521214
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.2763924915772265
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.2764117437966016
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.2764391348562168
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.2764552050284323
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.2764260366747651
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.2763972052118995
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.276384279920112
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.2764025293103415
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.2762696754215355
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.27633422702550886
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.2762710365868566
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.27627904950386256
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.2762907500154623
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.27633705875366044
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.27627683723414387
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.2762497855084283
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.2763016028398378
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.27630557127150834
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.27627486274702917
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.2762996120423805
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.276273040948413
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.2762975289115628
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.2763249151787515
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.27638771741286566
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.27639984036066445
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.27635868910986644
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.27634621567005735
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.276287119425655
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.27633324806047227
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.27627013648549714
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.2764274201526211
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.2764559133569776
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.2763966168001752
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.2763654199200419
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.2763870933827232
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.27634172487846564
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.27631298318261005
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.2763004336749839
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.2761703417712436
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.2761560031840968
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.276162539861595
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.27614362593050357
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.2760362798223319
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.2761084884748481
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.2760672904636668
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.2761845111231738
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.27625215759402827
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.27622170552407227
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.2760899840835015
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.27608251829038966
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.2761426601145003
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.27620556175169364
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.2762379705502271
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.27633595923045734
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.2763941803005304
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.2764188275743493
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.2763721694882284
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.2763868353462645
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.2764151406314697
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.27632656551069684
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.2762604092481131
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.27633875031518723
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.27629665997512554
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.2762909081604512
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.27627870921905223
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.27634420412543576
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.27634946480230377
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.2763749853297092
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.2763955781078027
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.2764305074253808
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.27641313799420564
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.27645715925987663
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.27639355537829596
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.27645184494683456
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.2764198878439524
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.27644802198261664
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.2764369854005552
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.27648006179011786
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.2765337354275209
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.2766043732457973
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.2765945360981988
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.2765942295792244
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.2765942295792244 valid 0.32506099343299866
LOSS train 0.2765942295792244 valid 0.312161386013031
LOSS train 0.2765942295792244 valid 0.3133308192094167
LOSS train 0.2765942295792244 valid 0.31382546573877335
LOSS train 0.2765942295792244 valid 0.30971344709396365
LOSS train 0.2765942295792244 valid 0.3119123975435893
LOSS train 0.2765942295792244 valid 0.3193655397210802
LOSS train 0.2765942295792244 valid 0.31754472479224205
LOSS train 0.2765942295792244 valid 0.3159816761811574
LOSS train 0.2765942295792244 valid 0.3195681035518646
LOSS train 0.2765942295792244 valid 0.31641952016136865
LOSS train 0.2765942295792244 valid 0.31900110344092053
LOSS train 0.2765942295792244 valid 0.3189768355626326
LOSS train 0.2765942295792244 valid 0.319066475544657
LOSS train 0.2765942295792244 valid 0.3153912882010142
LOSS train 0.2765942295792244 valid 0.315985145047307
LOSS train 0.2765942295792244 valid 0.31780192255973816
LOSS train 0.2765942295792244 valid 0.318760226170222
LOSS train 0.2765942295792244 valid 0.32055274279494034
LOSS train 0.2765942295792244 valid 0.31941108256578443
LOSS train 0.2765942295792244 valid 0.31815322382109507
LOSS train 0.2765942295792244 valid 0.3166905397718603
LOSS train 0.2765942295792244 valid 0.3163219003573708
LOSS train 0.2765942295792244 valid 0.3156423332790534
LOSS train 0.2765942295792244 valid 0.3144879961013794
LOSS train 0.2765942295792244 valid 0.31449696306998914
LOSS train 0.2765942295792244 valid 0.3141306296542839
LOSS train 0.2765942295792244 valid 0.31426443053143366
LOSS train 0.2765942295792244 valid 0.314205442009301
LOSS train 0.2765942295792244 valid 0.3159003188212713
LOSS train 0.2765942295792244 valid 0.31712871597659203
LOSS train 0.2765942295792244 valid 0.3166968235746026
LOSS train 0.2765942295792244 valid 0.31805397254047973
LOSS train 0.2765942295792244 valid 0.3178218226222431
LOSS train 0.2765942295792244 valid 0.3188589538846697
LOSS train 0.2765942295792244 valid 0.31849761224455303
LOSS train 0.2765942295792244 valid 0.3184609372873564
LOSS train 0.2765942295792244 valid 0.31961702124068614
LOSS train 0.2765942295792244 valid 0.31842112159117675
LOSS train 0.2765942295792244 valid 0.3187055416405201
LOSS train 0.2765942295792244 valid 0.31993017036740373
LOSS train 0.2765942295792244 valid 0.31976613118534997
LOSS train 0.2765942295792244 valid 0.3199046808619832
LOSS train 0.2765942295792244 valid 0.32055397738109936
LOSS train 0.2765942295792244 valid 0.31976395580503675
LOSS train 0.2765942295792244 valid 0.32049994753754657
LOSS train 0.2765942295792244 valid 0.3206408416971247
LOSS train 0.2765942295792244 valid 0.32095145372052986
LOSS train 0.2765942295792244 valid 0.32163388327676423
LOSS train 0.2765942295792244 valid 0.32148790776729586
LOSS train 0.2765942295792244 valid 0.322121999427384
LOSS train 0.2765942295792244 valid 0.32198441200531447
LOSS train 0.2765942295792244 valid 0.321819968381018
LOSS train 0.2765942295792244 valid 0.32170286112361485
LOSS train 0.2765942295792244 valid 0.32178250713781875
LOSS train 0.2765942295792244 valid 0.321480322097029
LOSS train 0.2765942295792244 valid 0.32105698815563266
LOSS train 0.2765942295792244 valid 0.320517606262503
LOSS train 0.2765942295792244 valid 0.3205239712181738
LOSS train 0.2765942295792244 valid 0.32025396128495537
LOSS train 0.2765942295792244 valid 0.32016209305309856
LOSS train 0.2765942295792244 valid 0.3205257038916311
LOSS train 0.2765942295792244 valid 0.3205583786207532
LOSS train 0.2765942295792244 valid 0.3214208479039371
LOSS train 0.2765942295792244 valid 0.3218870107944195
LOSS train 0.2765942295792244 valid 0.3221800110556863
LOSS train 0.2765942295792244 valid 0.32174396826260127
LOSS train 0.2765942295792244 valid 0.3220740845974754
LOSS train 0.2765942295792244 valid 0.32146357971688977
LOSS train 0.2765942295792244 valid 0.3214363076857158
LOSS train 0.2765942295792244 valid 0.32111004960369055
LOSS train 0.2765942295792244 valid 0.3213377098242442
LOSS train 0.2765942295792244 valid 0.32141573053516753
LOSS train 0.2765942295792244 valid 0.321321707722303
LOSS train 0.2765942295792244 valid 0.3213088572025299
LOSS train 0.2765942295792244 valid 0.321668626446473
LOSS train 0.2765942295792244 valid 0.3217364100666789
LOSS train 0.2765942295792244 valid 0.32178294238371724
LOSS train 0.2765942295792244 valid 0.3218469963043551
LOSS train 0.2765942295792244 valid 0.3212656233459711
LOSS train 0.2765942295792244 valid 0.3204253371115084
LOSS train 0.2765942295792244 valid 0.3206530086150983
LOSS train 0.2765942295792244 valid 0.32049578176923543
LOSS train 0.2765942295792244 valid 0.32034144053856534
LOSS train 0.2765942295792244 valid 0.32019123119466447
LOSS train 0.2765942295792244 valid 0.3193631335053333
LOSS train 0.2765942295792244 valid 0.3193917702669385
LOSS train 0.2765942295792244 valid 0.31899814206090843
LOSS train 0.2765942295792244 valid 0.3192982998456848
LOSS train 0.2765942295792244 valid 0.31949271493487885
LOSS train 0.2765942295792244 valid 0.3195827659014817
LOSS train 0.2765942295792244 valid 0.31943280962498294
LOSS train 0.2765942295792244 valid 0.3192480530149193
LOSS train 0.2765942295792244 valid 0.3195116722837408
LOSS train 0.2765942295792244 valid 0.31916288639369766
LOSS train 0.2765942295792244 valid 0.31943419265250367
LOSS train 0.2765942295792244 valid 0.31950293312367706
LOSS train 0.2765942295792244 valid 0.31943359149962053
LOSS train 0.2765942295792244 valid 0.31972167196899953
LOSS train 0.2765942295792244 valid 0.32008513420820234
LOSS train 0.2765942295792244 valid 0.3202909346264188
LOSS train 0.2765942295792244 valid 0.3205149980152355
LOSS train 0.2765942295792244 valid 0.3204631345364654
LOSS train 0.2765942295792244 valid 0.32024136730111563
LOSS train 0.2765942295792244 valid 0.32037383715311685
LOSS train 0.2765942295792244 valid 0.32074399995353986
LOSS train 0.2765942295792244 valid 0.32051381914415095
LOSS train 0.2765942295792244 valid 0.32059347436383917
LOSS train 0.2765942295792244 valid 0.3209805677243329
LOSS train 0.2765942295792244 valid 0.321514214981686
LOSS train 0.2765942295792244 valid 0.3211686372219979
LOSS train 0.2765942295792244 valid 0.3209933899343014
LOSS train 0.2765942295792244 valid 0.3208968407284897
LOSS train 0.2765942295792244 valid 0.32083255470844735
LOSS train 0.2765942295792244 valid 0.32103189800096593
LOSS train 0.2765942295792244 valid 0.32110676631845275
LOSS train 0.2765942295792244 valid 0.32124890348850155
LOSS train 0.2765942295792244 valid 0.32110096994092907
LOSS train 0.2765942295792244 valid 0.321165536632057
LOSS train 0.2765942295792244 valid 0.32085786362489066
LOSS train 0.2765942295792244 valid 0.3207615673049422
LOSS train 0.2765942295792244 valid 0.3207044823736441
LOSS train 0.2765942295792244 valid 0.32078929572570614
LOSS train 0.2765942295792244 valid 0.3208908022892091
LOSS train 0.2765942295792244 valid 0.321022784948349
LOSS train 0.2765942295792244 valid 0.32118012058356454
LOSS train 0.2765942295792244 valid 0.3212087644836095
LOSS train 0.2765942295792244 valid 0.3214662817772478
LOSS train 0.2765942295792244 valid 0.32149943108706514
LOSS train 0.2765942295792244 valid 0.3215052767441823
LOSS train 0.2765942295792244 valid 0.3214846692467464
LOSS train 0.2765942295792244 valid 0.3211467015472325
LOSS train 0.2765942295792244 valid 0.321066174067949
LOSS train 0.2765942295792244 valid 0.3211853982352499
LOSS train 0.2765942295792244 valid 0.3212590414064902
LOSS train 0.2765942295792244 valid 0.32119890411987023
LOSS train 0.2765942295792244 valid 0.3210867869592931
LOSS train 0.2765942295792244 valid 0.3209226392749427
LOSS train 0.2765942295792244 valid 0.3208165020822621
LOSS train 0.2765942295792244 valid 0.3208847861204829
LOSS train 0.2765942295792244 valid 0.3211064687434663
LOSS train 0.2765942295792244 valid 0.32132521859357055
LOSS train 0.2765942295792244 valid 0.3213119360950443
LOSS train 0.2765942295792244 valid 0.32132112855712575
LOSS train 0.2765942295792244 valid 0.32098774478353304
LOSS train 0.2765942295792244 valid 0.3213589477212462
LOSS train 0.2765942295792244 valid 0.32117986395245507
LOSS train 0.2765942295792244 valid 0.3219096096786293
LOSS train 0.2765942295792244 valid 0.3221374646129224
LOSS train 0.2765942295792244 valid 0.3220778085788091
LOSS train 0.2765942295792244 valid 0.3219591412717933
LOSS train 0.2765942295792244 valid 0.3218315102552113
LOSS train 0.2765942295792244 valid 0.3219666975775575
LOSS train 0.2765942295792244 valid 0.32213278018034897
LOSS train 0.2765942295792244 valid 0.32209228584843297
LOSS train 0.2765942295792244 valid 0.3225060048011633
LOSS train 0.2765942295792244 valid 0.32239659614623734
LOSS train 0.2765942295792244 valid 0.32251370971715904
LOSS train 0.2765942295792244 valid 0.32257674662572033
LOSS train 0.2765942295792244 valid 0.3223832868039608
LOSS train 0.2765942295792244 valid 0.32240711735642474
LOSS train 0.2765942295792244 valid 0.32218117699211024
LOSS train 0.2765942295792244 valid 0.3220079405907473
LOSS train 0.2765942295792244 valid 0.32180101733382155
LOSS train 0.2765942295792244 valid 0.32186332381132876
LOSS train 0.2765942295792244 valid 0.3220423919608794
LOSS train 0.2765942295792244 valid 0.3221711637731084
LOSS train 0.2765942295792244 valid 0.3220799962679545
LOSS train 0.2765942295792244 valid 0.322136854631661
LOSS train 0.2765942295792244 valid 0.3223460232510286
LOSS train 0.2765942295792244 valid 0.32233467122964693
LOSS train 0.2765942295792244 valid 0.3221648092879805
LOSS train 0.2765942295792244 valid 0.32233109191663
LOSS train 0.2765942295792244 valid 0.3223685247117075
LOSS train 0.2765942295792244 valid 0.3221851180280958
LOSS train 0.2765942295792244 valid 0.32221230441196397
LOSS train 0.2765942295792244 valid 0.32215840567303244
LOSS train 0.2765942295792244 valid 0.3221963344330198
LOSS train 0.2765942295792244 valid 0.3221901956550236
LOSS train 0.2765942295792244 valid 0.32233761234415903
LOSS train 0.2765942295792244 valid 0.3224141655376603
LOSS train 0.2765942295792244 valid 0.32265461277175733
LOSS train 0.2765942295792244 valid 0.32255201326693345
LOSS train 0.2765942295792244 valid 0.3227155310628207
LOSS train 0.2765942295792244 valid 0.322437216784503
LOSS train 0.2765942295792244 valid 0.3225034151666908
LOSS train 0.2765942295792244 valid 0.3222613223096266
LOSS train 0.2765942295792244 valid 0.32232146107770027
LOSS train 0.2765942295792244 valid 0.3223510546974404
LOSS train 0.2765942295792244 valid 0.32242861311686666
LOSS train 0.2765942295792244 valid 0.32236544889305274
LOSS train 0.2765942295792244 valid 0.32242086964348954
LOSS train 0.2765942295792244 valid 0.3224897599282042
LOSS train 0.2765942295792244 valid 0.32232510121827274
LOSS train 0.2765942295792244 valid 0.3221751608909705
LOSS train 0.2765942295792244 valid 0.32226985084767246
LOSS train 0.2765942295792244 valid 0.3223262622876821
LOSS train 0.2765942295792244 valid 0.32229716323240837
LOSS train 0.2765942295792244 valid 0.3222847423661294
LOSS train 0.2765942295792244 valid 0.3221806609630585
LOSS train 0.2765942295792244 valid 0.32208418564416874
LOSS train 0.2765942295792244 valid 0.32204238303227
LOSS train 0.2765942295792244 valid 0.32188741752666794
LOSS train 0.2765942295792244 valid 0.3218895534382147
LOSS train 0.2765942295792244 valid 0.3217343602238632
LOSS train 0.2765942295792244 valid 0.3217825319581819
LOSS train 0.2765942295792244 valid 0.32169566767803137
LOSS train 0.2765942295792244 valid 0.3215450296321741
LOSS train 0.2765942295792244 valid 0.3213543586753772
LOSS train 0.2765942295792244 valid 0.32141702856336324
LOSS train 0.2765942295792244 valid 0.32151953332232075
LOSS train 0.2765942295792244 valid 0.3213202729821205
LOSS train 0.2765942295792244 valid 0.3213330503080932
LOSS train 0.2765942295792244 valid 0.32122724760915633
LOSS train 0.2765942295792244 valid 0.3210555988688802
LOSS train 0.2765942295792244 valid 0.32109443732985743
LOSS train 0.2765942295792244 valid 0.321176033003539
LOSS train 0.2765942295792244 valid 0.3211835860659223
LOSS train 0.2765942295792244 valid 0.32116124537437474
LOSS train 0.2765942295792244 valid 0.3212515780871565
LOSS train 0.2765942295792244 valid 0.3211837551173042
LOSS train 0.2765942295792244 valid 0.32118710888935637
LOSS train 0.2765942295792244 valid 0.32133366361327237
LOSS train 0.2765942295792244 valid 0.32153458281287123
LOSS train 0.2765942295792244 valid 0.3215142756038242
LOSS train 0.2765942295792244 valid 0.32160732994037394
LOSS train 0.2765942295792244 valid 0.32186742579884464
LOSS train 0.2765942295792244 valid 0.3219497175070277
LOSS train 0.2765942295792244 valid 0.322061098558934
LOSS train 0.2765942295792244 valid 0.3221157248901284
LOSS train 0.2765942295792244 valid 0.322035620867948
LOSS train 0.2765942295792244 valid 0.32202519848942757
LOSS train 0.2765942295792244 valid 0.3219579178120445
LOSS train 0.2765942295792244 valid 0.3220101514687905
LOSS train 0.2765942295792244 valid 0.3221336628528351
LOSS train 0.2765942295792244 valid 0.32211121070688054
LOSS train 0.2765942295792244 valid 0.3221572138337646
LOSS train 0.2765942295792244 valid 0.32206090107685376
LOSS train 0.2765942295792244 valid 0.3219491028885462
LOSS train 0.2765942295792244 valid 0.3218588454027971
LOSS train 0.2765942295792244 valid 0.3218554437655136
LOSS train 0.2765942295792244 valid 0.32164192298227107
LOSS train 0.2765942295792244 valid 0.3217064998277421
LOSS train 0.2765942295792244 valid 0.3218281139604381
LOSS train 0.2765942295792244 valid 0.3220021714969557
LOSS train 0.2765942295792244 valid 0.3220061069823862
LOSS train 0.2765942295792244 valid 0.3220897748885367
LOSS train 0.2765942295792244 valid 0.32202696283498117
LOSS train 0.2765942295792244 valid 0.3219605231859598
LOSS train 0.2765942295792244 valid 0.32199910163879397
LOSS train 0.2765942295792244 valid 0.32204742070688197
LOSS train 0.2765942295792244 valid 0.3221448566468935
LOSS train 0.2765942295792244 valid 0.32211776130755426
LOSS train 0.2765942295792244 valid 0.32206725475825665
LOSS train 0.2765942295792244 valid 0.3221546411514282
LOSS train 0.2765942295792244 valid 0.3221353404223919
LOSS train 0.2765942295792244 valid 0.3220635759227471
LOSS train 0.2765942295792244 valid 0.3221846765788027
LOSS train 0.2765942295792244 valid 0.32213170348907527
LOSS train 0.2765942295792244 valid 0.32207856556543935
LOSS train 0.2765942295792244 valid 0.3221137230195305
LOSS train 0.2765942295792244 valid 0.3220619091778311
LOSS train 0.2765942295792244 valid 0.32205955893821137
LOSS train 0.2765942295792244 valid 0.32210675435084285
LOSS train 0.2765942295792244 valid 0.32208889812793373
LOSS train 0.2765942295792244 valid 0.3221031980855124
LOSS train 0.2765942295792244 valid 0.3222654785556293
LOSS train 0.2765942295792244 valid 0.32236930839161376
LOSS train 0.2765942295792244 valid 0.3225810020386508
LOSS train 0.2765942295792244 valid 0.3225675960381826
LOSS train 0.2765942295792244 valid 0.32260993079065836
LOSS train 0.2765942295792244 valid 0.32279073764734406
LOSS train 0.2765942295792244 valid 0.32282149704384716
LOSS train 0.2765942295792244 valid 0.32276855681064354
LOSS train 0.2765942295792244 valid 0.32274213010614566
LOSS train 0.2765942295792244 valid 0.32266381145387457
LOSS train 0.2765942295792244 valid 0.3225387791003561
LOSS train 0.2765942295792244 valid 0.32241696355154187
LOSS train 0.2765942295792244 valid 0.32251118192963274
LOSS train 0.2765942295792244 valid 0.3224530562758446
LOSS train 0.2765942295792244 valid 0.32232547538136247
LOSS train 0.2765942295792244 valid 0.32224192782074
LOSS train 0.2765942295792244 valid 0.32223203188538974
LOSS train 0.2765942295792244 valid 0.3223358434690556
LOSS train 0.2765942295792244 valid 0.32240986123419646
LOSS train 0.2765942295792244 valid 0.32238657908006146
LOSS train 0.2765942295792244 valid 0.32232905773750997
LOSS train 0.2765942295792244 valid 0.32241642889049316
LOSS train 0.2765942295792244 valid 0.3224290788792409
LOSS train 0.2765942295792244 valid 0.32246758732302433
LOSS train 0.2765942295792244 valid 0.3224409227723518
LOSS train 0.2765942295792244 valid 0.322532760974479
LOSS train 0.2765942295792244 valid 0.32258534105971404
LOSS train 0.2765942295792244 valid 0.32264600978011176
LOSS train 0.2765942295792244 valid 0.32268882818141226
LOSS train 0.2765942295792244 valid 0.32272092353653264
LOSS train 0.2765942295792244 valid 0.32281566097680164
LOSS train 0.2765942295792244 valid 0.32278802750894686
LOSS train 0.2765942295792244 valid 0.32285083725699615
LOSS train 0.2765942295792244 valid 0.32279211312532424
LOSS train 0.2765942295792244 valid 0.3228202840418515
LOSS train 0.2765942295792244 valid 0.3228496433488581
LOSS train 0.2765942295792244 valid 0.3228880599780445
LOSS train 0.2765942295792244 valid 0.3227951689378211
LOSS train 0.2765942295792244 valid 0.32273922279232836
LOSS train 0.2765942295792244 valid 0.3226798334542443
LOSS train 0.2765942295792244 valid 0.32265903982743377
LOSS train 0.2765942295792244 valid 0.32260198029991866
LOSS train 0.2765942295792244 valid 0.3226817035173521
LOSS train 0.2765942295792244 valid 0.32278596979956475
LOSS train 0.2765942295792244 valid 0.32277468766813494
LOSS train 0.2765942295792244 valid 0.3227902034727427
LOSS train 0.2765942295792244 valid 0.3228164998868022
LOSS train 0.2765942295792244 valid 0.32300254721550425
LOSS train 0.2765942295792244 valid 0.32304700480567083
LOSS train 0.2765942295792244 valid 0.32296805932552
LOSS train 0.2765942295792244 valid 0.3230044775956813
LOSS train 0.2765942295792244 valid 0.32316239862322055
LOSS train 0.2765942295792244 valid 0.32322559900418346
LOSS train 0.2765942295792244 valid 0.32313260054215787
LOSS train 0.2765942295792244 valid 0.3232268797831372
LOSS train 0.2765942295792244 valid 0.32317661137684534
LOSS train 0.2765942295792244 valid 0.3230709014662279
LOSS train 0.2765942295792244 valid 0.3230380745213709
LOSS train 0.2765942295792244 valid 0.32308045368928173
LOSS train 0.2765942295792244 valid 0.32318417417125467
LOSS train 0.2765942295792244 valid 0.3231587079140024
LOSS train 0.2765942295792244 valid 0.32314189932331805
LOSS train 0.2765942295792244 valid 0.323267635150521
LOSS train 0.2765942295792244 valid 0.32316893005009856
LOSS train 0.2765942295792244 valid 0.323015977428759
LOSS train 0.2765942295792244 valid 0.3230010489203844
LOSS train 0.2765942295792244 valid 0.32299965065162817
LOSS train 0.2765942295792244 valid 0.32322047873885335
LOSS train 0.2765942295792244 valid 0.3232078997056876
LOSS train 0.2765942295792244 valid 0.3231511037087157
LOSS train 0.2765942295792244 valid 0.32312024028789393
LOSS train 0.2765942295792244 valid 0.32307456495493825
LOSS train 0.2765942295792244 valid 0.32308273320704434
LOSS train 0.2765942295792244 valid 0.32306673447875417
LOSS train 0.2765942295792244 valid 0.3229588403904543
LOSS train 0.2765942295792244 valid 0.3229303868019093
LOSS train 0.2765942295792244 valid 0.32291005225987895
LOSS train 0.2765942295792244 valid 0.3230204216616098
LOSS train 0.2765942295792244 valid 0.3230598679487256
LOSS train 0.2765942295792244 valid 0.3229972856237709
LOSS train 0.2765942295792244 valid 0.32285455264344337
LOSS train 0.2765942295792244 valid 0.3227858559667379
LOSS train 0.2765942295792244 valid 0.32290255400034623
LOSS train 0.2765942295792244 valid 0.32281377375125886
LOSS train 0.2765942295792244 valid 0.3227453694554136
LOSS train 0.2765942295792244 valid 0.3226939821615815
LOSS train 0.2765942295792244 valid 0.3228125798465848
LOSS train 0.2765942295792244 valid 0.32294975818887267
LOSS train 0.2765942295792244 valid 0.32298972598263914
LOSS train 0.2765942295792244 valid 0.32307617450028325
LOSS train 0.2765942295792244 valid 0.32301941104963716
LOSS train 0.2765942295792244 valid 0.32299240108308847
LOSS train 0.2765942295792244 valid 0.32307212556969156
LOSS train 0.2765942295792244 valid 0.3230855809317695
LOSS train 0.2765942295792244 valid 0.32307279118210325
LOSS train 0.2765942295792244 valid 0.3231669064220144
LOSS train 0.2765942295792244 valid 0.3229926977275817
LOSS train 0.2765942295792244 valid 0.32303077359120924
LOSS train 0.2765942295792244 valid 0.3231012283122703
LOSS train 0.2765942295792244 valid 0.32305116845610365
LOSS train 0.2765942295792244 valid 0.32292415782281425
LOSS train 0.2765942295792244 valid 0.3228580073008071
LOSS train 0.2765942295792244 valid 0.3229839788058263
bichrom_seq(
  (conv1d): Conv1d(4, 256, kernel_size=(24,), stride=(1,))
  (relu): ReLU()
  (batchNorm1d): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (maxPool1d): MaxPool1d(kernel_size=15, stride=15, padding=0, dilation=1, ceil_mode=True)
  (lstm): LSTM(256, 32, batch_first=True)
  (tanh): Tanh()
  (model_dense_repeat): Sequential(
    (0): Linear(in_features=32, out_features=512, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.5, inplace=False)
    (3): Linear(in_features=512, out_features=512, bias=True)
    (4): ReLU()
    (5): Dropout(p=0.5, inplace=False)
    (6): Linear(in_features=512, out_features=512, bias=True)
    (7): ReLU()
    (8): Dropout(p=0.5, inplace=False)
  )
  (linear): Linear(in_features=512, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)
bimodal_network(
  (base_model): bichrom_seq(
    (conv1d): Conv1d(4, 256, kernel_size=(24,), stride=(1,))
    (relu): ReLU()
    (batchNorm1d): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (maxPool1d): MaxPool1d(kernel_size=15, stride=15, padding=0, dilation=1, ceil_mode=True)
    (lstm): LSTM(256, 32, batch_first=True)
    (tanh): Tanh()
    (model_dense_repeat): Sequential(
      (0): Linear(in_features=32, out_features=512, bias=True)
      (1): ReLU()
      (2): Dropout(p=0.5, inplace=False)
      (3): Linear(in_features=512, out_features=512, bias=True)
      (4): ReLU()
      (5): Dropout(p=0.5, inplace=False)
      (6): Linear(in_features=512, out_features=512, bias=True)
      (7): ReLU()
      (8): Dropout(p=0.5, inplace=False)
    )
    (linear): Linear(in_features=512, out_features=1, bias=True)
    (sigmoid): Sigmoid()
  )
  (linear): Linear(in_features=512, out_features=1, bias=True)
  (tanh): Tanh()
  (model): bichrom_chrom(
    (_reshape): _reshape()
    (conv1d): Conv1d(12, 15, kernel_size=(1,), stride=(1,), padding=valid)
    (relu): ReLU()
    (lstm): LSTM(15, 5, batch_first=True)
    (relu2): ReLU()
    (linear): Linear(in_features=5, out_features=1, bias=True)
    (tanh): Tanh()
  )
  (linear2): Linear(in_features=2, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)
0.40863787375415284
0.40863787375415284
