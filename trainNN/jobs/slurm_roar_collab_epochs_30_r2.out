Selected network (train.py) = bimodal
bimodal selected
Training seq
DEVICE = cpu
####################
Total Parameters = 605185
Total Trainable Parameters = 605185
bichrom_seq(
  (conv1d): Conv1d(4, 256, kernel_size=(24,), stride=(1,))
  (relu): ReLU()
  (batchNorm1d): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (maxPool1d): MaxPool1d(kernel_size=15, stride=15, padding=0, dilation=1, ceil_mode=True)
  (lstm): LSTM(256, 32, batch_first=True)
  (tanh): Tanh()
  (model_dense_repeat): Sequential(
    (0): Linear(in_features=32, out_features=512, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.5, inplace=False)
    (3): Linear(in_features=512, out_features=512, bias=True)
    (4): ReLU()
    (5): Dropout(p=0.5, inplace=False)
    (6): Linear(in_features=512, out_features=512, bias=True)
    (7): ReLU()
    (8): Dropout(p=0.5, inplace=False)
  )
  (linear): Linear(in_features=512, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)
####################
Epochs = 30
EPOCH 1:
  batch 1 loss: 0.6918201446533203
  batch 2 loss: 0.6927967071533203
  batch 3 loss: 0.6929927468299866
  batch 4 loss: 0.6924695074558258
  batch 5 loss: 0.6938172101974487
  batch 6 loss: 0.6943941116333008
  batch 7 loss: 0.6943299004009792
  batch 8 loss: 0.6943481117486954
  batch 9 loss: 0.6942797700564066
  batch 10 loss: 0.6945861518383026
  batch 11 loss: 0.6944122802127491
  batch 12 loss: 0.6940663605928421
  batch 13 loss: 0.6936699335391705
  batch 14 loss: 0.6933661103248596
  batch 15 loss: 0.6928683241208394
  batch 16 loss: 0.6923877038061619
  batch 17 loss: 0.691938042640686
  batch 18 loss: 0.6917331318060557
  batch 19 loss: 0.6915492007606908
  batch 20 loss: 0.6906113058328629
  batch 21 loss: 0.6896547675132751
  batch 22 loss: 0.6881748194044287
  batch 23 loss: 0.6876491463702658
  batch 24 loss: 0.6872034048040708
  batch 25 loss: 0.6869799995422363
  batch 26 loss: 0.6865545557095454
  batch 27 loss: 0.6856424521516871
  batch 28 loss: 0.6843822704894202
  batch 29 loss: 0.6835644142381077
  batch 30 loss: 0.6831971883773804
  batch 31 loss: 0.6821316884409997
  batch 32 loss: 0.6814007610082626
  batch 33 loss: 0.6814948573256984
  batch 34 loss: 0.6806574271005743
  batch 35 loss: 0.6800324252673557
  batch 36 loss: 0.6800385349326663
  batch 37 loss: 0.6799675210102184
  batch 38 loss: 0.6797561112203097
  batch 39 loss: 0.6793049405782651
  batch 40 loss: 0.6785848721861839
  batch 41 loss: 0.677807678536671
  batch 42 loss: 0.6772778644448235
  batch 43 loss: 0.6765247347743012
  batch 44 loss: 0.6762969859621741
  batch 45 loss: 0.6760721166928609
  batch 46 loss: 0.675229804671329
  batch 47 loss: 0.6748299129465793
  batch 48 loss: 0.6742926190296809
  batch 49 loss: 0.6733953539206057
  batch 50 loss: 0.6725980985164642
  batch 51 loss: 0.6718724206382153
  batch 52 loss: 0.6712863078484168
  batch 53 loss: 0.670843095149634
  batch 54 loss: 0.6695352417451365
  batch 55 loss: 0.6687349211085927
  batch 56 loss: 0.6684322474258286
  batch 57 loss: 0.6676105334047686
  batch 58 loss: 0.6668568119920534
  batch 59 loss: 0.666990344807253
  batch 60 loss: 0.6664439558982849
  batch 61 loss: 0.6655637191944435
  batch 62 loss: 0.6645928332882542
  batch 63 loss: 0.6634690619650341
  batch 64 loss: 0.6630128715187311
  batch 65 loss: 0.6622210686023419
  batch 66 loss: 0.6611954190514304
  batch 67 loss: 0.6602185928999488
  batch 68 loss: 0.6597540501286002
  batch 69 loss: 0.658475487128548
  batch 70 loss: 0.6575504405157907
  batch 71 loss: 0.6569551237871949
  batch 72 loss: 0.6562640319267908
  batch 73 loss: 0.6553729045880984
  batch 74 loss: 0.6547918005569561
  batch 75 loss: 0.6536459636688232
  batch 76 loss: 0.6527651697397232
  batch 77 loss: 0.6515664783391085
  batch 78 loss: 0.6505364745091169
  batch 79 loss: 0.6495786980737613
  batch 80 loss: 0.6483835227787494
  batch 81 loss: 0.6476466008174566
  batch 82 loss: 0.6466515071508361
  batch 83 loss: 0.6453445582504732
  batch 84 loss: 0.6438673642419633
  batch 85 loss: 0.6428812573937809
  batch 86 loss: 0.6417140974554905
  batch 87 loss: 0.6407368196838203
  batch 88 loss: 0.6396571770310402
  batch 89 loss: 0.6385141999534006
  batch 90 loss: 0.6378653526306153
  batch 91 loss: 0.6361088769121485
  batch 92 loss: 0.6352841034531593
  batch 93 loss: 0.6343119705236087
  batch 94 loss: 0.6330143894920958
  batch 95 loss: 0.6320599276768534
  batch 96 loss: 0.6308349498237172
  batch 97 loss: 0.6300045635896859
  batch 98 loss: 0.6288629639513639
  batch 99 loss: 0.6278568178114264
  batch 100 loss: 0.626847417652607
  batch 101 loss: 0.6255631824531177
  batch 102 loss: 0.6250096875078538
  batch 103 loss: 0.6233753538826137
  batch 104 loss: 0.6228708538871545
  batch 105 loss: 0.6213918495745886
  batch 106 loss: 0.6207421149285335
  batch 107 loss: 0.6194767328066246
  batch 108 loss: 0.6184829517647072
  batch 109 loss: 0.617280353099928
  batch 110 loss: 0.6162579568949613
  batch 111 loss: 0.6153088362367304
  batch 112 loss: 0.6140098130064351
  batch 113 loss: 0.6129099249312308
  batch 114 loss: 0.6116803241403479
  batch 115 loss: 0.610056880246038
  batch 116 loss: 0.6092872645320564
  batch 117 loss: 0.6084071698351803
  batch 118 loss: 0.6075617564936816
  batch 119 loss: 0.6066681602421928
  batch 120 loss: 0.605473784605662
  batch 121 loss: 0.6044960504721019
  batch 122 loss: 0.6032870265304066
  batch 123 loss: 0.6019903559510301
  batch 124 loss: 0.6010425338341344
  batch 125 loss: 0.5997891426086426
  batch 126 loss: 0.5987901708909443
  batch 127 loss: 0.5980060156874769
  batch 128 loss: 0.5971520063467324
  batch 129 loss: 0.5963507412477981
  batch 130 loss: 0.5952873470691534
  batch 131 loss: 0.5943882401662929
  batch 132 loss: 0.5933872185873262
  batch 133 loss: 0.5922158006438636
  batch 134 loss: 0.590950037116435
  batch 135 loss: 0.5902632905377282
  batch 136 loss: 0.5894714111790937
  batch 137 loss: 0.5882468956665401
  batch 138 loss: 0.5872822984837104
  batch 139 loss: 0.5864705447670367
  batch 140 loss: 0.5858304264289992
  batch 141 loss: 0.5849336193385699
  batch 142 loss: 0.5840796891232611
  batch 143 loss: 0.5831662080921494
  batch 144 loss: 0.5820648717797465
  batch 145 loss: 0.5809820697225373
  batch 146 loss: 0.5801517787861498
  batch 147 loss: 0.579190849446926
  batch 148 loss: 0.5782681902920878
  batch 149 loss: 0.5771154929327484
  batch 150 loss: 0.5761654627323151
  batch 151 loss: 0.5751210865595483
  batch 152 loss: 0.5738556963440619
  batch 153 loss: 0.5732574381080329
  batch 154 loss: 0.5722926046167102
  batch 155 loss: 0.5712522052949475
  batch 156 loss: 0.570316344499588
  batch 157 loss: 0.5695277184817442
  batch 158 loss: 0.5690318321502661
  batch 159 loss: 0.568313330786783
  batch 160 loss: 0.5671508071944118
  batch 161 loss: 0.5659924715942477
  batch 162 loss: 0.5651661371007378
  batch 163 loss: 0.5643313870839546
  batch 164 loss: 0.5636910340044556
  batch 165 loss: 0.5629940437548089
  batch 166 loss: 0.5623490393879902
  batch 167 loss: 0.5614972430431914
  batch 168 loss: 0.5605863724790868
  batch 169 loss: 0.5599136163849803
  batch 170 loss: 0.5592754311421338
  batch 171 loss: 0.5586144423972793
  batch 172 loss: 0.5579387455485588
  batch 173 loss: 0.5570036119463816
  batch 174 loss: 0.5562834092255297
  batch 175 loss: 0.5555942228862217
  batch 176 loss: 0.5549702989784154
  batch 177 loss: 0.5541403973843418
  batch 178 loss: 0.5531216292233949
  batch 179 loss: 0.5524409255168957
  batch 180 loss: 0.5515633631083701
  batch 181 loss: 0.550694947249323
  batch 182 loss: 0.5499535856338648
  batch 183 loss: 0.5491693953021628
  batch 184 loss: 0.5482697608354299
  batch 185 loss: 0.5473343749304076
  batch 186 loss: 0.5465193815449233
  batch 187 loss: 0.5460817225795379
  batch 188 loss: 0.5452618852574774
  batch 189 loss: 0.5446592626117525
  batch 190 loss: 0.5439308119447608
  batch 191 loss: 0.5432343642124955
  batch 192 loss: 0.5423466265201569
  batch 193 loss: 0.5416164169657416
  batch 194 loss: 0.5408283848430693
  batch 195 loss: 0.5403005549540887
  batch 196 loss: 0.5396597849167123
  batch 197 loss: 0.5390105887415445
  batch 198 loss: 0.5383243207076583
  batch 199 loss: 0.5378481518084081
  batch 200 loss: 0.5372820442914963
  batch 201 loss: 0.5363811310844042
  batch 202 loss: 0.535617654718975
  batch 203 loss: 0.5348904626122837
  batch 204 loss: 0.5343167780368936
  batch 205 loss: 0.5335736437541683
  batch 206 loss: 0.5328947269511455
  batch 207 loss: 0.5321706356345743
  batch 208 loss: 0.5313844639234818
  batch 209 loss: 0.5309182211542814
  batch 210 loss: 0.5303014055604026
  batch 211 loss: 0.5296615952564077
  batch 212 loss: 0.5289961362503609
  batch 213 loss: 0.528284190266345
  batch 214 loss: 0.527625230846004
  batch 215 loss: 0.5269967921944552
  batch 216 loss: 0.5265051853601579
  batch 217 loss: 0.5258220792640739
  batch 218 loss: 0.5251924134449127
  batch 219 loss: 0.5244491017300245
  batch 220 loss: 0.5239520047198643
  batch 221 loss: 0.5233850127161898
  batch 222 loss: 0.522983985038491
  batch 223 loss: 0.5222634786714887
  batch 224 loss: 0.5218277319467494
  batch 225 loss: 0.5214454318417443
  batch 226 loss: 0.5208645375956477
  batch 227 loss: 0.5202228180112293
  batch 228 loss: 0.5196536689212448
  batch 229 loss: 0.5193977337737271
  batch 230 loss: 0.518781119066736
  batch 231 loss: 0.5183501514521512
  batch 232 loss: 0.5177123695611954
  batch 233 loss: 0.5171327757221434
  batch 234 loss: 0.5166548174670619
  batch 235 loss: 0.5161256014032567
  batch 236 loss: 0.515467522508007
  batch 237 loss: 0.5151603951232846
  batch 238 loss: 0.5146467939895742
  batch 239 loss: 0.5143627129089883
  batch 240 loss: 0.5139459156741698
  batch 241 loss: 0.5134904666560319
  batch 242 loss: 0.5130388016296812
  batch 243 loss: 0.5126402694501995
  batch 244 loss: 0.512160843879473
  batch 245 loss: 0.5117488716329847
  batch 246 loss: 0.511363349188634
  batch 247 loss: 0.5109651970718554
  batch 248 loss: 0.5105406728483015
  batch 249 loss: 0.5100774287459362
  batch 250 loss: 0.5096493630409241
  batch 251 loss: 0.5090546160342684
  batch 252 loss: 0.5085468853986452
  batch 253 loss: 0.5080309372410001
  batch 254 loss: 0.5076062571345352
  batch 255 loss: 0.5069409358735179
  batch 256 loss: 0.5064392500789836
  batch 257 loss: 0.5060662775651954
  batch 258 loss: 0.5054712040248768
  batch 259 loss: 0.5048449997966354
  batch 260 loss: 0.5043789628606576
  batch 261 loss: 0.5040269553432977
  batch 262 loss: 0.5035238195466631
  batch 263 loss: 0.5029792078547605
  batch 264 loss: 0.502674040017706
  batch 265 loss: 0.5022642205346305
  batch 266 loss: 0.501760285823865
  batch 267 loss: 0.5011773690971989
  batch 268 loss: 0.5007107615915697
  batch 269 loss: 0.5001815068899034
  batch 270 loss: 0.49988176646056
  batch 271 loss: 0.49954151370428584
  batch 272 loss: 0.49902736603775444
  batch 273 loss: 0.4986850945722489
  batch 274 loss: 0.49820197009257156
  batch 275 loss: 0.4977736001664942
  batch 276 loss: 0.49734945670850034
  batch 277 loss: 0.4969650047565625
  batch 278 loss: 0.4965276675258609
  batch 279 loss: 0.4961112018768078
  batch 280 loss: 0.4955969360257898
  batch 281 loss: 0.49516931854957363
  batch 282 loss: 0.4946164694872308
  batch 283 loss: 0.49421818822938224
  batch 284 loss: 0.49385661799722996
  batch 285 loss: 0.4932727301329897
  batch 286 loss: 0.4929647748912131
  batch 287 loss: 0.4925148982411893
  batch 288 loss: 0.49217955333491165
  batch 289 loss: 0.49174209841394917
  batch 290 loss: 0.49130981132901946
  batch 291 loss: 0.4909110782072716
  batch 292 loss: 0.4906159844504644
  batch 293 loss: 0.4902772812916557
  batch 294 loss: 0.48980008227890043
  batch 295 loss: 0.48937588263366183
  batch 296 loss: 0.4891128523929699
  batch 297 loss: 0.48877343675905605
  batch 298 loss: 0.4884516677800441
  batch 299 loss: 0.48815606818550006
  batch 300 loss: 0.4877543783187866
  batch 301 loss: 0.48744125619679196
  batch 302 loss: 0.4871950331113196
  batch 303 loss: 0.4868838646427633
  batch 304 loss: 0.486511228312003
  batch 305 loss: 0.4861515653915093
  batch 306 loss: 0.4859162891417547
  batch 307 loss: 0.4855637168845447
  batch 308 loss: 0.4851916633643113
  batch 309 loss: 0.484842799628051
  batch 310 loss: 0.4845664044541697
  batch 311 loss: 0.4841021140288693
  batch 312 loss: 0.4836776738938613
  batch 313 loss: 0.4834111112946519
  batch 314 loss: 0.4831267857247857
  batch 315 loss: 0.48287887592164297
  batch 316 loss: 0.4825979176955887
  batch 317 loss: 0.482412321992477
  batch 318 loss: 0.48198963514289017
  batch 319 loss: 0.48148867291716574
  batch 320 loss: 0.48126373449340465
  batch 321 loss: 0.4808789028371235
  batch 322 loss: 0.48044283280831684
  batch 323 loss: 0.48014427735340487
  batch 324 loss: 0.4797687435775627
  batch 325 loss: 0.47952529109441316
  batch 326 loss: 0.4793716561209205
  batch 327 loss: 0.47907082823073827
  batch 328 loss: 0.4787947331078169
  batch 329 loss: 0.47849948464193603
  batch 330 loss: 0.47822425997618473
  batch 331 loss: 0.47790331512779627
  batch 332 loss: 0.47764901116669894
  batch 333 loss: 0.4773145100554904
  batch 334 loss: 0.4772221210474026
  batch 335 loss: 0.47689216261479395
  batch 336 loss: 0.4766183841441359
  batch 337 loss: 0.476291130399138
  batch 338 loss: 0.47589558609843957
  batch 339 loss: 0.4756328403773913
  batch 340 loss: 0.4752440458711456
  batch 341 loss: 0.47490938029680785
  batch 342 loss: 0.4746683114453366
  batch 343 loss: 0.474484151897208
  batch 344 loss: 0.474317011358433
  batch 345 loss: 0.47408258819925614
  batch 346 loss: 0.47385707695704665
  batch 347 loss: 0.4735702519286263
  batch 348 loss: 0.47333714228936996
  batch 349 loss: 0.4731522418537249
  batch 350 loss: 0.472973369870867
  batch 351 loss: 0.47267752350905
  batch 352 loss: 0.47240890994329343
  batch 353 loss: 0.47211415419835207
  batch 354 loss: 0.4718369000543982
  batch 355 loss: 0.47161479642693427
  batch 356 loss: 0.4713773780994201
  batch 357 loss: 0.4709322046832878
  batch 358 loss: 0.4706017182692469
  batch 359 loss: 0.4703106592956692
  batch 360 loss: 0.4699846205612024
  batch 361 loss: 0.469705665788492
  batch 362 loss: 0.4694132569415793
  batch 363 loss: 0.46925688709437685
  batch 364 loss: 0.46896706767134616
  batch 365 loss: 0.468752142909455
  batch 366 loss: 0.46850628727446486
  batch 367 loss: 0.4682864521602194
  batch 368 loss: 0.46802323149598163
  batch 369 loss: 0.46784739604164266
  batch 370 loss: 0.46774462759494784
  batch 371 loss: 0.46750891224714625
  batch 372 loss: 0.46728277751194536
  batch 373 loss: 0.46704917746638486
  batch 374 loss: 0.4667788919280557
  batch 375 loss: 0.46660200039545696
  batch 376 loss: 0.46643905262363716
  batch 377 loss: 0.46617380243081313
  batch 378 loss: 0.4658649328051421
  batch 379 loss: 0.4656319102385428
  batch 380 loss: 0.4654086625889728
  batch 381 loss: 0.46513221520451425
  batch 382 loss: 0.4648935232330991
  batch 383 loss: 0.4645509031204891
  batch 384 loss: 0.46435100918946165
  batch 385 loss: 0.46410181421738167
  batch 386 loss: 0.4638843691565212
  batch 387 loss: 0.4636443700131212
  batch 388 loss: 0.4633469697587269
  batch 389 loss: 0.46317586026951707
  batch 390 loss: 0.4629574470795118
  batch 391 loss: 0.4625799795398322
  batch 392 loss: 0.46243362843382113
  batch 393 loss: 0.4622564647943919
  batch 394 loss: 0.4620204147348549
  batch 395 loss: 0.4617206264145767
  batch 396 loss: 0.46152331800472857
  batch 397 loss: 0.4613281985374182
  batch 398 loss: 0.4610742580651039
  batch 399 loss: 0.46081071531862244
  batch 400 loss: 0.4607621308416128
  batch 401 loss: 0.4605191228841606
  batch 402 loss: 0.46026387625369264
  batch 403 loss: 0.4599847983633614
  batch 404 loss: 0.4596786897371311
  batch 405 loss: 0.4594959256089764
  batch 406 loss: 0.45925099905488526
  batch 407 loss: 0.45911017946294835
  batch 408 loss: 0.4588557897391273
  batch 409 loss: 0.4585883065803127
  batch 410 loss: 0.4582329840921774
  batch 411 loss: 0.45804118160203716
  batch 412 loss: 0.45801785588264465
  batch 413 loss: 0.45774496857248265
  batch 414 loss: 0.4575203387921559
  batch 415 loss: 0.4572740722851581
  batch 416 loss: 0.4571280585458645
  batch 417 loss: 0.4568578371469923
  batch 418 loss: 0.4566162761176032
  batch 419 loss: 0.4564025397801456
  batch 420 loss: 0.456217862027032
  batch 421 loss: 0.4560679919362918
  batch 422 loss: 0.45583230136977554
  batch 423 loss: 0.45563709658370233
  batch 424 loss: 0.45549780425598035
  batch 425 loss: 0.45520560397821314
  batch 426 loss: 0.45489562201388006
  batch 427 loss: 0.45467720992112887
  batch 428 loss: 0.45445608424249095
  batch 429 loss: 0.454241372141249
  batch 430 loss: 0.45408963124419366
  batch 431 loss: 0.453904315448969
  batch 432 loss: 0.45361672762643407
  batch 433 loss: 0.4533184855556929
  batch 434 loss: 0.4530368972621206
  batch 435 loss: 0.4529314151440544
  batch 436 loss: 0.4527359600050734
  batch 437 loss: 0.4524649855610435
  batch 438 loss: 0.45229583175759336
  batch 439 loss: 0.45207353890077945
  batch 440 loss: 0.45202022384513507
  batch 441 loss: 0.45189872194850256
  batch 442 loss: 0.45169379686877736
  batch 443 loss: 0.4514793284052382
  batch 444 loss: 0.4513226038029602
  batch 445 loss: 0.4511687653118305
  batch 446 loss: 0.4510451222481749
  batch 447 loss: 0.4509228011905747
  batch 448 loss: 0.450791213195771
  batch 449 loss: 0.4505898974387842
  batch 450 loss: 0.450389214820332
  batch 451 loss: 0.4502013397719009
  batch 452 loss: 0.4499373749821587
  batch 453 loss: 0.44975673251067827
  batch 454 loss: 0.449545200635158
  batch 455 loss: 0.4493474324325939
  batch 456 loss: 0.4491634986510402
  batch 457 loss: 0.44898271045486565
  batch 458 loss: 0.4487410281422877
  batch 459 loss: 0.4485285480817159
  batch 460 loss: 0.4483852146760277
  batch 461 loss: 0.4482384035747636
  batch 462 loss: 0.4481093804170559
  batch 463 loss: 0.44800265973381553
  batch 464 loss: 0.44774411631555394
  batch 465 loss: 0.4475544839776972
  batch 466 loss: 0.44736310021048453
  batch 467 loss: 0.4472713093941421
  batch 468 loss: 0.44705331911388624
  batch 469 loss: 0.44680469853283244
  batch 470 loss: 0.44667983987229937
  batch 471 loss: 0.4465004963733082
  batch 472 loss: 0.4463068987479654
LOSS train 0.4463068987479654 valid 0.24915796518325806
LOSS train 0.4463068987479654 valid 0.24507194757461548
LOSS train 0.4463068987479654 valid 0.24767643213272095
LOSS train 0.4463068987479654 valid 0.23807857930660248
LOSS train 0.4463068987479654 valid 0.2406343102455139
LOSS train 0.4463068987479654 valid 0.24705436825752258
LOSS train 0.4463068987479654 valid 0.24147642936025346
LOSS train 0.4463068987479654 valid 0.24020655266940594
LOSS train 0.4463068987479654 valid 0.23985908097691006
LOSS train 0.4463068987479654 valid 0.2386692926287651
LOSS train 0.4463068987479654 valid 0.2365499585866928
LOSS train 0.4463068987479654 valid 0.23817220951120058
LOSS train 0.4463068987479654 valid 0.23815366052664244
LOSS train 0.4463068987479654 valid 0.236556171306542
LOSS train 0.4463068987479654 valid 0.23538809617360432
LOSS train 0.4463068987479654 valid 0.23794145695865154
LOSS train 0.4463068987479654 valid 0.23743046557202058
LOSS train 0.4463068987479654 valid 0.23817980455027687
LOSS train 0.4463068987479654 valid 0.2402400327356238
LOSS train 0.4463068987479654 valid 0.23967509493231773
LOSS train 0.4463068987479654 valid 0.24022162883054643
LOSS train 0.4463068987479654 valid 0.2397794168103825
LOSS train 0.4463068987479654 valid 0.23901465794314508
LOSS train 0.4463068987479654 valid 0.23965915044148764
LOSS train 0.4463068987479654 valid 0.23871801197528839
LOSS train 0.4463068987479654 valid 0.23786427367192048
LOSS train 0.4463068987479654 valid 0.23802839661086048
LOSS train 0.4463068987479654 valid 0.23772386514714786
LOSS train 0.4463068987479654 valid 0.2372759503537211
LOSS train 0.4463068987479654 valid 0.2364462246497472
LOSS train 0.4463068987479654 valid 0.23627868535057192
LOSS train 0.4463068987479654 valid 0.23664025496691465
LOSS train 0.4463068987479654 valid 0.23624425494309628
LOSS train 0.4463068987479654 valid 0.23553714156150818
LOSS train 0.4463068987479654 valid 0.23608774372509547
LOSS train 0.4463068987479654 valid 0.23653925872511333
LOSS train 0.4463068987479654 valid 0.23631432370559588
LOSS train 0.4463068987479654 valid 0.23584307102780594
LOSS train 0.4463068987479654 valid 0.23606771383530054
LOSS train 0.4463068987479654 valid 0.23640992529690266
LOSS train 0.4463068987479654 valid 0.2359703284938161
LOSS train 0.4463068987479654 valid 0.23676336450236185
LOSS train 0.4463068987479654 valid 0.2369012129168178
LOSS train 0.4463068987479654 valid 0.23643249307166447
LOSS train 0.4463068987479654 valid 0.23556887639893426
LOSS train 0.4463068987479654 valid 0.23500034698973532
LOSS train 0.4463068987479654 valid 0.23454138731702845
LOSS train 0.4463068987479654 valid 0.23542506837596497
LOSS train 0.4463068987479654 valid 0.23477990134638183
LOSS train 0.4463068987479654 valid 0.23542721897363664
LOSS train 0.4463068987479654 valid 0.23513521487806358
LOSS train 0.4463068987479654 valid 0.23509173868940428
LOSS train 0.4463068987479654 valid 0.23578079494665255
LOSS train 0.4463068987479654 valid 0.23583751916885376
LOSS train 0.4463068987479654 valid 0.23572622645984997
LOSS train 0.4463068987479654 valid 0.23580801832888806
LOSS train 0.4463068987479654 valid 0.2353612712600775
LOSS train 0.4463068987479654 valid 0.2363326544391698
LOSS train 0.4463068987479654 valid 0.23646510203005905
LOSS train 0.4463068987479654 valid 0.23621081511179606
LOSS train 0.4463068987479654 valid 0.23615941478580724
LOSS train 0.4463068987479654 valid 0.23575944886092218
LOSS train 0.4463068987479654 valid 0.23563619123564827
LOSS train 0.4463068987479654 valid 0.23531189793720841
LOSS train 0.4463068987479654 valid 0.2343736666899461
LOSS train 0.4463068987479654 valid 0.23438046150135272
LOSS train 0.4463068987479654 valid 0.23464408116554147
LOSS train 0.4463068987479654 valid 0.2340975782888777
LOSS train 0.4463068987479654 valid 0.2347743331954099
LOSS train 0.4463068987479654 valid 0.2349139305097716
LOSS train 0.4463068987479654 valid 0.23521078409443438
LOSS train 0.4463068987479654 valid 0.23541920652820003
LOSS train 0.4463068987479654 valid 0.23612563475354076
LOSS train 0.4463068987479654 valid 0.23610535525792353
LOSS train 0.4463068987479654 valid 0.2357945078611374
LOSS train 0.4463068987479654 valid 0.23570909939314189
LOSS train 0.4463068987479654 valid 0.23566299541430039
LOSS train 0.4463068987479654 valid 0.2356441603639187
LOSS train 0.4463068987479654 valid 0.23579357074031346
LOSS train 0.4463068987479654 valid 0.23552676737308503
LOSS train 0.4463068987479654 valid 0.23551464485533444
LOSS train 0.4463068987479654 valid 0.23553948231586597
LOSS train 0.4463068987479654 valid 0.2355807883193694
LOSS train 0.4463068987479654 valid 0.23553282572400003
LOSS train 0.4463068987479654 valid 0.23566141496686374
LOSS train 0.4463068987479654 valid 0.23550967612238818
LOSS train 0.4463068987479654 valid 0.2352242485202592
LOSS train 0.4463068987479654 valid 0.23499918254938992
LOSS train 0.4463068987479654 valid 0.2353238429916039
LOSS train 0.4463068987479654 valid 0.23542423744996388
LOSS train 0.4463068987479654 valid 0.23530798986717893
LOSS train 0.4463068987479654 valid 0.23527962876402814
LOSS train 0.4463068987479654 valid 0.23513684189447792
LOSS train 0.4463068987479654 valid 0.23525566243110818
LOSS train 0.4463068987479654 valid 0.23537335223273226
LOSS train 0.4463068987479654 valid 0.23556477343663573
LOSS train 0.4463068987479654 valid 0.2358193749312273
LOSS train 0.4463068987479654 valid 0.23616962545380302
LOSS train 0.4463068987479654 valid 0.2363367961211638
LOSS train 0.4463068987479654 valid 0.23638093665242196
LOSS train 0.4463068987479654 valid 0.23637030000733858
LOSS train 0.4463068987479654 valid 0.23681632940675698
LOSS train 0.4463068987479654 valid 0.23655407998747038
LOSS train 0.4463068987479654 valid 0.23631234309421137
LOSS train 0.4463068987479654 valid 0.23651709911369143
LOSS train 0.4463068987479654 valid 0.236472912413894
LOSS train 0.4463068987479654 valid 0.23619615739194033
LOSS train 0.4463068987479654 valid 0.23616849355123662
LOSS train 0.4463068987479654 valid 0.23583663179786926
LOSS train 0.4463068987479654 valid 0.2358056354251775
LOSS train 0.4463068987479654 valid 0.2360506281927899
LOSS train 0.4463068987479654 valid 0.23625560943037271
LOSS train 0.4463068987479654 valid 0.23630408507532777
LOSS train 0.4463068987479654 valid 0.23618153731028238
LOSS train 0.4463068987479654 valid 0.23670567040858062
LOSS train 0.4463068987479654 valid 0.2365528244910569
LOSS train 0.4463068987479654 valid 0.23689462613855672
LOSS train 0.4463068987479654 valid 0.2370131328954535
LOSS train 0.4463068987479654 valid 0.23688817374846516
LOSS train 0.4463068987479654 valid 0.2366955518722534
LOSS train 0.4463068987479654 valid 0.23684665733132482
LOSS train 0.4463068987479654 valid 0.23705625387488818
LOSS train 0.4463068987479654 valid 0.23694056248277184
LOSS train 0.4463068987479654 valid 0.23698509748904936
LOSS train 0.4463068987479654 valid 0.23690432596206665
LOSS train 0.4463068987479654 valid 0.2369815455306144
LOSS train 0.4463068987479654 valid 0.2369919967228972
LOSS train 0.4463068987479654 valid 0.2368853152729571
LOSS train 0.4463068987479654 valid 0.23680615032366079
LOSS train 0.4463068987479654 valid 0.23660274503322748
LOSS train 0.4463068987479654 valid 0.23650029870389983
LOSS train 0.4463068987479654 valid 0.2365418722232183
LOSS train 0.4463068987479654 valid 0.23655163524742412
LOSS train 0.4463068987479654 valid 0.23673141669871203
LOSS train 0.4463068987479654 valid 0.23679631308273033
LOSS train 0.4463068987479654 valid 0.2367998483207296
LOSS train 0.4463068987479654 valid 0.23683644048053853
LOSS train 0.4463068987479654 valid 0.23669723367345505
LOSS train 0.4463068987479654 valid 0.23657419151968237
LOSS train 0.4463068987479654 valid 0.23666380175522395
LOSS train 0.4463068987479654 valid 0.23670727225905613
LOSS train 0.4463068987479654 valid 0.2369748721240272
LOSS train 0.4463068987479654 valid 0.23706352689883092
LOSS train 0.4463068987479654 valid 0.23710213570545116
LOSS train 0.4463068987479654 valid 0.23696373639435603
LOSS train 0.4463068987479654 valid 0.2369084111223482
LOSS train 0.4463068987479654 valid 0.23691436688916212
LOSS train 0.4463068987479654 valid 0.23683693078724113
LOSS train 0.4463068987479654 valid 0.23680673449631506
LOSS train 0.4463068987479654 valid 0.23680723398923875
LOSS train 0.4463068987479654 valid 0.23668002115180162
LOSS train 0.4463068987479654 valid 0.23658498464838454
LOSS train 0.4463068987479654 valid 0.2363570514652464
LOSS train 0.4463068987479654 valid 0.23642686744789024
LOSS train 0.4463068987479654 valid 0.23642149279194494
LOSS train 0.4463068987479654 valid 0.23655734937160444
LOSS train 0.4463068987479654 valid 0.236408768850527
LOSS train 0.4463068987479654 valid 0.23641701995194714
LOSS train 0.4463068987479654 valid 0.23666374591536493
LOSS train 0.4463068987479654 valid 0.2366267959587276
LOSS train 0.4463068987479654 valid 0.2365324532393343
LOSS train 0.4463068987479654 valid 0.23661283946331638
LOSS train 0.4463068987479654 valid 0.23632828946128212
LOSS train 0.4463068987479654 valid 0.23626362141676066
LOSS train 0.4463068987479654 valid 0.2362866927276958
LOSS train 0.4463068987479654 valid 0.23614538317344275
LOSS train 0.4463068987479654 valid 0.23623590920856613
LOSS train 0.4463068987479654 valid 0.23605602066076936
LOSS train 0.4463068987479654 valid 0.23582968406776
LOSS train 0.4463068987479654 valid 0.2358886684564983
LOSS train 0.4463068987479654 valid 0.23570013351259175
LOSS train 0.4463068987479654 valid 0.23572068366893503
LOSS train 0.4463068987479654 valid 0.2357063026442004
LOSS train 0.4463068987479654 valid 0.23590926483444785
LOSS train 0.4463068987479654 valid 0.23585754258292063
LOSS train 0.4463068987479654 valid 0.23578226430849594
LOSS train 0.4463068987479654 valid 0.23581496352529796
LOSS train 0.4463068987479654 valid 0.23590552103653384
LOSS train 0.4463068987479654 valid 0.23602774189837153
LOSS train 0.4463068987479654 valid 0.23602504067950777
LOSS train 0.4463068987479654 valid 0.2359652306822782
LOSS train 0.4463068987479654 valid 0.23590519533052548
LOSS train 0.4463068987479654 valid 0.2359969072003182
LOSS train 0.4463068987479654 valid 0.23598943585934845
LOSS train 0.4463068987479654 valid 0.23605957288999815
LOSS train 0.4463068987479654 valid 0.23617166088473412
LOSS train 0.4463068987479654 valid 0.23605304980341762
LOSS train 0.4463068987479654 valid 0.23609788192713516
LOSS train 0.4463068987479654 valid 0.23590521711521048
LOSS train 0.4463068987479654 valid 0.235988481578074
LOSS train 0.4463068987479654 valid 0.23616729325649002
LOSS train 0.4463068987479654 valid 0.2363120416800181
LOSS train 0.4463068987479654 valid 0.23630529171135758
LOSS train 0.4463068987479654 valid 0.23629778715753064
LOSS train 0.4463068987479654 valid 0.2363140417979314
LOSS train 0.4463068987479654 valid 0.2365477890992651
LOSS train 0.4463068987479654 valid 0.2366001227029084
LOSS train 0.4463068987479654 valid 0.23683106726167177
LOSS train 0.4463068987479654 valid 0.23682636119912018
LOSS train 0.4463068987479654 valid 0.23698662869632245
LOSS train 0.4463068987479654 valid 0.23685601702080436
LOSS train 0.4463068987479654 valid 0.23681980231315783
LOSS train 0.4463068987479654 valid 0.2369937882781616
LOSS train 0.4463068987479654 valid 0.23690728667904348
LOSS train 0.4463068987479654 valid 0.23683848860787182
LOSS train 0.4463068987479654 valid 0.23678558963595084
LOSS train 0.4463068987479654 valid 0.23674636229800716
LOSS train 0.4463068987479654 valid 0.23655078968463036
LOSS train 0.4463068987479654 valid 0.23659782881656902
LOSS train 0.4463068987479654 valid 0.2367167598434857
LOSS train 0.4463068987479654 valid 0.23692688508338838
LOSS train 0.4463068987479654 valid 0.23680314976932867
LOSS train 0.4463068987479654 valid 0.236715490129632
LOSS train 0.4463068987479654 valid 0.23668418302435743
LOSS train 0.4463068987479654 valid 0.2364746209493903
LOSS train 0.4463068987479654 valid 0.23636873163006925
LOSS train 0.4463068987479654 valid 0.23621187973681682
LOSS train 0.4463068987479654 valid 0.23621389611598548
LOSS train 0.4463068987479654 valid 0.23625799877458512
LOSS train 0.4463068987479654 valid 0.23631820956414396
LOSS train 0.4463068987479654 valid 0.2363366117169954
LOSS train 0.4463068987479654 valid 0.23641861538897763
LOSS train 0.4463068987479654 valid 0.23641506539064672
LOSS train 0.4463068987479654 valid 0.23631504370964
LOSS train 0.4463068987479654 valid 0.23620018826590644
LOSS train 0.4463068987479654 valid 0.23617034926351194
LOSS train 0.4463068987479654 valid 0.23621412218929913
LOSS train 0.4463068987479654 valid 0.2361120599926564
LOSS train 0.4463068987479654 valid 0.2359397134145795
LOSS train 0.4463068987479654 valid 0.23604559626268304
LOSS train 0.4463068987479654 valid 0.23601232649702014
LOSS train 0.4463068987479654 valid 0.2361027068490612
LOSS train 0.4463068987479654 valid 0.23631495269327205
LOSS train 0.4463068987479654 valid 0.23626897605056438
LOSS train 0.4463068987479654 valid 0.23635289859264455
LOSS train 0.4463068987479654 valid 0.23625026921094475
LOSS train 0.4463068987479654 valid 0.23628216950450767
LOSS train 0.4463068987479654 valid 0.23635177408196345
LOSS train 0.4463068987479654 valid 0.23630914653195498
LOSS train 0.4463068987479654 valid 0.23597982060164213
LOSS train 0.4463068987479654 valid 0.2358099038546511
LOSS train 0.4463068987479654 valid 0.23580171598875818
LOSS train 0.4463068987479654 valid 0.23582593685805553
LOSS train 0.4463068987479654 valid 0.23582427881535936
LOSS train 0.4463068987479654 valid 0.235962710027792
LOSS train 0.4463068987479654 valid 0.23599474731741882
LOSS train 0.4463068987479654 valid 0.2359869805304145
LOSS train 0.4463068987479654 valid 0.23602011913974438
LOSS train 0.4463068987479654 valid 0.2358804194922428
LOSS train 0.4463068987479654 valid 0.2360188646912575
LOSS train 0.4463068987479654 valid 0.23612539826398826
LOSS train 0.4463068987479654 valid 0.23613979442725105
LOSS train 0.4463068987479654 valid 0.23604168951982565
LOSS train 0.4463068987479654 valid 0.2361111729515819
LOSS train 0.4463068987479654 valid 0.23606172420230567
LOSS train 0.4463068987479654 valid 0.23602598573779687
LOSS train 0.4463068987479654 valid 0.2360552813176515
LOSS train 0.4463068987479654 valid 0.23614137411810632
LOSS train 0.4463068987479654 valid 0.2361176977056334
LOSS train 0.4463068987479654 valid 0.2360218397126748
LOSS train 0.4463068987479654 valid 0.23602187142518288
LOSS train 0.4463068987479654 valid 0.23619843109418417
LOSS train 0.4463068987479654 valid 0.23617262192098815
LOSS train 0.4463068987479654 valid 0.23626724856369424
LOSS train 0.4463068987479654 valid 0.23630824640112102
LOSS train 0.4463068987479654 valid 0.23631061518326738
LOSS train 0.4463068987479654 valid 0.2362444998508089
LOSS train 0.4463068987479654 valid 0.23623743947984568
LOSS train 0.4463068987479654 valid 0.23626556809728474
LOSS train 0.4463068987479654 valid 0.236252040553976
LOSS train 0.4463068987479654 valid 0.23627683580801495
LOSS train 0.4463068987479654 valid 0.23614614046014407
LOSS train 0.4463068987479654 valid 0.23601901815051124
LOSS train 0.4463068987479654 valid 0.23597274780490973
LOSS train 0.4463068987479654 valid 0.2361251486973329
LOSS train 0.4463068987479654 valid 0.23617033050328062
LOSS train 0.4463068987479654 valid 0.23624807149709778
LOSS train 0.4463068987479654 valid 0.23623529676910784
LOSS train 0.4463068987479654 valid 0.2362359734418999
LOSS train 0.4463068987479654 valid 0.23617044772420612
LOSS train 0.4463068987479654 valid 0.23604320986627259
LOSS train 0.4463068987479654 valid 0.23607794153140793
LOSS train 0.4463068987479654 valid 0.23617447881100456
LOSS train 0.4463068987479654 valid 0.23612493848506833
LOSS train 0.4463068987479654 valid 0.2361462219242464
LOSS train 0.4463068987479654 valid 0.23615202510898764
LOSS train 0.4463068987479654 valid 0.2361704679108663
LOSS train 0.4463068987479654 valid 0.23611905860404173
LOSS train 0.4463068987479654 valid 0.2360522697541128
LOSS train 0.4463068987479654 valid 0.23597821207909747
LOSS train 0.4463068987479654 valid 0.23593635723967732
LOSS train 0.4463068987479654 valid 0.23589743076734346
LOSS train 0.4463068987479654 valid 0.2359196790859561
LOSS train 0.4463068987479654 valid 0.23591064033256906
LOSS train 0.4463068987479654 valid 0.23592873094445568
LOSS train 0.4463068987479654 valid 0.23593697148199017
LOSS train 0.4463068987479654 valid 0.23591019975817967
LOSS train 0.4463068987479654 valid 0.23593965347781276
LOSS train 0.4463068987479654 valid 0.2359080233900842
LOSS train 0.4463068987479654 valid 0.23590003927548728
LOSS train 0.4463068987479654 valid 0.23583837081229567
LOSS train 0.4463068987479654 valid 0.23584623659486012
LOSS train 0.4463068987479654 valid 0.23579586432515198
LOSS train 0.4463068987479654 valid 0.23577445426857785
LOSS train 0.4463068987479654 valid 0.2357806462733472
LOSS train 0.4463068987479654 valid 0.23589817867949117
LOSS train 0.4463068987479654 valid 0.23585749720905813
LOSS train 0.4463068987479654 valid 0.23596317388794638
LOSS train 0.4463068987479654 valid 0.23599479735669196
LOSS train 0.4463068987479654 valid 0.23599138322376437
LOSS train 0.4463068987479654 valid 0.23607462007324795
LOSS train 0.4463068987479654 valid 0.2360189422391928
LOSS train 0.4463068987479654 valid 0.23612236053037186
LOSS train 0.4463068987479654 valid 0.23603941803904854
LOSS train 0.4463068987479654 valid 0.23600718047883776
LOSS train 0.4463068987479654 valid 0.23604551910222332
LOSS train 0.4463068987479654 valid 0.23597475773528548
LOSS train 0.4463068987479654 valid 0.23601710112214838
LOSS train 0.4463068987479654 valid 0.2359946396283595
LOSS train 0.4463068987479654 valid 0.23596164621412755
LOSS train 0.4463068987479654 valid 0.2359997956459396
LOSS train 0.4463068987479654 valid 0.23601632565259933
LOSS train 0.4463068987479654 valid 0.23588614710159714
LOSS train 0.4463068987479654 valid 0.23583910191132698
LOSS train 0.4463068987479654 valid 0.23570648055810195
LOSS train 0.4463068987479654 valid 0.2357548806199267
LOSS train 0.4463068987479654 valid 0.23566203234028013
LOSS train 0.4463068987479654 valid 0.2356734570660969
LOSS train 0.4463068987479654 valid 0.23561091657648695
LOSS train 0.4463068987479654 valid 0.2355928953849908
LOSS train 0.4463068987479654 valid 0.23548091151202913
LOSS train 0.4463068987479654 valid 0.2354274387639689
LOSS train 0.4463068987479654 valid 0.23550181157954103
LOSS train 0.4463068987479654 valid 0.23548867227788456
LOSS train 0.4463068987479654 valid 0.23541263012743707
LOSS train 0.4463068987479654 valid 0.23532082601672127
LOSS train 0.4463068987479654 valid 0.23543244432271057
LOSS train 0.4463068987479654 valid 0.2353417350960201
LOSS train 0.4463068987479654 valid 0.23531211635707755
LOSS train 0.4463068987479654 valid 0.23538670741459902
LOSS train 0.4463068987479654 valid 0.23536067313987138
LOSS train 0.4463068987479654 valid 0.23531275155425768
LOSS train 0.4463068987479654 valid 0.23519762525058002
LOSS train 0.4463068987479654 valid 0.23526874262579653
LOSS train 0.4463068987479654 valid 0.23538061319917872
LOSS train 0.4463068987479654 valid 0.23532161047692932
LOSS train 0.4463068987479654 valid 0.235267935723324
LOSS train 0.4463068987479654 valid 0.23531496353533077
LOSS train 0.4463068987479654 valid 0.2352289137748046
LOSS train 0.4463068987479654 valid 0.235236337184906
LOSS train 0.4463068987479654 valid 0.2352762337763425
LOSS train 0.4463068987479654 valid 0.23529429454356432
LOSS train 0.4463068987479654 valid 0.2353832896471699
LOSS train 0.4463068987479654 valid 0.23534235522403554
LOSS train 0.4463068987479654 valid 0.2353031100521625
LOSS train 0.4463068987479654 valid 0.23535367067945137
LOSS train 0.4463068987479654 valid 0.23538752718299044
LOSS train 0.4463068987479654 valid 0.23541211016351285
LOSS train 0.4463068987479654 valid 0.2354103335098968
LOSS train 0.4463068987479654 valid 0.23547395765781404
LOSS train 0.4463068987479654 valid 0.2354400544285444
LOSS train 0.4463068987479654 valid 0.23550034796006114
LOSS train 0.4463068987479654 valid 0.2355535853828609
LOSS train 0.4463068987479654 valid 0.23544062395672222
LOSS train 0.4463068987479654 valid 0.23551335040837118
LOSS train 0.4463068987479654 valid 0.23543760827818855
LOSS train 0.4463068987479654 valid 0.23543860943993042
LOSS train 0.4463068987479654 valid 0.2354044200366606
LOSS train 0.4463068987479654 valid 0.2354241438755175
EPOCH 2:
  batch 1 loss: 0.37330734729766846
  batch 2 loss: 0.4010334759950638
  batch 3 loss: 0.38525015115737915
  batch 4 loss: 0.38197454810142517
  batch 5 loss: 0.3861426115036011
  batch 6 loss: 0.3829413155714671
  batch 7 loss: 0.3818413998399462
  batch 8 loss: 0.37765415385365486
  batch 9 loss: 0.37904730770323014
  batch 10 loss: 0.3751263827085495
  batch 11 loss: 0.3734792552211068
  batch 12 loss: 0.3719220658143361
  batch 13 loss: 0.37059794710232663
  batch 14 loss: 0.36960916434015545
  batch 15 loss: 0.3715050419171651
  batch 16 loss: 0.3717663697898388
  batch 17 loss: 0.37225692412432504
  batch 18 loss: 0.3698728382587433
  batch 19 loss: 0.36963398833023875
  batch 20 loss: 0.3672133684158325
  batch 21 loss: 0.37117284820193336
  batch 22 loss: 0.37161405790935864
  batch 23 loss: 0.3697121933750484
  batch 24 loss: 0.3707392637928327
  batch 25 loss: 0.3717966616153717
  batch 26 loss: 0.37020965149769414
  batch 27 loss: 0.3715919421778785
  batch 28 loss: 0.37276021604027065
  batch 29 loss: 0.37306695970995674
  batch 30 loss: 0.37304641902446745
  batch 31 loss: 0.37289499851965136
  batch 32 loss: 0.37463085167109966
  batch 33 loss: 0.37593334281083307
  batch 34 loss: 0.3751765778836082
  batch 35 loss: 0.37597857883998326
  batch 36 loss: 0.3752829407652219
  batch 37 loss: 0.374149912112468
  batch 38 loss: 0.37515800014922496
  batch 39 loss: 0.37596925940269077
  batch 40 loss: 0.37615696638822554
  batch 41 loss: 0.3756425969484376
  batch 42 loss: 0.3748447100321452
  batch 43 loss: 0.37480188732923464
  batch 44 loss: 0.37463822893121024
  batch 45 loss: 0.37420061892933315
  batch 46 loss: 0.37325042810129083
  batch 47 loss: 0.373869648639192
  batch 48 loss: 0.3735393490642309
  batch 49 loss: 0.37409649515638543
  batch 50 loss: 0.37397237718105314
  batch 51 loss: 0.374359108653723
  batch 52 loss: 0.373880946292327
  batch 53 loss: 0.3733326393478322
  batch 54 loss: 0.37304497206652604
  batch 55 loss: 0.3730113777247342
  batch 56 loss: 0.3731287137738296
  batch 57 loss: 0.37233524939470125
  batch 58 loss: 0.3717599264506636
  batch 59 loss: 0.3720418780536975
  batch 60 loss: 0.3717525159319242
  batch 61 loss: 0.3723959448884745
  batch 62 loss: 0.37241881124434933
  batch 63 loss: 0.371882662886665
  batch 64 loss: 0.3715215241536498
  batch 65 loss: 0.37071893306878895
  batch 66 loss: 0.37046772106127307
  batch 67 loss: 0.37035692405344833
  batch 68 loss: 0.37026651072151523
  batch 69 loss: 0.3705271231955376
  batch 70 loss: 0.3702338925429753
  batch 71 loss: 0.3701152965216569
  batch 72 loss: 0.3699745080537266
  batch 73 loss: 0.3699357799471241
  batch 74 loss: 0.36961093182499344
  batch 75 loss: 0.36939900120099384
  batch 76 loss: 0.36963047675396266
  batch 77 loss: 0.3690016861859854
  batch 78 loss: 0.36881048824542606
  batch 79 loss: 0.36854987959318525
  batch 80 loss: 0.36822681240737437
  batch 81 loss: 0.36775411426285165
  batch 82 loss: 0.3677392303943634
  batch 83 loss: 0.3674253193010767
  batch 84 loss: 0.3667101107892536
  batch 85 loss: 0.36711903530008655
  batch 86 loss: 0.36674430481223175
  batch 87 loss: 0.36648740576601574
  batch 88 loss: 0.3662518140944568
  batch 89 loss: 0.365593872713239
  batch 90 loss: 0.36570045087072584
  batch 91 loss: 0.3651647708573184
  batch 92 loss: 0.3650216065023256
  batch 93 loss: 0.3645457003706245
  batch 94 loss: 0.3645962121638846
  batch 95 loss: 0.3646756583138516
  batch 96 loss: 0.3645512964576483
  batch 97 loss: 0.3649406002968857
  batch 98 loss: 0.364668147600427
  batch 99 loss: 0.36436254388154155
  batch 100 loss: 0.3642182147502899
  batch 101 loss: 0.3639248296765998
  batch 102 loss: 0.3640783084373848
  batch 103 loss: 0.36369707717478855
  batch 104 loss: 0.3641296031956489
  batch 105 loss: 0.36401510806310744
  batch 106 loss: 0.36424110548676186
  batch 107 loss: 0.3641294480484223
  batch 108 loss: 0.36390667960599615
  batch 109 loss: 0.36406775160667
  batch 110 loss: 0.3640313292091543
  batch 111 loss: 0.36409340865977174
  batch 112 loss: 0.3638300770627601
  batch 113 loss: 0.3634659777172899
  batch 114 loss: 0.3635477622350057
  batch 115 loss: 0.36318859095158784
  batch 116 loss: 0.36299122253368643
  batch 117 loss: 0.3630007482492007
  batch 118 loss: 0.3629906144687685
  batch 119 loss: 0.3633740561849931
  batch 120 loss: 0.3631625634928544
  batch 121 loss: 0.36326251911722923
  batch 122 loss: 0.3632417128222888
  batch 123 loss: 0.36309469691136986
  batch 124 loss: 0.3627716668190495
  batch 125 loss: 0.3622594051361084
  batch 126 loss: 0.36253640055656433
  batch 127 loss: 0.3624931653653543
  batch 128 loss: 0.36250442964956164
  batch 129 loss: 0.36258007805476816
  batch 130 loss: 0.36249649341289814
  batch 131 loss: 0.3624264365844144
  batch 132 loss: 0.3623066201354518
  batch 133 loss: 0.362173990423518
  batch 134 loss: 0.3618454199228714
  batch 135 loss: 0.3618426696017936
  batch 136 loss: 0.36189479030230465
  batch 137 loss: 0.36210040889517237
  batch 138 loss: 0.3621557981207751
  batch 139 loss: 0.3621169723195138
  batch 140 loss: 0.3622092302356448
  batch 141 loss: 0.36210228301954606
  batch 142 loss: 0.3619503840594224
  batch 143 loss: 0.3618966470231543
  batch 144 loss: 0.36156668493317234
  batch 145 loss: 0.36145511372336026
  batch 146 loss: 0.36148986710260994
  batch 147 loss: 0.3613739044082408
  batch 148 loss: 0.3612341055193463
  batch 149 loss: 0.3608267573302224
  batch 150 loss: 0.3608332318067551
  batch 151 loss: 0.360514406928953
  batch 152 loss: 0.3602374941110611
  batch 153 loss: 0.3605363368988037
  batch 154 loss: 0.36039508356676475
  batch 155 loss: 0.36037280713358233
  batch 156 loss: 0.360154831638703
  batch 157 loss: 0.35995117646114083
  batch 158 loss: 0.3601027581887909
  batch 159 loss: 0.36028147680954364
  batch 160 loss: 0.3599947888404131
  batch 161 loss: 0.35971280136463807
  batch 162 loss: 0.35960358013341454
  batch 163 loss: 0.35947280098324175
  batch 164 loss: 0.3596371739375882
  batch 165 loss: 0.3597329155965285
  batch 166 loss: 0.3599185342171106
  batch 167 loss: 0.35969154617029747
  batch 168 loss: 0.3594213016331196
  batch 169 loss: 0.35953226438640845
  batch 170 loss: 0.3596550408531638
  batch 171 loss: 0.3596938688155503
  batch 172 loss: 0.359502989365611
  batch 173 loss: 0.3594447544888954
  batch 174 loss: 0.35950543226181775
  batch 175 loss: 0.3594594871997833
  batch 176 loss: 0.3597116053781726
  batch 177 loss: 0.35981914064304976
  batch 178 loss: 0.35958887201346706
  batch 179 loss: 0.3595775094445191
  batch 180 loss: 0.35942611992359164
  batch 181 loss: 0.3592436853693335
  batch 182 loss: 0.35905491077637935
  batch 183 loss: 0.3589454925125414
  batch 184 loss: 0.35873547487932705
  batch 185 loss: 0.35848141795880084
  batch 186 loss: 0.3585277255824817
  batch 187 loss: 0.3587598327328177
  batch 188 loss: 0.35876503887962785
  batch 189 loss: 0.3587420647421842
  batch 190 loss: 0.3587789122995577
  batch 191 loss: 0.3588100738862422
  batch 192 loss: 0.358548391610384
  batch 193 loss: 0.35856733297436966
  batch 194 loss: 0.358529219768711
  batch 195 loss: 0.35874138535597383
  batch 196 loss: 0.3587349127141797
  batch 197 loss: 0.35869598555080784
  batch 198 loss: 0.3586825638106375
  batch 199 loss: 0.35860903104346004
  batch 200 loss: 0.35857698976993563
  batch 201 loss: 0.358298303772561
  batch 202 loss: 0.3580881487024893
  batch 203 loss: 0.3580240780496832
  batch 204 loss: 0.3579556372235803
  batch 205 loss: 0.35778992539498866
  batch 206 loss: 0.3577620741522428
  batch 207 loss: 0.3576622858715518
  batch 208 loss: 0.35743265885573167
  batch 209 loss: 0.35761655061438896
  batch 210 loss: 0.35754376323450177
  batch 211 loss: 0.35738068381191995
  batch 212 loss: 0.35737705863309355
  batch 213 loss: 0.3571968715235661
  batch 214 loss: 0.3571349202750999
  batch 215 loss: 0.3571839134360469
  batch 216 loss: 0.3572700887366577
  batch 217 loss: 0.35705306054809677
  batch 218 loss: 0.35682270628049834
  batch 219 loss: 0.3565554214666968
  batch 220 loss: 0.3565750619227236
  batch 221 loss: 0.3564637800956743
  batch 222 loss: 0.35653735321384294
  batch 223 loss: 0.3563746634887473
  batch 224 loss: 0.3563384734360235
  batch 225 loss: 0.3564533793926239
  batch 226 loss: 0.35633949342027177
  batch 227 loss: 0.3561835509564908
  batch 228 loss: 0.35612369511733977
  batch 229 loss: 0.35617897812456023
  batch 230 loss: 0.3560653743536576
  batch 231 loss: 0.3561046995641865
  batch 232 loss: 0.3560165254206493
  batch 233 loss: 0.35588160400226904
  batch 234 loss: 0.3558617574282182
  batch 235 loss: 0.35576413007492713
  batch 236 loss: 0.35567050124123945
  batch 237 loss: 0.3555805427867149
  batch 238 loss: 0.3554538174086258
  batch 239 loss: 0.3555586074934844
  batch 240 loss: 0.3556572825958331
  batch 241 loss: 0.3556320359350735
  batch 242 loss: 0.3556153207771049
  batch 243 loss: 0.35571099958792635
  batch 244 loss: 0.35572831467038296
  batch 245 loss: 0.35581959729291956
  batch 246 loss: 0.3557639074761693
  batch 247 loss: 0.3557519848771423
  batch 248 loss: 0.3559046111279918
  batch 249 loss: 0.3558638362760046
  batch 250 loss: 0.3558715670108795
  batch 251 loss: 0.3556965508783956
  batch 252 loss: 0.3556102922274953
  batch 253 loss: 0.35546668068222376
  batch 254 loss: 0.3554094736500988
  batch 255 loss: 0.3552076124677471
  batch 256 loss: 0.35512039298191667
  batch 257 loss: 0.3551732813105973
  batch 258 loss: 0.3550081847943077
  batch 259 loss: 0.3548763887992697
  batch 260 loss: 0.3547957747028424
  batch 261 loss: 0.3548753169076196
  batch 262 loss: 0.3548135436673201
  batch 263 loss: 0.35470749398148105
  batch 264 loss: 0.3548346260054545
  batch 265 loss: 0.35486409113092243
  batch 266 loss: 0.3547823202789278
  batch 267 loss: 0.3545970186758577
  batch 268 loss: 0.3544765572939346
  batch 269 loss: 0.3543160161121184
  batch 270 loss: 0.3542772309647666
  batch 271 loss: 0.3542999408561805
  batch 272 loss: 0.354156748884741
  batch 273 loss: 0.3541154878916758
  batch 274 loss: 0.3539437240492688
  batch 275 loss: 0.35394182974641974
  batch 276 loss: 0.353893607314946
  batch 277 loss: 0.35388512366084846
  batch 278 loss: 0.3537686917421629
  batch 279 loss: 0.35370126601615687
  batch 280 loss: 0.35353264798011097
  batch 281 loss: 0.35349751102118304
  batch 282 loss: 0.3532940611137566
  batch 283 loss: 0.35322630015784356
  batch 284 loss: 0.3532674480491961
  batch 285 loss: 0.35300481727248745
  batch 286 loss: 0.35299058466941324
  batch 287 loss: 0.35286120992504344
  batch 288 loss: 0.3528220717691713
  batch 289 loss: 0.35278282116028675
  batch 290 loss: 0.3526879448315193
  batch 291 loss: 0.35265481799738513
  batch 292 loss: 0.35270862444622875
  batch 293 loss: 0.35272040708886887
  batch 294 loss: 0.35254966381455766
  batch 295 loss: 0.3525340674287182
  batch 296 loss: 0.3526112429193548
  batch 297 loss: 0.35252887381849063
  batch 298 loss: 0.3525631141742604
  batch 299 loss: 0.3526178844597029
  batch 300 loss: 0.35257783442735674
  batch 301 loss: 0.35262208969094033
  batch 302 loss: 0.3526653818144704
  batch 303 loss: 0.3526395217026814
  batch 304 loss: 0.35266185738146305
  batch 305 loss: 0.3526417691199506
  batch 306 loss: 0.35269147303758885
  batch 307 loss: 0.3526601528305961
  batch 308 loss: 0.35262071825080105
  batch 309 loss: 0.35251344995977035
  batch 310 loss: 0.35255557048705316
  batch 311 loss: 0.352396986779676
  batch 312 loss: 0.35227547003290594
  batch 313 loss: 0.3522161013021256
  batch 314 loss: 0.3521419106775029
  batch 315 loss: 0.35211101656868343
  batch 316 loss: 0.3520982101371017
  batch 317 loss: 0.3522066360204378
  batch 318 loss: 0.35200102233661795
  batch 319 loss: 0.3517530844503062
  batch 320 loss: 0.3518961951136589
  batch 321 loss: 0.35181601944370805
  batch 322 loss: 0.3516558899272303
  batch 323 loss: 0.35163678418002997
  batch 324 loss: 0.3515056148171425
  batch 325 loss: 0.35152129934384274
  batch 326 loss: 0.3516593859788099
  batch 327 loss: 0.3515804155704078
  batch 328 loss: 0.35150901546202057
  batch 329 loss: 0.35144848745644636
  batch 330 loss: 0.3514114813371138
  batch 331 loss: 0.35133153317197935
  batch 332 loss: 0.3513808017933225
  batch 333 loss: 0.3513045311869086
  batch 334 loss: 0.35149311519668486
  batch 335 loss: 0.3514918079127127
  batch 336 loss: 0.3514548642117353
  batch 337 loss: 0.3514001706410586
  batch 338 loss: 0.3512573890255753
  batch 339 loss: 0.3512110791023502
  batch 340 loss: 0.351081977521672
  batch 341 loss: 0.3510278882868479
  batch 342 loss: 0.35101300852689127
  batch 343 loss: 0.3510777440258782
  batch 344 loss: 0.35116400959533317
  batch 345 loss: 0.3512043381082839
  batch 346 loss: 0.3511419606346615
  batch 347 loss: 0.3510669534247616
  batch 348 loss: 0.35102721612001286
  batch 349 loss: 0.3510769623569226
  batch 350 loss: 0.3511159146683557
  batch 351 loss: 0.3510408911779735
  batch 352 loss: 0.3509778890928084
  batch 353 loss: 0.3508686175933978
  batch 354 loss: 0.3508466044388248
  batch 355 loss: 0.3509180033710641
  batch 356 loss: 0.3509025134229928
  batch 357 loss: 0.35069102199137714
  batch 358 loss: 0.3505853255534305
  batch 359 loss: 0.35055449405752515
  batch 360 loss: 0.3504939499000708
  batch 361 loss: 0.35045923371064036
  batch 362 loss: 0.3503512661924678
  batch 363 loss: 0.35040281052103384
  batch 364 loss: 0.3503315683740836
  batch 365 loss: 0.35028801062335707
  batch 366 loss: 0.3503067905459899
  batch 367 loss: 0.35031122781275403
  batch 368 loss: 0.350250786251348
  batch 369 loss: 0.3502852962753637
  batch 370 loss: 0.3503929365325618
  batch 371 loss: 0.350377109976149
  batch 372 loss: 0.3504472960547734
  batch 373 loss: 0.3504363739458549
  batch 374 loss: 0.35041468881349513
  batch 375 loss: 0.3504025548299154
  batch 376 loss: 0.3504715715634062
  batch 377 loss: 0.3503975701426954
  batch 378 loss: 0.3502523535616183
  batch 379 loss: 0.3502027082569052
  batch 380 loss: 0.35012621495284535
  batch 381 loss: 0.3500650644771696
  batch 382 loss: 0.3499988376002037
  batch 383 loss: 0.3497909496565089
  batch 384 loss: 0.34975377819500864
  batch 385 loss: 0.34970904441622946
  batch 386 loss: 0.3496553877011482
  batch 387 loss: 0.34972645253790136
  batch 388 loss: 0.3496127699300186
  batch 389 loss: 0.3497173757816648
  batch 390 loss: 0.34967485811465826
  batch 391 loss: 0.34956723573567616
  batch 392 loss: 0.34961593979779554
  batch 393 loss: 0.3496424879584907
  batch 394 loss: 0.34961617711534354
  batch 395 loss: 0.34954559199417695
  batch 396 loss: 0.34953442712624866
  batch 397 loss: 0.34954020103219174
  batch 398 loss: 0.3494704019783729
  batch 399 loss: 0.3494157158491904
  batch 400 loss: 0.349500605687499
  batch 401 loss: 0.3494294065043813
  batch 402 loss: 0.34935609625643166
  batch 403 loss: 0.34923027481689645
  batch 404 loss: 0.34910501989692744
  batch 405 loss: 0.3491135283016864
  batch 406 loss: 0.34906654793934283
  batch 407 loss: 0.3490963204779848
  batch 408 loss: 0.34901246603797464
  batch 409 loss: 0.348878217064081
  batch 410 loss: 0.3487512499820895
  batch 411 loss: 0.3487866063332616
  batch 412 loss: 0.3489634268781514
  batch 413 loss: 0.34891120270435805
  batch 414 loss: 0.34890198146087537
  batch 415 loss: 0.34888122002762484
  batch 416 loss: 0.34891996855059493
  batch 417 loss: 0.3488967010586096
  batch 418 loss: 0.34886399916769784
  batch 419 loss: 0.3488677821893396
  batch 420 loss: 0.34885255367982954
  batch 421 loss: 0.3488260588991387
  batch 422 loss: 0.3487683603831377
  batch 423 loss: 0.34879061199249106
  batch 424 loss: 0.3488880210327652
  batch 425 loss: 0.3487740733343012
  batch 426 loss: 0.34867393641684535
  batch 427 loss: 0.3486414182521141
  batch 428 loss: 0.34859610223603027
  batch 429 loss: 0.34856160731860253
  batch 430 loss: 0.34857016531533974
  batch 431 loss: 0.34857898611756877
  batch 432 loss: 0.3484851231591569
  batch 433 loss: 0.34834895164256285
  batch 434 loss: 0.3482840792512015
  batch 435 loss: 0.34834604941565417
  batch 436 loss: 0.34835672658791234
  batch 437 loss: 0.3482486187593342
  batch 438 loss: 0.34818037078805164
  batch 439 loss: 0.3481410420701281
  batch 440 loss: 0.34824598506093024
  batch 441 loss: 0.34826039017733534
  batch 442 loss: 0.3482039199290772
  batch 443 loss: 0.3481580496641637
  batch 444 loss: 0.34819890108999907
  batch 445 loss: 0.34817001779427686
  batch 446 loss: 0.34817386355100727
  batch 447 loss: 0.34819134116439476
  batch 448 loss: 0.34821291021736606
  batch 449 loss: 0.3481627203573894
  batch 450 loss: 0.34807532469431557
  batch 451 loss: 0.3480215791588083
  batch 452 loss: 0.34789178474287014
  batch 453 loss: 0.3478872325378275
  batch 454 loss: 0.34781395017050437
  batch 455 loss: 0.3477473839298709
  batch 456 loss: 0.3477369699823229
  batch 457 loss: 0.34770751899389524
  batch 458 loss: 0.3476087158163562
  batch 459 loss: 0.34757532494260335
  batch 460 loss: 0.347567251065503
  batch 461 loss: 0.3475961113700122
  batch 462 loss: 0.34767407346597484
  batch 463 loss: 0.3477180285407451
  batch 464 loss: 0.3475963734090328
  batch 465 loss: 0.34755451788184466
  batch 466 loss: 0.3474981736215911
  batch 467 loss: 0.34757072926843957
  batch 468 loss: 0.34747391652602416
  batch 469 loss: 0.34738804345954455
  batch 470 loss: 0.3474454624855772
  batch 471 loss: 0.34741939170851577
  batch 472 loss: 0.347399453932451
LOSS train 0.347399453932451 valid 0.24979184567928314
LOSS train 0.347399453932451 valid 0.25214410573244095
LOSS train 0.347399453932451 valid 0.25255197783311206
LOSS train 0.347399453932451 valid 0.24160686880350113
LOSS train 0.347399453932451 valid 0.24487837553024291
LOSS train 0.347399453932451 valid 0.2508442203203837
LOSS train 0.347399453932451 valid 0.24465774425438472
LOSS train 0.347399453932451 valid 0.2429861482232809
LOSS train 0.347399453932451 valid 0.24209928181436327
LOSS train 0.347399453932451 valid 0.24124421179294586
LOSS train 0.347399453932451 valid 0.2394652176987041
LOSS train 0.347399453932451 valid 0.2424541860818863
LOSS train 0.347399453932451 valid 0.24263819249776694
LOSS train 0.347399453932451 valid 0.24082281440496445
LOSS train 0.347399453932451 valid 0.23971286217371623
LOSS train 0.347399453932451 valid 0.24264188669621944
LOSS train 0.347399453932451 valid 0.2423735269728829
LOSS train 0.347399453932451 valid 0.24303412520223194
LOSS train 0.347399453932451 valid 0.2450925449007436
LOSS train 0.347399453932451 valid 0.24490659907460213
LOSS train 0.347399453932451 valid 0.24506532365367525
LOSS train 0.347399453932451 valid 0.2443958500569517
LOSS train 0.347399453932451 valid 0.2437756845484609
LOSS train 0.347399453932451 valid 0.24450900591909885
LOSS train 0.347399453932451 valid 0.24377512454986572
LOSS train 0.347399453932451 valid 0.2428442881657527
LOSS train 0.347399453932451 valid 0.24287043898193925
LOSS train 0.347399453932451 valid 0.24253183071102416
LOSS train 0.347399453932451 valid 0.24168047000621928
LOSS train 0.347399453932451 valid 0.24085897306601206
LOSS train 0.347399453932451 valid 0.24096990208471974
LOSS train 0.347399453932451 valid 0.24158570542931557
LOSS train 0.347399453932451 valid 0.2408630373803052
LOSS train 0.347399453932451 valid 0.24043263977064805
LOSS train 0.347399453932451 valid 0.24100271250520433
LOSS train 0.347399453932451 valid 0.24154486879706383
LOSS train 0.347399453932451 valid 0.2413412429190971
LOSS train 0.347399453932451 valid 0.2410520817104139
LOSS train 0.347399453932451 valid 0.24147347991283125
LOSS train 0.347399453932451 valid 0.24183884188532828
LOSS train 0.347399453932451 valid 0.24146061626876272
LOSS train 0.347399453932451 valid 0.24249049240634554
LOSS train 0.347399453932451 valid 0.24262976473154024
LOSS train 0.347399453932451 valid 0.24211637100035494
LOSS train 0.347399453932451 valid 0.24142309261692896
LOSS train 0.347399453932451 valid 0.24088988420755966
LOSS train 0.347399453932451 valid 0.24044999258315308
LOSS train 0.347399453932451 valid 0.24137941903124252
LOSS train 0.347399453932451 valid 0.24063883356902063
LOSS train 0.347399453932451 valid 0.2413840612769127
LOSS train 0.347399453932451 valid 0.24117483081770877
LOSS train 0.347399453932451 valid 0.24085790950518388
LOSS train 0.347399453932451 valid 0.24164722494359286
LOSS train 0.347399453932451 valid 0.2416003324367382
LOSS train 0.347399453932451 valid 0.24133607772263613
LOSS train 0.347399453932451 valid 0.24154160251574858
LOSS train 0.347399453932451 valid 0.24122790521696993
LOSS train 0.347399453932451 valid 0.24213362587936993
LOSS train 0.347399453932451 valid 0.24230862951884835
LOSS train 0.347399453932451 valid 0.24209581092000007
LOSS train 0.347399453932451 valid 0.2420942092039546
LOSS train 0.347399453932451 valid 0.24163848666414137
LOSS train 0.347399453932451 valid 0.24151711118599725
LOSS train 0.347399453932451 valid 0.24146684142760932
LOSS train 0.347399453932451 valid 0.24060615645005154
LOSS train 0.347399453932451 valid 0.24061933927463763
LOSS train 0.347399453932451 valid 0.24104467345707453
LOSS train 0.347399453932451 valid 0.24048109199194348
LOSS train 0.347399453932451 valid 0.2412978581328323
LOSS train 0.347399453932451 valid 0.24134128327880586
LOSS train 0.347399453932451 valid 0.24166711934015783
LOSS train 0.347399453932451 valid 0.24185128944615522
LOSS train 0.347399453932451 valid 0.242540398076789
LOSS train 0.347399453932451 valid 0.24246337385596456
LOSS train 0.347399453932451 valid 0.24209208269913993
LOSS train 0.347399453932451 valid 0.24218286279785006
LOSS train 0.347399453932451 valid 0.24215925513923942
LOSS train 0.347399453932451 valid 0.241993020933408
LOSS train 0.347399453932451 valid 0.2422094682726679
LOSS train 0.347399453932451 valid 0.24191198106855155
LOSS train 0.347399453932451 valid 0.24197526754420481
LOSS train 0.347399453932451 valid 0.2420577157924815
LOSS train 0.347399453932451 valid 0.2422011119414525
LOSS train 0.347399453932451 valid 0.24217197476398378
LOSS train 0.347399453932451 valid 0.24231643326142255
LOSS train 0.347399453932451 valid 0.2421968288199846
LOSS train 0.347399453932451 valid 0.24181136094975744
LOSS train 0.347399453932451 valid 0.2416514725509015
LOSS train 0.347399453932451 valid 0.24195059581419057
LOSS train 0.347399453932451 valid 0.24203939156399834
LOSS train 0.347399453932451 valid 0.24195063539913722
LOSS train 0.347399453932451 valid 0.24199862648611484
LOSS train 0.347399453932451 valid 0.24184395260708305
LOSS train 0.347399453932451 valid 0.2419577180705172
LOSS train 0.347399453932451 valid 0.242123555195959
LOSS train 0.347399453932451 valid 0.2423725314438343
LOSS train 0.347399453932451 valid 0.24257340013366385
LOSS train 0.347399453932451 valid 0.2429506581048576
LOSS train 0.347399453932451 valid 0.24312395218646887
LOSS train 0.347399453932451 valid 0.24322187691926955
LOSS train 0.347399453932451 valid 0.24317774737235343
LOSS train 0.347399453932451 valid 0.2435873688436022
LOSS train 0.347399453932451 valid 0.24328518564839965
LOSS train 0.347399453932451 valid 0.2430688665749935
LOSS train 0.347399453932451 valid 0.24324462115764617
LOSS train 0.347399453932451 valid 0.24318745830711327
LOSS train 0.347399453932451 valid 0.2428643139444779
LOSS train 0.347399453932451 valid 0.2427685736782021
LOSS train 0.347399453932451 valid 0.2424177813147186
LOSS train 0.347399453932451 valid 0.24240013753825967
LOSS train 0.347399453932451 valid 0.24268502735339845
LOSS train 0.347399453932451 valid 0.24295131316674606
LOSS train 0.347399453932451 valid 0.24292014882100366
LOSS train 0.347399453932451 valid 0.24283227102275481
LOSS train 0.347399453932451 valid 0.24333577946476315
LOSS train 0.347399453932451 valid 0.24305693792371913
LOSS train 0.347399453932451 valid 0.24345885344550142
LOSS train 0.347399453932451 valid 0.2435355647388151
LOSS train 0.347399453932451 valid 0.24334557078966573
LOSS train 0.347399453932451 valid 0.24308908035357793
LOSS train 0.347399453932451 valid 0.24320621061916195
LOSS train 0.347399453932451 valid 0.24344685282863554
LOSS train 0.347399453932451 valid 0.24332355023399602
LOSS train 0.347399453932451 valid 0.24328630946336255
LOSS train 0.347399453932451 valid 0.24323856711387634
LOSS train 0.347399453932451 valid 0.24332208955098714
LOSS train 0.347399453932451 valid 0.24330508838019033
LOSS train 0.347399453932451 valid 0.24319337541237473
LOSS train 0.347399453932451 valid 0.243102161921272
LOSS train 0.347399453932451 valid 0.2428783689553921
LOSS train 0.347399453932451 valid 0.24282697025146194
LOSS train 0.347399453932451 valid 0.24286337287137003
LOSS train 0.347399453932451 valid 0.2428112200328282
LOSS train 0.347399453932451 valid 0.24300330966266234
LOSS train 0.347399453932451 valid 0.2430520208897414
LOSS train 0.347399453932451 valid 0.24307158501709208
LOSS train 0.347399453932451 valid 0.24313414336120995
LOSS train 0.347399453932451 valid 0.24299101974221243
LOSS train 0.347399453932451 valid 0.242797274276507
LOSS train 0.347399453932451 valid 0.24289239017026765
LOSS train 0.347399453932451 valid 0.24297968261208094
LOSS train 0.347399453932451 valid 0.24321126549596517
LOSS train 0.347399453932451 valid 0.243302405818359
LOSS train 0.347399453932451 valid 0.24327199212792847
LOSS train 0.347399453932451 valid 0.24312570660278715
LOSS train 0.347399453932451 valid 0.24308365215993907
LOSS train 0.347399453932451 valid 0.243076368659532
LOSS train 0.347399453932451 valid 0.24298091494553797
LOSS train 0.347399453932451 valid 0.24301532860970337
LOSS train 0.347399453932451 valid 0.24300165603558221
LOSS train 0.347399453932451 valid 0.24285921809689098
LOSS train 0.347399453932451 valid 0.24273435722448325
LOSS train 0.347399453932451 valid 0.24255692432908452
LOSS train 0.347399453932451 valid 0.2426337255285932
LOSS train 0.347399453932451 valid 0.24264951463668577
LOSS train 0.347399453932451 valid 0.24281622584049517
LOSS train 0.347399453932451 valid 0.24273070645560124
LOSS train 0.347399453932451 valid 0.24277495649419253
LOSS train 0.347399453932451 valid 0.24306882093162657
LOSS train 0.347399453932451 valid 0.24306268552318216
LOSS train 0.347399453932451 valid 0.24299514293670654
LOSS train 0.347399453932451 valid 0.2430443134572771
LOSS train 0.347399453932451 valid 0.24278805674585097
LOSS train 0.347399453932451 valid 0.24281651454000938
LOSS train 0.347399453932451 valid 0.2428962270418803
LOSS train 0.347399453932451 valid 0.24270602163062038
LOSS train 0.347399453932451 valid 0.2428378657309595
LOSS train 0.347399453932451 valid 0.24266286238673188
LOSS train 0.347399453932451 valid 0.2424072595037652
LOSS train 0.347399453932451 valid 0.24250831674127019
LOSS train 0.347399453932451 valid 0.24234347167419412
LOSS train 0.347399453932451 valid 0.2423712895360104
LOSS train 0.347399453932451 valid 0.242382279365738
LOSS train 0.347399453932451 valid 0.24265247720411454
LOSS train 0.347399453932451 valid 0.24260644682816096
LOSS train 0.347399453932451 valid 0.24252572418613869
LOSS train 0.347399453932451 valid 0.2425611546147341
LOSS train 0.347399453932451 valid 0.24265942580244515
LOSS train 0.347399453932451 valid 0.24275754966549368
LOSS train 0.347399453932451 valid 0.24276001999775568
LOSS train 0.347399453932451 valid 0.2426843464539196
LOSS train 0.347399453932451 valid 0.24259363245833052
LOSS train 0.347399453932451 valid 0.24275549871674001
LOSS train 0.347399453932451 valid 0.24272772896548975
LOSS train 0.347399453932451 valid 0.24281542172303072
LOSS train 0.347399453932451 valid 0.24291768173376718
LOSS train 0.347399453932451 valid 0.24275918567881866
LOSS train 0.347399453932451 valid 0.24286009323723773
LOSS train 0.347399453932451 valid 0.24266117771781942
LOSS train 0.347399453932451 valid 0.24271152780244226
LOSS train 0.347399453932451 valid 0.24286121980370026
LOSS train 0.347399453932451 valid 0.24295657042724392
LOSS train 0.347399453932451 valid 0.24291473364582952
LOSS train 0.347399453932451 valid 0.2429103582482977
LOSS train 0.347399453932451 valid 0.24289577290033681
LOSS train 0.347399453932451 valid 0.24314141129048503
LOSS train 0.347399453932451 valid 0.24315713633438052
LOSS train 0.347399453932451 valid 0.2434649049784198
LOSS train 0.347399453932451 valid 0.24345337827900546
LOSS train 0.347399453932451 valid 0.24359315998852252
LOSS train 0.347399453932451 valid 0.24344758830260282
LOSS train 0.347399453932451 valid 0.24342286520370163
LOSS train 0.347399453932451 valid 0.2435871712826743
LOSS train 0.347399453932451 valid 0.24347058610588895
LOSS train 0.347399453932451 valid 0.24341749745171246
LOSS train 0.347399453932451 valid 0.24333218289810477
LOSS train 0.347399453932451 valid 0.24328216334471955
LOSS train 0.347399453932451 valid 0.24308960447804287
LOSS train 0.347399453932451 valid 0.24319615767618116
LOSS train 0.347399453932451 valid 0.24332947184642156
LOSS train 0.347399453932451 valid 0.24351619918481998
LOSS train 0.347399453932451 valid 0.24339102658460726
LOSS train 0.347399453932451 valid 0.24328666862467646
LOSS train 0.347399453932451 valid 0.2432584406755795
LOSS train 0.347399453932451 valid 0.243048119267752
LOSS train 0.347399453932451 valid 0.24295905632553277
LOSS train 0.347399453932451 valid 0.2427688028817902
LOSS train 0.347399453932451 valid 0.24276957237118973
LOSS train 0.347399453932451 valid 0.24279658897826661
LOSS train 0.347399453932451 valid 0.2428713635964827
LOSS train 0.347399453932451 valid 0.24288085595245276
LOSS train 0.347399453932451 valid 0.24296750067859083
LOSS train 0.347399453932451 valid 0.24298956268571417
LOSS train 0.347399453932451 valid 0.2428840562435133
LOSS train 0.347399453932451 valid 0.24275531397925482
LOSS train 0.347399453932451 valid 0.2427222283013099
LOSS train 0.347399453932451 valid 0.24276932365043574
LOSS train 0.347399453932451 valid 0.24267059734515978
LOSS train 0.347399453932451 valid 0.2425015891483257
LOSS train 0.347399453932451 valid 0.24261123654635056
LOSS train 0.347399453932451 valid 0.24255937634608446
LOSS train 0.347399453932451 valid 0.24263954997576517
LOSS train 0.347399453932451 valid 0.24285931763730847
LOSS train 0.347399453932451 valid 0.24283045823248023
LOSS train 0.347399453932451 valid 0.242896376518493
LOSS train 0.347399453932451 valid 0.24277392762192226
LOSS train 0.347399453932451 valid 0.24283330551179652
LOSS train 0.347399453932451 valid 0.24290810149757802
LOSS train 0.347399453932451 valid 0.24288012904103332
LOSS train 0.347399453932451 valid 0.24250859829286733
LOSS train 0.347399453932451 valid 0.2423102482225885
LOSS train 0.347399453932451 valid 0.24229954115369104
LOSS train 0.347399453932451 valid 0.24231194082356283
LOSS train 0.347399453932451 valid 0.2423083294244086
LOSS train 0.347399453932451 valid 0.24244569266329005
LOSS train 0.347399453932451 valid 0.24245943716628765
LOSS train 0.347399453932451 valid 0.2424729429637855
LOSS train 0.347399453932451 valid 0.2425374738992222
LOSS train 0.347399453932451 valid 0.24236772098694462
LOSS train 0.347399453932451 valid 0.24249951994419097
LOSS train 0.347399453932451 valid 0.242575005706088
LOSS train 0.347399453932451 valid 0.24255139664525077
LOSS train 0.347399453932451 valid 0.2424281301941325
LOSS train 0.347399453932451 valid 0.24249359777593238
LOSS train 0.347399453932451 valid 0.24245858338533663
LOSS train 0.347399453932451 valid 0.24238356825662777
LOSS train 0.347399453932451 valid 0.2423950479536205
LOSS train 0.347399453932451 valid 0.24246471571598865
LOSS train 0.347399453932451 valid 0.2424188952418368
LOSS train 0.347399453932451 valid 0.24233699561311647
LOSS train 0.347399453932451 valid 0.2423659382081123
LOSS train 0.347399453932451 valid 0.24254058227511763
LOSS train 0.347399453932451 valid 0.24247712344497782
LOSS train 0.347399453932451 valid 0.2425811000055436
LOSS train 0.347399453932451 valid 0.2425769417353396
LOSS train 0.347399453932451 valid 0.24259278934476966
LOSS train 0.347399453932451 valid 0.2425227929813585
LOSS train 0.347399453932451 valid 0.2424997609497896
LOSS train 0.347399453932451 valid 0.24254995108093913
LOSS train 0.347399453932451 valid 0.24249405568396604
LOSS train 0.347399453932451 valid 0.24251216996419914
LOSS train 0.347399453932451 valid 0.24238341411246972
LOSS train 0.347399453932451 valid 0.24223170776070255
LOSS train 0.347399453932451 valid 0.24218378466193693
LOSS train 0.347399453932451 valid 0.24231055178425528
LOSS train 0.347399453932451 valid 0.2423808896995109
LOSS train 0.347399453932451 valid 0.24242910005770865
LOSS train 0.347399453932451 valid 0.24241799737909714
LOSS train 0.347399453932451 valid 0.24243085273277803
LOSS train 0.347399453932451 valid 0.24234309547713825
LOSS train 0.347399453932451 valid 0.24221762341324546
LOSS train 0.347399453932451 valid 0.24222551381334345
LOSS train 0.347399453932451 valid 0.2423177908039767
LOSS train 0.347399453932451 valid 0.24220978452915876
LOSS train 0.347399453932451 valid 0.24222571985763416
LOSS train 0.347399453932451 valid 0.24225686615580447
LOSS train 0.347399453932451 valid 0.2422743755350545
LOSS train 0.347399453932451 valid 0.24224160026965869
LOSS train 0.347399453932451 valid 0.24218764210242302
LOSS train 0.347399453932451 valid 0.2420974772551964
LOSS train 0.347399453932451 valid 0.24205461036913173
LOSS train 0.347399453932451 valid 0.24200963560644895
LOSS train 0.347399453932451 valid 0.24204281160652433
LOSS train 0.347399453932451 valid 0.242041606380015
LOSS train 0.347399453932451 valid 0.24208071241944523
LOSS train 0.347399453932451 valid 0.24205376816963828
LOSS train 0.347399453932451 valid 0.24200593286291117
LOSS train 0.347399453932451 valid 0.24205920315228852
LOSS train 0.347399453932451 valid 0.24204207657770968
LOSS train 0.347399453932451 valid 0.24202042147517205
LOSS train 0.347399453932451 valid 0.2419516290244074
LOSS train 0.347399453932451 valid 0.2419604898880649
LOSS train 0.347399453932451 valid 0.24190833805614573
LOSS train 0.347399453932451 valid 0.24191386331068843
LOSS train 0.347399453932451 valid 0.24191126696399
LOSS train 0.347399453932451 valid 0.24204941405579936
LOSS train 0.347399453932451 valid 0.24201739756602805
LOSS train 0.347399453932451 valid 0.24208191066206275
LOSS train 0.347399453932451 valid 0.24209174861028357
LOSS train 0.347399453932451 valid 0.2420866780223385
LOSS train 0.347399453932451 valid 0.24214290456181553
LOSS train 0.347399453932451 valid 0.2420897441319166
LOSS train 0.347399453932451 valid 0.2421500813751556
LOSS train 0.347399453932451 valid 0.2420782973622061
LOSS train 0.347399453932451 valid 0.24204600477029406
LOSS train 0.347399453932451 valid 0.24208901270846778
LOSS train 0.347399453932451 valid 0.24205031017963818
LOSS train 0.347399453932451 valid 0.2420576698843788
LOSS train 0.347399453932451 valid 0.242059071282608
LOSS train 0.347399453932451 valid 0.24200192443095148
LOSS train 0.347399453932451 valid 0.24205517281438702
LOSS train 0.347399453932451 valid 0.2420481354090738
LOSS train 0.347399453932451 valid 0.24191963423516358
LOSS train 0.347399453932451 valid 0.2418667666706038
LOSS train 0.347399453932451 valid 0.24173416816271268
LOSS train 0.347399453932451 valid 0.24178657310506319
LOSS train 0.347399453932451 valid 0.2417390423539217
LOSS train 0.347399453932451 valid 0.24176908806874986
LOSS train 0.347399453932451 valid 0.24172225422409893
LOSS train 0.347399453932451 valid 0.24171149694558347
LOSS train 0.347399453932451 valid 0.24160250769280955
LOSS train 0.347399453932451 valid 0.24156002718282032
LOSS train 0.347399453932451 valid 0.24167828391622137
LOSS train 0.347399453932451 valid 0.2416583143486948
LOSS train 0.347399453932451 valid 0.2415641172163522
LOSS train 0.347399453932451 valid 0.2414627124422363
LOSS train 0.347399453932451 valid 0.24160940264205197
LOSS train 0.347399453932451 valid 0.24153715623377342
LOSS train 0.347399453932451 valid 0.24153114565178357
LOSS train 0.347399453932451 valid 0.2416208948720904
LOSS train 0.347399453932451 valid 0.241587518509532
LOSS train 0.347399453932451 valid 0.24152994378094086
LOSS train 0.347399453932451 valid 0.24141802510610474
LOSS train 0.347399453932451 valid 0.24147831202419692
LOSS train 0.347399453932451 valid 0.24155971170335577
LOSS train 0.347399453932451 valid 0.24149586497186926
LOSS train 0.347399453932451 valid 0.24143618916400228
LOSS train 0.347399453932451 valid 0.24149295263763132
LOSS train 0.347399453932451 valid 0.2414019114223114
LOSS train 0.347399453932451 valid 0.2414072989991733
LOSS train 0.347399453932451 valid 0.2414713487071529
LOSS train 0.347399453932451 valid 0.24150318580425598
LOSS train 0.347399453932451 valid 0.24158709654558144
LOSS train 0.347399453932451 valid 0.24153640415877273
LOSS train 0.347399453932451 valid 0.24146483527942442
LOSS train 0.347399453932451 valid 0.24151199867718676
LOSS train 0.347399453932451 valid 0.24155151965070507
LOSS train 0.347399453932451 valid 0.2415574871877718
LOSS train 0.347399453932451 valid 0.24156448871810457
LOSS train 0.347399453932451 valid 0.24163794554769993
LOSS train 0.347399453932451 valid 0.24158420543756512
LOSS train 0.347399453932451 valid 0.24164113052477493
LOSS train 0.347399453932451 valid 0.24170663431492062
LOSS train 0.347399453932451 valid 0.2416043170615212
LOSS train 0.347399453932451 valid 0.2416883846260097
LOSS train 0.347399453932451 valid 0.24162983804778324
LOSS train 0.347399453932451 valid 0.2416180011686902
LOSS train 0.347399453932451 valid 0.24158737565512242
LOSS train 0.347399453932451 valid 0.24162364967147187
EPOCH 3:
  batch 1 loss: 0.35279855132102966
  batch 2 loss: 0.3676595240831375
  batch 3 loss: 0.35446863373120624
  batch 4 loss: 0.3566531464457512
  batch 5 loss: 0.356985342502594
  batch 6 loss: 0.3522402097781499
  batch 7 loss: 0.35056485874312265
  batch 8 loss: 0.34662554785609245
  batch 9 loss: 0.3469577464792464
  batch 10 loss: 0.34226168096065523
  batch 11 loss: 0.3410735103217038
  batch 12 loss: 0.3395386536916097
  batch 13 loss: 0.3373153507709503
  batch 14 loss: 0.3375772557088307
  batch 15 loss: 0.3391392807165782
  batch 16 loss: 0.3391547929495573
  batch 17 loss: 0.33952029487665963
  batch 18 loss: 0.33733247882790035
  batch 19 loss: 0.3365825587197354
  batch 20 loss: 0.33392584770917894
  batch 21 loss: 0.3391563367275965
  batch 22 loss: 0.33946666798808356
  batch 23 loss: 0.3378114985383075
  batch 24 loss: 0.3385506272315979
  batch 25 loss: 0.33984644889831545
  batch 26 loss: 0.33847655470554644
  batch 27 loss: 0.3406692721225597
  batch 28 loss: 0.3420854519520487
  batch 29 loss: 0.34177511005566036
  batch 30 loss: 0.342604997754097
  batch 31 loss: 0.34288817932528837
  batch 32 loss: 0.3445876073092222
  batch 33 loss: 0.3459975909103047
  batch 34 loss: 0.34563933312892914
  batch 35 loss: 0.3466953158378601
  batch 36 loss: 0.3459141403436661
  batch 37 loss: 0.34495327279374405
  batch 38 loss: 0.3452541930110831
  batch 39 loss: 0.34573259414770663
  batch 40 loss: 0.3454666718840599
  batch 41 loss: 0.3450313363133407
  batch 42 loss: 0.3443889312800907
  batch 43 loss: 0.3441545443479405
  batch 44 loss: 0.344257805835117
  batch 45 loss: 0.3444999549123976
  batch 46 loss: 0.34319751936456433
  batch 47 loss: 0.3429500530374811
  batch 48 loss: 0.3426385782659054
  batch 49 loss: 0.3430851935123911
  batch 50 loss: 0.3430985325574875
  batch 51 loss: 0.3429493319754507
  batch 52 loss: 0.34259751782967496
  batch 53 loss: 0.34211638063754674
  batch 54 loss: 0.3419111553165648
  batch 55 loss: 0.3420542348514904
  batch 56 loss: 0.3427552282810211
  batch 57 loss: 0.3419181333299269
  batch 58 loss: 0.3416255758754138
  batch 59 loss: 0.3418612818596727
  batch 60 loss: 0.3417239343126615
  batch 61 loss: 0.34235980227345325
  batch 62 loss: 0.3426880000099059
  batch 63 loss: 0.34188033474816215
  batch 64 loss: 0.3416279451921582
  batch 65 loss: 0.3407173541875986
  batch 66 loss: 0.34050112375707337
  batch 67 loss: 0.3403017151711592
  batch 68 loss: 0.3401756251559538
  batch 69 loss: 0.3402669265650321
  batch 70 loss: 0.3396806559392384
  batch 71 loss: 0.33991302067125345
  batch 72 loss: 0.3398192570441299
  batch 73 loss: 0.3396098462686147
  batch 74 loss: 0.33940672189802734
  batch 75 loss: 0.33944238742192584
  batch 76 loss: 0.3396346533769055
  batch 77 loss: 0.3391951616708334
  batch 78 loss: 0.3394296692732053
  batch 79 loss: 0.33919352632534655
  batch 80 loss: 0.33895963728427886
  batch 81 loss: 0.3387103901233202
  batch 82 loss: 0.3387214363348193
  batch 83 loss: 0.33839340095060416
  batch 84 loss: 0.33783175406001864
  batch 85 loss: 0.33820399361498216
  batch 86 loss: 0.33803031195041744
  batch 87 loss: 0.33775410158880825
  batch 88 loss: 0.3376061144200238
  batch 89 loss: 0.3369744249274222
  batch 90 loss: 0.3372098035282559
  batch 91 loss: 0.336860743525264
  batch 92 loss: 0.33671736620042636
  batch 93 loss: 0.33623309161073417
  batch 94 loss: 0.3362757573102383
  batch 95 loss: 0.3363247861987666
  batch 96 loss: 0.33628349533925456
  batch 97 loss: 0.33659409278446867
  batch 98 loss: 0.3362841000970529
  batch 99 loss: 0.3360858050861744
  batch 100 loss: 0.336079920232296
  batch 101 loss: 0.3356988849026142
  batch 102 loss: 0.3358226745736365
  batch 103 loss: 0.3355241056206157
  batch 104 loss: 0.33582067432311863
  batch 105 loss: 0.3356541843641372
  batch 106 loss: 0.335968406132932
  batch 107 loss: 0.33580234451828717
  batch 108 loss: 0.33556563600345896
  batch 109 loss: 0.335828169770197
  batch 110 loss: 0.33589836088093844
  batch 111 loss: 0.335874051124126
  batch 112 loss: 0.33555951874170986
  batch 113 loss: 0.335250722359767
  batch 114 loss: 0.33519203202766285
  batch 115 loss: 0.33497132477552993
  batch 116 loss: 0.3347516583985296
  batch 117 loss: 0.3345796855596396
  batch 118 loss: 0.3348287452580565
  batch 119 loss: 0.3352546063290925
  batch 120 loss: 0.33504529868563016
  batch 121 loss: 0.3352251582401843
  batch 122 loss: 0.33538746687232474
  batch 123 loss: 0.3352368941636589
  batch 124 loss: 0.335052230906102
  batch 125 loss: 0.33453592348098754
  batch 126 loss: 0.334711738758617
  batch 127 loss: 0.33488622777105315
  batch 128 loss: 0.335004142485559
  batch 129 loss: 0.33510683372963307
  batch 130 loss: 0.3351785040818728
  batch 131 loss: 0.3353649566646751
  batch 132 loss: 0.33543177152221854
  batch 133 loss: 0.3353290936552492
  batch 134 loss: 0.3351544815657744
  batch 135 loss: 0.3352298511399163
  batch 136 loss: 0.3353196568787098
  batch 137 loss: 0.33541687996718134
  batch 138 loss: 0.33551392347916315
  batch 139 loss: 0.3356676301081404
  batch 140 loss: 0.3357779324054718
  batch 141 loss: 0.33564993527764125
  batch 142 loss: 0.33547486889530237
  batch 143 loss: 0.33549024112574705
  batch 144 loss: 0.33509704615506863
  batch 145 loss: 0.33521160709446873
  batch 146 loss: 0.33536303165840775
  batch 147 loss: 0.3353525242026971
  batch 148 loss: 0.3352478162662403
  batch 149 loss: 0.33484710062910245
  batch 150 loss: 0.33497819662094114
  batch 151 loss: 0.3347189931680035
  batch 152 loss: 0.3345737582758853
  batch 153 loss: 0.3349958367986617
  batch 154 loss: 0.33496966145255347
  batch 155 loss: 0.3350444853305817
  batch 156 loss: 0.3348550947430806
  batch 157 loss: 0.3348025134794272
  batch 158 loss: 0.33499921122683757
  batch 159 loss: 0.3351199486345615
  batch 160 loss: 0.3348644431680441
  batch 161 loss: 0.3344915629543873
  batch 162 loss: 0.33442452365969433
  batch 163 loss: 0.33429813385009766
  batch 164 loss: 0.33455878005522055
  batch 165 loss: 0.3346641713922674
  batch 166 loss: 0.3348371104662677
  batch 167 loss: 0.3347021118252577
  batch 168 loss: 0.3344357418162482
  batch 169 loss: 0.3346271454935243
  batch 170 loss: 0.33469009224106283
  batch 171 loss: 0.33489990460942365
  batch 172 loss: 0.3347701061257096
  batch 173 loss: 0.3347180876773217
  batch 174 loss: 0.3348195345579893
  batch 175 loss: 0.334905834197998
  batch 176 loss: 0.33513539656996727
  batch 177 loss: 0.3353520214220898
  batch 178 loss: 0.33515067783634317
  batch 179 loss: 0.3352461627075792
  batch 180 loss: 0.33514147367742325
  batch 181 loss: 0.33500017003459825
  batch 182 loss: 0.33481552951283505
  batch 183 loss: 0.33472956187738095
  batch 184 loss: 0.3344672446989495
  batch 185 loss: 0.3342392367285651
  batch 186 loss: 0.3341959296695648
  batch 187 loss: 0.33436778076192275
  batch 188 loss: 0.3343964708929366
  batch 189 loss: 0.3344058027027776
  batch 190 loss: 0.3343971578698409
  batch 191 loss: 0.33447187343192974
  batch 192 loss: 0.33425544761121273
  batch 193 loss: 0.33428884108449514
  batch 194 loss: 0.3342456923624904
  batch 195 loss: 0.334373608766458
  batch 196 loss: 0.33436342480839515
  batch 197 loss: 0.3342839391703533
  batch 198 loss: 0.3342875660669924
  batch 199 loss: 0.3341877927732228
  batch 200 loss: 0.33418131440877913
  batch 201 loss: 0.3338940757424084
  batch 202 loss: 0.3336659674302186
  batch 203 loss: 0.33358602512058955
  batch 204 loss: 0.33352967074104384
  batch 205 loss: 0.33334374631323466
  batch 206 loss: 0.33329102903315166
  batch 207 loss: 0.3330927522862015
  batch 208 loss: 0.3328479284850451
  batch 209 loss: 0.332993741240798
  batch 210 loss: 0.33291933479763214
  batch 211 loss: 0.33275085292156276
  batch 212 loss: 0.33277314001659175
  batch 213 loss: 0.33259471019668757
  batch 214 loss: 0.3326026219352384
  batch 215 loss: 0.3325977678908858
  batch 216 loss: 0.3326972938246197
  batch 217 loss: 0.3325091174670628
  batch 218 loss: 0.33235810259613424
  batch 219 loss: 0.3320811086049363
  batch 220 loss: 0.33214175619862296
  batch 221 loss: 0.3320654886880072
  batch 222 loss: 0.332146472759075
  batch 223 loss: 0.33195112612215394
  batch 224 loss: 0.3318962811359337
  batch 225 loss: 0.3320000775655111
  batch 226 loss: 0.3319297378833315
  batch 227 loss: 0.33186195521627754
  batch 228 loss: 0.331782921103009
  batch 229 loss: 0.3317858798014545
  batch 230 loss: 0.33168753230053444
  batch 231 loss: 0.3317535274472588
  batch 232 loss: 0.331715722279302
  batch 233 loss: 0.3317099433600135
  batch 234 loss: 0.3317131752896513
  batch 235 loss: 0.3316497748202466
  batch 236 loss: 0.3316567115864511
  batch 237 loss: 0.33153293479847
  batch 238 loss: 0.3314253737946518
  batch 239 loss: 0.33144356129558517
  batch 240 loss: 0.3314247606943051
  batch 241 loss: 0.3313181997582131
  batch 242 loss: 0.33129197814740424
  batch 243 loss: 0.3313605363476914
  batch 244 loss: 0.33135537827601197
  batch 245 loss: 0.33141985365322657
  batch 246 loss: 0.33138522324038716
  batch 247 loss: 0.3314091306225008
  batch 248 loss: 0.33150130894876295
  batch 249 loss: 0.3314313253006303
  batch 250 loss: 0.33137577497959136
  batch 251 loss: 0.3312138619413414
  batch 252 loss: 0.3310843162120335
  batch 253 loss: 0.3309388860412266
  batch 254 loss: 0.33090538408343245
  batch 255 loss: 0.33073457163922926
  batch 256 loss: 0.33063395135104656
  batch 257 loss: 0.33068598801059945
  batch 258 loss: 0.3305357174817906
  batch 259 loss: 0.33036078012127673
  batch 260 loss: 0.33033294058763063
  batch 261 loss: 0.3303954395754584
  batch 262 loss: 0.3303696572325612
  batch 263 loss: 0.3302557940945426
  batch 264 loss: 0.3304299892801227
  batch 265 loss: 0.33041863733867427
  batch 266 loss: 0.3303548478542414
  batch 267 loss: 0.33015632160593955
  batch 268 loss: 0.3300905044176685
  batch 269 loss: 0.3300138571226907
  batch 270 loss: 0.3300024548062572
  batch 271 loss: 0.3299680248837629
  batch 272 loss: 0.32985892113955584
  batch 273 loss: 0.329796532025704
  batch 274 loss: 0.3296254701422949
  batch 275 loss: 0.32968049125237897
  batch 276 loss: 0.3296792798716089
  batch 277 loss: 0.32973377390458697
  batch 278 loss: 0.32960391987999565
  batch 279 loss: 0.32953330885124893
  batch 280 loss: 0.32935971011008536
  batch 281 loss: 0.329302302989247
  batch 282 loss: 0.3291456392893555
  batch 283 loss: 0.3290285149016566
  batch 284 loss: 0.3291158773739573
  batch 285 loss: 0.32893519443378116
  batch 286 loss: 0.32896092283975825
  batch 287 loss: 0.3289054071446329
  batch 288 loss: 0.3288251289890872
  batch 289 loss: 0.32880906538979815
  batch 290 loss: 0.32873162205876977
  batch 291 loss: 0.32868636176758204
  batch 292 loss: 0.32867776898488604
  batch 293 loss: 0.3287382339454755
  batch 294 loss: 0.3285795520154797
  batch 295 loss: 0.3286091271093336
  batch 296 loss: 0.3287425171080473
  batch 297 loss: 0.32868828777512316
  batch 298 loss: 0.3288026484067008
  batch 299 loss: 0.32889323350179156
  batch 300 loss: 0.3289153371254603
  batch 301 loss: 0.3289382098323087
  batch 302 loss: 0.3289976018351435
  batch 303 loss: 0.32894039793376484
  batch 304 loss: 0.3289505513875108
  batch 305 loss: 0.32893804663517434
  batch 306 loss: 0.3289775749047597
  batch 307 loss: 0.3289477477632827
  batch 308 loss: 0.328998302290966
  batch 309 loss: 0.3288920344272478
  batch 310 loss: 0.3289748782111752
  batch 311 loss: 0.32881553293807714
  batch 312 loss: 0.3287116650205392
  batch 313 loss: 0.3286434630997265
  batch 314 loss: 0.32862969226897903
  batch 315 loss: 0.3285975191328261
  batch 316 loss: 0.32874175085674356
  batch 317 loss: 0.32882216544557447
  batch 318 loss: 0.32865252163050307
  batch 319 loss: 0.3284506981836217
  batch 320 loss: 0.3285869018174708
  batch 321 loss: 0.328572593838255
  batch 322 loss: 0.32841929941443926
  batch 323 loss: 0.32841597335995537
  batch 324 loss: 0.3283929866221216
  batch 325 loss: 0.32843273208691526
  batch 326 loss: 0.3285904737886476
  batch 327 loss: 0.3285238063845795
  batch 328 loss: 0.3284364980531902
  batch 329 loss: 0.3284243586215567
  batch 330 loss: 0.3284328545584823
  batch 331 loss: 0.3283718525463
  batch 332 loss: 0.3284220237688846
  batch 333 loss: 0.32835775703281256
  batch 334 loss: 0.32858113989144744
  batch 335 loss: 0.3285937055722991
  batch 336 loss: 0.3286237959705648
  batch 337 loss: 0.32859103041162124
  batch 338 loss: 0.3284618629682699
  batch 339 loss: 0.32844954931278847
  batch 340 loss: 0.3283766828915652
  batch 341 loss: 0.32829724422647805
  batch 342 loss: 0.32828052551077125
  batch 343 loss: 0.3283830840281773
  batch 344 loss: 0.3284489298455937
  batch 345 loss: 0.32852923239486803
  batch 346 loss: 0.3284779079043107
  batch 347 loss: 0.32837164831436333
  batch 348 loss: 0.3283376069418315
  batch 349 loss: 0.32835578961153766
  batch 350 loss: 0.32839596314089636
  batch 351 loss: 0.328357217913
  batch 352 loss: 0.32827954180538654
  batch 353 loss: 0.32819239090252195
  batch 354 loss: 0.3281690734254438
  batch 355 loss: 0.3282247409014635
  batch 356 loss: 0.32825926992665516
  batch 357 loss: 0.3280453353869815
  batch 358 loss: 0.3279568741940919
  batch 359 loss: 0.3279851301466854
  batch 360 loss: 0.32792531069782044
  batch 361 loss: 0.3278820404055376
  batch 362 loss: 0.32783101103911744
  batch 363 loss: 0.3278465167550016
  batch 364 loss: 0.3277763958488192
  batch 365 loss: 0.32776501480847187
  batch 366 loss: 0.32774375802506517
  batch 367 loss: 0.32771874050678285
  batch 368 loss: 0.3276582829168309
  batch 369 loss: 0.3276844634434718
  batch 370 loss: 0.3277896867410557
  batch 371 loss: 0.32782954596123604
  batch 372 loss: 0.3278876197754696
  batch 373 loss: 0.32785356851749065
  batch 374 loss: 0.32781541729993363
  batch 375 loss: 0.32784658296902974
  batch 376 loss: 0.32796597512478526
  batch 377 loss: 0.32789918219062947
  batch 378 loss: 0.32779962265933
  batch 379 loss: 0.32777610421180725
  batch 380 loss: 0.32771766562210886
  batch 381 loss: 0.3276347231677198
  batch 382 loss: 0.32759842206358286
  batch 383 loss: 0.3274486348927799
  batch 384 loss: 0.3274124972522259
  batch 385 loss: 0.3273813407916527
  batch 386 loss: 0.32727470368610145
  batch 387 loss: 0.327326525397387
  batch 388 loss: 0.32723145056323905
  batch 389 loss: 0.3273083280321871
  batch 390 loss: 0.3273143415267651
  batch 391 loss: 0.3272913919995203
  batch 392 loss: 0.32736215246271116
  batch 393 loss: 0.3274254778414282
  batch 394 loss: 0.3274549860790901
  batch 395 loss: 0.3274019872840447
  batch 396 loss: 0.327418801233624
  batch 397 loss: 0.32745866864093903
  batch 398 loss: 0.32743474818653795
  batch 399 loss: 0.3273943847134
  batch 400 loss: 0.32748357571661474
  batch 401 loss: 0.3274088728606255
  batch 402 loss: 0.32736089165827525
  batch 403 loss: 0.3272603434457377
  batch 404 loss: 0.32716761197489086
  batch 405 loss: 0.32720001972751855
  batch 406 loss: 0.3270995428262673
  batch 407 loss: 0.32717479893557855
  batch 408 loss: 0.32706743822086093
  batch 409 loss: 0.3270037134877044
  batch 410 loss: 0.3268976925349817
  batch 411 loss: 0.32688553627680106
  batch 412 loss: 0.3270684213748256
  batch 413 loss: 0.3270453576169926
  batch 414 loss: 0.3270689838606378
  batch 415 loss: 0.3270324695541198
  batch 416 loss: 0.32707246084912467
  batch 417 loss: 0.32704996333705433
  batch 418 loss: 0.3270194884835248
  batch 419 loss: 0.3270278665501633
  batch 420 loss: 0.32700637167408353
  batch 421 loss: 0.32699733300050476
  batch 422 loss: 0.32691483206658567
  batch 423 loss: 0.3269325205197571
  batch 424 loss: 0.3270074978892533
  batch 425 loss: 0.32688857443192426
  batch 426 loss: 0.32680206770348436
  batch 427 loss: 0.32679374728884014
  batch 428 loss: 0.32678685225894516
  batch 429 loss: 0.32673037906626723
  batch 430 loss: 0.3267658960680629
  batch 431 loss: 0.32678658634493357
  batch 432 loss: 0.32676048970056903
  batch 433 loss: 0.3266297861692812
  batch 434 loss: 0.3265660113315978
  batch 435 loss: 0.32665098473943516
  batch 436 loss: 0.32666603462138305
  batch 437 loss: 0.3265976151558027
  batch 438 loss: 0.3265600013950644
  batch 439 loss: 0.3264760008433957
  batch 440 loss: 0.3265516308898276
  batch 441 loss: 0.3265662076386735
  batch 442 loss: 0.3264960684387932
  batch 443 loss: 0.326473671211346
  batch 444 loss: 0.3265309340513504
  batch 445 loss: 0.3265221319841535
  batch 446 loss: 0.32654709178503316
  batch 447 loss: 0.3265660726517372
  batch 448 loss: 0.32660413573362995
  batch 449 loss: 0.32654818759729176
  batch 450 loss: 0.32647972736093733
  batch 451 loss: 0.32640791520839785
  batch 452 loss: 0.326309170606917
  batch 453 loss: 0.3263151930525866
  batch 454 loss: 0.32630456235702865
  batch 455 loss: 0.3262454066957746
  batch 456 loss: 0.32622646285515083
  batch 457 loss: 0.32620496196872034
  batch 458 loss: 0.326110264099321
  batch 459 loss: 0.3260721198094437
  batch 460 loss: 0.32608778619247936
  batch 461 loss: 0.32609966507185567
  batch 462 loss: 0.3261375428536238
  batch 463 loss: 0.3261794474933369
  batch 464 loss: 0.32608422322262975
  batch 465 loss: 0.3260849838615746
  batch 466 loss: 0.3260289974222879
  batch 467 loss: 0.3261047825057706
  batch 468 loss: 0.3260302507979238
  batch 469 loss: 0.3259756944454047
  batch 470 loss: 0.32606429324505176
  batch 471 loss: 0.3260847297823353
  batch 472 loss: 0.3260999540403738
LOSS train 0.3260999540403738 valid 0.25758546590805054
LOSS train 0.3260999540403738 valid 0.25513918697834015
LOSS train 0.3260999540403738 valid 0.2553682525952657
LOSS train 0.3260999540403738 valid 0.244148850440979
LOSS train 0.3260999540403738 valid 0.24704366326332092
LOSS train 0.3260999540403738 valid 0.2531265417734782
LOSS train 0.3260999540403738 valid 0.24792709733758653
LOSS train 0.3260999540403738 valid 0.24606864899396896
LOSS train 0.3260999540403738 valid 0.24451207286781734
LOSS train 0.3260999540403738 valid 0.24344028681516647
LOSS train 0.3260999540403738 valid 0.2418163066560572
LOSS train 0.3260999540403738 valid 0.24523129810889563
LOSS train 0.3260999540403738 valid 0.24498906502356896
LOSS train 0.3260999540403738 valid 0.24292082020214625
LOSS train 0.3260999540403738 valid 0.2419079621632894
LOSS train 0.3260999540403738 valid 0.24460506066679955
LOSS train 0.3260999540403738 valid 0.2445753777728361
LOSS train 0.3260999540403738 valid 0.24519374635484484
LOSS train 0.3260999540403738 valid 0.24724133077420687
LOSS train 0.3260999540403738 valid 0.24701901897788048
LOSS train 0.3260999540403738 valid 0.24732541683174314
LOSS train 0.3260999540403738 valid 0.24683745137669824
LOSS train 0.3260999540403738 valid 0.24597386432730634
LOSS train 0.3260999540403738 valid 0.24666128431757292
LOSS train 0.3260999540403738 valid 0.24610656261444092
LOSS train 0.3260999540403738 valid 0.24508008933984315
LOSS train 0.3260999540403738 valid 0.24502029683854845
LOSS train 0.3260999540403738 valid 0.2448746556682246
LOSS train 0.3260999540403738 valid 0.2439043182751228
LOSS train 0.3260999540403738 valid 0.2434546341498693
LOSS train 0.3260999540403738 valid 0.24356123708909558
LOSS train 0.3260999540403738 valid 0.24413891043514013
LOSS train 0.3260999540403738 valid 0.24339016097964664
LOSS train 0.3260999540403738 valid 0.242975293275188
LOSS train 0.3260999540403738 valid 0.24355678941522327
LOSS train 0.3260999540403738 valid 0.24402298322982258
LOSS train 0.3260999540403738 valid 0.24383392325929693
LOSS train 0.3260999540403738 valid 0.24348315910289162
LOSS train 0.3260999540403738 valid 0.24409997768891165
LOSS train 0.3260999540403738 valid 0.2444394499063492
LOSS train 0.3260999540403738 valid 0.24412909004746414
LOSS train 0.3260999540403738 valid 0.24516518839768
LOSS train 0.3260999540403738 valid 0.2454417756823606
LOSS train 0.3260999540403738 valid 0.24483337050134485
LOSS train 0.3260999540403738 valid 0.24428440663549636
LOSS train 0.3260999540403738 valid 0.24364056470601456
LOSS train 0.3260999540403738 valid 0.24317519493559572
LOSS train 0.3260999540403738 valid 0.2442109544451038
LOSS train 0.3260999540403738 valid 0.24340034200220692
LOSS train 0.3260999540403738 valid 0.24399136781692504
LOSS train 0.3260999540403738 valid 0.24381726162106382
LOSS train 0.3260999540403738 valid 0.24347080462254012
LOSS train 0.3260999540403738 valid 0.24434054910012012
LOSS train 0.3260999540403738 valid 0.2442509967971731
LOSS train 0.3260999540403738 valid 0.2439566197720441
LOSS train 0.3260999540403738 valid 0.24407601755644595
LOSS train 0.3260999540403738 valid 0.2436449091162598
LOSS train 0.3260999540403738 valid 0.24443963760959692
LOSS train 0.3260999540403738 valid 0.24442704350261366
LOSS train 0.3260999540403738 valid 0.24432609627644222
LOSS train 0.3260999540403738 valid 0.2442415464119833
LOSS train 0.3260999540403738 valid 0.24372916260073263
LOSS train 0.3260999540403738 valid 0.24367139571242863
LOSS train 0.3260999540403738 valid 0.24383120820857584
LOSS train 0.3260999540403738 valid 0.2429434251326781
LOSS train 0.3260999540403738 valid 0.2430108433420008
LOSS train 0.3260999540403738 valid 0.2433888164918814
LOSS train 0.3260999540403738 valid 0.2427860594847623
LOSS train 0.3260999540403738 valid 0.24357718358869138
LOSS train 0.3260999540403738 valid 0.2437600974525724
LOSS train 0.3260999540403738 valid 0.24410145635336217
LOSS train 0.3260999540403738 valid 0.2442680067486233
LOSS train 0.3260999540403738 valid 0.24484977534372512
LOSS train 0.3260999540403738 valid 0.24475286417716258
LOSS train 0.3260999540403738 valid 0.24438007275263468
LOSS train 0.3260999540403738 valid 0.24448380187938087
LOSS train 0.3260999540403738 valid 0.24451041415140226
LOSS train 0.3260999540403738 valid 0.2443050843400833
LOSS train 0.3260999540403738 valid 0.2445066448631166
LOSS train 0.3260999540403738 valid 0.24424856901168823
LOSS train 0.3260999540403738 valid 0.2443621861345974
LOSS train 0.3260999540403738 valid 0.2444624802688273
LOSS train 0.3260999540403738 valid 0.24458769825567683
LOSS train 0.3260999540403738 valid 0.24446603336504527
LOSS train 0.3260999540403738 valid 0.24470093706074883
LOSS train 0.3260999540403738 valid 0.24463039359381034
LOSS train 0.3260999540403738 valid 0.24419073417954062
LOSS train 0.3260999540403738 valid 0.24409215067597953
LOSS train 0.3260999540403738 valid 0.24435314820723586
LOSS train 0.3260999540403738 valid 0.24444721589485804
LOSS train 0.3260999540403738 valid 0.24430835705537063
LOSS train 0.3260999540403738 valid 0.24442426536394202
LOSS train 0.3260999540403738 valid 0.24424043522086195
LOSS train 0.3260999540403738 valid 0.24432364328110473
LOSS train 0.3260999540403738 valid 0.24446770548820496
LOSS train 0.3260999540403738 valid 0.24473078083246946
LOSS train 0.3260999540403738 valid 0.2449310590925905
LOSS train 0.3260999540403738 valid 0.24530860477564287
LOSS train 0.3260999540403738 valid 0.24547333910007668
LOSS train 0.3260999540403738 valid 0.2455587938427925
LOSS train 0.3260999540403738 valid 0.24556725199269777
LOSS train 0.3260999540403738 valid 0.24598204081549363
LOSS train 0.3260999540403738 valid 0.24566855158620668
LOSS train 0.3260999540403738 valid 0.24543643843096036
LOSS train 0.3260999540403738 valid 0.24558977385361988
LOSS train 0.3260999540403738 valid 0.2454806829117379
LOSS train 0.3260999540403738 valid 0.24511899666808476
LOSS train 0.3260999540403738 valid 0.2450338368890462
LOSS train 0.3260999540403738 valid 0.244649944912403
LOSS train 0.3260999540403738 valid 0.24468616870316592
LOSS train 0.3260999540403738 valid 0.24503624519786318
LOSS train 0.3260999540403738 valid 0.24533388391137123
LOSS train 0.3260999540403738 valid 0.24520585125526495
LOSS train 0.3260999540403738 valid 0.2451625693785517
LOSS train 0.3260999540403738 valid 0.24566855948904287
LOSS train 0.3260999540403738 valid 0.24535749878348975
LOSS train 0.3260999540403738 valid 0.24573059698455355
LOSS train 0.3260999540403738 valid 0.24574203284110052
LOSS train 0.3260999540403738 valid 0.24550019192094563
LOSS train 0.3260999540403738 valid 0.2452248462786277
LOSS train 0.3260999540403738 valid 0.24535290601332327
LOSS train 0.3260999540403738 valid 0.24557963416713183
LOSS train 0.3260999540403738 valid 0.2455165732682236
LOSS train 0.3260999540403738 valid 0.24551983105559502
LOSS train 0.3260999540403738 valid 0.24548581981658935
LOSS train 0.3260999540403738 valid 0.24558368112359727
LOSS train 0.3260999540403738 valid 0.24553250421689252
LOSS train 0.3260999540403738 valid 0.2453907795716077
LOSS train 0.3260999540403738 valid 0.2452822489331859
LOSS train 0.3260999540403738 valid 0.24499797821044922
LOSS train 0.3260999540403738 valid 0.2449581654927203
LOSS train 0.3260999540403738 valid 0.24496456077604584
LOSS train 0.3260999540403738 valid 0.24493091435808884
LOSS train 0.3260999540403738 valid 0.24512626286318054
LOSS train 0.3260999540403738 valid 0.24521682538368084
LOSS train 0.3260999540403738 valid 0.24522117174723568
LOSS train 0.3260999540403738 valid 0.24529032115518612
LOSS train 0.3260999540403738 valid 0.24510761771513068
LOSS train 0.3260999540403738 valid 0.24492202259653764
LOSS train 0.3260999540403738 valid 0.2449457164321627
LOSS train 0.3260999540403738 valid 0.2450377400039781
LOSS train 0.3260999540403738 valid 0.24526126766708536
LOSS train 0.3260999540403738 valid 0.2453443710203771
LOSS train 0.3260999540403738 valid 0.24526442701203954
LOSS train 0.3260999540403738 valid 0.24510336271647748
LOSS train 0.3260999540403738 valid 0.2450880684877095
LOSS train 0.3260999540403738 valid 0.2450704395162816
LOSS train 0.3260999540403738 valid 0.24492583806450302
LOSS train 0.3260999540403738 valid 0.24498738838522227
LOSS train 0.3260999540403738 valid 0.244988115131855
LOSS train 0.3260999540403738 valid 0.244850447142361
LOSS train 0.3260999540403738 valid 0.244720174391803
LOSS train 0.3260999540403738 valid 0.24451230291058035
LOSS train 0.3260999540403738 valid 0.24462227668468053
LOSS train 0.3260999540403738 valid 0.24465969637517007
LOSS train 0.3260999540403738 valid 0.2448811679123304
LOSS train 0.3260999540403738 valid 0.2448431514440828
LOSS train 0.3260999540403738 valid 0.24492085140339936
LOSS train 0.3260999540403738 valid 0.24525339751498504
LOSS train 0.3260999540403738 valid 0.24531122213229536
LOSS train 0.3260999540403738 valid 0.2452651359464811
LOSS train 0.3260999540403738 valid 0.24538886629872853
LOSS train 0.3260999540403738 valid 0.24513602997262054
LOSS train 0.3260999540403738 valid 0.24519648521048268
LOSS train 0.3260999540403738 valid 0.24527098852576631
LOSS train 0.3260999540403738 valid 0.2450807778591133
LOSS train 0.3260999540403738 valid 0.24523662931904822
LOSS train 0.3260999540403738 valid 0.24507321621335687
LOSS train 0.3260999540403738 valid 0.24484179032272135
LOSS train 0.3260999540403738 valid 0.2449378500089926
LOSS train 0.3260999540403738 valid 0.24481284993085248
LOSS train 0.3260999540403738 valid 0.24481500693878464
LOSS train 0.3260999540403738 valid 0.24486231950321638
LOSS train 0.3260999540403738 valid 0.24513807935618806
LOSS train 0.3260999540403738 valid 0.24511709034442902
LOSS train 0.3260999540403738 valid 0.2450121133164926
LOSS train 0.3260999540403738 valid 0.24505998640410645
LOSS train 0.3260999540403738 valid 0.24515658298904977
LOSS train 0.3260999540403738 valid 0.24523444848353637
LOSS train 0.3260999540403738 valid 0.245248520705435
LOSS train 0.3260999540403738 valid 0.2451573819416004
LOSS train 0.3260999540403738 valid 0.24503319751430344
LOSS train 0.3260999540403738 valid 0.24517739535680885
LOSS train 0.3260999540403738 valid 0.24514402473426383
LOSS train 0.3260999540403738 valid 0.2452117621093183
LOSS train 0.3260999540403738 valid 0.24533596803103724
LOSS train 0.3260999540403738 valid 0.2451597451366843
LOSS train 0.3260999540403738 valid 0.24525259490659895
LOSS train 0.3260999540403738 valid 0.24505429427144387
LOSS train 0.3260999540403738 valid 0.2450822504727464
LOSS train 0.3260999540403738 valid 0.2452439658772883
LOSS train 0.3260999540403738 valid 0.24536446939843395
LOSS train 0.3260999540403738 valid 0.24525102834009754
LOSS train 0.3260999540403738 valid 0.2452337178833706
LOSS train 0.3260999540403738 valid 0.24521604371376526
LOSS train 0.3260999540403738 valid 0.2454635402833929
LOSS train 0.3260999540403738 valid 0.2454458818520386
LOSS train 0.3260999540403738 valid 0.24572661564205633
LOSS train 0.3260999540403738 valid 0.2456936153335188
LOSS train 0.3260999540403738 valid 0.24580959036946295
LOSS train 0.3260999540403738 valid 0.2456794677682184
LOSS train 0.3260999540403738 valid 0.2456712575979752
LOSS train 0.3260999540403738 valid 0.24580003556065957
LOSS train 0.3260999540403738 valid 0.2456869757642933
LOSS train 0.3260999540403738 valid 0.2456441713542473
LOSS train 0.3260999540403738 valid 0.24558029663794248
LOSS train 0.3260999540403738 valid 0.2455724597265179
LOSS train 0.3260999540403738 valid 0.24540058344316024
LOSS train 0.3260999540403738 valid 0.24549046821856613
LOSS train 0.3260999540403738 valid 0.2456017765970457
LOSS train 0.3260999540403738 valid 0.24576164753798624
LOSS train 0.3260999540403738 valid 0.2456276148276509
LOSS train 0.3260999540403738 valid 0.24554316202799478
LOSS train 0.3260999540403738 valid 0.24549165388969618
LOSS train 0.3260999540403738 valid 0.2452650314846704
LOSS train 0.3260999540403738 valid 0.24513459302209042
LOSS train 0.3260999540403738 valid 0.2449115886940934
LOSS train 0.3260999540403738 valid 0.24489650314827577
LOSS train 0.3260999540403738 valid 0.24489028724633397
LOSS train 0.3260999540403738 valid 0.24495189305056225
LOSS train 0.3260999540403738 valid 0.24492822425667518
LOSS train 0.3260999540403738 valid 0.24503891861385052
LOSS train 0.3260999540403738 valid 0.24507767957689516
LOSS train 0.3260999540403738 valid 0.24496389612821595
LOSS train 0.3260999540403738 valid 0.24483613888422648
LOSS train 0.3260999540403738 valid 0.24479148154501365
LOSS train 0.3260999540403738 valid 0.24483582790990233
LOSS train 0.3260999540403738 valid 0.24475479380864845
LOSS train 0.3260999540403738 valid 0.24456555679375427
LOSS train 0.3260999540403738 valid 0.24462952238062155
LOSS train 0.3260999540403738 valid 0.2445553064475328
LOSS train 0.3260999540403738 valid 0.24464742254851193
LOSS train 0.3260999540403738 valid 0.2448629631377085
LOSS train 0.3260999540403738 valid 0.2448376075197489
LOSS train 0.3260999540403738 valid 0.24489217482982797
LOSS train 0.3260999540403738 valid 0.24475990204235254
LOSS train 0.3260999540403738 valid 0.2448087849571735
LOSS train 0.3260999540403738 valid 0.2448730761138331
LOSS train 0.3260999540403738 valid 0.24484490531508393
LOSS train 0.3260999540403738 valid 0.24445121282090743
LOSS train 0.3260999540403738 valid 0.24425983632995873
LOSS train 0.3260999540403738 valid 0.2442385819702109
LOSS train 0.3260999540403738 valid 0.2442384930672469
LOSS train 0.3260999540403738 valid 0.24424101430617395
LOSS train 0.3260999540403738 valid 0.24437956670109107
LOSS train 0.3260999540403738 valid 0.24439661513741423
LOSS train 0.3260999540403738 valid 0.24441760187206962
LOSS train 0.3260999540403738 valid 0.24446635044390155
LOSS train 0.3260999540403738 valid 0.24430434124536782
LOSS train 0.3260999540403738 valid 0.24445508253574372
LOSS train 0.3260999540403738 valid 0.24451587221536977
LOSS train 0.3260999540403738 valid 0.2444823393154712
LOSS train 0.3260999540403738 valid 0.2443263402333844
LOSS train 0.3260999540403738 valid 0.24441272067272757
LOSS train 0.3260999540403738 valid 0.24434955844692155
LOSS train 0.3260999540403738 valid 0.24425831850385293
LOSS train 0.3260999540403738 valid 0.24426374488767483
LOSS train 0.3260999540403738 valid 0.24435357749462128
LOSS train 0.3260999540403738 valid 0.24432507283899316
LOSS train 0.3260999540403738 valid 0.24422938600182534
LOSS train 0.3260999540403738 valid 0.24424663728452733
LOSS train 0.3260999540403738 valid 0.24439197556890604
LOSS train 0.3260999540403738 valid 0.24432475275639798
LOSS train 0.3260999540403738 valid 0.24442137207723025
LOSS train 0.3260999540403738 valid 0.24442379575855328
LOSS train 0.3260999540403738 valid 0.24442786633744276
LOSS train 0.3260999540403738 valid 0.24435612693261566
LOSS train 0.3260999540403738 valid 0.24432357178250355
LOSS train 0.3260999540403738 valid 0.24436490741804187
LOSS train 0.3260999540403738 valid 0.24425569507810804
LOSS train 0.3260999540403738 valid 0.24426032201390424
LOSS train 0.3260999540403738 valid 0.24415513517006354
LOSS train 0.3260999540403738 valid 0.24399816400402194
LOSS train 0.3260999540403738 valid 0.2439582375605611
LOSS train 0.3260999540403738 valid 0.2440759361332113
LOSS train 0.3260999540403738 valid 0.24416351658494576
LOSS train 0.3260999540403738 valid 0.24420274331466385
LOSS train 0.3260999540403738 valid 0.24421367226101512
LOSS train 0.3260999540403738 valid 0.2442133417586699
LOSS train 0.3260999540403738 valid 0.24412809253803322
LOSS train 0.3260999540403738 valid 0.24400410612919152
LOSS train 0.3260999540403738 valid 0.24402188441009387
LOSS train 0.3260999540403738 valid 0.24408784129594324
LOSS train 0.3260999540403738 valid 0.24398485287814073
LOSS train 0.3260999540403738 valid 0.244017865051303
LOSS train 0.3260999540403738 valid 0.24406180014977089
LOSS train 0.3260999540403738 valid 0.24407293561858998
LOSS train 0.3260999540403738 valid 0.24402672274866039
LOSS train 0.3260999540403738 valid 0.2439769882850581
LOSS train 0.3260999540403738 valid 0.24389647101533823
LOSS train 0.3260999540403738 valid 0.2438547097632975
LOSS train 0.3260999540403738 valid 0.2438106860914459
LOSS train 0.3260999540403738 valid 0.24382663652961978
LOSS train 0.3260999540403738 valid 0.2438287412126859
LOSS train 0.3260999540403738 valid 0.24385992525997807
LOSS train 0.3260999540403738 valid 0.24380317572000865
LOSS train 0.3260999540403738 valid 0.24373291678701586
LOSS train 0.3260999540403738 valid 0.24377475128877882
LOSS train 0.3260999540403738 valid 0.24376030470615245
LOSS train 0.3260999540403738 valid 0.24372600093483926
LOSS train 0.3260999540403738 valid 0.24365883045814363
LOSS train 0.3260999540403738 valid 0.24365946092945062
LOSS train 0.3260999540403738 valid 0.24359369868099098
LOSS train 0.3260999540403738 valid 0.2436014140318883
LOSS train 0.3260999540403738 valid 0.24359401034527137
LOSS train 0.3260999540403738 valid 0.2437634633647071
LOSS train 0.3260999540403738 valid 0.24377282619088014
LOSS train 0.3260999540403738 valid 0.24384109492038752
LOSS train 0.3260999540403738 valid 0.24384163228439282
LOSS train 0.3260999540403738 valid 0.2438493816121932
LOSS train 0.3260999540403738 valid 0.24388282657436236
LOSS train 0.3260999540403738 valid 0.2438297173820245
LOSS train 0.3260999540403738 valid 0.24387866139602357
LOSS train 0.3260999540403738 valid 0.2437845692038536
LOSS train 0.3260999540403738 valid 0.24373346832063464
LOSS train 0.3260999540403738 valid 0.24375242513569095
LOSS train 0.3260999540403738 valid 0.24375351506640858
LOSS train 0.3260999540403738 valid 0.24375117841944005
LOSS train 0.3260999540403738 valid 0.24375690337632516
LOSS train 0.3260999540403738 valid 0.24370557614602148
LOSS train 0.3260999540403738 valid 0.2437460492611674
LOSS train 0.3260999540403738 valid 0.24374648294648768
LOSS train 0.3260999540403738 valid 0.24362149078041406
LOSS train 0.3260999540403738 valid 0.24356084298572422
LOSS train 0.3260999540403738 valid 0.24342588887764857
LOSS train 0.3260999540403738 valid 0.24350374164573985
LOSS train 0.3260999540403738 valid 0.24345768131429632
LOSS train 0.3260999540403738 valid 0.24348441361472373
LOSS train 0.3260999540403738 valid 0.24342598048205796
LOSS train 0.3260999540403738 valid 0.24342581968415866
LOSS train 0.3260999540403738 valid 0.24332675342293306
LOSS train 0.3260999540403738 valid 0.2433033800986876
LOSS train 0.3260999540403738 valid 0.24343709616331724
LOSS train 0.3260999540403738 valid 0.24338615154791735
LOSS train 0.3260999540403738 valid 0.2432848287162496
LOSS train 0.3260999540403738 valid 0.2431794899471459
LOSS train 0.3260999540403738 valid 0.24332284693017558
LOSS train 0.3260999540403738 valid 0.24326337790171776
LOSS train 0.3260999540403738 valid 0.24326953041342508
LOSS train 0.3260999540403738 valid 0.24335706667865023
LOSS train 0.3260999540403738 valid 0.24332034828201418
LOSS train 0.3260999540403738 valid 0.243263770613754
LOSS train 0.3260999540403738 valid 0.24315700091356438
LOSS train 0.3260999540403738 valid 0.2432063291239184
LOSS train 0.3260999540403738 valid 0.24328527968862781
LOSS train 0.3260999540403738 valid 0.24323029846297523
LOSS train 0.3260999540403738 valid 0.24317155756936637
LOSS train 0.3260999540403738 valid 0.2432347316851561
LOSS train 0.3260999540403738 valid 0.24315095993884997
LOSS train 0.3260999540403738 valid 0.24315097114869527
LOSS train 0.3260999540403738 valid 0.2432181526537974
LOSS train 0.3260999540403738 valid 0.24325971758331766
LOSS train 0.3260999540403738 valid 0.24334626997327669
LOSS train 0.3260999540403738 valid 0.24330683877575868
LOSS train 0.3260999540403738 valid 0.24322477360846292
LOSS train 0.3260999540403738 valid 0.24325713389710094
LOSS train 0.3260999540403738 valid 0.2433054356180987
LOSS train 0.3260999540403738 valid 0.24328299074865586
LOSS train 0.3260999540403738 valid 0.2432554189043125
LOSS train 0.3260999540403738 valid 0.2433450796537929
LOSS train 0.3260999540403738 valid 0.2433005481380505
LOSS train 0.3260999540403738 valid 0.24336833790850243
LOSS train 0.3260999540403738 valid 0.2434209819174995
LOSS train 0.3260999540403738 valid 0.2433294186798426
LOSS train 0.3260999540403738 valid 0.24342613044666916
LOSS train 0.3260999540403738 valid 0.24337593360188228
LOSS train 0.3260999540403738 valid 0.2433612442471351
LOSS train 0.3260999540403738 valid 0.24331357767400535
LOSS train 0.3260999540403738 valid 0.24334104904314366
EPOCH 4:
  batch 1 loss: 0.3362290561199188
  batch 2 loss: 0.35418206453323364
  batch 3 loss: 0.3389710485935211
  batch 4 loss: 0.33932098746299744
  batch 5 loss: 0.3456187069416046
  batch 6 loss: 0.3461676885684331
  batch 7 loss: 0.3462536760738918
  batch 8 loss: 0.343094639480114
  batch 9 loss: 0.34364533093240524
  batch 10 loss: 0.3386963397264481
  batch 11 loss: 0.33820066126910125
  batch 12 loss: 0.33738574385643005
  batch 13 loss: 0.3353416919708252
  batch 14 loss: 0.33362275787762236
  batch 15 loss: 0.3335835715134939
  batch 16 loss: 0.33377149142324924
  batch 17 loss: 0.3340730141190922
  batch 18 loss: 0.33134014076656765
  batch 19 loss: 0.33140963315963745
  batch 20 loss: 0.3286334127187729
  batch 21 loss: 0.3333351526941572
  batch 22 loss: 0.33380213379859924
  batch 23 loss: 0.33186367542847345
  batch 24 loss: 0.33230452239513397
  batch 25 loss: 0.33331010103225706
  batch 26 loss: 0.3313906708588967
  batch 27 loss: 0.3340521929440675
  batch 28 loss: 0.33546861580439974
  batch 29 loss: 0.33483677485893515
  batch 30 loss: 0.33531428972880045
  batch 31 loss: 0.3354084001433465
  batch 32 loss: 0.33679589070379734
  batch 33 loss: 0.33797828446735034
  batch 34 loss: 0.3373875854646458
  batch 35 loss: 0.3385830989905766
  batch 36 loss: 0.33795103513532215
  batch 37 loss: 0.3366612518155897
  batch 38 loss: 0.336843142383977
  batch 39 loss: 0.3370415094571236
  batch 40 loss: 0.33659874498844145
  batch 41 loss: 0.3361558347213559
  batch 42 loss: 0.33542247400397346
  batch 43 loss: 0.335159313540126
  batch 44 loss: 0.33489775454456155
  batch 45 loss: 0.33531845145755346
  batch 46 loss: 0.3338844005180442
  batch 47 loss: 0.33345058243325415
  batch 48 loss: 0.33352036587893963
  batch 49 loss: 0.3340475480167233
  batch 50 loss: 0.3340228968858719
  batch 51 loss: 0.33387696918319254
  batch 52 loss: 0.33343609078572345
  batch 53 loss: 0.33293363733111686
  batch 54 loss: 0.3325953174520422
  batch 55 loss: 0.33256401907313954
  batch 56 loss: 0.33297186131988254
  batch 57 loss: 0.33203488536048353
  batch 58 loss: 0.331466612630877
  batch 59 loss: 0.3313866333436158
  batch 60 loss: 0.33117032299439114
  batch 61 loss: 0.3317544308842206
  batch 62 loss: 0.33210833082276003
  batch 63 loss: 0.33115504847632515
  batch 64 loss: 0.33081931062042713
  batch 65 loss: 0.3299532596881573
  batch 66 loss: 0.32963309414458997
  batch 67 loss: 0.3291762617986594
  batch 68 loss: 0.32892238962299686
  batch 69 loss: 0.3287696946358335
  batch 70 loss: 0.3281156505857195
  batch 71 loss: 0.3283258789861706
  batch 72 loss: 0.328396026045084
  batch 73 loss: 0.3281961299785196
  batch 74 loss: 0.3277434787234744
  batch 75 loss: 0.32766364216804506
  batch 76 loss: 0.32820974015875864
  batch 77 loss: 0.3276403263791815
  batch 78 loss: 0.3276998568803836
  batch 79 loss: 0.32754824433145646
  batch 80 loss: 0.32721056416630745
  batch 81 loss: 0.32703530567663686
  batch 82 loss: 0.32710584271244886
  batch 83 loss: 0.32694941112794074
  batch 84 loss: 0.3263615242072514
  batch 85 loss: 0.32647127754548017
  batch 86 loss: 0.32627420231353405
  batch 87 loss: 0.32603752476045456
  batch 88 loss: 0.3259347159076821
  batch 89 loss: 0.32520130845937834
  batch 90 loss: 0.32552444537480674
  batch 91 loss: 0.325122560773577
  batch 92 loss: 0.32499072765526565
  batch 93 loss: 0.3244271195063027
  batch 94 loss: 0.32438243481707063
  batch 95 loss: 0.32441186559827706
  batch 96 loss: 0.3242854770893852
  batch 97 loss: 0.3244841304636493
  batch 98 loss: 0.3241848291791215
  batch 99 loss: 0.32392020628909873
  batch 100 loss: 0.3240088307857513
  batch 101 loss: 0.3237698721413565
  batch 102 loss: 0.32401640128855613
  batch 103 loss: 0.32393224059956743
  batch 104 loss: 0.3243442275203191
  batch 105 loss: 0.3241928248178391
  batch 106 loss: 0.3243724557588685
  batch 107 loss: 0.32426112472453966
  batch 108 loss: 0.32399619839809557
  batch 109 loss: 0.324098326197458
  batch 110 loss: 0.32407518137585034
  batch 111 loss: 0.3240362781662125
  batch 112 loss: 0.323822661702122
  batch 113 loss: 0.32343561723168973
  batch 114 loss: 0.3231708347274546
  batch 115 loss: 0.3228235636068427
  batch 116 loss: 0.3224890697105178
  batch 117 loss: 0.3222472993736593
  batch 118 loss: 0.32243019797034184
  batch 119 loss: 0.32270449650387806
  batch 120 loss: 0.32242267628510796
  batch 121 loss: 0.322580999086711
  batch 122 loss: 0.3226134332477069
  batch 123 loss: 0.3224910487973593
  batch 124 loss: 0.3223736228481416
  batch 125 loss: 0.3219353885650635
  batch 126 loss: 0.3219881634863596
  batch 127 loss: 0.32208058942021345
  batch 128 loss: 0.32214582012966275
  batch 129 loss: 0.32220564193503803
  batch 130 loss: 0.3222407240134019
  batch 131 loss: 0.3222074517766938
  batch 132 loss: 0.3223002318179969
  batch 133 loss: 0.3221213470276137
  batch 134 loss: 0.32183028646369477
  batch 135 loss: 0.32176831033494735
  batch 136 loss: 0.3219759700929417
  batch 137 loss: 0.3221429101307027
  batch 138 loss: 0.32216043878292694
  batch 139 loss: 0.3223078212292074
  batch 140 loss: 0.3224455716354506
  batch 141 loss: 0.32233533601388864
  batch 142 loss: 0.3221085325513088
  batch 143 loss: 0.3221186109772929
  batch 144 loss: 0.3217142151875628
  batch 145 loss: 0.32190220602627456
  batch 146 loss: 0.32191456752280667
  batch 147 loss: 0.3218706854346658
  batch 148 loss: 0.3218446440390638
  batch 149 loss: 0.3213368821264113
  batch 150 loss: 0.3216472390294075
  batch 151 loss: 0.3214340988571281
  batch 152 loss: 0.32126588246932153
  batch 153 loss: 0.3215835942746767
  batch 154 loss: 0.32159906945058275
  batch 155 loss: 0.3216947854526581
  batch 156 loss: 0.3215216913093359
  batch 157 loss: 0.32141386181305925
  batch 158 loss: 0.3215174864553198
  batch 159 loss: 0.3217886308641554
  batch 160 loss: 0.3215917921625078
  batch 161 loss: 0.32116298155384776
  batch 162 loss: 0.32107273102910433
  batch 163 loss: 0.3210601505874856
  batch 164 loss: 0.32134130896955004
  batch 165 loss: 0.32152895918398194
  batch 166 loss: 0.32179934369871416
  batch 167 loss: 0.321662676280844
  batch 168 loss: 0.3214410776715903
  batch 169 loss: 0.3216068828423348
  batch 170 loss: 0.3216910941635861
  batch 171 loss: 0.3219140465670859
  batch 172 loss: 0.3217724734094254
  batch 173 loss: 0.32184450948514
  batch 174 loss: 0.32194800203901597
  batch 175 loss: 0.322022254381861
  batch 176 loss: 0.3222202639180151
  batch 177 loss: 0.3224628319342931
  batch 178 loss: 0.3222258851936694
  batch 179 loss: 0.3223024640835863
  batch 180 loss: 0.3222027247978581
  batch 181 loss: 0.3220092855929011
  batch 182 loss: 0.32188080095655314
  batch 183 loss: 0.32184106021956665
  batch 184 loss: 0.3216114320346843
  batch 185 loss: 0.32146272876778165
  batch 186 loss: 0.32154627552916926
  batch 187 loss: 0.321825264211007
  batch 188 loss: 0.3217576917340147
  batch 189 loss: 0.32172569466961753
  batch 190 loss: 0.3218177259752625
  batch 191 loss: 0.32185592553066333
  batch 192 loss: 0.3216064326309909
  batch 193 loss: 0.3217205833740185
  batch 194 loss: 0.3217065362739809
  batch 195 loss: 0.3218896550245774
  batch 196 loss: 0.32186285398748454
  batch 197 loss: 0.321826063966388
  batch 198 loss: 0.3218404713152635
  batch 199 loss: 0.3218435960349126
  batch 200 loss: 0.32179198034107687
  batch 201 loss: 0.3215514991562165
  batch 202 loss: 0.3213789021172146
  batch 203 loss: 0.3212789999587195
  batch 204 loss: 0.3212808958601718
  batch 205 loss: 0.3210469192847973
  batch 206 loss: 0.32104916472747486
  batch 207 loss: 0.32094888187548964
  batch 208 loss: 0.3206716583182032
  batch 209 loss: 0.32084125914927303
  batch 210 loss: 0.3207689033377738
  batch 211 loss: 0.32065993342636884
  batch 212 loss: 0.32069898333470775
  batch 213 loss: 0.3205925960076247
  batch 214 loss: 0.3205631901031343
  batch 215 loss: 0.3206109884866448
  batch 216 loss: 0.32065119352881555
  batch 217 loss: 0.320476809473631
  batch 218 loss: 0.32032511779747974
  batch 219 loss: 0.32006686211448826
  batch 220 loss: 0.320149715854363
  batch 221 loss: 0.3200753551667632
  batch 222 loss: 0.32009161961776716
  batch 223 loss: 0.31991755381026077
  batch 224 loss: 0.31983668536746074
  batch 225 loss: 0.31998559137185417
  batch 226 loss: 0.3199377874225642
  batch 227 loss: 0.3198697538759215
  batch 228 loss: 0.3198325999342559
  batch 229 loss: 0.3198284018117788
  batch 230 loss: 0.3196825241912966
  batch 231 loss: 0.3196853855342576
  batch 232 loss: 0.31965564647368316
  batch 233 loss: 0.3195902528041422
  batch 234 loss: 0.3195480390364288
  batch 235 loss: 0.3194618549118651
  batch 236 loss: 0.3194494273324134
  batch 237 loss: 0.3193084287618283
  batch 238 loss: 0.31918580423132714
  batch 239 loss: 0.3191019454386444
  batch 240 loss: 0.3190680063640078
  batch 241 loss: 0.3188964120456292
  batch 242 loss: 0.31882149024196893
  batch 243 loss: 0.318927158360128
  batch 244 loss: 0.31887711773886057
  batch 245 loss: 0.3188719562730011
  batch 246 loss: 0.3188261412144677
  batch 247 loss: 0.3188163089728066
  batch 248 loss: 0.3189012536238278
  batch 249 loss: 0.31885076353109504
  batch 250 loss: 0.3187465841174126
  batch 251 loss: 0.3185843917359394
  batch 252 loss: 0.3184880048391365
  batch 253 loss: 0.318373316773784
  batch 254 loss: 0.3183871178528455
  batch 255 loss: 0.3182415434542824
  batch 256 loss: 0.3181716318358667
  batch 257 loss: 0.31821937372016534
  batch 258 loss: 0.3180714342598767
  batch 259 loss: 0.317887837189505
  batch 260 loss: 0.31784770952967495
  batch 261 loss: 0.3178382988062855
  batch 262 loss: 0.31784886070562685
  batch 263 loss: 0.31774908355207043
  batch 264 loss: 0.3178403908669045
  batch 265 loss: 0.3178092085527924
  batch 266 loss: 0.31770771545799154
  batch 267 loss: 0.3174844501951661
  batch 268 loss: 0.3173428606519948
  batch 269 loss: 0.3172945118194176
  batch 270 loss: 0.3172656833573624
  batch 271 loss: 0.3172368142767586
  batch 272 loss: 0.3171382880057482
  batch 273 loss: 0.3170709047234539
  batch 274 loss: 0.31690142129677057
  batch 275 loss: 0.3169313670830293
  batch 276 loss: 0.31690650211944094
  batch 277 loss: 0.3168508436813251
  batch 278 loss: 0.31669986349858825
  batch 279 loss: 0.3166366947487691
  batch 280 loss: 0.3164933175380741
  batch 281 loss: 0.31644316897909835
  batch 282 loss: 0.3163039726040042
  batch 283 loss: 0.31622803932575794
  batch 284 loss: 0.31632340683693616
  batch 285 loss: 0.31616031783714627
  batch 286 loss: 0.31618395890597695
  batch 287 loss: 0.31609847741882974
  batch 288 loss: 0.3160379725094471
  batch 289 loss: 0.31597033271946295
  batch 290 loss: 0.31589895625566616
  batch 291 loss: 0.3158510155796595
  batch 292 loss: 0.31583437614449084
  batch 293 loss: 0.31590072769964106
  batch 294 loss: 0.31570073075237726
  batch 295 loss: 0.315695111680839
  batch 296 loss: 0.3159109695839721
  batch 297 loss: 0.315910030545209
  batch 298 loss: 0.3160175721617353
  batch 299 loss: 0.31609832089282197
  batch 300 loss: 0.31614571834603944
  batch 301 loss: 0.3162011149218708
  batch 302 loss: 0.3163093284561934
  batch 303 loss: 0.31626213172284684
  batch 304 loss: 0.3162675458251646
  batch 305 loss: 0.31624771996599726
  batch 306 loss: 0.31630797299489477
  batch 307 loss: 0.31629021863789825
  batch 308 loss: 0.31633304393330175
  batch 309 loss: 0.31623425091161694
  batch 310 loss: 0.31635500303199215
  batch 311 loss: 0.31620027345284774
  batch 312 loss: 0.31608845422474235
  batch 313 loss: 0.31605115299598097
  batch 314 loss: 0.3160361946103679
  batch 315 loss: 0.3159568888327432
  batch 316 loss: 0.3160343377178983
  batch 317 loss: 0.3160892871182048
  batch 318 loss: 0.3158681404384427
  batch 319 loss: 0.3156692952291345
  batch 320 loss: 0.3157811886165291
  batch 321 loss: 0.3157839135867413
  batch 322 loss: 0.31561552853502844
  batch 323 loss: 0.3156117990197781
  batch 324 loss: 0.31560851546165386
  batch 325 loss: 0.31565194849784556
  batch 326 loss: 0.3158145710094575
  batch 327 loss: 0.3157519633890292
  batch 328 loss: 0.31566694419740177
  batch 329 loss: 0.31568434911477167
  batch 330 loss: 0.31571313905896564
  batch 331 loss: 0.31568985866816024
  batch 332 loss: 0.3157436232340623
  batch 333 loss: 0.31568653335621405
  batch 334 loss: 0.31590014013523116
  batch 335 loss: 0.3159322338762568
  batch 336 loss: 0.3159456049491252
  batch 337 loss: 0.31592035342217906
  batch 338 loss: 0.31586193892906406
  batch 339 loss: 0.3158355338998952
  batch 340 loss: 0.31575780120842595
  batch 341 loss: 0.3156668749635171
  batch 342 loss: 0.31556169971910836
  batch 343 loss: 0.31564986571104814
  batch 344 loss: 0.3157176862709051
  batch 345 loss: 0.3157473694155182
  batch 346 loss: 0.31569107337666386
  batch 347 loss: 0.31556956470012665
  batch 348 loss: 0.3155863992869854
  batch 349 loss: 0.31559661316837484
  batch 350 loss: 0.3156464176092829
  batch 351 loss: 0.3156657030864319
  batch 352 loss: 0.31559716897423973
  batch 353 loss: 0.315533693341628
  batch 354 loss: 0.31547269515566906
  batch 355 loss: 0.3154957258785275
  batch 356 loss: 0.3155237229938587
  batch 357 loss: 0.3153003942983157
  batch 358 loss: 0.3152560582956788
  batch 359 loss: 0.3153141905919423
  batch 360 loss: 0.3152215289986796
  batch 361 loss: 0.3151817824949518
  batch 362 loss: 0.31510249978106325
  batch 363 loss: 0.3151175977852062
  batch 364 loss: 0.3150843968512593
  batch 365 loss: 0.31509674840593993
  batch 366 loss: 0.31510126285377094
  batch 367 loss: 0.31514443522090807
  batch 368 loss: 0.31506600243079924
  batch 369 loss: 0.3150824176344445
  batch 370 loss: 0.31530104621842103
  batch 371 loss: 0.31533066285749006
  batch 372 loss: 0.31542717989894653
  batch 373 loss: 0.3155164365873899
  batch 374 loss: 0.31554189081338635
  batch 375 loss: 0.31557127805550894
  batch 376 loss: 0.31570422700903517
  batch 377 loss: 0.31574805586818677
  batch 378 loss: 0.3156652982351641
  batch 379 loss: 0.31564024566503185
  batch 380 loss: 0.3155565619076553
  batch 381 loss: 0.31548026325352235
  batch 382 loss: 0.3153932471783997
  batch 383 loss: 0.31530260646778985
  batch 384 loss: 0.3152520724494631
  batch 385 loss: 0.3152112315614502
  batch 386 loss: 0.3151525413048082
  batch 387 loss: 0.3151577318746606
  batch 388 loss: 0.31506747963655857
  batch 389 loss: 0.3151251653396072
  batch 390 loss: 0.31508710762629144
  batch 391 loss: 0.3150469142838817
  batch 392 loss: 0.3151270254153986
  batch 393 loss: 0.31520178629696827
  batch 394 loss: 0.31522019718050354
  batch 395 loss: 0.31520218513434445
  batch 396 loss: 0.3152246165531452
  batch 397 loss: 0.3152667008133919
  batch 398 loss: 0.3152380687881954
  batch 399 loss: 0.3152125621573967
  batch 400 loss: 0.31533097188919784
  batch 401 loss: 0.31520322897933667
  batch 402 loss: 0.3151875611635583
  batch 403 loss: 0.3150743273810181
  batch 404 loss: 0.3149523596731153
  batch 405 loss: 0.314977244278531
  batch 406 loss: 0.3148723673717729
  batch 407 loss: 0.3149132636680064
  batch 408 loss: 0.3147821648082897
  batch 409 loss: 0.31468326002723723
  batch 410 loss: 0.31461319876153293
  batch 411 loss: 0.3146600509719547
  batch 412 loss: 0.3148231594235573
  batch 413 loss: 0.31482714410988527
  batch 414 loss: 0.3148208497731006
  batch 415 loss: 0.3147514306278114
  batch 416 loss: 0.31476611697759765
  batch 417 loss: 0.3147367227206127
  batch 418 loss: 0.31473332276327187
  batch 419 loss: 0.3147448114606247
  batch 420 loss: 0.314697299720276
  batch 421 loss: 0.3146913885857883
  batch 422 loss: 0.31462380625484115
  batch 423 loss: 0.3146198356334763
  batch 424 loss: 0.31464637327447254
  batch 425 loss: 0.31453907044494855
  batch 426 loss: 0.3144944167822739
  batch 427 loss: 0.31451969867707413
  batch 428 loss: 0.31449114966476077
  batch 429 loss: 0.31440999252951785
  batch 430 loss: 0.3144184841319572
  batch 431 loss: 0.31443711634716687
  batch 432 loss: 0.314424097503501
  batch 433 loss: 0.31432390223336826
  batch 434 loss: 0.31428852234430577
  batch 435 loss: 0.3143490713560718
  batch 436 loss: 0.31438906835990216
  batch 437 loss: 0.31431729266381647
  batch 438 loss: 0.3142944706017024
  batch 439 loss: 0.31419357626883476
  batch 440 loss: 0.3142893417653712
  batch 441 loss: 0.31431350432961436
  batch 442 loss: 0.31424579020943577
  batch 443 loss: 0.31421563059562485
  batch 444 loss: 0.3142743790270509
  batch 445 loss: 0.3142107522554612
  batch 446 loss: 0.31427439556127174
  batch 447 loss: 0.31430129113480015
  batch 448 loss: 0.31435191847517024
  batch 449 loss: 0.31428914960216575
  batch 450 loss: 0.3142115803890758
  batch 451 loss: 0.31413753832919633
  batch 452 loss: 0.3140438356956022
  batch 453 loss: 0.3140411373992629
  batch 454 loss: 0.31402289299581543
  batch 455 loss: 0.3139753519506245
  batch 456 loss: 0.3139444615430476
  batch 457 loss: 0.31391276158310916
  batch 458 loss: 0.31382234488902655
  batch 459 loss: 0.31376953689231335
  batch 460 loss: 0.3137731956075067
  batch 461 loss: 0.31377252441812753
  batch 462 loss: 0.3138051202351397
  batch 463 loss: 0.31383062004399354
  batch 464 loss: 0.3137446132263747
  batch 465 loss: 0.3136931502370424
  batch 466 loss: 0.3136380004972347
  batch 467 loss: 0.3137582292584842
  batch 468 loss: 0.3136972238938523
  batch 469 loss: 0.31367066607419364
  batch 470 loss: 0.31379230849920436
  batch 471 loss: 0.3137980773254058
  batch 472 loss: 0.3137461355871568
LOSS train 0.3137461355871568 valid 0.23708955943584442
LOSS train 0.3137461355871568 valid 0.2341037541627884
LOSS train 0.3137461355871568 valid 0.23653865853945413
LOSS train 0.3137461355871568 valid 0.22583645954728127
LOSS train 0.3137461355871568 valid 0.22743869125843047
LOSS train 0.3137461355871568 valid 0.23349724958340326
LOSS train 0.3137461355871568 valid 0.22873069133077348
LOSS train 0.3137461355871568 valid 0.22778617218136787
LOSS train 0.3137461355871568 valid 0.2261052711142434
LOSS train 0.3137461355871568 valid 0.2252957969903946
LOSS train 0.3137461355871568 valid 0.2236493080854416
LOSS train 0.3137461355871568 valid 0.22678417588273683
LOSS train 0.3137461355871568 valid 0.22672054744683778
LOSS train 0.3137461355871568 valid 0.2243845143488475
LOSS train 0.3137461355871568 valid 0.2237734854221344
LOSS train 0.3137461355871568 valid 0.22675848938524723
LOSS train 0.3137461355871568 valid 0.2268731637912638
LOSS train 0.3137461355871568 valid 0.2268483050995403
LOSS train 0.3137461355871568 valid 0.2287021334234037
LOSS train 0.3137461355871568 valid 0.22842549607157708
LOSS train 0.3137461355871568 valid 0.2289472527447201
LOSS train 0.3137461355871568 valid 0.22852797941728073
LOSS train 0.3137461355871568 valid 0.22752539295217264
LOSS train 0.3137461355871568 valid 0.228376692160964
LOSS train 0.3137461355871568 valid 0.2278166300058365
LOSS train 0.3137461355871568 valid 0.22686964617325708
LOSS train 0.3137461355871568 valid 0.22690211291666385
LOSS train 0.3137461355871568 valid 0.22687146067619324
LOSS train 0.3137461355871568 valid 0.22582010051299786
LOSS train 0.3137461355871568 valid 0.22546906570593517
LOSS train 0.3137461355871568 valid 0.22567523679425638
LOSS train 0.3137461355871568 valid 0.22617581766098738
LOSS train 0.3137461355871568 valid 0.22550609391747098
LOSS train 0.3137461355871568 valid 0.22517580013064778
LOSS train 0.3137461355871568 valid 0.22587884962558746
LOSS train 0.3137461355871568 valid 0.22636052096883455
LOSS train 0.3137461355871568 valid 0.2264325981204574
LOSS train 0.3137461355871568 valid 0.22614821438726626
LOSS train 0.3137461355871568 valid 0.2269283598050093
LOSS train 0.3137461355871568 valid 0.22706591039896012
LOSS train 0.3137461355871568 valid 0.22683464899295713
LOSS train 0.3137461355871568 valid 0.2278336314927964
LOSS train 0.3137461355871568 valid 0.22818393277567486
LOSS train 0.3137461355871568 valid 0.22754888575185428
LOSS train 0.3137461355871568 valid 0.2270873698923323
LOSS train 0.3137461355871568 valid 0.22648001915734747
LOSS train 0.3137461355871568 valid 0.22607190716773906
LOSS train 0.3137461355871568 valid 0.22725154304256043
LOSS train 0.3137461355871568 valid 0.22657782599634055
LOSS train 0.3137461355871568 valid 0.22706919342279433
LOSS train 0.3137461355871568 valid 0.22695547166992636
LOSS train 0.3137461355871568 valid 0.22659435868263245
LOSS train 0.3137461355871568 valid 0.22765054399112486
LOSS train 0.3137461355871568 valid 0.2275794228469884
LOSS train 0.3137461355871568 valid 0.2273096900094639
LOSS train 0.3137461355871568 valid 0.22732885394777572
LOSS train 0.3137461355871568 valid 0.22676948915448106
LOSS train 0.3137461355871568 valid 0.22743945543108315
LOSS train 0.3137461355871568 valid 0.227290545479726
LOSS train 0.3137461355871568 valid 0.22714744607607523
LOSS train 0.3137461355871568 valid 0.22715864293887966
LOSS train 0.3137461355871568 valid 0.2268482386585205
LOSS train 0.3137461355871568 valid 0.2268457157271249
LOSS train 0.3137461355871568 valid 0.22691106772981584
LOSS train 0.3137461355871568 valid 0.22598859048806705
LOSS train 0.3137461355871568 valid 0.2259502625375083
LOSS train 0.3137461355871568 valid 0.2262972691165867
LOSS train 0.3137461355871568 valid 0.22575779259204865
LOSS train 0.3137461355871568 valid 0.22638196616932965
LOSS train 0.3137461355871568 valid 0.22659854952778136
LOSS train 0.3137461355871568 valid 0.2269083704327194
LOSS train 0.3137461355871568 valid 0.22711719427671698
LOSS train 0.3137461355871568 valid 0.2276404955207485
LOSS train 0.3137461355871568 valid 0.22750251256936305
LOSS train 0.3137461355871568 valid 0.2270777157942454
LOSS train 0.3137461355871568 valid 0.22715426903021962
LOSS train 0.3137461355871568 valid 0.22706639321593494
LOSS train 0.3137461355871568 valid 0.2269042881253438
LOSS train 0.3137461355871568 valid 0.22718025989170315
LOSS train 0.3137461355871568 valid 0.22689549811184406
LOSS train 0.3137461355871568 valid 0.22701451697467287
LOSS train 0.3137461355871568 valid 0.22707089545523249
LOSS train 0.3137461355871568 valid 0.22716327262930122
LOSS train 0.3137461355871568 valid 0.2270573643701417
LOSS train 0.3137461355871568 valid 0.22724750673069674
LOSS train 0.3137461355871568 valid 0.2271411720403405
LOSS train 0.3137461355871568 valid 0.2266748259807455
LOSS train 0.3137461355871568 valid 0.22671030834317207
LOSS train 0.3137461355871568 valid 0.22693915313549257
LOSS train 0.3137461355871568 valid 0.22703674170706006
LOSS train 0.3137461355871568 valid 0.22689483912436517
LOSS train 0.3137461355871568 valid 0.22695152348150377
LOSS train 0.3137461355871568 valid 0.22675418565350194
LOSS train 0.3137461355871568 valid 0.22689803214149273
LOSS train 0.3137461355871568 valid 0.22711236869034016
LOSS train 0.3137461355871568 valid 0.2273349699874719
LOSS train 0.3137461355871568 valid 0.22751211474851235
LOSS train 0.3137461355871568 valid 0.22777913510799408
LOSS train 0.3137461355871568 valid 0.22792670567228338
LOSS train 0.3137461355871568 valid 0.22796715632081033
LOSS train 0.3137461355871568 valid 0.2280503234060684
LOSS train 0.3137461355871568 valid 0.22844697710345774
LOSS train 0.3137461355871568 valid 0.228192968420612
LOSS train 0.3137461355871568 valid 0.22798361170750397
LOSS train 0.3137461355871568 valid 0.22814063273725055
LOSS train 0.3137461355871568 valid 0.22805858150405703
LOSS train 0.3137461355871568 valid 0.22771876994694504
LOSS train 0.3137461355871568 valid 0.22770129806465572
LOSS train 0.3137461355871568 valid 0.2273322540685671
LOSS train 0.3137461355871568 valid 0.22739432589574293
LOSS train 0.3137461355871568 valid 0.22773756717776392
LOSS train 0.3137461355871568 valid 0.22811962105333805
LOSS train 0.3137461355871568 valid 0.22802137326350255
LOSS train 0.3137461355871568 valid 0.2280010507817854
LOSS train 0.3137461355871568 valid 0.22851717316586037
LOSS train 0.3137461355871568 valid 0.2282401736954163
LOSS train 0.3137461355871568 valid 0.22868580950631034
LOSS train 0.3137461355871568 valid 0.2286569248821776
LOSS train 0.3137461355871568 valid 0.22840512888271267
LOSS train 0.3137461355871568 valid 0.22813181852300962
LOSS train 0.3137461355871568 valid 0.22819882385001694
LOSS train 0.3137461355871568 valid 0.2284401525728038
LOSS train 0.3137461355871568 valid 0.22841931952209008
LOSS train 0.3137461355871568 valid 0.22844965984263726
LOSS train 0.3137461355871568 valid 0.22837658512592315
LOSS train 0.3137461355871568 valid 0.228509455446213
LOSS train 0.3137461355871568 valid 0.22845785472336716
LOSS train 0.3137461355871568 valid 0.22839927696622908
LOSS train 0.3137461355871568 valid 0.22829196931317794
LOSS train 0.3137461355871568 valid 0.2279878868506505
LOSS train 0.3137461355871568 valid 0.2279032286796861
LOSS train 0.3137461355871568 valid 0.22786264527927746
LOSS train 0.3137461355871568 valid 0.22783944783802318
LOSS train 0.3137461355871568 valid 0.22806652568614305
LOSS train 0.3137461355871568 valid 0.22815253811853903
LOSS train 0.3137461355871568 valid 0.2281273643979255
LOSS train 0.3137461355871568 valid 0.22823541540734088
LOSS train 0.3137461355871568 valid 0.2280422723379688
LOSS train 0.3137461355871568 valid 0.2278878744557607
LOSS train 0.3137461355871568 valid 0.22785036585160665
LOSS train 0.3137461355871568 valid 0.22789639128860853
LOSS train 0.3137461355871568 valid 0.22813122541132108
LOSS train 0.3137461355871568 valid 0.22814880446954208
LOSS train 0.3137461355871568 valid 0.22810893640336063
LOSS train 0.3137461355871568 valid 0.22796491620869472
LOSS train 0.3137461355871568 valid 0.22790112030016232
LOSS train 0.3137461355871568 valid 0.2278705867780309
LOSS train 0.3137461355871568 valid 0.22774784589135968
LOSS train 0.3137461355871568 valid 0.2277903889849682
LOSS train 0.3137461355871568 valid 0.22783123860756557
LOSS train 0.3137461355871568 valid 0.22767022982338406
LOSS train 0.3137461355871568 valid 0.22757464529652344
LOSS train 0.3137461355871568 valid 0.2273818794029211
LOSS train 0.3137461355871568 valid 0.22750414675706393
LOSS train 0.3137461355871568 valid 0.22754148514040054
LOSS train 0.3137461355871568 valid 0.22775390935249817
LOSS train 0.3137461355871568 valid 0.2276884320245427
LOSS train 0.3137461355871568 valid 0.22775779249547404
LOSS train 0.3137461355871568 valid 0.22807626529309735
LOSS train 0.3137461355871568 valid 0.2281715903431177
LOSS train 0.3137461355871568 valid 0.22817392508435694
LOSS train 0.3137461355871568 valid 0.22825341975247418
LOSS train 0.3137461355871568 valid 0.22804304975673464
LOSS train 0.3137461355871568 valid 0.22807535738116358
LOSS train 0.3137461355871568 valid 0.22811333257140534
LOSS train 0.3137461355871568 valid 0.22795985315937595
LOSS train 0.3137461355871568 valid 0.22813918847523765
LOSS train 0.3137461355871568 valid 0.22797960486440433
LOSS train 0.3137461355871568 valid 0.22777754359344055
LOSS train 0.3137461355871568 valid 0.2278414075865465
LOSS train 0.3137461355871568 valid 0.22771017715247752
LOSS train 0.3137461355871568 valid 0.22777156445176103
LOSS train 0.3137461355871568 valid 0.22786181527755164
LOSS train 0.3137461355871568 valid 0.22812863447885404
LOSS train 0.3137461355871568 valid 0.22809730887413024
LOSS train 0.3137461355871568 valid 0.22799246301027862
LOSS train 0.3137461355871568 valid 0.2280538171361395
LOSS train 0.3137461355871568 valid 0.22808410913756724
LOSS train 0.3137461355871568 valid 0.22816907843397982
LOSS train 0.3137461355871568 valid 0.2281782575779491
LOSS train 0.3137461355871568 valid 0.2280881677378607
LOSS train 0.3137461355871568 valid 0.2279442378452846
LOSS train 0.3137461355871568 valid 0.22808487509769168
LOSS train 0.3137461355871568 valid 0.22806519942115183
LOSS train 0.3137461355871568 valid 0.22811833595907366
LOSS train 0.3137461355871568 valid 0.22820421652768247
LOSS train 0.3137461355871568 valid 0.22804501509283953
LOSS train 0.3137461355871568 valid 0.2281355857849121
LOSS train 0.3137461355871568 valid 0.22792360612324306
LOSS train 0.3137461355871568 valid 0.22794020779823002
LOSS train 0.3137461355871568 valid 0.2280618146608013
LOSS train 0.3137461355871568 valid 0.228134904600059
LOSS train 0.3137461355871568 valid 0.22799889689282432
LOSS train 0.3137461355871568 valid 0.22796164952295342
LOSS train 0.3137461355871568 valid 0.22790964268721067
LOSS train 0.3137461355871568 valid 0.2280928873438008
LOSS train 0.3137461355871568 valid 0.2280685823732221
LOSS train 0.3137461355871568 valid 0.2283510792285505
LOSS train 0.3137461355871568 valid 0.22831689270597008
LOSS train 0.3137461355871568 valid 0.22841400891542435
LOSS train 0.3137461355871568 valid 0.22828433688600264
LOSS train 0.3137461355871568 valid 0.22825774107829178
LOSS train 0.3137461355871568 valid 0.2283949696371708
LOSS train 0.3137461355871568 valid 0.2282986108432798
LOSS train 0.3137461355871568 valid 0.2282606138688762
LOSS train 0.3137461355871568 valid 0.2281601630078936
LOSS train 0.3137461355871568 valid 0.228126246644103
LOSS train 0.3137461355871568 valid 0.2279789186297701
LOSS train 0.3137461355871568 valid 0.22801295029774807
LOSS train 0.3137461355871568 valid 0.2280991624508585
LOSS train 0.3137461355871568 valid 0.2282523832756196
LOSS train 0.3137461355871568 valid 0.2281086990996352
LOSS train 0.3137461355871568 valid 0.22803062347459122
LOSS train 0.3137461355871568 valid 0.227999536512054
LOSS train 0.3137461355871568 valid 0.2278148499339126
LOSS train 0.3137461355871568 valid 0.22768681896505533
LOSS train 0.3137461355871568 valid 0.22745895365141502
LOSS train 0.3137461355871568 valid 0.22745614095565375
LOSS train 0.3137461355871568 valid 0.22745089607151675
LOSS train 0.3137461355871568 valid 0.22751336910507897
LOSS train 0.3137461355871568 valid 0.2275037430395368
LOSS train 0.3137461355871568 valid 0.22763312983888764
LOSS train 0.3137461355871568 valid 0.22768379066289807
LOSS train 0.3137461355871568 valid 0.2275759516549962
LOSS train 0.3137461355871568 valid 0.22743418441878424
LOSS train 0.3137461355871568 valid 0.22737195995529141
LOSS train 0.3137461355871568 valid 0.2274211460010596
LOSS train 0.3137461355871568 valid 0.22735533529990598
LOSS train 0.3137461355871568 valid 0.22719811482200455
LOSS train 0.3137461355871568 valid 0.22724272751289865
LOSS train 0.3137461355871568 valid 0.2271712338511562
LOSS train 0.3137461355871568 valid 0.22723666655606237
LOSS train 0.3137461355871568 valid 0.22748146202942843
LOSS train 0.3137461355871568 valid 0.22745648975300992
LOSS train 0.3137461355871568 valid 0.22748008364058556
LOSS train 0.3137461355871568 valid 0.2273710426890244
LOSS train 0.3137461355871568 valid 0.2274045776968767
LOSS train 0.3137461355871568 valid 0.22743520616483287
LOSS train 0.3137461355871568 valid 0.22743274252783804
LOSS train 0.3137461355871568 valid 0.22702261966963608
LOSS train 0.3137461355871568 valid 0.22685098487311875
LOSS train 0.3137461355871568 valid 0.22681126583459948
LOSS train 0.3137461355871568 valid 0.2268210960391127
LOSS train 0.3137461355871568 valid 0.22680241315335523
LOSS train 0.3137461355871568 valid 0.22691966836549798
LOSS train 0.3137461355871568 valid 0.2269277959698584
LOSS train 0.3137461355871568 valid 0.22697045252873346
LOSS train 0.3137461355871568 valid 0.22702342740470363
LOSS train 0.3137461355871568 valid 0.22686939940873876
LOSS train 0.3137461355871568 valid 0.2269987450838089
LOSS train 0.3137461355871568 valid 0.22703909167493008
LOSS train 0.3137461355871568 valid 0.22702687860481321
LOSS train 0.3137461355871568 valid 0.2268901143973995
LOSS train 0.3137461355871568 valid 0.22701080430915035
LOSS train 0.3137461355871568 valid 0.22690487544910581
LOSS train 0.3137461355871568 valid 0.22680251294514164
LOSS train 0.3137461355871568 valid 0.22680261052768055
LOSS train 0.3137461355871568 valid 0.2268757901565973
LOSS train 0.3137461355871568 valid 0.2268494392338867
LOSS train 0.3137461355871568 valid 0.226735959144739
LOSS train 0.3137461355871568 valid 0.22671623499457408
LOSS train 0.3137461355871568 valid 0.2268657818550372
LOSS train 0.3137461355871568 valid 0.22677415065439027
LOSS train 0.3137461355871568 valid 0.2268464078167171
LOSS train 0.3137461355871568 valid 0.22683781964598962
LOSS train 0.3137461355871568 valid 0.22687942882005432
LOSS train 0.3137461355871568 valid 0.2268050948667169
LOSS train 0.3137461355871568 valid 0.22681139681988688
LOSS train 0.3137461355871568 valid 0.22686179253691635
LOSS train 0.3137461355871568 valid 0.22675586607721115
LOSS train 0.3137461355871568 valid 0.22675805756086792
LOSS train 0.3137461355871568 valid 0.22667171833488872
LOSS train 0.3137461355871568 valid 0.2265097066050484
LOSS train 0.3137461355871568 valid 0.22646400022463206
LOSS train 0.3137461355871568 valid 0.22656347811222077
LOSS train 0.3137461355871568 valid 0.22663581560271373
LOSS train 0.3137461355871568 valid 0.22665600177398226
LOSS train 0.3137461355871568 valid 0.22666800022125244
LOSS train 0.3137461355871568 valid 0.22663671894526397
LOSS train 0.3137461355871568 valid 0.2265563138361488
LOSS train 0.3137461355871568 valid 0.2264580185197002
LOSS train 0.3137461355871568 valid 0.22646980732679367
LOSS train 0.3137461355871568 valid 0.22652852382550392
LOSS train 0.3137461355871568 valid 0.2264539015964723
LOSS train 0.3137461355871568 valid 0.22650116587940014
LOSS train 0.3137461355871568 valid 0.22651511930919194
LOSS train 0.3137461355871568 valid 0.2265046677107595
LOSS train 0.3137461355871568 valid 0.2264640788651175
LOSS train 0.3137461355871568 valid 0.22642039200838873
LOSS train 0.3137461355871568 valid 0.22633776762362184
LOSS train 0.3137461355871568 valid 0.22630802001740105
LOSS train 0.3137461355871568 valid 0.22627560641259364
LOSS train 0.3137461355871568 valid 0.2262716508969512
LOSS train 0.3137461355871568 valid 0.22628993779218115
LOSS train 0.3137461355871568 valid 0.22629335659035182
LOSS train 0.3137461355871568 valid 0.22624478257588437
LOSS train 0.3137461355871568 valid 0.22615670600924828
LOSS train 0.3137461355871568 valid 0.22621366026617537
LOSS train 0.3137461355871568 valid 0.22619209924469824
LOSS train 0.3137461355871568 valid 0.22615480894843737
LOSS train 0.3137461355871568 valid 0.22607887192223952
LOSS train 0.3137461355871568 valid 0.2260550582251012
LOSS train 0.3137461355871568 valid 0.22599924697537627
LOSS train 0.3137461355871568 valid 0.22601356234793601
LOSS train 0.3137461355871568 valid 0.22600079965396005
LOSS train 0.3137461355871568 valid 0.22616138815685036
LOSS train 0.3137461355871568 valid 0.22618083786886756
LOSS train 0.3137461355871568 valid 0.22626398078032903
LOSS train 0.3137461355871568 valid 0.22630855315711507
LOSS train 0.3137461355871568 valid 0.22631804592186405
LOSS train 0.3137461355871568 valid 0.22633699076160357
LOSS train 0.3137461355871568 valid 0.22628483949945524
LOSS train 0.3137461355871568 valid 0.22632158735689645
LOSS train 0.3137461355871568 valid 0.2262302398396905
LOSS train 0.3137461355871568 valid 0.22620616777548713
LOSS train 0.3137461355871568 valid 0.2262176574691187
LOSS train 0.3137461355871568 valid 0.22618549150247302
LOSS train 0.3137461355871568 valid 0.22620089445848884
LOSS train 0.3137461355871568 valid 0.22618576449847147
LOSS train 0.3137461355871568 valid 0.22611612752079963
LOSS train 0.3137461355871568 valid 0.22616055347837763
LOSS train 0.3137461355871568 valid 0.22614569545532606
LOSS train 0.3137461355871568 valid 0.22601276331641726
LOSS train 0.3137461355871568 valid 0.22595773669488636
LOSS train 0.3137461355871568 valid 0.2258221188416848
LOSS train 0.3137461355871568 valid 0.22590812789333378
LOSS train 0.3137461355871568 valid 0.2258573102932822
LOSS train 0.3137461355871568 valid 0.22586213815503003
LOSS train 0.3137461355871568 valid 0.22579529482905264
LOSS train 0.3137461355871568 valid 0.2258015602827072
LOSS train 0.3137461355871568 valid 0.22571234418546327
LOSS train 0.3137461355871568 valid 0.22568991717444845
LOSS train 0.3137461355871568 valid 0.22582260642323765
LOSS train 0.3137461355871568 valid 0.22577608416894238
LOSS train 0.3137461355871568 valid 0.22569704567318533
LOSS train 0.3137461355871568 valid 0.22560995826054187
LOSS train 0.3137461355871568 valid 0.22572413590966064
LOSS train 0.3137461355871568 valid 0.22567607356599095
LOSS train 0.3137461355871568 valid 0.22567441786988296
LOSS train 0.3137461355871568 valid 0.22573754861950873
LOSS train 0.3137461355871568 valid 0.22569330796992673
LOSS train 0.3137461355871568 valid 0.22564442669264753
LOSS train 0.3137461355871568 valid 0.22554648139733963
LOSS train 0.3137461355871568 valid 0.225600769570054
LOSS train 0.3137461355871568 valid 0.22566507199536198
LOSS train 0.3137461355871568 valid 0.22562067154687265
LOSS train 0.3137461355871568 valid 0.22554102385421995
LOSS train 0.3137461355871568 valid 0.22562590583987618
LOSS train 0.3137461355871568 valid 0.22554466798688075
LOSS train 0.3137461355871568 valid 0.2255488971727235
LOSS train 0.3137461355871568 valid 0.2256411635383242
LOSS train 0.3137461355871568 valid 0.225683881520209
LOSS train 0.3137461355871568 valid 0.2257570477872665
LOSS train 0.3137461355871568 valid 0.22572447612123975
LOSS train 0.3137461355871568 valid 0.22564462868260665
LOSS train 0.3137461355871568 valid 0.22567895180388783
LOSS train 0.3137461355871568 valid 0.2257270732430183
LOSS train 0.3137461355871568 valid 0.22572043771184355
LOSS train 0.3137461355871568 valid 0.22570021305243618
LOSS train 0.3137461355871568 valid 0.2257999796834257
LOSS train 0.3137461355871568 valid 0.22574206163018035
LOSS train 0.3137461355871568 valid 0.225804238852875
LOSS train 0.3137461355871568 valid 0.22583549057156588
LOSS train 0.3137461355871568 valid 0.2257501719580902
LOSS train 0.3137461355871568 valid 0.22584105044195096
LOSS train 0.3137461355871568 valid 0.22579282188350386
LOSS train 0.3137461355871568 valid 0.225783815785065
LOSS train 0.3137461355871568 valid 0.22572736996833398
LOSS train 0.3137461355871568 valid 0.2257317055854694
EPOCH 5:
  batch 1 loss: 0.3582793176174164
  batch 2 loss: 0.36487817764282227
  batch 3 loss: 0.3405391573905945
  batch 4 loss: 0.3398260325193405
  batch 5 loss: 0.3362869143486023
  batch 6 loss: 0.33135852217674255
  batch 7 loss: 0.3295865740094866
  batch 8 loss: 0.32518270611763
  batch 9 loss: 0.3248638477590349
  batch 10 loss: 0.3188718020915985
  batch 11 loss: 0.31800244884057477
  batch 12 loss: 0.31634831925233203
  batch 13 loss: 0.3144131532082191
  batch 14 loss: 0.3134402973311288
  batch 15 loss: 0.31524594823519386
  batch 16 loss: 0.3143869377672672
  batch 17 loss: 0.3142794861513026
  batch 18 loss: 0.3126504404677285
  batch 19 loss: 0.3119030469342282
  batch 20 loss: 0.30883820801973344
  batch 21 loss: 0.31392285795438857
  batch 22 loss: 0.3142191862518137
  batch 23 loss: 0.3126676406549371
  batch 24 loss: 0.3129236549139023
  batch 25 loss: 0.3141749310493469
  batch 26 loss: 0.31293714161102587
  batch 27 loss: 0.3148573327947546
  batch 28 loss: 0.31661722915513174
  batch 29 loss: 0.31600323627735005
  batch 30 loss: 0.31658049325148263
  batch 31 loss: 0.3171282256803205
  batch 32 loss: 0.3185281679034233
  batch 33 loss: 0.3199775589234901
  batch 34 loss: 0.3196464627981186
  batch 35 loss: 0.32055940883500234
  batch 36 loss: 0.31997595561875236
  batch 37 loss: 0.3190295366016594
  batch 38 loss: 0.31907493189761515
  batch 39 loss: 0.319186575901814
  batch 40 loss: 0.31875283271074295
  batch 41 loss: 0.31827970030831126
  batch 42 loss: 0.31747302129155114
  batch 43 loss: 0.31690810447515444
  batch 44 loss: 0.31684972616759216
  batch 45 loss: 0.3169179545508491
  batch 46 loss: 0.3151823683925297
  batch 47 loss: 0.3146903172452399
  batch 48 loss: 0.31486740459998447
  batch 49 loss: 0.3156516448575623
  batch 50 loss: 0.3156122291088104
  batch 51 loss: 0.31544074240852804
  batch 52 loss: 0.3152169075149756
  batch 53 loss: 0.314532773674659
  batch 54 loss: 0.31424411248277734
  batch 55 loss: 0.3144198889082128
  batch 56 loss: 0.31501600784914835
  batch 57 loss: 0.3143543465095654
  batch 58 loss: 0.3137349748405917
  batch 59 loss: 0.3135998451103598
  batch 60 loss: 0.31340671082337695
  batch 61 loss: 0.31435546718659946
  batch 62 loss: 0.31458978595272186
  batch 63 loss: 0.3139168194362095
  batch 64 loss: 0.31384766940027475
  batch 65 loss: 0.3130353047297551
  batch 66 loss: 0.31287840911836334
  batch 67 loss: 0.31244628464997704
  batch 68 loss: 0.3122518018764608
  batch 69 loss: 0.3118887392507083
  batch 70 loss: 0.3113049941403525
  batch 71 loss: 0.3116106768728982
  batch 72 loss: 0.3116391934454441
  batch 73 loss: 0.31136762728429823
  batch 74 loss: 0.31091818414829875
  batch 75 loss: 0.3108991499741872
  batch 76 loss: 0.3109673351833695
  batch 77 loss: 0.3105362781456539
  batch 78 loss: 0.3106384926881546
  batch 79 loss: 0.31057528679883933
  batch 80 loss: 0.310222165286541
  batch 81 loss: 0.3100530453670172
  batch 82 loss: 0.31013816377011744
  batch 83 loss: 0.30978219624025277
  batch 84 loss: 0.30921117145390736
  batch 85 loss: 0.3096231611335979
  batch 86 loss: 0.30958553836789243
  batch 87 loss: 0.30925845072187225
  batch 88 loss: 0.30921299586241896
  batch 89 loss: 0.3084976189257054
  batch 90 loss: 0.3088616932431857
  batch 91 loss: 0.3084432854429706
  batch 92 loss: 0.3082236642746822
  batch 93 loss: 0.30778766463520707
  batch 94 loss: 0.307827557813614
  batch 95 loss: 0.30784734283622944
  batch 96 loss: 0.3078744270217915
  batch 97 loss: 0.30830441323137775
  batch 98 loss: 0.3080931362144801
  batch 99 loss: 0.3079130651071818
  batch 100 loss: 0.3079606686532497
  batch 101 loss: 0.307719032805745
  batch 102 loss: 0.30808479251230464
  batch 103 loss: 0.3079449978557605
  batch 104 loss: 0.3082659122749017
  batch 105 loss: 0.30827496520110537
  batch 106 loss: 0.30849110419457815
  batch 107 loss: 0.3084253291381854
  batch 108 loss: 0.3082046957203635
  batch 109 loss: 0.3083753477816188
  batch 110 loss: 0.30837773504582316
  batch 111 loss: 0.3084536102172491
  batch 112 loss: 0.30833338427224327
  batch 113 loss: 0.30801480309625645
  batch 114 loss: 0.307803813433438
  batch 115 loss: 0.3073497045299281
  batch 116 loss: 0.30698588220723744
  batch 117 loss: 0.3068410090656362
  batch 118 loss: 0.30696773895267715
  batch 119 loss: 0.30722186332490264
  batch 120 loss: 0.3069111251582702
  batch 121 loss: 0.30705435647944773
  batch 122 loss: 0.3071371086063932
  batch 123 loss: 0.30696657554405493
  batch 124 loss: 0.306827456600243
  batch 125 loss: 0.3062954843044281
  batch 126 loss: 0.3062388662781034
  batch 127 loss: 0.3064347564235447
  batch 128 loss: 0.30664726672694087
  batch 129 loss: 0.30671500351077824
  batch 130 loss: 0.30681679065410905
  batch 131 loss: 0.3066492485636063
  batch 132 loss: 0.30669020229216776
  batch 133 loss: 0.3065067785126822
  batch 134 loss: 0.30625875360930144
  batch 135 loss: 0.30620219972398544
  batch 136 loss: 0.30649469924323697
  batch 137 loss: 0.30688172752839804
  batch 138 loss: 0.30695433417956036
  batch 139 loss: 0.3070870397759856
  batch 140 loss: 0.3073751070669719
  batch 141 loss: 0.30730816658507004
  batch 142 loss: 0.30703566863503257
  batch 143 loss: 0.30710027142838164
  batch 144 loss: 0.306858508537213
  batch 145 loss: 0.3068992271505553
  batch 146 loss: 0.30698677019713677
  batch 147 loss: 0.30696501273687193
  batch 148 loss: 0.3068462805570783
  batch 149 loss: 0.30648455183778034
  batch 150 loss: 0.3068085048596064
  batch 151 loss: 0.30646068055108683
  batch 152 loss: 0.3063157892933017
  batch 153 loss: 0.306852766890931
  batch 154 loss: 0.3069691203244321
  batch 155 loss: 0.3070766618174891
  batch 156 loss: 0.30696611182811934
  batch 157 loss: 0.30698629416477907
  batch 158 loss: 0.3071594938069959
  batch 159 loss: 0.3074031620280548
  batch 160 loss: 0.30728106331080196
  batch 161 loss: 0.306923173414254
  batch 162 loss: 0.306841211186515
  batch 163 loss: 0.306769116890211
  batch 164 loss: 0.3070649372004881
  batch 165 loss: 0.30730188579270334
  batch 166 loss: 0.3076246247952243
  batch 167 loss: 0.3075877044372216
  batch 168 loss: 0.3073605905686106
  batch 169 loss: 0.30749017723213284
  batch 170 loss: 0.3075937495512121
  batch 171 loss: 0.3078880784107231
  batch 172 loss: 0.3077792842374292
  batch 173 loss: 0.30788263437375857
  batch 174 loss: 0.30800327744292116
  batch 175 loss: 0.30810895834650315
  batch 176 loss: 0.3083357601003213
  batch 177 loss: 0.30858739183447453
  batch 178 loss: 0.30839860908101113
  batch 179 loss: 0.3084851358522916
  batch 180 loss: 0.308423464331362
  batch 181 loss: 0.30828840281423286
  batch 182 loss: 0.3081784977035208
  batch 183 loss: 0.30807246259652854
  batch 184 loss: 0.30791200422074483
  batch 185 loss: 0.30774133285960636
  batch 186 loss: 0.30779997011025745
  batch 187 loss: 0.3079818177988185
  batch 188 loss: 0.30795108923252595
  batch 189 loss: 0.3079095774542087
  batch 190 loss: 0.3078605265993821
  batch 191 loss: 0.30792146453058533
  batch 192 loss: 0.307653709469984
  batch 193 loss: 0.30766639761974157
  batch 194 loss: 0.3075641670177892
  batch 195 loss: 0.3077003402587695
  batch 196 loss: 0.3076477576883472
  batch 197 loss: 0.3076117245981536
  batch 198 loss: 0.30762183410350724
  batch 199 loss: 0.3075531093618978
  batch 200 loss: 0.30760050371289255
  batch 201 loss: 0.30750319554438044
  batch 202 loss: 0.307415448055409
  batch 203 loss: 0.3073529655416611
  batch 204 loss: 0.3073738077107598
  batch 205 loss: 0.30712654546993534
  batch 206 loss: 0.3071537962526951
  batch 207 loss: 0.30701654760733893
  batch 208 loss: 0.30674082921961177
  batch 209 loss: 0.3068991503789664
  batch 210 loss: 0.30681452020293193
  batch 211 loss: 0.30675574805216765
  batch 212 loss: 0.306766198842593
  batch 213 loss: 0.3067057232761607
  batch 214 loss: 0.30671014845649774
  batch 215 loss: 0.3068409946768783
  batch 216 loss: 0.3069068159632109
  batch 217 loss: 0.3067604071945639
  batch 218 loss: 0.3066137704417246
  batch 219 loss: 0.30636567061078057
  batch 220 loss: 0.30642501095479185
  batch 221 loss: 0.3063558205894755
  batch 222 loss: 0.3063918519261721
  batch 223 loss: 0.30626521470033535
  batch 224 loss: 0.30621500772290994
  batch 225 loss: 0.3062590652704239
  batch 226 loss: 0.3062323633152827
  batch 227 loss: 0.30609602280912945
  batch 228 loss: 0.3059976615273116
  batch 229 loss: 0.30599399353479195
  batch 230 loss: 0.3058272425895152
  batch 231 loss: 0.3058215213931484
  batch 232 loss: 0.3057475005886678
  batch 233 loss: 0.30580417300242724
  batch 234 loss: 0.30580298201395917
  batch 235 loss: 0.305685803674637
  batch 236 loss: 0.30565092857863946
  batch 237 loss: 0.305476044112117
  batch 238 loss: 0.3053981604821542
  batch 239 loss: 0.3053534549025811
  batch 240 loss: 0.30534711324920255
  batch 241 loss: 0.3051838613273692
  batch 242 loss: 0.30512640415883263
  batch 243 loss: 0.30518900017434186
  batch 244 loss: 0.3051167435211236
  batch 245 loss: 0.3051277622276423
  batch 246 loss: 0.3050768777001195
  batch 247 loss: 0.30512393238814733
  batch 248 loss: 0.3052095605601226
  batch 249 loss: 0.30513954982460745
  batch 250 loss: 0.30506905490159986
  batch 251 loss: 0.3048871857117847
  batch 252 loss: 0.30479457542773275
  batch 253 loss: 0.30465756615866785
  batch 254 loss: 0.3046146825425268
  batch 255 loss: 0.30454369239947376
  batch 256 loss: 0.30442692624637857
  batch 257 loss: 0.3045843326512014
  batch 258 loss: 0.3044515680088553
  batch 259 loss: 0.30420744160197416
  batch 260 loss: 0.30419499123325716
  batch 261 loss: 0.3041694071215231
  batch 262 loss: 0.30415031524332425
  batch 263 loss: 0.304030136747052
  batch 264 loss: 0.3041381021453576
  batch 265 loss: 0.30409656914899935
  batch 266 loss: 0.30398546960344885
  batch 267 loss: 0.3038632820049922
  batch 268 loss: 0.30373390102341996
  batch 269 loss: 0.30371913297247266
  batch 270 loss: 0.3037313198049863
  batch 271 loss: 0.303692538085019
  batch 272 loss: 0.30364748786258344
  batch 273 loss: 0.30355346699555713
  batch 274 loss: 0.3034050013788425
  batch 275 loss: 0.30342426413839513
  batch 276 loss: 0.30343525095478346
  batch 277 loss: 0.30345733227067045
  batch 278 loss: 0.3033523071262476
  batch 279 loss: 0.30334783118471875
  batch 280 loss: 0.3032150748052767
  batch 281 loss: 0.30315924714256437
  batch 282 loss: 0.3029789678910945
  batch 283 loss: 0.30285152777134317
  batch 284 loss: 0.30296138742230305
  batch 285 loss: 0.3027758934518747
  batch 286 loss: 0.3028009981437997
  batch 287 loss: 0.3027345318515957
  batch 288 loss: 0.3027282235626545
  batch 289 loss: 0.30258189595488116
  batch 290 loss: 0.3024871435144852
  batch 291 loss: 0.30244089314822886
  batch 292 loss: 0.30240873558676407
  batch 293 loss: 0.3024234961425892
  batch 294 loss: 0.3022477570541051
  batch 295 loss: 0.3022151129225553
  batch 296 loss: 0.30233554154433107
  batch 297 loss: 0.30230070740887616
  batch 298 loss: 0.30243091910277436
  batch 299 loss: 0.30248486352804116
  batch 300 loss: 0.30242021029194194
  batch 301 loss: 0.3024742766175159
  batch 302 loss: 0.30254680150194674
  batch 303 loss: 0.30247914205880055
  batch 304 loss: 0.3024764424189925
  batch 305 loss: 0.3024426219404721
  batch 306 loss: 0.30250438828678694
  batch 307 loss: 0.3024646260742256
  batch 308 loss: 0.30254370269256753
  batch 309 loss: 0.3024722783793138
  batch 310 loss: 0.3025512717904583
  batch 311 loss: 0.30235329538679584
  batch 312 loss: 0.30228617596320617
  batch 313 loss: 0.30220033604496965
  batch 314 loss: 0.30210004205916335
  batch 315 loss: 0.302033671689412
  batch 316 loss: 0.3020469703817669
  batch 317 loss: 0.3021401589037116
  batch 318 loss: 0.3019315113530219
  batch 319 loss: 0.3017450576198512
  batch 320 loss: 0.30187361263670026
  batch 321 loss: 0.3019026477481717
  batch 322 loss: 0.301740269668354
  batch 323 loss: 0.3017649261205927
  batch 324 loss: 0.30174027622481925
  batch 325 loss: 0.3018117559873141
  batch 326 loss: 0.3020144026337957
  batch 327 loss: 0.3019719133865578
  batch 328 loss: 0.3018726380314769
  batch 329 loss: 0.3018772958984491
  batch 330 loss: 0.3018677462231029
  batch 331 loss: 0.30179798864165824
  batch 332 loss: 0.3018534673624728
  batch 333 loss: 0.3017763815842591
  batch 334 loss: 0.3020196812238522
  batch 335 loss: 0.3020245846527726
  batch 336 loss: 0.3020573149302176
  batch 337 loss: 0.3020403390468405
  batch 338 loss: 0.3019203264156037
  batch 339 loss: 0.3018616726264841
  batch 340 loss: 0.3017905888312003
  batch 341 loss: 0.3017412181998278
  batch 342 loss: 0.30168908180897697
  batch 343 loss: 0.30176510720489325
  batch 344 loss: 0.3018144673213016
  batch 345 loss: 0.30184420165808307
  batch 346 loss: 0.30179804141466327
  batch 347 loss: 0.3016777662962933
  batch 348 loss: 0.301678611458033
  batch 349 loss: 0.3017214943139806
  batch 350 loss: 0.3017496853215354
  batch 351 loss: 0.30173296763686375
  batch 352 loss: 0.3016756938093088
  batch 353 loss: 0.30160511097219106
  batch 354 loss: 0.30155489059154594
  batch 355 loss: 0.30159164238983477
  batch 356 loss: 0.30164123745111937
  batch 357 loss: 0.3014227362240062
  batch 358 loss: 0.30133985207733494
  batch 359 loss: 0.30138647655922723
  batch 360 loss: 0.3012791838083002
  batch 361 loss: 0.30123907550550233
  batch 362 loss: 0.3011507576341787
  batch 363 loss: 0.30120121595288113
  batch 364 loss: 0.30118143673126513
  batch 365 loss: 0.30117734303213145
  batch 366 loss: 0.30118672281015113
  batch 367 loss: 0.3012497382040569
  batch 368 loss: 0.3011910895128613
  batch 369 loss: 0.30119413307042625
  batch 370 loss: 0.3013512811145267
  batch 371 loss: 0.3013731095026124
  batch 372 loss: 0.30148806094482383
  batch 373 loss: 0.3015393297570001
  batch 374 loss: 0.3015050754986982
  batch 375 loss: 0.3015625658035278
  batch 376 loss: 0.30168987667941033
  batch 377 loss: 0.30166794948615827
  batch 378 loss: 0.3015897203847845
  batch 379 loss: 0.301611526188561
  batch 380 loss: 0.30154570701875183
  batch 381 loss: 0.30144466033444944
  batch 382 loss: 0.30137522112948734
  batch 383 loss: 0.30125706633139526
  batch 384 loss: 0.30118310265243053
  batch 385 loss: 0.30113155176113177
  batch 386 loss: 0.30107476373101766
  batch 387 loss: 0.30107338892089
  batch 388 loss: 0.3009858162132735
  batch 389 loss: 0.3010761538630586
  batch 390 loss: 0.3010600413267429
  batch 391 loss: 0.30102037064864506
  batch 392 loss: 0.3010823937246994
  batch 393 loss: 0.3012145533968171
  batch 394 loss: 0.3012141990782646
  batch 395 loss: 0.3011882639383968
  batch 396 loss: 0.30122193385555285
  batch 397 loss: 0.3012778978203646
  batch 398 loss: 0.30129962949896577
  batch 399 loss: 0.30128274570431623
  batch 400 loss: 0.30141586244106294
  batch 401 loss: 0.3013321278695751
  batch 402 loss: 0.3012929687749094
  batch 403 loss: 0.3012156363012773
  batch 404 loss: 0.3011190797580351
  batch 405 loss: 0.30113468287903583
  batch 406 loss: 0.3010364577100782
  batch 407 loss: 0.3010621457486539
  batch 408 loss: 0.3009483764568965
  batch 409 loss: 0.3008901197782064
  batch 410 loss: 0.3008534352953841
  batch 411 loss: 0.30086396775976587
  batch 412 loss: 0.30105865298254975
  batch 413 loss: 0.30105590856392794
  batch 414 loss: 0.30106924294273635
  batch 415 loss: 0.30104008374443975
  batch 416 loss: 0.3010942773320354
  batch 417 loss: 0.30106873177795956
  batch 418 loss: 0.30107490910867757
  batch 419 loss: 0.3010919971084822
  batch 420 loss: 0.30105640646957216
  batch 421 loss: 0.30107067118347963
  batch 422 loss: 0.30104041346724
  batch 423 loss: 0.30100833376248676
  batch 424 loss: 0.3010232587046218
  batch 425 loss: 0.30092988743501553
  batch 426 loss: 0.3009019344345505
  batch 427 loss: 0.3009052174013169
  batch 428 loss: 0.3008889507885291
  batch 429 loss: 0.3008168773217635
  batch 430 loss: 0.3008546509715014
  batch 431 loss: 0.30084326980838644
  batch 432 loss: 0.30081267199582523
  batch 433 loss: 0.30073010646168136
  batch 434 loss: 0.3006465532537979
  batch 435 loss: 0.30069968857984436
  batch 436 loss: 0.30076669334271633
  batch 437 loss: 0.3007370665908133
  batch 438 loss: 0.3007208732165158
  batch 439 loss: 0.3006549349949952
  batch 440 loss: 0.300755949318409
  batch 441 loss: 0.300771829762967
  batch 442 loss: 0.30071449178646054
  batch 443 loss: 0.3007081886311955
  batch 444 loss: 0.3007521282982182
  batch 445 loss: 0.3007240106550495
  batch 446 loss: 0.300804793968329
  batch 447 loss: 0.30081815127558353
  batch 448 loss: 0.3008770460395941
  batch 449 loss: 0.30081114445072504
  batch 450 loss: 0.3007337607940038
  batch 451 loss: 0.300658071384726
  batch 452 loss: 0.3005498767690321
  batch 453 loss: 0.30053621017380266
  batch 454 loss: 0.3005103187939144
  batch 455 loss: 0.30048276749286024
  batch 456 loss: 0.30048955009694683
  batch 457 loss: 0.30047324332418796
  batch 458 loss: 0.30042559311900074
  batch 459 loss: 0.30038549515676394
  batch 460 loss: 0.3003626967901769
  batch 461 loss: 0.3003832464492243
  batch 462 loss: 0.3004119548446688
  batch 463 loss: 0.3004470128348785
  batch 464 loss: 0.3004129994660616
  batch 465 loss: 0.30038359286964583
  batch 466 loss: 0.3003205338516972
  batch 467 loss: 0.3005066207057659
  batch 468 loss: 0.30047525777521294
  batch 469 loss: 0.30045348145305983
  batch 470 loss: 0.3006315232591426
  batch 471 loss: 0.3006784647007151
  batch 472 loss: 0.30065845338217284
LOSS train 0.30065845338217284 valid 0.2226152867078781
LOSS train 0.30065845338217284 valid 0.21740857511758804
LOSS train 0.30065845338217284 valid 0.22002523640791574
LOSS train 0.30065845338217284 valid 0.20872988551855087
LOSS train 0.30065845338217284 valid 0.21017592251300812
LOSS train 0.30065845338217284 valid 0.215801902115345
LOSS train 0.30065845338217284 valid 0.2122280512537275
LOSS train 0.30065845338217284 valid 0.21124807000160217
LOSS train 0.30065845338217284 valid 0.2096293568611145
LOSS train 0.30065845338217284 valid 0.20886440128087996
LOSS train 0.30065845338217284 valid 0.2075668383728374
LOSS train 0.30065845338217284 valid 0.210864478101333
LOSS train 0.30065845338217284 valid 0.21090110907187828
LOSS train 0.30065845338217284 valid 0.20880197414330073
LOSS train 0.30065845338217284 valid 0.2080828716357549
LOSS train 0.30065845338217284 valid 0.21078838128596544
LOSS train 0.30065845338217284 valid 0.21082510930650375
LOSS train 0.30065845338217284 valid 0.21069631063275868
LOSS train 0.30065845338217284 valid 0.21243547373696378
LOSS train 0.30065845338217284 valid 0.21250194683670998
LOSS train 0.30065845338217284 valid 0.21334180306820644
LOSS train 0.30065845338217284 valid 0.21307928318327124
LOSS train 0.30065845338217284 valid 0.21192254450010217
LOSS train 0.30065845338217284 valid 0.21281317311028639
LOSS train 0.30065845338217284 valid 0.21224081158638
LOSS train 0.30065845338217284 valid 0.21135690005925986
LOSS train 0.30065845338217284 valid 0.21136482059955597
LOSS train 0.30065845338217284 valid 0.21137149365884916
LOSS train 0.30065845338217284 valid 0.2104030528972889
LOSS train 0.30065845338217284 valid 0.21007721076409022
LOSS train 0.30065845338217284 valid 0.21032359715430968
LOSS train 0.30065845338217284 valid 0.21080851322039962
LOSS train 0.30065845338217284 valid 0.21028751360647607
LOSS train 0.30065845338217284 valid 0.20983980815200245
LOSS train 0.30065845338217284 valid 0.21046841400010244
LOSS train 0.30065845338217284 valid 0.21122999158170488
LOSS train 0.30065845338217284 valid 0.2113510040012566
LOSS train 0.30065845338217284 valid 0.21117802081923737
LOSS train 0.30065845338217284 valid 0.21189470550952813
LOSS train 0.30065845338217284 valid 0.21205682791769503
LOSS train 0.30065845338217284 valid 0.21170058773785103
LOSS train 0.30065845338217284 valid 0.21288285510880606
LOSS train 0.30065845338217284 valid 0.21316649054372033
LOSS train 0.30065845338217284 valid 0.21257757090709425
LOSS train 0.30065845338217284 valid 0.21220097773604923
LOSS train 0.30065845338217284 valid 0.21168908379648044
LOSS train 0.30065845338217284 valid 0.21132344895220817
LOSS train 0.30065845338217284 valid 0.21251675114035606
LOSS train 0.30065845338217284 valid 0.21186998668982057
LOSS train 0.30065845338217284 valid 0.21243661791086196
LOSS train 0.30065845338217284 valid 0.2122336415683522
LOSS train 0.30065845338217284 valid 0.21183998309648955
LOSS train 0.30065845338217284 valid 0.2129426621041208
LOSS train 0.30065845338217284 valid 0.21285957870660005
LOSS train 0.30065845338217284 valid 0.21258910379626533
LOSS train 0.30065845338217284 valid 0.2125856660838638
LOSS train 0.30065845338217284 valid 0.21195739010969797
LOSS train 0.30065845338217284 valid 0.21262307644918046
LOSS train 0.30065845338217284 valid 0.21242201126227944
LOSS train 0.30065845338217284 valid 0.21225498840212823
LOSS train 0.30065845338217284 valid 0.2123699173575542
LOSS train 0.30065845338217284 valid 0.21205947740424064
LOSS train 0.30065845338217284 valid 0.21205397754434555
LOSS train 0.30065845338217284 valid 0.21219261642545462
LOSS train 0.30065845338217284 valid 0.21128860826675708
LOSS train 0.30065845338217284 valid 0.21125582289515119
LOSS train 0.30065845338217284 valid 0.21161660618746458
LOSS train 0.30065845338217284 valid 0.21107152717954972
LOSS train 0.30065845338217284 valid 0.21168620517288428
LOSS train 0.30065845338217284 valid 0.21184197791985104
LOSS train 0.30065845338217284 valid 0.2121622774382712
LOSS train 0.30065845338217284 valid 0.21245513463185894
LOSS train 0.30065845338217284 valid 0.2129242863148859
LOSS train 0.30065845338217284 valid 0.21274625570387454
LOSS train 0.30065845338217284 valid 0.21238251348336537
LOSS train 0.30065845338217284 valid 0.21247177767126182
LOSS train 0.30065845338217284 valid 0.2123992903666063
LOSS train 0.30065845338217284 valid 0.21222581676183602
LOSS train 0.30065845338217284 valid 0.2123753913218462
LOSS train 0.30065845338217284 valid 0.2120195584371686
LOSS train 0.30065845338217284 valid 0.21218477263126845
LOSS train 0.30065845338217284 valid 0.21212410236277232
LOSS train 0.30065845338217284 valid 0.21224004461104612
LOSS train 0.30065845338217284 valid 0.2120580534849848
LOSS train 0.30065845338217284 valid 0.21237008589155534
LOSS train 0.30065845338217284 valid 0.21225836439881213
LOSS train 0.30065845338217284 valid 0.2117946512740234
LOSS train 0.30065845338217284 valid 0.21186171522872013
LOSS train 0.30065845338217284 valid 0.212064716085959
LOSS train 0.30065845338217284 valid 0.21227013998561436
LOSS train 0.30065845338217284 valid 0.21220668298857553
LOSS train 0.30065845338217284 valid 0.21227629920062813
LOSS train 0.30065845338217284 valid 0.21214069313900444
LOSS train 0.30065845338217284 valid 0.21228939405781158
LOSS train 0.30065845338217284 valid 0.21248793413764552
LOSS train 0.30065845338217284 valid 0.21272650407627225
LOSS train 0.30065845338217284 valid 0.2128535831711956
LOSS train 0.30065845338217284 valid 0.21311003562747216
LOSS train 0.30065845338217284 valid 0.2132561733626356
LOSS train 0.30065845338217284 valid 0.21323606878519058
LOSS train 0.30065845338217284 valid 0.21331633789704577
LOSS train 0.30065845338217284 valid 0.2137659417063582
LOSS train 0.30065845338217284 valid 0.21347229095917303
LOSS train 0.30065845338217284 valid 0.21331090881274298
LOSS train 0.30065845338217284 valid 0.21349944259439196
LOSS train 0.30065845338217284 valid 0.21344702170704896
LOSS train 0.30065845338217284 valid 0.21313788289221647
LOSS train 0.30065845338217284 valid 0.21305029259787667
LOSS train 0.30065845338217284 valid 0.21267477675862267
LOSS train 0.30065845338217284 valid 0.2127517739480192
LOSS train 0.30065845338217284 valid 0.21307305951376218
LOSS train 0.30065845338217284 valid 0.2134077554302556
LOSS train 0.30065845338217284 valid 0.2133168884083233
LOSS train 0.30065845338217284 valid 0.21332233007016935
LOSS train 0.30065845338217284 valid 0.21386502631332563
LOSS train 0.30065845338217284 valid 0.21351552138040805
LOSS train 0.30065845338217284 valid 0.21393168380117825
LOSS train 0.30065845338217284 valid 0.21381844340239542
LOSS train 0.30065845338217284 valid 0.2136246733555273
LOSS train 0.30065845338217284 valid 0.21331847844024499
LOSS train 0.30065845338217284 valid 0.21339425149042746
LOSS train 0.30065845338217284 valid 0.21361139514407174
LOSS train 0.30065845338217284 valid 0.21359443179960172
LOSS train 0.30065845338217284 valid 0.2136687682040276
LOSS train 0.30065845338217284 valid 0.2136251482963562
LOSS train 0.30065845338217284 valid 0.21378376429516172
LOSS train 0.30065845338217284 valid 0.21369910334038922
LOSS train 0.30065845338217284 valid 0.21361024340149015
LOSS train 0.30065845338217284 valid 0.21348783154358236
LOSS train 0.30065845338217284 valid 0.21319200866497479
LOSS train 0.30065845338217284 valid 0.21307586611227225
LOSS train 0.30065845338217284 valid 0.21309409155087036
LOSS train 0.30065845338217284 valid 0.2130553068074965
LOSS train 0.30065845338217284 valid 0.21332815456301418
LOSS train 0.30065845338217284 valid 0.2133796637808835
LOSS train 0.30065845338217284 valid 0.213367661132532
LOSS train 0.30065845338217284 valid 0.21351587837629946
LOSS train 0.30065845338217284 valid 0.21333588011886762
LOSS train 0.30065845338217284 valid 0.21314895142325394
LOSS train 0.30065845338217284 valid 0.21313049803887094
LOSS train 0.30065845338217284 valid 0.21315246429426452
LOSS train 0.30065845338217284 valid 0.21337213199323332
LOSS train 0.30065845338217284 valid 0.21340826341322253
LOSS train 0.30065845338217284 valid 0.21335800343917477
LOSS train 0.30065845338217284 valid 0.21321963698699556
LOSS train 0.30065845338217284 valid 0.2131516760955118
LOSS train 0.30065845338217284 valid 0.21315180647129917
LOSS train 0.30065845338217284 valid 0.2130328268819564
LOSS train 0.30065845338217284 valid 0.21306024681801763
LOSS train 0.30065845338217284 valid 0.2130917834242185
LOSS train 0.30065845338217284 valid 0.2129553807097555
LOSS train 0.30065845338217284 valid 0.2128483693262464
LOSS train 0.30065845338217284 valid 0.21263346728546167
LOSS train 0.30065845338217284 valid 0.2128093471000721
LOSS train 0.30065845338217284 valid 0.21285967038523768
LOSS train 0.30065845338217284 valid 0.21305929095699236
LOSS train 0.30065845338217284 valid 0.21299186832965558
LOSS train 0.30065845338217284 valid 0.21307506723494468
LOSS train 0.30065845338217284 valid 0.21339464824904436
LOSS train 0.30065845338217284 valid 0.21349193900823593
LOSS train 0.30065845338217284 valid 0.21351410355997383
LOSS train 0.30065845338217284 valid 0.21361122795461138
LOSS train 0.30065845338217284 valid 0.21336706878583125
LOSS train 0.30065845338217284 valid 0.21342559213318477
LOSS train 0.30065845338217284 valid 0.21348328626517094
LOSS train 0.30065845338217284 valid 0.21332732668842178
LOSS train 0.30065845338217284 valid 0.21350531431729208
LOSS train 0.30065845338217284 valid 0.21334511529476868
LOSS train 0.30065845338217284 valid 0.2131466848786766
LOSS train 0.30065845338217284 valid 0.2131911352276802
LOSS train 0.30065845338217284 valid 0.2130990185235676
LOSS train 0.30065845338217284 valid 0.2131595395851967
LOSS train 0.30065845338217284 valid 0.21323753557453265
LOSS train 0.30065845338217284 valid 0.21348931854483724
LOSS train 0.30065845338217284 valid 0.21344983620303018
LOSS train 0.30065845338217284 valid 0.21334555821323936
LOSS train 0.30065845338217284 valid 0.2133731226631477
LOSS train 0.30065845338217284 valid 0.21340415492821274
LOSS train 0.30065845338217284 valid 0.21347469024818036
LOSS train 0.30065845338217284 valid 0.2134822792477078
LOSS train 0.30065845338217284 valid 0.2134242583868912
LOSS train 0.30065845338217284 valid 0.21333191977752433
LOSS train 0.30065845338217284 valid 0.21346897692953953
LOSS train 0.30065845338217284 valid 0.2134704967231854
LOSS train 0.30065845338217284 valid 0.2135297878368481
LOSS train 0.30065845338217284 valid 0.21361321210861206
LOSS train 0.30065845338217284 valid 0.21344876169839644
LOSS train 0.30065845338217284 valid 0.21354444190225702
LOSS train 0.30065845338217284 valid 0.2133277188690882
LOSS train 0.30065845338217284 valid 0.21332475331268813
LOSS train 0.30065845338217284 valid 0.21345046279630112
LOSS train 0.30065845338217284 valid 0.21353985058764616
LOSS train 0.30065845338217284 valid 0.21336718552161993
LOSS train 0.30065845338217284 valid 0.21334931477136218
LOSS train 0.30065845338217284 valid 0.21330698002607396
LOSS train 0.30065845338217284 valid 0.2134716202866058
LOSS train 0.30065845338217284 valid 0.21346798319804486
LOSS train 0.30065845338217284 valid 0.21371911067252208
LOSS train 0.30065845338217284 valid 0.21367896733871058
LOSS train 0.30065845338217284 valid 0.21380496002733707
LOSS train 0.30065845338217284 valid 0.2136490118147722
LOSS train 0.30065845338217284 valid 0.21361179118699367
LOSS train 0.30065845338217284 valid 0.21376114430392318
LOSS train 0.30065845338217284 valid 0.21365977188243584
LOSS train 0.30065845338217284 valid 0.21364853941812748
LOSS train 0.30065845338217284 valid 0.2135777196403846
LOSS train 0.30065845338217284 valid 0.213534713583292
LOSS train 0.30065845338217284 valid 0.213396736420691
LOSS train 0.30065845338217284 valid 0.2134079738524542
LOSS train 0.30065845338217284 valid 0.21348046965542294
LOSS train 0.30065845338217284 valid 0.21364579515717042
LOSS train 0.30065845338217284 valid 0.21349045339058031
LOSS train 0.30065845338217284 valid 0.2134341300233429
LOSS train 0.30065845338217284 valid 0.21336631964300282
LOSS train 0.30065845338217284 valid 0.21315707696038624
LOSS train 0.30065845338217284 valid 0.21301281321103926
LOSS train 0.30065845338217284 valid 0.21282552677090816
LOSS train 0.30065845338217284 valid 0.21282264870514564
LOSS train 0.30065845338217284 valid 0.21281730060435866
LOSS train 0.30065845338217284 valid 0.21290304416959935
LOSS train 0.30065845338217284 valid 0.2128843642872383
LOSS train 0.30065845338217284 valid 0.21299195652072495
LOSS train 0.30065845338217284 valid 0.2130601858477956
LOSS train 0.30065845338217284 valid 0.21298516668113215
LOSS train 0.30065845338217284 valid 0.21284163137276968
LOSS train 0.30065845338217284 valid 0.21275945179230343
LOSS train 0.30065845338217284 valid 0.21278301163654495
LOSS train 0.30065845338217284 valid 0.21275065277229277
LOSS train 0.30065845338217284 valid 0.21260900893065607
LOSS train 0.30065845338217284 valid 0.2126604364618011
LOSS train 0.30065845338217284 valid 0.21258952433154696
LOSS train 0.30065845338217284 valid 0.21268163293856998
LOSS train 0.30065845338217284 valid 0.21295934103333386
LOSS train 0.30065845338217284 valid 0.2129161440027066
LOSS train 0.30065845338217284 valid 0.2129566990314646
LOSS train 0.30065845338217284 valid 0.21285141739299743
LOSS train 0.30065845338217284 valid 0.21288014011544015
LOSS train 0.30065845338217284 valid 0.21291013663055516
LOSS train 0.30065845338217284 valid 0.21292490552658816
LOSS train 0.30065845338217284 valid 0.21254924709598225
LOSS train 0.30065845338217284 valid 0.2123830644916202
LOSS train 0.30065845338217284 valid 0.2123449399821029
LOSS train 0.30065845338217284 valid 0.2123438600036833
LOSS train 0.30065845338217284 valid 0.21232222765684128
LOSS train 0.30065845338217284 valid 0.21243129524649407
LOSS train 0.30065845338217284 valid 0.2124550162049813
LOSS train 0.30065845338217284 valid 0.21249633955086775
LOSS train 0.30065845338217284 valid 0.21254365170194256
LOSS train 0.30065845338217284 valid 0.21239683032035828
LOSS train 0.30065845338217284 valid 0.21250944459438323
LOSS train 0.30065845338217284 valid 0.21256083475641047
LOSS train 0.30065845338217284 valid 0.21253407332632276
LOSS train 0.30065845338217284 valid 0.21239288675455237
LOSS train 0.30065845338217284 valid 0.21250747390619412
LOSS train 0.30065845338217284 valid 0.21240855250872817
LOSS train 0.30065845338217284 valid 0.21232186880661175
LOSS train 0.30065845338217284 valid 0.21232096572562414
LOSS train 0.30065845338217284 valid 0.2124128561149272
LOSS train 0.30065845338217284 valid 0.21238072417877815
LOSS train 0.30065845338217284 valid 0.21227192036234416
LOSS train 0.30065845338217284 valid 0.21226545597401617
LOSS train 0.30065845338217284 valid 0.21241836955074136
LOSS train 0.30065845338217284 valid 0.21233994595904768
LOSS train 0.30065845338217284 valid 0.21240342199576623
LOSS train 0.30065845338217284 valid 0.21242206670203298
LOSS train 0.30065845338217284 valid 0.21244462585090695
LOSS train 0.30065845338217284 valid 0.21239190238915132
LOSS train 0.30065845338217284 valid 0.21240875475219825
LOSS train 0.30065845338217284 valid 0.21246944135228055
LOSS train 0.30065845338217284 valid 0.2123679502694695
LOSS train 0.30065845338217284 valid 0.21235349213505142
LOSS train 0.30065845338217284 valid 0.21227870656944373
LOSS train 0.30065845338217284 valid 0.21213672935089348
LOSS train 0.30065845338217284 valid 0.2120732049981173
LOSS train 0.30065845338217284 valid 0.21218563399531626
LOSS train 0.30065845338217284 valid 0.21224539268060008
LOSS train 0.30065845338217284 valid 0.21225740327516618
LOSS train 0.30065845338217284 valid 0.21226720013421216
LOSS train 0.30065845338217284 valid 0.2122547611540791
LOSS train 0.30065845338217284 valid 0.21219268749867168
LOSS train 0.30065845338217284 valid 0.21206680428090893
LOSS train 0.30065845338217284 valid 0.21209440322209758
LOSS train 0.30065845338217284 valid 0.21214460168204965
LOSS train 0.30065845338217284 valid 0.2120879136655532
LOSS train 0.30065845338217284 valid 0.21209542489888375
LOSS train 0.30065845338217284 valid 0.21209079449201798
LOSS train 0.30065845338217284 valid 0.21209017397634658
LOSS train 0.30065845338217284 valid 0.21205888911046916
LOSS train 0.30065845338217284 valid 0.2120018106430872
LOSS train 0.30065845338217284 valid 0.21193855050308952
LOSS train 0.30065845338217284 valid 0.21189589839415862
LOSS train 0.30065845338217284 valid 0.21185311108623464
LOSS train 0.30065845338217284 valid 0.21184030368466425
LOSS train 0.30065845338217284 valid 0.21184852313833172
LOSS train 0.30065845338217284 valid 0.21185305598428694
LOSS train 0.30065845338217284 valid 0.21181258821004145
LOSS train 0.30065845338217284 valid 0.21175426540752051
LOSS train 0.30065845338217284 valid 0.21180249815619231
LOSS train 0.30065845338217284 valid 0.21179772988210951
LOSS train 0.30065845338217284 valid 0.2117349337041378
LOSS train 0.30065845338217284 valid 0.21165161676383099
LOSS train 0.30065845338217284 valid 0.21163386742997642
LOSS train 0.30065845338217284 valid 0.2115661757122172
LOSS train 0.30065845338217284 valid 0.2115740344222439
LOSS train 0.30065845338217284 valid 0.2115594682634854
LOSS train 0.30065845338217284 valid 0.21174090667292964
LOSS train 0.30065845338217284 valid 0.211768413454003
LOSS train 0.30065845338217284 valid 0.21184957612838065
LOSS train 0.30065845338217284 valid 0.211881922286691
LOSS train 0.30065845338217284 valid 0.2119014586652479
LOSS train 0.30065845338217284 valid 0.2118944854885819
LOSS train 0.30065845338217284 valid 0.21184914912550878
LOSS train 0.30065845338217284 valid 0.21189250023410724
LOSS train 0.30065845338217284 valid 0.21179615644512662
LOSS train 0.30065845338217284 valid 0.21175385817648873
LOSS train 0.30065845338217284 valid 0.21175561590662487
LOSS train 0.30065845338217284 valid 0.2117497284047611
LOSS train 0.30065845338217284 valid 0.21177860846122107
LOSS train 0.30065845338217284 valid 0.21176870335418976
LOSS train 0.30065845338217284 valid 0.21170370155014098
LOSS train 0.30065845338217284 valid 0.21174874321508258
LOSS train 0.30065845338217284 valid 0.21172691775219782
LOSS train 0.30065845338217284 valid 0.2115993044878307
LOSS train 0.30065845338217284 valid 0.21152433145929267
LOSS train 0.30065845338217284 valid 0.2113750631075639
LOSS train 0.30065845338217284 valid 0.21147854726738724
LOSS train 0.30065845338217284 valid 0.21143868137207963
LOSS train 0.30065845338217284 valid 0.21143049097097502
LOSS train 0.30065845338217284 valid 0.21134484566091405
LOSS train 0.30065845338217284 valid 0.2113584170738856
LOSS train 0.30065845338217284 valid 0.2112682308675299
LOSS train 0.30065845338217284 valid 0.21125031258147883
LOSS train 0.30065845338217284 valid 0.2113532116552731
LOSS train 0.30065845338217284 valid 0.21129486308304848
LOSS train 0.30065845338217284 valid 0.21120850087991402
LOSS train 0.30065845338217284 valid 0.21112287962543114
LOSS train 0.30065845338217284 valid 0.21122452453088336
LOSS train 0.30065845338217284 valid 0.21118257194757462
LOSS train 0.30065845338217284 valid 0.21118152581133323
LOSS train 0.30065845338217284 valid 0.2112609640640371
LOSS train 0.30065845338217284 valid 0.2112133766461677
LOSS train 0.30065845338217284 valid 0.2111628488229032
LOSS train 0.30065845338217284 valid 0.2110781026216707
LOSS train 0.30065845338217284 valid 0.21112942006872143
LOSS train 0.30065845338217284 valid 0.21116846527742303
LOSS train 0.30065845338217284 valid 0.21111783815946192
LOSS train 0.30065845338217284 valid 0.21106061648566715
LOSS train 0.30065845338217284 valid 0.21114363127398764
LOSS train 0.30065845338217284 valid 0.21108536896186436
LOSS train 0.30065845338217284 valid 0.21108972587755748
LOSS train 0.30065845338217284 valid 0.21116803684125937
LOSS train 0.30065845338217284 valid 0.21121700290082532
LOSS train 0.30065845338217284 valid 0.21129692179116583
LOSS train 0.30065845338217284 valid 0.21128525071393298
LOSS train 0.30065845338217284 valid 0.21118996374204124
LOSS train 0.30065845338217284 valid 0.21120562123950948
LOSS train 0.30065845338217284 valid 0.21123505143725238
LOSS train 0.30065845338217284 valid 0.21120060193638562
LOSS train 0.30065845338217284 valid 0.2111597030441741
LOSS train 0.30065845338217284 valid 0.2112572052826484
LOSS train 0.30065845338217284 valid 0.21120403661622236
LOSS train 0.30065845338217284 valid 0.21125177524531086
LOSS train 0.30065845338217284 valid 0.2112955993282565
LOSS train 0.30065845338217284 valid 0.21122539051613964
LOSS train 0.30065845338217284 valid 0.21132894652347042
LOSS train 0.30065845338217284 valid 0.21129454130846295
LOSS train 0.30065845338217284 valid 0.21128518968739365
LOSS train 0.30065845338217284 valid 0.21120789873859155
LOSS train 0.30065845338217284 valid 0.2112012833841448
EPOCH 6:
  batch 1 loss: 0.360724538564682
  batch 2 loss: 0.3580879122018814
  batch 3 loss: 0.3365064561367035
  batch 4 loss: 0.3396689370274544
  batch 5 loss: 0.3354962646961212
  batch 6 loss: 0.3293674687544505
  batch 7 loss: 0.3253331482410431
  batch 8 loss: 0.31978530809283257
  batch 9 loss: 0.31847163372569615
  batch 10 loss: 0.31328755915164946
  batch 11 loss: 0.31242747469381854
  batch 12 loss: 0.3107953717311223
  batch 13 loss: 0.3086142035631033
  batch 14 loss: 0.30829121598175596
  batch 15 loss: 0.3091755290826162
  batch 16 loss: 0.30820650048553944
  batch 17 loss: 0.3081226962454179
  batch 18 loss: 0.30635109543800354
  batch 19 loss: 0.30518154250948054
  batch 20 loss: 0.30257672518491746
  batch 21 loss: 0.30636338818640935
  batch 22 loss: 0.3062931231477044
  batch 23 loss: 0.30434286983116815
  batch 24 loss: 0.3039093650877476
  batch 25 loss: 0.304907363653183
  batch 26 loss: 0.3045561508490489
  batch 27 loss: 0.3059798710876041
  batch 28 loss: 0.30720612726041246
  batch 29 loss: 0.30693728245537855
  batch 30 loss: 0.3073919236660004
  batch 31 loss: 0.30742201401341346
  batch 32 loss: 0.3087373897433281
  batch 33 loss: 0.3100160260995229
  batch 34 loss: 0.309655025601387
  batch 35 loss: 0.3107288990701948
  batch 36 loss: 0.31031514290306306
  batch 37 loss: 0.3097286264638643
  batch 38 loss: 0.3098019620305614
  batch 39 loss: 0.30975925616728955
  batch 40 loss: 0.3092125192284584
  batch 41 loss: 0.30864718483715525
  batch 42 loss: 0.30817492590064094
  batch 43 loss: 0.3078757427459539
  batch 44 loss: 0.30781386250799353
  batch 45 loss: 0.3081739624341329
  batch 46 loss: 0.30679890707783075
  batch 47 loss: 0.306215837914893
  batch 48 loss: 0.30649083107709885
  batch 49 loss: 0.30710921299700833
  batch 50 loss: 0.3072192221879959
  batch 51 loss: 0.3069689577701045
  batch 52 loss: 0.3069787151538409
  batch 53 loss: 0.3061962515678046
  batch 54 loss: 0.30571603940592873
  batch 55 loss: 0.3058025018735365
  batch 56 loss: 0.3064527101814747
  batch 57 loss: 0.30592560036140576
  batch 58 loss: 0.3052179088880276
  batch 59 loss: 0.3049766714290037
  batch 60 loss: 0.3048457493384679
  batch 61 loss: 0.3055246609156249
  batch 62 loss: 0.30594115295717794
  batch 63 loss: 0.3053163777268122
  batch 64 loss: 0.3050950593315065
  batch 65 loss: 0.30432239449941195
  batch 66 loss: 0.30406701971184125
  batch 67 loss: 0.3035766064231075
  batch 68 loss: 0.3033091504784191
  batch 69 loss: 0.30305425239645917
  batch 70 loss: 0.3023945369890758
  batch 71 loss: 0.3026282229054142
  batch 72 loss: 0.30283056903216576
  batch 73 loss: 0.302765478418298
  batch 74 loss: 0.30209477084714015
  batch 75 loss: 0.3018424125512441
  batch 76 loss: 0.3017400126708181
  batch 77 loss: 0.30136065707578286
  batch 78 loss: 0.301621234187713
  batch 79 loss: 0.3016268573229826
  batch 80 loss: 0.3012883745133877
  batch 81 loss: 0.30100625459058783
  batch 82 loss: 0.30107788087391274
  batch 83 loss: 0.3008339986743697
  batch 84 loss: 0.3003278026978175
  batch 85 loss: 0.30057781478937934
  batch 86 loss: 0.30043338585731594
  batch 87 loss: 0.3002531316088534
  batch 88 loss: 0.3000605980103666
  batch 89 loss: 0.29938625083880477
  batch 90 loss: 0.2997929980357488
  batch 91 loss: 0.29946420186168543
  batch 92 loss: 0.29926624051902606
  batch 93 loss: 0.2987830440203349
  batch 94 loss: 0.298638697317306
  batch 95 loss: 0.2986041031385723
  batch 96 loss: 0.29853807327648
  batch 97 loss: 0.29885313160640675
  batch 98 loss: 0.29855346922971765
  batch 99 loss: 0.29831329680452445
  batch 100 loss: 0.29834856420755385
  batch 101 loss: 0.2980765189864848
  batch 102 loss: 0.29858574504945795
  batch 103 loss: 0.2985625680789207
  batch 104 loss: 0.2991722893829529
  batch 105 loss: 0.29927611634844825
  batch 106 loss: 0.2994696064940039
  batch 107 loss: 0.2994614613947467
  batch 108 loss: 0.2993222234977616
  batch 109 loss: 0.29961250927470146
  batch 110 loss: 0.2996359730308706
  batch 111 loss: 0.29967053060059073
  batch 112 loss: 0.2995443320167916
  batch 113 loss: 0.29921303333434385
  batch 114 loss: 0.2989225865978944
  batch 115 loss: 0.29845948387747223
  batch 116 loss: 0.2981837246695469
  batch 117 loss: 0.29800487570783013
  batch 118 loss: 0.29813109451936465
  batch 119 loss: 0.2982517735046499
  batch 120 loss: 0.29794851603607336
  batch 121 loss: 0.2982882861017196
  batch 122 loss: 0.2982575073838234
  batch 123 loss: 0.298154106590806
  batch 124 loss: 0.29806210449145687
  batch 125 loss: 0.29771170794963836
  batch 126 loss: 0.2977409240981889
  batch 127 loss: 0.29791626768318685
  batch 128 loss: 0.29788718407507986
  batch 129 loss: 0.29795165729615114
  batch 130 loss: 0.2979260129424242
  batch 131 loss: 0.2977715524780841
  batch 132 loss: 0.29785598825776216
  batch 133 loss: 0.29765950569084715
  batch 134 loss: 0.29744586715502525
  batch 135 loss: 0.29745506478680506
  batch 136 loss: 0.2977072751039968
  batch 137 loss: 0.2979772466160085
  batch 138 loss: 0.2980926914707474
  batch 139 loss: 0.29812516336389583
  batch 140 loss: 0.29850038194230627
  batch 141 loss: 0.2985401412485339
  batch 142 loss: 0.29839958323979043
  batch 143 loss: 0.2984748325356237
  batch 144 loss: 0.2983283730637696
  batch 145 loss: 0.2985644766996647
  batch 146 loss: 0.29863832247992084
  batch 147 loss: 0.2985712027265912
  batch 148 loss: 0.29860752810900276
  batch 149 loss: 0.2982955079950742
  batch 150 loss: 0.29844137440125146
  batch 151 loss: 0.298231827995635
  batch 152 loss: 0.29801072719457905
  batch 153 loss: 0.29849060249874015
  batch 154 loss: 0.2985562779880189
  batch 155 loss: 0.29871126442186297
  batch 156 loss: 0.29860483062190885
  batch 157 loss: 0.29856552999870034
  batch 158 loss: 0.29877424136370045
  batch 159 loss: 0.29902671183805046
  batch 160 loss: 0.29891163026914
  batch 161 loss: 0.29857180420286167
  batch 162 loss: 0.29850903107428256
  batch 163 loss: 0.2985287596660158
  batch 164 loss: 0.298756618052721
  batch 165 loss: 0.2990579214059945
  batch 166 loss: 0.2994497069213764
  batch 167 loss: 0.29946738579672966
  batch 168 loss: 0.29924202825696694
  batch 169 loss: 0.2993886584537269
  batch 170 loss: 0.29951845644151465
  batch 171 loss: 0.2998146758964884
  batch 172 loss: 0.2997454660403174
  batch 173 loss: 0.2999288085740426
  batch 174 loss: 0.3001337314302894
  batch 175 loss: 0.30021781725542884
  batch 176 loss: 0.3004470111124895
  batch 177 loss: 0.3006951885539933
  batch 178 loss: 0.30057054180442616
  batch 179 loss: 0.3007536010202749
  batch 180 loss: 0.3006236840453413
  batch 181 loss: 0.3004435924041337
  batch 182 loss: 0.3003836278240759
  batch 183 loss: 0.30027615087605564
  batch 184 loss: 0.30010393108038796
  batch 185 loss: 0.2999601625912898
  batch 186 loss: 0.2999663663006598
  batch 187 loss: 0.3000511204335779
  batch 188 loss: 0.30000714157172975
  batch 189 loss: 0.2999279151360194
  batch 190 loss: 0.2998408694800578
  batch 191 loss: 0.29990989398893886
  batch 192 loss: 0.29971967831564444
  batch 193 loss: 0.2997217918770301
  batch 194 loss: 0.2995962029726235
  batch 195 loss: 0.29968677170765706
  batch 196 loss: 0.2996033584432943
  batch 197 loss: 0.29957042150388513
  batch 198 loss: 0.2995903900626934
  batch 199 loss: 0.2995338427211771
  batch 200 loss: 0.2995783846825361
  batch 201 loss: 0.29945705979321136
  batch 202 loss: 0.29941968228852395
  batch 203 loss: 0.2994011189109586
  batch 204 loss: 0.2994689831108439
  batch 205 loss: 0.29923381420170386
  batch 206 loss: 0.29931719536052165
  batch 207 loss: 0.29916869593415285
  batch 208 loss: 0.2988985047604029
  batch 209 loss: 0.29897189625142295
  batch 210 loss: 0.29896187966778165
  batch 211 loss: 0.298978168817493
  batch 212 loss: 0.2989607352130818
  batch 213 loss: 0.29887659742798606
  batch 214 loss: 0.29886401632678844
  batch 215 loss: 0.29889873973158904
  batch 216 loss: 0.2989716723009392
  batch 217 loss: 0.2988865529337237
  batch 218 loss: 0.29873532877055875
  batch 219 loss: 0.2984643692839636
  batch 220 loss: 0.298491947081956
  batch 221 loss: 0.2984572640910947
  batch 222 loss: 0.2984972393459028
  batch 223 loss: 0.2982993724634829
  batch 224 loss: 0.298285931880985
  batch 225 loss: 0.2983828712834252
  batch 226 loss: 0.2984074910657596
  batch 227 loss: 0.2983163156698454
  batch 228 loss: 0.29830004706194524
  batch 229 loss: 0.29825022322121664
  batch 230 loss: 0.2980951377879018
  batch 231 loss: 0.2980658294576587
  batch 232 loss: 0.29807547595480394
  batch 233 loss: 0.29800076239098805
  batch 234 loss: 0.2979688122231736
  batch 235 loss: 0.2978525775544187
  batch 236 loss: 0.2978177506287219
  batch 237 loss: 0.2976057731904058
  batch 238 loss: 0.2975278375279002
  batch 239 loss: 0.2974794362379417
  batch 240 loss: 0.2974591391781966
  batch 241 loss: 0.29732306604563447
  batch 242 loss: 0.2972105567859224
  batch 243 loss: 0.29726004710903875
  batch 244 loss: 0.297203464464086
  batch 245 loss: 0.2972050644913498
  batch 246 loss: 0.2971860753084586
  batch 247 loss: 0.29723475276217287
  batch 248 loss: 0.2973385769273004
  batch 249 loss: 0.29721715735144405
  batch 250 loss: 0.2971582124233246
  batch 251 loss: 0.29699118571927346
  batch 252 loss: 0.2969103733462001
  batch 253 loss: 0.2967901935454885
  batch 254 loss: 0.2967697677649851
  batch 255 loss: 0.2966383090206221
  batch 256 loss: 0.29654501797631383
  batch 257 loss: 0.2966811222093115
  batch 258 loss: 0.2965756728667621
  batch 259 loss: 0.29635210087860875
  batch 260 loss: 0.2963481590151787
  batch 261 loss: 0.2963282138223392
  batch 262 loss: 0.2963283039231337
  batch 263 loss: 0.2962187126335536
  batch 264 loss: 0.2963672797336723
  batch 265 loss: 0.29635480631072564
  batch 266 loss: 0.29619813279101725
  batch 267 loss: 0.29605479108706845
  batch 268 loss: 0.2958786737229397
  batch 269 loss: 0.2958397236566118
  batch 270 loss: 0.29579944991403156
  batch 271 loss: 0.29580951996175125
  batch 272 loss: 0.2957676180693157
  batch 273 loss: 0.295664308763249
  batch 274 loss: 0.2955168814663469
  batch 275 loss: 0.2955171423066746
  batch 276 loss: 0.2955202283742635
  batch 277 loss: 0.29552674449523003
  batch 278 loss: 0.29541885097035403
  batch 279 loss: 0.29543531149305324
  batch 280 loss: 0.29532984873013834
  batch 281 loss: 0.2953364886741197
  batch 282 loss: 0.2951606094097415
  batch 283 loss: 0.29503181613586815
  batch 284 loss: 0.2951152337793733
  batch 285 loss: 0.29494264361105466
  batch 286 loss: 0.2949682028664576
  batch 287 loss: 0.2948707903407592
  batch 288 loss: 0.2948288574504356
  batch 289 loss: 0.29469392523427324
  batch 290 loss: 0.29460728060582586
  batch 291 loss: 0.29458960199478973
  batch 292 loss: 0.29457694103252396
  batch 293 loss: 0.2946138217689234
  batch 294 loss: 0.29442046212704004
  batch 295 loss: 0.2944072245036141
  batch 296 loss: 0.29450923340344753
  batch 297 loss: 0.2944728420539336
  batch 298 loss: 0.29456599991793603
  batch 299 loss: 0.29461937509850916
  batch 300 loss: 0.2945531357824802
  batch 301 loss: 0.2945972816195599
  batch 302 loss: 0.2947277683789367
  batch 303 loss: 0.29466507610904896
  batch 304 loss: 0.2946614393180138
  batch 305 loss: 0.2946628067825661
  batch 306 loss: 0.2947493267117762
  batch 307 loss: 0.2946890257177601
  batch 308 loss: 0.29475003408340666
  batch 309 loss: 0.294715132094124
  batch 310 loss: 0.2947676858113658
  batch 311 loss: 0.2945780985512534
  batch 312 loss: 0.29453749338594765
  batch 313 loss: 0.2944590054666653
  batch 314 loss: 0.29437539925810635
  batch 315 loss: 0.29426397515667807
  batch 316 loss: 0.29427964688290525
  batch 317 loss: 0.2943576819231081
  batch 318 loss: 0.2941408671860425
  batch 319 loss: 0.29393415325861366
  batch 320 loss: 0.29402843620628116
  batch 321 loss: 0.2940087324919359
  batch 322 loss: 0.29380506476075013
  batch 323 loss: 0.29382491633065344
  batch 324 loss: 0.29384982949237765
  batch 325 loss: 0.2939324786571356
  batch 326 loss: 0.29420462163862277
  batch 327 loss: 0.29414151135023214
  batch 328 loss: 0.29404687168212923
  batch 329 loss: 0.2940668688809618
  batch 330 loss: 0.2940733449477138
  batch 331 loss: 0.2939926212557853
  batch 332 loss: 0.2940391736127526
  batch 333 loss: 0.2940122671760954
  batch 334 loss: 0.2942244055831504
  batch 335 loss: 0.2942643807450337
  batch 336 loss: 0.2942924076868665
  batch 337 loss: 0.29429390933640986
  batch 338 loss: 0.29417186124614003
  batch 339 loss: 0.29413109239751256
  batch 340 loss: 0.2940571766127558
  batch 341 loss: 0.2939980165087932
  batch 342 loss: 0.2939543988533884
  batch 343 loss: 0.29401700468000797
  batch 344 loss: 0.2940967137990303
  batch 345 loss: 0.2941127514925556
  batch 346 loss: 0.29405949488712874
  batch 347 loss: 0.29394294622136813
  batch 348 loss: 0.2939683325246148
  batch 349 loss: 0.2939548692679337
  batch 350 loss: 0.29400376613651
  batch 351 loss: 0.29401298385230223
  batch 352 loss: 0.2939726311120797
  batch 353 loss: 0.29391994662899473
  batch 354 loss: 0.2938368245959282
  batch 355 loss: 0.2938157877032186
  batch 356 loss: 0.2938817240130366
  batch 357 loss: 0.293649911212654
  batch 358 loss: 0.29358812800332823
  batch 359 loss: 0.29363198897964776
  batch 360 loss: 0.29353653863072393
  batch 361 loss: 0.2934645763559685
  batch 362 loss: 0.29336406339927273
  batch 363 loss: 0.29341099692441874
  batch 364 loss: 0.29339848876326946
  batch 365 loss: 0.2933994968460031
  batch 366 loss: 0.2934338186607986
  batch 367 loss: 0.2935701423509894
  batch 368 loss: 0.293510798121924
  batch 369 loss: 0.2934914896804789
  batch 370 loss: 0.29367248295126736
  batch 371 loss: 0.29374214251729036
  batch 372 loss: 0.29380885639818766
  batch 373 loss: 0.29387244997650946
  batch 374 loss: 0.29389102023871827
  batch 375 loss: 0.29392368523279827
  batch 376 loss: 0.2940147583313445
  batch 377 loss: 0.2940215911884207
  batch 378 loss: 0.29392160987727856
  batch 379 loss: 0.2939129750143569
  batch 380 loss: 0.29386003723270016
  batch 381 loss: 0.293773636417439
  batch 382 loss: 0.2937070249106872
  batch 383 loss: 0.2935489174124466
  batch 384 loss: 0.29349391379704076
  batch 385 loss: 0.2934319309599988
  batch 386 loss: 0.2933575735654238
  batch 387 loss: 0.2933346216376746
  batch 388 loss: 0.2932253535260859
  batch 389 loss: 0.2932744468239095
  batch 390 loss: 0.29324671962322335
  batch 391 loss: 0.29317985234967886
  batch 392 loss: 0.2932243809408071
  batch 393 loss: 0.29327196163378905
  batch 394 loss: 0.2932405968759266
  batch 395 loss: 0.2931960079488875
  batch 396 loss: 0.29324381474894706
  batch 397 loss: 0.2933002840962158
  batch 398 loss: 0.2932788079707467
  batch 399 loss: 0.29327135531227094
  batch 400 loss: 0.2933658494800329
  batch 401 loss: 0.29324474941911244
  batch 402 loss: 0.29326510344126927
  batch 403 loss: 0.29318806924683877
  batch 404 loss: 0.29308261718637874
  batch 405 loss: 0.29308879033282953
  batch 406 loss: 0.29299800086931643
  batch 407 loss: 0.2930246527496661
  batch 408 loss: 0.2928885210524587
  batch 409 loss: 0.2927978977305965
  batch 410 loss: 0.2927105229075362
  batch 411 loss: 0.29270483346751136
  batch 412 loss: 0.2928307940369671
  batch 413 loss: 0.2928043003521012
  batch 414 loss: 0.29283673154271167
  batch 415 loss: 0.29276816528963756
  batch 416 loss: 0.2927760171632354
  batch 417 loss: 0.29272906485793115
  batch 418 loss: 0.2927384561186202
  batch 419 loss: 0.29272485462179615
  batch 420 loss: 0.29268699181931357
  batch 421 loss: 0.29270091737817416
  batch 422 loss: 0.29265517081129605
  batch 423 loss: 0.29260753901855885
  batch 424 loss: 0.2926387349670788
  batch 425 loss: 0.2925207565461888
  batch 426 loss: 0.29247541701150054
  batch 427 loss: 0.29251471507325943
  batch 428 loss: 0.2924961270503352
  batch 429 loss: 0.2924151022186924
  batch 430 loss: 0.2924511949336806
  batch 431 loss: 0.29242721238020125
  batch 432 loss: 0.2924124622993447
  batch 433 loss: 0.29233083937652665
  batch 434 loss: 0.2922539113922053
  batch 435 loss: 0.29231266204653117
  batch 436 loss: 0.2923928118148528
  batch 437 loss: 0.2923554908015362
  batch 438 loss: 0.2923195756393481
  batch 439 loss: 0.292230930680023
  batch 440 loss: 0.292326479439031
  batch 441 loss: 0.29231832387631174
  batch 442 loss: 0.29228524954745133
  batch 443 loss: 0.2923059572821008
  batch 444 loss: 0.2923551761285142
  batch 445 loss: 0.2923032374529356
  batch 446 loss: 0.29237931007895235
  batch 447 loss: 0.2924062606005594
  batch 448 loss: 0.29246573745539145
  batch 449 loss: 0.29239272848933207
  batch 450 loss: 0.292335459821754
  batch 451 loss: 0.2922566839371975
  batch 452 loss: 0.2921696118351105
  batch 453 loss: 0.29220475335389573
  batch 454 loss: 0.29216510067295925
  batch 455 loss: 0.2921520098552599
  batch 456 loss: 0.29214382018044327
  batch 457 loss: 0.2921080614401423
  batch 458 loss: 0.29205430735016497
  batch 459 loss: 0.29200876528531117
  batch 460 loss: 0.2920031061315018
  batch 461 loss: 0.2920135441609939
  batch 462 loss: 0.29205165400262517
  batch 463 loss: 0.2920792641250928
  batch 464 loss: 0.2920274492674347
  batch 465 loss: 0.2920496682646454
  batch 466 loss: 0.29195387546340795
  batch 467 loss: 0.29205498609859376
  batch 468 loss: 0.29204861940736443
  batch 469 loss: 0.29194210240962915
  batch 470 loss: 0.29206395171424177
  batch 471 loss: 0.29211596189157724
  batch 472 loss: 0.2919618493672145
LOSS train 0.2919618493672145 valid 0.2473602592945099
LOSS train 0.2919618493672145 valid 0.24184948205947876
LOSS train 0.2919618493672145 valid 0.24537391463915506
LOSS train 0.2919618493672145 valid 0.2306823544204235
LOSS train 0.2919618493672145 valid 0.23300924599170686
LOSS train 0.2919618493672145 valid 0.23840653647979101
LOSS train 0.2919618493672145 valid 0.23331087401935033
LOSS train 0.2919618493672145 valid 0.23170705698430538
LOSS train 0.2919618493672145 valid 0.23044191466437447
LOSS train 0.2919618493672145 valid 0.23065428733825682
LOSS train 0.2919618493672145 valid 0.22948791493069043
LOSS train 0.2919618493672145 valid 0.2334695359071096
LOSS train 0.2919618493672145 valid 0.2337920333330448
LOSS train 0.2919618493672145 valid 0.23121762701443263
LOSS train 0.2919618493672145 valid 0.2305219312508901
LOSS train 0.2919618493672145 valid 0.23420156352221966
LOSS train 0.2919618493672145 valid 0.23423766213304856
LOSS train 0.2919618493672145 valid 0.234257058136993
LOSS train 0.2919618493672145 valid 0.23642791023379878
LOSS train 0.2919618493672145 valid 0.23650580495595933
LOSS train 0.2919618493672145 valid 0.23772363861401877
LOSS train 0.2919618493672145 valid 0.23760911280458624
LOSS train 0.2919618493672145 valid 0.23613375749277032
LOSS train 0.2919618493672145 valid 0.23707027547061443
LOSS train 0.2919618493672145 valid 0.23653738737106322
LOSS train 0.2919618493672145 valid 0.2355654388666153
LOSS train 0.2919618493672145 valid 0.23550474698896762
LOSS train 0.2919618493672145 valid 0.2355161170874323
LOSS train 0.2919618493672145 valid 0.23404115541227932
LOSS train 0.2919618493672145 valid 0.23404304484526317
LOSS train 0.2919618493672145 valid 0.23448247582681717
LOSS train 0.2919618493672145 valid 0.23499720636755228
LOSS train 0.2919618493672145 valid 0.2340760131676992
LOSS train 0.2919618493672145 valid 0.23371416067375855
LOSS train 0.2919618493672145 valid 0.23431792429515294
LOSS train 0.2919618493672145 valid 0.23516735434532166
LOSS train 0.2919618493672145 valid 0.23538722621428
LOSS train 0.2919618493672145 valid 0.23527794841088748
LOSS train 0.2919618493672145 valid 0.23635046833600754
LOSS train 0.2919618493672145 valid 0.2366419643163681
LOSS train 0.2919618493672145 valid 0.23634347683045923
LOSS train 0.2919618493672145 valid 0.23755799872534616
LOSS train 0.2919618493672145 valid 0.23774681742801224
LOSS train 0.2919618493672145 valid 0.23700236495245586
LOSS train 0.2919618493672145 valid 0.2366119616561466
LOSS train 0.2919618493672145 valid 0.23617351249508237
LOSS train 0.2919618493672145 valid 0.23574789661042234
LOSS train 0.2919618493672145 valid 0.23735606173674265
LOSS train 0.2919618493672145 valid 0.23656775452652756
LOSS train 0.2919618493672145 valid 0.23705789268016816
LOSS train 0.2919618493672145 valid 0.23671256005764008
LOSS train 0.2919618493672145 valid 0.23633944816314256
LOSS train 0.2919618493672145 valid 0.2378457947722021
LOSS train 0.2919618493672145 valid 0.2378415404646485
LOSS train 0.2919618493672145 valid 0.2374297559261322
LOSS train 0.2919618493672145 valid 0.2374485951981374
LOSS train 0.2919618493672145 valid 0.23669557953089998
LOSS train 0.2919618493672145 valid 0.23735717016047445
LOSS train 0.2919618493672145 valid 0.2370352719799947
LOSS train 0.2919618493672145 valid 0.2368762192626794
LOSS train 0.2919618493672145 valid 0.23705005254901823
LOSS train 0.2919618493672145 valid 0.23682606268313625
LOSS train 0.2919618493672145 valid 0.23671292430824703
LOSS train 0.2919618493672145 valid 0.23680358403362334
LOSS train 0.2919618493672145 valid 0.23582644118712498
LOSS train 0.2919618493672145 valid 0.2357837867104646
LOSS train 0.2919618493672145 valid 0.23633589962525153
LOSS train 0.2919618493672145 valid 0.2357180567786974
LOSS train 0.2919618493672145 valid 0.23648937996746838
LOSS train 0.2919618493672145 valid 0.23668962738343646
LOSS train 0.2919618493672145 valid 0.2369625490735954
LOSS train 0.2919618493672145 valid 0.2373062390834093
LOSS train 0.2919618493672145 valid 0.2379185722707069
LOSS train 0.2919618493672145 valid 0.23772219971225067
LOSS train 0.2919618493672145 valid 0.2372383753458659
LOSS train 0.2919618493672145 valid 0.23732605713762736
LOSS train 0.2919618493672145 valid 0.23728251766848873
LOSS train 0.2919618493672145 valid 0.23698559613564077
LOSS train 0.2919618493672145 valid 0.23701967091500004
LOSS train 0.2919618493672145 valid 0.236465659737587
LOSS train 0.2919618493672145 valid 0.23668418622311252
LOSS train 0.2919618493672145 valid 0.23664325330315567
LOSS train 0.2919618493672145 valid 0.23674310187259354
LOSS train 0.2919618493672145 valid 0.2365123319129149
LOSS train 0.2919618493672145 valid 0.23693602523382973
LOSS train 0.2919618493672145 valid 0.2368258277690688
LOSS train 0.2919618493672145 valid 0.2362198819374216
LOSS train 0.2919618493672145 valid 0.23627028682015158
LOSS train 0.2919618493672145 valid 0.2365693175390865
LOSS train 0.2919618493672145 valid 0.23677204814222125
LOSS train 0.2919618493672145 valid 0.2367316477901333
LOSS train 0.2919618493672145 valid 0.23673630311437274
LOSS train 0.2919618493672145 valid 0.23664486408233643
LOSS train 0.2919618493672145 valid 0.2367409586589387
LOSS train 0.2919618493672145 valid 0.2370571056478902
LOSS train 0.2919618493672145 valid 0.2372749545611441
LOSS train 0.2919618493672145 valid 0.2373711594294027
LOSS train 0.2919618493672145 valid 0.2377301656774112
LOSS train 0.2919618493672145 valid 0.2377911444866296
LOSS train 0.2919618493672145 valid 0.23783975586295128
LOSS train 0.2919618493672145 valid 0.23786424424978767
LOSS train 0.2919618493672145 valid 0.23830836412369036
LOSS train 0.2919618493672145 valid 0.23796765202457465
LOSS train 0.2919618493672145 valid 0.23777441809383723
LOSS train 0.2919618493672145 valid 0.23803833595343998
LOSS train 0.2919618493672145 valid 0.23791295711724264
LOSS train 0.2919618493672145 valid 0.23757198076938915
LOSS train 0.2919618493672145 valid 0.2374960649068709
LOSS train 0.2919618493672145 valid 0.23709665867713614
LOSS train 0.2919618493672145 valid 0.23717801096764476
LOSS train 0.2919618493672145 valid 0.2375357568532497
LOSS train 0.2919618493672145 valid 0.23791464804006474
LOSS train 0.2919618493672145 valid 0.23778438238443528
LOSS train 0.2919618493672145 valid 0.23781015867726846
LOSS train 0.2919618493672145 valid 0.23846175566963526
LOSS train 0.2919618493672145 valid 0.23805310358775073
LOSS train 0.2919618493672145 valid 0.2385424249447309
LOSS train 0.2919618493672145 valid 0.23843580572786977
LOSS train 0.2919618493672145 valid 0.2381505386418655
LOSS train 0.2919618493672145 valid 0.23786915068825085
LOSS train 0.2919618493672145 valid 0.23792543563960997
LOSS train 0.2919618493672145 valid 0.23819449495096676
LOSS train 0.2919618493672145 valid 0.2382433620410237
LOSS train 0.2919618493672145 valid 0.23833044690470542
LOSS train 0.2919618493672145 valid 0.23828918528556825
LOSS train 0.2919618493672145 valid 0.2384341817999643
LOSS train 0.2919618493672145 valid 0.23831250630025788
LOSS train 0.2919618493672145 valid 0.23825940489768982
LOSS train 0.2919618493672145 valid 0.2380581235931825
LOSS train 0.2919618493672145 valid 0.23771713788692767
LOSS train 0.2919618493672145 valid 0.2375872118099955
LOSS train 0.2919618493672145 valid 0.23755914616313847
LOSS train 0.2919618493672145 valid 0.23756651759595798
LOSS train 0.2919618493672145 valid 0.23787323274274372
LOSS train 0.2919618493672145 valid 0.23795598888838732
LOSS train 0.2919618493672145 valid 0.23796147929833217
LOSS train 0.2919618493672145 valid 0.23811286437685472
LOSS train 0.2919618493672145 valid 0.23793446628943735
LOSS train 0.2919618493672145 valid 0.23772519848329557
LOSS train 0.2919618493672145 valid 0.23767879179545812
LOSS train 0.2919618493672145 valid 0.2376902794584315
LOSS train 0.2919618493672145 valid 0.23802654256283398
LOSS train 0.2919618493672145 valid 0.23810292738717753
LOSS train 0.2919618493672145 valid 0.23801723381297457
LOSS train 0.2919618493672145 valid 0.23783563231599741
LOSS train 0.2919618493672145 valid 0.23775550529156644
LOSS train 0.2919618493672145 valid 0.2377266673003735
LOSS train 0.2919618493672145 valid 0.2375913800017254
LOSS train 0.2919618493672145 valid 0.23766845084676808
LOSS train 0.2919618493672145 valid 0.23765643825133642
LOSS train 0.2919618493672145 valid 0.23752668776259517
LOSS train 0.2919618493672145 valid 0.23739114070409223
LOSS train 0.2919618493672145 valid 0.237171796509643
LOSS train 0.2919618493672145 valid 0.23733844395194734
LOSS train 0.2919618493672145 valid 0.2373929541918539
LOSS train 0.2919618493672145 valid 0.2375848865470825
LOSS train 0.2919618493672145 valid 0.2374665774167723
LOSS train 0.2919618493672145 valid 0.2375491647214829
LOSS train 0.2919618493672145 valid 0.23784725724151298
LOSS train 0.2919618493672145 valid 0.23797917841002345
LOSS train 0.2919618493672145 valid 0.23794433816989757
LOSS train 0.2919618493672145 valid 0.23798989476980986
LOSS train 0.2919618493672145 valid 0.23771460683433557
LOSS train 0.2919618493672145 valid 0.23781394676827802
LOSS train 0.2919618493672145 valid 0.2378474505561771
LOSS train 0.2919618493672145 valid 0.2376789446157145
LOSS train 0.2919618493672145 valid 0.23782866345551198
LOSS train 0.2919618493672145 valid 0.2377167364493722
LOSS train 0.2919618493672145 valid 0.23746360133032826
LOSS train 0.2919618493672145 valid 0.23758234741056666
LOSS train 0.2919618493672145 valid 0.2374472221610142
LOSS train 0.2919618493672145 valid 0.23748360842812893
LOSS train 0.2919618493672145 valid 0.23757815352409561
LOSS train 0.2919618493672145 valid 0.23792679993243054
LOSS train 0.2919618493672145 valid 0.23789066808564321
LOSS train 0.2919618493672145 valid 0.23779093321751466
LOSS train 0.2919618493672145 valid 0.23785997295783737
LOSS train 0.2919618493672145 valid 0.23787930381766867
LOSS train 0.2919618493672145 valid 0.238010362956111
LOSS train 0.2919618493672145 valid 0.23799587140480677
LOSS train 0.2919618493672145 valid 0.2379261540115209
LOSS train 0.2919618493672145 valid 0.23779760554924115
LOSS train 0.2919618493672145 valid 0.23791591171684162
LOSS train 0.2919618493672145 valid 0.23791744090292766
LOSS train 0.2919618493672145 valid 0.2379644620257455
LOSS train 0.2919618493672145 valid 0.23800504648236817
LOSS train 0.2919618493672145 valid 0.23779506957467228
LOSS train 0.2919618493672145 valid 0.2378983125090599
LOSS train 0.2919618493672145 valid 0.2376938610480576
LOSS train 0.2919618493672145 valid 0.23769315308646152
LOSS train 0.2919618493672145 valid 0.23784274096888397
LOSS train 0.2919618493672145 valid 0.23793733802934489
LOSS train 0.2919618493672145 valid 0.23774403796912474
LOSS train 0.2919618493672145 valid 0.23768568100388518
LOSS train 0.2919618493672145 valid 0.2376290955604651
LOSS train 0.2919618493672145 valid 0.23786147455779874
LOSS train 0.2919618493672145 valid 0.23785654391129005
LOSS train 0.2919618493672145 valid 0.23813853661219278
LOSS train 0.2919618493672145 valid 0.2380967728756181
LOSS train 0.2919618493672145 valid 0.23827651411294937
LOSS train 0.2919618493672145 valid 0.2380681470851993
LOSS train 0.2919618493672145 valid 0.23805093868534163
LOSS train 0.2919618493672145 valid 0.23822561288114838
LOSS train 0.2919618493672145 valid 0.23813287044564882
LOSS train 0.2919618493672145 valid 0.23814517194178048
LOSS train 0.2919618493672145 valid 0.23806404770867337
LOSS train 0.2919618493672145 valid 0.23801062238101223
LOSS train 0.2919618493672145 valid 0.23782792035490274
LOSS train 0.2919618493672145 valid 0.23784415067382977
LOSS train 0.2919618493672145 valid 0.23794864863157272
LOSS train 0.2919618493672145 valid 0.23815016829854505
LOSS train 0.2919618493672145 valid 0.23794683989770007
LOSS train 0.2919618493672145 valid 0.23783346961641535
LOSS train 0.2919618493672145 valid 0.23773332643453207
LOSS train 0.2919618493672145 valid 0.23748359298983285
LOSS train 0.2919618493672145 valid 0.23729842321740258
LOSS train 0.2919618493672145 valid 0.2370285034729039
LOSS train 0.2919618493672145 valid 0.23706340810301108
LOSS train 0.2919618493672145 valid 0.23705683546523526
LOSS train 0.2919618493672145 valid 0.237122005359693
LOSS train 0.2919618493672145 valid 0.2371033685644288
LOSS train 0.2919618493672145 valid 0.23720548046870274
LOSS train 0.2919618493672145 valid 0.2373001970384153
LOSS train 0.2919618493672145 valid 0.2372133490363402
LOSS train 0.2919618493672145 valid 0.23703310231367747
LOSS train 0.2919618493672145 valid 0.23693115270770757
LOSS train 0.2919618493672145 valid 0.23693754054638783
LOSS train 0.2919618493672145 valid 0.23689343205146624
LOSS train 0.2919618493672145 valid 0.23675030046935686
LOSS train 0.2919618493672145 valid 0.2368008183396381
LOSS train 0.2919618493672145 valid 0.23674864718666325
LOSS train 0.2919618493672145 valid 0.23685573690153403
LOSS train 0.2919618493672145 valid 0.23718685839053388
LOSS train 0.2919618493672145 valid 0.23715120261041528
LOSS train 0.2919618493672145 valid 0.2372221661374924
LOSS train 0.2919618493672145 valid 0.23711394979539563
LOSS train 0.2919618493672145 valid 0.23716642170012753
LOSS train 0.2919618493672145 valid 0.2371534246857427
LOSS train 0.2919618493672145 valid 0.23718896894774177
LOSS train 0.2919618493672145 valid 0.2367511839295427
LOSS train 0.2919618493672145 valid 0.23656841667361278
LOSS train 0.2919618493672145 valid 0.23653006344294744
LOSS train 0.2919618493672145 valid 0.23654711160640168
LOSS train 0.2919618493672145 valid 0.23653346770366684
LOSS train 0.2919618493672145 valid 0.23668736128174528
LOSS train 0.2919618493672145 valid 0.2367447032555332
LOSS train 0.2919618493672145 valid 0.23678319699127182
LOSS train 0.2919618493672145 valid 0.23681234406127083
LOSS train 0.2919618493672145 valid 0.23670259417300243
LOSS train 0.2919618493672145 valid 0.23685314202308655
LOSS train 0.2919618493672145 valid 0.23691393132228775
LOSS train 0.2919618493672145 valid 0.23688591821562677
LOSS train 0.2919618493672145 valid 0.23672588287370477
LOSS train 0.2919618493672145 valid 0.23685536169865
LOSS train 0.2919618493672145 valid 0.2367375788151049
LOSS train 0.2919618493672145 valid 0.23664073063991964
LOSS train 0.2919618493672145 valid 0.23663012184753493
LOSS train 0.2919618493672145 valid 0.2367557102973147
LOSS train 0.2919618493672145 valid 0.2367228185002868
LOSS train 0.2919618493672145 valid 0.23663611773114937
LOSS train 0.2919618493672145 valid 0.23664549946556604
LOSS train 0.2919618493672145 valid 0.23681720346212387
LOSS train 0.2919618493672145 valid 0.2366705766308897
LOSS train 0.2919618493672145 valid 0.23673222637311978
LOSS train 0.2919618493672145 valid 0.23670013861836128
LOSS train 0.2919618493672145 valid 0.2367320918947234
LOSS train 0.2919618493672145 valid 0.2366449111856325
LOSS train 0.2919618493672145 valid 0.23668269890902646
LOSS train 0.2919618493672145 valid 0.23673828572145625
LOSS train 0.2919618493672145 valid 0.23662557469473944
LOSS train 0.2919618493672145 valid 0.2365949900057923
LOSS train 0.2919618493672145 valid 0.23651769718922236
LOSS train 0.2919618493672145 valid 0.23637085115953244
LOSS train 0.2919618493672145 valid 0.2363235605132841
LOSS train 0.2919618493672145 valid 0.2364543694257736
LOSS train 0.2919618493672145 valid 0.23654445849251057
LOSS train 0.2919618493672145 valid 0.23655931436413033
LOSS train 0.2919618493672145 valid 0.2365582652872415
LOSS train 0.2919618493672145 valid 0.23654919926838208
LOSS train 0.2919618493672145 valid 0.2364370158208268
LOSS train 0.2919618493672145 valid 0.23631491806371357
LOSS train 0.2919618493672145 valid 0.23634572443387186
LOSS train 0.2919618493672145 valid 0.23640240175985194
LOSS train 0.2919618493672145 valid 0.23631019669939096
LOSS train 0.2919618493672145 valid 0.2362900243516554
LOSS train 0.2919618493672145 valid 0.23628510326355487
LOSS train 0.2919618493672145 valid 0.2363172310569976
LOSS train 0.2919618493672145 valid 0.2362672889398204
LOSS train 0.2919618493672145 valid 0.23620433899151824
LOSS train 0.2919618493672145 valid 0.23611251033585648
LOSS train 0.2919618493672145 valid 0.23606120833416575
LOSS train 0.2919618493672145 valid 0.2359761893749237
LOSS train 0.2919618493672145 valid 0.23594346184779352
LOSS train 0.2919618493672145 valid 0.23598395511001147
LOSS train 0.2919618493672145 valid 0.23600262130721142
LOSS train 0.2919618493672145 valid 0.2359784161923705
LOSS train 0.2919618493672145 valid 0.23589341723758364
LOSS train 0.2919618493672145 valid 0.23592886453706946
LOSS train 0.2919618493672145 valid 0.2359028187483848
LOSS train 0.2919618493672145 valid 0.23580273429552714
LOSS train 0.2919618493672145 valid 0.23569126719256175
LOSS train 0.2919618493672145 valid 0.2356743591017281
LOSS train 0.2919618493672145 valid 0.23560526612961646
LOSS train 0.2919618493672145 valid 0.2356414109664528
LOSS train 0.2919618493672145 valid 0.23560630684016182
LOSS train 0.2919618493672145 valid 0.23585844492795421
LOSS train 0.2919618493672145 valid 0.23589016468012372
LOSS train 0.2919618493672145 valid 0.23593893613327632
LOSS train 0.2919618493672145 valid 0.23596177937336338
LOSS train 0.2919618493672145 valid 0.2359776894411733
LOSS train 0.2919618493672145 valid 0.23595569294748583
LOSS train 0.2919618493672145 valid 0.23591572800889993
LOSS train 0.2919618493672145 valid 0.2359322404709106
LOSS train 0.2919618493672145 valid 0.23583111949976843
LOSS train 0.2919618493672145 valid 0.2357905938511803
LOSS train 0.2919618493672145 valid 0.23579490562028524
LOSS train 0.2919618493672145 valid 0.23579966857418277
LOSS train 0.2919618493672145 valid 0.23581730621228428
LOSS train 0.2919618493672145 valid 0.23578952439713255
LOSS train 0.2919618493672145 valid 0.23567636562511324
LOSS train 0.2919618493672145 valid 0.23574236648105015
LOSS train 0.2919618493672145 valid 0.23571145145789438
LOSS train 0.2919618493672145 valid 0.23559682652492642
LOSS train 0.2919618493672145 valid 0.23552995225713577
LOSS train 0.2919618493672145 valid 0.23536279958028059
LOSS train 0.2919618493672145 valid 0.23547646078778192
LOSS train 0.2919618493672145 valid 0.23542707069386765
LOSS train 0.2919618493672145 valid 0.23542515865368088
LOSS train 0.2919618493672145 valid 0.23533727237699967
LOSS train 0.2919618493672145 valid 0.23535612826094482
LOSS train 0.2919618493672145 valid 0.23524106494249533
LOSS train 0.2919618493672145 valid 0.23522857733699212
LOSS train 0.2919618493672145 valid 0.2353333954786037
LOSS train 0.2919618493672145 valid 0.2352450444937466
LOSS train 0.2919618493672145 valid 0.23512924762804116
LOSS train 0.2919618493672145 valid 0.23502112095732064
LOSS train 0.2919618493672145 valid 0.23514110181558026
LOSS train 0.2919618493672145 valid 0.2351127450282757
LOSS train 0.2919618493672145 valid 0.23510796913003498
LOSS train 0.2919618493672145 valid 0.23519566260716496
LOSS train 0.2919618493672145 valid 0.2351293727711848
LOSS train 0.2919618493672145 valid 0.2350710478355313
LOSS train 0.2919618493672145 valid 0.23495583757540922
LOSS train 0.2919618493672145 valid 0.23502138867801012
LOSS train 0.2919618493672145 valid 0.23508472412392714
LOSS train 0.2919618493672145 valid 0.2350371168614123
LOSS train 0.2919618493672145 valid 0.234983885021993
LOSS train 0.2919618493672145 valid 0.23507814250629525
LOSS train 0.2919618493672145 valid 0.2350278176089754
LOSS train 0.2919618493672145 valid 0.23502371477229256
LOSS train 0.2919618493672145 valid 0.235117226881519
LOSS train 0.2919618493672145 valid 0.2351808111522008
LOSS train 0.2919618493672145 valid 0.23527743606155385
LOSS train 0.2919618493672145 valid 0.23527022194390917
LOSS train 0.2919618493672145 valid 0.23518819720812248
LOSS train 0.2919618493672145 valid 0.23520627341578515
LOSS train 0.2919618493672145 valid 0.23522356937245495
LOSS train 0.2919618493672145 valid 0.23519943675348878
LOSS train 0.2919618493672145 valid 0.23515677389826284
LOSS train 0.2919618493672145 valid 0.2352679482350747
LOSS train 0.2919618493672145 valid 0.23517750773238344
LOSS train 0.2919618493672145 valid 0.235239844007716
LOSS train 0.2919618493672145 valid 0.23530447708837915
LOSS train 0.2919618493672145 valid 0.23523454904392527
LOSS train 0.2919618493672145 valid 0.23535630502929425
LOSS train 0.2919618493672145 valid 0.2353098725930589
LOSS train 0.2919618493672145 valid 0.23528685406540656
LOSS train 0.2919618493672145 valid 0.23520035608469145
LOSS train 0.2919618493672145 valid 0.23519210141848743
EPOCH 7:
  batch 1 loss: 0.30683088302612305
  batch 2 loss: 0.32113511860370636
  batch 3 loss: 0.3104444642861684
  batch 4 loss: 0.31064029783010483
  batch 5 loss: 0.30949715375900266
  batch 6 loss: 0.3060080260038376
  batch 7 loss: 0.3048365158694131
  batch 8 loss: 0.2995058260858059
  batch 9 loss: 0.29850532942348057
  batch 10 loss: 0.29441727995872496
  batch 11 loss: 0.2941944057291204
  batch 12 loss: 0.29311008006334305
  batch 13 loss: 0.29076822445942807
  batch 14 loss: 0.29095399379730225
  batch 15 loss: 0.2918026983737946
  batch 16 loss: 0.29079864732921124
  batch 17 loss: 0.2911718242308673
  batch 18 loss: 0.2903027484814326
  batch 19 loss: 0.29084076222620514
  batch 20 loss: 0.28945285975933077
  batch 21 loss: 0.29205444313230966
  batch 22 loss: 0.29246157949621027
  batch 23 loss: 0.29079570977584174
  batch 24 loss: 0.29012330373128253
  batch 25 loss: 0.2913295352458954
  batch 26 loss: 0.2911951312652001
  batch 27 loss: 0.2929705690454554
  batch 28 loss: 0.2949400395154953
  batch 29 loss: 0.2943368307475386
  batch 30 loss: 0.2950053890546163
  batch 31 loss: 0.2955347578371725
  batch 32 loss: 0.2967028757557273
  batch 33 loss: 0.29807225863138836
  batch 34 loss: 0.298033645047861
  batch 35 loss: 0.29936000875064306
  batch 36 loss: 0.2987862270739343
  batch 37 loss: 0.2983351390104036
  batch 38 loss: 0.29840476183514847
  batch 39 loss: 0.29845188290644914
  batch 40 loss: 0.297946372628212
  batch 41 loss: 0.2972626853279951
  batch 42 loss: 0.2966238387993404
  batch 43 loss: 0.2964124478573023
  batch 44 loss: 0.2968995212153955
  batch 45 loss: 0.2967485964298248
  batch 46 loss: 0.29524446375991986
  batch 47 loss: 0.2948520912768993
  batch 48 loss: 0.2952835001051426
  batch 49 loss: 0.29612330392915376
  batch 50 loss: 0.29629297614097594
  batch 51 loss: 0.2958608117758059
  batch 52 loss: 0.2957371278451039
  batch 53 loss: 0.2951573347145656
  batch 54 loss: 0.2947309916770017
  batch 55 loss: 0.295033076134595
  batch 56 loss: 0.29576372674533297
  batch 57 loss: 0.2951009132360157
  batch 58 loss: 0.29465258224257107
  batch 59 loss: 0.29465902962927093
  batch 60 loss: 0.2945595656832059
  batch 61 loss: 0.2950779013946408
  batch 62 loss: 0.2953023189498532
  batch 63 loss: 0.2948077581231556
  batch 64 loss: 0.2946182396262884
  batch 65 loss: 0.2938868903196775
  batch 66 loss: 0.2938158241185275
  batch 67 loss: 0.2933577494834786
  batch 68 loss: 0.2933557528783293
  batch 69 loss: 0.2931020462858504
  batch 70 loss: 0.29263498953410555
  batch 71 loss: 0.29285651277488384
  batch 72 loss: 0.2930092554953363
  batch 73 loss: 0.2928726546568413
  batch 74 loss: 0.2922676119047242
  batch 75 loss: 0.29208024164040886
  batch 76 loss: 0.2921530716121197
  batch 77 loss: 0.29182678790061506
  batch 78 loss: 0.29204416217712253
  batch 79 loss: 0.29206612385526487
  batch 80 loss: 0.2919544132426381
  batch 81 loss: 0.29187156039255635
  batch 82 loss: 0.2920546449902581
  batch 83 loss: 0.2917994804770114
  batch 84 loss: 0.2912523741168635
  batch 85 loss: 0.2917110704323825
  batch 86 loss: 0.2917162829706835
  batch 87 loss: 0.2915897950016219
  batch 88 loss: 0.29145804911174555
  batch 89 loss: 0.29080535906754185
  batch 90 loss: 0.29125795281595657
  batch 91 loss: 0.29085356570207155
  batch 92 loss: 0.29060712570081587
  batch 93 loss: 0.29014666058043
  batch 94 loss: 0.2901098685378724
  batch 95 loss: 0.29011370963171906
  batch 96 loss: 0.29008706643556553
  batch 97 loss: 0.2903843336191374
  batch 98 loss: 0.29016501030751635
  batch 99 loss: 0.2900446103979843
  batch 100 loss: 0.29009683594107627
  batch 101 loss: 0.2899502632346484
  batch 102 loss: 0.290518187278626
  batch 103 loss: 0.2904566328213053
  batch 104 loss: 0.2909892520938928
  batch 105 loss: 0.2910769134759903
  batch 106 loss: 0.2912892925570596
  batch 107 loss: 0.2913027621596773
  batch 108 loss: 0.29112198272788964
  batch 109 loss: 0.2913436932027887
  batch 110 loss: 0.2913352526047013
  batch 111 loss: 0.2913692871461043
  batch 112 loss: 0.2911426910598363
  batch 113 loss: 0.29090649034597177
  batch 114 loss: 0.29059966496731104
  batch 115 loss: 0.2902150353659754
  batch 116 loss: 0.2899092299671009
  batch 117 loss: 0.2896719656438909
  batch 118 loss: 0.28973701343698016
  batch 119 loss: 0.28992965842495444
  batch 120 loss: 0.2896151214838028
  batch 121 loss: 0.289926085590331
  batch 122 loss: 0.289802005056475
  batch 123 loss: 0.2896451465482634
  batch 124 loss: 0.289628014208809
  batch 125 loss: 0.28935836791992187
  batch 126 loss: 0.2893921938680467
  batch 127 loss: 0.28966538995269714
  batch 128 loss: 0.2897657488938421
  batch 129 loss: 0.2898056879062061
  batch 130 loss: 0.28977944598748134
  batch 131 loss: 0.28958859675713167
  batch 132 loss: 0.2896727036797639
  batch 133 loss: 0.2895184372152601
  batch 134 loss: 0.2893780249713072
  batch 135 loss: 0.2893261913900022
  batch 136 loss: 0.28966818881385464
  batch 137 loss: 0.2899770564841528
  batch 138 loss: 0.2900718491578448
  batch 139 loss: 0.29018282804557743
  batch 140 loss: 0.29061057439872195
  batch 141 loss: 0.29063637797714126
  batch 142 loss: 0.2904943979122269
  batch 143 loss: 0.2907033014547575
  batch 144 loss: 0.29064482553965515
  batch 145 loss: 0.2907847353096666
  batch 146 loss: 0.29093327852961137
  batch 147 loss: 0.2909100937600039
  batch 148 loss: 0.29087764528152105
  batch 149 loss: 0.2904928068586644
  batch 150 loss: 0.29076437413692474
  batch 151 loss: 0.29065366987361024
  batch 152 loss: 0.2904485811136271
  batch 153 loss: 0.29092930734547134
  batch 154 loss: 0.29102285186965743
  batch 155 loss: 0.2912078709371628
  batch 156 loss: 0.29108832165216786
  batch 157 loss: 0.2911118331608499
  batch 158 loss: 0.29136057631878914
  batch 159 loss: 0.29167900651505907
  batch 160 loss: 0.2915622005239129
  batch 161 loss: 0.2911955101519638
  batch 162 loss: 0.2911881368469309
  batch 163 loss: 0.2910575091473164
  batch 164 loss: 0.2912928607405686
  batch 165 loss: 0.2915017512711612
  batch 166 loss: 0.29182990398033554
  batch 167 loss: 0.29178635856348595
  batch 168 loss: 0.2916017242130779
  batch 169 loss: 0.2917213425833798
  batch 170 loss: 0.2918194847948411
  batch 171 loss: 0.2920575791980788
  batch 172 loss: 0.29199244237916416
  batch 173 loss: 0.29214867415455725
  batch 174 loss: 0.2922560000556639
  batch 175 loss: 0.2923578909465245
  batch 176 loss: 0.29258517785505816
  batch 177 loss: 0.2928289144726123
  batch 178 loss: 0.29282742119237276
  batch 179 loss: 0.29288866250208634
  batch 180 loss: 0.2928306384219064
  batch 181 loss: 0.2926994229548544
  batch 182 loss: 0.2926327805597704
  batch 183 loss: 0.2925828740896423
  batch 184 loss: 0.29246640561715415
  batch 185 loss: 0.2922674746126742
  batch 186 loss: 0.292303099747627
  batch 187 loss: 0.2924216472528835
  batch 188 loss: 0.2923705948794142
  batch 189 loss: 0.2923481417080713
  batch 190 loss: 0.2922968687195527
  batch 191 loss: 0.29235180822342477
  batch 192 loss: 0.29211140602516633
  batch 193 loss: 0.29218021413514034
  batch 194 loss: 0.29201187846279636
  batch 195 loss: 0.29210120600003464
  batch 196 loss: 0.2920797629167839
  batch 197 loss: 0.29204685184253654
  batch 198 loss: 0.2920488410375335
  batch 199 loss: 0.29201886035389635
  batch 200 loss: 0.29208972536027433
  batch 201 loss: 0.2919731597550473
  batch 202 loss: 0.29192226527646037
  batch 203 loss: 0.29193519129248086
  batch 204 loss: 0.2919239031479639
  batch 205 loss: 0.2916861825599903
  batch 206 loss: 0.29177870688218516
  batch 207 loss: 0.291611820243407
  batch 208 loss: 0.29138146054286224
  batch 209 loss: 0.2914588555479734
  batch 210 loss: 0.29138480793862115
  batch 211 loss: 0.291361539990981
  batch 212 loss: 0.29129001301414564
  batch 213 loss: 0.29113139317069253
  batch 214 loss: 0.291049759800189
  batch 215 loss: 0.29107960171477737
  batch 216 loss: 0.2911127126879162
  batch 217 loss: 0.2909923322189788
  batch 218 loss: 0.29080748995509714
  batch 219 loss: 0.29054555188030956
  batch 220 loss: 0.2905758780511943
  batch 221 loss: 0.2905167025678298
  batch 222 loss: 0.2905120523394765
  batch 223 loss: 0.2902772156646968
  batch 224 loss: 0.29026683353419813
  batch 225 loss: 0.2903886600335439
  batch 226 loss: 0.2904221908708589
  batch 227 loss: 0.2902849610681576
  batch 228 loss: 0.2903010318415207
  batch 229 loss: 0.2902370224092725
  batch 230 loss: 0.2901106795539027
  batch 231 loss: 0.2901135412129489
  batch 232 loss: 0.2901490980695034
  batch 233 loss: 0.2900638682647836
  batch 234 loss: 0.2900065099581694
  batch 235 loss: 0.2899533933781563
  batch 236 loss: 0.2899693191304045
  batch 237 loss: 0.2897750906300444
  batch 238 loss: 0.28973113889453794
  batch 239 loss: 0.28968989849090576
  batch 240 loss: 0.28968742278714976
  batch 241 loss: 0.2895596499759627
  batch 242 loss: 0.2894957693647747
  batch 243 loss: 0.28947980524090583
  batch 244 loss: 0.28938986812947226
  batch 245 loss: 0.2893995896894105
  batch 246 loss: 0.2893736068068481
  batch 247 loss: 0.28938261965508405
  batch 248 loss: 0.28952770656155
  batch 249 loss: 0.28940117359161377
  batch 250 loss: 0.28937923622131345
  batch 251 loss: 0.28922825707382416
  batch 252 loss: 0.2891488312965348
  batch 253 loss: 0.28902700743656384
  batch 254 loss: 0.28896164307444117
  batch 255 loss: 0.288837806968128
  batch 256 loss: 0.2887546658748761
  batch 257 loss: 0.2888892880905463
  batch 258 loss: 0.2888156039308208
  batch 259 loss: 0.2885753291454094
  batch 260 loss: 0.28860907600476193
  batch 261 loss: 0.28856845514070945
  batch 262 loss: 0.2885614651760072
  batch 263 loss: 0.2884588196250422
  batch 264 loss: 0.28858858245340263
  batch 265 loss: 0.2885570486761489
  batch 266 loss: 0.28841458126566466
  batch 267 loss: 0.2882558200578118
  batch 268 loss: 0.2880887862906527
  batch 269 loss: 0.2880767152876659
  batch 270 loss: 0.28807135423024494
  batch 271 loss: 0.28802950025924456
  batch 272 loss: 0.2880063254167052
  batch 273 loss: 0.2879136207994524
  batch 274 loss: 0.28774935855482614
  batch 275 loss: 0.28775884541598235
  batch 276 loss: 0.2877488853274912
  batch 277 loss: 0.28773897423640915
  batch 278 loss: 0.287613008626931
  batch 279 loss: 0.2876531635561297
  batch 280 loss: 0.2875422243561063
  batch 281 loss: 0.287510654362071
  batch 282 loss: 0.28735470364913873
  batch 283 loss: 0.28723535652506055
  batch 284 loss: 0.2873476150275116
  batch 285 loss: 0.28718789638134473
  batch 286 loss: 0.2871939724350309
  batch 287 loss: 0.28704870915371367
  batch 288 loss: 0.2870177616779175
  batch 289 loss: 0.2868528087658866
  batch 290 loss: 0.28673003007625714
  batch 291 loss: 0.2867090028176193
  batch 292 loss: 0.2866925992173691
  batch 293 loss: 0.28668911790685037
  batch 294 loss: 0.28649417138626787
  batch 295 loss: 0.2864834128294961
  batch 296 loss: 0.2866021177756625
  batch 297 loss: 0.28654796572446023
  batch 298 loss: 0.2866215741374349
  batch 299 loss: 0.2867048091214636
  batch 300 loss: 0.2865998404721419
  batch 301 loss: 0.2866405170918303
  batch 302 loss: 0.2867847097334483
  batch 303 loss: 0.2867305924495061
  batch 304 loss: 0.2867133015962808
  batch 305 loss: 0.28670263422317194
  batch 306 loss: 0.2868039545183088
  batch 307 loss: 0.28676478448248066
  batch 308 loss: 0.28685759893291957
  batch 309 loss: 0.2868037175109857
  batch 310 loss: 0.28685051322944705
  batch 311 loss: 0.28666770975689415
  batch 312 loss: 0.2865938792626063
  batch 313 loss: 0.2864747287366337
  batch 314 loss: 0.28637188729966523
  batch 315 loss: 0.2863482597328368
  batch 316 loss: 0.2863412530550474
  batch 317 loss: 0.28640053315493585
  batch 318 loss: 0.28617697279408294
  batch 319 loss: 0.28597037386744745
  batch 320 loss: 0.28606311045587063
  batch 321 loss: 0.28604463030616073
  batch 322 loss: 0.2858802050999973
  batch 323 loss: 0.2858766849288261
  batch 324 loss: 0.28592289151784817
  batch 325 loss: 0.2860332903953699
  batch 326 loss: 0.2862959707532924
  batch 327 loss: 0.2862182537988056
  batch 328 loss: 0.28615170794471007
  batch 329 loss: 0.2861374131542571
  batch 330 loss: 0.2861322392568444
  batch 331 loss: 0.2860543107968443
  batch 332 loss: 0.2860881002642304
  batch 333 loss: 0.28603238283513904
  batch 334 loss: 0.2862562732157593
  batch 335 loss: 0.2862765294847204
  batch 336 loss: 0.2862979402056053
  batch 337 loss: 0.2862888244298516
  batch 338 loss: 0.28618174980907046
  batch 339 loss: 0.28614984628549367
  batch 340 loss: 0.28602382702862517
  batch 341 loss: 0.2859767570086588
  batch 342 loss: 0.28590645797942815
  batch 343 loss: 0.2859909722565214
  batch 344 loss: 0.2860122065298086
  batch 345 loss: 0.28603839533052583
  batch 346 loss: 0.28598924872675385
  batch 347 loss: 0.28591940635734747
  batch 348 loss: 0.2859154100818881
  batch 349 loss: 0.28591513783166606
  batch 350 loss: 0.28600864976644513
  batch 351 loss: 0.28598824886345114
  batch 352 loss: 0.2859073001468046
  batch 353 loss: 0.2858759451435916
  batch 354 loss: 0.28579773324525964
  batch 355 loss: 0.2857714059906946
  batch 356 loss: 0.2858375535670961
  batch 357 loss: 0.2856276074126989
  batch 358 loss: 0.2855228870357881
  batch 359 loss: 0.2855602904199558
  batch 360 loss: 0.28547253306541176
  batch 361 loss: 0.2854223780932519
  batch 362 loss: 0.28536275099160263
  batch 363 loss: 0.285407790755109
  batch 364 loss: 0.2853996741820823
  batch 365 loss: 0.2853859419283802
  batch 366 loss: 0.28544754013826285
  batch 367 loss: 0.2855448321279453
  batch 368 loss: 0.28552655693467544
  batch 369 loss: 0.28546783735920095
  batch 370 loss: 0.28565599342455733
  batch 371 loss: 0.2857052836055062
  batch 372 loss: 0.2857754638678925
  batch 373 loss: 0.2858061062986026
  batch 374 loss: 0.28581405359315365
  batch 375 loss: 0.28586845846970876
  batch 376 loss: 0.28597205096578343
  batch 377 loss: 0.28598091832364586
  batch 378 loss: 0.2858761139805355
  batch 379 loss: 0.28588512893717016
  batch 380 loss: 0.2858131967092815
  batch 381 loss: 0.2857453462489321
  batch 382 loss: 0.28564150124320187
  batch 383 loss: 0.2854807250026623
  batch 384 loss: 0.2854132365901023
  batch 385 loss: 0.28535218215607977
  batch 386 loss: 0.28532450788997
  batch 387 loss: 0.2852901559159429
  batch 388 loss: 0.28520731474320915
  batch 389 loss: 0.28526185824509454
  batch 390 loss: 0.2852387997584465
  batch 391 loss: 0.2852044517884169
  batch 392 loss: 0.2852405912565942
  batch 393 loss: 0.28531247096813966
  batch 394 loss: 0.2852708811536053
  batch 395 loss: 0.2852335557907443
  batch 396 loss: 0.2852615385194017
  batch 397 loss: 0.2852948259946982
  batch 398 loss: 0.2853086057619833
  batch 399 loss: 0.28527581848596273
  batch 400 loss: 0.28539857268333435
  batch 401 loss: 0.28533423251641954
  batch 402 loss: 0.28534203478649484
  batch 403 loss: 0.28526626989504245
  batch 404 loss: 0.28519564564570343
  batch 405 loss: 0.285207098428114
  batch 406 loss: 0.2851110544844801
  batch 407 loss: 0.28512705756346773
  batch 408 loss: 0.2849952129318434
  batch 409 loss: 0.2849361373334759
  batch 410 loss: 0.2848757606454012
  batch 411 loss: 0.2848717202670383
  batch 412 loss: 0.28501679444486655
  batch 413 loss: 0.285031218361335
  batch 414 loss: 0.2850843932893541
  batch 415 loss: 0.28505883432296386
  batch 416 loss: 0.2850765078686751
  batch 417 loss: 0.2850353434789095
  batch 418 loss: 0.2850262512002835
  batch 419 loss: 0.2849898424381857
  batch 420 loss: 0.28493886653866085
  batch 421 loss: 0.28500435067752194
  batch 422 loss: 0.28496313681252194
  batch 423 loss: 0.2849010029724022
  batch 424 loss: 0.2849740852724831
  batch 425 loss: 0.2848647496630164
  batch 426 loss: 0.28482333549731215
  batch 427 loss: 0.28486422747722556
  batch 428 loss: 0.28487662473153846
  batch 429 loss: 0.284800905621413
  batch 430 loss: 0.2848383503944375
  batch 431 loss: 0.28483618788957044
  batch 432 loss: 0.28481388557702303
  batch 433 loss: 0.2847430483367096
  batch 434 loss: 0.28465055962724073
  batch 435 loss: 0.2847124021286252
  batch 436 loss: 0.2847494619360211
  batch 437 loss: 0.28469787599429386
  batch 438 loss: 0.2846290308250684
  batch 439 loss: 0.2845306955193061
  batch 440 loss: 0.2846553368324583
  batch 441 loss: 0.28466424521675454
  batch 442 loss: 0.2846235961261378
  batch 443 loss: 0.2845972830904795
  batch 444 loss: 0.28465094222678794
  batch 445 loss: 0.28459917856066413
  batch 446 loss: 0.284665244978105
  batch 447 loss: 0.28466069251632264
  batch 448 loss: 0.2847077320329845
  batch 449 loss: 0.28461283863520037
  batch 450 loss: 0.2845878838830524
  batch 451 loss: 0.2845215884650626
  batch 452 loss: 0.2844245724577819
  batch 453 loss: 0.28445282933727795
  batch 454 loss: 0.28446875427262897
  batch 455 loss: 0.2844712163720812
  batch 456 loss: 0.284462249396663
  batch 457 loss: 0.2844607465507165
  batch 458 loss: 0.2844012179228937
  batch 459 loss: 0.2843508624303315
  batch 460 loss: 0.2843664844398913
  batch 461 loss: 0.28436054958438667
  batch 462 loss: 0.28437749880216856
  batch 463 loss: 0.28442435915990216
  batch 464 loss: 0.2843580502888252
  batch 465 loss: 0.28437802093003384
  batch 466 loss: 0.2842647594123951
  batch 467 loss: 0.28441025393295083
  batch 468 loss: 0.28441022059474236
  batch 469 loss: 0.2843025862408091
  batch 470 loss: 0.2844755629909799
  batch 471 loss: 0.2845404086725474
  batch 472 loss: 0.2844197115845094
LOSS train 0.2844197115845094 valid 0.2346099317073822
LOSS train 0.2844197115845094 valid 0.22920949757099152
LOSS train 0.2844197115845094 valid 0.23221312959988913
LOSS train 0.2844197115845094 valid 0.21645299345254898
LOSS train 0.2844197115845094 valid 0.21861608028411866
LOSS train 0.2844197115845094 valid 0.22300948450962702
LOSS train 0.2844197115845094 valid 0.2184366945709501
LOSS train 0.2844197115845094 valid 0.21727722696959972
LOSS train 0.2844197115845094 valid 0.21607488559352028
LOSS train 0.2844197115845094 valid 0.2157840311527252
LOSS train 0.2844197115845094 valid 0.21576701240106064
LOSS train 0.2844197115845094 valid 0.21975057820479074
LOSS train 0.2844197115845094 valid 0.2192395833822397
LOSS train 0.2844197115845094 valid 0.21700494842869894
LOSS train 0.2844197115845094 valid 0.21635779639085134
LOSS train 0.2844197115845094 valid 0.2196167716756463
LOSS train 0.2844197115845094 valid 0.21974188615294063
LOSS train 0.2844197115845094 valid 0.21911068757375082
LOSS train 0.2844197115845094 valid 0.2207754771960409
LOSS train 0.2844197115845094 valid 0.22089151293039322
LOSS train 0.2844197115845094 valid 0.2219246370451791
LOSS train 0.2844197115845094 valid 0.22177733210000125
LOSS train 0.2844197115845094 valid 0.22001255271227463
LOSS train 0.2844197115845094 valid 0.22072933117548624
LOSS train 0.2844197115845094 valid 0.2204866749048233
LOSS train 0.2844197115845094 valid 0.21971537573979452
LOSS train 0.2844197115845094 valid 0.21986367967393664
LOSS train 0.2844197115845094 valid 0.22001428955367633
LOSS train 0.2844197115845094 valid 0.2186066590506455
LOSS train 0.2844197115845094 valid 0.21851093570391336
LOSS train 0.2844197115845094 valid 0.21874605503774458
LOSS train 0.2844197115845094 valid 0.2192750396206975
LOSS train 0.2844197115845094 valid 0.21853237757177063
LOSS train 0.2844197115845094 valid 0.21823246119653478
LOSS train 0.2844197115845094 valid 0.2188526762383325
LOSS train 0.2844197115845094 valid 0.21971877705719736
LOSS train 0.2844197115845094 valid 0.22005574848200823
LOSS train 0.2844197115845094 valid 0.2199569293542912
LOSS train 0.2844197115845094 valid 0.22097420119322264
LOSS train 0.2844197115845094 valid 0.22131863310933114
LOSS train 0.2844197115845094 valid 0.22099559772305372
LOSS train 0.2844197115845094 valid 0.2223544418811798
LOSS train 0.2844197115845094 valid 0.22274126424345858
LOSS train 0.2844197115845094 valid 0.22209773754531686
LOSS train 0.2844197115845094 valid 0.22172803183396658
LOSS train 0.2844197115845094 valid 0.22134917650533759
LOSS train 0.2844197115845094 valid 0.22115898322551808
LOSS train 0.2844197115845094 valid 0.22265291896959147
LOSS train 0.2844197115845094 valid 0.22192329776530362
LOSS train 0.2844197115845094 valid 0.2224453842639923
LOSS train 0.2844197115845094 valid 0.22213181502678814
LOSS train 0.2844197115845094 valid 0.22173494157882837
LOSS train 0.2844197115845094 valid 0.22321372043411686
LOSS train 0.2844197115845094 valid 0.22314024330289275
LOSS train 0.2844197115845094 valid 0.2227957094257528
LOSS train 0.2844197115845094 valid 0.2229635255145175
LOSS train 0.2844197115845094 valid 0.22228591196369707
LOSS train 0.2844197115845094 valid 0.22298801741723356
LOSS train 0.2844197115845094 valid 0.222635824043872
LOSS train 0.2844197115845094 valid 0.22244319890936215
LOSS train 0.2844197115845094 valid 0.22253897615143511
LOSS train 0.2844197115845094 valid 0.2222677370713603
LOSS train 0.2844197115845094 valid 0.2221420986784829
LOSS train 0.2844197115845094 valid 0.2223593534436077
LOSS train 0.2844197115845094 valid 0.22136021233521974
LOSS train 0.2844197115845094 valid 0.22134943599953796
LOSS train 0.2844197115845094 valid 0.22184543178152683
LOSS train 0.2844197115845094 valid 0.2211836306926082
LOSS train 0.2844197115845094 valid 0.22181370184905286
LOSS train 0.2844197115845094 valid 0.22205772336040225
LOSS train 0.2844197115845094 valid 0.22231465808942286
LOSS train 0.2844197115845094 valid 0.2227347060624096
LOSS train 0.2844197115845094 valid 0.22331438142142884
LOSS train 0.2844197115845094 valid 0.22314130051715955
LOSS train 0.2844197115845094 valid 0.22273380815982818
LOSS train 0.2844197115845094 valid 0.22273906006624825
LOSS train 0.2844197115845094 valid 0.2226229937045605
LOSS train 0.2844197115845094 valid 0.22241984384182173
LOSS train 0.2844197115845094 valid 0.22249105082282536
LOSS train 0.2844197115845094 valid 0.22197466026991605
LOSS train 0.2844197115845094 valid 0.2221798214279575
LOSS train 0.2844197115845094 valid 0.22211640655267528
LOSS train 0.2844197115845094 valid 0.22224225157714753
LOSS train 0.2844197115845094 valid 0.2220221133459182
LOSS train 0.2844197115845094 valid 0.2224162873099832
LOSS train 0.2844197115845094 valid 0.2222320196586986
LOSS train 0.2844197115845094 valid 0.22172448741293502
LOSS train 0.2844197115845094 valid 0.22178475609557194
LOSS train 0.2844197115845094 valid 0.22197706578822618
LOSS train 0.2844197115845094 valid 0.2221467536356714
LOSS train 0.2844197115845094 valid 0.2221153661772445
LOSS train 0.2844197115845094 valid 0.22214745163269664
LOSS train 0.2844197115845094 valid 0.22202866612583078
LOSS train 0.2844197115845094 valid 0.22216105255040716
LOSS train 0.2844197115845094 valid 0.22245945130523884
LOSS train 0.2844197115845094 valid 0.2226903250751396
LOSS train 0.2844197115845094 valid 0.22280410777047738
LOSS train 0.2844197115845094 valid 0.22310870019148807
LOSS train 0.2844197115845094 valid 0.22326300496404822
LOSS train 0.2844197115845094 valid 0.22332112714648247
LOSS train 0.2844197115845094 valid 0.22333517950950282
LOSS train 0.2844197115845094 valid 0.22375986520566193
LOSS train 0.2844197115845094 valid 0.22343553269951089
LOSS train 0.2844197115845094 valid 0.22331603759756455
LOSS train 0.2844197115845094 valid 0.22353102422895885
LOSS train 0.2844197115845094 valid 0.22343698889017105
LOSS train 0.2844197115845094 valid 0.22313260767504434
LOSS train 0.2844197115845094 valid 0.22303926020308776
LOSS train 0.2844197115845094 valid 0.22266079608453523
LOSS train 0.2844197115845094 valid 0.22271214452656832
LOSS train 0.2844197115845094 valid 0.22304285807652516
LOSS train 0.2844197115845094 valid 0.2233743236533233
LOSS train 0.2844197115845094 valid 0.22322419101158075
LOSS train 0.2844197115845094 valid 0.22324640512989277
LOSS train 0.2844197115845094 valid 0.22388156173021898
LOSS train 0.2844197115845094 valid 0.22349958532843098
LOSS train 0.2844197115845094 valid 0.22396810747619367
LOSS train 0.2844197115845094 valid 0.22384328268847223
LOSS train 0.2844197115845094 valid 0.2235800925673557
LOSS train 0.2844197115845094 valid 0.22327370531857013
LOSS train 0.2844197115845094 valid 0.22324786833987748
LOSS train 0.2844197115845094 valid 0.2234686321166695
LOSS train 0.2844197115845094 valid 0.2235101755799317
LOSS train 0.2844197115845094 valid 0.22362808090063832
LOSS train 0.2844197115845094 valid 0.22357065308094023
LOSS train 0.2844197115845094 valid 0.22374897804998217
LOSS train 0.2844197115845094 valid 0.2236208677526534
LOSS train 0.2844197115845094 valid 0.22352592658717185
LOSS train 0.2844197115845094 valid 0.22329715714436169
LOSS train 0.2844197115845094 valid 0.2229877015719047
LOSS train 0.2844197115845094 valid 0.2228803759767809
LOSS train 0.2844197115845094 valid 0.22285710636413458
LOSS train 0.2844197115845094 valid 0.22284970975908122
LOSS train 0.2844197115845094 valid 0.22317521685539787
LOSS train 0.2844197115845094 valid 0.22319983343283337
LOSS train 0.2844197115845094 valid 0.2232226453283254
LOSS train 0.2844197115845094 valid 0.22335891371225788
LOSS train 0.2844197115845094 valid 0.22321398459050967
LOSS train 0.2844197115845094 valid 0.22301008161023367
LOSS train 0.2844197115845094 valid 0.22293738424777984
LOSS train 0.2844197115845094 valid 0.22295463581879935
LOSS train 0.2844197115845094 valid 0.22326825905433842
LOSS train 0.2844197115845094 valid 0.2233620033814357
LOSS train 0.2844197115845094 valid 0.2233095938960711
LOSS train 0.2844197115845094 valid 0.22318588084188
LOSS train 0.2844197115845094 valid 0.22305053156124402
LOSS train 0.2844197115845094 valid 0.2229813656636647
LOSS train 0.2844197115845094 valid 0.22287436202168465
LOSS train 0.2844197115845094 valid 0.2228893585653113
LOSS train 0.2844197115845094 valid 0.22288186778624852
LOSS train 0.2844197115845094 valid 0.22275426164762863
LOSS train 0.2844197115845094 valid 0.22262525254566418
LOSS train 0.2844197115845094 valid 0.2224172242716247
LOSS train 0.2844197115845094 valid 0.2225488352698165
LOSS train 0.2844197115845094 valid 0.22256747734162116
LOSS train 0.2844197115845094 valid 0.22270330309103697
LOSS train 0.2844197115845094 valid 0.22257665407126118
LOSS train 0.2844197115845094 valid 0.22264557699613932
LOSS train 0.2844197115845094 valid 0.2228980688553936
LOSS train 0.2844197115845094 valid 0.22300380719825624
LOSS train 0.2844197115845094 valid 0.2230098030952193
LOSS train 0.2844197115845094 valid 0.22308018840389487
LOSS train 0.2844197115845094 valid 0.2228322499003147
LOSS train 0.2844197115845094 valid 0.22288545003024543
LOSS train 0.2844197115845094 valid 0.22292720702561467
LOSS train 0.2844197115845094 valid 0.2227986302361431
LOSS train 0.2844197115845094 valid 0.22297266179216121
LOSS train 0.2844197115845094 valid 0.22287071895386493
LOSS train 0.2844197115845094 valid 0.22264051208129296
LOSS train 0.2844197115845094 valid 0.2227541785906343
LOSS train 0.2844197115845094 valid 0.22263768873019527
LOSS train 0.2844197115845094 valid 0.22265396327819936
LOSS train 0.2844197115845094 valid 0.22271996331697255
LOSS train 0.2844197115845094 valid 0.2229961846237895
LOSS train 0.2844197115845094 valid 0.2229245035137449
LOSS train 0.2844197115845094 valid 0.22282099673016506
LOSS train 0.2844197115845094 valid 0.22285621811104359
LOSS train 0.2844197115845094 valid 0.22285295813606026
LOSS train 0.2844197115845094 valid 0.22298793869311584
LOSS train 0.2844197115845094 valid 0.22293886029058033
LOSS train 0.2844197115845094 valid 0.22288043615897057
LOSS train 0.2844197115845094 valid 0.22277407225344206
LOSS train 0.2844197115845094 valid 0.22288604732093914
LOSS train 0.2844197115845094 valid 0.22288876051164191
LOSS train 0.2844197115845094 valid 0.22294182076647476
LOSS train 0.2844197115845094 valid 0.22298418161689595
LOSS train 0.2844197115845094 valid 0.22281303762752105
LOSS train 0.2844197115845094 valid 0.22292603805978248
LOSS train 0.2844197115845094 valid 0.2227449570856397
LOSS train 0.2844197115845094 valid 0.22275353238770837
LOSS train 0.2844197115845094 valid 0.22291310412409418
LOSS train 0.2844197115845094 valid 0.22300676776406667
LOSS train 0.2844197115845094 valid 0.2228129582547153
LOSS train 0.2844197115845094 valid 0.22276403393942057
LOSS train 0.2844197115845094 valid 0.2227096948868189
LOSS train 0.2844197115845094 valid 0.22288358637264796
LOSS train 0.2844197115845094 valid 0.2228733974362388
LOSS train 0.2844197115845094 valid 0.22312415383680903
LOSS train 0.2844197115845094 valid 0.2230839065121646
LOSS train 0.2844197115845094 valid 0.2232412927597761
LOSS train 0.2844197115845094 valid 0.22303150571993927
LOSS train 0.2844197115845094 valid 0.22298740389028399
LOSS train 0.2844197115845094 valid 0.22314766086087437
LOSS train 0.2844197115845094 valid 0.22307525538638526
LOSS train 0.2844197115845094 valid 0.22306606951283245
LOSS train 0.2844197115845094 valid 0.22302565333044644
LOSS train 0.2844197115845094 valid 0.22298065540583237
LOSS train 0.2844197115845094 valid 0.22283538130040353
LOSS train 0.2844197115845094 valid 0.2228179523391587
LOSS train 0.2844197115845094 valid 0.22290810587860288
LOSS train 0.2844197115845094 valid 0.22311466861675136
LOSS train 0.2844197115845094 valid 0.22293421548773656
LOSS train 0.2844197115845094 valid 0.22284780554648295
LOSS train 0.2844197115845094 valid 0.22272433256991556
LOSS train 0.2844197115845094 valid 0.22249560203663138
LOSS train 0.2844197115845094 valid 0.22233993422102044
LOSS train 0.2844197115845094 valid 0.22212881632664236
LOSS train 0.2844197115845094 valid 0.2221670148022678
LOSS train 0.2844197115845094 valid 0.22216104685443722
LOSS train 0.2844197115845094 valid 0.2222079306163571
LOSS train 0.2844197115845094 valid 0.22220649984896992
LOSS train 0.2844197115845094 valid 0.22228999872196903
LOSS train 0.2844197115845094 valid 0.22238502360780144
LOSS train 0.2844197115845094 valid 0.22234047490305134
LOSS train 0.2844197115845094 valid 0.22217521826426187
LOSS train 0.2844197115845094 valid 0.22208788120641118
LOSS train 0.2844197115845094 valid 0.22207754843823185
LOSS train 0.2844197115845094 valid 0.22204943125446638
LOSS train 0.2844197115845094 valid 0.22191487402374568
LOSS train 0.2844197115845094 valid 0.2219672002222227
LOSS train 0.2844197115845094 valid 0.22190501137729332
LOSS train 0.2844197115845094 valid 0.22202315412718673
LOSS train 0.2844197115845094 valid 0.22235348974174696
LOSS train 0.2844197115845094 valid 0.22230877676325986
LOSS train 0.2844197115845094 valid 0.22234448289617578
LOSS train 0.2844197115845094 valid 0.22225262281500688
LOSS train 0.2844197115845094 valid 0.2223040137109877
LOSS train 0.2844197115845094 valid 0.2222781360399823
LOSS train 0.2844197115845094 valid 0.22231499308322764
LOSS train 0.2844197115845094 valid 0.2219189097483953
LOSS train 0.2844197115845094 valid 0.2217397147565462
LOSS train 0.2844197115845094 valid 0.221688494640441
LOSS train 0.2844197115845094 valid 0.22166543236239947
LOSS train 0.2844197115845094 valid 0.22163509638583073
LOSS train 0.2844197115845094 valid 0.22176217205670415
LOSS train 0.2844197115845094 valid 0.22179979843095066
LOSS train 0.2844197115845094 valid 0.22183484786193863
LOSS train 0.2844197115845094 valid 0.2218449848193315
LOSS train 0.2844197115845094 valid 0.22171767157723146
LOSS train 0.2844197115845094 valid 0.22184838449954986
LOSS train 0.2844197115845094 valid 0.22191201405696184
LOSS train 0.2844197115845094 valid 0.2218853696471169
LOSS train 0.2844197115845094 valid 0.2217383263374977
LOSS train 0.2844197115845094 valid 0.22188022979131833
LOSS train 0.2844197115845094 valid 0.22176595400361454
LOSS train 0.2844197115845094 valid 0.22162242227932438
LOSS train 0.2844197115845094 valid 0.22162895525012036
LOSS train 0.2844197115845094 valid 0.22176740324312402
LOSS train 0.2844197115845094 valid 0.22172101451853526
LOSS train 0.2844197115845094 valid 0.2216059782757209
LOSS train 0.2844197115845094 valid 0.2216273518243512
LOSS train 0.2844197115845094 valid 0.22177313160122805
LOSS train 0.2844197115845094 valid 0.2216370544279483
LOSS train 0.2844197115845094 valid 0.22167130638704155
LOSS train 0.2844197115845094 valid 0.22165124421974397
LOSS train 0.2844197115845094 valid 0.2216746160074284
LOSS train 0.2844197115845094 valid 0.22157902066850485
LOSS train 0.2844197115845094 valid 0.2216289830185584
LOSS train 0.2844197115845094 valid 0.22169424604527568
LOSS train 0.2844197115845094 valid 0.22158888915070782
LOSS train 0.2844197115845094 valid 0.22155520275949991
LOSS train 0.2844197115845094 valid 0.2214986889020485
LOSS train 0.2844197115845094 valid 0.22133887185281886
LOSS train 0.2844197115845094 valid 0.22128237386907104
LOSS train 0.2844197115845094 valid 0.2213818660107526
LOSS train 0.2844197115845094 valid 0.22145943834945775
LOSS train 0.2844197115845094 valid 0.2214826666598716
LOSS train 0.2844197115845094 valid 0.22149210739478792
LOSS train 0.2844197115845094 valid 0.22149773625703695
LOSS train 0.2844197115845094 valid 0.22141004438911166
LOSS train 0.2844197115845094 valid 0.22126798485521745
LOSS train 0.2844197115845094 valid 0.2212978162347002
LOSS train 0.2844197115845094 valid 0.2213161169640167
LOSS train 0.2844197115845094 valid 0.22123863327671106
LOSS train 0.2844197115845094 valid 0.22120650285168697
LOSS train 0.2844197115845094 valid 0.2211889139005354
LOSS train 0.2844197115845094 valid 0.22123740224057373
LOSS train 0.2844197115845094 valid 0.2212183478081392
LOSS train 0.2844197115845094 valid 0.2211568044646801
LOSS train 0.2844197115845094 valid 0.22107899302038653
LOSS train 0.2844197115845094 valid 0.22104418861497308
LOSS train 0.2844197115845094 valid 0.22096326584889464
LOSS train 0.2844197115845094 valid 0.22094680363814578
LOSS train 0.2844197115845094 valid 0.2209884531441189
LOSS train 0.2844197115845094 valid 0.22100494978791577
LOSS train 0.2844197115845094 valid 0.22096507633860046
LOSS train 0.2844197115845094 valid 0.22090003998191268
LOSS train 0.2844197115845094 valid 0.22095022700576974
LOSS train 0.2844197115845094 valid 0.22094383511854254
LOSS train 0.2844197115845094 valid 0.2208551646769047
LOSS train 0.2844197115845094 valid 0.22077065374566074
LOSS train 0.2844197115845094 valid 0.22073136289782871
LOSS train 0.2844197115845094 valid 0.2206510043773714
LOSS train 0.2844197115845094 valid 0.22067086173123435
LOSS train 0.2844197115845094 valid 0.22062692007080453
LOSS train 0.2844197115845094 valid 0.2208410573551078
LOSS train 0.2844197115845094 valid 0.22088135900248923
LOSS train 0.2844197115845094 valid 0.22094013522584716
LOSS train 0.2844197115845094 valid 0.22094617190870267
LOSS train 0.2844197115845094 valid 0.22097194454362315
LOSS train 0.2844197115845094 valid 0.22094719495229015
LOSS train 0.2844197115845094 valid 0.22090130891555396
LOSS train 0.2844197115845094 valid 0.22092042653895796
LOSS train 0.2844197115845094 valid 0.22081616160216605
LOSS train 0.2844197115845094 valid 0.22077718017593262
LOSS train 0.2844197115845094 valid 0.22075036300134054
LOSS train 0.2844197115845094 valid 0.22074932489861449
LOSS train 0.2844197115845094 valid 0.2207583282260025
LOSS train 0.2844197115845094 valid 0.22072973081310715
LOSS train 0.2844197115845094 valid 0.2206383696757257
LOSS train 0.2844197115845094 valid 0.22068494381934312
LOSS train 0.2844197115845094 valid 0.2206534274986812
LOSS train 0.2844197115845094 valid 0.22054219301270997
LOSS train 0.2844197115845094 valid 0.22046243008456112
LOSS train 0.2844197115845094 valid 0.2203008186358672
LOSS train 0.2844197115845094 valid 0.2204170919543395
LOSS train 0.2844197115845094 valid 0.22038115825310395
LOSS train 0.2844197115845094 valid 0.2203822383066503
LOSS train 0.2844197115845094 valid 0.22031612291162136
LOSS train 0.2844197115845094 valid 0.2203465180866646
LOSS train 0.2844197115845094 valid 0.2202468427106333
LOSS train 0.2844197115845094 valid 0.22024713712463895
LOSS train 0.2844197115845094 valid 0.22033451381209376
LOSS train 0.2844197115845094 valid 0.22028244634766778
LOSS train 0.2844197115845094 valid 0.22017535496113905
LOSS train 0.2844197115845094 valid 0.22008451863768555
LOSS train 0.2844197115845094 valid 0.22019254506872388
LOSS train 0.2844197115845094 valid 0.2201701183996257
LOSS train 0.2844197115845094 valid 0.22014739414935267
LOSS train 0.2844197115845094 valid 0.22023401461979922
LOSS train 0.2844197115845094 valid 0.2201624965825039
LOSS train 0.2844197115845094 valid 0.2201196273777917
LOSS train 0.2844197115845094 valid 0.22001309358343787
LOSS train 0.2844197115845094 valid 0.220075742276602
LOSS train 0.2844197115845094 valid 0.22011615867200104
LOSS train 0.2844197115845094 valid 0.2200683645171926
LOSS train 0.2844197115845094 valid 0.22000716472049955
LOSS train 0.2844197115845094 valid 0.22008563698023215
LOSS train 0.2844197115845094 valid 0.22005658048101004
LOSS train 0.2844197115845094 valid 0.22007768741675784
LOSS train 0.2844197115845094 valid 0.2201514934372698
LOSS train 0.2844197115845094 valid 0.22019587584178557
LOSS train 0.2844197115845094 valid 0.22026617820809988
LOSS train 0.2844197115845094 valid 0.2202682617188847
LOSS train 0.2844197115845094 valid 0.22017474623633101
LOSS train 0.2844197115845094 valid 0.22020478550805134
LOSS train 0.2844197115845094 valid 0.2202126074905823
LOSS train 0.2844197115845094 valid 0.22017561469497626
LOSS train 0.2844197115845094 valid 0.2201096609359332
LOSS train 0.2844197115845094 valid 0.22023197217947907
LOSS train 0.2844197115845094 valid 0.2201645587702537
LOSS train 0.2844197115845094 valid 0.2202167113759241
LOSS train 0.2844197115845094 valid 0.22026580198573015
LOSS train 0.2844197115845094 valid 0.22019522616660203
LOSS train 0.2844197115845094 valid 0.2203031347630775
LOSS train 0.2844197115845094 valid 0.22026906553350512
LOSS train 0.2844197115845094 valid 0.2202443311990769
LOSS train 0.2844197115845094 valid 0.22017339349764845
LOSS train 0.2844197115845094 valid 0.22015681689186148
EPOCH 8:
  batch 1 loss: 0.32963722944259644
  batch 2 loss: 0.3308357298374176
  batch 3 loss: 0.31073181827863056
  batch 4 loss: 0.3074576407670975
  batch 5 loss: 0.3055013597011566
  batch 6 loss: 0.3006051381429036
  batch 7 loss: 0.2997100438390459
  batch 8 loss: 0.292515080422163
  batch 9 loss: 0.29288838969336617
  batch 10 loss: 0.28773945569992065
  batch 11 loss: 0.2890198908068917
  batch 12 loss: 0.28738811363776523
  batch 13 loss: 0.28508002712176395
  batch 14 loss: 0.284948862024716
  batch 15 loss: 0.28506368199984233
  batch 16 loss: 0.28430476412177086
  batch 17 loss: 0.2847764456973356
  batch 18 loss: 0.2834995852576362
  batch 19 loss: 0.2828996150117171
  batch 20 loss: 0.28119734302163124
  batch 21 loss: 0.28439101505847203
  batch 22 loss: 0.2846673300320452
  batch 23 loss: 0.2831742795913116
  batch 24 loss: 0.28272866147259873
  batch 25 loss: 0.283624672293663
  batch 26 loss: 0.2832411407278134
  batch 27 loss: 0.2850485571004726
  batch 28 loss: 0.2867045801665102
  batch 29 loss: 0.28638513283482914
  batch 30 loss: 0.2868122413754463
  batch 31 loss: 0.28714162351623657
  batch 32 loss: 0.28863608511164784
  batch 33 loss: 0.28996116690563434
  batch 34 loss: 0.2899952079443371
  batch 35 loss: 0.2915076149361474
  batch 36 loss: 0.29112741889225113
  batch 37 loss: 0.29102990796437134
  batch 38 loss: 0.29164555159054306
  batch 39 loss: 0.29164362297608304
  batch 40 loss: 0.29111504517495634
  batch 41 loss: 0.29085895455465083
  batch 42 loss: 0.2905532181972549
  batch 43 loss: 0.29036315547865493
  batch 44 loss: 0.2906951853497462
  batch 45 loss: 0.2906129171450933
  batch 46 loss: 0.2891007988997128
  batch 47 loss: 0.2886273680215186
  batch 48 loss: 0.28921075568844873
  batch 49 loss: 0.28997909323293336
  batch 50 loss: 0.29036602228879926
  batch 51 loss: 0.29011017639263004
  batch 52 loss: 0.29015639567604434
  batch 53 loss: 0.2895130866541053
  batch 54 loss: 0.28921103725830716
  batch 55 loss: 0.28950174207037144
  batch 56 loss: 0.29049450451774256
  batch 57 loss: 0.28988379684456606
  batch 58 loss: 0.2894469327453909
  batch 59 loss: 0.2895379195273933
  batch 60 loss: 0.289414760718743
  batch 61 loss: 0.29006392325534197
  batch 62 loss: 0.2902244860606809
  batch 63 loss: 0.28974376193114687
  batch 64 loss: 0.2897133759688586
  batch 65 loss: 0.28911991554957167
  batch 66 loss: 0.2891110342108842
  batch 67 loss: 0.28863989039143517
  batch 68 loss: 0.28856853812056427
  batch 69 loss: 0.28834930213465204
  batch 70 loss: 0.2878671401313373
  batch 71 loss: 0.28793517825469167
  batch 72 loss: 0.28820046927365994
  batch 73 loss: 0.2880379294287668
  batch 74 loss: 0.28780579466272044
  batch 75 loss: 0.28782382746537527
  batch 76 loss: 0.2880190754015195
  batch 77 loss: 0.2877870521375111
  batch 78 loss: 0.28792588546490056
  batch 79 loss: 0.2880000006171721
  batch 80 loss: 0.2879496729001403
  batch 81 loss: 0.288101895171919
  batch 82 loss: 0.288240938833574
  batch 83 loss: 0.287995182426579
  batch 84 loss: 0.28727694210552035
  batch 85 loss: 0.2875937980764052
  batch 86 loss: 0.2873293438623118
  batch 87 loss: 0.28705646560109893
  batch 88 loss: 0.2869041348722848
  batch 89 loss: 0.2865417469083593
  batch 90 loss: 0.28701625963052113
  batch 91 loss: 0.2868728519796015
  batch 92 loss: 0.28656173108712485
  batch 93 loss: 0.2860559853174353
  batch 94 loss: 0.2860301717164669
  batch 95 loss: 0.28611659815436913
  batch 96 loss: 0.2862972461928924
  batch 97 loss: 0.28670554800131887
  batch 98 loss: 0.286626857762434
  batch 99 loss: 0.2864859245642267
  batch 100 loss: 0.2866693902015686
  batch 101 loss: 0.2863796288424199
  batch 102 loss: 0.2866978291787353
  batch 103 loss: 0.2866970439559048
  batch 104 loss: 0.287336375277776
  batch 105 loss: 0.287057109673818
  batch 106 loss: 0.2874826968840833
  batch 107 loss: 0.28750758806121685
  batch 108 loss: 0.2872617581376323
  batch 109 loss: 0.2873657312961893
  batch 110 loss: 0.28739507144147697
  batch 111 loss: 0.28743951760970793
  batch 112 loss: 0.28736410449658123
  batch 113 loss: 0.2870762527516458
  batch 114 loss: 0.28684348730664505
  batch 115 loss: 0.28644874588302943
  batch 116 loss: 0.286225033217463
  batch 117 loss: 0.28597390702647024
  batch 118 loss: 0.2860594000351631
  batch 119 loss: 0.2863055279775828
  batch 120 loss: 0.2861234940588474
  batch 121 loss: 0.28636935821249465
  batch 122 loss: 0.28619080391086515
  batch 123 loss: 0.28598792281577257
  batch 124 loss: 0.2858460112925499
  batch 125 loss: 0.28556258177757265
  batch 126 loss: 0.2856698374426554
  batch 127 loss: 0.2859858457497724
  batch 128 loss: 0.28608514345251024
  batch 129 loss: 0.28607036901074784
  batch 130 loss: 0.28589278344924635
  batch 131 loss: 0.28588967719150865
  batch 132 loss: 0.28587126031969534
  batch 133 loss: 0.28569990620577246
  batch 134 loss: 0.2856165455348456
  batch 135 loss: 0.28566908527303625
  batch 136 loss: 0.28589390700354295
  batch 137 loss: 0.2861256623355142
  batch 138 loss: 0.2863876081031302
  batch 139 loss: 0.28657149904065854
  batch 140 loss: 0.2869374277336257
  batch 141 loss: 0.28699476055219664
  batch 142 loss: 0.28696465891012
  batch 143 loss: 0.287142162348007
  batch 144 loss: 0.28694859084983665
  batch 145 loss: 0.2871876287049261
  batch 146 loss: 0.28737121065185495
  batch 147 loss: 0.2872432861603847
  batch 148 loss: 0.2872560461227958
  batch 149 loss: 0.2870897938741134
  batch 150 loss: 0.2872790737946828
  batch 151 loss: 0.28720249166551803
  batch 152 loss: 0.2871827592975215
  batch 153 loss: 0.2874871811835594
  batch 154 loss: 0.2874917494399207
  batch 155 loss: 0.2875754473670836
  batch 156 loss: 0.2874910974731812
  batch 157 loss: 0.2874509094247393
  batch 158 loss: 0.2876706400626822
  batch 159 loss: 0.28806035567379595
  batch 160 loss: 0.288091410510242
  batch 161 loss: 0.2877024869370905
  batch 162 loss: 0.2876457730193197
  batch 163 loss: 0.2875385072333681
  batch 164 loss: 0.287775960273859
  batch 165 loss: 0.28805990038496077
  batch 166 loss: 0.2883363852658904
  batch 167 loss: 0.28843399757396676
  batch 168 loss: 0.2881852707692555
  batch 169 loss: 0.2882586950381127
  batch 170 loss: 0.28838516298462363
  batch 171 loss: 0.2886189818382263
  batch 172 loss: 0.2885113020622453
  batch 173 loss: 0.2885829775319623
  batch 174 loss: 0.2888209855762021
  batch 175 loss: 0.2890014135837555
  batch 176 loss: 0.28928158144381916
  batch 177 loss: 0.2895900159232361
  batch 178 loss: 0.2894092255763793
  batch 179 loss: 0.2894601407330795
  batch 180 loss: 0.28942601316505007
  batch 181 loss: 0.2892355439741967
  batch 182 loss: 0.28923122142697427
  batch 183 loss: 0.28927926759902245
  batch 184 loss: 0.28920496514310007
  batch 185 loss: 0.28897819454605517
  batch 186 loss: 0.28910588192683395
  batch 187 loss: 0.28930704829527093
  batch 188 loss: 0.2892058475854549
  batch 189 loss: 0.28923417919527283
  batch 190 loss: 0.28927997335007316
  batch 191 loss: 0.2894144147166407
  batch 192 loss: 0.2891482845103989
  batch 193 loss: 0.2891985637381905
  batch 194 loss: 0.289067699353105
  batch 195 loss: 0.28918566482189373
  batch 196 loss: 0.28909272039119077
  batch 197 loss: 0.2891489962484631
  batch 198 loss: 0.2892351632918974
  batch 199 loss: 0.289220582974616
  batch 200 loss: 0.2892167697101831
  batch 201 loss: 0.2891629542136074
  batch 202 loss: 0.2891414301967857
  batch 203 loss: 0.2892088236039495
  batch 204 loss: 0.2892739960522044
  batch 205 loss: 0.289129423877088
  batch 206 loss: 0.28922811562864525
  batch 207 loss: 0.2891771955334622
  batch 208 loss: 0.2888951088803319
  batch 209 loss: 0.2889683486884861
  batch 210 loss: 0.2889842136275201
  batch 211 loss: 0.28902810254085687
  batch 212 loss: 0.2889295621722374
  batch 213 loss: 0.28881887084161734
  batch 214 loss: 0.2887415213980407
  batch 215 loss: 0.2888184402571168
  batch 216 loss: 0.28887583943152867
  batch 217 loss: 0.2888807406485905
  batch 218 loss: 0.2888382642504272
  batch 219 loss: 0.28858930273959627
  batch 220 loss: 0.28861913647164
  batch 221 loss: 0.28858777064813207
  batch 222 loss: 0.2886249721721486
  batch 223 loss: 0.2883698116369846
  batch 224 loss: 0.28836011613852214
  batch 225 loss: 0.288481208814515
  batch 226 loss: 0.2885571037114194
  batch 227 loss: 0.2884338946463253
  batch 228 loss: 0.288368424890857
  batch 229 loss: 0.2883689897419584
  batch 230 loss: 0.28839167401842447
  batch 231 loss: 0.2883272134251409
  batch 232 loss: 0.2882848219496423
  batch 233 loss: 0.2881838896115962
  batch 234 loss: 0.2882113477740532
  batch 235 loss: 0.28809261277635045
  batch 236 loss: 0.2880873736309803
  batch 237 loss: 0.287825442379034
  batch 238 loss: 0.287723543325893
  batch 239 loss: 0.2876747167634166
  batch 240 loss: 0.2876887435093522
  batch 241 loss: 0.28755197009357675
  batch 242 loss: 0.2874754059536398
  batch 243 loss: 0.2874691841411002
  batch 244 loss: 0.28733597958429913
  batch 245 loss: 0.2873405156695113
  batch 246 loss: 0.28730955362562244
  batch 247 loss: 0.2873263137663907
  batch 248 loss: 0.28737229415245596
  batch 249 loss: 0.287229566030713
  batch 250 loss: 0.287163662135601
  batch 251 loss: 0.28694154132172406
  batch 252 loss: 0.2868902614074094
  batch 253 loss: 0.28670608673406683
  batch 254 loss: 0.2866360020097785
  batch 255 loss: 0.2864439708929436
  batch 256 loss: 0.28630892507499084
  batch 257 loss: 0.2863479715261014
  batch 258 loss: 0.28621769095807115
  batch 259 loss: 0.2860216483416244
  batch 260 loss: 0.2861051107828434
  batch 261 loss: 0.286080703424768
  batch 262 loss: 0.28596381012720007
  batch 263 loss: 0.28583389701952044
  batch 264 loss: 0.2859608877563115
  batch 265 loss: 0.28589219912043157
  batch 266 loss: 0.285777330174482
  batch 267 loss: 0.2855838848075617
  batch 268 loss: 0.2854069741581803
  batch 269 loss: 0.28534303777279907
  batch 270 loss: 0.2852906979896404
  batch 271 loss: 0.2851876463617346
  batch 272 loss: 0.2850811503389302
  batch 273 loss: 0.28497623469366695
  batch 274 loss: 0.28481172449397346
  batch 275 loss: 0.2848564956404946
  batch 276 loss: 0.2848573023642319
  batch 277 loss: 0.28481296102923176
  batch 278 loss: 0.2847093862595318
  batch 279 loss: 0.2846945429147358
  batch 280 loss: 0.2845601801893541
  batch 281 loss: 0.28454521208259137
  batch 282 loss: 0.2844207453283858
  batch 283 loss: 0.2843459814576294
  batch 284 loss: 0.28443062573041716
  batch 285 loss: 0.28426923924370817
  batch 286 loss: 0.2842775628595919
  batch 287 loss: 0.2841899916789258
  batch 288 loss: 0.28417227753541535
  batch 289 loss: 0.2840919073374626
  batch 290 loss: 0.28398410621388204
  batch 291 loss: 0.28393440849797424
  batch 292 loss: 0.2839377361719739
  batch 293 loss: 0.28398081614499204
  batch 294 loss: 0.28378885776615465
  batch 295 loss: 0.28374742648359075
  batch 296 loss: 0.28390473907662406
  batch 297 loss: 0.2838432028739139
  batch 298 loss: 0.28387198457181856
  batch 299 loss: 0.2839851216428655
  batch 300 loss: 0.28386249164740246
  batch 301 loss: 0.28391603655197295
  batch 302 loss: 0.28406898253011387
  batch 303 loss: 0.28399240646031826
  batch 304 loss: 0.283982429849474
  batch 305 loss: 0.28397460208564507
  batch 306 loss: 0.28403454132718975
  batch 307 loss: 0.28395457881281355
  batch 308 loss: 0.2840214698732673
  batch 309 loss: 0.28393900452308285
  batch 310 loss: 0.28394642328062364
  batch 311 loss: 0.2837580212442821
  batch 312 loss: 0.28363801596256405
  batch 313 loss: 0.2835281555073711
  batch 314 loss: 0.2833767206805527
  batch 315 loss: 0.28328451608854627
  batch 316 loss: 0.28330110419022886
  batch 317 loss: 0.28339632273848503
  batch 318 loss: 0.28314957139814423
  batch 319 loss: 0.2829282963444073
  batch 320 loss: 0.2830408640671521
  batch 321 loss: 0.2829917344813035
  batch 322 loss: 0.2828247964567279
  batch 323 loss: 0.28283710127275424
  batch 324 loss: 0.2828696411516931
  batch 325 loss: 0.28293774678156924
  batch 326 loss: 0.28318510565655364
  batch 327 loss: 0.28312663337506283
  batch 328 loss: 0.28305050767049555
  batch 329 loss: 0.28306404163772214
  batch 330 loss: 0.2830468846993013
  batch 331 loss: 0.28299149285630515
  batch 332 loss: 0.28303058382617424
  batch 333 loss: 0.28298144467600117
  batch 334 loss: 0.28320865074317614
  batch 335 loss: 0.283259117603302
  batch 336 loss: 0.28327450351346106
  batch 337 loss: 0.28324235263847103
  batch 338 loss: 0.28311899903963306
  batch 339 loss: 0.2830639860447178
  batch 340 loss: 0.282938564568758
  batch 341 loss: 0.28288964997113974
  batch 342 loss: 0.2828175015529694
  batch 343 loss: 0.2828397265589272
  batch 344 loss: 0.28284170439596784
  batch 345 loss: 0.28284754455089567
  batch 346 loss: 0.28278946906672736
  batch 347 loss: 0.2827364213614368
  batch 348 loss: 0.28271176499025574
  batch 349 loss: 0.2827004178461851
  batch 350 loss: 0.2827706714613097
  batch 351 loss: 0.2827300402598503
  batch 352 loss: 0.2826401224275204
  batch 353 loss: 0.2826015886352015
  batch 354 loss: 0.2825282942187988
  batch 355 loss: 0.28246869877190656
  batch 356 loss: 0.2824264613681295
  batch 357 loss: 0.28220862191932206
  batch 358 loss: 0.28209603748508005
  batch 359 loss: 0.2821100030436821
  batch 360 loss: 0.2820590786635876
  batch 361 loss: 0.28203661694421
  batch 362 loss: 0.28196352703795247
  batch 363 loss: 0.28203470196277314
  batch 364 loss: 0.2820569537989386
  batch 365 loss: 0.2820218790883887
  batch 366 loss: 0.28205036196878047
  batch 367 loss: 0.2822090388156413
  batch 368 loss: 0.28220905873762525
  batch 369 loss: 0.28217535992948023
  batch 370 loss: 0.2823514607307073
  batch 371 loss: 0.28251537182581715
  batch 372 loss: 0.2825890966160323
  batch 373 loss: 0.28257838594689766
  batch 374 loss: 0.2826613069855593
  batch 375 loss: 0.2827703904310862
  batch 376 loss: 0.2829243205487728
  batch 377 loss: 0.28292942023403767
  batch 378 loss: 0.2828689821498104
  batch 379 loss: 0.28288630720493346
  batch 380 loss: 0.2828982125771673
  batch 381 loss: 0.2828572879782499
  batch 382 loss: 0.2828001942466067
  batch 383 loss: 0.2826539029807397
  batch 384 loss: 0.2825956641851614
  batch 385 loss: 0.282537626136433
  batch 386 loss: 0.2824798785343071
  batch 387 loss: 0.28245448727324335
  batch 388 loss: 0.28235980015747325
  batch 389 loss: 0.28238413151554703
  batch 390 loss: 0.28238113606587434
  batch 391 loss: 0.2822853276491775
  batch 392 loss: 0.28230988736055335
  batch 393 loss: 0.28239088264737117
  batch 394 loss: 0.2823650307461695
  batch 395 loss: 0.2823702456830423
  batch 396 loss: 0.28241362610850673
  batch 397 loss: 0.2824568168942814
  batch 398 loss: 0.28245006509162673
  batch 399 loss: 0.2824508146683973
  batch 400 loss: 0.28258776761591436
  batch 401 loss: 0.28250326784768903
  batch 402 loss: 0.28247135151084973
  batch 403 loss: 0.2823878829887428
  batch 404 loss: 0.28229559980111546
  batch 405 loss: 0.2823012424104008
  batch 406 loss: 0.2821969377436661
  batch 407 loss: 0.28222675976647793
  batch 408 loss: 0.2821077213567846
  batch 409 loss: 0.282080422185161
  batch 410 loss: 0.282039084449047
  batch 411 loss: 0.28204920213587964
  batch 412 loss: 0.28215526022668025
  batch 413 loss: 0.2821288992071267
  batch 414 loss: 0.28218958249702547
  batch 415 loss: 0.2821690782725093
  batch 416 loss: 0.2821513769001915
  batch 417 loss: 0.2821080031440698
  batch 418 loss: 0.2821061986199977
  batch 419 loss: 0.28211606132102185
  batch 420 loss: 0.28206576257944105
  batch 421 loss: 0.2821248694015512
  batch 422 loss: 0.28208688346413074
  batch 423 loss: 0.28203160888196166
  batch 424 loss: 0.28215709370824527
  batch 425 loss: 0.2820965609830969
  batch 426 loss: 0.2820489670329251
  batch 427 loss: 0.28211220128753983
  batch 428 loss: 0.2821243437372635
  batch 429 loss: 0.2820730380245022
  batch 430 loss: 0.28215802415858865
  batch 431 loss: 0.2822036110469747
  batch 432 loss: 0.28219118121045605
  batch 433 loss: 0.2821218644636608
  batch 434 loss: 0.28202631200925543
  batch 435 loss: 0.282098219278215
  batch 436 loss: 0.28211307515381673
  batch 437 loss: 0.2820310715747916
  batch 438 loss: 0.28196685203270283
  batch 439 loss: 0.2818919707064205
  batch 440 loss: 0.2820038095455278
  batch 441 loss: 0.2819996300130204
  batch 442 loss: 0.28194285480819675
  batch 443 loss: 0.2819490063742106
  batch 444 loss: 0.28196708539182
  batch 445 loss: 0.28190759363469114
  batch 446 loss: 0.28199882461217485
  batch 447 loss: 0.2820138382231629
  batch 448 loss: 0.28207926229307695
  batch 449 loss: 0.2820265341243128
  batch 450 loss: 0.2819682462679015
  batch 451 loss: 0.2819347952858043
  batch 452 loss: 0.2818658193326102
  batch 453 loss: 0.2818500634126579
  batch 454 loss: 0.28188985517240306
  batch 455 loss: 0.28189841636589597
  batch 456 loss: 0.28184163678241403
  batch 457 loss: 0.28185328166255286
  batch 458 loss: 0.2818021096581975
  batch 459 loss: 0.28175066398509446
  batch 460 loss: 0.28175368286345315
  batch 461 loss: 0.28174313550528113
  batch 462 loss: 0.2817471834036695
  batch 463 loss: 0.2817585294177156
  batch 464 loss: 0.28166199761346494
  batch 465 loss: 0.2816495232364183
  batch 466 loss: 0.2815256838570849
  batch 467 loss: 0.2816156886310598
  batch 468 loss: 0.2815569810824007
  batch 469 loss: 0.28147313030543863
  batch 470 loss: 0.2815633589917041
  batch 471 loss: 0.28158216554900894
  batch 472 loss: 0.2814450093813367
LOSS train 0.2814450093813367 valid 0.22802779078483582
LOSS train 0.2814450093813367 valid 0.22555913031101227
LOSS train 0.2814450093813367 valid 0.22842954099178314
LOSS train 0.2814450093813367 valid 0.21161429956555367
LOSS train 0.2814450093813367 valid 0.2145838052034378
LOSS train 0.2814450093813367 valid 0.21904321014881134
LOSS train 0.2814450093813367 valid 0.2151355424097606
LOSS train 0.2814450093813367 valid 0.21338696032762527
LOSS train 0.2814450093813367 valid 0.21207645701037514
LOSS train 0.2814450093813367 valid 0.21152289658784867
LOSS train 0.2814450093813367 valid 0.21150588582862506
LOSS train 0.2814450093813367 valid 0.21468307822942734
LOSS train 0.2814450093813367 valid 0.21449714440565842
LOSS train 0.2814450093813367 valid 0.21252518679414475
LOSS train 0.2814450093813367 valid 0.21194993456204733
LOSS train 0.2814450093813367 valid 0.21532275713980198
LOSS train 0.2814450093813367 valid 0.21573177330634175
LOSS train 0.2814450093813367 valid 0.2151032802131441
LOSS train 0.2814450093813367 valid 0.21684722837648893
LOSS train 0.2814450093813367 valid 0.21679465994238853
LOSS train 0.2814450093813367 valid 0.21817040514378322
LOSS train 0.2814450093813367 valid 0.21788971667939966
LOSS train 0.2814450093813367 valid 0.21599416110826575
LOSS train 0.2814450093813367 valid 0.2166927345097065
LOSS train 0.2814450093813367 valid 0.21654982924461363
LOSS train 0.2814450093813367 valid 0.21560834921323335
LOSS train 0.2814450093813367 valid 0.21573710331210383
LOSS train 0.2814450093813367 valid 0.21590247005224228
LOSS train 0.2814450093813367 valid 0.21444149418123837
LOSS train 0.2814450093813367 valid 0.21448741108179092
LOSS train 0.2814450093813367 valid 0.21502488034386788
LOSS train 0.2814450093813367 valid 0.21556247491389513
LOSS train 0.2814450093813367 valid 0.2145887432676373
LOSS train 0.2814450093813367 valid 0.2141142395489356
LOSS train 0.2814450093813367 valid 0.21473120663847242
LOSS train 0.2814450093813367 valid 0.21566751392351258
LOSS train 0.2814450093813367 valid 0.21594874923293655
LOSS train 0.2814450093813367 valid 0.2158577301000294
LOSS train 0.2814450093813367 valid 0.21690851679215065
LOSS train 0.2814450093813367 valid 0.2172868385910988
LOSS train 0.2814450093813367 valid 0.21667905224532616
LOSS train 0.2814450093813367 valid 0.2182275883498646
LOSS train 0.2814450093813367 valid 0.21839974439421364
LOSS train 0.2814450093813367 valid 0.21789309619502586
LOSS train 0.2814450093813367 valid 0.21749630835321215
LOSS train 0.2814450093813367 valid 0.21713088323240695
LOSS train 0.2814450093813367 valid 0.21688830725690153
LOSS train 0.2814450093813367 valid 0.21840164065361023
LOSS train 0.2814450093813367 valid 0.21761124505072224
LOSS train 0.2814450093813367 valid 0.21806358367204667
LOSS train 0.2814450093813367 valid 0.21768435865056282
LOSS train 0.2814450093813367 valid 0.21728203990138495
LOSS train 0.2814450093813367 valid 0.21861328128373847
LOSS train 0.2814450093813367 valid 0.21848423116736942
LOSS train 0.2814450093813367 valid 0.21816569024866278
LOSS train 0.2814450093813367 valid 0.21836783098323004
LOSS train 0.2814450093813367 valid 0.21773890131398252
LOSS train 0.2814450093813367 valid 0.21839059169950156
LOSS train 0.2814450093813367 valid 0.2181413696478989
LOSS train 0.2814450093813367 valid 0.21785986969868343
LOSS train 0.2814450093813367 valid 0.2179237513757143
LOSS train 0.2814450093813367 valid 0.21763061251371138
LOSS train 0.2814450093813367 valid 0.21729757672264463
LOSS train 0.2814450093813367 valid 0.21753632882609963
LOSS train 0.2814450093813367 valid 0.216588972394283
LOSS train 0.2814450093813367 valid 0.21653861597631918
LOSS train 0.2814450093813367 valid 0.21715218251320853
LOSS train 0.2814450093813367 valid 0.21657994389533997
LOSS train 0.2814450093813367 valid 0.21725108571674512
LOSS train 0.2814450093813367 valid 0.21749548209565026
LOSS train 0.2814450093813367 valid 0.21770212075240175
LOSS train 0.2814450093813367 valid 0.21809545614653164
LOSS train 0.2814450093813367 valid 0.21870675764671743
LOSS train 0.2814450093813367 valid 0.2184813548181508
LOSS train 0.2814450093813367 valid 0.21803987403710684
LOSS train 0.2814450093813367 valid 0.21803175246244982
LOSS train 0.2814450093813367 valid 0.2178675721992146
LOSS train 0.2814450093813367 valid 0.21766588722284025
LOSS train 0.2814450093813367 valid 0.2177273553760746
LOSS train 0.2814450093813367 valid 0.2172799501568079
LOSS train 0.2814450093813367 valid 0.21746419885276277
LOSS train 0.2814450093813367 valid 0.21735988193895758
LOSS train 0.2814450093813367 valid 0.21755334016788436
LOSS train 0.2814450093813367 valid 0.21734109911180677
LOSS train 0.2814450093813367 valid 0.21781433610355153
LOSS train 0.2814450093813367 valid 0.21768110921216566
LOSS train 0.2814450093813367 valid 0.21717957741227642
LOSS train 0.2814450093813367 valid 0.2173472996801138
LOSS train 0.2814450093813367 valid 0.2176106506853961
LOSS train 0.2814450093813367 valid 0.21780430475870768
LOSS train 0.2814450093813367 valid 0.21781269665602798
LOSS train 0.2814450093813367 valid 0.21786110751006915
LOSS train 0.2814450093813367 valid 0.21768751163636485
LOSS train 0.2814450093813367 valid 0.21779852503157676
LOSS train 0.2814450093813367 valid 0.21809163250421224
LOSS train 0.2814450093813367 valid 0.21834200217078129
LOSS train 0.2814450093813367 valid 0.21841726198638836
LOSS train 0.2814450093813367 valid 0.2187280858657798
LOSS train 0.2814450093813367 valid 0.2189295334045333
LOSS train 0.2814450093813367 valid 0.21889766946434974
LOSS train 0.2814450093813367 valid 0.2188931799466067
LOSS train 0.2814450093813367 valid 0.21928409896060533
LOSS train 0.2814450093813367 valid 0.21899606258545107
LOSS train 0.2814450093813367 valid 0.21887246490671083
LOSS train 0.2814450093813367 valid 0.21902498872507187
LOSS train 0.2814450093813367 valid 0.2189619688774055
LOSS train 0.2814450093813367 valid 0.2186283164213751
LOSS train 0.2814450093813367 valid 0.21859798966734498
LOSS train 0.2814450093813367 valid 0.21818539910360213
LOSS train 0.2814450093813367 valid 0.2182787759737535
LOSS train 0.2814450093813367 valid 0.2185824712117513
LOSS train 0.2814450093813367 valid 0.2188890834472009
LOSS train 0.2814450093813367 valid 0.21872020110619805
LOSS train 0.2814450093813367 valid 0.21873366466739722
LOSS train 0.2814450093813367 valid 0.21929386776426565
LOSS train 0.2814450093813367 valid 0.2188773949084611
LOSS train 0.2814450093813367 valid 0.2193406557934916
LOSS train 0.2814450093813367 valid 0.21920839015205026
LOSS train 0.2814450093813367 valid 0.21883699235295048
LOSS train 0.2814450093813367 valid 0.21858586209515732
LOSS train 0.2814450093813367 valid 0.21852730337745888
LOSS train 0.2814450093813367 valid 0.21873380976622223
LOSS train 0.2814450093813367 valid 0.21881489467814685
LOSS train 0.2814450093813367 valid 0.21893214530521823
LOSS train 0.2814450093813367 valid 0.21888870763778687
LOSS train 0.2814450093813367 valid 0.21912333144555016
LOSS train 0.2814450093813367 valid 0.21899677155994055
LOSS train 0.2814450093813367 valid 0.2189200259745121
LOSS train 0.2814450093813367 valid 0.21863568914953127
LOSS train 0.2814450093813367 valid 0.21831982353558907
LOSS train 0.2814450093813367 valid 0.21827875908094507
LOSS train 0.2814450093813367 valid 0.21826426752588965
LOSS train 0.2814450093813367 valid 0.21829737241106822
LOSS train 0.2814450093813367 valid 0.21864415594001316
LOSS train 0.2814450093813367 valid 0.218661234776179
LOSS train 0.2814450093813367 valid 0.21868440026746078
LOSS train 0.2814450093813367 valid 0.21880945976633226
LOSS train 0.2814450093813367 valid 0.2186467314975849
LOSS train 0.2814450093813367 valid 0.21841067333015607
LOSS train 0.2814450093813367 valid 0.2183262945285865
LOSS train 0.2814450093813367 valid 0.2183631697656415
LOSS train 0.2814450093813367 valid 0.21869814133560153
LOSS train 0.2814450093813367 valid 0.21880235621979185
LOSS train 0.2814450093813367 valid 0.21875637355777952
LOSS train 0.2814450093813367 valid 0.21865231322831122
LOSS train 0.2814450093813367 valid 0.21852059795023643
LOSS train 0.2814450093813367 valid 0.21849056713435114
LOSS train 0.2814450093813367 valid 0.21836969447699753
LOSS train 0.2814450093813367 valid 0.21843111865072443
LOSS train 0.2814450093813367 valid 0.21839604924122494
LOSS train 0.2814450093813367 valid 0.21829737880766786
LOSS train 0.2814450093813367 valid 0.21818084357992598
LOSS train 0.2814450093813367 valid 0.21795564899647157
LOSS train 0.2814450093813367 valid 0.2180619296895993
LOSS train 0.2814450093813367 valid 0.21807720142026102
LOSS train 0.2814450093813367 valid 0.21816804451056016
LOSS train 0.2814450093813367 valid 0.21806712087931907
LOSS train 0.2814450093813367 valid 0.2181649391975584
LOSS train 0.2814450093813367 valid 0.21838125877035489
LOSS train 0.2814450093813367 valid 0.21850806856527924
LOSS train 0.2814450093813367 valid 0.21848121387247713
LOSS train 0.2814450093813367 valid 0.2185702233770747
LOSS train 0.2814450093813367 valid 0.21828885010780733
LOSS train 0.2814450093813367 valid 0.21836623249620926
LOSS train 0.2814450093813367 valid 0.21840306583679084
LOSS train 0.2814450093813367 valid 0.21826629071350556
LOSS train 0.2814450093813367 valid 0.21841631446056023
LOSS train 0.2814450093813367 valid 0.21829706634439172
LOSS train 0.2814450093813367 valid 0.21804975325891957
LOSS train 0.2814450093813367 valid 0.21818272059454638
LOSS train 0.2814450093813367 valid 0.21805812392318458
LOSS train 0.2814450093813367 valid 0.2180520523079606
LOSS train 0.2814450093813367 valid 0.21804834764472322
LOSS train 0.2814450093813367 valid 0.21834032503010212
LOSS train 0.2814450093813367 valid 0.21830194532871247
LOSS train 0.2814450093813367 valid 0.2181977637281472
LOSS train 0.2814450093813367 valid 0.21824449359697137
LOSS train 0.2814450093813367 valid 0.21826739776670262
LOSS train 0.2814450093813367 valid 0.21841294364223268
LOSS train 0.2814450093813367 valid 0.21835994919141133
LOSS train 0.2814450093813367 valid 0.21827678835194414
LOSS train 0.2814450093813367 valid 0.2181296489395938
LOSS train 0.2814450093813367 valid 0.2182335176103102
LOSS train 0.2814450093813367 valid 0.21821241882508216
LOSS train 0.2814450093813367 valid 0.21822599696146475
LOSS train 0.2814450093813367 valid 0.21826655625976543
LOSS train 0.2814450093813367 valid 0.21807714372395196
LOSS train 0.2814450093813367 valid 0.21817649433270414
LOSS train 0.2814450093813367 valid 0.21799346591745103
LOSS train 0.2814450093813367 valid 0.21801038941270426
LOSS train 0.2814450093813367 valid 0.21818014175792014
LOSS train 0.2814450093813367 valid 0.21828151812466481
LOSS train 0.2814450093813367 valid 0.21807504989631435
LOSS train 0.2814450093813367 valid 0.21798848214837693
LOSS train 0.2814450093813367 valid 0.21794318090646694
LOSS train 0.2814450093813367 valid 0.21810411912750224
LOSS train 0.2814450093813367 valid 0.2180846862230204
LOSS train 0.2814450093813367 valid 0.21836561167782004
LOSS train 0.2814450093813367 valid 0.2183373553219752
LOSS train 0.2814450093813367 valid 0.21844676479697228
LOSS train 0.2814450093813367 valid 0.21823997224741315
LOSS train 0.2814450093813367 valid 0.21818258369913196
LOSS train 0.2814450093813367 valid 0.2183318506614328
LOSS train 0.2814450093813367 valid 0.21826498203125655
LOSS train 0.2814450093813367 valid 0.21826009721290773
LOSS train 0.2814450093813367 valid 0.21823672769428457
LOSS train 0.2814450093813367 valid 0.21822089158394486
LOSS train 0.2814450093813367 valid 0.21809629594477323
LOSS train 0.2814450093813367 valid 0.2180764454831347
LOSS train 0.2814450093813367 valid 0.21815982489358812
LOSS train 0.2814450093813367 valid 0.2183720774560178
LOSS train 0.2814450093813367 valid 0.21819251775741577
LOSS train 0.2814450093813367 valid 0.21809198829769527
LOSS train 0.2814450093813367 valid 0.21795864730516326
LOSS train 0.2814450093813367 valid 0.21771586641322735
LOSS train 0.2814450093813367 valid 0.21754917881830974
LOSS train 0.2814450093813367 valid 0.2173118530193232
LOSS train 0.2814450093813367 valid 0.21736399070658816
LOSS train 0.2814450093813367 valid 0.21734302371876424
LOSS train 0.2814450093813367 valid 0.2174021642993797
LOSS train 0.2814450093813367 valid 0.21736722658662236
LOSS train 0.2814450093813367 valid 0.21745607095795708
LOSS train 0.2814450093813367 valid 0.21755031245706327
LOSS train 0.2814450093813367 valid 0.21748269355988928
LOSS train 0.2814450093813367 valid 0.21732503751913707
LOSS train 0.2814450093813367 valid 0.2172367719158662
LOSS train 0.2814450093813367 valid 0.21723403279476755
LOSS train 0.2814450093813367 valid 0.21719794498201003
LOSS train 0.2814450093813367 valid 0.21706033690006973
LOSS train 0.2814450093813367 valid 0.21710939368476037
LOSS train 0.2814450093813367 valid 0.21702861482704872
LOSS train 0.2814450093813367 valid 0.21717130861662584
LOSS train 0.2814450093813367 valid 0.21752959918310713
LOSS train 0.2814450093813367 valid 0.2174579228600885
LOSS train 0.2814450093813367 valid 0.21748458210458146
LOSS train 0.2814450093813367 valid 0.21742543508693324
LOSS train 0.2814450093813367 valid 0.2174811395286005
LOSS train 0.2814450093813367 valid 0.21744923863340826
LOSS train 0.2814450093813367 valid 0.2174775150281116
LOSS train 0.2814450093813367 valid 0.2170907961204648
LOSS train 0.2814450093813367 valid 0.21692785150777255
LOSS train 0.2814450093813367 valid 0.21686064839855698
LOSS train 0.2814450093813367 valid 0.21683028420057807
LOSS train 0.2814450093813367 valid 0.21679059190095448
LOSS train 0.2814450093813367 valid 0.21689113925914374
LOSS train 0.2814450093813367 valid 0.21691499262805877
LOSS train 0.2814450093813367 valid 0.2169688467675375
LOSS train 0.2814450093813367 valid 0.21700299501178727
LOSS train 0.2814450093813367 valid 0.21689062418947258
LOSS train 0.2814450093813367 valid 0.21702141892910004
LOSS train 0.2814450093813367 valid 0.21708321986920331
LOSS train 0.2814450093813367 valid 0.2170637577535614
LOSS train 0.2814450093813367 valid 0.21691777029998688
LOSS train 0.2814450093813367 valid 0.2170609144244607
LOSS train 0.2814450093813367 valid 0.21695701661063174
LOSS train 0.2814450093813367 valid 0.21679879305884242
LOSS train 0.2814450093813367 valid 0.21679543532750023
LOSS train 0.2814450093813367 valid 0.21693168661391088
LOSS train 0.2814450093813367 valid 0.21689199503784473
LOSS train 0.2814450093813367 valid 0.21677025613876488
LOSS train 0.2814450093813367 valid 0.21677694169954323
LOSS train 0.2814450093813367 valid 0.21691906236055244
LOSS train 0.2814450093813367 valid 0.21681037652855137
LOSS train 0.2814450093813367 valid 0.21685142769958032
LOSS train 0.2814450093813367 valid 0.21682698276807677
LOSS train 0.2814450093813367 valid 0.21684124858531736
LOSS train 0.2814450093813367 valid 0.21674149648080604
LOSS train 0.2814450093813367 valid 0.2167994350528539
LOSS train 0.2814450093813367 valid 0.21687635164500169
LOSS train 0.2814450093813367 valid 0.21674585502456736
LOSS train 0.2814450093813367 valid 0.2167336782406177
LOSS train 0.2814450093813367 valid 0.21668065887163668
LOSS train 0.2814450093813367 valid 0.21652291666020404
LOSS train 0.2814450093813367 valid 0.2164626316758838
LOSS train 0.2814450093813367 valid 0.21657120287418366
LOSS train 0.2814450093813367 valid 0.21664965563062308
LOSS train 0.2814450093813367 valid 0.2166780898824926
LOSS train 0.2814450093813367 valid 0.21668808065730033
LOSS train 0.2814450093813367 valid 0.21667287271723526
LOSS train 0.2814450093813367 valid 0.2166030353201287
LOSS train 0.2814450093813367 valid 0.21645812700863834
LOSS train 0.2814450093813367 valid 0.21648658785625552
LOSS train 0.2814450093813367 valid 0.21651293442653682
LOSS train 0.2814450093813367 valid 0.2164334274933372
LOSS train 0.2814450093813367 valid 0.2163907674843805
LOSS train 0.2814450093813367 valid 0.21637623678971005
LOSS train 0.2814450093813367 valid 0.21643530569633124
LOSS train 0.2814450093813367 valid 0.21642231656652358
LOSS train 0.2814450093813367 valid 0.21637079846693982
LOSS train 0.2814450093813367 valid 0.21628885716199875
LOSS train 0.2814450093813367 valid 0.21626803837076494
LOSS train 0.2814450093813367 valid 0.21618828177452087
LOSS train 0.2814450093813367 valid 0.21615045688054668
LOSS train 0.2814450093813367 valid 0.21621769828861262
LOSS train 0.2814450093813367 valid 0.21621975166312718
LOSS train 0.2814450093813367 valid 0.21618354914558902
LOSS train 0.2814450093813367 valid 0.21611451595921308
LOSS train 0.2814450093813367 valid 0.21617001360094787
LOSS train 0.2814450093813367 valid 0.2161471207803707
LOSS train 0.2814450093813367 valid 0.2160640091697375
LOSS train 0.2814450093813367 valid 0.21597109551841634
LOSS train 0.2814450093813367 valid 0.21592882028873392
LOSS train 0.2814450093813367 valid 0.2158558221146612
LOSS train 0.2814450093813367 valid 0.2158593872169915
LOSS train 0.2814450093813367 valid 0.2158183968946582
LOSS train 0.2814450093813367 valid 0.2160185393846892
LOSS train 0.2814450093813367 valid 0.21604327454629085
LOSS train 0.2814450093813367 valid 0.2161037844013084
LOSS train 0.2814450093813367 valid 0.21610756830875927
LOSS train 0.2814450093813367 valid 0.21613889009721818
LOSS train 0.2814450093813367 valid 0.2161024838111025
LOSS train 0.2814450093813367 valid 0.21606556249734682
LOSS train 0.2814450093813367 valid 0.21609680506939324
LOSS train 0.2814450093813367 valid 0.21598816705737145
LOSS train 0.2814450093813367 valid 0.21595042943954468
LOSS train 0.2814450093813367 valid 0.21591326396299315
LOSS train 0.2814450093813367 valid 0.2159328859311173
LOSS train 0.2814450093813367 valid 0.21593811227090703
LOSS train 0.2814450093813367 valid 0.21592184322007396
LOSS train 0.2814450093813367 valid 0.215825707744807
LOSS train 0.2814450093813367 valid 0.21587857339426736
LOSS train 0.2814450093813367 valid 0.2158524847086172
LOSS train 0.2814450093813367 valid 0.2157401643209044
LOSS train 0.2814450093813367 valid 0.21566904055667513
LOSS train 0.2814450093813367 valid 0.2155209571123123
LOSS train 0.2814450093813367 valid 0.21563046763828195
LOSS train 0.2814450093813367 valid 0.21557720757405693
LOSS train 0.2814450093813367 valid 0.2155586529250552
LOSS train 0.2814450093813367 valid 0.2154958685208961
LOSS train 0.2814450093813367 valid 0.21551825905388053
LOSS train 0.2814450093813367 valid 0.21541726012964624
LOSS train 0.2814450093813367 valid 0.21541508291679692
LOSS train 0.2814450093813367 valid 0.215489079420631
LOSS train 0.2814450093813367 valid 0.21543386699018363
LOSS train 0.2814450093813367 valid 0.21531408375768518
LOSS train 0.2814450093813367 valid 0.21524265275469848
LOSS train 0.2814450093813367 valid 0.21535629228597578
LOSS train 0.2814450093813367 valid 0.21534920497053472
LOSS train 0.2814450093813367 valid 0.21533434708385693
LOSS train 0.2814450093813367 valid 0.21543606147170066
LOSS train 0.2814450093813367 valid 0.21537379046514238
LOSS train 0.2814450093813367 valid 0.2153305382146473
LOSS train 0.2814450093813367 valid 0.21521175720079994
LOSS train 0.2814450093813367 valid 0.21526422555190186
LOSS train 0.2814450093813367 valid 0.2153205106223839
LOSS train 0.2814450093813367 valid 0.215271636369945
LOSS train 0.2814450093813367 valid 0.21521592359206174
LOSS train 0.2814450093813367 valid 0.21528800975145965
LOSS train 0.2814450093813367 valid 0.2152858743267961
LOSS train 0.2814450093813367 valid 0.2153082711781774
LOSS train 0.2814450093813367 valid 0.21538637582732742
LOSS train 0.2814450093813367 valid 0.21540602783418514
LOSS train 0.2814450093813367 valid 0.21545535997189477
LOSS train 0.2814450093813367 valid 0.21547645936578008
LOSS train 0.2814450093813367 valid 0.21536418887930855
LOSS train 0.2814450093813367 valid 0.21539791217178442
LOSS train 0.2814450093813367 valid 0.21541291137202448
LOSS train 0.2814450093813367 valid 0.21537215370872168
LOSS train 0.2814450093813367 valid 0.2152992700716911
LOSS train 0.2814450093813367 valid 0.2154096856299374
LOSS train 0.2814450093813367 valid 0.2153242753400697
LOSS train 0.2814450093813367 valid 0.21538234172769674
LOSS train 0.2814450093813367 valid 0.21543010716267524
LOSS train 0.2814450093813367 valid 0.21535452729561827
LOSS train 0.2814450093813367 valid 0.2154580537995247
LOSS train 0.2814450093813367 valid 0.2154226912582507
LOSS train 0.2814450093813367 valid 0.21540746892667922
LOSS train 0.2814450093813367 valid 0.2153297268340121
LOSS train 0.2814450093813367 valid 0.2153155711204379
EPOCH 9:
  batch 1 loss: 0.30697670578956604
  batch 2 loss: 0.31632302701473236
  batch 3 loss: 0.2996036509672801
  batch 4 loss: 0.3003550171852112
  batch 5 loss: 0.29753982424736025
  batch 6 loss: 0.2941601872444153
  batch 7 loss: 0.28952304806028095
  batch 8 loss: 0.282526183873415
  batch 9 loss: 0.28235519263479447
  batch 10 loss: 0.2766385093331337
  batch 11 loss: 0.2778361724181609
  batch 12 loss: 0.2773870962361495
  batch 13 loss: 0.2748268498824193
  batch 14 loss: 0.2744098220552717
  batch 15 loss: 0.27474434971809386
  batch 16 loss: 0.27422175370156765
  batch 17 loss: 0.2750095339382396
  batch 18 loss: 0.27453501853677964
  batch 19 loss: 0.2742409815913753
  batch 20 loss: 0.2726144403219223
  batch 21 loss: 0.27608401009014677
  batch 22 loss: 0.2758640254085714
  batch 23 loss: 0.27438698773798736
  batch 24 loss: 0.27417391041914624
  batch 25 loss: 0.27492803931236265
  batch 26 loss: 0.27511201798915863
  batch 27 loss: 0.2762003342310588
  batch 28 loss: 0.2781262568065098
  batch 29 loss: 0.27769237140129355
  batch 30 loss: 0.2781529446442922
  batch 31 loss: 0.279043415861745
  batch 32 loss: 0.2805115319788456
  batch 33 loss: 0.28189057472980383
  batch 34 loss: 0.28210532577598796
  batch 35 loss: 0.2832704075745174
  batch 36 loss: 0.28279780182573533
  batch 37 loss: 0.2828268134916151
  batch 38 loss: 0.2835519211856942
  batch 39 loss: 0.2837649560891665
  batch 40 loss: 0.2834243893623352
  batch 41 loss: 0.283041206074924
  batch 42 loss: 0.2829775050992057
  batch 43 loss: 0.28311554983604786
  batch 44 loss: 0.2832938398827206
  batch 45 loss: 0.2832630813121796
  batch 46 loss: 0.2828860198673995
  batch 47 loss: 0.28251282078154544
  batch 48 loss: 0.28263881367941696
  batch 49 loss: 0.2837714893477304
  batch 50 loss: 0.2849406224489212
  batch 51 loss: 0.28499598421302497
  batch 52 loss: 0.2852250744517033
  batch 53 loss: 0.2850955634746911
  batch 54 loss: 0.28511723324104593
  batch 55 loss: 0.2852969451384111
  batch 56 loss: 0.28569439479282926
  batch 57 loss: 0.2849956243707423
  batch 58 loss: 0.2846491270024201
  batch 59 loss: 0.28481984946687344
  batch 60 loss: 0.2848967581987381
  batch 61 loss: 0.28680391780665665
  batch 62 loss: 0.28697975556696614
  batch 63 loss: 0.2866000433762868
  batch 64 loss: 0.28673729579895735
  batch 65 loss: 0.2864671652133648
  batch 66 loss: 0.2867903257861282
  batch 67 loss: 0.28710021857005447
  batch 68 loss: 0.28726155179388385
  batch 69 loss: 0.2870847872201947
  batch 70 loss: 0.28662291382040295
  batch 71 loss: 0.2866696761527532
  batch 72 loss: 0.2868235943218072
  batch 73 loss: 0.28695281441897563
  batch 74 loss: 0.2868385979452649
  batch 75 loss: 0.2870983823140462
  batch 76 loss: 0.28759030723258067
  batch 77 loss: 0.28769469028943545
  batch 78 loss: 0.2881635905076296
  batch 79 loss: 0.28840439983561067
  batch 80 loss: 0.288175505399704
  batch 81 loss: 0.28813760111361375
  batch 82 loss: 0.2884897869534609
  batch 83 loss: 0.2883766297116337
  batch 84 loss: 0.2878035580118497
  batch 85 loss: 0.28814492681447196
  batch 86 loss: 0.28798893440601436
  batch 87 loss: 0.2874710564983302
  batch 88 loss: 0.2872693980620666
  batch 89 loss: 0.28667776554488067
  batch 90 loss: 0.28676974293258456
  batch 91 loss: 0.28655073652555657
  batch 92 loss: 0.2863223972203939
  batch 93 loss: 0.28581518111049486
  batch 94 loss: 0.2858168354376833
  batch 95 loss: 0.28586556676187014
  batch 96 loss: 0.285924326783667
  batch 97 loss: 0.2863035188200547
  batch 98 loss: 0.28598910342065653
  batch 99 loss: 0.2857845336800874
  batch 100 loss: 0.28591436192393305
  batch 101 loss: 0.28583389918993013
  batch 102 loss: 0.2861788575263584
  batch 103 loss: 0.2860113942218058
  batch 104 loss: 0.2864250069340834
  batch 105 loss: 0.2862407711290178
  batch 106 loss: 0.28636220336522694
  batch 107 loss: 0.28636794850648006
  batch 108 loss: 0.2861027879019578
  batch 109 loss: 0.28640648025438326
  batch 110 loss: 0.286464027789506
  batch 111 loss: 0.28650941384268236
  batch 112 loss: 0.286295736075512
  batch 113 loss: 0.28613936993400607
  batch 114 loss: 0.28589705090250883
  batch 115 loss: 0.28551023019396743
  batch 116 loss: 0.285209650494929
  batch 117 loss: 0.28500300149122876
  batch 118 loss: 0.2849503885638916
  batch 119 loss: 0.284997175095462
  batch 120 loss: 0.284773209815224
  batch 121 loss: 0.2848904139000522
  batch 122 loss: 0.28463129687016125
  batch 123 loss: 0.28431901747618266
  batch 124 loss: 0.2842400025936865
  batch 125 loss: 0.2837616307735443
  batch 126 loss: 0.28374680569247596
  batch 127 loss: 0.2839429160272043
  batch 128 loss: 0.2840058922301978
  batch 129 loss: 0.28405713295751767
  batch 130 loss: 0.28378045031657584
  batch 131 loss: 0.28372410849760504
  batch 132 loss: 0.2837731300881415
  batch 133 loss: 0.2834926047047278
  batch 134 loss: 0.2832834888527642
  batch 135 loss: 0.283274546707118
  batch 136 loss: 0.2835313499192981
  batch 137 loss: 0.28373563431040216
  batch 138 loss: 0.2839827657393787
  batch 139 loss: 0.284341714686627
  batch 140 loss: 0.2846242963203362
  batch 141 loss: 0.2845716678293039
  batch 142 loss: 0.2845490632552496
  batch 143 loss: 0.2848141217356795
  batch 144 loss: 0.2847285642185145
  batch 145 loss: 0.28480877084978695
  batch 146 loss: 0.2849701098995666
  batch 147 loss: 0.28489552924827655
  batch 148 loss: 0.28481038106051654
  batch 149 loss: 0.28445020958081185
  batch 150 loss: 0.2846116336186727
  batch 151 loss: 0.2843585264998556
  batch 152 loss: 0.2842904084215039
  batch 153 loss: 0.28467261752271966
  batch 154 loss: 0.2846453274999346
  batch 155 loss: 0.2846793553521556
  batch 156 loss: 0.28453622892116887
  batch 157 loss: 0.2844797067201821
  batch 158 loss: 0.28459964598281473
  batch 159 loss: 0.28491205314420304
  batch 160 loss: 0.28490041848272085
  batch 161 loss: 0.2846546038892699
  batch 162 loss: 0.2846578272220529
  batch 163 loss: 0.28450838161026776
  batch 164 loss: 0.28465768231487854
  batch 165 loss: 0.2849371912804517
  batch 166 loss: 0.28504171671278505
  batch 167 loss: 0.28493556139354936
  batch 168 loss: 0.2847639119163865
  batch 169 loss: 0.28491286333848737
  batch 170 loss: 0.2849104212487445
  batch 171 loss: 0.28516692280420786
  batch 172 loss: 0.2851162292063236
  batch 173 loss: 0.2851483845125044
  batch 174 loss: 0.28534191701261474
  batch 175 loss: 0.2855516928434372
  batch 176 loss: 0.28575188895179465
  batch 177 loss: 0.2859687134202591
  batch 178 loss: 0.28579130006975
  batch 179 loss: 0.2858454059788635
  batch 180 loss: 0.28579461781515014
  batch 181 loss: 0.2856254685484902
  batch 182 loss: 0.2855290175302998
  batch 183 loss: 0.2854007256975591
  batch 184 loss: 0.2853675286245087
  batch 185 loss: 0.2851911490833437
  batch 186 loss: 0.28532380690818193
  batch 187 loss: 0.2856341970954987
  batch 188 loss: 0.2854269826190269
  batch 189 loss: 0.28533051734563536
  batch 190 loss: 0.28535387853258537
  batch 191 loss: 0.28541278800103054
  batch 192 loss: 0.2851622783734153
  batch 193 loss: 0.2851582897605056
  batch 194 loss: 0.2850795000330689
  batch 195 loss: 0.28523278671961566
  batch 196 loss: 0.2850952387920448
  batch 197 loss: 0.28512754045464667
  batch 198 loss: 0.28520755311756424
  batch 199 loss: 0.2852690123283683
  batch 200 loss: 0.2852493082731962
  batch 201 loss: 0.2851851839776063
  batch 202 loss: 0.28520763931002946
  batch 203 loss: 0.2852353301688368
  batch 204 loss: 0.28537170435575876
  batch 205 loss: 0.2851832916096943
  batch 206 loss: 0.2852304450516562
  batch 207 loss: 0.2852851889559612
  batch 208 loss: 0.285063649312808
  batch 209 loss: 0.2850740626097866
  batch 210 loss: 0.28499860238461266
  batch 211 loss: 0.28512002435905676
  batch 212 loss: 0.28522130832919534
  batch 213 loss: 0.2850540793137931
  batch 214 loss: 0.2851084646916835
  batch 215 loss: 0.28517329879971437
  batch 216 loss: 0.2852184831130284
  batch 217 loss: 0.28516891042483017
  batch 218 loss: 0.2851549826494051
  batch 219 loss: 0.28498375619927496
  batch 220 loss: 0.2851556813175028
  batch 221 loss: 0.2850753349416396
  batch 222 loss: 0.2851628095985533
  batch 223 loss: 0.2851117376254813
  batch 224 loss: 0.28521965710180147
  batch 225 loss: 0.28518416166305544
  batch 226 loss: 0.28518281415500474
  batch 227 loss: 0.2852269617208825
  batch 228 loss: 0.2851970174856353
  batch 229 loss: 0.2851522236932313
  batch 230 loss: 0.2850992000621298
  batch 231 loss: 0.2851160514406312
  batch 232 loss: 0.28512872883985785
  batch 233 loss: 0.28501544771276316
  batch 234 loss: 0.2849807492178729
  batch 235 loss: 0.28491186689823234
  batch 236 loss: 0.284858758292966
  batch 237 loss: 0.2846849023792814
  batch 238 loss: 0.2845449654244575
  batch 239 loss: 0.2845131113938208
  batch 240 loss: 0.28461197204887867
  batch 241 loss: 0.2844993300210391
  batch 242 loss: 0.28439409858431697
  batch 243 loss: 0.28447323559243004
  batch 244 loss: 0.28440802605425725
  batch 245 loss: 0.28440350123814173
  batch 246 loss: 0.2843522248956246
  batch 247 loss: 0.28432784473847766
  batch 248 loss: 0.28440970999579274
  batch 249 loss: 0.28434008814723616
  batch 250 loss: 0.28428287863731383
  batch 251 loss: 0.2840774434496207
  batch 252 loss: 0.2840885456119265
  batch 253 loss: 0.28385982035177026
  batch 254 loss: 0.2837394751667038
  batch 255 loss: 0.2836091318551232
  batch 256 loss: 0.28349299647379667
  batch 257 loss: 0.28348388688109727
  batch 258 loss: 0.28337087869182115
  batch 259 loss: 0.2831721732169965
  batch 260 loss: 0.28322738426236005
  batch 261 loss: 0.2831447836425569
  batch 262 loss: 0.2830091419679518
  batch 263 loss: 0.28292142919941093
  batch 264 loss: 0.28296339833600953
  batch 265 loss: 0.28287516275666796
  batch 266 loss: 0.2826902758806272
  batch 267 loss: 0.28248293256938234
  batch 268 loss: 0.2822890257212653
  batch 269 loss: 0.28225075567078856
  batch 270 loss: 0.2822130944993761
  batch 271 loss: 0.2821079770797293
  batch 272 loss: 0.2820261564544019
  batch 273 loss: 0.28189222296297334
  batch 274 loss: 0.2817163712134326
  batch 275 loss: 0.2817287295514887
  batch 276 loss: 0.28165956230267236
  batch 277 loss: 0.28159352943355
  batch 278 loss: 0.28147202617616107
  batch 279 loss: 0.28145353261074285
  batch 280 loss: 0.28136215939053466
  batch 281 loss: 0.2812216373631114
  batch 282 loss: 0.28108169270534045
  batch 283 loss: 0.2809961715667071
  batch 284 loss: 0.281046607702131
  batch 285 loss: 0.2808916281712683
  batch 286 loss: 0.280862149189819
  batch 287 loss: 0.28075920138832583
  batch 288 loss: 0.28081294268162715
  batch 289 loss: 0.28061897011157966
  batch 290 loss: 0.2805110774677375
  batch 291 loss: 0.28049460140495364
  batch 292 loss: 0.28035840427834696
  batch 293 loss: 0.2803686491560204
  batch 294 loss: 0.28012633597364234
  batch 295 loss: 0.2801000296059301
  batch 296 loss: 0.2801808947445573
  batch 297 loss: 0.2801396402245017
  batch 298 loss: 0.28015538890089764
  batch 299 loss: 0.2801997073517994
  batch 300 loss: 0.2800917429725329
  batch 301 loss: 0.2801289137613734
  batch 302 loss: 0.2802165663794966
  batch 303 loss: 0.2801349144367495
  batch 304 loss: 0.2801029241006625
  batch 305 loss: 0.2800776274477849
  batch 306 loss: 0.2801446008915995
  batch 307 loss: 0.2800693012992412
  batch 308 loss: 0.2802390301382387
  batch 309 loss: 0.2802279466178425
  batch 310 loss: 0.28025114670876533
  batch 311 loss: 0.28010820187748053
  batch 312 loss: 0.2800699833016365
  batch 313 loss: 0.2799677699804306
  batch 314 loss: 0.27980783671899967
  batch 315 loss: 0.2797640723368478
  batch 316 loss: 0.27969676477811006
  batch 317 loss: 0.2797517576710283
  batch 318 loss: 0.27950891472820966
  batch 319 loss: 0.2793534702174716
  batch 320 loss: 0.2795641172211617
  batch 321 loss: 0.2795279415504212
  batch 322 loss: 0.2793863497165419
  batch 323 loss: 0.27948026086892874
  batch 324 loss: 0.27951140646581296
  batch 325 loss: 0.27954766942904546
  batch 326 loss: 0.27979884511488345
  batch 327 loss: 0.2797841136244094
  batch 328 loss: 0.27974889381993107
  batch 329 loss: 0.27977831992334873
  batch 330 loss: 0.27978528810269904
  batch 331 loss: 0.27975705256995115
  batch 332 loss: 0.27986177712319843
  batch 333 loss: 0.279819422387504
  batch 334 loss: 0.28005650853682423
  batch 335 loss: 0.2801222991587511
  batch 336 loss: 0.28014068083748933
  batch 337 loss: 0.2801352258252107
  batch 338 loss: 0.28001228076290097
  batch 339 loss: 0.2799355297577416
  batch 340 loss: 0.27975773859549974
  batch 341 loss: 0.27972258190430505
  batch 342 loss: 0.27964550984358927
  batch 343 loss: 0.27972760748758957
  batch 344 loss: 0.27971152610383754
  batch 345 loss: 0.279788766088693
  batch 346 loss: 0.2797544166618
  batch 347 loss: 0.2796324533155397
  batch 348 loss: 0.27962402035963946
  batch 349 loss: 0.2796246636135872
  batch 350 loss: 0.27971699889217105
  batch 351 loss: 0.27964870937359637
  batch 352 loss: 0.2795560964467851
  batch 353 loss: 0.27953284641163206
  batch 354 loss: 0.2794755316723538
  batch 355 loss: 0.27943953107780134
  batch 356 loss: 0.2793897514262896
  batch 357 loss: 0.279175966268494
  batch 358 loss: 0.27903400831881847
  batch 359 loss: 0.27905863120861374
  batch 360 loss: 0.27897587505479654
  batch 361 loss: 0.2789614692206528
  batch 362 loss: 0.2789118416319236
  batch 363 loss: 0.27894280629217133
  batch 364 loss: 0.27890382514713885
  batch 365 loss: 0.27887412803630307
  batch 366 loss: 0.27887844823585833
  batch 367 loss: 0.2788981419736748
  batch 368 loss: 0.27888583849467663
  batch 369 loss: 0.2788759996978248
  batch 370 loss: 0.27895409549410277
  batch 371 loss: 0.27900949381593104
  batch 372 loss: 0.2791297783854828
  batch 373 loss: 0.2791406130742771
  batch 374 loss: 0.2791061805770359
  batch 375 loss: 0.2791837998628616
  batch 376 loss: 0.2793593789113963
  batch 377 loss: 0.2793534974757809
  batch 378 loss: 0.2793062066353818
  batch 379 loss: 0.2793358890315159
  batch 380 loss: 0.2794016153796723
  batch 381 loss: 0.2793797026547234
  batch 382 loss: 0.2793479153389082
  batch 383 loss: 0.27919669869518776
  batch 384 loss: 0.2791394965800767
  batch 385 loss: 0.2790408644196275
  batch 386 loss: 0.27895585478433055
  batch 387 loss: 0.2789416625441199
  batch 388 loss: 0.2788231460012726
  batch 389 loss: 0.27891156855003385
  batch 390 loss: 0.2788977601971382
  batch 391 loss: 0.2788397618343153
  batch 392 loss: 0.2788286302238703
  batch 393 loss: 0.2788702962735227
  batch 394 loss: 0.2788500772590565
  batch 395 loss: 0.2788841754198074
  batch 396 loss: 0.2789190332922671
  batch 397 loss: 0.2789475304879229
  batch 398 loss: 0.2789528634605096
  batch 399 loss: 0.278946788127261
  batch 400 loss: 0.2791277163103223
  batch 401 loss: 0.27905024108744025
  batch 402 loss: 0.2790481037464901
  batch 403 loss: 0.2789537719920019
  batch 404 loss: 0.2788733098619055
  batch 405 loss: 0.27885107000668846
  batch 406 loss: 0.27878048707698955
  batch 407 loss: 0.2787748091197424
  batch 408 loss: 0.2786642967485914
  batch 409 loss: 0.27863170414215777
  batch 410 loss: 0.27853064744210826
  batch 411 loss: 0.2785181180911633
  batch 412 loss: 0.27867919268104635
  batch 413 loss: 0.2786989031995469
  batch 414 loss: 0.2787583358405869
  batch 415 loss: 0.2787164890981582
  batch 416 loss: 0.27870710391121417
  batch 417 loss: 0.27866058544718103
  batch 418 loss: 0.27864440466370877
  batch 419 loss: 0.27868708416926263
  batch 420 loss: 0.2786756146167006
  batch 421 loss: 0.278731779679833
  batch 422 loss: 0.27867311574695236
  batch 423 loss: 0.27862204266073576
  batch 424 loss: 0.2787312390458471
  batch 425 loss: 0.27869626644779655
  batch 426 loss: 0.2786658771091224
  batch 427 loss: 0.27869232536339367
  batch 428 loss: 0.27874499799631464
  batch 429 loss: 0.27867033921338463
  batch 430 loss: 0.27874582163816275
  batch 431 loss: 0.278791092837493
  batch 432 loss: 0.2788308400453793
  batch 433 loss: 0.2787561712928512
  batch 434 loss: 0.2786583572557445
  batch 435 loss: 0.2787176469619247
  batch 436 loss: 0.27869694902125847
  batch 437 loss: 0.27860040386426915
  batch 438 loss: 0.27849789487716814
  batch 439 loss: 0.27842510700361606
  batch 440 loss: 0.2785877402871847
  batch 441 loss: 0.2786183657627257
  batch 442 loss: 0.27858734505074056
  batch 443 loss: 0.27857624559450905
  batch 444 loss: 0.27862945766196595
  batch 445 loss: 0.27856225157051945
  batch 446 loss: 0.2786222136341403
  batch 447 loss: 0.2786326913092227
  batch 448 loss: 0.27868267534566776
  batch 449 loss: 0.2786763951613802
  batch 450 loss: 0.27862277375327216
  batch 451 loss: 0.27852983861963393
  batch 452 loss: 0.278477537684736
  batch 453 loss: 0.27848732037260043
  batch 454 loss: 0.27843293160331406
  batch 455 loss: 0.27841510098059097
  batch 456 loss: 0.27836870258314567
  batch 457 loss: 0.27839246835698184
  batch 458 loss: 0.2783587905517312
  batch 459 loss: 0.2783308171368892
  batch 460 loss: 0.2783353234762731
  batch 461 loss: 0.2783353227208855
  batch 462 loss: 0.27831682317700734
  batch 463 loss: 0.2783263917875599
  batch 464 loss: 0.2783069361386628
  batch 465 loss: 0.2782973409339946
  batch 466 loss: 0.2781938397257625
  batch 467 loss: 0.27832937843666855
  batch 468 loss: 0.27828940376639366
  batch 469 loss: 0.27818877194354785
  batch 470 loss: 0.2783117820290809
  batch 471 loss: 0.2784061841587605
  batch 472 loss: 0.2782575667725276
LOSS train 0.2782575667725276 valid 0.24023084342479706
LOSS train 0.2782575667725276 valid 0.23452302068471909
LOSS train 0.2782575667725276 valid 0.24146649738152823
LOSS train 0.2782575667725276 valid 0.2252843640744686
LOSS train 0.2782575667725276 valid 0.2289772778749466
LOSS train 0.2782575667725276 valid 0.23323805381854376
LOSS train 0.2782575667725276 valid 0.22903974780014583
LOSS train 0.2782575667725276 valid 0.2277775164693594
LOSS train 0.2782575667725276 valid 0.22671184109316933
LOSS train 0.2782575667725276 valid 0.22540383040905
LOSS train 0.2782575667725276 valid 0.22507624328136444
LOSS train 0.2782575667725276 valid 0.22940858826041222
LOSS train 0.2782575667725276 valid 0.2295189259143976
LOSS train 0.2782575667725276 valid 0.22677616136414663
LOSS train 0.2782575667725276 valid 0.22593433260917664
LOSS train 0.2782575667725276 valid 0.22944971732795238
LOSS train 0.2782575667725276 valid 0.23034650613279903
LOSS train 0.2782575667725276 valid 0.2299940147333675
LOSS train 0.2782575667725276 valid 0.23194013222267754
LOSS train 0.2782575667725276 valid 0.23161811605095864
LOSS train 0.2782575667725276 valid 0.2332048082635516
LOSS train 0.2782575667725276 valid 0.23290844871239227
LOSS train 0.2782575667725276 valid 0.23110862529796103
LOSS train 0.2782575667725276 valid 0.23200073714057604
LOSS train 0.2782575667725276 valid 0.23172897696495057
LOSS train 0.2782575667725276 valid 0.23073416203260422
LOSS train 0.2782575667725276 valid 0.23079802758163875
LOSS train 0.2782575667725276 valid 0.23103260887520655
LOSS train 0.2782575667725276 valid 0.22925380180621968
LOSS train 0.2782575667725276 valid 0.22927105128765107
LOSS train 0.2782575667725276 valid 0.22965180970007373
LOSS train 0.2782575667725276 valid 0.23017059359699488
LOSS train 0.2782575667725276 valid 0.22955951952573025
LOSS train 0.2782575667725276 valid 0.22893610202214298
LOSS train 0.2782575667725276 valid 0.22952040476458413
LOSS train 0.2782575667725276 valid 0.23048359942105082
LOSS train 0.2782575667725276 valid 0.23108354895501523
LOSS train 0.2782575667725276 valid 0.23099124431610107
LOSS train 0.2782575667725276 valid 0.23193342716265947
LOSS train 0.2782575667725276 valid 0.23208145648241044
LOSS train 0.2782575667725276 valid 0.23170569393692947
LOSS train 0.2782575667725276 valid 0.23312467336654663
LOSS train 0.2782575667725276 valid 0.2335680168728496
LOSS train 0.2782575667725276 valid 0.23269238424572078
LOSS train 0.2782575667725276 valid 0.23220053613185881
LOSS train 0.2782575667725276 valid 0.23183184246654095
LOSS train 0.2782575667725276 valid 0.2315962349480771
LOSS train 0.2782575667725276 valid 0.23342946078628302
LOSS train 0.2782575667725276 valid 0.23262145963250375
LOSS train 0.2782575667725276 valid 0.23313918143510817
LOSS train 0.2782575667725276 valid 0.2327350442316018
LOSS train 0.2782575667725276 valid 0.23242384252639917
LOSS train 0.2782575667725276 valid 0.23388991041003532
LOSS train 0.2782575667725276 valid 0.233605921820358
LOSS train 0.2782575667725276 valid 0.2332902111790397
LOSS train 0.2782575667725276 valid 0.23335614587579454
LOSS train 0.2782575667725276 valid 0.23267374566772528
LOSS train 0.2782575667725276 valid 0.2333108035141024
LOSS train 0.2782575667725276 valid 0.2330404371023178
LOSS train 0.2782575667725276 valid 0.23276784941554068
LOSS train 0.2782575667725276 valid 0.23279853482715418
LOSS train 0.2782575667725276 valid 0.2325611189007759
LOSS train 0.2782575667725276 valid 0.23221360052388812
LOSS train 0.2782575667725276 valid 0.23249757406301796
LOSS train 0.2782575667725276 valid 0.2314627745976815
LOSS train 0.2782575667725276 valid 0.23138158587795316
LOSS train 0.2782575667725276 valid 0.23206508893575242
LOSS train 0.2782575667725276 valid 0.23145740435403936
LOSS train 0.2782575667725276 valid 0.23219063878059387
LOSS train 0.2782575667725276 valid 0.23261166938713618
LOSS train 0.2782575667725276 valid 0.23271269907414074
LOSS train 0.2782575667725276 valid 0.23301533071531189
LOSS train 0.2782575667725276 valid 0.23365401527653
LOSS train 0.2782575667725276 valid 0.23324752015036507
LOSS train 0.2782575667725276 valid 0.23288782914479575
LOSS train 0.2782575667725276 valid 0.2328308665830838
LOSS train 0.2782575667725276 valid 0.23267748797094667
LOSS train 0.2782575667725276 valid 0.2324463413693966
LOSS train 0.2782575667725276 valid 0.23240385270571406
LOSS train 0.2782575667725276 valid 0.23191778138279914
LOSS train 0.2782575667725276 valid 0.2320792524167049
LOSS train 0.2782575667725276 valid 0.23195979944089565
LOSS train 0.2782575667725276 valid 0.23204595532761998
LOSS train 0.2782575667725276 valid 0.2318474597164563
LOSS train 0.2782575667725276 valid 0.2324520493254942
LOSS train 0.2782575667725276 valid 0.23233299789040587
LOSS train 0.2782575667725276 valid 0.23185903193621799
LOSS train 0.2782575667725276 valid 0.23210214264690876
LOSS train 0.2782575667725276 valid 0.23250568900885207
LOSS train 0.2782575667725276 valid 0.23269201384650337
LOSS train 0.2782575667725276 valid 0.2327378423331858
LOSS train 0.2782575667725276 valid 0.23279982853842818
LOSS train 0.2782575667725276 valid 0.23273299057637492
LOSS train 0.2782575667725276 valid 0.23283068376018645
LOSS train 0.2782575667725276 valid 0.2331491358970341
LOSS train 0.2782575667725276 valid 0.2334352764301002
LOSS train 0.2782575667725276 valid 0.2334819167237921
LOSS train 0.2782575667725276 valid 0.23389374495160822
LOSS train 0.2782575667725276 valid 0.23396172517477865
LOSS train 0.2782575667725276 valid 0.23410406082868576
LOSS train 0.2782575667725276 valid 0.23413646767045013
LOSS train 0.2782575667725276 valid 0.23451074212789536
LOSS train 0.2782575667725276 valid 0.2342804676120721
LOSS train 0.2782575667725276 valid 0.23421058617532253
LOSS train 0.2782575667725276 valid 0.23439374012606484
LOSS train 0.2782575667725276 valid 0.23426123905294347
LOSS train 0.2782575667725276 valid 0.23384945011027505
LOSS train 0.2782575667725276 valid 0.23387108011930077
LOSS train 0.2782575667725276 valid 0.2334511855600077
LOSS train 0.2782575667725276 valid 0.2334975165399638
LOSS train 0.2782575667725276 valid 0.23381987496002302
LOSS train 0.2782575667725276 valid 0.23413247362311398
LOSS train 0.2782575667725276 valid 0.23397944814863458
LOSS train 0.2782575667725276 valid 0.23403280646654598
LOSS train 0.2782575667725276 valid 0.23467261519121088
LOSS train 0.2782575667725276 valid 0.23427317741102185
LOSS train 0.2782575667725276 valid 0.2348061052397785
LOSS train 0.2782575667725276 valid 0.2346552718999022
LOSS train 0.2782575667725276 valid 0.23422880896500178
LOSS train 0.2782575667725276 valid 0.23392427153885365
LOSS train 0.2782575667725276 valid 0.2339024478492658
LOSS train 0.2782575667725276 valid 0.23413688371904562
LOSS train 0.2782575667725276 valid 0.23422389289712517
LOSS train 0.2782575667725276 valid 0.23439111933112144
LOSS train 0.2782575667725276 valid 0.23433603525161742
LOSS train 0.2782575667725276 valid 0.2346074143572459
LOSS train 0.2782575667725276 valid 0.23451970247771797
LOSS train 0.2782575667725276 valid 0.23445318872109056
LOSS train 0.2782575667725276 valid 0.23412869189136712
LOSS train 0.2782575667725276 valid 0.23385347678111149
LOSS train 0.2782575667725276 valid 0.23382129400741053
LOSS train 0.2782575667725276 valid 0.23386790230870247
LOSS train 0.2782575667725276 valid 0.23394252516721425
LOSS train 0.2782575667725276 valid 0.2342836390902747
LOSS train 0.2782575667725276 valid 0.2343670180550328
LOSS train 0.2782575667725276 valid 0.23442712119396994
LOSS train 0.2782575667725276 valid 0.2345309881833348
LOSS train 0.2782575667725276 valid 0.23439067699339078
LOSS train 0.2782575667725276 valid 0.2341803224395505
LOSS train 0.2782575667725276 valid 0.23406806396586555
LOSS train 0.2782575667725276 valid 0.23412064542161656
LOSS train 0.2782575667725276 valid 0.2344778127653498
LOSS train 0.2782575667725276 valid 0.2345845304377429
LOSS train 0.2782575667725276 valid 0.23457080881214804
LOSS train 0.2782575667725276 valid 0.23450331780417213
LOSS train 0.2782575667725276 valid 0.2343890547548255
LOSS train 0.2782575667725276 valid 0.23434242829173602
LOSS train 0.2782575667725276 valid 0.2342266885412706
LOSS train 0.2782575667725276 valid 0.23432098999119444
LOSS train 0.2782575667725276 valid 0.2342227824529012
LOSS train 0.2782575667725276 valid 0.23407844429379268
LOSS train 0.2782575667725276 valid 0.23399317431214608
LOSS train 0.2782575667725276 valid 0.23378960476591695
LOSS train 0.2782575667725276 valid 0.23385704657087078
LOSS train 0.2782575667725276 valid 0.23387378271548978
LOSS train 0.2782575667725276 valid 0.23392966408760119
LOSS train 0.2782575667725276 valid 0.23376118709706956
LOSS train 0.2782575667725276 valid 0.23380853396050538
LOSS train 0.2782575667725276 valid 0.23406384604157143
LOSS train 0.2782575667725276 valid 0.23422071868553757
LOSS train 0.2782575667725276 valid 0.23409864232406852
LOSS train 0.2782575667725276 valid 0.23418316659000185
LOSS train 0.2782575667725276 valid 0.2339090918836418
LOSS train 0.2782575667725276 valid 0.23396536753308483
LOSS train 0.2782575667725276 valid 0.23404098306641435
LOSS train 0.2782575667725276 valid 0.23387963252972407
LOSS train 0.2782575667725276 valid 0.2339673566068718
LOSS train 0.2782575667725276 valid 0.23386946897066774
LOSS train 0.2782575667725276 valid 0.23363116939039624
LOSS train 0.2782575667725276 valid 0.23378979519886128
LOSS train 0.2782575667725276 valid 0.23364579398729649
LOSS train 0.2782575667725276 valid 0.23369212266664172
LOSS train 0.2782575667725276 valid 0.23363804567411456
LOSS train 0.2782575667725276 valid 0.2340163012174354
LOSS train 0.2782575667725276 valid 0.23394529623644691
LOSS train 0.2782575667725276 valid 0.2338150344281034
LOSS train 0.2782575667725276 valid 0.23384934066042387
LOSS train 0.2782575667725276 valid 0.23396771719281592
LOSS train 0.2782575667725276 valid 0.23422675800390083
LOSS train 0.2782575667725276 valid 0.23416400452454886
LOSS train 0.2782575667725276 valid 0.23410567534233326
LOSS train 0.2782575667725276 valid 0.23392256398449887
LOSS train 0.2782575667725276 valid 0.2339691484723586
LOSS train 0.2782575667725276 valid 0.23393821481453336
LOSS train 0.2782575667725276 valid 0.23392613280463864
LOSS train 0.2782575667725276 valid 0.23397971617598687
LOSS train 0.2782575667725276 valid 0.23376145799529743
LOSS train 0.2782575667725276 valid 0.23381368229363828
LOSS train 0.2782575667725276 valid 0.2336388030853221
LOSS train 0.2782575667725276 valid 0.23364384919404985
LOSS train 0.2782575667725276 valid 0.23377493874252778
LOSS train 0.2782575667725276 valid 0.23383613543895385
LOSS train 0.2782575667725276 valid 0.23359233426615364
LOSS train 0.2782575667725276 valid 0.23349409235506943
LOSS train 0.2782575667725276 valid 0.23341191640267006
LOSS train 0.2782575667725276 valid 0.23360736348799296
LOSS train 0.2782575667725276 valid 0.23355293296617904
LOSS train 0.2782575667725276 valid 0.23382270163056826
LOSS train 0.2782575667725276 valid 0.23379974426636144
LOSS train 0.2782575667725276 valid 0.23399740569293498
LOSS train 0.2782575667725276 valid 0.2338233703552787
LOSS train 0.2782575667725276 valid 0.23375653327986745
LOSS train 0.2782575667725276 valid 0.23388754080962665
LOSS train 0.2782575667725276 valid 0.2337843856829054
LOSS train 0.2782575667725276 valid 0.23374967364276328
LOSS train 0.2782575667725276 valid 0.23370028460778078
LOSS train 0.2782575667725276 valid 0.2336685257257471
LOSS train 0.2782575667725276 valid 0.2335050762272798
LOSS train 0.2782575667725276 valid 0.23347444703989623
LOSS train 0.2782575667725276 valid 0.23357744479463213
LOSS train 0.2782575667725276 valid 0.23383801448119196
LOSS train 0.2782575667725276 valid 0.2336434873769868
LOSS train 0.2782575667725276 valid 0.23351587921800748
LOSS train 0.2782575667725276 valid 0.2333714964790879
LOSS train 0.2782575667725276 valid 0.23311534219009933
LOSS train 0.2782575667725276 valid 0.23290401786841727
LOSS train 0.2782575667725276 valid 0.23261308443436424
LOSS train 0.2782575667725276 valid 0.2326522008814943
LOSS train 0.2782575667725276 valid 0.23263539543979245
LOSS train 0.2782575667725276 valid 0.23266312811862339
LOSS train 0.2782575667725276 valid 0.23262814037940083
LOSS train 0.2782575667725276 valid 0.2327367306292594
LOSS train 0.2782575667725276 valid 0.23284548788327272
LOSS train 0.2782575667725276 valid 0.2327704893292061
LOSS train 0.2782575667725276 valid 0.23259581228097279
LOSS train 0.2782575667725276 valid 0.2324850635059112
LOSS train 0.2782575667725276 valid 0.23250044527820554
LOSS train 0.2782575667725276 valid 0.2324644290564353
LOSS train 0.2782575667725276 valid 0.23235649800977332
LOSS train 0.2782575667725276 valid 0.23242663909559663
LOSS train 0.2782575667725276 valid 0.23237314065555473
LOSS train 0.2782575667725276 valid 0.23252932579609856
LOSS train 0.2782575667725276 valid 0.2329262922889685
LOSS train 0.2782575667725276 valid 0.23288996161049247
LOSS train 0.2782575667725276 valid 0.23292850224261588
LOSS train 0.2782575667725276 valid 0.23282130416166985
LOSS train 0.2782575667725276 valid 0.23289263273593241
LOSS train 0.2782575667725276 valid 0.2328644479397966
LOSS train 0.2782575667725276 valid 0.2329002541478209
LOSS train 0.2782575667725276 valid 0.2324876081198454
LOSS train 0.2782575667725276 valid 0.2323157595030005
LOSS train 0.2782575667725276 valid 0.23227826499741924
LOSS train 0.2782575667725276 valid 0.2322401085508213
LOSS train 0.2782575667725276 valid 0.23220709650242916
LOSS train 0.2782575667725276 valid 0.23236530727269697
LOSS train 0.2782575667725276 valid 0.2323970057494272
LOSS train 0.2782575667725276 valid 0.23245376638072704
LOSS train 0.2782575667725276 valid 0.23246010206639767
LOSS train 0.2782575667725276 valid 0.23230880654480562
LOSS train 0.2782575667725276 valid 0.2324466654062271
LOSS train 0.2782575667725276 valid 0.2325059086440569
LOSS train 0.2782575667725276 valid 0.23248136256422317
LOSS train 0.2782575667725276 valid 0.23230238217610144
LOSS train 0.2782575667725276 valid 0.23241213656316592
LOSS train 0.2782575667725276 valid 0.2322618749211816
LOSS train 0.2782575667725276 valid 0.23206641635624692
LOSS train 0.2782575667725276 valid 0.23206380137209762
LOSS train 0.2782575667725276 valid 0.23221921065981074
LOSS train 0.2782575667725276 valid 0.2321600705730409
LOSS train 0.2782575667725276 valid 0.23201944197599705
LOSS train 0.2782575667725276 valid 0.23206812994005122
LOSS train 0.2782575667725276 valid 0.2322465539092326
LOSS train 0.2782575667725276 valid 0.23209905210783274
LOSS train 0.2782575667725276 valid 0.2321177695623853
LOSS train 0.2782575667725276 valid 0.23205750449648443
LOSS train 0.2782575667725276 valid 0.23208078579571015
LOSS train 0.2782575667725276 valid 0.23197948804285642
LOSS train 0.2782575667725276 valid 0.2320447985583277
LOSS train 0.2782575667725276 valid 0.23214142750186992
LOSS train 0.2782575667725276 valid 0.2319948097621953
LOSS train 0.2782575667725276 valid 0.23197494689168965
LOSS train 0.2782575667725276 valid 0.23190571669050875
LOSS train 0.2782575667725276 valid 0.2317636251558751
LOSS train 0.2782575667725276 valid 0.23169840442655731
LOSS train 0.2782575667725276 valid 0.23180428662083366
LOSS train 0.2782575667725276 valid 0.23187520101234532
LOSS train 0.2782575667725276 valid 0.2318963297114906
LOSS train 0.2782575667725276 valid 0.23188317052537588
LOSS train 0.2782575667725276 valid 0.23186252423725676
LOSS train 0.2782575667725276 valid 0.23175761720963886
LOSS train 0.2782575667725276 valid 0.23162214590561347
LOSS train 0.2782575667725276 valid 0.23163884603385385
LOSS train 0.2782575667725276 valid 0.23162628220585124
LOSS train 0.2782575667725276 valid 0.23156339229202608
LOSS train 0.2782575667725276 valid 0.2315118730068207
LOSS train 0.2782575667725276 valid 0.2314744545863225
LOSS train 0.2782575667725276 valid 0.23151875793518506
LOSS train 0.2782575667725276 valid 0.23149930261489418
LOSS train 0.2782575667725276 valid 0.23145965587309075
LOSS train 0.2782575667725276 valid 0.23134536753440726
LOSS train 0.2782575667725276 valid 0.23132389445894772
LOSS train 0.2782575667725276 valid 0.23123124935855605
LOSS train 0.2782575667725276 valid 0.2311929746089128
LOSS train 0.2782575667725276 valid 0.23124291413292594
LOSS train 0.2782575667725276 valid 0.23127060815439385
LOSS train 0.2782575667725276 valid 0.23121105411366835
LOSS train 0.2782575667725276 valid 0.23112724947206903
LOSS train 0.2782575667725276 valid 0.23115615876729056
LOSS train 0.2782575667725276 valid 0.23112948562787927
LOSS train 0.2782575667725276 valid 0.23103536382317544
LOSS train 0.2782575667725276 valid 0.2309447433366332
LOSS train 0.2782575667725276 valid 0.23090540543692
LOSS train 0.2782575667725276 valid 0.2308300505376885
LOSS train 0.2782575667725276 valid 0.23084244279092864
LOSS train 0.2782575667725276 valid 0.2307871599666408
LOSS train 0.2782575667725276 valid 0.2310294518673342
LOSS train 0.2782575667725276 valid 0.23107129417918015
LOSS train 0.2782575667725276 valid 0.2311496011332258
LOSS train 0.2782575667725276 valid 0.23114248888392278
LOSS train 0.2782575667725276 valid 0.23116568908576043
LOSS train 0.2782575667725276 valid 0.23119622536027545
LOSS train 0.2782575667725276 valid 0.23114093345327255
LOSS train 0.2782575667725276 valid 0.23110627190183147
LOSS train 0.2782575667725276 valid 0.23099607010935522
LOSS train 0.2782575667725276 valid 0.23093699699356443
LOSS train 0.2782575667725276 valid 0.23089839999057069
LOSS train 0.2782575667725276 valid 0.23093521012116683
LOSS train 0.2782575667725276 valid 0.2309422584356002
LOSS train 0.2782575667725276 valid 0.23093843053687702
LOSS train 0.2782575667725276 valid 0.23081225492060184
LOSS train 0.2782575667725276 valid 0.23089854795242024
LOSS train 0.2782575667725276 valid 0.2308475551027689
LOSS train 0.2782575667725276 valid 0.2307325658783455
LOSS train 0.2782575667725276 valid 0.23068812645879794
LOSS train 0.2782575667725276 valid 0.2305326446203085
LOSS train 0.2782575667725276 valid 0.23065376656552766
LOSS train 0.2782575667725276 valid 0.23057714519333036
LOSS train 0.2782575667725276 valid 0.23058472510154654
LOSS train 0.2782575667725276 valid 0.23055812240915097
LOSS train 0.2782575667725276 valid 0.23057953503095743
LOSS train 0.2782575667725276 valid 0.23047659440343113
LOSS train 0.2782575667725276 valid 0.23046113721218453
LOSS train 0.2782575667725276 valid 0.2305495867858062
LOSS train 0.2782575667725276 valid 0.2304664743160773
LOSS train 0.2782575667725276 valid 0.2303339294978042
LOSS train 0.2782575667725276 valid 0.2302794824576094
LOSS train 0.2782575667725276 valid 0.23041370608332604
LOSS train 0.2782575667725276 valid 0.2304007267281854
LOSS train 0.2782575667725276 valid 0.23037631458061636
LOSS train 0.2782575667725276 valid 0.2304957847823115
LOSS train 0.2782575667725276 valid 0.23044433079856583
LOSS train 0.2782575667725276 valid 0.2304087408889107
LOSS train 0.2782575667725276 valid 0.23027597007181486
LOSS train 0.2782575667725276 valid 0.2303578842344672
LOSS train 0.2782575667725276 valid 0.23045138042906058
LOSS train 0.2782575667725276 valid 0.2304152745990395
LOSS train 0.2782575667725276 valid 0.23033954821505534
LOSS train 0.2782575667725276 valid 0.23039163220887895
LOSS train 0.2782575667725276 valid 0.23038706153545818
LOSS train 0.2782575667725276 valid 0.23040687850543431
LOSS train 0.2782575667725276 valid 0.230508268813462
LOSS train 0.2782575667725276 valid 0.23052658729085868
LOSS train 0.2782575667725276 valid 0.23060814799904486
LOSS train 0.2782575667725276 valid 0.23062458840635536
LOSS train 0.2782575667725276 valid 0.2304956448330006
LOSS train 0.2782575667725276 valid 0.2305228373224146
LOSS train 0.2782575667725276 valid 0.23051210081710868
LOSS train 0.2782575667725276 valid 0.23045485882952227
LOSS train 0.2782575667725276 valid 0.23040307526966988
LOSS train 0.2782575667725276 valid 0.23054005747867956
LOSS train 0.2782575667725276 valid 0.2304422288224968
LOSS train 0.2782575667725276 valid 0.23049071653442488
LOSS train 0.2782575667725276 valid 0.23054863672611142
LOSS train 0.2782575667725276 valid 0.23048187071805473
LOSS train 0.2782575667725276 valid 0.2305804278752575
LOSS train 0.2782575667725276 valid 0.2305154641632174
LOSS train 0.2782575667725276 valid 0.23048737990271492
LOSS train 0.2782575667725276 valid 0.23041538363727537
LOSS train 0.2782575667725276 valid 0.2304031292032097
EPOCH 10:
  batch 1 loss: 0.28513312339782715
  batch 2 loss: 0.30302824079990387
  batch 3 loss: 0.28818122545878094
  batch 4 loss: 0.2880700007081032
  batch 5 loss: 0.2861858308315277
  batch 6 loss: 0.28145883480707806
  batch 7 loss: 0.2831211728709085
  batch 8 loss: 0.2778580132871866
  batch 9 loss: 0.2774786137872272
  batch 10 loss: 0.2715670675039291
  batch 11 loss: 0.27460765025832434
  batch 12 loss: 0.27414771169424057
  batch 13 loss: 0.27122874328723323
  batch 14 loss: 0.27047743435416904
  batch 15 loss: 0.27145313918590547
  batch 16 loss: 0.2709764176979661
  batch 17 loss: 0.2716105606626062
  batch 18 loss: 0.27017295360565186
  batch 19 loss: 0.2705540311963935
  batch 20 loss: 0.26945879980921744
  batch 21 loss: 0.2728004079489481
  batch 22 loss: 0.2726395380767909
  batch 23 loss: 0.27101819541143335
  batch 24 loss: 0.27168020730217296
  batch 25 loss: 0.2725613451004028
  batch 26 loss: 0.27177990743747127
  batch 27 loss: 0.27430475420422024
  batch 28 loss: 0.2756252203668867
  batch 29 loss: 0.2751878222514843
  batch 30 loss: 0.2758389006058375
  batch 31 loss: 0.27650754201796746
  batch 32 loss: 0.27823584992438555
  batch 33 loss: 0.27980768861192645
  batch 34 loss: 0.2802128712920582
  batch 35 loss: 0.2811530990259988
  batch 36 loss: 0.28096779104736114
  batch 37 loss: 0.2808132099138724
  batch 38 loss: 0.28086572571804647
  batch 39 loss: 0.2809025064492837
  batch 40 loss: 0.2804486766457558
  batch 41 loss: 0.2802783439799053
  batch 42 loss: 0.28001718365010764
  batch 43 loss: 0.2798841089703316
  batch 44 loss: 0.2801521840420636
  batch 45 loss: 0.2797478980488247
  batch 46 loss: 0.2784164748761965
  batch 47 loss: 0.27781926063781087
  batch 48 loss: 0.2778406410167615
  batch 49 loss: 0.2788440737189079
  batch 50 loss: 0.2798024207353592
  batch 51 loss: 0.2797172998680788
  batch 52 loss: 0.27963125934967625
  batch 53 loss: 0.2790398285636362
  batch 54 loss: 0.2787444434232182
  batch 55 loss: 0.27906684143976734
  batch 56 loss: 0.27954436865236076
  batch 57 loss: 0.2786704341047688
  batch 58 loss: 0.27817812330763914
  batch 59 loss: 0.2781277801525795
  batch 60 loss: 0.27804231668512025
  batch 61 loss: 0.2794660684026656
  batch 62 loss: 0.27964992845250713
  batch 63 loss: 0.27898496247473215
  batch 64 loss: 0.2787921503186226
  batch 65 loss: 0.2783041853171129
  batch 66 loss: 0.2784061847311078
  batch 67 loss: 0.2784361149837722
  batch 68 loss: 0.27847358891192603
  batch 69 loss: 0.27836775606957037
  batch 70 loss: 0.2780585923365184
  batch 71 loss: 0.2782639634441322
  batch 72 loss: 0.27839455381035805
  batch 73 loss: 0.2784723435362724
  batch 74 loss: 0.2782766210871774
  batch 75 loss: 0.2783927341302236
  batch 76 loss: 0.27885992589749786
  batch 77 loss: 0.2788246517831629
  batch 78 loss: 0.27911940140601915
  batch 79 loss: 0.2792098284522189
  batch 80 loss: 0.2790594581514597
  batch 81 loss: 0.2789601709371732
  batch 82 loss: 0.2793343510569596
  batch 83 loss: 0.2792916358953499
  batch 84 loss: 0.27867258056288674
  batch 85 loss: 0.2789902900948244
  batch 86 loss: 0.27873327842978546
  batch 87 loss: 0.27841644143236094
  batch 88 loss: 0.2781415581703186
  batch 89 loss: 0.27764027875460934
  batch 90 loss: 0.27784845299190947
  batch 91 loss: 0.27749091616043675
  batch 92 loss: 0.2773550952906194
  batch 93 loss: 0.2768419860832153
  batch 94 loss: 0.2767475280672946
  batch 95 loss: 0.27681498480470557
  batch 96 loss: 0.27686373532439273
  batch 97 loss: 0.2772182820691276
  batch 98 loss: 0.27709823709969617
  batch 99 loss: 0.27687506799143974
  batch 100 loss: 0.27714525297284126
  batch 101 loss: 0.2770948263973293
  batch 102 loss: 0.2773957709763564
  batch 103 loss: 0.27736793284856
  batch 104 loss: 0.27790764819544095
  batch 105 loss: 0.27759250899155935
  batch 106 loss: 0.27774158471597815
  batch 107 loss: 0.27779673346292194
  batch 108 loss: 0.2775047715339396
  batch 109 loss: 0.27764113903592486
  batch 110 loss: 0.27758321558887306
  batch 111 loss: 0.2775832952948304
  batch 112 loss: 0.2775430084605302
  batch 113 loss: 0.2773603310363483
  batch 114 loss: 0.27716305865007534
  batch 115 loss: 0.2767742571623429
  batch 116 loss: 0.2764083926790747
  batch 117 loss: 0.27620452476872337
  batch 118 loss: 0.2762002986619028
  batch 119 loss: 0.27626321433472034
  batch 120 loss: 0.27598980652789273
  batch 121 loss: 0.2761710814454339
  batch 122 loss: 0.27590305394813663
  batch 123 loss: 0.27567122319364934
  batch 124 loss: 0.2755586840212345
  batch 125 loss: 0.27521528613567353
  batch 126 loss: 0.2753104308531398
  batch 127 loss: 0.2754102768156472
  batch 128 loss: 0.2754897183040157
  batch 129 loss: 0.2756439067365587
  batch 130 loss: 0.27544709959855446
  batch 131 loss: 0.27529530067935243
  batch 132 loss: 0.2753749777647582
  batch 133 loss: 0.2751931827095218
  batch 134 loss: 0.27486968185029814
  batch 135 loss: 0.27486123674445684
  batch 136 loss: 0.2752383611000636
  batch 137 loss: 0.2755841576922549
  batch 138 loss: 0.27558649871228397
  batch 139 loss: 0.2758723367675603
  batch 140 loss: 0.27608451023697855
  batch 141 loss: 0.27597706252378773
  batch 142 loss: 0.27584138250267004
  batch 143 loss: 0.27610199424353515
  batch 144 loss: 0.2760214102971885
  batch 145 loss: 0.2760945832934873
  batch 146 loss: 0.2763013267149664
  batch 147 loss: 0.2761322202122941
  batch 148 loss: 0.27607523941913165
  batch 149 loss: 0.27563670337600193
  batch 150 loss: 0.27572901248931886
  batch 151 loss: 0.2754143556419587
  batch 152 loss: 0.2752869241919957
  batch 153 loss: 0.2755773446333954
  batch 154 loss: 0.27537557298873927
  batch 155 loss: 0.275439951400603
  batch 156 loss: 0.2752703526654305
  batch 157 loss: 0.275247892091988
  batch 158 loss: 0.27540146521752396
  batch 159 loss: 0.2757587546260102
  batch 160 loss: 0.2757469126023352
  batch 161 loss: 0.2754461690320732
  batch 162 loss: 0.2754534379015734
  batch 163 loss: 0.2752732570551656
  batch 164 loss: 0.27552944767038995
  batch 165 loss: 0.2757492453762979
  batch 166 loss: 0.27593736781413297
  batch 167 loss: 0.27587325380233946
  batch 168 loss: 0.275700631773188
  batch 169 loss: 0.2758140343533465
  batch 170 loss: 0.27586538213140827
  batch 171 loss: 0.27617541844384713
  batch 172 loss: 0.27614624139874483
  batch 173 loss: 0.27606973723869105
  batch 174 loss: 0.2762317203584759
  batch 175 loss: 0.27642696823392593
  batch 176 loss: 0.276676552539522
  batch 177 loss: 0.2769275076981992
  batch 178 loss: 0.2767777206857553
  batch 179 loss: 0.27682930747223966
  batch 180 loss: 0.2768546298146248
  batch 181 loss: 0.2767600603195844
  batch 182 loss: 0.27671992303905907
  batch 183 loss: 0.27664366527333284
  batch 184 loss: 0.2765915559037872
  batch 185 loss: 0.27643340995182863
  batch 186 loss: 0.27650400187059115
  batch 187 loss: 0.27674878559966776
  batch 188 loss: 0.27663871527035183
  batch 189 loss: 0.27664725829369174
  batch 190 loss: 0.2765715620235393
  batch 191 loss: 0.2765972644714785
  batch 192 loss: 0.27647492359392345
  batch 193 loss: 0.2764321906115725
  batch 194 loss: 0.27626717958560926
  batch 195 loss: 0.2764288264207351
  batch 196 loss: 0.27633531247170606
  batch 197 loss: 0.2764260562392056
  batch 198 loss: 0.276423324764979
  batch 199 loss: 0.2764506042752434
  batch 200 loss: 0.27644198544323445
  batch 201 loss: 0.276383294765629
  batch 202 loss: 0.2763574253952149
  batch 203 loss: 0.27643608320229157
  batch 204 loss: 0.27656616423936453
  batch 205 loss: 0.27644697013424663
  batch 206 loss: 0.2764904190180371
  batch 207 loss: 0.2764654269684916
  batch 208 loss: 0.2762315364267964
  batch 209 loss: 0.2762775625766179
  batch 210 loss: 0.2762643549413908
  batch 211 loss: 0.2762911163658892
  batch 212 loss: 0.2763109524154438
  batch 213 loss: 0.2761274756680072
  batch 214 loss: 0.2760947243632557
  batch 215 loss: 0.27610644989235456
  batch 216 loss: 0.2761039686975656
  batch 217 loss: 0.2760396948607836
  batch 218 loss: 0.2760100241647948
  batch 219 loss: 0.27584303793025344
  batch 220 loss: 0.2759956281970848
  batch 221 loss: 0.2759044870936493
  batch 222 loss: 0.2760528507936108
  batch 223 loss: 0.275957332932361
  batch 224 loss: 0.275998942487474
  batch 225 loss: 0.2760654406415092
  batch 226 loss: 0.2760686260540928
  batch 227 loss: 0.2761485724328373
  batch 228 loss: 0.27610657462163973
  batch 229 loss: 0.276046337844503
  batch 230 loss: 0.27603320693192274
  batch 231 loss: 0.27598527002902257
  batch 232 loss: 0.27597123069752905
  batch 233 loss: 0.2758779896047494
  batch 234 loss: 0.27583343191788745
  batch 235 loss: 0.27577600117693557
  batch 236 loss: 0.2757019210543673
  batch 237 loss: 0.2755175828304975
  batch 238 loss: 0.27537243218482044
  batch 239 loss: 0.2753074710339183
  batch 240 loss: 0.27539127754668397
  batch 241 loss: 0.2752763583205053
  batch 242 loss: 0.27513728358528833
  batch 243 loss: 0.275134206921966
  batch 244 loss: 0.2751001142820374
  batch 245 loss: 0.27506717930034713
  batch 246 loss: 0.2749743782650165
  batch 247 loss: 0.2749740162841704
  batch 248 loss: 0.27510443678305996
  batch 249 loss: 0.27502773217408055
  batch 250 loss: 0.27492306447029113
  batch 251 loss: 0.274736502434153
  batch 252 loss: 0.2747890008465638
  batch 253 loss: 0.2746050978013178
  batch 254 loss: 0.2745248659506557
  batch 255 loss: 0.2744552850138907
  batch 256 loss: 0.2743571680621244
  batch 257 loss: 0.27434543673862277
  batch 258 loss: 0.274274766849455
  batch 259 loss: 0.2740966149508723
  batch 260 loss: 0.27415022815649326
  batch 261 loss: 0.274084547584541
  batch 262 loss: 0.273999975496576
  batch 263 loss: 0.2739264383742111
  batch 264 loss: 0.27399330642638786
  batch 265 loss: 0.2739339107612394
  batch 266 loss: 0.27374364514099925
  batch 267 loss: 0.273519262894709
  batch 268 loss: 0.27338385209441185
  batch 269 loss: 0.2733382952146814
  batch 270 loss: 0.2732937750441057
  batch 271 loss: 0.27322035450557064
  batch 272 loss: 0.27312956229947943
  batch 273 loss: 0.2729909732228234
  batch 274 loss: 0.27281711359311195
  batch 275 loss: 0.2728454557873986
  batch 276 loss: 0.2727746872258359
  batch 277 loss: 0.272721910358336
  batch 278 loss: 0.2726250345543992
  batch 279 loss: 0.2726060106121938
  batch 280 loss: 0.27251796147653035
  batch 281 loss: 0.27241622362272594
  batch 282 loss: 0.2723009913948411
  batch 283 loss: 0.272236433130271
  batch 284 loss: 0.272269129228424
  batch 285 loss: 0.2721416050927681
  batch 286 loss: 0.27214741237930484
  batch 287 loss: 0.2720474286673376
  batch 288 loss: 0.2720758826471865
  batch 289 loss: 0.2718927676186842
  batch 290 loss: 0.27176339061095794
  batch 291 loss: 0.27168145632416113
  batch 292 loss: 0.27162655943060576
  batch 293 loss: 0.2716276648711833
  batch 294 loss: 0.27138187062172664
  batch 295 loss: 0.27134041220454846
  batch 296 loss: 0.2714673064246371
  batch 297 loss: 0.2714391887589336
  batch 298 loss: 0.2714572799885833
  batch 299 loss: 0.2715438102001331
  batch 300 loss: 0.27139247963825863
  batch 301 loss: 0.27142605502344047
  batch 302 loss: 0.2715921704934922
  batch 303 loss: 0.2715115158668052
  batch 304 loss: 0.27149413310383497
  batch 305 loss: 0.2714472294830885
  batch 306 loss: 0.2715175555032842
  batch 307 loss: 0.2714467061264895
  batch 308 loss: 0.27153289850269047
  batch 309 loss: 0.27151767316373804
  batch 310 loss: 0.2715367232599566
  batch 311 loss: 0.27141390812741983
  batch 312 loss: 0.27138010870951873
  batch 313 loss: 0.2712868433029126
  batch 314 loss: 0.27117369167364325
  batch 315 loss: 0.2711200690458691
  batch 316 loss: 0.27105301962811734
  batch 317 loss: 0.27112514133145005
  batch 318 loss: 0.27088041805058904
  batch 319 loss: 0.2706677783339001
  batch 320 loss: 0.27080911383964124
  batch 321 loss: 0.2707906206170346
  batch 322 loss: 0.27062729560995696
  batch 323 loss: 0.27074746538426486
  batch 324 loss: 0.27080944718586075
  batch 325 loss: 0.27088184874791366
  batch 326 loss: 0.2710774310070313
  batch 327 loss: 0.27104582885901135
  batch 328 loss: 0.2710107106533719
  batch 329 loss: 0.2710594088744972
  batch 330 loss: 0.27105985297398133
  batch 331 loss: 0.2710696585318833
  batch 332 loss: 0.27117941165006304
  batch 333 loss: 0.27111331415963963
  batch 334 loss: 0.27134823709904793
  batch 335 loss: 0.27144104909541
  batch 336 loss: 0.2714613936841488
  batch 337 loss: 0.27144559483853575
  batch 338 loss: 0.27132723180676355
  batch 339 loss: 0.2712965694236896
  batch 340 loss: 0.2711393964202965
  batch 341 loss: 0.2710962975042656
  batch 342 loss: 0.2710308272674767
  batch 343 loss: 0.27112802780578155
  batch 344 loss: 0.27114484862012916
  batch 345 loss: 0.27119198972764225
  batch 346 loss: 0.2711884039588746
  batch 347 loss: 0.27107843249950353
  batch 348 loss: 0.2710973151113795
  batch 349 loss: 0.27111362727801913
  batch 350 loss: 0.27119982736451287
  batch 351 loss: 0.27113857254003865
  batch 352 loss: 0.27103879929266195
  batch 353 loss: 0.2710138155785904
  batch 354 loss: 0.2709355058390542
  batch 355 loss: 0.2708846833924173
  batch 356 loss: 0.27088108964348107
  batch 357 loss: 0.2706765924014297
  batch 358 loss: 0.2705498301949581
  batch 359 loss: 0.27055129500152675
  batch 360 loss: 0.27049003574583264
  batch 361 loss: 0.27049636271191435
  batch 362 loss: 0.2704380594646733
  batch 363 loss: 0.270463410610995
  batch 364 loss: 0.2704012111893722
  batch 365 loss: 0.2703561584018681
  batch 366 loss: 0.2703726948366139
  batch 367 loss: 0.27038643823819847
  batch 368 loss: 0.27037794025533873
  batch 369 loss: 0.27033292984736323
  batch 370 loss: 0.27038033165641734
  batch 371 loss: 0.270428889965754
  batch 372 loss: 0.27050951399630113
  batch 373 loss: 0.2705215941565286
  batch 374 loss: 0.2704897642852788
  batch 375 loss: 0.2705750626325607
  batch 376 loss: 0.2707885088834991
  batch 377 loss: 0.270789696817689
  batch 378 loss: 0.27074839044657967
  batch 379 loss: 0.2707289838303362
  batch 380 loss: 0.2707775457517097
  batch 381 loss: 0.2707674475557848
  batch 382 loss: 0.27075538979306896
  batch 383 loss: 0.2706177966868597
  batch 384 loss: 0.2705816845409572
  batch 385 loss: 0.2705118728535516
  batch 386 loss: 0.27041809002198086
  batch 387 loss: 0.27034673083198163
  batch 388 loss: 0.2702635406065233
  batch 389 loss: 0.27030590651580794
  batch 390 loss: 0.270300276692097
  batch 391 loss: 0.2701943761399945
  batch 392 loss: 0.27020011773827124
  batch 393 loss: 0.27032700400922743
  batch 394 loss: 0.2703042512163898
  batch 395 loss: 0.27035890485666975
  batch 396 loss: 0.270426792177287
  batch 397 loss: 0.2705133133180796
  batch 398 loss: 0.2705680534018943
  batch 399 loss: 0.2705807592486379
  batch 400 loss: 0.2707103496044874
  batch 401 loss: 0.27065779328197614
  batch 402 loss: 0.2706412967683664
  batch 403 loss: 0.270533093336794
  batch 404 loss: 0.2704761502249996
  batch 405 loss: 0.2704538375875096
  batch 406 loss: 0.2703578100709492
  batch 407 loss: 0.2703763803041538
  batch 408 loss: 0.2702762108369201
  batch 409 loss: 0.2702746089629847
  batch 410 loss: 0.27015291572344013
  batch 411 loss: 0.27013589231492247
  batch 412 loss: 0.27030356543012035
  batch 413 loss: 0.2702841007175515
  batch 414 loss: 0.270323780387784
  batch 415 loss: 0.2702965442315642
  batch 416 loss: 0.2702888091667913
  batch 417 loss: 0.2702429763776221
  batch 418 loss: 0.2702420557443605
  batch 419 loss: 0.270263381853593
  batch 420 loss: 0.27028775530911625
  batch 421 loss: 0.27037841828447057
  batch 422 loss: 0.2703108559428798
  batch 423 loss: 0.2702981056755598
  batch 424 loss: 0.2703550804476693
  batch 425 loss: 0.2702872354844037
  batch 426 loss: 0.27029152144568624
  batch 427 loss: 0.2703169341388854
  batch 428 loss: 0.2703522617850348
  batch 429 loss: 0.2702780572267679
  batch 430 loss: 0.27035044521786444
  batch 431 loss: 0.27039070078225813
  batch 432 loss: 0.27044238301890866
  batch 433 loss: 0.2703785727329673
  batch 434 loss: 0.27028106819100095
  batch 435 loss: 0.270341446481902
  batch 436 loss: 0.27032946159533405
  batch 437 loss: 0.2702053796917677
  batch 438 loss: 0.2701098246321286
  batch 439 loss: 0.2700495659829815
  batch 440 loss: 0.2701943806965243
  batch 441 loss: 0.27022296828883036
  batch 442 loss: 0.2702159903505269
  batch 443 loss: 0.27021614527729243
  batch 444 loss: 0.27022813545825247
  batch 445 loss: 0.27019669544161035
  batch 446 loss: 0.2702645450629996
  batch 447 loss: 0.2703176828505475
  batch 448 loss: 0.2703919969831726
  batch 449 loss: 0.2703724262244452
  batch 450 loss: 0.27033483117818835
  batch 451 loss: 0.2702397303345991
  batch 452 loss: 0.2701893745534188
  batch 453 loss: 0.27017940906499394
  batch 454 loss: 0.27014247600465097
  batch 455 loss: 0.27014048649714545
  batch 456 loss: 0.27007664952492505
  batch 457 loss: 0.2700854361513772
  batch 458 loss: 0.2700741377915357
  batch 459 loss: 0.2700200124698527
  batch 460 loss: 0.27000479429312374
  batch 461 loss: 0.27004141750020216
  batch 462 loss: 0.27004156403598334
  batch 463 loss: 0.2700789376095613
  batch 464 loss: 0.27005432716345995
  batch 465 loss: 0.2700473541534075
  batch 466 loss: 0.2699359110176819
  batch 467 loss: 0.27006159256944307
  batch 468 loss: 0.2700333112898545
  batch 469 loss: 0.2699493482740703
  batch 470 loss: 0.2700574536589866
  batch 471 loss: 0.27015988807136593
  batch 472 loss: 0.26998519786965036
LOSS train 0.26998519786965036 valid 0.24578508734703064
LOSS train 0.26998519786965036 valid 0.24116727709770203
LOSS train 0.26998519786965036 valid 0.24746874968210855
LOSS train 0.26998519786965036 valid 0.23017145693302155
LOSS train 0.26998519786965036 valid 0.2341233491897583
LOSS train 0.26998519786965036 valid 0.23943636318047842
LOSS train 0.26998519786965036 valid 0.23621620450701034
LOSS train 0.26998519786965036 valid 0.23470059968531132
LOSS train 0.26998519786965036 valid 0.234137248661783
LOSS train 0.26998519786965036 valid 0.23295197039842605
LOSS train 0.26998519786965036 valid 0.23279564624482935
LOSS train 0.26998519786965036 valid 0.23609371607502302
LOSS train 0.26998519786965036 valid 0.23625065844792587
LOSS train 0.26998519786965036 valid 0.23441356952701295
LOSS train 0.26998519786965036 valid 0.23400396804014842
LOSS train 0.26998519786965036 valid 0.2372533781453967
LOSS train 0.26998519786965036 valid 0.23830861642080195
LOSS train 0.26998519786965036 valid 0.23822622994581857
LOSS train 0.26998519786965036 valid 0.24034712659685234
LOSS train 0.26998519786965036 valid 0.2399447187781334
LOSS train 0.26998519786965036 valid 0.24166787238348097
LOSS train 0.26998519786965036 valid 0.2414603897116401
LOSS train 0.26998519786965036 valid 0.23937905612199203
LOSS train 0.26998519786965036 valid 0.24032539005080858
LOSS train 0.26998519786965036 valid 0.240188826918602
LOSS train 0.26998519786965036 valid 0.23939392257195252
LOSS train 0.26998519786965036 valid 0.23941826985941994
LOSS train 0.26998519786965036 valid 0.23979669223938668
LOSS train 0.26998519786965036 valid 0.23814207675128146
LOSS train 0.26998519786965036 valid 0.2381374587615331
LOSS train 0.26998519786965036 valid 0.23859843707853748
LOSS train 0.26998519786965036 valid 0.2392013594508171
LOSS train 0.26998519786965036 valid 0.23847085508433255
LOSS train 0.26998519786965036 valid 0.23778751141884746
LOSS train 0.26998519786965036 valid 0.23841777954782759
LOSS train 0.26998519786965036 valid 0.23926961090829638
LOSS train 0.26998519786965036 valid 0.2397832677171037
LOSS train 0.26998519786965036 valid 0.23985585099772402
LOSS train 0.26998519786965036 valid 0.24080978983487838
LOSS train 0.26998519786965036 valid 0.24095224887132644
LOSS train 0.26998519786965036 valid 0.2404979689092171
LOSS train 0.26998519786965036 valid 0.24213361420801707
LOSS train 0.26998519786965036 valid 0.24261151670023454
LOSS train 0.26998519786965036 valid 0.24193521622907033
LOSS train 0.26998519786965036 valid 0.24150977929433187
LOSS train 0.26998519786965036 valid 0.2412153009487235
LOSS train 0.26998519786965036 valid 0.2410211277768967
LOSS train 0.26998519786965036 valid 0.24277032849689326
LOSS train 0.26998519786965036 valid 0.24198362596180975
LOSS train 0.26998519786965036 valid 0.2426255625486374
LOSS train 0.26998519786965036 valid 0.24220415277808321
LOSS train 0.26998519786965036 valid 0.24183978684819663
LOSS train 0.26998519786965036 valid 0.2433075227265088
LOSS train 0.26998519786965036 valid 0.24309170825613868
LOSS train 0.26998519786965036 valid 0.24281711768020284
LOSS train 0.26998519786965036 valid 0.24292162619531155
LOSS train 0.26998519786965036 valid 0.24222234231338166
LOSS train 0.26998519786965036 valid 0.24294589896654262
LOSS train 0.26998519786965036 valid 0.2426990475189888
LOSS train 0.26998519786965036 valid 0.2423701047897339
LOSS train 0.26998519786965036 valid 0.24242313645902228
LOSS train 0.26998519786965036 valid 0.24204478196559415
LOSS train 0.26998519786965036 valid 0.2417318858797588
LOSS train 0.26998519786965036 valid 0.24193408899009228
LOSS train 0.26998519786965036 valid 0.24080731158073132
LOSS train 0.26998519786965036 valid 0.2406778335571289
LOSS train 0.26998519786965036 valid 0.24135582260231472
LOSS train 0.26998519786965036 valid 0.24075345826499603
LOSS train 0.26998519786965036 valid 0.24157006412312604
LOSS train 0.26998519786965036 valid 0.24196841631616864
LOSS train 0.26998519786965036 valid 0.24205738180120226
LOSS train 0.26998519786965036 valid 0.24249139552315077
LOSS train 0.26998519786965036 valid 0.2430994931149156
LOSS train 0.26998519786965036 valid 0.2426454726908658
LOSS train 0.26998519786965036 valid 0.24223652104536691
LOSS train 0.26998519786965036 valid 0.2422451057324284
LOSS train 0.26998519786965036 valid 0.24217383021658118
LOSS train 0.26998519786965036 valid 0.2418807095442063
LOSS train 0.26998519786965036 valid 0.2418098809975612
LOSS train 0.26998519786965036 valid 0.24139841925352812
LOSS train 0.26998519786965036 valid 0.2415919592719019
LOSS train 0.26998519786965036 valid 0.24156864805192482
LOSS train 0.26998519786965036 valid 0.2417594566043601
LOSS train 0.26998519786965036 valid 0.24148230503002802
LOSS train 0.26998519786965036 valid 0.2421080371912788
LOSS train 0.26998519786965036 valid 0.24206054349278294
LOSS train 0.26998519786965036 valid 0.24159411229621405
LOSS train 0.26998519786965036 valid 0.24188579839061608
LOSS train 0.26998519786965036 valid 0.24227385825655434
LOSS train 0.26998519786965036 valid 0.24250984970066283
LOSS train 0.26998519786965036 valid 0.2426436374803166
LOSS train 0.26998519786965036 valid 0.2427119133265122
LOSS train 0.26998519786965036 valid 0.24257685884993563
LOSS train 0.26998519786965036 valid 0.24273674903397865
LOSS train 0.26998519786965036 valid 0.2430059214955882
LOSS train 0.26998519786965036 valid 0.24322306380296746
LOSS train 0.26998519786965036 valid 0.24326774769837095
LOSS train 0.26998519786965036 valid 0.24372106532053073
LOSS train 0.26998519786965036 valid 0.243859552223273
LOSS train 0.26998519786965036 valid 0.24396753653883935
LOSS train 0.26998519786965036 valid 0.24402972214882918
LOSS train 0.26998519786965036 valid 0.24444327883276284
LOSS train 0.26998519786965036 valid 0.24420961028742558
LOSS train 0.26998519786965036 valid 0.2441211835696147
LOSS train 0.26998519786965036 valid 0.2443151173137483
LOSS train 0.26998519786965036 valid 0.24422435125090042
LOSS train 0.26998519786965036 valid 0.2437941771801387
LOSS train 0.26998519786965036 valid 0.24383438764898865
LOSS train 0.26998519786965036 valid 0.24342518580069236
LOSS train 0.26998519786965036 valid 0.24352444301952014
LOSS train 0.26998519786965036 valid 0.24375393020140157
LOSS train 0.26998519786965036 valid 0.24411462167544024
LOSS train 0.26998519786965036 valid 0.2438862582223605
LOSS train 0.26998519786965036 valid 0.24391313903687292
LOSS train 0.26998519786965036 valid 0.24452272977518
LOSS train 0.26998519786965036 valid 0.24415556457022142
LOSS train 0.26998519786965036 valid 0.24475169653057033
LOSS train 0.26998519786965036 valid 0.24457459682125157
LOSS train 0.26998519786965036 valid 0.24418608105483175
LOSS train 0.26998519786965036 valid 0.24384033506115277
LOSS train 0.26998519786965036 valid 0.2437407179804873
LOSS train 0.26998519786965036 valid 0.24395189788497862
LOSS train 0.26998519786965036 valid 0.2440349137395378
LOSS train 0.26998519786965036 valid 0.2442047992540944
LOSS train 0.26998519786965036 valid 0.24412792146205903
LOSS train 0.26998519786965036 valid 0.24441042672547084
LOSS train 0.26998519786965036 valid 0.24430803391407793
LOSS train 0.26998519786965036 valid 0.24421606736723334
LOSS train 0.26998519786965036 valid 0.2438235006822172
LOSS train 0.26998519786965036 valid 0.2435172616289212
LOSS train 0.26998519786965036 valid 0.24351010625143998
LOSS train 0.26998519786965036 valid 0.24344928641662453
LOSS train 0.26998519786965036 valid 0.24355214576524004
LOSS train 0.26998519786965036 valid 0.24392246810802773
LOSS train 0.26998519786965036 valid 0.243954794715952
LOSS train 0.26998519786965036 valid 0.24397900755352833
LOSS train 0.26998519786965036 valid 0.24411741149251479
LOSS train 0.26998519786965036 valid 0.24394723773002625
LOSS train 0.26998519786965036 valid 0.24371931042602593
LOSS train 0.26998519786965036 valid 0.24361897983721326
LOSS train 0.26998519786965036 valid 0.24364393674735482
LOSS train 0.26998519786965036 valid 0.24398866526677576
LOSS train 0.26998519786965036 valid 0.24407805554516665
LOSS train 0.26998519786965036 valid 0.2441255742063125
LOSS train 0.26998519786965036 valid 0.24405808880411345
LOSS train 0.26998519786965036 valid 0.2439687190807029
LOSS train 0.26998519786965036 valid 0.24389747383237695
LOSS train 0.26998519786965036 valid 0.24379648580341726
LOSS train 0.26998519786965036 valid 0.24384494765892925
LOSS train 0.26998519786965036 valid 0.2437971896926562
LOSS train 0.26998519786965036 valid 0.24364608822279418
LOSS train 0.26998519786965036 valid 0.2435567084895937
LOSS train 0.26998519786965036 valid 0.24335626634507398
LOSS train 0.26998519786965036 valid 0.2434449109744716
LOSS train 0.26998519786965036 valid 0.2434832577743838
LOSS train 0.26998519786965036 valid 0.24352790539463362
LOSS train 0.26998519786965036 valid 0.24338399822924547
LOSS train 0.26998519786965036 valid 0.2434519351094584
LOSS train 0.26998519786965036 valid 0.24368620996580184
LOSS train 0.26998519786965036 valid 0.24388793716207147
LOSS train 0.26998519786965036 valid 0.24375287651645472
LOSS train 0.26998519786965036 valid 0.24381472602670576
LOSS train 0.26998519786965036 valid 0.24354030356451045
LOSS train 0.26998519786965036 valid 0.24360339098223827
LOSS train 0.26998519786965036 valid 0.24370103975137075
LOSS train 0.26998519786965036 valid 0.24358844954565348
LOSS train 0.26998519786965036 valid 0.24365791267977505
LOSS train 0.26998519786965036 valid 0.24354637529523598
LOSS train 0.26998519786965036 valid 0.2433545808114949
LOSS train 0.26998519786965036 valid 0.24346708199557135
LOSS train 0.26998519786965036 valid 0.2433296539630109
LOSS train 0.26998519786965036 valid 0.24337134181067002
LOSS train 0.26998519786965036 valid 0.24329612442868292
LOSS train 0.26998519786965036 valid 0.24364598326641937
LOSS train 0.26998519786965036 valid 0.2436270431961332
LOSS train 0.26998519786965036 valid 0.2435126488351009
LOSS train 0.26998519786965036 valid 0.243558392938921
LOSS train 0.26998519786965036 valid 0.24364107604442017
LOSS train 0.26998519786965036 valid 0.2438533495091859
LOSS train 0.26998519786965036 valid 0.24377013304167325
LOSS train 0.26998519786965036 valid 0.24370819075002195
LOSS train 0.26998519786965036 valid 0.24350610072468662
LOSS train 0.26998519786965036 valid 0.24358074301904667
LOSS train 0.26998519786965036 valid 0.2435729220185591
LOSS train 0.26998519786965036 valid 0.24355395001334113
LOSS train 0.26998519786965036 valid 0.2436030872406498
LOSS train 0.26998519786965036 valid 0.2434014322604725
LOSS train 0.26998519786965036 valid 0.24347178067298644
LOSS train 0.26998519786965036 valid 0.2432790987548374
LOSS train 0.26998519786965036 valid 0.2433220776288133
LOSS train 0.26998519786965036 valid 0.24348076171587898
LOSS train 0.26998519786965036 valid 0.24357040498095253
LOSS train 0.26998519786965036 valid 0.24333944067436178
LOSS train 0.26998519786965036 valid 0.2432186723369913
LOSS train 0.26998519786965036 valid 0.24312262275280097
LOSS train 0.26998519786965036 valid 0.24329085724086177
LOSS train 0.26998519786965036 valid 0.2432356766486531
LOSS train 0.26998519786965036 valid 0.24349948756321513
LOSS train 0.26998519786965036 valid 0.24354178844085292
LOSS train 0.26998519786965036 valid 0.24372647486627103
LOSS train 0.26998519786965036 valid 0.24355045998867472
LOSS train 0.26998519786965036 valid 0.2434627727264225
LOSS train 0.26998519786965036 valid 0.2435542777400886
LOSS train 0.26998519786965036 valid 0.24347072890868374
LOSS train 0.26998519786965036 valid 0.2434213904345908
LOSS train 0.26998519786965036 valid 0.24339746430660913
LOSS train 0.26998519786965036 valid 0.24338383936651664
LOSS train 0.26998519786965036 valid 0.24322793704385942
LOSS train 0.26998519786965036 valid 0.24319810744678005
LOSS train 0.26998519786965036 valid 0.24329863545440492
LOSS train 0.26998519786965036 valid 0.24349713607986956
LOSS train 0.26998519786965036 valid 0.2433203710418827
LOSS train 0.26998519786965036 valid 0.2432045146053386
LOSS train 0.26998519786965036 valid 0.24306834287175508
LOSS train 0.26998519786965036 valid 0.24280710095583005
LOSS train 0.26998519786965036 valid 0.24259142605242906
LOSS train 0.26998519786965036 valid 0.24232088922080905
LOSS train 0.26998519786965036 valid 0.24233988617811728
LOSS train 0.26998519786965036 valid 0.24233535823484534
LOSS train 0.26998519786965036 valid 0.2424274783920158
LOSS train 0.26998519786965036 valid 0.24242140555004188
LOSS train 0.26998519786965036 valid 0.24253177032008902
LOSS train 0.26998519786965036 valid 0.2426353972989882
LOSS train 0.26998519786965036 valid 0.2425642698071897
LOSS train 0.26998519786965036 valid 0.24240012877517275
LOSS train 0.26998519786965036 valid 0.24226490421369012
LOSS train 0.26998519786965036 valid 0.24227888245414533
LOSS train 0.26998519786965036 valid 0.24224396259115452
LOSS train 0.26998519786965036 valid 0.24211667133210527
LOSS train 0.26998519786965036 valid 0.24218537677889285
LOSS train 0.26998519786965036 valid 0.2421050016885196
LOSS train 0.26998519786965036 valid 0.24225186485925626
LOSS train 0.26998519786965036 valid 0.2426465426775519
LOSS train 0.26998519786965036 valid 0.24261140906148487
LOSS train 0.26998519786965036 valid 0.24263327204166574
LOSS train 0.26998519786965036 valid 0.2425150823290065
LOSS train 0.26998519786965036 valid 0.24256328565661917
LOSS train 0.26998519786965036 valid 0.2425399438298049
LOSS train 0.26998519786965036 valid 0.24255456142595125
LOSS train 0.26998519786965036 valid 0.2421215834096074
LOSS train 0.26998519786965036 valid 0.2419366476075778
LOSS train 0.26998519786965036 valid 0.24187970610935827
LOSS train 0.26998519786965036 valid 0.24181882380948636
LOSS train 0.26998519786965036 valid 0.24177399937246666
LOSS train 0.26998519786965036 valid 0.24195115225655692
LOSS train 0.26998519786965036 valid 0.24198502823104703
LOSS train 0.26998519786965036 valid 0.2420351703157309
LOSS train 0.26998519786965036 valid 0.24203013075936225
LOSS train 0.26998519786965036 valid 0.2418890953901781
LOSS train 0.26998519786965036 valid 0.24203297728300094
LOSS train 0.26998519786965036 valid 0.24207544059629935
LOSS train 0.26998519786965036 valid 0.2420241679582331
LOSS train 0.26998519786965036 valid 0.24182970768849368
LOSS train 0.26998519786965036 valid 0.2419313239300345
LOSS train 0.26998519786965036 valid 0.2417873717990576
LOSS train 0.26998519786965036 valid 0.24156836158363149
LOSS train 0.26998519786965036 valid 0.24159606931738353
LOSS train 0.26998519786965036 valid 0.2417646603759869
LOSS train 0.26998519786965036 valid 0.24172178681752857
LOSS train 0.26998519786965036 valid 0.2415847252194698
LOSS train 0.26998519786965036 valid 0.24158332939348914
LOSS train 0.26998519786965036 valid 0.2417521716756675
LOSS train 0.26998519786965036 valid 0.2416150505891771
LOSS train 0.26998519786965036 valid 0.24162104347664298
LOSS train 0.26998519786965036 valid 0.24157776528934263
LOSS train 0.26998519786965036 valid 0.24157132544463739
LOSS train 0.26998519786965036 valid 0.24148009656073896
LOSS train 0.26998519786965036 valid 0.2415466801221691
LOSS train 0.26998519786965036 valid 0.24165253579394969
LOSS train 0.26998519786965036 valid 0.2414934738918587
LOSS train 0.26998519786965036 valid 0.24149352900436444
LOSS train 0.26998519786965036 valid 0.24141883888446233
LOSS train 0.26998519786965036 valid 0.24126732977100343
LOSS train 0.26998519786965036 valid 0.24117020441450343
LOSS train 0.26998519786965036 valid 0.24126018085262993
LOSS train 0.26998519786965036 valid 0.24131434092271156
LOSS train 0.26998519786965036 valid 0.2413387490093493
LOSS train 0.26998519786965036 valid 0.24133323427798936
LOSS train 0.26998519786965036 valid 0.24132725441541295
LOSS train 0.26998519786965036 valid 0.24121601288872105
LOSS train 0.26998519786965036 valid 0.24106353542558664
LOSS train 0.26998519786965036 valid 0.24106143668611
LOSS train 0.26998519786965036 valid 0.24109876092668137
LOSS train 0.26998519786965036 valid 0.241030575838727
LOSS train 0.26998519786965036 valid 0.24100530450804192
LOSS train 0.26998519786965036 valid 0.2409834362618573
LOSS train 0.26998519786965036 valid 0.24102435138998132
LOSS train 0.26998519786965036 valid 0.24099973646096057
LOSS train 0.26998519786965036 valid 0.24093135474668653
LOSS train 0.26998519786965036 valid 0.24081591254678267
LOSS train 0.26998519786965036 valid 0.2407986947760959
LOSS train 0.26998519786965036 valid 0.24070996872774542
LOSS train 0.26998519786965036 valid 0.24068595479169397
LOSS train 0.26998519786965036 valid 0.24073669400547637
LOSS train 0.26998519786965036 valid 0.24077644605757825
LOSS train 0.26998519786965036 valid 0.24072407231338927
LOSS train 0.26998519786965036 valid 0.2406350514202407
LOSS train 0.26998519786965036 valid 0.24067412031776952
LOSS train 0.26998519786965036 valid 0.24063354755524408
LOSS train 0.26998519786965036 valid 0.24053758790095647
LOSS train 0.26998519786965036 valid 0.24042663037578926
LOSS train 0.26998519786965036 valid 0.24038004638343458
LOSS train 0.26998519786965036 valid 0.24029772873758876
LOSS train 0.26998519786965036 valid 0.24030107235241877
LOSS train 0.26998519786965036 valid 0.24024459983481736
LOSS train 0.26998519786965036 valid 0.24045594844942778
LOSS train 0.26998519786965036 valid 0.24047671074005214
LOSS train 0.26998519786965036 valid 0.24055213795660377
LOSS train 0.26998519786965036 valid 0.24055598425440805
LOSS train 0.26998519786965036 valid 0.24058560118559869
LOSS train 0.26998519786965036 valid 0.24058834092026737
LOSS train 0.26998519786965036 valid 0.24054561469417352
LOSS train 0.26998519786965036 valid 0.24052934258128889
LOSS train 0.26998519786965036 valid 0.24042581577020086
LOSS train 0.26998519786965036 valid 0.24038159227560438
LOSS train 0.26998519786965036 valid 0.24032220400021045
LOSS train 0.26998519786965036 valid 0.24035063409466847
LOSS train 0.26998519786965036 valid 0.24037191787230894
LOSS train 0.26998519786965036 valid 0.24034324736804424
LOSS train 0.26998519786965036 valid 0.24023323673754932
LOSS train 0.26998519786965036 valid 0.2403099120034607
LOSS train 0.26998519786965036 valid 0.24028404843733178
LOSS train 0.26998519786965036 valid 0.24017104571079692
LOSS train 0.26998519786965036 valid 0.24011206222169193
LOSS train 0.26998519786965036 valid 0.23995512696412893
LOSS train 0.26998519786965036 valid 0.2400822757394767
LOSS train 0.26998519786965036 valid 0.240001983507693
LOSS train 0.26998519786965036 valid 0.24000289740904077
LOSS train 0.26998519786965036 valid 0.2399680126220622
LOSS train 0.26998519786965036 valid 0.23999987262668032
LOSS train 0.26998519786965036 valid 0.2399184724985653
LOSS train 0.26998519786965036 valid 0.23992295850472278
LOSS train 0.26998519786965036 valid 0.24000403168681148
LOSS train 0.26998519786965036 valid 0.2399131554864838
LOSS train 0.26998519786965036 valid 0.2397685227109425
LOSS train 0.26998519786965036 valid 0.2397174404135772
LOSS train 0.26998519786965036 valid 0.23987077755814845
LOSS train 0.26998519786965036 valid 0.2398592148130462
LOSS train 0.26998519786965036 valid 0.23983385965535775
LOSS train 0.26998519786965036 valid 0.23995713538983288
LOSS train 0.26998519786965036 valid 0.23988843803182025
LOSS train 0.26998519786965036 valid 0.2398639914323712
LOSS train 0.26998519786965036 valid 0.23972470341548976
LOSS train 0.26998519786965036 valid 0.23978488450480062
LOSS train 0.26998519786965036 valid 0.23989133091940396
LOSS train 0.26998519786965036 valid 0.2398528721118938
LOSS train 0.26998519786965036 valid 0.2397589408698618
LOSS train 0.26998519786965036 valid 0.23981527453181387
LOSS train 0.26998519786965036 valid 0.2398045005378204
LOSS train 0.26998519786965036 valid 0.23982154973915645
LOSS train 0.26998519786965036 valid 0.2399424366292111
LOSS train 0.26998519786965036 valid 0.23997181086716327
LOSS train 0.26998519786965036 valid 0.24003053825907936
LOSS train 0.26998519786965036 valid 0.24006837345449264
LOSS train 0.26998519786965036 valid 0.2399499856250387
LOSS train 0.26998519786965036 valid 0.2399780661118834
LOSS train 0.26998519786965036 valid 0.23997577786946497
LOSS train 0.26998519786965036 valid 0.23990200039561235
LOSS train 0.26998519786965036 valid 0.23985043313343876
LOSS train 0.26998519786965036 valid 0.23998001321322388
LOSS train 0.26998519786965036 valid 0.2398888161225332
LOSS train 0.26998519786965036 valid 0.2399448574165613
LOSS train 0.26998519786965036 valid 0.24001880367761144
LOSS train 0.26998519786965036 valid 0.23994624979057155
LOSS train 0.26998519786965036 valid 0.24005204965806987
LOSS train 0.26998519786965036 valid 0.24000025634231464
LOSS train 0.26998519786965036 valid 0.23997233296447618
LOSS train 0.26998519786965036 valid 0.23990178096067646
LOSS train 0.26998519786965036 valid 0.239887480571018
EPOCH 11:
  batch 1 loss: 0.2735396921634674
  batch 2 loss: 0.28851403295993805
  batch 3 loss: 0.27988911668459576
  batch 4 loss: 0.27690180391073227
  batch 5 loss: 0.27851265072822573
  batch 6 loss: 0.27506662905216217
  batch 7 loss: 0.27534806728363037
  batch 8 loss: 0.2712701726704836
  batch 9 loss: 0.2717229376236598
  batch 10 loss: 0.2663981065154076
  batch 11 loss: 0.2677417167208411
  batch 12 loss: 0.2696091867983341
  batch 13 loss: 0.2660915163847116
  batch 14 loss: 0.2654149149145399
  batch 15 loss: 0.2673238178094228
  batch 16 loss: 0.2678156699985266
  batch 17 loss: 0.2703736424446106
  batch 18 loss: 0.2690020178755124
  batch 19 loss: 0.26844297820015955
  batch 20 loss: 0.2667094372212887
  batch 21 loss: 0.2703124348606382
  batch 22 loss: 0.27035460756583646
  batch 23 loss: 0.26957817246084625
  batch 24 loss: 0.27007693238556385
  batch 25 loss: 0.27116392910480497
  batch 26 loss: 0.2714428105033361
  batch 27 loss: 0.2729089994121481
  batch 28 loss: 0.27483807876706123
  batch 29 loss: 0.2745865208321604
  batch 30 loss: 0.2752337301770846
  batch 31 loss: 0.27593854983006755
  batch 32 loss: 0.27741913916543126
  batch 33 loss: 0.2792220969091762
  batch 34 loss: 0.2797333760296597
  batch 35 loss: 0.28121409714221957
  batch 36 loss: 0.2811131812632084
  batch 37 loss: 0.28059358250450445
  batch 38 loss: 0.28090563926257583
  batch 39 loss: 0.2812407433222502
  batch 40 loss: 0.2810072500258684
  batch 41 loss: 0.28049008766325506
  batch 42 loss: 0.2803511821797916
  batch 43 loss: 0.2810609025317569
  batch 44 loss: 0.2815721912140196
  batch 45 loss: 0.28086421887079877
  batch 46 loss: 0.28023229928120325
  batch 47 loss: 0.28001104961050316
  batch 48 loss: 0.27958650700747967
  batch 49 loss: 0.2803985403508556
  batch 50 loss: 0.2814360684156418
  batch 51 loss: 0.28195826972232146
  batch 52 loss: 0.28204849935494936
  batch 53 loss: 0.2815216386093284
  batch 54 loss: 0.28137135947192154
  batch 55 loss: 0.28133576512336733
  batch 56 loss: 0.2816619080092226
  batch 57 loss: 0.2807074406167917
  batch 58 loss: 0.28038364109294167
  batch 59 loss: 0.280589078189963
  batch 60 loss: 0.28141621028383573
  batch 61 loss: 0.282624501918183
  batch 62 loss: 0.28291179456057086
  batch 63 loss: 0.2822840287098809
  batch 64 loss: 0.2818906733300537
  batch 65 loss: 0.28127838602432836
  batch 66 loss: 0.28134448239297577
  batch 67 loss: 0.2813259469929026
  batch 68 loss: 0.28127379803096547
  batch 69 loss: 0.2812399963537852
  batch 70 loss: 0.28095451295375823
  batch 71 loss: 0.28078693739125427
  batch 72 loss: 0.2808077509204547
  batch 73 loss: 0.2806851892438653
  batch 74 loss: 0.2803695632799252
  batch 75 loss: 0.28042018254597983
  batch 76 loss: 0.28097421793561234
  batch 77 loss: 0.28080051131062694
  batch 78 loss: 0.2812752505907646
  batch 79 loss: 0.28127197336546983
  batch 80 loss: 0.28102318458259107
  batch 81 loss: 0.28087822412267144
  batch 82 loss: 0.280969517260063
  batch 83 loss: 0.28058282169232884
  batch 84 loss: 0.27990330081610454
  batch 85 loss: 0.28012684197986826
  batch 86 loss: 0.2799680437459502
  batch 87 loss: 0.2796473040662963
  batch 88 loss: 0.27955262803218583
  batch 89 loss: 0.2792065381334069
  batch 90 loss: 0.2792619983355204
  batch 91 loss: 0.2787709468668634
  batch 92 loss: 0.278492142000924
  batch 93 loss: 0.2780446299622136
  batch 94 loss: 0.2780545529849986
  batch 95 loss: 0.2780384811915849
  batch 96 loss: 0.2781616581293444
  batch 97 loss: 0.2786396832502994
  batch 98 loss: 0.2784036451152393
  batch 99 loss: 0.2780216498808427
  batch 100 loss: 0.278177844285965
  batch 101 loss: 0.2781812076521392
  batch 102 loss: 0.2783508014445211
  batch 103 loss: 0.27819326028083136
  batch 104 loss: 0.2786492169476472
  batch 105 loss: 0.2783263369685128
  batch 106 loss: 0.2784149656599423
  batch 107 loss: 0.2783834150182867
  batch 108 loss: 0.2779949052190339
  batch 109 loss: 0.27815417699310757
  batch 110 loss: 0.278103189712221
  batch 111 loss: 0.278156399592623
  batch 112 loss: 0.2779186651376741
  batch 113 loss: 0.27766321414867334
  batch 114 loss: 0.2775181836464949
  batch 115 loss: 0.2771752506494522
  batch 116 loss: 0.2768517154557952
  batch 117 loss: 0.27661733902417696
  batch 118 loss: 0.2766492364265151
  batch 119 loss: 0.2767601028209975
  batch 120 loss: 0.2765751247604688
  batch 121 loss: 0.2765344617780575
  batch 122 loss: 0.2762612954514926
  batch 123 loss: 0.2758836038713533
  batch 124 loss: 0.2758964932253284
  batch 125 loss: 0.2754368689060211
  batch 126 loss: 0.275551119021007
  batch 127 loss: 0.2758682025229837
  batch 128 loss: 0.2760224405210465
  batch 129 loss: 0.2761805117592331
  batch 130 loss: 0.27597110271453856
  batch 131 loss: 0.27605436230433805
  batch 132 loss: 0.27623183528582257
  batch 133 loss: 0.27603676601460103
  batch 134 loss: 0.2757966585123717
  batch 135 loss: 0.2757095447293034
  batch 136 loss: 0.2761218363747877
  batch 137 loss: 0.2766270700597415
  batch 138 loss: 0.2767009028921957
  batch 139 loss: 0.2767646443500793
  batch 140 loss: 0.2770497933030128
  batch 141 loss: 0.27680143777360305
  batch 142 loss: 0.2765816632710712
  batch 143 loss: 0.27674796781339844
  batch 144 loss: 0.2766232724404997
  batch 145 loss: 0.27667177997786424
  batch 146 loss: 0.27685122130668327
  batch 147 loss: 0.2766898471076472
  batch 148 loss: 0.27654880849090785
  batch 149 loss: 0.27602113273319784
  batch 150 loss: 0.27616804897785185
  batch 151 loss: 0.27584064737061004
  batch 152 loss: 0.2756282320540202
  batch 153 loss: 0.2759554259527742
  batch 154 loss: 0.2757867418906905
  batch 155 loss: 0.275696589581428
  batch 156 loss: 0.2755003912517658
  batch 157 loss: 0.2753862208052046
  batch 158 loss: 0.2755200865713856
  batch 159 loss: 0.2757853583532309
  batch 160 loss: 0.2756903017871082
  batch 161 loss: 0.2753913180428262
  batch 162 loss: 0.2754232993832341
  batch 163 loss: 0.2751844311418709
  batch 164 loss: 0.27538245825505836
  batch 165 loss: 0.2756261323437546
  batch 166 loss: 0.2757968295769519
  batch 167 loss: 0.27572912156224966
  batch 168 loss: 0.27543999307921957
  batch 169 loss: 0.2755446687957944
  batch 170 loss: 0.2754987730699427
  batch 171 loss: 0.2756883629232819
  batch 172 loss: 0.27563606947660446
  batch 173 loss: 0.2755801479595934
  batch 174 loss: 0.2757170517211673
  batch 175 loss: 0.2758545914718083
  batch 176 loss: 0.27603118223222817
  batch 177 loss: 0.276182899872462
  batch 178 loss: 0.27611068945922207
  batch 179 loss: 0.27606669817556884
  batch 180 loss: 0.2760524201724264
  batch 181 loss: 0.27596407272539086
  batch 182 loss: 0.2759133407047817
  batch 183 loss: 0.2758007003961365
  batch 184 loss: 0.2757556069156398
  batch 185 loss: 0.2755805535090936
  batch 186 loss: 0.2755638291278193
  batch 187 loss: 0.275745514959575
  batch 188 loss: 0.27557512658073546
  batch 189 loss: 0.27560426476140504
  batch 190 loss: 0.27548045045451114
  batch 191 loss: 0.2755183513563965
  batch 192 loss: 0.2753577070931594
  batch 193 loss: 0.2752544303940985
  batch 194 loss: 0.2750106138974121
  batch 195 loss: 0.2751551687717438
  batch 196 loss: 0.27505962505024306
  batch 197 loss: 0.2751877819221032
  batch 198 loss: 0.27511890021839525
  batch 199 loss: 0.27515139172424624
  batch 200 loss: 0.2751143392920494
  batch 201 loss: 0.27503852568455595
  batch 202 loss: 0.2750415793149778
  batch 203 loss: 0.2751339227695183
  batch 204 loss: 0.275308761964826
  batch 205 loss: 0.2751901787955586
  batch 206 loss: 0.2752358460599936
  batch 207 loss: 0.275178919667783
  batch 208 loss: 0.2749610048933671
  batch 209 loss: 0.2749591404171081
  batch 210 loss: 0.27487956100986116
  batch 211 loss: 0.27482230601152535
  batch 212 loss: 0.27488052029654664
  batch 213 loss: 0.2746937703078901
  batch 214 loss: 0.27456931254574074
  batch 215 loss: 0.2746701101924098
  batch 216 loss: 0.2748529710979373
  batch 217 loss: 0.274710918145795
  batch 218 loss: 0.2746756283925214
  batch 219 loss: 0.27454442840460774
  batch 220 loss: 0.2748696635392579
  batch 221 loss: 0.27477141776505637
  batch 222 loss: 0.2748882317462483
  batch 223 loss: 0.274924235694077
  batch 224 loss: 0.2750842291196542
  batch 225 loss: 0.2751962873008516
  batch 226 loss: 0.2751496702564501
  batch 227 loss: 0.2751372806432489
  batch 228 loss: 0.27513995472537844
  batch 229 loss: 0.27508226082574855
  batch 230 loss: 0.2750475724754126
  batch 231 loss: 0.27501891875937906
  batch 232 loss: 0.2749577807583686
  batch 233 loss: 0.27485825927216606
  batch 234 loss: 0.274796614725875
  batch 235 loss: 0.2746826696269056
  batch 236 loss: 0.27457832500843676
  batch 237 loss: 0.2743992713056033
  batch 238 loss: 0.2742611268738739
  batch 239 loss: 0.27419382869947906
  batch 240 loss: 0.274216328933835
  batch 241 loss: 0.274074128925553
  batch 242 loss: 0.2739715890204611
  batch 243 loss: 0.27395938073166115
  batch 244 loss: 0.2738476058498758
  batch 245 loss: 0.2738208453265988
  batch 246 loss: 0.2737013110663833
  batch 247 loss: 0.27366317084685027
  batch 248 loss: 0.2737353552853869
  batch 249 loss: 0.2737630448307857
  batch 250 loss: 0.273730466067791
  batch 251 loss: 0.2735262442513766
  batch 252 loss: 0.2736287797609019
  batch 253 loss: 0.2734806925765139
  batch 254 loss: 0.27341852172857195
  batch 255 loss: 0.27326073699137743
  batch 256 loss: 0.2731927671120502
  batch 257 loss: 0.27324134140395007
  batch 258 loss: 0.27312064552029897
  batch 259 loss: 0.2729022785963699
  batch 260 loss: 0.2729688957333565
  batch 261 loss: 0.2729479071052595
  batch 262 loss: 0.2728007983501631
  batch 263 loss: 0.27266524941748993
  batch 264 loss: 0.2727102409258033
  batch 265 loss: 0.2726030323302971
  batch 266 loss: 0.2723786235415846
  batch 267 loss: 0.2721471433224303
  batch 268 loss: 0.27202009682112666
  batch 269 loss: 0.2720010438483887
  batch 270 loss: 0.2719183282167823
  batch 271 loss: 0.2718704782178921
  batch 272 loss: 0.2717898104558973
  batch 273 loss: 0.27161113415663934
  batch 274 loss: 0.2714136085697334
  batch 275 loss: 0.2714291702075438
  batch 276 loss: 0.27138553543583205
  batch 277 loss: 0.2713160714410272
  batch 278 loss: 0.27117137139220887
  batch 279 loss: 0.2711424809416562
  batch 280 loss: 0.2710612453520298
  batch 281 loss: 0.2709303425618338
  batch 282 loss: 0.2708131213348808
  batch 283 loss: 0.2707497320621679
  batch 284 loss: 0.2707355406502603
  batch 285 loss: 0.27059277249009983
  batch 286 loss: 0.27057369256561453
  batch 287 loss: 0.27042030171857895
  batch 288 loss: 0.2704057976500028
  batch 289 loss: 0.2702221865781863
  batch 290 loss: 0.27008872736116935
  batch 291 loss: 0.269996803906775
  batch 292 loss: 0.26984181397990004
  batch 293 loss: 0.26984600900789985
  batch 294 loss: 0.26959419220077746
  batch 295 loss: 0.269552860118575
  batch 296 loss: 0.26967629387572006
  batch 297 loss: 0.2696501014609931
  batch 298 loss: 0.26965981321846877
  batch 299 loss: 0.2697025330170341
  batch 300 loss: 0.2695537501573563
  batch 301 loss: 0.2696152130432699
  batch 302 loss: 0.269770253869082
  batch 303 loss: 0.26971025750188543
  batch 304 loss: 0.2696721810651453
  batch 305 loss: 0.26963543188376504
  batch 306 loss: 0.2697574735856524
  batch 307 loss: 0.26968685506026985
  batch 308 loss: 0.2697871191451302
  batch 309 loss: 0.26984936707806817
  batch 310 loss: 0.2698867049428724
  batch 311 loss: 0.26976388137056895
  batch 312 loss: 0.2697605951092182
  batch 313 loss: 0.2697077382105989
  batch 314 loss: 0.2696303662600791
  batch 315 loss: 0.26953817043039535
  batch 316 loss: 0.2694569580445561
  batch 317 loss: 0.2696244639083038
  batch 318 loss: 0.26944413992031563
  batch 319 loss: 0.2692561571874589
  batch 320 loss: 0.26952969739213584
  batch 321 loss: 0.2695627849421397
  batch 322 loss: 0.2694117978955648
  batch 323 loss: 0.26961765869667653
  batch 324 loss: 0.2697563927942588
  batch 325 loss: 0.26984288330261524
  batch 326 loss: 0.26997790183940545
  batch 327 loss: 0.26994305088068005
  batch 328 loss: 0.26994611527316453
  batch 329 loss: 0.27002623174509377
  batch 330 loss: 0.2700242428617044
  batch 331 loss: 0.270072266459465
  batch 332 loss: 0.27022375269647103
  batch 333 loss: 0.27023348717897144
  batch 334 loss: 0.27048171962985024
  batch 335 loss: 0.2705627358226634
  batch 336 loss: 0.27064303472815526
  batch 337 loss: 0.2707193375783434
  batch 338 loss: 0.27067083583251966
  batch 339 loss: 0.2706813726541215
  batch 340 loss: 0.2705641249085174
  batch 341 loss: 0.2705450614264284
  batch 342 loss: 0.2705626921259869
  batch 343 loss: 0.2706515352423615
  batch 344 loss: 0.2706892234381548
  batch 345 loss: 0.2708080591067024
  batch 346 loss: 0.2708390172937013
  batch 347 loss: 0.2707222191246511
  batch 348 loss: 0.2707504987631036
  batch 349 loss: 0.2708297987978233
  batch 350 loss: 0.27093088213886535
  batch 351 loss: 0.2708696551047839
  batch 352 loss: 0.2707634926773608
  batch 353 loss: 0.2707199542532562
  batch 354 loss: 0.27068297745986175
  batch 355 loss: 0.2706226303963594
  batch 356 loss: 0.27065247937702064
  batch 357 loss: 0.270434310414544
  batch 358 loss: 0.270288833567883
  batch 359 loss: 0.2702873882560013
  batch 360 loss: 0.2702075053834253
  batch 361 loss: 0.2701574707212871
  batch 362 loss: 0.2701128254032267
  batch 363 loss: 0.2701408287956695
  batch 364 loss: 0.27004082050624784
  batch 365 loss: 0.26997528059841835
  batch 366 loss: 0.2700026004366536
  batch 367 loss: 0.2699698264170083
  batch 368 loss: 0.269943590802343
  batch 369 loss: 0.26983636072496087
  batch 370 loss: 0.2698303945563935
  batch 371 loss: 0.2698336088994764
  batch 372 loss: 0.2699018863820902
  batch 373 loss: 0.2699260182339129
  batch 374 loss: 0.269859318985021
  batch 375 loss: 0.26991821098327634
  batch 376 loss: 0.27008945645487054
  batch 377 loss: 0.27009170164480134
  batch 378 loss: 0.2700568044942523
  batch 379 loss: 0.2700653162669381
  batch 380 loss: 0.27006335619248845
  batch 381 loss: 0.2700271037776326
  batch 382 loss: 0.270049818211201
  batch 383 loss: 0.2699282700025693
  batch 384 loss: 0.26987962222968537
  batch 385 loss: 0.2698122088011209
  batch 386 loss: 0.2697266231778372
  batch 387 loss: 0.26961754424165385
  batch 388 loss: 0.2695829373275496
  batch 389 loss: 0.26963733841490317
  batch 390 loss: 0.269697879254818
  batch 391 loss: 0.26963298236165206
  batch 392 loss: 0.2696362338123881
  batch 393 loss: 0.26973987683540085
  batch 394 loss: 0.26973074564927724
  batch 395 loss: 0.2697568380757223
  batch 396 loss: 0.269844446406521
  batch 397 loss: 0.2699266415774372
  batch 398 loss: 0.27000787573393864
  batch 399 loss: 0.27003150338068943
  batch 400 loss: 0.2701869276538491
  batch 401 loss: 0.2701690169864165
  batch 402 loss: 0.2701686784180243
  batch 403 loss: 0.2701340060745812
  batch 404 loss: 0.2701707660487973
  batch 405 loss: 0.2701311605580059
  batch 406 loss: 0.27003534501556103
  batch 407 loss: 0.27007010912280116
  batch 408 loss: 0.27004392777441766
  batch 409 loss: 0.27005295950946717
  batch 410 loss: 0.26989595722861404
  batch 411 loss: 0.26984647481980983
  batch 412 loss: 0.2701184963717044
  batch 413 loss: 0.27006651320411273
  batch 414 loss: 0.27010123892394816
  batch 415 loss: 0.2700674716966698
  batch 416 loss: 0.2700941934465216
  batch 417 loss: 0.27006212891720466
  batch 418 loss: 0.27006956835112506
  batch 419 loss: 0.2700956280305448
  batch 420 loss: 0.27014140720878327
  batch 421 loss: 0.270288571821539
  batch 422 loss: 0.2702425805172084
  batch 423 loss: 0.27019333761916375
  batch 424 loss: 0.27021935785997586
  batch 425 loss: 0.2701209869104273
  batch 426 loss: 0.2700765826892405
  batch 427 loss: 0.2701213008067647
  batch 428 loss: 0.2701155949279527
  batch 429 loss: 0.27003291933547646
  batch 430 loss: 0.2700858697988266
  batch 431 loss: 0.27014551344576127
  batch 432 loss: 0.2701342665693826
  batch 433 loss: 0.2701082233399772
  batch 434 loss: 0.2700240191089393
  batch 435 loss: 0.27007834767473154
  batch 436 loss: 0.2700220537841867
  batch 437 loss: 0.2699228400019268
  batch 438 loss: 0.26981064405071137
  batch 439 loss: 0.26970903996318785
  batch 440 loss: 0.26980508711527695
  batch 441 loss: 0.26983739145083224
  batch 442 loss: 0.26984294301649026
  batch 443 loss: 0.26989549152872633
  batch 444 loss: 0.26990000967372646
  batch 445 loss: 0.26985688916083134
  batch 446 loss: 0.26991179495248024
  batch 447 loss: 0.26991185029214393
  batch 448 loss: 0.2699735780313079
  batch 449 loss: 0.26995806243345305
  batch 450 loss: 0.2699168732100063
  batch 451 loss: 0.26983952413244944
  batch 452 loss: 0.26979356882187117
  batch 453 loss: 0.26979528696463334
  batch 454 loss: 0.2697100721726334
  batch 455 loss: 0.2696733637497975
  batch 456 loss: 0.2695931754352754
  batch 457 loss: 0.2695863712538254
  batch 458 loss: 0.269579963067213
  batch 459 loss: 0.2695181991150177
  batch 460 loss: 0.269536178915397
  batch 461 loss: 0.2695379712317874
  batch 462 loss: 0.2695280079330717
  batch 463 loss: 0.2695609935545252
  batch 464 loss: 0.2695261215598419
  batch 465 loss: 0.2695190184859819
  batch 466 loss: 0.26943729459727783
  batch 467 loss: 0.2695443830602429
  batch 468 loss: 0.26951286451429385
  batch 469 loss: 0.2694014198363209
  batch 470 loss: 0.2694861757628461
  batch 471 loss: 0.26956764350777723
  batch 472 loss: 0.2693962182028819
LOSS train 0.2693962182028819 valid 0.24218913912773132
LOSS train 0.2693962182028819 valid 0.23802858591079712
LOSS train 0.2693962182028819 valid 0.2443953057130178
LOSS train 0.2693962182028819 valid 0.22842326015233994
LOSS train 0.2693962182028819 valid 0.23125987350940705
LOSS train 0.2693962182028819 valid 0.23671699315309525
LOSS train 0.2693962182028819 valid 0.23288724252155849
LOSS train 0.2693962182028819 valid 0.23120369017124176
LOSS train 0.2693962182028819 valid 0.22947107089890373
LOSS train 0.2693962182028819 valid 0.2285004660487175
LOSS train 0.2693962182028819 valid 0.2293843857266686
LOSS train 0.2693962182028819 valid 0.2328072302043438
LOSS train 0.2693962182028819 valid 0.2334503050033863
LOSS train 0.2693962182028819 valid 0.23101170254605158
LOSS train 0.2693962182028819 valid 0.23055903216203052
LOSS train 0.2693962182028819 valid 0.23401607293635607
LOSS train 0.2693962182028819 valid 0.234731285887606
LOSS train 0.2693962182028819 valid 0.23481227540307575
LOSS train 0.2693962182028819 valid 0.23683679966550125
LOSS train 0.2693962182028819 valid 0.23642860427498819
LOSS train 0.2693962182028819 valid 0.2381697360958372
LOSS train 0.2693962182028819 valid 0.23769226534800095
LOSS train 0.2693962182028819 valid 0.2356899944336518
LOSS train 0.2693962182028819 valid 0.2363042626529932
LOSS train 0.2693962182028819 valid 0.23617298543453216
LOSS train 0.2693962182028819 valid 0.23544257420759934
LOSS train 0.2693962182028819 valid 0.235621252545604
LOSS train 0.2693962182028819 valid 0.2357088252902031
LOSS train 0.2693962182028819 valid 0.23406501165751753
LOSS train 0.2693962182028819 valid 0.23379876613616943
LOSS train 0.2693962182028819 valid 0.23431503965008643
LOSS train 0.2693962182028819 valid 0.23488566000014544
LOSS train 0.2693962182028819 valid 0.234300923166853
LOSS train 0.2693962182028819 valid 0.2334435012410669
LOSS train 0.2693962182028819 valid 0.2338086805173329
LOSS train 0.2693962182028819 valid 0.23442760482430458
LOSS train 0.2693962182028819 valid 0.23501798953558947
LOSS train 0.2693962182028819 valid 0.23492531360764252
LOSS train 0.2693962182028819 valid 0.23588270445664725
LOSS train 0.2693962182028819 valid 0.23592616207897663
LOSS train 0.2693962182028819 valid 0.23562064621506668
LOSS train 0.2693962182028819 valid 0.23716153701146445
LOSS train 0.2693962182028819 valid 0.23766816008922664
LOSS train 0.2693962182028819 valid 0.2369214641776952
LOSS train 0.2693962182028819 valid 0.23643665512402853
LOSS train 0.2693962182028819 valid 0.23610526290924652
LOSS train 0.2693962182028819 valid 0.2358932485605808
LOSS train 0.2693962182028819 valid 0.23762512300163507
LOSS train 0.2693962182028819 valid 0.2368894982702878
LOSS train 0.2693962182028819 valid 0.23752841502428054
LOSS train 0.2693962182028819 valid 0.23726977320278392
LOSS train 0.2693962182028819 valid 0.23673773270386916
LOSS train 0.2693962182028819 valid 0.23817823068150934
LOSS train 0.2693962182028819 valid 0.23808854128475543
LOSS train 0.2693962182028819 valid 0.23807750642299652
LOSS train 0.2693962182028819 valid 0.23819096945226192
LOSS train 0.2693962182028819 valid 0.23751912488226304
LOSS train 0.2693962182028819 valid 0.23824506612687274
LOSS train 0.2693962182028819 valid 0.23795307137198368
LOSS train 0.2693962182028819 valid 0.23757898037632305
LOSS train 0.2693962182028819 valid 0.2375853318171423
LOSS train 0.2693962182028819 valid 0.23724885020525224
LOSS train 0.2693962182028819 valid 0.23687855166102212
LOSS train 0.2693962182028819 valid 0.2371617159806192
LOSS train 0.2693962182028819 valid 0.2360039947124628
LOSS train 0.2693962182028819 valid 0.23587079413912512
LOSS train 0.2693962182028819 valid 0.23671375481940027
LOSS train 0.2693962182028819 valid 0.2360088095507201
LOSS train 0.2693962182028819 valid 0.2368232597043549
LOSS train 0.2693962182028819 valid 0.23722301317112787
LOSS train 0.2693962182028819 valid 0.2373770380943594
LOSS train 0.2693962182028819 valid 0.23790063191619185
LOSS train 0.2693962182028819 valid 0.23849459849808313
LOSS train 0.2693962182028819 valid 0.23789145515577212
LOSS train 0.2693962182028819 valid 0.23739786008993785
LOSS train 0.2693962182028819 valid 0.23740927443692558
LOSS train 0.2693962182028819 valid 0.23736286472964596
LOSS train 0.2693962182028819 valid 0.2370056993304155
LOSS train 0.2693962182028819 valid 0.23676435573946072
LOSS train 0.2693962182028819 valid 0.23636543992906808
LOSS train 0.2693962182028819 valid 0.23657258480419346
LOSS train 0.2693962182028819 valid 0.23650828748941422
LOSS train 0.2693962182028819 valid 0.23662794480122715
LOSS train 0.2693962182028819 valid 0.236400695251567
LOSS train 0.2693962182028819 valid 0.23717280082842884
LOSS train 0.2693962182028819 valid 0.2371380893990051
LOSS train 0.2693962182028819 valid 0.2367189442631842
LOSS train 0.2693962182028819 valid 0.23704550093547863
LOSS train 0.2693962182028819 valid 0.2373770721507876
LOSS train 0.2693962182028819 valid 0.23769167678223715
LOSS train 0.2693962182028819 valid 0.23775303183676122
LOSS train 0.2693962182028819 valid 0.2377418611684571
LOSS train 0.2693962182028819 valid 0.23767920479338656
LOSS train 0.2693962182028819 valid 0.2377913034659751
LOSS train 0.2693962182028819 valid 0.23811532368785457
LOSS train 0.2693962182028819 valid 0.23838333211218318
LOSS train 0.2693962182028819 valid 0.23844164072238294
LOSS train 0.2693962182028819 valid 0.2389493367197562
LOSS train 0.2693962182028819 valid 0.23902229904526412
LOSS train 0.2693962182028819 valid 0.23920477464795112
LOSS train 0.2693962182028819 valid 0.2392608238623874
LOSS train 0.2693962182028819 valid 0.2396842049909573
LOSS train 0.2693962182028819 valid 0.23948902846540063
LOSS train 0.2693962182028819 valid 0.23933286119539005
LOSS train 0.2693962182028819 valid 0.23961202942189716
LOSS train 0.2693962182028819 valid 0.23951490223407745
LOSS train 0.2693962182028819 valid 0.2391551865316997
LOSS train 0.2693962182028819 valid 0.2393324451038131
LOSS train 0.2693962182028819 valid 0.23890514138641708
LOSS train 0.2693962182028819 valid 0.23893187615004452
LOSS train 0.2693962182028819 valid 0.23923916859669728
LOSS train 0.2693962182028819 valid 0.23965351097285748
LOSS train 0.2693962182028819 valid 0.23943151942396587
LOSS train 0.2693962182028819 valid 0.23952578557165047
LOSS train 0.2693962182028819 valid 0.24022523009258767
LOSS train 0.2693962182028819 valid 0.2398611614159469
LOSS train 0.2693962182028819 valid 0.24048988801291865
LOSS train 0.2693962182028819 valid 0.240350590040118
LOSS train 0.2693962182028819 valid 0.23994729999734574
LOSS train 0.2693962182028819 valid 0.23958223251005015
LOSS train 0.2693962182028819 valid 0.2395122536450378
LOSS train 0.2693962182028819 valid 0.2397768797933078
LOSS train 0.2693962182028819 valid 0.23987455775098102
LOSS train 0.2693962182028819 valid 0.24010056473555103
LOSS train 0.2693962182028819 valid 0.24004535710811614
LOSS train 0.2693962182028819 valid 0.24025308340787888
LOSS train 0.2693962182028819 valid 0.24013704378304518
LOSS train 0.2693962182028819 valid 0.24012576579116285
LOSS train 0.2693962182028819 valid 0.23975244395492612
LOSS train 0.2693962182028819 valid 0.23947418859371772
LOSS train 0.2693962182028819 valid 0.23945398753835956
LOSS train 0.2693962182028819 valid 0.2394829374371153
LOSS train 0.2693962182028819 valid 0.23953067292844443
LOSS train 0.2693962182028819 valid 0.2398187958482486
LOSS train 0.2693962182028819 valid 0.23989727762010363
LOSS train 0.2693962182028819 valid 0.2399260101511198
LOSS train 0.2693962182028819 valid 0.24000925369506335
LOSS train 0.2693962182028819 valid 0.23989519358113193
LOSS train 0.2693962182028819 valid 0.23967634012802042
LOSS train 0.2693962182028819 valid 0.23962161487766676
LOSS train 0.2693962182028819 valid 0.2396828054536319
LOSS train 0.2693962182028819 valid 0.24000531749826082
LOSS train 0.2693962182028819 valid 0.24008422047941835
LOSS train 0.2693962182028819 valid 0.24010209201110733
LOSS train 0.2693962182028819 valid 0.24003960408013442
LOSS train 0.2693962182028819 valid 0.23996360497931912
LOSS train 0.2693962182028819 valid 0.23985053426554415
LOSS train 0.2693962182028819 valid 0.23975708194681117
LOSS train 0.2693962182028819 valid 0.23984822210849532
LOSS train 0.2693962182028819 valid 0.2397741108139356
LOSS train 0.2693962182028819 valid 0.23965500255688926
LOSS train 0.2693962182028819 valid 0.23956555500626564
LOSS train 0.2693962182028819 valid 0.2393788254144145
LOSS train 0.2693962182028819 valid 0.23948843051473817
LOSS train 0.2693962182028819 valid 0.23948854775198045
LOSS train 0.2693962182028819 valid 0.2394890107978613
LOSS train 0.2693962182028819 valid 0.2393262347408161
LOSS train 0.2693962182028819 valid 0.23943468579385854
LOSS train 0.2693962182028819 valid 0.23971949873855278
LOSS train 0.2693962182028819 valid 0.2399622338823974
LOSS train 0.2693962182028819 valid 0.23979888208534406
LOSS train 0.2693962182028819 valid 0.239878102309174
LOSS train 0.2693962182028819 valid 0.23963124574693434
LOSS train 0.2693962182028819 valid 0.23973019957178976
LOSS train 0.2693962182028819 valid 0.23985348425128244
LOSS train 0.2693962182028819 valid 0.23973908338202052
LOSS train 0.2693962182028819 valid 0.23974150570923697
LOSS train 0.2693962182028819 valid 0.23964325489387625
LOSS train 0.2693962182028819 valid 0.2393769769097221
LOSS train 0.2693962182028819 valid 0.23944948482162812
LOSS train 0.2693962182028819 valid 0.2392962632472055
LOSS train 0.2693962182028819 valid 0.23937099149753882
LOSS train 0.2693962182028819 valid 0.23929255959615542
LOSS train 0.2693962182028819 valid 0.23969829647705473
LOSS train 0.2693962182028819 valid 0.23965372000421797
LOSS train 0.2693962182028819 valid 0.23955536430532282
LOSS train 0.2693962182028819 valid 0.23960149700695513
LOSS train 0.2693962182028819 valid 0.23966841491755475
LOSS train 0.2693962182028819 valid 0.23994451869466452
LOSS train 0.2693962182028819 valid 0.23986269972390598
LOSS train 0.2693962182028819 valid 0.23975606213287753
LOSS train 0.2693962182028819 valid 0.23956143520363085
LOSS train 0.2693962182028819 valid 0.239675966849744
LOSS train 0.2693962182028819 valid 0.2396144352691329
LOSS train 0.2693962182028819 valid 0.23954527740542952
LOSS train 0.2693962182028819 valid 0.2395728069447702
LOSS train 0.2693962182028819 valid 0.23937968081012767
LOSS train 0.2693962182028819 valid 0.23941630521353255
LOSS train 0.2693962182028819 valid 0.23921110563808018
LOSS train 0.2693962182028819 valid 0.23919826840099537
LOSS train 0.2693962182028819 valid 0.23935993819336618
LOSS train 0.2693962182028819 valid 0.23944997306292257
LOSS train 0.2693962182028819 valid 0.23921468977483443
LOSS train 0.2693962182028819 valid 0.2390715845466889
LOSS train 0.2693962182028819 valid 0.23896607496799566
LOSS train 0.2693962182028819 valid 0.2391512165872418
LOSS train 0.2693962182028819 valid 0.23909471883689087
LOSS train 0.2693962182028819 valid 0.23933823429273837
LOSS train 0.2693962182028819 valid 0.23935233820323368
LOSS train 0.2693962182028819 valid 0.2395194099098444
LOSS train 0.2693962182028819 valid 0.23934358892156116
LOSS train 0.2693962182028819 valid 0.2393246135204145
LOSS train 0.2693962182028819 valid 0.23944169987598662
LOSS train 0.2693962182028819 valid 0.23936455989000843
LOSS train 0.2693962182028819 valid 0.23927368456270637
LOSS train 0.2693962182028819 valid 0.23922205144919237
LOSS train 0.2693962182028819 valid 0.23917829680845934
LOSS train 0.2693962182028819 valid 0.2390395270373959
LOSS train 0.2693962182028819 valid 0.23906412392712095
LOSS train 0.2693962182028819 valid 0.23924555026349567
LOSS train 0.2693962182028819 valid 0.23943729098374245
LOSS train 0.2693962182028819 valid 0.23920647167372253
LOSS train 0.2693962182028819 valid 0.23905899833905306
LOSS train 0.2693962182028819 valid 0.23892166284478714
LOSS train 0.2693962182028819 valid 0.2386580953764361
LOSS train 0.2693962182028819 valid 0.2384625910608857
LOSS train 0.2693962182028819 valid 0.23816104851285433
LOSS train 0.2693962182028819 valid 0.23813992807077705
LOSS train 0.2693962182028819 valid 0.2381242523862891
LOSS train 0.2693962182028819 valid 0.2381950530816208
LOSS train 0.2693962182028819 valid 0.23815962193508494
LOSS train 0.2693962182028819 valid 0.23825821944990674
LOSS train 0.2693962182028819 valid 0.23836729418269187
LOSS train 0.2693962182028819 valid 0.23826434097385832
LOSS train 0.2693962182028819 valid 0.2380706677834193
LOSS train 0.2693962182028819 valid 0.23788213624363452
LOSS train 0.2693962182028819 valid 0.23794879774165048
LOSS train 0.2693962182028819 valid 0.23791413898007913
LOSS train 0.2693962182028819 valid 0.23780645625299762
LOSS train 0.2693962182028819 valid 0.23788213904785074
LOSS train 0.2693962182028819 valid 0.2378280404068175
LOSS train 0.2693962182028819 valid 0.23798914292249188
LOSS train 0.2693962182028819 valid 0.2384158293576711
LOSS train 0.2693962182028819 valid 0.238390270079303
LOSS train 0.2693962182028819 valid 0.2383971798927226
LOSS train 0.2693962182028819 valid 0.2382529359626568
LOSS train 0.2693962182028819 valid 0.23833806083423678
LOSS train 0.2693962182028819 valid 0.23827150313794113
LOSS train 0.2693962182028819 valid 0.23827384992124645
LOSS train 0.2693962182028819 valid 0.23784909229725598
LOSS train 0.2693962182028819 valid 0.23769896536199878
LOSS train 0.2693962182028819 valid 0.23764051182949839
LOSS train 0.2693962182028819 valid 0.2375596106175042
LOSS train 0.2693962182028819 valid 0.23750651963665836
LOSS train 0.2693962182028819 valid 0.23770215785016818
LOSS train 0.2693962182028819 valid 0.23775052763824542
LOSS train 0.2693962182028819 valid 0.2377928105684427
LOSS train 0.2693962182028819 valid 0.2377918922612744
LOSS train 0.2693962182028819 valid 0.23763820391342821
LOSS train 0.2693962182028819 valid 0.2378084507584572
LOSS train 0.2693962182028819 valid 0.23786386461609388
LOSS train 0.2693962182028819 valid 0.23777009777369953
LOSS train 0.2693962182028819 valid 0.23758410447435416
LOSS train 0.2693962182028819 valid 0.23769252770763682
LOSS train 0.2693962182028819 valid 0.23755688603017844
LOSS train 0.2693962182028819 valid 0.2373356789466925
LOSS train 0.2693962182028819 valid 0.23734719517861821
LOSS train 0.2693962182028819 valid 0.23752794495617696
LOSS train 0.2693962182028819 valid 0.23747976452227265
LOSS train 0.2693962182028819 valid 0.23734372195142966
LOSS train 0.2693962182028819 valid 0.23734866356712647
LOSS train 0.2693962182028819 valid 0.23754951397653754
LOSS train 0.2693962182028819 valid 0.23740492772466784
LOSS train 0.2693962182028819 valid 0.23744308316346371
LOSS train 0.2693962182028819 valid 0.23740082124494158
LOSS train 0.2693962182028819 valid 0.23742427144731795
LOSS train 0.2693962182028819 valid 0.23732628938410613
LOSS train 0.2693962182028819 valid 0.23739079241432362
LOSS train 0.2693962182028819 valid 0.23752451009466746
LOSS train 0.2693962182028819 valid 0.23734636930403885
LOSS train 0.2693962182028819 valid 0.23733558684477507
LOSS train 0.2693962182028819 valid 0.23724094882388325
LOSS train 0.2693962182028819 valid 0.23710371925062312
LOSS train 0.2693962182028819 valid 0.23699349594594787
LOSS train 0.2693962182028819 valid 0.23707985677502372
LOSS train 0.2693962182028819 valid 0.2371242404308008
LOSS train 0.2693962182028819 valid 0.23713292880824327
LOSS train 0.2693962182028819 valid 0.23711062270960362
LOSS train 0.2693962182028819 valid 0.23710929962896532
LOSS train 0.2693962182028819 valid 0.23701787191842283
LOSS train 0.2693962182028819 valid 0.23685510127781972
LOSS train 0.2693962182028819 valid 0.23682900940906917
LOSS train 0.2693962182028819 valid 0.23685683473260158
LOSS train 0.2693962182028819 valid 0.2367978769708687
LOSS train 0.2693962182028819 valid 0.23675574142681924
LOSS train 0.2693962182028819 valid 0.23670768638799242
LOSS train 0.2693962182028819 valid 0.23674763401626295
LOSS train 0.2693962182028819 valid 0.2367299406064881
LOSS train 0.2693962182028819 valid 0.23667064252402956
LOSS train 0.2693962182028819 valid 0.2365408114832023
LOSS train 0.2693962182028819 valid 0.23650439672453707
LOSS train 0.2693962182028819 valid 0.23641553378268465
LOSS train 0.2693962182028819 valid 0.23637348881353698
LOSS train 0.2693962182028819 valid 0.23641797570752449
LOSS train 0.2693962182028819 valid 0.23645277887077654
LOSS train 0.2693962182028819 valid 0.2363809027482529
LOSS train 0.2693962182028819 valid 0.2362895039998321
LOSS train 0.2693962182028819 valid 0.23632552799762496
LOSS train 0.2693962182028819 valid 0.23629252497966474
LOSS train 0.2693962182028819 valid 0.2362076654533545
LOSS train 0.2693962182028819 valid 0.23607872782949593
LOSS train 0.2693962182028819 valid 0.23604619483284603
LOSS train 0.2693962182028819 valid 0.23598037630614668
LOSS train 0.2693962182028819 valid 0.23598670675174185
LOSS train 0.2693962182028819 valid 0.23592044857681774
LOSS train 0.2693962182028819 valid 0.23615678774765114
LOSS train 0.2693962182028819 valid 0.236184540759857
LOSS train 0.2693962182028819 valid 0.2362408305627185
LOSS train 0.2693962182028819 valid 0.23627556416787762
LOSS train 0.2693962182028819 valid 0.23631245546763943
LOSS train 0.2693962182028819 valid 0.2363277118401512
LOSS train 0.2693962182028819 valid 0.23630434260345423
LOSS train 0.2693962182028819 valid 0.23626985265233647
LOSS train 0.2693962182028819 valid 0.23616618326135502
LOSS train 0.2693962182028819 valid 0.23610809946817066
LOSS train 0.2693962182028819 valid 0.23603315769305713
LOSS train 0.2693962182028819 valid 0.23603946337188456
LOSS train 0.2693962182028819 valid 0.23602299036094979
LOSS train 0.2693962182028819 valid 0.23600858917056955
LOSS train 0.2693962182028819 valid 0.23587395837530495
LOSS train 0.2693962182028819 valid 0.23598245417588967
LOSS train 0.2693962182028819 valid 0.235980108241487
LOSS train 0.2693962182028819 valid 0.23587171576525034
LOSS train 0.2693962182028819 valid 0.23586839092550455
LOSS train 0.2693962182028819 valid 0.2357323769881175
LOSS train 0.2693962182028819 valid 0.23586298219082547
LOSS train 0.2693962182028819 valid 0.2357495001970081
LOSS train 0.2693962182028819 valid 0.23573977713723007
LOSS train 0.2693962182028819 valid 0.23571472395336013
LOSS train 0.2693962182028819 valid 0.23573542991370866
LOSS train 0.2693962182028819 valid 0.23566623212167503
LOSS train 0.2693962182028819 valid 0.2356681689530252
LOSS train 0.2693962182028819 valid 0.23575364035350066
LOSS train 0.2693962182028819 valid 0.23567312683709368
LOSS train 0.2693962182028819 valid 0.23551522399952163
LOSS train 0.2693962182028819 valid 0.2354634315041559
LOSS train 0.2693962182028819 valid 0.23560950969199398
LOSS train 0.2693962182028819 valid 0.2355922444684971
LOSS train 0.2693962182028819 valid 0.23555593289281063
LOSS train 0.2693962182028819 valid 0.23569684760535464
LOSS train 0.2693962182028819 valid 0.23565511567152142
LOSS train 0.2693962182028819 valid 0.23562718949645584
LOSS train 0.2693962182028819 valid 0.235486375781607
LOSS train 0.2693962182028819 valid 0.23556775487093037
LOSS train 0.2693962182028819 valid 0.23571021202681722
LOSS train 0.2693962182028819 valid 0.2356649014905009
LOSS train 0.2693962182028819 valid 0.23553832540938077
LOSS train 0.2693962182028819 valid 0.23561823239614224
LOSS train 0.2693962182028819 valid 0.23559704713288557
LOSS train 0.2693962182028819 valid 0.2356107233251844
LOSS train 0.2693962182028819 valid 0.23570654373562913
LOSS train 0.2693962182028819 valid 0.23573765399950472
LOSS train 0.2693962182028819 valid 0.23579718316074133
LOSS train 0.2693962182028819 valid 0.23584719351624364
LOSS train 0.2693962182028819 valid 0.2356955173569666
LOSS train 0.2693962182028819 valid 0.23573513517386457
LOSS train 0.2693962182028819 valid 0.2357294205941406
LOSS train 0.2693962182028819 valid 0.23565772013457795
LOSS train 0.2693962182028819 valid 0.23559691642818345
LOSS train 0.2693962182028819 valid 0.23572937924828793
LOSS train 0.2693962182028819 valid 0.2356492694972955
LOSS train 0.2693962182028819 valid 0.23573326401782957
LOSS train 0.2693962182028819 valid 0.2358112047444362
LOSS train 0.2693962182028819 valid 0.23574322527581518
LOSS train 0.2693962182028819 valid 0.2358535600035158
LOSS train 0.2693962182028819 valid 0.23580904712116785
LOSS train 0.2693962182028819 valid 0.2357647442119323
LOSS train 0.2693962182028819 valid 0.2356640709888028
LOSS train 0.2693962182028819 valid 0.2356363092334613
EPOCH 12:
  batch 1 loss: 0.2621966302394867
  batch 2 loss: 0.29555843770504
  batch 3 loss: 0.2787231703599294
  batch 4 loss: 0.27319999039173126
  batch 5 loss: 0.27460338473320006
  batch 6 loss: 0.27093836665153503
  batch 7 loss: 0.2690298301833017
  batch 8 loss: 0.2656997814774513
  batch 9 loss: 0.266387144724528
  batch 10 loss: 0.2629424214363098
  batch 11 loss: 0.263634129004045
  batch 12 loss: 0.2629512771964073
  batch 13 loss: 0.2609245249858269
  batch 14 loss: 0.2601108636174883
  batch 15 loss: 0.2614429473876953
  batch 16 loss: 0.26118483021855354
  batch 17 loss: 0.2634691560969633
  batch 18 loss: 0.26188770764403874
  batch 19 loss: 0.26192413505754975
  batch 20 loss: 0.26080609634518626
  batch 21 loss: 0.26482663339092616
  batch 22 loss: 0.26444444534453476
  batch 23 loss: 0.26287997999916907
  batch 24 loss: 0.2640953231602907
  batch 25 loss: 0.26517622530460355
  batch 26 loss: 0.2649070660655315
  batch 27 loss: 0.26534171954349234
  batch 28 loss: 0.2666702925094536
  batch 29 loss: 0.26687478967781725
  batch 30 loss: 0.2675753697752953
  batch 31 loss: 0.26810019391198314
  batch 32 loss: 0.2699744454585016
  batch 33 loss: 0.27155088701031427
  batch 34 loss: 0.2717713945928742
  batch 35 loss: 0.27328291152204787
  batch 36 loss: 0.2730410806834698
  batch 37 loss: 0.2728445533965085
  batch 38 loss: 0.27328159660100937
  batch 39 loss: 0.2735724659302296
  batch 40 loss: 0.27299900315701964
  batch 41 loss: 0.2724798448928973
  batch 42 loss: 0.2723821866370383
  batch 43 loss: 0.27296275766782985
  batch 44 loss: 0.2737759883430871
  batch 45 loss: 0.2731824060281118
  batch 46 loss: 0.2719799328757369
  batch 47 loss: 0.2713962650679527
  batch 48 loss: 0.27125446653614443
  batch 49 loss: 0.27183768244422213
  batch 50 loss: 0.27277377873659137
  batch 51 loss: 0.2727503177582049
  batch 52 loss: 0.2729948526964738
  batch 53 loss: 0.2726977071109808
  batch 54 loss: 0.27282117555538815
  batch 55 loss: 0.2727880437265743
  batch 56 loss: 0.27322800111557755
  batch 57 loss: 0.2725100153893755
  batch 58 loss: 0.2719213379354313
  batch 59 loss: 0.27163696213293886
  batch 60 loss: 0.27191607976953186
  batch 61 loss: 0.27322177882077264
  batch 62 loss: 0.27352784453861173
  batch 63 loss: 0.2728277345498403
  batch 64 loss: 0.27266340469941497
  batch 65 loss: 0.27201101917486925
  batch 66 loss: 0.2720517299392007
  batch 67 loss: 0.2719796669127336
  batch 68 loss: 0.27201076772283106
  batch 69 loss: 0.2720994102782097
  batch 70 loss: 0.27224876071725573
  batch 71 loss: 0.27229361634858895
  batch 72 loss: 0.27231700387265945
  batch 73 loss: 0.2723464402433944
  batch 74 loss: 0.27217990119714996
  batch 75 loss: 0.27227840065956116
  batch 76 loss: 0.2726358402716486
  batch 77 loss: 0.27247624428241285
  batch 78 loss: 0.27279220062952775
  batch 79 loss: 0.2727074679694598
  batch 80 loss: 0.2725769281387329
  batch 81 loss: 0.2722650174005532
  batch 82 loss: 0.27234753421167046
  batch 83 loss: 0.27202223995363856
  batch 84 loss: 0.271310040106376
  batch 85 loss: 0.2716462273808087
  batch 86 loss: 0.2715359428940817
  batch 87 loss: 0.2712747369004392
  batch 88 loss: 0.27112243832512334
  batch 89 loss: 0.2709277272224426
  batch 90 loss: 0.2711009853416019
  batch 91 loss: 0.270693138733015
  batch 92 loss: 0.2703335492507271
  batch 93 loss: 0.2698779865618675
  batch 94 loss: 0.26978258566653474
  batch 95 loss: 0.2696791165753415
  batch 96 loss: 0.2697342938433091
  batch 97 loss: 0.2701971260542722
  batch 98 loss: 0.2700311340847794
  batch 99 loss: 0.26964349683487054
  batch 100 loss: 0.269893931299448
  batch 101 loss: 0.26970555065291946
  batch 102 loss: 0.26985520606531815
  batch 103 loss: 0.26971207559108734
  batch 104 loss: 0.27012452497505224
  batch 105 loss: 0.2698303601571492
  batch 106 loss: 0.2698895755522656
  batch 107 loss: 0.2698701078368125
  batch 108 loss: 0.26942052719769655
  batch 109 loss: 0.2696113378629772
  batch 110 loss: 0.26952102861621163
  batch 111 loss: 0.26963521043459576
  batch 112 loss: 0.26954737172595095
  batch 113 loss: 0.2693529920240419
  batch 114 loss: 0.2691400777875331
  batch 115 loss: 0.26876610387926514
  batch 116 loss: 0.26847310312863054
  batch 117 loss: 0.26837807409783715
  batch 118 loss: 0.2683479871790288
  batch 119 loss: 0.2684686657260446
  batch 120 loss: 0.26837140892942746
  batch 121 loss: 0.2682481457379239
  batch 122 loss: 0.26796564997219646
  batch 123 loss: 0.2676454885461466
  batch 124 loss: 0.2678317801365929
  batch 125 loss: 0.267535159945488
  batch 126 loss: 0.26755846197169925
  batch 127 loss: 0.2678570037517022
  batch 128 loss: 0.2680373286129907
  batch 129 loss: 0.26830370701098627
  batch 130 loss: 0.2681457165342111
  batch 131 loss: 0.2681343511088204
  batch 132 loss: 0.2684320176415371
  batch 133 loss: 0.2683732994741067
  batch 134 loss: 0.2682073779070555
  batch 135 loss: 0.2681607562082785
  batch 136 loss: 0.2685087130350225
  batch 137 loss: 0.2691234800502332
  batch 138 loss: 0.26908292481000873
  batch 139 loss: 0.26907597921735094
  batch 140 loss: 0.2693565881678036
  batch 141 loss: 0.2691544066084192
  batch 142 loss: 0.2689300553059914
  batch 143 loss: 0.2690231502056122
  batch 144 loss: 0.2689185852391852
  batch 145 loss: 0.2689250150631214
  batch 146 loss: 0.26916297678261586
  batch 147 loss: 0.26902213956223053
  batch 148 loss: 0.2689840300260363
  batch 149 loss: 0.26855127783429705
  batch 150 loss: 0.26859893679618835
  batch 151 loss: 0.26822880167045343
  batch 152 loss: 0.26795610893321664
  batch 153 loss: 0.26823018939276927
  batch 154 loss: 0.26805191912821363
  batch 155 loss: 0.2679852396249771
  batch 156 loss: 0.2678893020328803
  batch 157 loss: 0.26777640488117366
  batch 158 loss: 0.2678677978206284
  batch 159 loss: 0.2681795316484739
  batch 160 loss: 0.26821350445970893
  batch 161 loss: 0.2679175398549678
  batch 162 loss: 0.2679351716313833
  batch 163 loss: 0.2677850976494924
  batch 164 loss: 0.2680325056539803
  batch 165 loss: 0.26827811661994816
  batch 166 loss: 0.2685075399566846
  batch 167 loss: 0.2684615721067269
  batch 168 loss: 0.2682072638223569
  batch 169 loss: 0.2683179968383891
  batch 170 loss: 0.2684039190411568
  batch 171 loss: 0.268488486172163
  batch 172 loss: 0.2684505004002604
  batch 173 loss: 0.2684779710680074
  batch 174 loss: 0.26863503447551834
  batch 175 loss: 0.26863091681684764
  batch 176 loss: 0.2687734605067156
  batch 177 loss: 0.26899451160498256
  batch 178 loss: 0.2690814504964968
  batch 179 loss: 0.2691045086310562
  batch 180 loss: 0.2690424865318669
  batch 181 loss: 0.26898462230658665
  batch 182 loss: 0.26905899871509154
  batch 183 loss: 0.26904435944361765
  batch 184 loss: 0.26899962313473225
  batch 185 loss: 0.2688485207589897
  batch 186 loss: 0.26889759406287184
  batch 187 loss: 0.268952220120532
  batch 188 loss: 0.26877166069251424
  batch 189 loss: 0.26884375922578985
  batch 190 loss: 0.26871983973603497
  batch 191 loss: 0.26873677095193516
  batch 192 loss: 0.2685583259444684
  batch 193 loss: 0.2685018576334178
  batch 194 loss: 0.268283692370985
  batch 195 loss: 0.26839308860974437
  batch 196 loss: 0.2682965506552433
  batch 197 loss: 0.2685162849383911
  batch 198 loss: 0.26844954197153903
  batch 199 loss: 0.26840331873402523
  batch 200 loss: 0.2683050912618637
  batch 201 loss: 0.2683267055162743
  batch 202 loss: 0.2684782132653907
  batch 203 loss: 0.26851793316197514
  batch 204 loss: 0.2686854272204287
  batch 205 loss: 0.26860352071320137
  batch 206 loss: 0.26869205595220175
  batch 207 loss: 0.2686386802346234
  batch 208 loss: 0.2684294410909598
  batch 209 loss: 0.2684983047857239
  batch 210 loss: 0.26846600941249305
  batch 211 loss: 0.2683054074574421
  batch 212 loss: 0.2682074110884711
  batch 213 loss: 0.26801965431148456
  batch 214 loss: 0.2678638662551051
  batch 215 loss: 0.2678243974613589
  batch 216 loss: 0.2679521461465844
  batch 217 loss: 0.26786613725297465
  batch 218 loss: 0.2677278718270293
  batch 219 loss: 0.2675406773612924
  batch 220 loss: 0.26788146536458624
  batch 221 loss: 0.2678206585920774
  batch 222 loss: 0.26792861306452537
  batch 223 loss: 0.26787966997634133
  batch 224 loss: 0.2680319252290896
  batch 225 loss: 0.26813818017641705
  batch 226 loss: 0.2681825402299915
  batch 227 loss: 0.2681928966276446
  batch 228 loss: 0.26826116298897223
  batch 229 loss: 0.26830888093819266
  batch 230 loss: 0.26831693156905795
  batch 231 loss: 0.26828899115195004
  batch 232 loss: 0.2682087437474522
  batch 233 loss: 0.2682913280224084
  batch 234 loss: 0.2682542354505286
  batch 235 loss: 0.2681798339524168
  batch 236 loss: 0.26816056081551615
  batch 237 loss: 0.2681189403881001
  batch 238 loss: 0.2680571137481377
  batch 239 loss: 0.26804880536001596
  batch 240 loss: 0.2681668372824788
  batch 241 loss: 0.26809988285248704
  batch 242 loss: 0.26811995578945175
  batch 243 loss: 0.2681364165044125
  batch 244 loss: 0.2679608570503407
  batch 245 loss: 0.2680336781910488
  batch 246 loss: 0.2679492109190158
  batch 247 loss: 0.2679531587521557
  batch 248 loss: 0.2680203280141277
  batch 249 loss: 0.2680756335995762
  batch 250 loss: 0.26815308034420016
  batch 251 loss: 0.26795062090296196
  batch 252 loss: 0.2680410882310262
  batch 253 loss: 0.2680176852248874
  batch 254 loss: 0.2680477372069997
  batch 255 loss: 0.26794010839041543
  batch 256 loss: 0.2678908543311991
  batch 257 loss: 0.26795714067345927
  batch 258 loss: 0.26792145358730657
  batch 259 loss: 0.267744431049207
  batch 260 loss: 0.26785130088145914
  batch 261 loss: 0.2679002438011754
  batch 262 loss: 0.26781763580009227
  batch 263 loss: 0.2677270453919023
  batch 264 loss: 0.2677248878912492
  batch 265 loss: 0.2676619069756202
  batch 266 loss: 0.2675277719036081
  batch 267 loss: 0.2673446618104249
  batch 268 loss: 0.2672139780957307
  batch 269 loss: 0.26725907662544107
  batch 270 loss: 0.267182870429975
  batch 271 loss: 0.26714588734716505
  batch 272 loss: 0.26705390472403345
  batch 273 loss: 0.2669093641606006
  batch 274 loss: 0.26680516103540897
  batch 275 loss: 0.26680679164149546
  batch 276 loss: 0.2667559189317019
  batch 277 loss: 0.2666884974882491
  batch 278 loss: 0.2665613248086662
  batch 279 loss: 0.2665817844931797
  batch 280 loss: 0.2664704909282071
  batch 281 loss: 0.26637971989836984
  batch 282 loss: 0.26627120116712355
  batch 283 loss: 0.2661961927852024
  batch 284 loss: 0.26620741115069724
  batch 285 loss: 0.2661010253847691
  batch 286 loss: 0.26610023919102194
  batch 287 loss: 0.2659305337725616
  batch 288 loss: 0.2659386123737527
  batch 289 loss: 0.26581306673044974
  batch 290 loss: 0.26573680664958627
  batch 291 loss: 0.2656518211684276
  batch 292 loss: 0.26553278069381847
  batch 293 loss: 0.265526248428195
  batch 294 loss: 0.2652841388773756
  batch 295 loss: 0.265273727501853
  batch 296 loss: 0.26536299570186717
  batch 297 loss: 0.26532337390613875
  batch 298 loss: 0.26531386545440494
  batch 299 loss: 0.26535411423265337
  batch 300 loss: 0.26517006720105807
  batch 301 loss: 0.26516820886404413
  batch 302 loss: 0.2653282321742828
  batch 303 loss: 0.26528331789836634
  batch 304 loss: 0.2652469172975735
  batch 305 loss: 0.2652059665957435
  batch 306 loss: 0.2652633625015714
  batch 307 loss: 0.26522784507818253
  batch 308 loss: 0.2652592370165633
  batch 309 loss: 0.26522944751873756
  batch 310 loss: 0.26528317894666426
  batch 311 loss: 0.2652515358384399
  batch 312 loss: 0.26518599813183147
  batch 313 loss: 0.2650986321913168
  batch 314 loss: 0.26503261520414595
  batch 315 loss: 0.2649909937665576
  batch 316 loss: 0.2649310168020333
  batch 317 loss: 0.26510250182933986
  batch 318 loss: 0.26499667857428016
  batch 319 loss: 0.2647730394216914
  batch 320 loss: 0.264969356264919
  batch 321 loss: 0.2650018782816201
  batch 322 loss: 0.2649466479602067
  batch 323 loss: 0.2650055742300701
  batch 324 loss: 0.26519577039612663
  batch 325 loss: 0.26535667162675125
  batch 326 loss: 0.2654943948874444
  batch 327 loss: 0.26544142913198615
  batch 328 loss: 0.26537747776544673
  batch 329 loss: 0.2654558596368256
  batch 330 loss: 0.2655021586652958
  batch 331 loss: 0.2654925050422144
  batch 332 loss: 0.2656472684837968
  batch 333 loss: 0.2656709569263029
  batch 334 loss: 0.26595671184940967
  batch 335 loss: 0.2660672972896206
  batch 336 loss: 0.266171042984795
  batch 337 loss: 0.26624907137555254
  batch 338 loss: 0.26626611324633365
  batch 339 loss: 0.26628721626283147
  batch 340 loss: 0.2662123078809065
  batch 341 loss: 0.2661895172512077
  batch 342 loss: 0.26616711975538243
  batch 343 loss: 0.26625311018426634
  batch 344 loss: 0.2663003387319487
  batch 345 loss: 0.26637313875599183
  batch 346 loss: 0.2663888681486163
  batch 347 loss: 0.2663012217805434
  batch 348 loss: 0.26635174080729485
  batch 349 loss: 0.2664572275897493
  batch 350 loss: 0.2665444513303893
  batch 351 loss: 0.2664782636270904
  batch 352 loss: 0.26636566153981467
  batch 353 loss: 0.26633738846684313
  batch 354 loss: 0.26629531450864286
  batch 355 loss: 0.26623698909517746
  batch 356 loss: 0.2662885746594225
  batch 357 loss: 0.2661434404489373
  batch 358 loss: 0.2660357148787163
  batch 359 loss: 0.2660370863413744
  batch 360 loss: 0.26601622278491655
  batch 361 loss: 0.2659891149508986
  batch 362 loss: 0.2659298058866796
  batch 363 loss: 0.2659940107138032
  batch 364 loss: 0.26588640169619204
  batch 365 loss: 0.26585742802652595
  batch 366 loss: 0.2658407281412453
  batch 367 loss: 0.265814557346726
  batch 368 loss: 0.26583105321649625
  batch 369 loss: 0.26574531787133154
  batch 370 loss: 0.26576760652902964
  batch 371 loss: 0.2657710088873809
  batch 372 loss: 0.26582838699061384
  batch 373 loss: 0.2658262836070227
  batch 374 loss: 0.2657739195753546
  batch 375 loss: 0.2658659524122874
  batch 376 loss: 0.26602255402410285
  batch 377 loss: 0.26601009643994844
  batch 378 loss: 0.26598690300391464
  batch 379 loss: 0.2660270772068356
  batch 380 loss: 0.2659695203759168
  batch 381 loss: 0.2659204662863038
  batch 382 loss: 0.26595046519452986
  batch 383 loss: 0.2658946383793111
  batch 384 loss: 0.26588951497493934
  batch 385 loss: 0.2658688380733713
  batch 386 loss: 0.2658271860196183
  batch 387 loss: 0.2657782436572304
  batch 388 loss: 0.2656176433197616
  batch 389 loss: 0.2656222730875015
  batch 390 loss: 0.26563848856932076
  batch 391 loss: 0.2655717313975629
  batch 392 loss: 0.26570357865064725
  batch 393 loss: 0.26574525218125217
  batch 394 loss: 0.2657094619373985
  batch 395 loss: 0.2657401640581179
  batch 396 loss: 0.2658872309733521
  batch 397 loss: 0.26592807591862283
  batch 398 loss: 0.26596762355997333
  batch 399 loss: 0.26600787039836843
  batch 400 loss: 0.26614743787795303
  batch 401 loss: 0.26619509312131456
  batch 402 loss: 0.2661839606082855
  batch 403 loss: 0.26614462053657467
  batch 404 loss: 0.2661431655656583
  batch 405 loss: 0.26611876248577493
  batch 406 loss: 0.2660368342191128
  batch 407 loss: 0.2660297455966326
  batch 408 loss: 0.2659492915240573
  batch 409 loss: 0.2660182218607014
  batch 410 loss: 0.2658598169320967
  batch 411 loss: 0.2658211299537742
  batch 412 loss: 0.2660908210046083
  batch 413 loss: 0.26605922452762687
  batch 414 loss: 0.2661014013532279
  batch 415 loss: 0.2660648534096867
  batch 416 loss: 0.2661099395929621
  batch 417 loss: 0.2660725750654436
  batch 418 loss: 0.26605737394693363
  batch 419 loss: 0.2660705586606392
  batch 420 loss: 0.26608504589114873
  batch 421 loss: 0.26618851123116644
  batch 422 loss: 0.2661517621216616
  batch 423 loss: 0.266104649039025
  batch 424 loss: 0.26613852160297474
  batch 425 loss: 0.26602091052952936
  batch 426 loss: 0.265984381308578
  batch 427 loss: 0.2660268547244597
  batch 428 loss: 0.2660520241778588
  batch 429 loss: 0.2659877276087141
  batch 430 loss: 0.2660154391859853
  batch 431 loss: 0.26601981369912486
  batch 432 loss: 0.2660378671630665
  batch 433 loss: 0.26600533365928
  batch 434 loss: 0.26592812611622746
  batch 435 loss: 0.26599105552010155
  batch 436 loss: 0.2659790996769699
  batch 437 loss: 0.2659171742964664
  batch 438 loss: 0.26583720692624785
  batch 439 loss: 0.2657559782877598
  batch 440 loss: 0.2658598305149512
  batch 441 loss: 0.2658885833627783
  batch 442 loss: 0.26585012370914357
  batch 443 loss: 0.2658614996593639
  batch 444 loss: 0.26590416694546604
  batch 445 loss: 0.2658547869559084
  batch 446 loss: 0.26588177266676866
  batch 447 loss: 0.26589103339769166
  batch 448 loss: 0.26597917206319316
  batch 449 loss: 0.26595225080615426
  batch 450 loss: 0.26590856624974146
  batch 451 loss: 0.2658144592115461
  batch 452 loss: 0.2657842070062076
  batch 453 loss: 0.2657804652620888
  batch 454 loss: 0.2657089770037172
  batch 455 loss: 0.2656919496727514
  batch 456 loss: 0.2656230116099642
  batch 457 loss: 0.2655956255398381
  batch 458 loss: 0.26558816062050616
  batch 459 loss: 0.2655403071888651
  batch 460 loss: 0.2654940026285856
  batch 461 loss: 0.26553691768465226
  batch 462 loss: 0.2655599374985282
  batch 463 loss: 0.26561020836073174
  batch 464 loss: 0.26559878163168144
  batch 465 loss: 0.265614307856047
  batch 466 loss: 0.2655266504494929
  batch 467 loss: 0.2656343496649883
  batch 468 loss: 0.26562350526706785
  batch 469 loss: 0.2655605670295036
  batch 470 loss: 0.2655862397652991
  batch 471 loss: 0.26565681923093815
  batch 472 loss: 0.26548934469031077
LOSS train 0.26548934469031077 valid 0.24381700158119202
LOSS train 0.26548934469031077 valid 0.23532283306121826
LOSS train 0.26548934469031077 valid 0.24497956037521362
LOSS train 0.26548934469031077 valid 0.2279021218419075
LOSS train 0.26548934469031077 valid 0.23144007921218873
LOSS train 0.26548934469031077 valid 0.23732304573059082
LOSS train 0.26548934469031077 valid 0.23215375414916448
LOSS train 0.26548934469031077 valid 0.2300297673791647
LOSS train 0.26548934469031077 valid 0.22885911663373312
LOSS train 0.26548934469031077 valid 0.22691603899002075
LOSS train 0.26548934469031077 valid 0.2274682277982885
LOSS train 0.26548934469031077 valid 0.23032534619172415
LOSS train 0.26548934469031077 valid 0.23065006389067724
LOSS train 0.26548934469031077 valid 0.2288289059485708
LOSS train 0.26548934469031077 valid 0.22853538095951081
LOSS train 0.26548934469031077 valid 0.23190716188400984
LOSS train 0.26548934469031077 valid 0.23336606954827027
LOSS train 0.26548934469031077 valid 0.2334708637661404
LOSS train 0.26548934469031077 valid 0.23567043480120206
LOSS train 0.26548934469031077 valid 0.234940554946661
LOSS train 0.26548934469031077 valid 0.2361992278269359
LOSS train 0.26548934469031077 valid 0.23570220104672693
LOSS train 0.26548934469031077 valid 0.23358045842336572
LOSS train 0.26548934469031077 valid 0.23403610102832317
LOSS train 0.26548934469031077 valid 0.23402663171291352
LOSS train 0.26548934469031077 valid 0.2335379679615681
LOSS train 0.26548934469031077 valid 0.2335288574298223
LOSS train 0.26548934469031077 valid 0.2333657874592713
LOSS train 0.26548934469031077 valid 0.23192860083333378
LOSS train 0.26548934469031077 valid 0.23148920039335888
LOSS train 0.26548934469031077 valid 0.23207400018169033
LOSS train 0.26548934469031077 valid 0.23224347084760666
LOSS train 0.26548934469031077 valid 0.23166740437348685
LOSS train 0.26548934469031077 valid 0.23091407251708648
LOSS train 0.26548934469031077 valid 0.23121252911431447
LOSS train 0.26548934469031077 valid 0.231905707054668
LOSS train 0.26548934469031077 valid 0.2323633516962464
LOSS train 0.26548934469031077 valid 0.2324825944869142
LOSS train 0.26548934469031077 valid 0.23338823708204123
LOSS train 0.26548934469031077 valid 0.2335393738001585
LOSS train 0.26548934469031077 valid 0.23320268712392667
LOSS train 0.26548934469031077 valid 0.23457231053284236
LOSS train 0.26548934469031077 valid 0.23504547532214676
LOSS train 0.26548934469031077 valid 0.23443965478376907
LOSS train 0.26548934469031077 valid 0.2337747828827964
LOSS train 0.26548934469031077 valid 0.23338713820861734
LOSS train 0.26548934469031077 valid 0.23313218925861603
LOSS train 0.26548934469031077 valid 0.23477419776221117
LOSS train 0.26548934469031077 valid 0.23407676268597039
LOSS train 0.26548934469031077 valid 0.23468565165996552
LOSS train 0.26548934469031077 valid 0.23447407168500564
LOSS train 0.26548934469031077 valid 0.2340309608441133
LOSS train 0.26548934469031077 valid 0.2353746531144628
LOSS train 0.26548934469031077 valid 0.23526259125382812
LOSS train 0.26548934469031077 valid 0.23513295596296138
LOSS train 0.26548934469031077 valid 0.2351183564003025
LOSS train 0.26548934469031077 valid 0.23455892477119178
LOSS train 0.26548934469031077 valid 0.23532684198741255
LOSS train 0.26548934469031077 valid 0.2349492533732269
LOSS train 0.26548934469031077 valid 0.23462771450479825
LOSS train 0.26548934469031077 valid 0.23464765827186773
LOSS train 0.26548934469031077 valid 0.23429126244398854
LOSS train 0.26548934469031077 valid 0.2340484826810776
LOSS train 0.26548934469031077 valid 0.23433113633655012
LOSS train 0.26548934469031077 valid 0.23328135151129503
LOSS train 0.26548934469031077 valid 0.2331582334908572
LOSS train 0.26548934469031077 valid 0.23390561386720457
LOSS train 0.26548934469031077 valid 0.2332451961496297
LOSS train 0.26548934469031077 valid 0.23390478975530984
LOSS train 0.26548934469031077 valid 0.23432487802846091
LOSS train 0.26548934469031077 valid 0.23453937344987627
LOSS train 0.26548934469031077 valid 0.23514734436240461
LOSS train 0.26548934469031077 valid 0.23575286485560953
LOSS train 0.26548934469031077 valid 0.23520080241802577
LOSS train 0.26548934469031077 valid 0.2347567198673884
LOSS train 0.26548934469031077 valid 0.2349213598982284
LOSS train 0.26548934469031077 valid 0.2348192868294654
LOSS train 0.26548934469031077 valid 0.23456294299700323
LOSS train 0.26548934469031077 valid 0.23441556322423718
LOSS train 0.26548934469031077 valid 0.23415420837700368
LOSS train 0.26548934469031077 valid 0.2343712717662623
LOSS train 0.26548934469031077 valid 0.23428629756700703
LOSS train 0.26548934469031077 valid 0.23430197048618132
LOSS train 0.26548934469031077 valid 0.23407587746069544
LOSS train 0.26548934469031077 valid 0.2348357346128015
LOSS train 0.26548934469031077 valid 0.23475963050542875
LOSS train 0.26548934469031077 valid 0.2344620275771481
LOSS train 0.26548934469031077 valid 0.2347380295395851
LOSS train 0.26548934469031077 valid 0.23508550142973997
LOSS train 0.26548934469031077 valid 0.23538939853509266
LOSS train 0.26548934469031077 valid 0.23545860998578125
LOSS train 0.26548934469031077 valid 0.23545194627798122
LOSS train 0.26548934469031077 valid 0.2352908032555734
LOSS train 0.26548934469031077 valid 0.2354546975582204
LOSS train 0.26548934469031077 valid 0.2357998311519623
LOSS train 0.26548934469031077 valid 0.2360310284420848
LOSS train 0.26548934469031077 valid 0.2360794315325845
LOSS train 0.26548934469031077 valid 0.23664395617587225
LOSS train 0.26548934469031077 valid 0.23680000187772693
LOSS train 0.26548934469031077 valid 0.23698590472340583
LOSS train 0.26548934469031077 valid 0.23699758578054975
LOSS train 0.26548934469031077 valid 0.2373636604524126
LOSS train 0.26548934469031077 valid 0.23709856812815064
LOSS train 0.26548934469031077 valid 0.23694687866820738
LOSS train 0.26548934469031077 valid 0.23719651230743954
LOSS train 0.26548934469031077 valid 0.23718886035230924
LOSS train 0.26548934469031077 valid 0.23686244888840435
LOSS train 0.26548934469031077 valid 0.23698799588062144
LOSS train 0.26548934469031077 valid 0.23650798570672307
LOSS train 0.26548934469031077 valid 0.2365176963535222
LOSS train 0.26548934469031077 valid 0.23680336853942355
LOSS train 0.26548934469031077 valid 0.23720381422234432
LOSS train 0.26548934469031077 valid 0.23693124659820997
LOSS train 0.26548934469031077 valid 0.23705244913958667
LOSS train 0.26548934469031077 valid 0.23777691708958668
LOSS train 0.26548934469031077 valid 0.2374009838135078
LOSS train 0.26548934469031077 valid 0.2380576641895832
LOSS train 0.26548934469031077 valid 0.23790759871066627
LOSS train 0.26548934469031077 valid 0.23759432237188355
LOSS train 0.26548934469031077 valid 0.23724791146814822
LOSS train 0.26548934469031077 valid 0.23713517398381037
LOSS train 0.26548934469031077 valid 0.23735489989401864
LOSS train 0.26548934469031077 valid 0.23742786449630085
LOSS train 0.26548934469031077 valid 0.23766292355233623
LOSS train 0.26548934469031077 valid 0.23762625336647034
LOSS train 0.26548934469031077 valid 0.23791627964330098
LOSS train 0.26548934469031077 valid 0.23778655292011622
LOSS train 0.26548934469031077 valid 0.23776473209727556
LOSS train 0.26548934469031077 valid 0.2373433772676675
LOSS train 0.26548934469031077 valid 0.23710027520473187
LOSS train 0.26548934469031077 valid 0.23703549051557787
LOSS train 0.26548934469031077 valid 0.23704865439371628
LOSS train 0.26548934469031077 valid 0.23708304516354897
LOSS train 0.26548934469031077 valid 0.237334610588515
LOSS train 0.26548934469031077 valid 0.23745952756316574
LOSS train 0.26548934469031077 valid 0.23752910928691134
LOSS train 0.26548934469031077 valid 0.2376759459067435
LOSS train 0.26548934469031077 valid 0.23759844694016635
LOSS train 0.26548934469031077 valid 0.23736876485159072
LOSS train 0.26548934469031077 valid 0.23730033582874707
LOSS train 0.26548934469031077 valid 0.23740830415106834
LOSS train 0.26548934469031077 valid 0.2376903466565508
LOSS train 0.26548934469031077 valid 0.23779107130074
LOSS train 0.26548934469031077 valid 0.237854795013037
LOSS train 0.26548934469031077 valid 0.23781289499381492
LOSS train 0.26548934469031077 valid 0.2377332941922423
LOSS train 0.26548934469031077 valid 0.23762629287583487
LOSS train 0.26548934469031077 valid 0.23758554498891574
LOSS train 0.26548934469031077 valid 0.23769355960340308
LOSS train 0.26548934469031077 valid 0.23770995557308197
LOSS train 0.26548934469031077 valid 0.23766738492132022
LOSS train 0.26548934469031077 valid 0.23761124928530894
LOSS train 0.26548934469031077 valid 0.23737268295942568
LOSS train 0.26548934469031077 valid 0.2374526657067336
LOSS train 0.26548934469031077 valid 0.23747535290256624
LOSS train 0.26548934469031077 valid 0.2374798875206556
LOSS train 0.26548934469031077 valid 0.23736378208846803
LOSS train 0.26548934469031077 valid 0.23747174969956844
LOSS train 0.26548934469031077 valid 0.2377477451315466
LOSS train 0.26548934469031077 valid 0.2379908947274089
LOSS train 0.26548934469031077 valid 0.23790478400932336
LOSS train 0.26548934469031077 valid 0.23798637432448658
LOSS train 0.26548934469031077 valid 0.2377699252469408
LOSS train 0.26548934469031077 valid 0.23783025345424327
LOSS train 0.26548934469031077 valid 0.23797794779141743
LOSS train 0.26548934469031077 valid 0.23793494342321372
LOSS train 0.26548934469031077 valid 0.23798187139505397
LOSS train 0.26548934469031077 valid 0.23787381180695125
LOSS train 0.26548934469031077 valid 0.23766693472862244
LOSS train 0.26548934469031077 valid 0.23772353065364502
LOSS train 0.26548934469031077 valid 0.23758734675527315
LOSS train 0.26548934469031077 valid 0.23764488508188447
LOSS train 0.26548934469031077 valid 0.23750153950528602
LOSS train 0.26548934469031077 valid 0.2378669356306394
LOSS train 0.26548934469031077 valid 0.2378535911015102
LOSS train 0.26548934469031077 valid 0.2377599753778089
LOSS train 0.26548934469031077 valid 0.23784837665530922
LOSS train 0.26548934469031077 valid 0.23785703570655223
LOSS train 0.26548934469031077 valid 0.23809987589633663
LOSS train 0.26548934469031077 valid 0.23800870279471079
LOSS train 0.26548934469031077 valid 0.237908611146126
LOSS train 0.26548934469031077 valid 0.23773322786603654
LOSS train 0.26548934469031077 valid 0.23784883703038992
LOSS train 0.26548934469031077 valid 0.23778071708005408
LOSS train 0.26548934469031077 valid 0.23768548836579195
LOSS train 0.26548934469031077 valid 0.2377336373732936
LOSS train 0.26548934469031077 valid 0.23755121422323952
LOSS train 0.26548934469031077 valid 0.23758195475377936
LOSS train 0.26548934469031077 valid 0.23735557961716222
LOSS train 0.26548934469031077 valid 0.23735943781702143
LOSS train 0.26548934469031077 valid 0.2374967327292677
LOSS train 0.26548934469031077 valid 0.2375644358495871
LOSS train 0.26548934469031077 valid 0.2373297523340413
LOSS train 0.26548934469031077 valid 0.23722113715004675
LOSS train 0.26548934469031077 valid 0.23709673369542147
LOSS train 0.26548934469031077 valid 0.2372819523574138
LOSS train 0.26548934469031077 valid 0.2372191337310723
LOSS train 0.26548934469031077 valid 0.23745077901115322
LOSS train 0.26548934469031077 valid 0.2374669043113239
LOSS train 0.26548934469031077 valid 0.23764604054391383
LOSS train 0.26548934469031077 valid 0.23746133206495598
LOSS train 0.26548934469031077 valid 0.2374048275817739
LOSS train 0.26548934469031077 valid 0.23750833117315923
LOSS train 0.26548934469031077 valid 0.2374368296826587
LOSS train 0.26548934469031077 valid 0.2373517309747091
LOSS train 0.26548934469031077 valid 0.23727502232616388
LOSS train 0.26548934469031077 valid 0.23721111324674266
LOSS train 0.26548934469031077 valid 0.2370998587172765
LOSS train 0.26548934469031077 valid 0.23713683896657953
LOSS train 0.26548934469031077 valid 0.23731273909409842
LOSS train 0.26548934469031077 valid 0.2375182741061206
LOSS train 0.26548934469031077 valid 0.2373268944474886
LOSS train 0.26548934469031077 valid 0.2372101026241768
LOSS train 0.26548934469031077 valid 0.23708808784172913
LOSS train 0.26548934469031077 valid 0.23682073077490162
LOSS train 0.26548934469031077 valid 0.23661430193870156
LOSS train 0.26548934469031077 valid 0.23636228141422097
LOSS train 0.26548934469031077 valid 0.23632190144116724
LOSS train 0.26548934469031077 valid 0.23628139659150005
LOSS train 0.26548934469031077 valid 0.2363587043502114
LOSS train 0.26548934469031077 valid 0.2363367116424293
LOSS train 0.26548934469031077 valid 0.23640924148462913
LOSS train 0.26548934469031077 valid 0.23653071470592055
LOSS train 0.26548934469031077 valid 0.23646623208852752
LOSS train 0.26548934469031077 valid 0.23628248393535614
LOSS train 0.26548934469031077 valid 0.2360989305034148
LOSS train 0.26548934469031077 valid 0.23614043949196517
LOSS train 0.26548934469031077 valid 0.2360896453784223
LOSS train 0.26548934469031077 valid 0.2359691860894449
LOSS train 0.26548934469031077 valid 0.23602562935455987
LOSS train 0.26548934469031077 valid 0.23595879107584686
LOSS train 0.26548934469031077 valid 0.23609930026377068
LOSS train 0.26548934469031077 valid 0.23650802876560473
LOSS train 0.26548934469031077 valid 0.23646215534108317
LOSS train 0.26548934469031077 valid 0.23647314975870418
LOSS train 0.26548934469031077 valid 0.23634332102739206
LOSS train 0.26548934469031077 valid 0.236436900207262
LOSS train 0.26548934469031077 valid 0.23638950188119873
LOSS train 0.26548934469031077 valid 0.23637794768710516
LOSS train 0.26548934469031077 valid 0.23597602614512045
LOSS train 0.26548934469031077 valid 0.2358453292940662
LOSS train 0.26548934469031077 valid 0.23575008260317085
LOSS train 0.26548934469031077 valid 0.2356483121958289
LOSS train 0.26548934469031077 valid 0.23562588890800712
LOSS train 0.26548934469031077 valid 0.23579800743229534
LOSS train 0.26548934469031077 valid 0.2358506385509561
LOSS train 0.26548934469031077 valid 0.23586903958909425
LOSS train 0.26548934469031077 valid 0.2358416732280485
LOSS train 0.26548934469031077 valid 0.2356953963816884
LOSS train 0.26548934469031077 valid 0.23583784061670304
LOSS train 0.26548934469031077 valid 0.23586743114241565
LOSS train 0.26548934469031077 valid 0.23578289476415468
LOSS train 0.26548934469031077 valid 0.23559958867640363
LOSS train 0.26548934469031077 valid 0.23569775325810816
LOSS train 0.26548934469031077 valid 0.23558119734128316
LOSS train 0.26548934469031077 valid 0.23533549410058185
LOSS train 0.26548934469031077 valid 0.23533890950076775
LOSS train 0.26548934469031077 valid 0.2355377402647521
LOSS train 0.26548934469031077 valid 0.23547766159169922
LOSS train 0.26548934469031077 valid 0.2352966719521926
LOSS train 0.26548934469031077 valid 0.23528824421181077
LOSS train 0.26548934469031077 valid 0.23544290968934997
LOSS train 0.26548934469031077 valid 0.23525766121797234
LOSS train 0.26548934469031077 valid 0.23532782523243717
LOSS train 0.26548934469031077 valid 0.23523011471865313
LOSS train 0.26548934469031077 valid 0.235269638940804
LOSS train 0.26548934469031077 valid 0.23515163216921275
LOSS train 0.26548934469031077 valid 0.2352260299471777
LOSS train 0.26548934469031077 valid 0.2353401923600626
LOSS train 0.26548934469031077 valid 0.23517539650201796
LOSS train 0.26548934469031077 valid 0.23514241656474083
LOSS train 0.26548934469031077 valid 0.23503975137410796
LOSS train 0.26548934469031077 valid 0.23490381098928906
LOSS train 0.26548934469031077 valid 0.23480183484345457
LOSS train 0.26548934469031077 valid 0.23486552335999228
LOSS train 0.26548934469031077 valid 0.23489257714886597
LOSS train 0.26548934469031077 valid 0.23490884926990482
LOSS train 0.26548934469031077 valid 0.23487693644899257
LOSS train 0.26548934469031077 valid 0.23487076579883534
LOSS train 0.26548934469031077 valid 0.23477684647909233
LOSS train 0.26548934469031077 valid 0.23461334460358602
LOSS train 0.26548934469031077 valid 0.23458659939520748
LOSS train 0.26548934469031077 valid 0.23460274160750763
LOSS train 0.26548934469031077 valid 0.2345646156496565
LOSS train 0.26548934469031077 valid 0.23451394872707232
LOSS train 0.26548934469031077 valid 0.2344928053500769
LOSS train 0.26548934469031077 valid 0.23455714192955335
LOSS train 0.26548934469031077 valid 0.23454081272292468
LOSS train 0.26548934469031077 valid 0.23449832680522364
LOSS train 0.26548934469031077 valid 0.2343725010238845
LOSS train 0.26548934469031077 valid 0.2343431576011107
LOSS train 0.26548934469031077 valid 0.23428927464027927
LOSS train 0.26548934469031077 valid 0.2342363394567991
LOSS train 0.26548934469031077 valid 0.2342897343899117
LOSS train 0.26548934469031077 valid 0.23434077016377852
LOSS train 0.26548934469031077 valid 0.23428544804856583
LOSS train 0.26548934469031077 valid 0.23419865714981902
LOSS train 0.26548934469031077 valid 0.234195843038943
LOSS train 0.26548934469031077 valid 0.2341601160358027
LOSS train 0.26548934469031077 valid 0.23409949496388435
LOSS train 0.26548934469031077 valid 0.23398067032379963
LOSS train 0.26548934469031077 valid 0.23394874646174196
LOSS train 0.26548934469031077 valid 0.23389866943209872
LOSS train 0.26548934469031077 valid 0.23389830861828828
LOSS train 0.26548934469031077 valid 0.23385810226690573
LOSS train 0.26548934469031077 valid 0.23410695411410987
LOSS train 0.26548934469031077 valid 0.23413698151950338
LOSS train 0.26548934469031077 valid 0.23422400774313257
LOSS train 0.26548934469031077 valid 0.23425034667488825
LOSS train 0.26548934469031077 valid 0.23429331394933886
LOSS train 0.26548934469031077 valid 0.23430717796374748
LOSS train 0.26548934469031077 valid 0.23428468836041597
LOSS train 0.26548934469031077 valid 0.2342729822705729
LOSS train 0.26548934469031077 valid 0.2341706200390105
LOSS train 0.26548934469031077 valid 0.23413272542612892
LOSS train 0.26548934469031077 valid 0.23407491504014294
LOSS train 0.26548934469031077 valid 0.23408249904304648
LOSS train 0.26548934469031077 valid 0.2340727710873826
LOSS train 0.26548934469031077 valid 0.23406785404233724
LOSS train 0.26548934469031077 valid 0.23394351759925486
LOSS train 0.26548934469031077 valid 0.23405296065354272
LOSS train 0.26548934469031077 valid 0.23405469380180288
LOSS train 0.26548934469031077 valid 0.23396053471985986
LOSS train 0.26548934469031077 valid 0.23395736372949164
LOSS train 0.26548934469031077 valid 0.2338367079312985
LOSS train 0.26548934469031077 valid 0.23394599817464687
LOSS train 0.26548934469031077 valid 0.23385298206536412
LOSS train 0.26548934469031077 valid 0.23382354582228312
LOSS train 0.26548934469031077 valid 0.23380160675947426
LOSS train 0.26548934469031077 valid 0.23380802129254197
LOSS train 0.26548934469031077 valid 0.23373178854089488
LOSS train 0.26548934469031077 valid 0.23373294271618486
LOSS train 0.26548934469031077 valid 0.23381877580920496
LOSS train 0.26548934469031077 valid 0.23373494685410026
LOSS train 0.26548934469031077 valid 0.2336029552256883
LOSS train 0.26548934469031077 valid 0.23354366723270642
LOSS train 0.26548934469031077 valid 0.23368750145598052
LOSS train 0.26548934469031077 valid 0.23368323409169384
LOSS train 0.26548934469031077 valid 0.23365932162356587
LOSS train 0.26548934469031077 valid 0.23381665037835345
LOSS train 0.26548934469031077 valid 0.23376888578588312
LOSS train 0.26548934469031077 valid 0.23376336563051792
LOSS train 0.26548934469031077 valid 0.2336372819568951
LOSS train 0.26548934469031077 valid 0.23370319202022496
LOSS train 0.26548934469031077 valid 0.2338311067957809
LOSS train 0.26548934469031077 valid 0.23379259879533956
LOSS train 0.26548934469031077 valid 0.23367691383581685
LOSS train 0.26548934469031077 valid 0.23376317913162298
LOSS train 0.26548934469031077 valid 0.23372173719214848
LOSS train 0.26548934469031077 valid 0.23373848140239717
LOSS train 0.26548934469031077 valid 0.23383214696180446
LOSS train 0.26548934469031077 valid 0.2338794907554984
LOSS train 0.26548934469031077 valid 0.23394379796454995
LOSS train 0.26548934469031077 valid 0.2339848484528267
LOSS train 0.26548934469031077 valid 0.23382248689591045
LOSS train 0.26548934469031077 valid 0.23385320875919266
LOSS train 0.26548934469031077 valid 0.23385270090163254
LOSS train 0.26548934469031077 valid 0.23379413658680195
LOSS train 0.26548934469031077 valid 0.23372241867119886
LOSS train 0.26548934469031077 valid 0.23386307338045703
LOSS train 0.26548934469031077 valid 0.2337972641899315
LOSS train 0.26548934469031077 valid 0.2338531854979241
LOSS train 0.26548934469031077 valid 0.23392210055152904
LOSS train 0.26548934469031077 valid 0.23383518672742687
LOSS train 0.26548934469031077 valid 0.2339606518206531
LOSS train 0.26548934469031077 valid 0.23393799787987776
LOSS train 0.26548934469031077 valid 0.23388909708901387
LOSS train 0.26548934469031077 valid 0.2337989166136021
LOSS train 0.26548934469031077 valid 0.2337748227403739
EPOCH 13:
  batch 1 loss: 0.26510781049728394
  batch 2 loss: 0.28488726913928986
  batch 3 loss: 0.26972195506095886
  batch 4 loss: 0.26743409037590027
  batch 5 loss: 0.271579909324646
  batch 6 loss: 0.26828618347644806
  batch 7 loss: 0.2673571152346475
  batch 8 loss: 0.26419464498758316
  batch 9 loss: 0.26629451248380875
  batch 10 loss: 0.2630092576146126
  batch 11 loss: 0.26287001235918567
  batch 12 loss: 0.26209690049290657
  batch 13 loss: 0.2602832844624153
  batch 14 loss: 0.2591605910233089
  batch 15 loss: 0.2610282282034556
  batch 16 loss: 0.26016190089285374
  batch 17 loss: 0.2626120553297155
  batch 18 loss: 0.26135477009746766
  batch 19 loss: 0.26035949509394796
  batch 20 loss: 0.25830413177609446
  batch 21 loss: 0.2618049361876079
  batch 22 loss: 0.26247482204979117
  batch 23 loss: 0.2616130897532339
  batch 24 loss: 0.26228254723052186
  batch 25 loss: 0.26333244621753693
  batch 26 loss: 0.2638405292079999
  batch 27 loss: 0.26372088823053574
  batch 28 loss: 0.26421355083584785
  batch 29 loss: 0.2643258484273121
  batch 30 loss: 0.2659279669324557
  batch 31 loss: 0.2665315486731068
  batch 32 loss: 0.26783301727846265
  batch 33 loss: 0.26945558687051135
  batch 34 loss: 0.27047036369057265
  batch 35 loss: 0.2722479143312999
  batch 36 loss: 0.2722622797720962
  batch 37 loss: 0.27225132367095434
  batch 38 loss: 0.27325281461602763
  batch 39 loss: 0.27430654450868946
  batch 40 loss: 0.2744425978511572
  batch 41 loss: 0.27373988853722087
  batch 42 loss: 0.2727177639802297
  batch 43 loss: 0.27350396333738813
  batch 44 loss: 0.27481346171010623
  batch 45 loss: 0.2740216030014886
  batch 46 loss: 0.2731944685396941
  batch 47 loss: 0.27363921289748333
  batch 48 loss: 0.2734453411151965
  batch 49 loss: 0.2741157345625819
  batch 50 loss: 0.27530880451202394
  batch 51 loss: 0.2755567104208703
  batch 52 loss: 0.27575545471448165
  batch 53 loss: 0.27565293604472896
  batch 54 loss: 0.2764449500375324
  batch 55 loss: 0.2767235506664623
  batch 56 loss: 0.27700804280383245
  batch 57 loss: 0.27640592293781147
  batch 58 loss: 0.27593980190055123
  batch 59 loss: 0.2762669968908116
  batch 60 loss: 0.2766689774890741
  batch 61 loss: 0.2771063967317831
  batch 62 loss: 0.277573540566429
  batch 63 loss: 0.27772094640466904
  batch 64 loss: 0.2772610322572291
  batch 65 loss: 0.2765133275435521
  batch 66 loss: 0.2769392236615672
  batch 67 loss: 0.27728486950717757
  batch 68 loss: 0.27732377236380296
  batch 69 loss: 0.27720713485842163
  batch 70 loss: 0.27757940079484666
  batch 71 loss: 0.2777044743719235
  batch 72 loss: 0.2777629962397946
  batch 73 loss: 0.2777036248821102
  batch 74 loss: 0.2774288581835257
  batch 75 loss: 0.27752349376678465
  batch 76 loss: 0.2776023729851371
  batch 77 loss: 0.2772981969567088
  batch 78 loss: 0.2774807421060709
  batch 79 loss: 0.27724013109750384
  batch 80 loss: 0.27720216028392314
  batch 81 loss: 0.27707595148204284
  batch 82 loss: 0.2770703987377446
  batch 83 loss: 0.2765949266143592
  batch 84 loss: 0.27590432443789076
  batch 85 loss: 0.27621632428730236
  batch 86 loss: 0.2760350485873777
  batch 87 loss: 0.27568909286082477
  batch 88 loss: 0.2754698653112758
  batch 89 loss: 0.2750924244690477
  batch 90 loss: 0.2752693167991108
  batch 91 loss: 0.27481218288232995
  batch 92 loss: 0.27450411442829215
  batch 93 loss: 0.27396650131671657
  batch 94 loss: 0.2736505650459452
  batch 95 loss: 0.2734453979291414
  batch 96 loss: 0.27343827703346807
  batch 97 loss: 0.27378632850253704
  batch 98 loss: 0.27348073617536195
  batch 99 loss: 0.27299655371844167
  batch 100 loss: 0.2732174848020077
  batch 101 loss: 0.2730231507874952
  batch 102 loss: 0.2731502507539356
  batch 103 loss: 0.2729156904139565
  batch 104 loss: 0.273334890174178
  batch 105 loss: 0.273143719065757
  batch 106 loss: 0.2729424565749348
  batch 107 loss: 0.27289599444821616
  batch 108 loss: 0.2723483951003463
  batch 109 loss: 0.2724983347665279
  batch 110 loss: 0.2722626659003171
  batch 111 loss: 0.27224859299960436
  batch 112 loss: 0.27200338310961214
  batch 113 loss: 0.2716986492144323
  batch 114 loss: 0.2715064104188952
  batch 115 loss: 0.27109882818615955
  batch 116 loss: 0.2706483490765095
  batch 117 loss: 0.27054638320054764
  batch 118 loss: 0.2705100763399722
  batch 119 loss: 0.2705384183581136
  batch 120 loss: 0.27021238940457504
  batch 121 loss: 0.2701275214429729
  batch 122 loss: 0.2698447699429559
  batch 123 loss: 0.269494055369036
  batch 124 loss: 0.26961488841522124
  batch 125 loss: 0.2692891644239426
  batch 126 loss: 0.2693397408676526
  batch 127 loss: 0.26944455117221894
  batch 128 loss: 0.2695032701594755
  batch 129 loss: 0.26975427059702173
  batch 130 loss: 0.2695838743677506
  batch 131 loss: 0.2694386142583294
  batch 132 loss: 0.26963707703081047
  batch 133 loss: 0.26942522353247594
  batch 134 loss: 0.2692830014362264
  batch 135 loss: 0.2691711954496525
  batch 136 loss: 0.26933816064368277
  batch 137 loss: 0.2698254662491109
  batch 138 loss: 0.2697585960445197
  batch 139 loss: 0.2696238999529708
  batch 140 loss: 0.26994818331939835
  batch 141 loss: 0.26967082769735484
  batch 142 loss: 0.2692723306761661
  batch 143 loss: 0.2694088448802908
  batch 144 loss: 0.26925061198158395
  batch 145 loss: 0.26924883877408917
  batch 146 loss: 0.26948331265825115
  batch 147 loss: 0.26938948390029727
  batch 148 loss: 0.2693531173910644
  batch 149 loss: 0.26884424006378893
  batch 150 loss: 0.2688926587502162
  batch 151 loss: 0.26854430168669746
  batch 152 loss: 0.26826625660453973
  batch 153 loss: 0.26846159332328373
  batch 154 loss: 0.2681966575902778
  batch 155 loss: 0.2680840152886606
  batch 156 loss: 0.26790710013264263
  batch 157 loss: 0.26777839859959424
  batch 158 loss: 0.26779196298197855
  batch 159 loss: 0.2679281694896566
  batch 160 loss: 0.26791582023724914
  batch 161 loss: 0.26758853757973783
  batch 162 loss: 0.2675358241906873
  batch 163 loss: 0.26727932399035964
  batch 164 loss: 0.26754930433703633
  batch 165 loss: 0.26772070292270544
  batch 166 loss: 0.2679662742169507
  batch 167 loss: 0.2678622358930325
  batch 168 loss: 0.26760260494691984
  batch 169 loss: 0.2676120573246973
  batch 170 loss: 0.26767877165008996
  batch 171 loss: 0.26775065412995414
  batch 172 loss: 0.26764383582874784
  batch 173 loss: 0.26768261883300165
  batch 174 loss: 0.2678617638075489
  batch 175 loss: 0.26792337707110814
  batch 176 loss: 0.2680717029354789
  batch 177 loss: 0.26826145605178875
  batch 178 loss: 0.26822595934519605
  batch 179 loss: 0.2682403272756651
  batch 180 loss: 0.26821525477700764
  batch 181 loss: 0.26809937387540195
  batch 182 loss: 0.26813617970917253
  batch 183 loss: 0.26805715043036665
  batch 184 loss: 0.2679491851316846
  batch 185 loss: 0.2677322071951789
  batch 186 loss: 0.2676939461179959
  batch 187 loss: 0.2677787614059958
  batch 188 loss: 0.2676709214265042
  batch 189 loss: 0.2676677397161564
  batch 190 loss: 0.26754132619029597
  batch 191 loss: 0.2676735328442139
  batch 192 loss: 0.26751685584895313
  batch 193 loss: 0.26744391286620206
  batch 194 loss: 0.26725879749379206
  batch 195 loss: 0.2674555120559839
  batch 196 loss: 0.2672833558552119
  batch 197 loss: 0.26736234060398817
  batch 198 loss: 0.2672693511151304
  batch 199 loss: 0.26724909163599636
  batch 200 loss: 0.2671909798681736
  batch 201 loss: 0.26713376258736227
  batch 202 loss: 0.26723548592907365
  batch 203 loss: 0.2672655783556952
  batch 204 loss: 0.2673424844648324
  batch 205 loss: 0.26719282531156774
  batch 206 loss: 0.26735011900512917
  batch 207 loss: 0.2673399900468651
  batch 208 loss: 0.26712521731566924
  batch 209 loss: 0.2671484005365645
  batch 210 loss: 0.26709106798682897
  batch 211 loss: 0.26693248247365814
  batch 212 loss: 0.2668273588115314
  batch 213 loss: 0.26657232866040975
  batch 214 loss: 0.2663802711345325
  batch 215 loss: 0.2663420080445534
  batch 216 loss: 0.26625547951294315
  batch 217 loss: 0.26616032586394367
  batch 218 loss: 0.2660403973465666
  batch 219 loss: 0.2657551386448891
  batch 220 loss: 0.26602391878312287
  batch 221 loss: 0.26585994775478655
  batch 222 loss: 0.2658399773073626
  batch 223 loss: 0.2657273445829682
  batch 224 loss: 0.2657623360864818
  batch 225 loss: 0.2657850308550729
  batch 226 loss: 0.2658316936772481
  batch 227 loss: 0.2658563720365978
  batch 228 loss: 0.26590134837386903
  batch 229 loss: 0.26592089274824965
  batch 230 loss: 0.2659038075286409
  batch 231 loss: 0.2658897540915064
  batch 232 loss: 0.26579034200002405
  batch 233 loss: 0.26578361640160686
  batch 234 loss: 0.2657277564971875
  batch 235 loss: 0.265622866470763
  batch 236 loss: 0.26554951759970796
  batch 237 loss: 0.26543727957246677
  batch 238 loss: 0.2653277027381568
  batch 239 loss: 0.2652761901896369
  batch 240 loss: 0.2654016828164458
  batch 241 loss: 0.2652617192119978
  batch 242 loss: 0.26521737787349164
  batch 243 loss: 0.2651671831009319
  batch 244 loss: 0.26511100533067206
  batch 245 loss: 0.2651175453954813
  batch 246 loss: 0.26498985181494455
  batch 247 loss: 0.2649816128647762
  batch 248 loss: 0.26496622927727237
  batch 249 loss: 0.26496586179637527
  batch 250 loss: 0.26511836218833923
  batch 251 loss: 0.2649257925164652
  batch 252 loss: 0.26489249285724425
  batch 253 loss: 0.26476792737900506
  batch 254 loss: 0.26467202392619427
  batch 255 loss: 0.2644971738843357
  batch 256 loss: 0.26437337778043
  batch 257 loss: 0.26435816102454635
  batch 258 loss: 0.2643702224243519
  batch 259 loss: 0.26414408014087604
  batch 260 loss: 0.2641802876041486
  batch 261 loss: 0.2642219373777908
  batch 262 loss: 0.2640986275923161
  batch 263 loss: 0.26397719690328314
  batch 264 loss: 0.2639829029300899
  batch 265 loss: 0.2638804756807831
  batch 266 loss: 0.2636979002701609
  batch 267 loss: 0.2635802692465121
  batch 268 loss: 0.2634261038654776
  batch 269 loss: 0.2634223748870942
  batch 270 loss: 0.2633633970110505
  batch 271 loss: 0.26335870878722833
  batch 272 loss: 0.263316682923366
  batch 273 loss: 0.2630804963819273
  batch 274 loss: 0.26289324527674346
  batch 275 loss: 0.2628782244162126
  batch 276 loss: 0.26280247851990274
  batch 277 loss: 0.2627425823185848
  batch 278 loss: 0.2626266424818862
  batch 279 loss: 0.2626059110873916
  batch 280 loss: 0.2625847469483103
  batch 281 loss: 0.2625149208986038
  batch 282 loss: 0.2623796451387676
  batch 283 loss: 0.26229577682675403
  batch 284 loss: 0.2622845317171493
  batch 285 loss: 0.26217563826786844
  batch 286 loss: 0.26214667638907063
  batch 287 loss: 0.2619730730505355
  batch 288 loss: 0.261938971777757
  batch 289 loss: 0.26177804012199585
  batch 290 loss: 0.26164055744121817
  batch 291 loss: 0.26151480666550575
  batch 292 loss: 0.2613880495502524
  batch 293 loss: 0.26139179695995185
  batch 294 loss: 0.2611626745486746
  batch 295 loss: 0.2611019049660634
  batch 296 loss: 0.2612322121858597
  batch 297 loss: 0.2611862176696861
  batch 298 loss: 0.2611531453744677
  batch 299 loss: 0.2611681533398038
  batch 300 loss: 0.2609920954207579
  batch 301 loss: 0.2609938286566655
  batch 302 loss: 0.2611384561026333
  batch 303 loss: 0.261086052843053
  batch 304 loss: 0.2610345418123822
  batch 305 loss: 0.2609799581472991
  batch 306 loss: 0.261018764251977
  batch 307 loss: 0.2609880059860428
  batch 308 loss: 0.26103916251427167
  batch 309 loss: 0.26099995675596216
  batch 310 loss: 0.2610609845769021
  batch 311 loss: 0.2609985991114589
  batch 312 loss: 0.2609592955081891
  batch 313 loss: 0.26090202820948516
  batch 314 loss: 0.2608259626824385
  batch 315 loss: 0.2607823636323687
  batch 316 loss: 0.2607431840858882
  batch 317 loss: 0.2607996094302048
  batch 318 loss: 0.2606817698984776
  batch 319 loss: 0.2604459802091682
  batch 320 loss: 0.2605816597584635
  batch 321 loss: 0.2605658576960133
  batch 322 loss: 0.26043062694139363
  batch 323 loss: 0.2605466366091011
  batch 324 loss: 0.26066318352097345
  batch 325 loss: 0.2607649048933616
  batch 326 loss: 0.2608578196423917
  batch 327 loss: 0.2607982037902243
  batch 328 loss: 0.2607112365615804
  batch 329 loss: 0.2607861116392634
  batch 330 loss: 0.2608165773929972
  batch 331 loss: 0.2608102081495469
  batch 332 loss: 0.26091295404427023
  batch 333 loss: 0.26090537915537665
  batch 334 loss: 0.261165890463455
  batch 335 loss: 0.26126529676700705
  batch 336 loss: 0.26132784362527584
  batch 337 loss: 0.2613487597831279
  batch 338 loss: 0.2613263041574574
  batch 339 loss: 0.2613225007391257
  batch 340 loss: 0.26124170391875157
  batch 341 loss: 0.26120208107656046
  batch 342 loss: 0.2611649409372207
  batch 343 loss: 0.2612457459591568
  batch 344 loss: 0.26129066571593285
  batch 345 loss: 0.26134305475414665
  batch 346 loss: 0.26132217480268094
  batch 347 loss: 0.26118470380560466
  batch 348 loss: 0.26118923800772637
  batch 349 loss: 0.2612536400129597
  batch 350 loss: 0.2612836329426084
  batch 351 loss: 0.26121355138952573
  batch 352 loss: 0.26110310323367064
  batch 353 loss: 0.2610792089200898
  batch 354 loss: 0.26100767824777776
  batch 355 loss: 0.2609283067810703
  batch 356 loss: 0.2609545358278778
  batch 357 loss: 0.2607918346629423
  batch 358 loss: 0.2606492781772294
  batch 359 loss: 0.260638864674608
  batch 360 loss: 0.2605792103542222
  batch 361 loss: 0.26051543231981283
  batch 362 loss: 0.26048174990310197
  batch 363 loss: 0.26053821888672746
  batch 364 loss: 0.26039995494615903
  batch 365 loss: 0.260367868044605
  batch 366 loss: 0.26034897305274923
  batch 367 loss: 0.2603045591947493
  batch 368 loss: 0.2602568861178082
  batch 369 loss: 0.26018398002718846
  batch 370 loss: 0.26019457550467673
  batch 371 loss: 0.2601696459671879
  batch 372 loss: 0.2602071881855047
  batch 373 loss: 0.26021450386450057
  batch 374 loss: 0.26014245500698446
  batch 375 loss: 0.26019787601629896
  batch 376 loss: 0.2603742559935818
  batch 377 loss: 0.2603520468391221
  batch 378 loss: 0.2603109385522585
  batch 379 loss: 0.2603015093265549
  batch 380 loss: 0.26024257038768966
  batch 381 loss: 0.2601703721394376
  batch 382 loss: 0.2601710816172405
  batch 383 loss: 0.2600539722346139
  batch 384 loss: 0.2599968795354168
  batch 385 loss: 0.25999990833270087
  batch 386 loss: 0.2599710098665613
  batch 387 loss: 0.25989052749573416
  batch 388 loss: 0.25971014786166013
  batch 389 loss: 0.2597457028369977
  batch 390 loss: 0.25975776135157314
  batch 391 loss: 0.2596666956572886
  batch 392 loss: 0.25976433494717494
  batch 393 loss: 0.25975170560920513
  batch 394 loss: 0.25973350312655347
  batch 395 loss: 0.2597361789851249
  batch 396 loss: 0.2598137168796978
  batch 397 loss: 0.259853441896907
  batch 398 loss: 0.25990171741750373
  batch 399 loss: 0.2598865391840612
  batch 400 loss: 0.25997902881354096
  batch 401 loss: 0.25996987581104414
  batch 402 loss: 0.2599594862529294
  batch 403 loss: 0.2598854229778569
  batch 404 loss: 0.25985720012300084
  batch 405 loss: 0.25981444819473926
  batch 406 loss: 0.25969944846600734
  batch 407 loss: 0.25969813412618287
  batch 408 loss: 0.2596022331466277
  batch 409 loss: 0.25960165273152236
  batch 410 loss: 0.2594552705200707
  batch 411 loss: 0.2593971068841697
  batch 412 loss: 0.2595882166213202
  batch 413 loss: 0.25954128177633584
  batch 414 loss: 0.259587898850441
  batch 415 loss: 0.2595255214406783
  batch 416 loss: 0.2595574206386048
  batch 417 loss: 0.25952148891324334
  batch 418 loss: 0.25950737308229554
  batch 419 loss: 0.2595153356010988
  batch 420 loss: 0.25952263109031176
  batch 421 loss: 0.25961524053452417
  batch 422 loss: 0.2595639998243318
  batch 423 loss: 0.25950464687307956
  batch 424 loss: 0.25953082026878616
  batch 425 loss: 0.2594105247890248
  batch 426 loss: 0.25936765660982175
  batch 427 loss: 0.2594016449792044
  batch 428 loss: 0.2594496751917857
  batch 429 loss: 0.2593392380328723
  batch 430 loss: 0.2593286137941272
  batch 431 loss: 0.2593446025439039
  batch 432 loss: 0.25937127515121744
  batch 433 loss: 0.2593531057961268
  batch 434 loss: 0.25930495100087286
  batch 435 loss: 0.259392295829181
  batch 436 loss: 0.2593860794371421
  batch 437 loss: 0.25931385115164096
  batch 438 loss: 0.2592424150879525
  batch 439 loss: 0.2591602219128663
  batch 440 loss: 0.2592689644206654
  batch 441 loss: 0.2593058296342015
  batch 442 loss: 0.2592833551548725
  batch 443 loss: 0.2592907923002544
  batch 444 loss: 0.2593117823807506
  batch 445 loss: 0.2592565676134624
  batch 446 loss: 0.2592573730199861
  batch 447 loss: 0.25922419507498173
  batch 448 loss: 0.25930428731122185
  batch 449 loss: 0.2593043297861095
  batch 450 loss: 0.2592879252963596
  batch 451 loss: 0.2592179188773267
  batch 452 loss: 0.2592042444883722
  batch 453 loss: 0.25920555064615036
  batch 454 loss: 0.25915616918109063
  batch 455 loss: 0.25914597246018084
  batch 456 loss: 0.25908707281607285
  batch 457 loss: 0.25909706757809453
  batch 458 loss: 0.25909092088583774
  batch 459 loss: 0.2590379815280827
  batch 460 loss: 0.258990830409786
  batch 461 loss: 0.2590396065838683
  batch 462 loss: 0.2590665640188502
  batch 463 loss: 0.25912042586515066
  batch 464 loss: 0.2591263930923466
  batch 465 loss: 0.2591311523670791
  batch 466 loss: 0.2590493494285023
  batch 467 loss: 0.2591674667673397
  batch 468 loss: 0.25918442364304495
  batch 469 loss: 0.2591048388847156
  batch 470 loss: 0.2591636556260129
  batch 471 loss: 0.2592343957560837
  batch 472 loss: 0.25909031675023547
LOSS train 0.25909031675023547 valid 0.23839572072029114
LOSS train 0.25909031675023547 valid 0.22878611087799072
LOSS train 0.25909031675023547 valid 0.23522062102953592
LOSS train 0.25909031675023547 valid 0.21822745725512505
LOSS train 0.25909031675023547 valid 0.21903041303157805
LOSS train 0.25909031675023547 valid 0.22419251749912897
LOSS train 0.25909031675023547 valid 0.21982852263110025
LOSS train 0.25909031675023547 valid 0.21790926344692707
LOSS train 0.25909031675023547 valid 0.21670896311601004
LOSS train 0.25909031675023547 valid 0.21464064419269563
LOSS train 0.25909031675023547 valid 0.21549815074964004
LOSS train 0.25909031675023547 valid 0.21840406830112138
LOSS train 0.25909031675023547 valid 0.2193736067185035
LOSS train 0.25909031675023547 valid 0.2171513640454837
LOSS train 0.25909031675023547 valid 0.21678329010804495
LOSS train 0.25909031675023547 valid 0.22005648631602526
LOSS train 0.25909031675023547 valid 0.2215558334308512
LOSS train 0.25909031675023547 valid 0.2218772917985916
LOSS train 0.25909031675023547 valid 0.22421998099276894
LOSS train 0.25909031675023547 valid 0.22355028688907624
LOSS train 0.25909031675023547 valid 0.22486992960884458
LOSS train 0.25909031675023547 valid 0.22420561449094253
LOSS train 0.25909031675023547 valid 0.22215257390685703
LOSS train 0.25909031675023547 valid 0.22227611082295576
LOSS train 0.25909031675023547 valid 0.22257785141468048
LOSS train 0.25909031675023547 valid 0.222153755907829
LOSS train 0.25909031675023547 valid 0.22204892999596065
LOSS train 0.25909031675023547 valid 0.22210707462259702
LOSS train 0.25909031675023547 valid 0.22058945945624647
LOSS train 0.25909031675023547 valid 0.22008667141199112
LOSS train 0.25909031675023547 valid 0.22041300371769937
LOSS train 0.25909031675023547 valid 0.22057325160130858
LOSS train 0.25909031675023547 valid 0.21981145125446896
LOSS train 0.25909031675023547 valid 0.21911665709579692
LOSS train 0.25909031675023547 valid 0.21927755219595774
LOSS train 0.25909031675023547 valid 0.21989875245425436
LOSS train 0.25909031675023547 valid 0.2204525196874464
LOSS train 0.25909031675023547 valid 0.22053634885110354
LOSS train 0.25909031675023547 valid 0.22133606060957298
LOSS train 0.25909031675023547 valid 0.22156625986099243
LOSS train 0.25909031675023547 valid 0.2213927892650046
LOSS train 0.25909031675023547 valid 0.2227179216487067
LOSS train 0.25909031675023547 valid 0.22316203768863235
LOSS train 0.25909031675023547 valid 0.22263715619390662
LOSS train 0.25909031675023547 valid 0.22204747166898514
LOSS train 0.25909031675023547 valid 0.22172811303449713
LOSS train 0.25909031675023547 valid 0.22155370293779575
LOSS train 0.25909031675023547 valid 0.22320500885446867
LOSS train 0.25909031675023547 valid 0.22250380321424834
LOSS train 0.25909031675023547 valid 0.22315245628356933
LOSS train 0.25909031675023547 valid 0.2229433714174757
LOSS train 0.25909031675023547 valid 0.22260342309108147
LOSS train 0.25909031675023547 valid 0.22392670388491648
LOSS train 0.25909031675023547 valid 0.22387361305731315
LOSS train 0.25909031675023547 valid 0.22388547442176127
LOSS train 0.25909031675023547 valid 0.22380911133119039
LOSS train 0.25909031675023547 valid 0.22336404417690478
LOSS train 0.25909031675023547 valid 0.22403144168442693
LOSS train 0.25909031675023547 valid 0.22360774723149962
LOSS train 0.25909031675023547 valid 0.22332848807175953
LOSS train 0.25909031675023547 valid 0.22333458456836763
LOSS train 0.25909031675023547 valid 0.22304248377200095
LOSS train 0.25909031675023547 valid 0.22274617874433125
LOSS train 0.25909031675023547 valid 0.22311506792902946
LOSS train 0.25909031675023547 valid 0.22209902864236097
LOSS train 0.25909031675023547 valid 0.22204448756846515
LOSS train 0.25909031675023547 valid 0.22284160598890104
LOSS train 0.25909031675023547 valid 0.22217579383183927
LOSS train 0.25909031675023547 valid 0.2228521611811458
LOSS train 0.25909031675023547 valid 0.22331002418484006
LOSS train 0.25909031675023547 valid 0.2236010213972817
LOSS train 0.25909031675023547 valid 0.22416911977860662
LOSS train 0.25909031675023547 valid 0.22479353538931232
LOSS train 0.25909031675023547 valid 0.2242310405582995
LOSS train 0.25909031675023547 valid 0.2238356508811315
LOSS train 0.25909031675023547 valid 0.22394372307156263
LOSS train 0.25909031675023547 valid 0.223845942260383
LOSS train 0.25909031675023547 valid 0.22358203106201613
LOSS train 0.25909031675023547 valid 0.22350421774236462
LOSS train 0.25909031675023547 valid 0.2232538614422083
LOSS train 0.25909031675023547 valid 0.22346090185053555
LOSS train 0.25909031675023547 valid 0.22337602605906928
LOSS train 0.25909031675023547 valid 0.22345481166638523
LOSS train 0.25909031675023547 valid 0.22325643879317103
LOSS train 0.25909031675023547 valid 0.22409008653724896
LOSS train 0.25909031675023547 valid 0.22400517363187877
LOSS train 0.25909031675023547 valid 0.2237389044857573
LOSS train 0.25909031675023547 valid 0.22404083084653725
LOSS train 0.25909031675023547 valid 0.22440309119358492
LOSS train 0.25909031675023547 valid 0.2246879615717464
LOSS train 0.25909031675023547 valid 0.22466608030455454
LOSS train 0.25909031675023547 valid 0.22470571635209996
LOSS train 0.25909031675023547 valid 0.22461242653349395
LOSS train 0.25909031675023547 valid 0.22475379388383093
LOSS train 0.25909031675023547 valid 0.22503022708390888
LOSS train 0.25909031675023547 valid 0.22525909869000316
LOSS train 0.25909031675023547 valid 0.22532005577357775
LOSS train 0.25909031675023547 valid 0.2258264487799333
LOSS train 0.25909031675023547 valid 0.22597565991107862
LOSS train 0.25909031675023547 valid 0.22612124770879746
LOSS train 0.25909031675023547 valid 0.22610245851596983
LOSS train 0.25909031675023547 valid 0.22652345033837298
LOSS train 0.25909031675023547 valid 0.2263617100356852
LOSS train 0.25909031675023547 valid 0.22621269638721758
LOSS train 0.25909031675023547 valid 0.22647838393847147
LOSS train 0.25909031675023547 valid 0.2264730618247446
LOSS train 0.25909031675023547 valid 0.22612649898662746
LOSS train 0.25909031675023547 valid 0.2262297965310238
LOSS train 0.25909031675023547 valid 0.22575656946645964
LOSS train 0.25909031675023547 valid 0.22574962892315603
LOSS train 0.25909031675023547 valid 0.22603952992069828
LOSS train 0.25909031675023547 valid 0.22641288409275667
LOSS train 0.25909031675023547 valid 0.22614655663481856
LOSS train 0.25909031675023547 valid 0.2262134123266789
LOSS train 0.25909031675023547 valid 0.22686153650283813
LOSS train 0.25909031675023547 valid 0.2265406570814807
LOSS train 0.25909031675023547 valid 0.2272793190856265
LOSS train 0.25909031675023547 valid 0.22715280192383266
LOSS train 0.25909031675023547 valid 0.22688999834681758
LOSS train 0.25909031675023547 valid 0.22653240809837977
LOSS train 0.25909031675023547 valid 0.22639264498860384
LOSS train 0.25909031675023547 valid 0.22663682161784562
LOSS train 0.25909031675023547 valid 0.22668736591571714
LOSS train 0.25909031675023547 valid 0.226992764540257
LOSS train 0.25909031675023547 valid 0.2269795924425125
LOSS train 0.25909031675023547 valid 0.22726142252721485
LOSS train 0.25909031675023547 valid 0.22716445528616117
LOSS train 0.25909031675023547 valid 0.22715405118651688
LOSS train 0.25909031675023547 valid 0.2267559542674427
LOSS train 0.25909031675023547 valid 0.22652623068827848
LOSS train 0.25909031675023547 valid 0.22647993073208642
LOSS train 0.25909031675023547 valid 0.2264713216008562
LOSS train 0.25909031675023547 valid 0.22650627814289323
LOSS train 0.25909031675023547 valid 0.22679171800168593
LOSS train 0.25909031675023547 valid 0.22687686869391688
LOSS train 0.25909031675023547 valid 0.22690489252700524
LOSS train 0.25909031675023547 valid 0.22694895780869645
LOSS train 0.25909031675023547 valid 0.22683877346740253
LOSS train 0.25909031675023547 valid 0.22661406991721914
LOSS train 0.25909031675023547 valid 0.22657069595796722
LOSS train 0.25909031675023547 valid 0.22665836973815945
LOSS train 0.25909031675023547 valid 0.22690458333408328
LOSS train 0.25909031675023547 valid 0.22701120136917888
LOSS train 0.25909031675023547 valid 0.22708807099196646
LOSS train 0.25909031675023547 valid 0.2270684809520327
LOSS train 0.25909031675023547 valid 0.22703235984256823
LOSS train 0.25909031675023547 valid 0.22688825955601777
LOSS train 0.25909031675023547 valid 0.22688589204807538
LOSS train 0.25909031675023547 valid 0.22698443007949215
LOSS train 0.25909031675023547 valid 0.2269633803764979
LOSS train 0.25909031675023547 valid 0.22689870197251932
LOSS train 0.25909031675023547 valid 0.2268956760434728
LOSS train 0.25909031675023547 valid 0.2267186883228277
LOSS train 0.25909031675023547 valid 0.2268328626047481
LOSS train 0.25909031675023547 valid 0.2268524089167195
LOSS train 0.25909031675023547 valid 0.22687572107101098
LOSS train 0.25909031675023547 valid 0.2267857025003737
LOSS train 0.25909031675023547 valid 0.22684738756735115
LOSS train 0.25909031675023547 valid 0.2270774082192835
LOSS train 0.25909031675023547 valid 0.2273607974871993
LOSS train 0.25909031675023547 valid 0.22724898853657408
LOSS train 0.25909031675023547 valid 0.22734157455924117
LOSS train 0.25909031675023547 valid 0.22714873691278
LOSS train 0.25909031675023547 valid 0.2272093097066007
LOSS train 0.25909031675023547 valid 0.22733565388303814
LOSS train 0.25909031675023547 valid 0.2273057850908084
LOSS train 0.25909031675023547 valid 0.227306406565769
LOSS train 0.25909031675023547 valid 0.22718471191113904
LOSS train 0.25909031675023547 valid 0.22698649235025664
LOSS train 0.25909031675023547 valid 0.22704431773985134
LOSS train 0.25909031675023547 valid 0.22689625673126756
LOSS train 0.25909031675023547 valid 0.226937523241653
LOSS train 0.25909031675023547 valid 0.2268062912659838
LOSS train 0.25909031675023547 valid 0.22717830658644095
LOSS train 0.25909031675023547 valid 0.22713228745119912
LOSS train 0.25909031675023547 valid 0.22704898168078877
LOSS train 0.25909031675023547 valid 0.2271005566005653
LOSS train 0.25909031675023547 valid 0.22711922646908278
LOSS train 0.25909031675023547 valid 0.22736166875455632
LOSS train 0.25909031675023547 valid 0.22726331022050644
LOSS train 0.25909031675023547 valid 0.22717624318204532
LOSS train 0.25909031675023547 valid 0.22699663837204923
LOSS train 0.25909031675023547 valid 0.22711210148256333
LOSS train 0.25909031675023547 valid 0.22703079709216303
LOSS train 0.25909031675023547 valid 0.22689670429036424
LOSS train 0.25909031675023547 valid 0.2269420025329436
LOSS train 0.25909031675023547 valid 0.2267957871610468
LOSS train 0.25909031675023547 valid 0.226813276397421
LOSS train 0.25909031675023547 valid 0.22657238049481912
LOSS train 0.25909031675023547 valid 0.22654984907100076
LOSS train 0.25909031675023547 valid 0.22669236588228436
LOSS train 0.25909031675023547 valid 0.22672744506659606
LOSS train 0.25909031675023547 valid 0.22650147171526994
LOSS train 0.25909031675023547 valid 0.2263862410771478
LOSS train 0.25909031675023547 valid 0.22629046684656387
LOSS train 0.25909031675023547 valid 0.2264324207695163
LOSS train 0.25909031675023547 valid 0.22636682627164773
LOSS train 0.25909031675023547 valid 0.22657057477368248
LOSS train 0.25909031675023547 valid 0.2265679052277426
LOSS train 0.25909031675023547 valid 0.2267382239550352
LOSS train 0.25909031675023547 valid 0.22655609368684873
LOSS train 0.25909031675023547 valid 0.22652189492589175
LOSS train 0.25909031675023547 valid 0.2266103874873645
LOSS train 0.25909031675023547 valid 0.22652341235502094
LOSS train 0.25909031675023547 valid 0.22643706020785542
LOSS train 0.25909031675023547 valid 0.22639877153831778
LOSS train 0.25909031675023547 valid 0.22635922138241754
LOSS train 0.25909031675023547 valid 0.22627064055548265
LOSS train 0.25909031675023547 valid 0.22629946955083088
LOSS train 0.25909031675023547 valid 0.2264489500295548
LOSS train 0.25909031675023547 valid 0.22664147092832773
LOSS train 0.25909031675023547 valid 0.2264372521554524
LOSS train 0.25909031675023547 valid 0.22633686444848916
LOSS train 0.25909031675023547 valid 0.22625754453311456
LOSS train 0.25909031675023547 valid 0.22600134881429895
LOSS train 0.25909031675023547 valid 0.22576906200912264
LOSS train 0.25909031675023547 valid 0.22552657882738772
LOSS train 0.25909031675023547 valid 0.22547060634018085
LOSS train 0.25909031675023547 valid 0.22544812625401642
LOSS train 0.25909031675023547 valid 0.22552335512909022
LOSS train 0.25909031675023547 valid 0.22551651595674488
LOSS train 0.25909031675023547 valid 0.22563778461368234
LOSS train 0.25909031675023547 valid 0.2257655660534119
LOSS train 0.25909031675023547 valid 0.22568630373903684
LOSS train 0.25909031675023547 valid 0.225544403857655
LOSS train 0.25909031675023547 valid 0.22537363346962802
LOSS train 0.25909031675023547 valid 0.22540850202155008
LOSS train 0.25909031675023547 valid 0.22534006108578883
LOSS train 0.25909031675023547 valid 0.22523096776425058
LOSS train 0.25909031675023547 valid 0.22529111899759457
LOSS train 0.25909031675023547 valid 0.2252594842797234
LOSS train 0.25909031675023547 valid 0.22540216006595512
LOSS train 0.25909031675023547 valid 0.22579652953557192
LOSS train 0.25909031675023547 valid 0.22576401178908145
LOSS train 0.25909031675023547 valid 0.2257857566818278
LOSS train 0.25909031675023547 valid 0.225651043388298
LOSS train 0.25909031675023547 valid 0.22578250663944438
LOSS train 0.25909031675023547 valid 0.22574999383768113
LOSS train 0.25909031675023547 valid 0.22575587208799738
LOSS train 0.25909031675023547 valid 0.22539101025710503
LOSS train 0.25909031675023547 valid 0.2252616566370137
LOSS train 0.25909031675023547 valid 0.22517917028143386
LOSS train 0.25909031675023547 valid 0.2250573029856623
LOSS train 0.25909031675023547 valid 0.22500477450304343
LOSS train 0.25909031675023547 valid 0.22521539342646696
LOSS train 0.25909031675023547 valid 0.22527062777823548
LOSS train 0.25909031675023547 valid 0.22529234776371404
LOSS train 0.25909031675023547 valid 0.22525756007000322
LOSS train 0.25909031675023547 valid 0.2250792877860816
LOSS train 0.25909031675023547 valid 0.2252245256304741
LOSS train 0.25909031675023547 valid 0.22526302524059416
LOSS train 0.25909031675023547 valid 0.22518092760490993
LOSS train 0.25909031675023547 valid 0.2249899879392428
LOSS train 0.25909031675023547 valid 0.2251228609774995
LOSS train 0.25909031675023547 valid 0.22501661795027117
LOSS train 0.25909031675023547 valid 0.2247794937575236
LOSS train 0.25909031675023547 valid 0.2247831141902315
LOSS train 0.25909031675023547 valid 0.22497935304346012
LOSS train 0.25909031675023547 valid 0.224920471375053
LOSS train 0.25909031675023547 valid 0.224757708551792
LOSS train 0.25909031675023547 valid 0.22476373846960251
LOSS train 0.25909031675023547 valid 0.22491294498207004
LOSS train 0.25909031675023547 valid 0.22475662260907683
LOSS train 0.25909031675023547 valid 0.22482573274861684
LOSS train 0.25909031675023547 valid 0.22472611913141213
LOSS train 0.25909031675023547 valid 0.22477727198511138
LOSS train 0.25909031675023547 valid 0.22466880507237008
LOSS train 0.25909031675023547 valid 0.22474311103126896
LOSS train 0.25909031675023547 valid 0.22487082988799284
LOSS train 0.25909031675023547 valid 0.22470552844029887
LOSS train 0.25909031675023547 valid 0.22467753252639983
LOSS train 0.25909031675023547 valid 0.2245837975950802
LOSS train 0.25909031675023547 valid 0.22450382761902862
LOSS train 0.25909031675023547 valid 0.2244037221165469
LOSS train 0.25909031675023547 valid 0.22448397945274007
LOSS train 0.25909031675023547 valid 0.22451570019989775
LOSS train 0.25909031675023547 valid 0.22452133689546414
LOSS train 0.25909031675023547 valid 0.22450295535566137
LOSS train 0.25909031675023547 valid 0.2244818196181328
LOSS train 0.25909031675023547 valid 0.22439230430339063
LOSS train 0.25909031675023547 valid 0.22423644575996332
LOSS train 0.25909031675023547 valid 0.22420614656615764
LOSS train 0.25909031675023547 valid 0.22419150931885723
LOSS train 0.25909031675023547 valid 0.22416978429110956
LOSS train 0.25909031675023547 valid 0.22413303350147448
LOSS train 0.25909031675023547 valid 0.22407488498862807
LOSS train 0.25909031675023547 valid 0.2241139625319205
LOSS train 0.25909031675023547 valid 0.22408908864276278
LOSS train 0.25909031675023547 valid 0.22404022550912878
LOSS train 0.25909031675023547 valid 0.22390984933951805
LOSS train 0.25909031675023547 valid 0.22388907539885478
LOSS train 0.25909031675023547 valid 0.2238641551708522
LOSS train 0.25909031675023547 valid 0.223824104300538
LOSS train 0.25909031675023547 valid 0.22387806974908933
LOSS train 0.25909031675023547 valid 0.22391169035838823
LOSS train 0.25909031675023547 valid 0.22385712884165146
LOSS train 0.25909031675023547 valid 0.2237635893552793
LOSS train 0.25909031675023547 valid 0.22377655500133567
LOSS train 0.25909031675023547 valid 0.2237526235173777
LOSS train 0.25909031675023547 valid 0.22368033493558565
LOSS train 0.25909031675023547 valid 0.2235634414559583
LOSS train 0.25909031675023547 valid 0.223522323498268
LOSS train 0.25909031675023547 valid 0.2234879338210172
LOSS train 0.25909031675023547 valid 0.2234986255827703
LOSS train 0.25909031675023547 valid 0.22346532764982005
LOSS train 0.25909031675023547 valid 0.22372031338464202
LOSS train 0.25909031675023547 valid 0.22375470098921063
LOSS train 0.25909031675023547 valid 0.2238421401420197
LOSS train 0.25909031675023547 valid 0.2238787483532452
LOSS train 0.25909031675023547 valid 0.22391737319769398
LOSS train 0.25909031675023547 valid 0.2239281127882157
LOSS train 0.25909031675023547 valid 0.2239032777456137
LOSS train 0.25909031675023547 valid 0.2238983525254856
LOSS train 0.25909031675023547 valid 0.22379327598650745
LOSS train 0.25909031675023547 valid 0.223742504110412
LOSS train 0.25909031675023547 valid 0.22367576316376275
LOSS train 0.25909031675023547 valid 0.22369244981451367
LOSS train 0.25909031675023547 valid 0.2236812102063647
LOSS train 0.25909031675023547 valid 0.2236807647169944
LOSS train 0.25909031675023547 valid 0.22354528023861348
LOSS train 0.25909031675023547 valid 0.2236440778242836
LOSS train 0.25909031675023547 valid 0.22364192678136116
LOSS train 0.25909031675023547 valid 0.22356089336030624
LOSS train 0.25909031675023547 valid 0.22356724702281716
LOSS train 0.25909031675023547 valid 0.2234600776892442
LOSS train 0.25909031675023547 valid 0.22355571992558204
LOSS train 0.25909031675023547 valid 0.22347509168339066
LOSS train 0.25909031675023547 valid 0.2234383703368466
LOSS train 0.25909031675023547 valid 0.2234220983953099
LOSS train 0.25909031675023547 valid 0.22341033951802688
LOSS train 0.25909031675023547 valid 0.22332302946878704
LOSS train 0.25909031675023547 valid 0.22332448611058384
LOSS train 0.25909031675023547 valid 0.22341441571175516
LOSS train 0.25909031675023547 valid 0.2233253710730347
LOSS train 0.25909031675023547 valid 0.22319601885418394
LOSS train 0.25909031675023547 valid 0.22313874077406667
LOSS train 0.25909031675023547 valid 0.22328860031921716
LOSS train 0.25909031675023547 valid 0.22327753522339658
LOSS train 0.25909031675023547 valid 0.22321527734794447
LOSS train 0.25909031675023547 valid 0.22334810306920724
LOSS train 0.25909031675023547 valid 0.22330551369448912
LOSS train 0.25909031675023547 valid 0.22328897064540817
LOSS train 0.25909031675023547 valid 0.22317647973059218
LOSS train 0.25909031675023547 valid 0.22324521293820337
LOSS train 0.25909031675023547 valid 0.22339274511821028
LOSS train 0.25909031675023547 valid 0.2233591349741627
LOSS train 0.25909031675023547 valid 0.22325035036469054
LOSS train 0.25909031675023547 valid 0.2233628810479723
LOSS train 0.25909031675023547 valid 0.22331538830582257
LOSS train 0.25909031675023547 valid 0.2233323440381459
LOSS train 0.25909031675023547 valid 0.2234241572710184
LOSS train 0.25909031675023547 valid 0.22347563522105868
LOSS train 0.25909031675023547 valid 0.22351608938265793
LOSS train 0.25909031675023547 valid 0.22356606295890052
LOSS train 0.25909031675023547 valid 0.22341469962831953
LOSS train 0.25909031675023547 valid 0.2234395303334413
LOSS train 0.25909031675023547 valid 0.22347855922721682
LOSS train 0.25909031675023547 valid 0.22342314887479697
LOSS train 0.25909031675023547 valid 0.2233394906132334
LOSS train 0.25909031675023547 valid 0.22348494484192796
LOSS train 0.25909031675023547 valid 0.22341559765411545
LOSS train 0.25909031675023547 valid 0.22348004612474812
LOSS train 0.25909031675023547 valid 0.22355147908870182
LOSS train 0.25909031675023547 valid 0.22346453585631246
LOSS train 0.25909031675023547 valid 0.2235875424865174
LOSS train 0.25909031675023547 valid 0.22357120010696474
LOSS train 0.25909031675023547 valid 0.22350411778899565
LOSS train 0.25909031675023547 valid 0.22338747046887875
LOSS train 0.25909031675023547 valid 0.22334096146631371
EPOCH 14:
  batch 1 loss: 0.25705137848854065
  batch 2 loss: 0.2756028473377228
  batch 3 loss: 0.265512357155482
  batch 4 loss: 0.2629779875278473
  batch 5 loss: 0.26703397631645204
  batch 6 loss: 0.26433029274145764
  batch 7 loss: 0.2637557472501482
  batch 8 loss: 0.26148558780550957
  batch 9 loss: 0.26264848974015975
  batch 10 loss: 0.25911651104688643
  batch 11 loss: 0.25936218147928064
  batch 12 loss: 0.25939304505785304
  batch 13 loss: 0.25681762397289276
  batch 14 loss: 0.2549290263227054
  batch 15 loss: 0.256448154648145
  batch 16 loss: 0.2561678746715188
  batch 17 loss: 0.25901181820560903
  batch 18 loss: 0.25681186881330276
  batch 19 loss: 0.2561812730211961
  batch 20 loss: 0.25502075254917145
  batch 21 loss: 0.2579845402921949
  batch 22 loss: 0.258477891033346
  batch 23 loss: 0.2574944107428841
  batch 24 loss: 0.25824082146088284
  batch 25 loss: 0.25903923869132994
  batch 26 loss: 0.258699710552509
  batch 27 loss: 0.25855214507491503
  batch 28 loss: 0.25934920672859463
  batch 29 loss: 0.2593596865390909
  batch 30 loss: 0.2604373753070831
  batch 31 loss: 0.2606268669328382
  batch 32 loss: 0.26165315601974726
  batch 33 loss: 0.2634549511201454
  batch 34 loss: 0.2640766071922639
  batch 35 loss: 0.26578147837093896
  batch 36 loss: 0.265555443863074
  batch 37 loss: 0.2656601594912039
  batch 38 loss: 0.2666305930990922
  batch 39 loss: 0.2673981610016945
  batch 40 loss: 0.2673309579491615
  batch 41 loss: 0.26678669707077307
  batch 42 loss: 0.2662591203337624
  batch 43 loss: 0.2670460343360901
  batch 44 loss: 0.26757122576236725
  batch 45 loss: 0.26672650112046137
  batch 46 loss: 0.26534826438064163
  batch 47 loss: 0.2655348406827196
  batch 48 loss: 0.2653939925755064
  batch 49 loss: 0.2657858172849733
  batch 50 loss: 0.26708916932344434
  batch 51 loss: 0.2673119198457867
  batch 52 loss: 0.2673341795228995
  batch 53 loss: 0.26689224507448805
  batch 54 loss: 0.2670851954155498
  batch 55 loss: 0.26745308610526
  batch 56 loss: 0.2677007341491325
  batch 57 loss: 0.26707197281352263
  batch 58 loss: 0.26663905459231346
  batch 59 loss: 0.2669586607460248
  batch 60 loss: 0.26717951372265814
  batch 61 loss: 0.26758785350400893
  batch 62 loss: 0.26760629756796744
  batch 63 loss: 0.26679473878845333
  batch 64 loss: 0.266234184615314
  batch 65 loss: 0.2657860570229017
  batch 66 loss: 0.2660778207760869
  batch 67 loss: 0.266212960454955
  batch 68 loss: 0.2661556134767392
  batch 69 loss: 0.26608924870041833
  batch 70 loss: 0.2665075325540134
  batch 71 loss: 0.26667929010491975
  batch 72 loss: 0.26666897845764953
  batch 73 loss: 0.2666815158030758
  batch 74 loss: 0.2666882125509752
  batch 75 loss: 0.26681197226047515
  batch 76 loss: 0.2669791138093722
  batch 77 loss: 0.26678395948626776
  batch 78 loss: 0.26689028644408935
  batch 79 loss: 0.2668450816145426
  batch 80 loss: 0.2668171161785722
  batch 81 loss: 0.2667490219996299
  batch 82 loss: 0.2668674441009033
  batch 83 loss: 0.26645040206880455
  batch 84 loss: 0.26571837315956753
  batch 85 loss: 0.2660057825200698
  batch 86 loss: 0.2658780192913011
  batch 87 loss: 0.26578697493706627
  batch 88 loss: 0.265585719184442
  batch 89 loss: 0.26525082548012896
  batch 90 loss: 0.26540423896577625
  batch 91 loss: 0.2652718595096043
  batch 92 loss: 0.2650263374266417
  batch 93 loss: 0.26470484816899864
  batch 94 loss: 0.26441145878522954
  batch 95 loss: 0.26422139092495567
  batch 96 loss: 0.2642548990746339
  batch 97 loss: 0.2646506806624304
  batch 98 loss: 0.26441012170850015
  batch 99 loss: 0.2639542818069458
  batch 100 loss: 0.26424752175807953
  batch 101 loss: 0.26403754536468205
  batch 102 loss: 0.26418249367498886
  batch 103 loss: 0.2639140713851429
  batch 104 loss: 0.26428003480228096
  batch 105 loss: 0.2640296127114977
  batch 106 loss: 0.26396842385238073
  batch 107 loss: 0.26393493536476775
  batch 108 loss: 0.2634877234145447
  batch 109 loss: 0.26359507386837533
  batch 110 loss: 0.26349648318507457
  batch 111 loss: 0.2634566004748817
  batch 112 loss: 0.263206889187651
  batch 113 loss: 0.26297646127970875
  batch 114 loss: 0.2628664369123024
  batch 115 loss: 0.26249212493067203
  batch 116 loss: 0.26212954957937373
  batch 117 loss: 0.26191852795772064
  batch 118 loss: 0.2620026284860352
  batch 119 loss: 0.26200268924737175
  batch 120 loss: 0.26174448654055593
  batch 121 loss: 0.26169309069302454
  batch 122 loss: 0.26141969090113876
  batch 123 loss: 0.26108553545261787
  batch 124 loss: 0.26118608347831235
  batch 125 loss: 0.26075553941726687
  batch 126 loss: 0.26078461583644624
  batch 127 loss: 0.26091268118910904
  batch 128 loss: 0.26090024085715413
  batch 129 loss: 0.2610576314519542
  batch 130 loss: 0.2609111806521049
  batch 131 loss: 0.2607895793350598
  batch 132 loss: 0.26098614863374014
  batch 133 loss: 0.2608331343285123
  batch 134 loss: 0.2607054127686059
  batch 135 loss: 0.26056473641483874
  batch 136 loss: 0.26067871501778855
  batch 137 loss: 0.2611176370486726
  batch 138 loss: 0.26108346479958383
  batch 139 loss: 0.2609557092404194
  batch 140 loss: 0.2612228937447071
  batch 141 loss: 0.2609495336705066
  batch 142 loss: 0.2606146817266102
  batch 143 loss: 0.2607900169971106
  batch 144 loss: 0.26066879607323146
  batch 145 loss: 0.260650134600442
  batch 146 loss: 0.2607848034941987
  batch 147 loss: 0.2606211382718313
  batch 148 loss: 0.2606050767809958
  batch 149 loss: 0.2601530438901594
  batch 150 loss: 0.2602028182148933
  batch 151 loss: 0.2598327247314895
  batch 152 loss: 0.25962554339907673
  batch 153 loss: 0.25976517475118827
  batch 154 loss: 0.2596051334947735
  batch 155 loss: 0.2595387921217949
  batch 156 loss: 0.2593796548361962
  batch 157 loss: 0.25927023475716826
  batch 158 loss: 0.25934229649697677
  batch 159 loss: 0.2595744934284462
  batch 160 loss: 0.25960709964856504
  batch 161 loss: 0.25934155914724244
  batch 162 loss: 0.2593135420738915
  batch 163 loss: 0.25910784121305663
  batch 164 loss: 0.25940036946317047
  batch 165 loss: 0.2596497525771459
  batch 166 loss: 0.259898078280041
  batch 167 loss: 0.2598062226336873
  batch 168 loss: 0.25958174121166977
  batch 169 loss: 0.25957817465243255
  batch 170 loss: 0.2595618879970382
  batch 171 loss: 0.25963565332499167
  batch 172 loss: 0.25956792176462884
  batch 173 loss: 0.25953215257280826
  batch 174 loss: 0.25974165862319115
  batch 175 loss: 0.25982234307697843
  batch 176 loss: 0.26005769097669557
  batch 177 loss: 0.26024457239835275
  batch 178 loss: 0.26018805366553616
  batch 179 loss: 0.260151669965776
  batch 180 loss: 0.26009520275725256
  batch 181 loss: 0.2600447817072684
  batch 182 loss: 0.26010215970186085
  batch 183 loss: 0.26002360383669537
  batch 184 loss: 0.2599270909862674
  batch 185 loss: 0.2596941173882098
  batch 186 loss: 0.259675242968144
  batch 187 loss: 0.2597838827473595
  batch 188 loss: 0.25966956648737827
  batch 189 loss: 0.25959609961383556
  batch 190 loss: 0.2593875836384924
  batch 191 loss: 0.25945123640030465
  batch 192 loss: 0.2592656298850973
  batch 193 loss: 0.2592473355908468
  batch 194 loss: 0.2590443842189828
  batch 195 loss: 0.2592061820702675
  batch 196 loss: 0.259053034973996
  batch 197 loss: 0.2591346350266849
  batch 198 loss: 0.2590569059054057
  batch 199 loss: 0.2590771215644913
  batch 200 loss: 0.2590711463987827
  batch 201 loss: 0.2590144831149732
  batch 202 loss: 0.2591291592262759
  batch 203 loss: 0.25918093309026635
  batch 204 loss: 0.2593440246932647
  batch 205 loss: 0.2592096061968222
  batch 206 loss: 0.25932342399960584
  batch 207 loss: 0.25929326589268764
  batch 208 loss: 0.2590895952083744
  batch 209 loss: 0.2591488487555079
  batch 210 loss: 0.25914247213375
  batch 211 loss: 0.25904646981948926
  batch 212 loss: 0.25895109210374223
  batch 213 loss: 0.25870381134776443
  batch 214 loss: 0.25857895431674527
  batch 215 loss: 0.2585298482068749
  batch 216 loss: 0.25845581120638933
  batch 217 loss: 0.25832446496332845
  batch 218 loss: 0.2581216979327552
  batch 219 loss: 0.2578394504850858
  batch 220 loss: 0.25807484977624634
  batch 221 loss: 0.2580107584663106
  batch 222 loss: 0.2580619688640844
  batch 223 loss: 0.2579395426228442
  batch 224 loss: 0.2579370747719492
  batch 225 loss: 0.2579702235592736
  batch 226 loss: 0.257974480237581
  batch 227 loss: 0.2580208679915525
  batch 228 loss: 0.2580919018701503
  batch 229 loss: 0.25809475714462815
  batch 230 loss: 0.25805446829484857
  batch 231 loss: 0.2580352671992727
  batch 232 loss: 0.25794174926805086
  batch 233 loss: 0.25789192095079133
  batch 234 loss: 0.2578103331546498
  batch 235 loss: 0.2577626034300378
  batch 236 loss: 0.2577031042616246
  batch 237 loss: 0.25750665398086675
  batch 238 loss: 0.25737883951984536
  batch 239 loss: 0.2574206528304511
  batch 240 loss: 0.2575118200232585
  batch 241 loss: 0.25733342593636255
  batch 242 loss: 0.2571642753506495
  batch 243 loss: 0.2571874089201782
  batch 244 loss: 0.25708389776896257
  batch 245 loss: 0.2570445585615781
  batch 246 loss: 0.2569179359853752
  batch 247 loss: 0.25686061128913634
  batch 248 loss: 0.2568887992491645
  batch 249 loss: 0.2569069345313382
  batch 250 loss: 0.25691390216350557
  batch 251 loss: 0.2567317535677754
  batch 252 loss: 0.2568113762471411
  batch 253 loss: 0.25667271206501446
  batch 254 loss: 0.25658262380230146
  batch 255 loss: 0.2563992139171152
  batch 256 loss: 0.25631852727383375
  batch 257 loss: 0.25629808385548425
  batch 258 loss: 0.2562758394683054
  batch 259 loss: 0.2560560859193213
  batch 260 loss: 0.25612233229554615
  batch 261 loss: 0.256117122783058
  batch 262 loss: 0.25602484983569795
  batch 263 loss: 0.2559396333907494
  batch 264 loss: 0.25599840469658375
  batch 265 loss: 0.2558641924610678
  batch 266 loss: 0.25569087418174385
  batch 267 loss: 0.25554333828138504
  batch 268 loss: 0.2553951913185084
  batch 269 loss: 0.25540325815571285
  batch 270 loss: 0.25528920210070083
  batch 271 loss: 0.25521559529418875
  batch 272 loss: 0.25513260504778695
  batch 273 loss: 0.2548978138210136
  batch 274 loss: 0.25472533039367984
  batch 275 loss: 0.2546965053948489
  batch 276 loss: 0.254624569437642
  batch 277 loss: 0.2545819534829377
  batch 278 loss: 0.25446289327504823
  batch 279 loss: 0.2544666415474321
  batch 280 loss: 0.2543984474880355
  batch 281 loss: 0.2543366321783473
  batch 282 loss: 0.25417947684619446
  batch 283 loss: 0.25414446133185614
  batch 284 loss: 0.25408544590775395
  batch 285 loss: 0.2539942535392025
  batch 286 loss: 0.2539583430527807
  batch 287 loss: 0.25378223775986597
  batch 288 loss: 0.2537491109946536
  batch 289 loss: 0.2535901051079113
  batch 290 loss: 0.25345056257371246
  batch 291 loss: 0.2534046332348663
  batch 292 loss: 0.25327420341846063
  batch 293 loss: 0.2532884871695229
  batch 294 loss: 0.25301812345884284
  batch 295 loss: 0.2530178825734025
  batch 296 loss: 0.2531241454385422
  batch 297 loss: 0.2531116844628395
  batch 298 loss: 0.2530901510543471
  batch 299 loss: 0.2531436288536193
  batch 300 loss: 0.25298013965288796
  batch 301 loss: 0.2529880652990056
  batch 302 loss: 0.2531678916405368
  batch 303 loss: 0.2531443182686374
  batch 304 loss: 0.25309014737017843
  batch 305 loss: 0.25305448159819743
  batch 306 loss: 0.25313112251704034
  batch 307 loss: 0.25307061545041176
  batch 308 loss: 0.2531280146500507
  batch 309 loss: 0.2531137628462708
  batch 310 loss: 0.25318477961324876
  batch 311 loss: 0.2531176829453067
  batch 312 loss: 0.253080465281621
  batch 313 loss: 0.2530688304489794
  batch 314 loss: 0.2530357345084476
  batch 315 loss: 0.2529834875038692
  batch 316 loss: 0.25293674343560313
  batch 317 loss: 0.25301559598475976
  batch 318 loss: 0.2527982727536615
  batch 319 loss: 0.2525802574654732
  batch 320 loss: 0.2527133632916957
  batch 321 loss: 0.2527726651444985
  batch 322 loss: 0.25268964455549764
  batch 323 loss: 0.25272944398893293
  batch 324 loss: 0.25290012336623524
  batch 325 loss: 0.253084322810173
  batch 326 loss: 0.2531761768862514
  batch 327 loss: 0.2530809248988417
  batch 328 loss: 0.2530464359172961
  batch 329 loss: 0.25313584139644196
  batch 330 loss: 0.25313306495998844
  batch 331 loss: 0.2531184750381193
  batch 332 loss: 0.25326443840581253
  batch 333 loss: 0.2532684786004705
  batch 334 loss: 0.25354893862487315
  batch 335 loss: 0.2536093198541385
  batch 336 loss: 0.2536988730231921
  batch 337 loss: 0.2537768194866463
  batch 338 loss: 0.2537338110293157
  batch 339 loss: 0.2537297134378315
  batch 340 loss: 0.2536306979463381
  batch 341 loss: 0.2536236736868833
  batch 342 loss: 0.25358832292040884
  batch 343 loss: 0.2536677314136883
  batch 344 loss: 0.25373757396673047
  batch 345 loss: 0.25387825585793755
  batch 346 loss: 0.25387438589093314
  batch 347 loss: 0.2537198308908974
  batch 348 loss: 0.2536985353063578
  batch 349 loss: 0.25379163808159977
  batch 350 loss: 0.253882974258491
  batch 351 loss: 0.25383972183421805
  batch 352 loss: 0.2537238580090078
  batch 353 loss: 0.2537267619253218
  batch 354 loss: 0.2536684529761137
  batch 355 loss: 0.25357078519505516
  batch 356 loss: 0.25362041462840657
  batch 357 loss: 0.253434105079715
  batch 358 loss: 0.25330863268681747
  batch 359 loss: 0.25325834771882855
  batch 360 loss: 0.25319562550220226
  batch 361 loss: 0.25311382676260624
  batch 362 loss: 0.25311497919961234
  batch 363 loss: 0.25316484091859875
  batch 364 loss: 0.2530484330523145
  batch 365 loss: 0.25306137881866875
  batch 366 loss: 0.2530786143463166
  batch 367 loss: 0.2531165103821404
  batch 368 loss: 0.25310179157911433
  batch 369 loss: 0.25300687608033984
  batch 370 loss: 0.25304192159626937
  batch 371 loss: 0.25308154286400003
  batch 372 loss: 0.2531913248922235
  batch 373 loss: 0.2532340348086472
  batch 374 loss: 0.25317107137033645
  batch 375 loss: 0.25325179811318715
  batch 376 loss: 0.2534046949462054
  batch 377 loss: 0.2534250343114691
  batch 378 loss: 0.2534216137395965
  batch 379 loss: 0.2534274914569779
  batch 380 loss: 0.2533722460269928
  batch 381 loss: 0.25331194452413425
  batch 382 loss: 0.2533149239436494
  batch 383 loss: 0.25322907338254447
  batch 384 loss: 0.2532308535495152
  batch 385 loss: 0.2531998961002796
  batch 386 loss: 0.2531762354732177
  batch 387 loss: 0.2531399696834328
  batch 388 loss: 0.25300444643368425
  batch 389 loss: 0.25299289205846565
  batch 390 loss: 0.25305102119843165
  batch 391 loss: 0.25291967731150217
  batch 392 loss: 0.25302483510149987
  batch 393 loss: 0.25305837540681125
  batch 394 loss: 0.25305419369851273
  batch 395 loss: 0.25304171906996376
  batch 396 loss: 0.2531508437988132
  batch 397 loss: 0.2531924073957376
  batch 398 loss: 0.2532871928271936
  batch 399 loss: 0.2532674138035093
  batch 400 loss: 0.25340081091970207
  batch 401 loss: 0.2534500515446104
  batch 402 loss: 0.2534477228474854
  batch 403 loss: 0.2533892603503268
  batch 404 loss: 0.2533477921739663
  batch 405 loss: 0.25333103199063994
  batch 406 loss: 0.253230214265767
  batch 407 loss: 0.2532545212445739
  batch 408 loss: 0.2531905377320215
  batch 409 loss: 0.25322253604972855
  batch 410 loss: 0.25306423088399377
  batch 411 loss: 0.2530420621236165
  batch 412 loss: 0.2531592754773723
  batch 413 loss: 0.25312973807856765
  batch 414 loss: 0.2531671123804102
  batch 415 loss: 0.2531049215649984
  batch 416 loss: 0.25318377644110185
  batch 417 loss: 0.2531676638683827
  batch 418 loss: 0.2531837447051797
  batch 419 loss: 0.2531969591014994
  batch 420 loss: 0.2532169507727737
  batch 421 loss: 0.2533282483629263
  batch 422 loss: 0.2532700806220561
  batch 423 loss: 0.2532353218354232
  batch 424 loss: 0.2532262272500205
  batch 425 loss: 0.25311825580456676
  batch 426 loss: 0.2530910135574744
  batch 427 loss: 0.253093639009172
  batch 428 loss: 0.25312222366299586
  batch 429 loss: 0.2530034806900647
  batch 430 loss: 0.25298737616733064
  batch 431 loss: 0.2529938392116409
  batch 432 loss: 0.2530189650823121
  batch 433 loss: 0.25300497515234605
  batch 434 loss: 0.2529169062178256
  batch 435 loss: 0.2529610826366249
  batch 436 loss: 0.2529516134992105
  batch 437 loss: 0.25287919544382553
  batch 438 loss: 0.2528077737177344
  batch 439 loss: 0.2527200023989471
  batch 440 loss: 0.2527806660329754
  batch 441 loss: 0.25282730810901743
  batch 442 loss: 0.25284844384069355
  batch 443 loss: 0.25283592666917676
  batch 444 loss: 0.25282862933503614
  batch 445 loss: 0.25275241163339507
  batch 446 loss: 0.2527349759644992
  batch 447 loss: 0.25270564484116215
  batch 448 loss: 0.25278387285236803
  batch 449 loss: 0.25273228026843547
  batch 450 loss: 0.2527170691556401
  batch 451 loss: 0.2526676614506546
  batch 452 loss: 0.25260568726642996
  batch 453 loss: 0.25259397516055876
  batch 454 loss: 0.2525490060579934
  batch 455 loss: 0.2525262768779482
  batch 456 loss: 0.25246802746857466
  batch 457 loss: 0.252451063245861
  batch 458 loss: 0.252447704105398
  batch 459 loss: 0.25238507206923044
  batch 460 loss: 0.25232540906771367
  batch 461 loss: 0.2523677279918158
  batch 462 loss: 0.2524070781675768
  batch 463 loss: 0.2524510191403506
  batch 464 loss: 0.25243068380474015
  batch 465 loss: 0.2524344622127471
  batch 466 loss: 0.25232735765466363
  batch 467 loss: 0.2524451877945751
  batch 468 loss: 0.2524084998692712
  batch 469 loss: 0.2523503051256574
  batch 470 loss: 0.25239525925605855
  batch 471 loss: 0.252432109697103
  batch 472 loss: 0.2522798834715859
LOSS train 0.2522798834715859 valid 0.2261996865272522
LOSS train 0.2522798834715859 valid 0.2213178500533104
LOSS train 0.2522798834715859 valid 0.22760054469108582
LOSS train 0.2522798834715859 valid 0.2107204794883728
LOSS train 0.2522798834715859 valid 0.21244096159934997
LOSS train 0.2522798834715859 valid 0.2166014090180397
LOSS train 0.2522798834715859 valid 0.2120314644915717
LOSS train 0.2522798834715859 valid 0.2103411853313446
LOSS train 0.2522798834715859 valid 0.20938338504897225
LOSS train 0.2522798834715859 valid 0.20760132372379303
LOSS train 0.2522798834715859 valid 0.207532367923043
LOSS train 0.2522798834715859 valid 0.211012481401364
LOSS train 0.2522798834715859 valid 0.21168949053837702
LOSS train 0.2522798834715859 valid 0.21006135216781072
LOSS train 0.2522798834715859 valid 0.2099562664826711
LOSS train 0.2522798834715859 valid 0.2132017258554697
LOSS train 0.2522798834715859 valid 0.2151462716214797
LOSS train 0.2522798834715859 valid 0.21510238614347246
LOSS train 0.2522798834715859 valid 0.21731588087583842
LOSS train 0.2522798834715859 valid 0.21649253368377686
LOSS train 0.2522798834715859 valid 0.21764965142522538
LOSS train 0.2522798834715859 valid 0.21723698689178986
LOSS train 0.2522798834715859 valid 0.2151061458432156
LOSS train 0.2522798834715859 valid 0.2151562925428152
LOSS train 0.2522798834715859 valid 0.21528011977672576
LOSS train 0.2522798834715859 valid 0.21490979595826223
LOSS train 0.2522798834715859 valid 0.2145041525363922
LOSS train 0.2522798834715859 valid 0.2143383930836405
LOSS train 0.2522798834715859 valid 0.21296079251272926
LOSS train 0.2522798834715859 valid 0.21240981022516886
LOSS train 0.2522798834715859 valid 0.21269181803349527
LOSS train 0.2522798834715859 valid 0.21279140748083591
LOSS train 0.2522798834715859 valid 0.21200233697891235
LOSS train 0.2522798834715859 valid 0.21183172394247615
LOSS train 0.2522798834715859 valid 0.2122383726494653
LOSS train 0.2522798834715859 valid 0.21272096948491204
LOSS train 0.2522798834715859 valid 0.2130240475809252
LOSS train 0.2522798834715859 valid 0.2131006384366437
LOSS train 0.2522798834715859 valid 0.21415554445523483
LOSS train 0.2522798834715859 valid 0.2145670510828495
LOSS train 0.2522798834715859 valid 0.21432261641432598
LOSS train 0.2522798834715859 valid 0.21575066731089637
LOSS train 0.2522798834715859 valid 0.2161877862242765
LOSS train 0.2522798834715859 valid 0.21579660881649365
LOSS train 0.2522798834715859 valid 0.21515355540646447
LOSS train 0.2522798834715859 valid 0.21485444223103317
LOSS train 0.2522798834715859 valid 0.21468824782270066
LOSS train 0.2522798834715859 valid 0.21623342484235764
LOSS train 0.2522798834715859 valid 0.21554017036545034
LOSS train 0.2522798834715859 valid 0.21625383883714677
LOSS train 0.2522798834715859 valid 0.21602047395472432
LOSS train 0.2522798834715859 valid 0.2157280298952873
LOSS train 0.2522798834715859 valid 0.21712761808116482
LOSS train 0.2522798834715859 valid 0.217153364861453
LOSS train 0.2522798834715859 valid 0.21705736274069007
LOSS train 0.2522798834715859 valid 0.21703946989561831
LOSS train 0.2522798834715859 valid 0.21655353537777014
LOSS train 0.2522798834715859 valid 0.21729441963393112
LOSS train 0.2522798834715859 valid 0.21705354421825732
LOSS train 0.2522798834715859 valid 0.21683653096357983
LOSS train 0.2522798834715859 valid 0.2169523371047661
LOSS train 0.2522798834715859 valid 0.2166003505549123
LOSS train 0.2522798834715859 valid 0.21635222103860643
LOSS train 0.2522798834715859 valid 0.21675373520702124
LOSS train 0.2522798834715859 valid 0.21574809711713058
LOSS train 0.2522798834715859 valid 0.21560352408524716
LOSS train 0.2522798834715859 valid 0.21638956443587345
LOSS train 0.2522798834715859 valid 0.2157514969653943
LOSS train 0.2522798834715859 valid 0.2163793788008068
LOSS train 0.2522798834715859 valid 0.21690158993005754
LOSS train 0.2522798834715859 valid 0.21726207393155972
LOSS train 0.2522798834715859 valid 0.21793476761215264
LOSS train 0.2522798834715859 valid 0.21857424366147551
LOSS train 0.2522798834715859 valid 0.2180936994987565
LOSS train 0.2522798834715859 valid 0.2176873751481374
LOSS train 0.2522798834715859 valid 0.21790910296534238
LOSS train 0.2522798834715859 valid 0.21779325972129773
LOSS train 0.2522798834715859 valid 0.21760504711896944
LOSS train 0.2522798834715859 valid 0.21759572681746905
LOSS train 0.2522798834715859 valid 0.21745666339993477
LOSS train 0.2522798834715859 valid 0.21767355446462278
LOSS train 0.2522798834715859 valid 0.21760482958904126
LOSS train 0.2522798834715859 valid 0.21777380392493972
LOSS train 0.2522798834715859 valid 0.21757526677988825
LOSS train 0.2522798834715859 valid 0.21840665568323697
LOSS train 0.2522798834715859 valid 0.21830400304738865
LOSS train 0.2522798834715859 valid 0.21803832533715786
LOSS train 0.2522798834715859 valid 0.21843780170787463
LOSS train 0.2522798834715859 valid 0.21890319532222963
LOSS train 0.2522798834715859 valid 0.21922670006752015
LOSS train 0.2522798834715859 valid 0.21927783908424797
LOSS train 0.2522798834715859 valid 0.21927893323742825
LOSS train 0.2522798834715859 valid 0.219099800432882
LOSS train 0.2522798834715859 valid 0.2192792183858283
LOSS train 0.2522798834715859 valid 0.21955346872932033
LOSS train 0.2522798834715859 valid 0.21972937198976675
LOSS train 0.2522798834715859 valid 0.2197778214191653
LOSS train 0.2522798834715859 valid 0.22030808989490783
LOSS train 0.2522798834715859 valid 0.22051424179414306
LOSS train 0.2522798834715859 valid 0.22068624705076217
LOSS train 0.2522798834715859 valid 0.2207727063410353
LOSS train 0.2522798834715859 valid 0.2211601491652283
LOSS train 0.2522798834715859 valid 0.22101270573810466
LOSS train 0.2522798834715859 valid 0.22088347447033113
LOSS train 0.2522798834715859 valid 0.22110765618937356
LOSS train 0.2522798834715859 valid 0.22116619854603173
LOSS train 0.2522798834715859 valid 0.2208250485290991
LOSS train 0.2522798834715859 valid 0.22097049322393206
LOSS train 0.2522798834715859 valid 0.22054405031947916
LOSS train 0.2522798834715859 valid 0.22047556638717652
LOSS train 0.2522798834715859 valid 0.22068442672759564
LOSS train 0.2522798834715859 valid 0.22096180716263397
LOSS train 0.2522798834715859 valid 0.22066184878349304
LOSS train 0.2522798834715859 valid 0.220808757501736
LOSS train 0.2522798834715859 valid 0.22148927191029424
LOSS train 0.2522798834715859 valid 0.22117582438834782
LOSS train 0.2522798834715859 valid 0.22186231855143848
LOSS train 0.2522798834715859 valid 0.22171469891475418
LOSS train 0.2522798834715859 valid 0.22144600661361918
LOSS train 0.2522798834715859 valid 0.22112299737830957
LOSS train 0.2522798834715859 valid 0.22087817866940143
LOSS train 0.2522798834715859 valid 0.2210486606984842
LOSS train 0.2522798834715859 valid 0.2210703105703602
LOSS train 0.2522798834715859 valid 0.22132449657205613
LOSS train 0.2522798834715859 valid 0.22129432594776152
LOSS train 0.2522798834715859 valid 0.22152596156275461
LOSS train 0.2522798834715859 valid 0.2213975673350762
LOSS train 0.2522798834715859 valid 0.22133527358528227
LOSS train 0.2522798834715859 valid 0.2209825434888056
LOSS train 0.2522798834715859 valid 0.22076609432697297
LOSS train 0.2522798834715859 valid 0.22081570759529376
LOSS train 0.2522798834715859 valid 0.22077516150293927
LOSS train 0.2522798834715859 valid 0.22076792622867383
LOSS train 0.2522798834715859 valid 0.22107983324954758
LOSS train 0.2522798834715859 valid 0.2211561691981775
LOSS train 0.2522798834715859 valid 0.2211697306483984
LOSS train 0.2522798834715859 valid 0.2212333119046079
LOSS train 0.2522798834715859 valid 0.22114846293908963
LOSS train 0.2522798834715859 valid 0.22099337142577274
LOSS train 0.2522798834715859 valid 0.22092762174350875
LOSS train 0.2522798834715859 valid 0.22102657351510743
LOSS train 0.2522798834715859 valid 0.22136483257505254
LOSS train 0.2522798834715859 valid 0.22146859598326515
LOSS train 0.2522798834715859 valid 0.2215116587984893
LOSS train 0.2522798834715859 valid 0.2214750875686777
LOSS train 0.2522798834715859 valid 0.2214127879028451
LOSS train 0.2522798834715859 valid 0.2212963584734469
LOSS train 0.2522798834715859 valid 0.2212712311664143
LOSS train 0.2522798834715859 valid 0.22132867394677744
LOSS train 0.2522798834715859 valid 0.22128596931695937
LOSS train 0.2522798834715859 valid 0.22124410761113197
LOSS train 0.2522798834715859 valid 0.22122647907388837
LOSS train 0.2522798834715859 valid 0.22098317089813208
LOSS train 0.2522798834715859 valid 0.2210631879506173
LOSS train 0.2522798834715859 valid 0.2211335983007185
LOSS train 0.2522798834715859 valid 0.22112736956049234
LOSS train 0.2522798834715859 valid 0.22105722414080503
LOSS train 0.2522798834715859 valid 0.22115669380637665
LOSS train 0.2522798834715859 valid 0.22137272704697256
LOSS train 0.2522798834715859 valid 0.22164215063676238
LOSS train 0.2522798834715859 valid 0.22157596532972704
LOSS train 0.2522798834715859 valid 0.22168992239016075
LOSS train 0.2522798834715859 valid 0.22150245318017855
LOSS train 0.2522798834715859 valid 0.2215292250237814
LOSS train 0.2522798834715859 valid 0.22167367230762136
LOSS train 0.2522798834715859 valid 0.2216377834598702
LOSS train 0.2522798834715859 valid 0.22167424313322512
LOSS train 0.2522798834715859 valid 0.22150399748768126
LOSS train 0.2522798834715859 valid 0.22129555108279167
LOSS train 0.2522798834715859 valid 0.22134616567808038
LOSS train 0.2522798834715859 valid 0.22122196373883743
LOSS train 0.2522798834715859 valid 0.22126872906851214
LOSS train 0.2522798834715859 valid 0.2211290285249666
LOSS train 0.2522798834715859 valid 0.22150848865851588
LOSS train 0.2522798834715859 valid 0.22148044390337807
LOSS train 0.2522798834715859 valid 0.22136331324211575
LOSS train 0.2522798834715859 valid 0.2214797661129364
LOSS train 0.2522798834715859 valid 0.2215064430002416
LOSS train 0.2522798834715859 valid 0.22170819508629805
LOSS train 0.2522798834715859 valid 0.2216066005329291
LOSS train 0.2522798834715859 valid 0.22154323065149192
LOSS train 0.2522798834715859 valid 0.22141739783378747
LOSS train 0.2522798834715859 valid 0.22150244649316445
LOSS train 0.2522798834715859 valid 0.2214229660027701
LOSS train 0.2522798834715859 valid 0.2212864708256077
LOSS train 0.2522798834715859 valid 0.22129637748003006
LOSS train 0.2522798834715859 valid 0.22118610207409783
LOSS train 0.2522798834715859 valid 0.221243291459185
LOSS train 0.2522798834715859 valid 0.22099398234218517
LOSS train 0.2522798834715859 valid 0.22097337724346863
LOSS train 0.2522798834715859 valid 0.22110343720588385
LOSS train 0.2522798834715859 valid 0.22114886716008186
LOSS train 0.2522798834715859 valid 0.22093734423113612
LOSS train 0.2522798834715859 valid 0.22080230774338713
LOSS train 0.2522798834715859 valid 0.2207156118674156
LOSS train 0.2522798834715859 valid 0.22083033058716325
LOSS train 0.2522798834715859 valid 0.2207927057131898
LOSS train 0.2522798834715859 valid 0.2209907587278973
LOSS train 0.2522798834715859 valid 0.220977287271514
LOSS train 0.2522798834715859 valid 0.2211271322518587
LOSS train 0.2522798834715859 valid 0.2209146866454414
LOSS train 0.2522798834715859 valid 0.22087806123908205
LOSS train 0.2522798834715859 valid 0.22099129143606852
LOSS train 0.2522798834715859 valid 0.2208718039533671
LOSS train 0.2522798834715859 valid 0.22075097560882567
LOSS train 0.2522798834715859 valid 0.2207092289785737
LOSS train 0.2522798834715859 valid 0.22069567974638823
LOSS train 0.2522798834715859 valid 0.22058593546255276
LOSS train 0.2522798834715859 valid 0.22064117645247702
LOSS train 0.2522798834715859 valid 0.22079958738315672
LOSS train 0.2522798834715859 valid 0.22099078047614526
LOSS train 0.2522798834715859 valid 0.22082942063516042
LOSS train 0.2522798834715859 valid 0.22073814755594226
LOSS train 0.2522798834715859 valid 0.22068215635056807
LOSS train 0.2522798834715859 valid 0.22043355772661608
LOSS train 0.2522798834715859 valid 0.2202049008122197
LOSS train 0.2522798834715859 valid 0.2199752840990295
LOSS train 0.2522798834715859 valid 0.21990551936243652
LOSS train 0.2522798834715859 valid 0.21987003026759788
LOSS train 0.2522798834715859 valid 0.21996114369143138
LOSS train 0.2522798834715859 valid 0.21993361689925733
LOSS train 0.2522798834715859 valid 0.22002156072103227
LOSS train 0.2522798834715859 valid 0.22014341680458308
LOSS train 0.2522798834715859 valid 0.22004177161891544
LOSS train 0.2522798834715859 valid 0.21989284416039784
LOSS train 0.2522798834715859 valid 0.21972096493813845
LOSS train 0.2522798834715859 valid 0.21974078464087934
LOSS train 0.2522798834715859 valid 0.2196878122263833
LOSS train 0.2522798834715859 valid 0.21957918032027748
LOSS train 0.2522798834715859 valid 0.2196128107931303
LOSS train 0.2522798834715859 valid 0.21955730182267888
LOSS train 0.2522798834715859 valid 0.21969922220912472
LOSS train 0.2522798834715859 valid 0.22007669783457154
LOSS train 0.2522798834715859 valid 0.22000970207473153
LOSS train 0.2522798834715859 valid 0.22005785450022272
LOSS train 0.2522798834715859 valid 0.21994617139383899
LOSS train 0.2522798834715859 valid 0.220057465670481
LOSS train 0.2522798834715859 valid 0.22001802952600127
LOSS train 0.2522798834715859 valid 0.2200146962159847
LOSS train 0.2522798834715859 valid 0.21962625247736772
LOSS train 0.2522798834715859 valid 0.21950158346985385
LOSS train 0.2522798834715859 valid 0.21941888627927164
LOSS train 0.2522798834715859 valid 0.21927994833071046
LOSS train 0.2522798834715859 valid 0.21926633158668143
LOSS train 0.2522798834715859 valid 0.2194353018488203
LOSS train 0.2522798834715859 valid 0.21947162501453385
LOSS train 0.2522798834715859 valid 0.21951464373573118
LOSS train 0.2522798834715859 valid 0.21947644729047053
LOSS train 0.2522798834715859 valid 0.21930088449912857
LOSS train 0.2522798834715859 valid 0.21944088357686997
LOSS train 0.2522798834715859 valid 0.21946899212926507
LOSS train 0.2522798834715859 valid 0.21937404945492744
LOSS train 0.2522798834715859 valid 0.21920830040816733
LOSS train 0.2522798834715859 valid 0.21933104514371693
LOSS train 0.2522798834715859 valid 0.21924737014022527
LOSS train 0.2522798834715859 valid 0.21899635659065098
LOSS train 0.2522798834715859 valid 0.21898145666382193
LOSS train 0.2522798834715859 valid 0.21915939173033072
LOSS train 0.2522798834715859 valid 0.21909242293089054
LOSS train 0.2522798834715859 valid 0.21892903349720516
LOSS train 0.2522798834715859 valid 0.2189537843410996
LOSS train 0.2522798834715859 valid 0.21914564624769997
LOSS train 0.2522798834715859 valid 0.2189818795189658
LOSS train 0.2522798834715859 valid 0.2190581587227908
LOSS train 0.2522798834715859 valid 0.21897101824013693
LOSS train 0.2522798834715859 valid 0.21900186721319542
LOSS train 0.2522798834715859 valid 0.21890860634126913
LOSS train 0.2522798834715859 valid 0.21897512663211396
LOSS train 0.2522798834715859 valid 0.21909734207901368
LOSS train 0.2522798834715859 valid 0.21892897481167758
LOSS train 0.2522798834715859 valid 0.21890607349767016
LOSS train 0.2522798834715859 valid 0.2188142053235103
LOSS train 0.2522798834715859 valid 0.218759142271765
LOSS train 0.2522798834715859 valid 0.21865315126241558
LOSS train 0.2522798834715859 valid 0.21867789750749414
LOSS train 0.2522798834715859 valid 0.21873536182270534
LOSS train 0.2522798834715859 valid 0.21876077440026004
LOSS train 0.2522798834715859 valid 0.2187353851233455
LOSS train 0.2522798834715859 valid 0.2186990730117299
LOSS train 0.2522798834715859 valid 0.21858490141374723
LOSS train 0.2522798834715859 valid 0.2184177489679479
LOSS train 0.2522798834715859 valid 0.2183786838824022
LOSS train 0.2522798834715859 valid 0.218338281863991
LOSS train 0.2522798834715859 valid 0.21827105847252926
LOSS train 0.2522798834715859 valid 0.21823685430643852
LOSS train 0.2522798834715859 valid 0.2182161334928099
LOSS train 0.2522798834715859 valid 0.21823458337202306
LOSS train 0.2522798834715859 valid 0.21821315840093625
LOSS train 0.2522798834715859 valid 0.21818578330909505
LOSS train 0.2522798834715859 valid 0.21807165120182367
LOSS train 0.2522798834715859 valid 0.21804491417719327
LOSS train 0.2522798834715859 valid 0.21801513402837597
LOSS train 0.2522798834715859 valid 0.2179890360950203
LOSS train 0.2522798834715859 valid 0.21806010608973148
LOSS train 0.2522798834715859 valid 0.21809484509088226
LOSS train 0.2522798834715859 valid 0.21804574836750287
LOSS train 0.2522798834715859 valid 0.2179627236392763
LOSS train 0.2522798834715859 valid 0.21798416942158003
LOSS train 0.2522798834715859 valid 0.21797372515384966
LOSS train 0.2522798834715859 valid 0.2179191421965758
LOSS train 0.2522798834715859 valid 0.2177956074575253
LOSS train 0.2522798834715859 valid 0.21776604479709208
LOSS train 0.2522798834715859 valid 0.2177095794343319
LOSS train 0.2522798834715859 valid 0.21772584828891253
LOSS train 0.2522798834715859 valid 0.21769736931949366
LOSS train 0.2522798834715859 valid 0.21796468470026464
LOSS train 0.2522798834715859 valid 0.2179998880197637
LOSS train 0.2522798834715859 valid 0.21805470378754974
LOSS train 0.2522798834715859 valid 0.2180862576733901
LOSS train 0.2522798834715859 valid 0.21809093053302459
LOSS train 0.2522798834715859 valid 0.21810671926694666
LOSS train 0.2522798834715859 valid 0.21807371247082186
LOSS train 0.2522798834715859 valid 0.21810176263982878
LOSS train 0.2522798834715859 valid 0.21800771678329273
LOSS train 0.2522798834715859 valid 0.21796472886252025
LOSS train 0.2522798834715859 valid 0.21789806175835524
LOSS train 0.2522798834715859 valid 0.21788727203002112
LOSS train 0.2522798834715859 valid 0.21787007780944775
LOSS train 0.2522798834715859 valid 0.2178374566254571
LOSS train 0.2522798834715859 valid 0.21772626372985543
LOSS train 0.2522798834715859 valid 0.2178619000306382
LOSS train 0.2522798834715859 valid 0.21787832316404543
LOSS train 0.2522798834715859 valid 0.21780393879295504
LOSS train 0.2522798834715859 valid 0.2178240892143897
LOSS train 0.2522798834715859 valid 0.21770264506340026
LOSS train 0.2522798834715859 valid 0.2177685987364295
LOSS train 0.2522798834715859 valid 0.21771069112538563
LOSS train 0.2522798834715859 valid 0.21765133747603835
LOSS train 0.2522798834715859 valid 0.2176604215678473
LOSS train 0.2522798834715859 valid 0.21765840378674595
LOSS train 0.2522798834715859 valid 0.2175872835507926
LOSS train 0.2522798834715859 valid 0.21759471315217305
LOSS train 0.2522798834715859 valid 0.21771446631119415
LOSS train 0.2522798834715859 valid 0.21763245350943355
LOSS train 0.2522798834715859 valid 0.21751905666358434
LOSS train 0.2522798834715859 valid 0.21746402668456236
LOSS train 0.2522798834715859 valid 0.2176169228659893
LOSS train 0.2522798834715859 valid 0.21761339946964084
LOSS train 0.2522798834715859 valid 0.21753032421995405
LOSS train 0.2522798834715859 valid 0.21762998665080352
LOSS train 0.2522798834715859 valid 0.21757874216152426
LOSS train 0.2522798834715859 valid 0.2175727582030129
LOSS train 0.2522798834715859 valid 0.21744471145440816
LOSS train 0.2522798834715859 valid 0.21753526167121046
LOSS train 0.2522798834715859 valid 0.21767396624537483
LOSS train 0.2522798834715859 valid 0.21764244358835882
LOSS train 0.2522798834715859 valid 0.21750937531076867
LOSS train 0.2522798834715859 valid 0.2176209064307569
LOSS train 0.2522798834715859 valid 0.21756402396711716
LOSS train 0.2522798834715859 valid 0.21756631689412254
LOSS train 0.2522798834715859 valid 0.21764255041240627
LOSS train 0.2522798834715859 valid 0.21768495118753475
LOSS train 0.2522798834715859 valid 0.21771573859976642
LOSS train 0.2522798834715859 valid 0.21774119185020693
LOSS train 0.2522798834715859 valid 0.2175877374242729
LOSS train 0.2522798834715859 valid 0.2176205695428875
LOSS train 0.2522798834715859 valid 0.21767430282941386
LOSS train 0.2522798834715859 valid 0.21761160177392
LOSS train 0.2522798834715859 valid 0.21752512766292167
LOSS train 0.2522798834715859 valid 0.21768422172301347
LOSS train 0.2522798834715859 valid 0.217609348967465
LOSS train 0.2522798834715859 valid 0.21767610307391835
LOSS train 0.2522798834715859 valid 0.21774667977629944
LOSS train 0.2522798834715859 valid 0.2176693840423128
LOSS train 0.2522798834715859 valid 0.21779768381216755
LOSS train 0.2522798834715859 valid 0.21779108914684076
LOSS train 0.2522798834715859 valid 0.217752145445964
LOSS train 0.2522798834715859 valid 0.21763293700211722
LOSS train 0.2522798834715859 valid 0.21759402461168242
EPOCH 15:
  batch 1 loss: 0.25780490040779114
  batch 2 loss: 0.27505432069301605
  batch 3 loss: 0.2628050049146016
  batch 4 loss: 0.2599517032504082
  batch 5 loss: 0.26796562075614927
  batch 6 loss: 0.2653210461139679
  batch 7 loss: 0.2661604881286621
  batch 8 loss: 0.26678795740008354
  batch 9 loss: 0.26824315388997394
  batch 10 loss: 0.26328455805778506
  batch 11 loss: 0.26242214712229645
  batch 12 loss: 0.26110223184029263
  batch 13 loss: 0.2590975153904695
  batch 14 loss: 0.2574047584618841
  batch 15 loss: 0.2572809229294459
  batch 16 loss: 0.25655810814350843
  batch 17 loss: 0.25766828743850484
  batch 18 loss: 0.2552306420273251
  batch 19 loss: 0.25485676527023315
  batch 20 loss: 0.25309676602482795
  batch 21 loss: 0.25588042182581766
  batch 22 loss: 0.25593398308212106
  batch 23 loss: 0.2552818493998569
  batch 24 loss: 0.25580997702976066
  batch 25 loss: 0.25691638290882113
  batch 26 loss: 0.256493920317063
  batch 27 loss: 0.25617372768896596
  batch 28 loss: 0.2569801190069744
  batch 29 loss: 0.25660441764469805
  batch 30 loss: 0.2572724928458532
  batch 31 loss: 0.2573618754263847
  batch 32 loss: 0.2580778989940882
  batch 33 loss: 0.2596723328937184
  batch 34 loss: 0.2597543816356098
  batch 35 loss: 0.2611059844493866
  batch 36 loss: 0.26066265006860095
  batch 37 loss: 0.2608628683799022
  batch 38 loss: 0.26111671642253276
  batch 39 loss: 0.26161526181758976
  batch 40 loss: 0.2613196399062872
  batch 41 loss: 0.26120237715360595
  batch 42 loss: 0.26073257341271355
  batch 43 loss: 0.26101451765659245
  batch 44 loss: 0.26124798371033237
  batch 45 loss: 0.2605777429209815
  batch 46 loss: 0.25911367749390396
  batch 47 loss: 0.25914061988921877
  batch 48 loss: 0.2590762261922161
  batch 49 loss: 0.25968500029067604
  batch 50 loss: 0.26115988224744796
  batch 51 loss: 0.2614685551208608
  batch 52 loss: 0.2615869500889228
  batch 53 loss: 0.26125349610481624
  batch 54 loss: 0.26139449659321046
  batch 55 loss: 0.26160724677822805
  batch 56 loss: 0.2618141081184149
  batch 57 loss: 0.2612127057815853
  batch 58 loss: 0.26087258602010793
  batch 59 loss: 0.26092713515637284
  batch 60 loss: 0.2609132409095764
  batch 61 loss: 0.26157530604815876
  batch 62 loss: 0.26170439297153103
  batch 63 loss: 0.2607555484014844
  batch 64 loss: 0.26033428963273764
  batch 65 loss: 0.25980634391307833
  batch 66 loss: 0.2598422569307414
  batch 67 loss: 0.25976710154939053
  batch 68 loss: 0.26003750917666096
  batch 69 loss: 0.2599457504524701
  batch 70 loss: 0.26006690370185037
  batch 71 loss: 0.2600529133014276
  batch 72 loss: 0.26011534180078244
  batch 73 loss: 0.2603771304839278
  batch 74 loss: 0.26018161407193624
  batch 75 loss: 0.26031908333301546
  batch 76 loss: 0.2605007610430843
  batch 77 loss: 0.26029481787186165
  batch 78 loss: 0.26053800300145763
  batch 79 loss: 0.26040756438351886
  batch 80 loss: 0.2603318314999342
  batch 81 loss: 0.2601164608825872
  batch 82 loss: 0.26029707273332087
  batch 83 loss: 0.25981186545756924
  batch 84 loss: 0.25912774070387795
  batch 85 loss: 0.25946181697003984
  batch 86 loss: 0.2591561141402222
  batch 87 loss: 0.259139405242328
  batch 88 loss: 0.2589555692604997
  batch 89 loss: 0.2586958289481281
  batch 90 loss: 0.25904349436362584
  batch 91 loss: 0.25884460387649116
  batch 92 loss: 0.25856377718889195
  batch 93 loss: 0.2580820142581899
  batch 94 loss: 0.2577850637917823
  batch 95 loss: 0.257654490753224
  batch 96 loss: 0.2576798183533053
  batch 97 loss: 0.2581694760580653
  batch 98 loss: 0.2579005976416627
  batch 99 loss: 0.2574742537255239
  batch 100 loss: 0.2576238985359669
  batch 101 loss: 0.25760047668867775
  batch 102 loss: 0.25762011388353273
  batch 103 loss: 0.2573344886881634
  batch 104 loss: 0.2577398061179198
  batch 105 loss: 0.2576838850975037
  batch 106 loss: 0.2576708596832347
  batch 107 loss: 0.25764067819185343
  batch 108 loss: 0.2572480902351715
  batch 109 loss: 0.2572955515133132
  batch 110 loss: 0.2573399168523875
  batch 111 loss: 0.2573056568702062
  batch 112 loss: 0.25730073385472807
  batch 113 loss: 0.25712679761700924
  batch 114 loss: 0.25705562113669883
  batch 115 loss: 0.2567269081654756
  batch 116 loss: 0.256444470250401
  batch 117 loss: 0.2563620734418559
  batch 118 loss: 0.2563930580676612
  batch 119 loss: 0.2564524314483675
  batch 120 loss: 0.2563123588760694
  batch 121 loss: 0.25629373916909715
  batch 122 loss: 0.25609654945428256
  batch 123 loss: 0.2558345937631964
  batch 124 loss: 0.2558740137084838
  batch 125 loss: 0.2555581226348877
  batch 126 loss: 0.25567187321564505
  batch 127 loss: 0.255914149556573
  batch 128 loss: 0.25603035232052207
  batch 129 loss: 0.2562592630238496
  batch 130 loss: 0.2560784507256288
  batch 131 loss: 0.2559739000924671
  batch 132 loss: 0.2561177412668864
  batch 133 loss: 0.2559252828359604
  batch 134 loss: 0.2558182276674171
  batch 135 loss: 0.2556634873151779
  batch 136 loss: 0.25585312516812014
  batch 137 loss: 0.256305553508501
  batch 138 loss: 0.25629469331191934
  batch 139 loss: 0.2562176197124042
  batch 140 loss: 0.25654845301594054
  batch 141 loss: 0.25628609162695865
  batch 142 loss: 0.2559025133369674
  batch 143 loss: 0.2560802515778508
  batch 144 loss: 0.25598827552878195
  batch 145 loss: 0.2559359954348926
  batch 146 loss: 0.256082231969866
  batch 147 loss: 0.25588817549806064
  batch 148 loss: 0.25578365313845713
  batch 149 loss: 0.25535199896201194
  batch 150 loss: 0.25547076334555946
  batch 151 loss: 0.2551310876544738
  batch 152 loss: 0.25481575864710304
  batch 153 loss: 0.2549939114673465
  batch 154 loss: 0.2547865616423743
  batch 155 loss: 0.25473946602113784
  batch 156 loss: 0.2545859668499384
  batch 157 loss: 0.2544472225152763
  batch 158 loss: 0.2544555454691754
  batch 159 loss: 0.2546746734553163
  batch 160 loss: 0.25471881609410046
  batch 161 loss: 0.2544696329173094
  batch 162 loss: 0.2544246522916688
  batch 163 loss: 0.25425557664201304
  batch 164 loss: 0.25454407666878004
  batch 165 loss: 0.25475096783854745
  batch 166 loss: 0.2549784751721175
  batch 167 loss: 0.2548281142633118
  batch 168 loss: 0.2546431262578283
  batch 169 loss: 0.2546463598161054
  batch 170 loss: 0.2546278432888143
  batch 171 loss: 0.2546193120423813
  batch 172 loss: 0.25452768118229024
  batch 173 loss: 0.2544290479380271
  batch 174 loss: 0.2545818639383919
  batch 175 loss: 0.25462647582803455
  batch 176 loss: 0.254862348549068
  batch 177 loss: 0.2550860349572984
  batch 178 loss: 0.2550613492559851
  batch 179 loss: 0.25504916947980166
  batch 180 loss: 0.25498898724714913
  batch 181 loss: 0.2549414802651379
  batch 182 loss: 0.25500765688471744
  batch 183 loss: 0.25495651893602694
  batch 184 loss: 0.25486198545474076
  batch 185 loss: 0.2546804030199309
  batch 186 loss: 0.25471402945057037
  batch 187 loss: 0.2548121315910217
  batch 188 loss: 0.25469222926396007
  batch 189 loss: 0.2546501311833266
  batch 190 loss: 0.2545016557762497
  batch 191 loss: 0.25457386719306724
  batch 192 loss: 0.2544457196102788
  batch 193 loss: 0.25439532772864704
  batch 194 loss: 0.2542622178485713
  batch 195 loss: 0.2544303148220747
  batch 196 loss: 0.2543059363200956
  batch 197 loss: 0.25440909497931524
  batch 198 loss: 0.2542927393106499
  batch 199 loss: 0.25430991601704356
  batch 200 loss: 0.2542706955969334
  batch 201 loss: 0.25415949256562476
  batch 202 loss: 0.2542893719112519
  batch 203 loss: 0.2543198141117989
  batch 204 loss: 0.2544369651552509
  batch 205 loss: 0.2543512629299629
  batch 206 loss: 0.25446374236958696
  batch 207 loss: 0.2544549506355599
  batch 208 loss: 0.25430632948588866
  batch 209 loss: 0.25435347954907483
  batch 210 loss: 0.2542942208193597
  batch 211 loss: 0.2541940793465664
  batch 212 loss: 0.25414545742689437
  batch 213 loss: 0.2539593708906935
  batch 214 loss: 0.2538017215572785
  batch 215 loss: 0.25378758131071577
  batch 216 loss: 0.2537574881894721
  batch 217 loss: 0.25365431023083523
  batch 218 loss: 0.2535006375897915
  batch 219 loss: 0.25320056822474146
  batch 220 loss: 0.25335873941128906
  batch 221 loss: 0.25331593267788177
  batch 222 loss: 0.2533775923085642
  batch 223 loss: 0.2532789822116561
  batch 224 loss: 0.2532670133348022
  batch 225 loss: 0.2532594900661045
  batch 226 loss: 0.2532967190563151
  batch 227 loss: 0.25332508554542643
  batch 228 loss: 0.2533228053596982
  batch 229 loss: 0.2533364669464561
  batch 230 loss: 0.2533617646797844
  batch 231 loss: 0.2533636193770867
  batch 232 loss: 0.25329937503255645
  batch 233 loss: 0.2533075415525314
  batch 234 loss: 0.2532824223112856
  batch 235 loss: 0.2531735160249345
  batch 236 loss: 0.2531363449485625
  batch 237 loss: 0.25299497451711805
  batch 238 loss: 0.2528459932498571
  batch 239 loss: 0.2528002285807701
  batch 240 loss: 0.2529456005742153
  batch 241 loss: 0.25278416875734366
  batch 242 loss: 0.25260547480799933
  batch 243 loss: 0.25259985538667123
  batch 244 loss: 0.252500058502936
  batch 245 loss: 0.25246291306554053
  batch 246 loss: 0.25233926151583835
  batch 247 loss: 0.2522344041449821
  batch 248 loss: 0.25233065216772016
  batch 249 loss: 0.25228901147603033
  batch 250 loss: 0.25224143236875535
  batch 251 loss: 0.25208574064461836
  batch 252 loss: 0.2521961177034037
  batch 253 loss: 0.25203617588569055
  batch 254 loss: 0.2519385563576315
  batch 255 loss: 0.25181200872449316
  batch 256 loss: 0.2517428519204259
  batch 257 loss: 0.2517094154650135
  batch 258 loss: 0.25165765813385793
  batch 259 loss: 0.2514391792902155
  batch 260 loss: 0.251550797946178
  batch 261 loss: 0.2515429289747472
  batch 262 loss: 0.2514066537486688
  batch 263 loss: 0.25133462862143496
  batch 264 loss: 0.25132134398727707
  batch 265 loss: 0.25119247324061844
  batch 266 loss: 0.25101422115152044
  batch 267 loss: 0.25084205697315015
  batch 268 loss: 0.25067663009264574
  batch 269 loss: 0.25072299076943594
  batch 270 loss: 0.25063038170337676
  batch 271 loss: 0.2505592764303693
  batch 272 loss: 0.25044077256804004
  batch 273 loss: 0.25024367044696877
  batch 274 loss: 0.25005583378085255
  batch 275 loss: 0.2500183330340819
  batch 276 loss: 0.2499485428350559
  batch 277 loss: 0.24989795717091337
  batch 278 loss: 0.2497519212339422
  batch 279 loss: 0.24980407937239574
  batch 280 loss: 0.2497994582035712
  batch 281 loss: 0.24975369764604602
  batch 282 loss: 0.2496160642369419
  batch 283 loss: 0.2495770774018217
  batch 284 loss: 0.24951725741717176
  batch 285 loss: 0.2494577136478926
  batch 286 loss: 0.2494640758300161
  batch 287 loss: 0.24932271294062147
  batch 288 loss: 0.24931688327342272
  batch 289 loss: 0.24916233029538784
  batch 290 loss: 0.24900993864084112
  batch 291 loss: 0.24892289721474206
  batch 292 loss: 0.2488490772267727
  batch 293 loss: 0.2488786561395528
  batch 294 loss: 0.2486158285518082
  batch 295 loss: 0.24864421966722455
  batch 296 loss: 0.24879009996515675
  batch 297 loss: 0.2487122064788735
  batch 298 loss: 0.24873736155713164
  batch 299 loss: 0.24884120073406193
  batch 300 loss: 0.24870373631517093
  batch 301 loss: 0.24872103962193295
  batch 302 loss: 0.2488635571685848
  batch 303 loss: 0.24882109935330873
  batch 304 loss: 0.24875277530794082
  batch 305 loss: 0.24875327378022866
  batch 306 loss: 0.24882347404567245
  batch 307 loss: 0.2487790494768938
  batch 308 loss: 0.24881961782063758
  batch 309 loss: 0.24880072604683998
  batch 310 loss: 0.24883776619549722
  batch 311 loss: 0.24877057298778338
  batch 312 loss: 0.2487985662256296
  batch 313 loss: 0.24878293904252707
  batch 314 loss: 0.2487442887322918
  batch 315 loss: 0.2487087027894126
  batch 316 loss: 0.2486472937125194
  batch 317 loss: 0.24876074629251138
  batch 318 loss: 0.24861313168357754
  batch 319 loss: 0.24840655637945874
  batch 320 loss: 0.24859313941560685
  batch 321 loss: 0.2486797979223394
  batch 322 loss: 0.2486275718634173
  batch 323 loss: 0.24867160133902133
  batch 324 loss: 0.2488709243911284
  batch 325 loss: 0.24900246317570027
  batch 326 loss: 0.249096521753475
  batch 327 loss: 0.24904813164053344
  batch 328 loss: 0.24902235775640824
  batch 329 loss: 0.24912219619135
  batch 330 loss: 0.2491619544047298
  batch 331 loss: 0.24914445563745644
  batch 332 loss: 0.24929276979472265
  batch 333 loss: 0.24927271173165008
  batch 334 loss: 0.24955957173230406
  batch 335 loss: 0.2496167234520414
  batch 336 loss: 0.24966730168532758
  batch 337 loss: 0.2497540646030924
  batch 338 loss: 0.24969241340484846
  batch 339 loss: 0.24971518140275217
  batch 340 loss: 0.24958137072184505
  batch 341 loss: 0.24953454942647313
  batch 342 loss: 0.24949359305595098
  batch 343 loss: 0.2495670354661719
  batch 344 loss: 0.2496404108526402
  batch 345 loss: 0.24975703328415969
  batch 346 loss: 0.24974741511089954
  batch 347 loss: 0.24958332617275997
  batch 348 loss: 0.24958193567634998
  batch 349 loss: 0.24967675555742913
  batch 350 loss: 0.24977778698716846
  batch 351 loss: 0.24973589415394004
  batch 352 loss: 0.24965115331790663
  batch 353 loss: 0.2496257407891514
  batch 354 loss: 0.249565626394614
  batch 355 loss: 0.24948388203768662
  batch 356 loss: 0.24953654659598062
  batch 357 loss: 0.24937939113762533
  batch 358 loss: 0.24926853408860095
  batch 359 loss: 0.24919400096437724
  batch 360 loss: 0.2491248865508371
  batch 361 loss: 0.24906541324553397
  batch 362 loss: 0.249078885749888
  batch 363 loss: 0.2491132262734999
  batch 364 loss: 0.24899029617126173
  batch 365 loss: 0.24903336371461007
  batch 366 loss: 0.24902750352204173
  batch 367 loss: 0.24908722701611896
  batch 368 loss: 0.2490665214136243
  batch 369 loss: 0.249008870221735
  batch 370 loss: 0.2490356020025305
  batch 371 loss: 0.24906562931454085
  batch 372 loss: 0.24912112470596068
  batch 373 loss: 0.2491549086474861
  batch 374 loss: 0.2490825936756032
  batch 375 loss: 0.2491573009490967
  batch 376 loss: 0.24927496545492334
  batch 377 loss: 0.24927114258552419
  batch 378 loss: 0.24925846894266745
  batch 379 loss: 0.24928609307334418
  batch 380 loss: 0.2492331656186204
  batch 381 loss: 0.24918673510157216
  batch 382 loss: 0.24919956481737615
  batch 383 loss: 0.2491081825046564
  batch 384 loss: 0.2490804575694104
  batch 385 loss: 0.24907786265596166
  batch 386 loss: 0.24905535667517026
  batch 387 loss: 0.24901520035371608
  batch 388 loss: 0.24885746967239478
  batch 389 loss: 0.24886201563408564
  batch 390 loss: 0.24883602154560577
  batch 391 loss: 0.24873481157338223
  batch 392 loss: 0.24883705046864188
  batch 393 loss: 0.24885707680353986
  batch 394 loss: 0.24883875757607107
  batch 395 loss: 0.24881967343861544
  batch 396 loss: 0.24889525974338705
  batch 397 loss: 0.24897223891178968
  batch 398 loss: 0.24904937166065427
  batch 399 loss: 0.24904548266627138
  batch 400 loss: 0.24917553570121526
  batch 401 loss: 0.24920578085424894
  batch 402 loss: 0.24920589198817067
  batch 403 loss: 0.24917055936368465
  batch 404 loss: 0.24914771997102417
  batch 405 loss: 0.249117495837035
  batch 406 loss: 0.2490068530214244
  batch 407 loss: 0.24901899358564278
  batch 408 loss: 0.24891481183323205
  batch 409 loss: 0.2489498394625694
  batch 410 loss: 0.24878821511094162
  batch 411 loss: 0.2487878838232254
  batch 412 loss: 0.2488914782778152
  batch 413 loss: 0.24887353607586452
  batch 414 loss: 0.24893197150909957
  batch 415 loss: 0.2488994860146419
  batch 416 loss: 0.24891822502160302
  batch 417 loss: 0.24889225074999052
  batch 418 loss: 0.248885294859204
  batch 419 loss: 0.2488787735063307
  batch 420 loss: 0.24887310213276317
  batch 421 loss: 0.24895917664939038
  batch 422 loss: 0.24890231966124893
  batch 423 loss: 0.24885630255331667
  batch 424 loss: 0.2488987256052359
  batch 425 loss: 0.24880858526510352
  batch 426 loss: 0.24875908075643818
  batch 427 loss: 0.2487802388238125
  batch 428 loss: 0.24883759787706572
  batch 429 loss: 0.24869052343296283
  batch 430 loss: 0.24868720634732136
  batch 431 loss: 0.2487433904354091
  batch 432 loss: 0.2487660975160974
  batch 433 loss: 0.2487374179013607
  batch 434 loss: 0.24867753200404655
  batch 435 loss: 0.24874372054105517
  batch 436 loss: 0.2487232381939341
  batch 437 loss: 0.24866762337886497
  batch 438 loss: 0.24859887691529375
  batch 439 loss: 0.24852184679758033
  batch 440 loss: 0.24858148595826193
  batch 441 loss: 0.24857585317964187
  batch 442 loss: 0.2485595532604472
  batch 443 loss: 0.2485622969155387
  batch 444 loss: 0.24859797031627046
  batch 445 loss: 0.24856132181842674
  batch 446 loss: 0.24855732647159173
  batch 447 loss: 0.24849217520064157
  batch 448 loss: 0.24856294062919915
  batch 449 loss: 0.2484937403143647
  batch 450 loss: 0.24848271403047772
  batch 451 loss: 0.24843691249776573
  batch 452 loss: 0.24833705319108162
  batch 453 loss: 0.24830728292333631
  batch 454 loss: 0.2482900759000085
  batch 455 loss: 0.24825946262904575
  batch 456 loss: 0.24819338341292582
  batch 457 loss: 0.24813672300240666
  batch 458 loss: 0.24814240465257886
  batch 459 loss: 0.248094716395428
  batch 460 loss: 0.24807077659213025
  batch 461 loss: 0.2481085195200045
  batch 462 loss: 0.2481582694368445
  batch 463 loss: 0.24818517116439523
  batch 464 loss: 0.24816682048398872
  batch 465 loss: 0.2481344017610755
  batch 466 loss: 0.24804719339240774
  batch 467 loss: 0.24818638589320907
  batch 468 loss: 0.24816848423618537
  batch 469 loss: 0.24811426834511097
  batch 470 loss: 0.24816624604641124
  batch 471 loss: 0.24825711152862337
  batch 472 loss: 0.24809875844393747
LOSS train 0.24809875844393747 valid 0.21712622046470642
LOSS train 0.24809875844393747 valid 0.21038293093442917
LOSS train 0.24809875844393747 valid 0.2162896047035853
LOSS train 0.24809875844393747 valid 0.20087869092822075
LOSS train 0.24809875844393747 valid 0.20156644880771638
LOSS train 0.24809875844393747 valid 0.20533851037422815
LOSS train 0.24809875844393747 valid 0.20077526569366455
LOSS train 0.24809875844393747 valid 0.19931736402213573
LOSS train 0.24809875844393747 valid 0.1986902786625756
LOSS train 0.24809875844393747 valid 0.1970839262008667
LOSS train 0.24809875844393747 valid 0.19709853421558032
LOSS train 0.24809875844393747 valid 0.20070305094122887
LOSS train 0.24809875844393747 valid 0.20153538768108076
LOSS train 0.24809875844393747 valid 0.1999217697552272
LOSS train 0.24809875844393747 valid 0.19995815058549246
LOSS train 0.24809875844393747 valid 0.20358847174793482
LOSS train 0.24809875844393747 valid 0.20518429752658396
LOSS train 0.24809875844393747 valid 0.20505854570203358
LOSS train 0.24809875844393747 valid 0.20757219744356056
LOSS train 0.24809875844393747 valid 0.20676275268197059
LOSS train 0.24809875844393747 valid 0.20816048128264292
LOSS train 0.24809875844393747 valid 0.2078840285539627
LOSS train 0.24809875844393747 valid 0.2056967389324437
LOSS train 0.24809875844393747 valid 0.20606826742490134
LOSS train 0.24809875844393747 valid 0.2062221026420593
LOSS train 0.24809875844393747 valid 0.20599247285952935
LOSS train 0.24809875844393747 valid 0.2055356474938216
LOSS train 0.24809875844393747 valid 0.20575472127114022
LOSS train 0.24809875844393747 valid 0.20457115461086406
LOSS train 0.24809875844393747 valid 0.20395747522513072
LOSS train 0.24809875844393747 valid 0.20425576356149489
LOSS train 0.24809875844393747 valid 0.20429523615166545
LOSS train 0.24809875844393747 valid 0.20322360640222376
LOSS train 0.24809875844393747 valid 0.20286907869226792
LOSS train 0.24809875844393747 valid 0.2031246100153242
LOSS train 0.24809875844393747 valid 0.20368219829267925
LOSS train 0.24809875844393747 valid 0.2041178877289231
LOSS train 0.24809875844393747 valid 0.20422900468111038
LOSS train 0.24809875844393747 valid 0.20525552981939071
LOSS train 0.24809875844393747 valid 0.20567040257155894
LOSS train 0.24809875844393747 valid 0.205618060943557
LOSS train 0.24809875844393747 valid 0.20675575307437352
LOSS train 0.24809875844393747 valid 0.20734714595384376
LOSS train 0.24809875844393747 valid 0.20698773860931396
LOSS train 0.24809875844393747 valid 0.206515011522505
LOSS train 0.24809875844393747 valid 0.20627512296904688
LOSS train 0.24809875844393747 valid 0.20618197860869955
LOSS train 0.24809875844393747 valid 0.20784924272447824
LOSS train 0.24809875844393747 valid 0.20717503221667544
LOSS train 0.24809875844393747 valid 0.20795926868915557
LOSS train 0.24809875844393747 valid 0.20758326848347983
LOSS train 0.24809875844393747 valid 0.20730046870616767
LOSS train 0.24809875844393747 valid 0.2086029530696149
LOSS train 0.24809875844393747 valid 0.2087734454759845
LOSS train 0.24809875844393747 valid 0.2087155978788029
LOSS train 0.24809875844393747 valid 0.20877678639122418
LOSS train 0.24809875844393747 valid 0.20829629192226812
LOSS train 0.24809875844393747 valid 0.20892584272499742
LOSS train 0.24809875844393747 valid 0.20856560892977957
LOSS train 0.24809875844393747 valid 0.2081936076283455
LOSS train 0.24809875844393747 valid 0.20829586293853697
LOSS train 0.24809875844393747 valid 0.2080930548329507
LOSS train 0.24809875844393747 valid 0.20788008400372096
LOSS train 0.24809875844393747 valid 0.20836733537726104
LOSS train 0.24809875844393747 valid 0.2074193782531298
LOSS train 0.24809875844393747 valid 0.2073361643336036
LOSS train 0.24809875844393747 valid 0.20813481798812525
LOSS train 0.24809875844393747 valid 0.20756006481892922
LOSS train 0.24809875844393747 valid 0.20824945383313773
LOSS train 0.24809875844393747 valid 0.20887114980391094
LOSS train 0.24809875844393747 valid 0.20917501919706102
LOSS train 0.24809875844393747 valid 0.20974651931060684
LOSS train 0.24809875844393747 valid 0.21035143977975193
LOSS train 0.24809875844393747 valid 0.2098406293504947
LOSS train 0.24809875844393747 valid 0.20951425035794577
LOSS train 0.24809875844393747 valid 0.20967337685196022
LOSS train 0.24809875844393747 valid 0.20959925341915775
LOSS train 0.24809875844393747 valid 0.20941256941893163
LOSS train 0.24809875844393747 valid 0.20930390795574913
LOSS train 0.24809875844393747 valid 0.20904690101742746
LOSS train 0.24809875844393747 valid 0.20926143137025244
LOSS train 0.24809875844393747 valid 0.2091335152707449
LOSS train 0.24809875844393747 valid 0.20932813329869007
LOSS train 0.24809875844393747 valid 0.20911727898887225
LOSS train 0.24809875844393747 valid 0.20999803665806266
LOSS train 0.24809875844393747 valid 0.2099759826479956
LOSS train 0.24809875844393747 valid 0.20977249998470832
LOSS train 0.24809875844393747 valid 0.2101519319482825
LOSS train 0.24809875844393747 valid 0.21058498088563427
LOSS train 0.24809875844393747 valid 0.21081520749462976
LOSS train 0.24809875844393747 valid 0.21079086438640132
LOSS train 0.24809875844393747 valid 0.21080095353333847
LOSS train 0.24809875844393747 valid 0.21070098620589062
LOSS train 0.24809875844393747 valid 0.2108152889190836
LOSS train 0.24809875844393747 valid 0.21110208536449232
LOSS train 0.24809875844393747 valid 0.21120494045317173
LOSS train 0.24809875844393747 valid 0.21127674843847138
LOSS train 0.24809875844393747 valid 0.2117131057442451
LOSS train 0.24809875844393747 valid 0.21192100704318345
LOSS train 0.24809875844393747 valid 0.21205282658338548
LOSS train 0.24809875844393747 valid 0.21213098180176007
LOSS train 0.24809875844393747 valid 0.21259175211775536
LOSS train 0.24809875844393747 valid 0.21245850318843879
LOSS train 0.24809875844393747 valid 0.21233670585430586
LOSS train 0.24809875844393747 valid 0.21254158658640726
LOSS train 0.24809875844393747 valid 0.2126497155371702
LOSS train 0.24809875844393747 valid 0.21232415011553007
LOSS train 0.24809875844393747 valid 0.21249497599071926
LOSS train 0.24809875844393747 valid 0.21209170676152642
LOSS train 0.24809875844393747 valid 0.21198880008675836
LOSS train 0.24809875844393747 valid 0.21224153726487546
LOSS train 0.24809875844393747 valid 0.2125403701460787
LOSS train 0.24809875844393747 valid 0.21224402511014348
LOSS train 0.24809875844393747 valid 0.2124144084620894
LOSS train 0.24809875844393747 valid 0.21308645865191583
LOSS train 0.24809875844393747 valid 0.21276262620913572
LOSS train 0.24809875844393747 valid 0.21337095195921058
LOSS train 0.24809875844393747 valid 0.21325150207947877
LOSS train 0.24809875844393747 valid 0.21303518892837173
LOSS train 0.24809875844393747 valid 0.21271392069756984
LOSS train 0.24809875844393747 valid 0.21245578719564706
LOSS train 0.24809875844393747 valid 0.21264984971675716
LOSS train 0.24809875844393747 valid 0.21271988083192003
LOSS train 0.24809875844393747 valid 0.21298672631382942
LOSS train 0.24809875844393747 valid 0.21295031881332396
LOSS train 0.24809875844393747 valid 0.2131987436423226
LOSS train 0.24809875844393747 valid 0.21298614449388398
LOSS train 0.24809875844393747 valid 0.21303390455432236
LOSS train 0.24809875844393747 valid 0.21269661788792574
LOSS train 0.24809875844393747 valid 0.21249505155361614
LOSS train 0.24809875844393747 valid 0.21250147423671403
LOSS train 0.24809875844393747 valid 0.21245681223544208
LOSS train 0.24809875844393747 valid 0.21247303205773346
LOSS train 0.24809875844393747 valid 0.21272863703432368
LOSS train 0.24809875844393747 valid 0.21282389241236227
LOSS train 0.24809875844393747 valid 0.21283400343621478
LOSS train 0.24809875844393747 valid 0.21286933613519599
LOSS train 0.24809875844393747 valid 0.21273315708706345
LOSS train 0.24809875844393747 valid 0.21257162340682187
LOSS train 0.24809875844393747 valid 0.2125101864337921
LOSS train 0.24809875844393747 valid 0.21264618603473015
LOSS train 0.24809875844393747 valid 0.21293760279954319
LOSS train 0.24809875844393747 valid 0.21300243258059443
LOSS train 0.24809875844393747 valid 0.21305369968629545
LOSS train 0.24809875844393747 valid 0.21300650721993938
LOSS train 0.24809875844393747 valid 0.21294719552340574
LOSS train 0.24809875844393747 valid 0.2128051845800309
LOSS train 0.24809875844393747 valid 0.21281469301194758
LOSS train 0.24809875844393747 valid 0.2129038547309453
LOSS train 0.24809875844393747 valid 0.21283824394146603
LOSS train 0.24809875844393747 valid 0.2127923902296862
LOSS train 0.24809875844393747 valid 0.2128033032150645
LOSS train 0.24809875844393747 valid 0.21255258417207432
LOSS train 0.24809875844393747 valid 0.21259475334898217
LOSS train 0.24809875844393747 valid 0.21266610391678348
LOSS train 0.24809875844393747 valid 0.21266888244411883
LOSS train 0.24809875844393747 valid 0.21257316525195055
LOSS train 0.24809875844393747 valid 0.21267332089475438
LOSS train 0.24809875844393747 valid 0.21282087135239966
LOSS train 0.24809875844393747 valid 0.2130633826367557
LOSS train 0.24809875844393747 valid 0.2129906199363448
LOSS train 0.24809875844393747 valid 0.2130643313313708
LOSS train 0.24809875844393747 valid 0.2128665776340508
LOSS train 0.24809875844393747 valid 0.21292514408507
LOSS train 0.24809875844393747 valid 0.2130531423922741
LOSS train 0.24809875844393747 valid 0.21297926617315016
LOSS train 0.24809875844393747 valid 0.21297493594849182
LOSS train 0.24809875844393747 valid 0.2128363178954238
LOSS train 0.24809875844393747 valid 0.21257235215613124
LOSS train 0.24809875844393747 valid 0.2126579587073887
LOSS train 0.24809875844393747 valid 0.21258742845894998
LOSS train 0.24809875844393747 valid 0.21265635050313417
LOSS train 0.24809875844393747 valid 0.2124963075439365
LOSS train 0.24809875844393747 valid 0.21288548324299955
LOSS train 0.24809875844393747 valid 0.21281570068427494
LOSS train 0.24809875844393747 valid 0.2127321639352224
LOSS train 0.24809875844393747 valid 0.21280869008120845
LOSS train 0.24809875844393747 valid 0.2128669936837775
LOSS train 0.24809875844393747 valid 0.21306283255862124
LOSS train 0.24809875844393747 valid 0.21292089828186564
LOSS train 0.24809875844393747 valid 0.2128892981380389
LOSS train 0.24809875844393747 valid 0.21279849963528769
LOSS train 0.24809875844393747 valid 0.21289815888053082
LOSS train 0.24809875844393747 valid 0.21279234422937685
LOSS train 0.24809875844393747 valid 0.2126256025320775
LOSS train 0.24809875844393747 valid 0.21267506095670885
LOSS train 0.24809875844393747 valid 0.2125542568650475
LOSS train 0.24809875844393747 valid 0.21260747424465545
LOSS train 0.24809875844393747 valid 0.2124224499892936
LOSS train 0.24809875844393747 valid 0.21239643983150783
LOSS train 0.24809875844393747 valid 0.21252233710588586
LOSS train 0.24809875844393747 valid 0.21254607406444848
LOSS train 0.24809875844393747 valid 0.2123430122674438
LOSS train 0.24809875844393747 valid 0.21219692847777888
LOSS train 0.24809875844393747 valid 0.21209780833660027
LOSS train 0.24809875844393747 valid 0.21221020779743485
LOSS train 0.24809875844393747 valid 0.2121604215070076
LOSS train 0.24809875844393747 valid 0.2123432589901818
LOSS train 0.24809875844393747 valid 0.2122986195374973
LOSS train 0.24809875844393747 valid 0.21245929904282093
LOSS train 0.24809875844393747 valid 0.21224979062875113
LOSS train 0.24809875844393747 valid 0.21224737403416397
LOSS train 0.24809875844393747 valid 0.2123314360973283
LOSS train 0.24809875844393747 valid 0.21219789083389676
LOSS train 0.24809875844393747 valid 0.21210787972299064
LOSS train 0.24809875844393747 valid 0.21208979240030917
LOSS train 0.24809875844393747 valid 0.21206275348502082
LOSS train 0.24809875844393747 valid 0.21196807713176197
LOSS train 0.24809875844393747 valid 0.21202225141833272
LOSS train 0.24809875844393747 valid 0.2121903501805805
LOSS train 0.24809875844393747 valid 0.21238212181493568
LOSS train 0.24809875844393747 valid 0.21222559615688505
LOSS train 0.24809875844393747 valid 0.21214924053127218
LOSS train 0.24809875844393747 valid 0.21209227154466592
LOSS train 0.24809875844393747 valid 0.21182312833708386
LOSS train 0.24809875844393747 valid 0.2115970995553114
LOSS train 0.24809875844393747 valid 0.2113776956835101
LOSS train 0.24809875844393747 valid 0.21135888836525996
LOSS train 0.24809875844393747 valid 0.21129056709269955
LOSS train 0.24809875844393747 valid 0.21137051914225924
LOSS train 0.24809875844393747 valid 0.21132829697693095
LOSS train 0.24809875844393747 valid 0.21139908259784854
LOSS train 0.24809875844393747 valid 0.21153120095152492
LOSS train 0.24809875844393747 valid 0.21146615528102433
LOSS train 0.24809875844393747 valid 0.21131319291061826
LOSS train 0.24809875844393747 valid 0.2111424055379049
LOSS train 0.24809875844393747 valid 0.21114867807484933
LOSS train 0.24809875844393747 valid 0.21112102113272013
LOSS train 0.24809875844393747 valid 0.21100747422620197
LOSS train 0.24809875844393747 valid 0.2110410698082136
LOSS train 0.24809875844393747 valid 0.21100392363546214
LOSS train 0.24809875844393747 valid 0.21112646721303463
LOSS train 0.24809875844393747 valid 0.21145709831059747
LOSS train 0.24809875844393747 valid 0.21142440645867944
LOSS train 0.24809875844393747 valid 0.21146650504558645
LOSS train 0.24809875844393747 valid 0.2113279085290634
LOSS train 0.24809875844393747 valid 0.2114436928858737
LOSS train 0.24809875844393747 valid 0.21141151438991562
LOSS train 0.24809875844393747 valid 0.2113834213636909
LOSS train 0.24809875844393747 valid 0.21100670204808314
LOSS train 0.24809875844393747 valid 0.21089592001002855
LOSS train 0.24809875844393747 valid 0.21081361587136244
LOSS train 0.24809875844393747 valid 0.21066084258840898
LOSS train 0.24809875844393747 valid 0.21063258865329085
LOSS train 0.24809875844393747 valid 0.21075629987278763
LOSS train 0.24809875844393747 valid 0.21080981697735748
LOSS train 0.24809875844393747 valid 0.2108422177523254
LOSS train 0.24809875844393747 valid 0.21077950082478986
LOSS train 0.24809875844393747 valid 0.210602394787662
LOSS train 0.24809875844393747 valid 0.21071527361869813
LOSS train 0.24809875844393747 valid 0.21072368680006004
LOSS train 0.24809875844393747 valid 0.21059624631963078
LOSS train 0.24809875844393747 valid 0.21042964675209738
LOSS train 0.24809875844393747 valid 0.21055575026067222
LOSS train 0.24809875844393747 valid 0.2105019359027638
LOSS train 0.24809875844393747 valid 0.21028635662514716
LOSS train 0.24809875844393747 valid 0.21028471085811867
LOSS train 0.24809875844393747 valid 0.21046079984006955
LOSS train 0.24809875844393747 valid 0.21036902121825568
LOSS train 0.24809875844393747 valid 0.21024803628142064
LOSS train 0.24809875844393747 valid 0.2102723031436803
LOSS train 0.24809875844393747 valid 0.210450419146596
LOSS train 0.24809875844393747 valid 0.21029402513921033
LOSS train 0.24809875844393747 valid 0.21034048034837752
LOSS train 0.24809875844393747 valid 0.21026359177985282
LOSS train 0.24809875844393747 valid 0.21030435996844357
LOSS train 0.24809875844393747 valid 0.21022083735867833
LOSS train 0.24809875844393747 valid 0.21026948611460514
LOSS train 0.24809875844393747 valid 0.21041778178684772
LOSS train 0.24809875844393747 valid 0.2102361269571163
LOSS train 0.24809875844393747 valid 0.21024411658299366
LOSS train 0.24809875844393747 valid 0.21013944470049703
LOSS train 0.24809875844393747 valid 0.2100809735583735
LOSS train 0.24809875844393747 valid 0.20997002573996565
LOSS train 0.24809875844393747 valid 0.2100268731875853
LOSS train 0.24809875844393747 valid 0.21009529457576032
LOSS train 0.24809875844393747 valid 0.2100897775660353
LOSS train 0.24809875844393747 valid 0.21005906346890565
LOSS train 0.24809875844393747 valid 0.21000974227450656
LOSS train 0.24809875844393747 valid 0.20988360441156795
LOSS train 0.24809875844393747 valid 0.20970912823897664
LOSS train 0.24809875844393747 valid 0.2096529412248456
LOSS train 0.24809875844393747 valid 0.20962009592106823
LOSS train 0.24809875844393747 valid 0.20959068887250523
LOSS train 0.24809875844393747 valid 0.20957932932335033
LOSS train 0.24809875844393747 valid 0.20955736059825736
LOSS train 0.24809875844393747 valid 0.2095673929523508
LOSS train 0.24809875844393747 valid 0.2095347549766302
LOSS train 0.24809875844393747 valid 0.20949669440106125
LOSS train 0.24809875844393747 valid 0.20938258607839716
LOSS train 0.24809875844393747 valid 0.209362036686173
LOSS train 0.24809875844393747 valid 0.2093606090606892
LOSS train 0.24809875844393747 valid 0.20933937416141757
LOSS train 0.24809875844393747 valid 0.20941133716073976
LOSS train 0.24809875844393747 valid 0.2094276412058685
LOSS train 0.24809875844393747 valid 0.20938941232255986
LOSS train 0.24809875844393747 valid 0.20931525096949505
LOSS train 0.24809875844393747 valid 0.2093596894969076
LOSS train 0.24809875844393747 valid 0.20935197772589018
LOSS train 0.24809875844393747 valid 0.20928846841057142
LOSS train 0.24809875844393747 valid 0.2091675587369754
LOSS train 0.24809875844393747 valid 0.209139078312757
LOSS train 0.24809875844393747 valid 0.20909872798636409
LOSS train 0.24809875844393747 valid 0.20911108673011003
LOSS train 0.24809875844393747 valid 0.20905322852681896
LOSS train 0.24809875844393747 valid 0.2093391088294048
LOSS train 0.24809875844393747 valid 0.2093610564544069
LOSS train 0.24809875844393747 valid 0.20938457553456355
LOSS train 0.24809875844393747 valid 0.2094173442970202
LOSS train 0.24809875844393747 valid 0.20942831260542716
LOSS train 0.24809875844393747 valid 0.20945141270421325
LOSS train 0.24809875844393747 valid 0.2094349211607224
LOSS train 0.24809875844393747 valid 0.20947115166118732
LOSS train 0.24809875844393747 valid 0.20942600555480664
LOSS train 0.24809875844393747 valid 0.20936692016465322
LOSS train 0.24809875844393747 valid 0.20930969583082804
LOSS train 0.24809875844393747 valid 0.20930806577957767
LOSS train 0.24809875844393747 valid 0.20927722588087777
LOSS train 0.24809875844393747 valid 0.20924911071140564
LOSS train 0.24809875844393747 valid 0.20910058259032666
LOSS train 0.24809875844393747 valid 0.2092337170586779
LOSS train 0.24809875844393747 valid 0.2092346517103059
LOSS train 0.24809875844393747 valid 0.20917123205521526
LOSS train 0.24809875844393747 valid 0.20920299683456067
LOSS train 0.24809875844393747 valid 0.20907834594066327
LOSS train 0.24809875844393747 valid 0.2091362079136942
LOSS train 0.24809875844393747 valid 0.20906646804889772
LOSS train 0.24809875844393747 valid 0.20902412416549718
LOSS train 0.24809875844393747 valid 0.20901835530905738
LOSS train 0.24809875844393747 valid 0.20903068550608375
LOSS train 0.24809875844393747 valid 0.20897339851654548
LOSS train 0.24809875844393747 valid 0.20896717605282025
LOSS train 0.24809875844393747 valid 0.20907884997290535
LOSS train 0.24809875844393747 valid 0.20900081905597698
LOSS train 0.24809875844393747 valid 0.20888331612544273
LOSS train 0.24809875844393747 valid 0.20883101331336157
LOSS train 0.24809875844393747 valid 0.20897907929533666
LOSS train 0.24809875844393747 valid 0.2089921248382365
LOSS train 0.24809875844393747 valid 0.20890200749083607
LOSS train 0.24809875844393747 valid 0.2090161734205835
LOSS train 0.24809875844393747 valid 0.20895129673816593
LOSS train 0.24809875844393747 valid 0.20891959438024207
LOSS train 0.24809875844393747 valid 0.2087940031081525
LOSS train 0.24809875844393747 valid 0.208878644200605
LOSS train 0.24809875844393747 valid 0.20901078387447025
LOSS train 0.24809875844393747 valid 0.20898794723039418
LOSS train 0.24809875844393747 valid 0.2088756168781852
LOSS train 0.24809875844393747 valid 0.20899571426983538
LOSS train 0.24809875844393747 valid 0.20896399580475936
LOSS train 0.24809875844393747 valid 0.20897657492331095
LOSS train 0.24809875844393747 valid 0.20904692366422073
LOSS train 0.24809875844393747 valid 0.2091214504008266
LOSS train 0.24809875844393747 valid 0.2091505343025872
LOSS train 0.24809875844393747 valid 0.20919058459290005
LOSS train 0.24809875844393747 valid 0.20904149824464824
LOSS train 0.24809875844393747 valid 0.2090819381763426
LOSS train 0.24809875844393747 valid 0.20911635971870743
LOSS train 0.24809875844393747 valid 0.20908473656830176
LOSS train 0.24809875844393747 valid 0.20899686615779206
LOSS train 0.24809875844393747 valid 0.20916858952906398
LOSS train 0.24809875844393747 valid 0.20910275964855818
LOSS train 0.24809875844393747 valid 0.20918117998713287
LOSS train 0.24809875844393747 valid 0.20926302113152076
LOSS train 0.24809875844393747 valid 0.20919557517537704
LOSS train 0.24809875844393747 valid 0.20931001105537153
LOSS train 0.24809875844393747 valid 0.2093056306405797
LOSS train 0.24809875844393747 valid 0.20924822425809803
LOSS train 0.24809875844393747 valid 0.20912474057758632
LOSS train 0.24809875844393747 valid 0.20905202328351133
EPOCH 16:
  batch 1 loss: 0.25121450424194336
  batch 2 loss: 0.2688722014427185
  batch 3 loss: 0.25452856222788495
  batch 4 loss: 0.25215186923742294
  batch 5 loss: 0.2546643316745758
  batch 6 loss: 0.25376299023628235
  batch 7 loss: 0.2540797506059919
  batch 8 loss: 0.25461258739233017
  batch 9 loss: 0.25692199336157906
  batch 10 loss: 0.25295097529888155
  batch 11 loss: 0.2540809701789509
  batch 12 loss: 0.2531990110874176
  batch 13 loss: 0.25103921500536114
  batch 14 loss: 0.2501945293375424
  batch 15 loss: 0.25058999756971995
  batch 16 loss: 0.250181064940989
  batch 17 loss: 0.25057456510908466
  batch 18 loss: 0.24805794076787102
  batch 19 loss: 0.24800294088689903
  batch 20 loss: 0.2458428367972374
  batch 21 loss: 0.24862448232514517
  batch 22 loss: 0.24881000952287155
  batch 23 loss: 0.24847823316636292
  batch 24 loss: 0.24985964285830656
  batch 25 loss: 0.25086062014102933
  batch 26 loss: 0.2503419461158606
  batch 27 loss: 0.25042648889400343
  batch 28 loss: 0.2517183018582208
  batch 29 loss: 0.2514692190392264
  batch 30 loss: 0.2518504107991854
  batch 31 loss: 0.2519218493853846
  batch 32 loss: 0.2526841019280255
  batch 33 loss: 0.25502231762264715
  batch 34 loss: 0.2554484233260155
  batch 35 loss: 0.2572529856647764
  batch 36 loss: 0.25700629047221607
  batch 37 loss: 0.2573356938523215
  batch 38 loss: 0.2576704750719823
  batch 39 loss: 0.2583742351868214
  batch 40 loss: 0.2580631460994482
  batch 41 loss: 0.257667997261373
  batch 42 loss: 0.2573893694650559
  batch 43 loss: 0.25772544949553733
  batch 44 loss: 0.25807178765535355
  batch 45 loss: 0.2572368244330088
  batch 46 loss: 0.25558484799188114
  batch 47 loss: 0.2555889262163893
  batch 48 loss: 0.25525980597982806
  batch 49 loss: 0.25584844819137026
  batch 50 loss: 0.2573621967434883
  batch 51 loss: 0.2576381006077224
  batch 52 loss: 0.2576855355157302
  batch 53 loss: 0.2572485922642474
  batch 54 loss: 0.2572787878689943
  batch 55 loss: 0.25746322707696395
  batch 56 loss: 0.25777350100023405
  batch 57 loss: 0.2571814593515898
  batch 58 loss: 0.25687802865587434
  batch 59 loss: 0.25702973442562554
  batch 60 loss: 0.2575467169284821
  batch 61 loss: 0.257881047784305
  batch 62 loss: 0.25777819031669247
  batch 63 loss: 0.2570784744762239
  batch 64 loss: 0.25656032282859087
  batch 65 loss: 0.2559942644376021
  batch 66 loss: 0.25600010563026776
  batch 67 loss: 0.25594898213201495
  batch 68 loss: 0.2560270336620948
  batch 69 loss: 0.2561668235322703
  batch 70 loss: 0.25650982856750487
  batch 71 loss: 0.25644930567539914
  batch 72 loss: 0.2565423871080081
  batch 73 loss: 0.25676099609022274
  batch 74 loss: 0.2567257949629346
  batch 75 loss: 0.2569272641340892
  batch 76 loss: 0.2571499633945917
  batch 77 loss: 0.2568180019205267
  batch 78 loss: 0.256942854095728
  batch 79 loss: 0.2569111664838429
  batch 80 loss: 0.2567620072513819
  batch 81 loss: 0.25660784395388614
  batch 82 loss: 0.2568487221511399
  batch 83 loss: 0.25638488258223935
  batch 84 loss: 0.2558010603700365
  batch 85 loss: 0.2563546292922076
  batch 86 loss: 0.25612018725206687
  batch 87 loss: 0.25586610707058305
  batch 88 loss: 0.25547929582270706
  batch 89 loss: 0.2551572989211993
  batch 90 loss: 0.2553026361597909
  batch 91 loss: 0.2551318814793786
  batch 92 loss: 0.25482580681209976
  batch 93 loss: 0.25433288338363813
  batch 94 loss: 0.25401679394727056
  batch 95 loss: 0.2538648741809945
  batch 96 loss: 0.2538922402697305
  batch 97 loss: 0.2544084008514267
  batch 98 loss: 0.25432339827625117
  batch 99 loss: 0.2539618271167832
  batch 100 loss: 0.25416341722011565
  batch 101 loss: 0.2540623111890094
  batch 102 loss: 0.254022993293463
  batch 103 loss: 0.2537681509568853
  batch 104 loss: 0.25411086730085886
  batch 105 loss: 0.25402959216208687
  batch 106 loss: 0.2539513444281974
  batch 107 loss: 0.2538676779960918
  batch 108 loss: 0.2534809258800966
  batch 109 loss: 0.25357230704858763
  batch 110 loss: 0.2535885434259068
  batch 111 loss: 0.2534526426781405
  batch 112 loss: 0.25321576012564556
  batch 113 loss: 0.25299988581543476
  batch 114 loss: 0.2529144332858554
  batch 115 loss: 0.2526022708934286
  batch 116 loss: 0.25239729650061704
  batch 117 loss: 0.2522911507095027
  batch 118 loss: 0.2522855765991292
  batch 119 loss: 0.25234457086615203
  batch 120 loss: 0.25213926943639914
  batch 121 loss: 0.25211114725790734
  batch 122 loss: 0.25183090543160674
  batch 123 loss: 0.25155410691490016
  batch 124 loss: 0.2517516633435603
  batch 125 loss: 0.251383243560791
  batch 126 loss: 0.2514474148315097
  batch 127 loss: 0.2515841776461113
  batch 128 loss: 0.25166915846057236
  batch 129 loss: 0.2518782719623211
  batch 130 loss: 0.2517851885694724
  batch 131 loss: 0.251730187362387
  batch 132 loss: 0.2518369123113878
  batch 133 loss: 0.2517186402154148
  batch 134 loss: 0.2515666160120893
  batch 135 loss: 0.2514350893320861
  batch 136 loss: 0.25160672449890303
  batch 137 loss: 0.2519635129584013
  batch 138 loss: 0.25192912883948587
  batch 139 loss: 0.25191581388600437
  batch 140 loss: 0.25229309339608463
  batch 141 loss: 0.25203716871163523
  batch 142 loss: 0.25173616325351555
  batch 143 loss: 0.25195900022566736
  batch 144 loss: 0.25191464989135665
  batch 145 loss: 0.2518831364039717
  batch 146 loss: 0.2520267022390888
  batch 147 loss: 0.2519077304877391
  batch 148 loss: 0.25181292712285713
  batch 149 loss: 0.25135391730590156
  batch 150 loss: 0.2514972635110219
  batch 151 loss: 0.2512288020541336
  batch 152 loss: 0.2509830607787559
  batch 153 loss: 0.2512285060742322
  batch 154 loss: 0.2509977385014683
  batch 155 loss: 0.2509226163548808
  batch 156 loss: 0.25078105229215747
  batch 157 loss: 0.25066034628707135
  batch 158 loss: 0.25076249835989145
  batch 159 loss: 0.2509401578385875
  batch 160 loss: 0.25092552779242394
  batch 161 loss: 0.2506013563515977
  batch 162 loss: 0.25050028772265825
  batch 163 loss: 0.25027397914898175
  batch 164 loss: 0.2504916834395106
  batch 165 loss: 0.2506256244399331
  batch 166 loss: 0.2508578686470009
  batch 167 loss: 0.25076836793722507
  batch 168 loss: 0.2505742728355385
  batch 169 loss: 0.2505209551760431
  batch 170 loss: 0.2504536978462163
  batch 171 loss: 0.2505404181473436
  batch 172 loss: 0.2504949256951033
  batch 173 loss: 0.2504294006466177
  batch 174 loss: 0.25059090674608603
  batch 175 loss: 0.2506927331856319
  batch 176 loss: 0.2509705249897458
  batch 177 loss: 0.25122948942211387
  batch 178 loss: 0.2511302526579814
  batch 179 loss: 0.25109650727423877
  batch 180 loss: 0.25107547053032453
  batch 181 loss: 0.2510397339558733
  batch 182 loss: 0.2510757507694947
  batch 183 loss: 0.25102068630398294
  batch 184 loss: 0.2509327421531729
  batch 185 loss: 0.2507192570615459
  batch 186 loss: 0.25068173237064834
  batch 187 loss: 0.25080104761901384
  batch 188 loss: 0.2507039006561675
  batch 189 loss: 0.2506492107160508
  batch 190 loss: 0.25047372273708646
  batch 191 loss: 0.25056090677907955
  batch 192 loss: 0.25042683390590054
  batch 193 loss: 0.2504316538263479
  batch 194 loss: 0.25033610575285153
  batch 195 loss: 0.2504261992680721
  batch 196 loss: 0.25030941044797705
  batch 197 loss: 0.2503468984274695
  batch 198 loss: 0.2503537397191982
  batch 199 loss: 0.2503916654155482
  batch 200 loss: 0.2503724586963654
  batch 201 loss: 0.25016777076531405
  batch 202 loss: 0.2503024960803513
  batch 203 loss: 0.2502885946792922
  batch 204 loss: 0.2504311794159459
  batch 205 loss: 0.25039699964407014
  batch 206 loss: 0.25051033164112313
  batch 207 loss: 0.25041763311710913
  batch 208 loss: 0.2502406725230125
  batch 209 loss: 0.2503026522802964
  batch 210 loss: 0.25031632028874895
  batch 211 loss: 0.2502513965724204
  batch 212 loss: 0.2502337599278621
  batch 213 loss: 0.2500112575824272
  batch 214 loss: 0.2498499941185256
  batch 215 loss: 0.24986344347166461
  batch 216 loss: 0.24976876288376473
  batch 217 loss: 0.24969938088397275
  batch 218 loss: 0.24960841559762253
  batch 219 loss: 0.24931690185309546
  batch 220 loss: 0.2494509167969227
  batch 221 loss: 0.24934928530481607
  batch 222 loss: 0.24938519540670756
  batch 223 loss: 0.24930609862900635
  batch 224 loss: 0.24929048167541623
  batch 225 loss: 0.24931472877661387
  batch 226 loss: 0.24943318442169543
  batch 227 loss: 0.24945080759504293
  batch 228 loss: 0.24947097773353258
  batch 229 loss: 0.2494163315369052
  batch 230 loss: 0.2494224497805471
  batch 231 loss: 0.24934685668904027
  batch 232 loss: 0.24931891159764652
  batch 233 loss: 0.24931781435217468
  batch 234 loss: 0.2492746433131715
  batch 235 loss: 0.24915456670395872
  batch 236 loss: 0.2491290985401404
  batch 237 loss: 0.24897433892835544
  batch 238 loss: 0.2488637058800008
  batch 239 loss: 0.24885389364663527
  batch 240 loss: 0.24896123427897693
  batch 241 loss: 0.24881482891027362
  batch 242 loss: 0.2487378632726748
  batch 243 loss: 0.24876852462321153
  batch 244 loss: 0.2486771086200339
  batch 245 loss: 0.24863520124737098
  batch 246 loss: 0.2485380673675033
  batch 247 loss: 0.24851960661681557
  batch 248 loss: 0.24862545806794398
  batch 249 loss: 0.2486207236846288
  batch 250 loss: 0.24862445878982545
  batch 251 loss: 0.24845463191845502
  batch 252 loss: 0.24857416547953137
  batch 253 loss: 0.24846520289602017
  batch 254 loss: 0.2484404939835466
  batch 255 loss: 0.24828940717612996
  batch 256 loss: 0.24820676684612408
  batch 257 loss: 0.24816842696082267
  batch 258 loss: 0.2481176541872727
  batch 259 loss: 0.2479429120722885
  batch 260 loss: 0.24798487356075874
  batch 261 loss: 0.2479872567101
  batch 262 loss: 0.24788925051689148
  batch 263 loss: 0.24780709282300317
  batch 264 loss: 0.24782089036748264
  batch 265 loss: 0.24768742441006428
  batch 266 loss: 0.24749671987124852
  batch 267 loss: 0.24731354354026167
  batch 268 loss: 0.24717152947142942
  batch 269 loss: 0.24720645820562726
  batch 270 loss: 0.2471152420949053
  batch 271 loss: 0.2470813680069033
  batch 272 loss: 0.2469700743060778
  batch 273 loss: 0.24679900580273442
  batch 274 loss: 0.24658182693006347
  batch 275 loss: 0.24653342718427831
  batch 276 loss: 0.24652437811744385
  batch 277 loss: 0.24646267623032042
  batch 278 loss: 0.24633627685068324
  batch 279 loss: 0.24641134966445224
  batch 280 loss: 0.24638566779238838
  batch 281 loss: 0.24636417521276507
  batch 282 loss: 0.2462456523315281
  batch 283 loss: 0.2462143837984375
  batch 284 loss: 0.24611952087618935
  batch 285 loss: 0.24606895661144926
  batch 286 loss: 0.24608193468172235
  batch 287 loss: 0.24591803618216765
  batch 288 loss: 0.24593302767930758
  batch 289 loss: 0.24583917887153098
  batch 290 loss: 0.2457219926447704
  batch 291 loss: 0.24567473810358145
  batch 292 loss: 0.24556929163940966
  batch 293 loss: 0.24557482357114654
  batch 294 loss: 0.24533031379081766
  batch 295 loss: 0.24530880097615518
  batch 296 loss: 0.24541569581708392
  batch 297 loss: 0.24542874559408887
  batch 298 loss: 0.24541508361397174
  batch 299 loss: 0.24547965500665747
  batch 300 loss: 0.24534497355421384
  batch 301 loss: 0.2453337583529989
  batch 302 loss: 0.24546955639360757
  batch 303 loss: 0.2454376839863585
  batch 304 loss: 0.2453336164257244
  batch 305 loss: 0.24529152292697157
  batch 306 loss: 0.24537483758591358
  batch 307 loss: 0.24533664010440875
  batch 308 loss: 0.24539016975791422
  batch 309 loss: 0.24533448983164666
  batch 310 loss: 0.24540762795555976
  batch 311 loss: 0.24533056484541327
  batch 312 loss: 0.24535280609360108
  batch 313 loss: 0.24535847881350653
  batch 314 loss: 0.2453195408556112
  batch 315 loss: 0.2452731862900749
  batch 316 loss: 0.24527473361054553
  batch 317 loss: 0.24540705616917896
  batch 318 loss: 0.2452227183670368
  batch 319 loss: 0.24501848211482774
  batch 320 loss: 0.24518489530310034
  batch 321 loss: 0.24523699330020918
  batch 322 loss: 0.2451848334962537
  batch 323 loss: 0.2451364370783785
  batch 324 loss: 0.24524216947180252
  batch 325 loss: 0.2454292651781669
  batch 326 loss: 0.24553082138299942
  batch 327 loss: 0.24549374522054596
  batch 328 loss: 0.24547338022328005
  batch 329 loss: 0.24556375301717626
  batch 330 loss: 0.24556421000849116
  batch 331 loss: 0.24556500764051592
  batch 332 loss: 0.24569874624890017
  batch 333 loss: 0.2457458481595323
  batch 334 loss: 0.24607360782380588
  batch 335 loss: 0.24617272651017602
  batch 336 loss: 0.24623247431147666
  batch 337 loss: 0.24627747769171832
  batch 338 loss: 0.24623490768600498
  batch 339 loss: 0.2462072161007068
  batch 340 loss: 0.24606220437323345
  batch 341 loss: 0.2460474338699296
  batch 342 loss: 0.24601269748650098
  batch 343 loss: 0.24614671893613332
  batch 344 loss: 0.24623913226952387
  batch 345 loss: 0.24638991437960361
  batch 346 loss: 0.24639582125782278
  batch 347 loss: 0.2462607262715139
  batch 348 loss: 0.24625108271152124
  batch 349 loss: 0.24632725742963119
  batch 350 loss: 0.24647351443767548
  batch 351 loss: 0.24643700487083858
  batch 352 loss: 0.24632959072054786
  batch 353 loss: 0.24630465909374336
  batch 354 loss: 0.2462449958600567
  batch 355 loss: 0.24616739758303469
  batch 356 loss: 0.24621111455928074
  batch 357 loss: 0.24606834823677853
  batch 358 loss: 0.24597580335493194
  batch 359 loss: 0.2459204830333051
  batch 360 loss: 0.24586813694073095
  batch 361 loss: 0.24583313562533202
  batch 362 loss: 0.2458310064278255
  batch 363 loss: 0.24589554497525712
  batch 364 loss: 0.24580560243882976
  batch 365 loss: 0.2458286791223369
  batch 366 loss: 0.24581999453853387
  batch 367 loss: 0.2458616083015863
  batch 368 loss: 0.24582920883498763
  batch 369 loss: 0.24571326867674748
  batch 370 loss: 0.2457111474227261
  batch 371 loss: 0.24575442971405637
  batch 372 loss: 0.24584682794508114
  batch 373 loss: 0.2458900583573385
  batch 374 loss: 0.245788801919649
  batch 375 loss: 0.2458558141787847
  batch 376 loss: 0.24601699931031845
  batch 377 loss: 0.24599858430715707
  batch 378 loss: 0.2460074486240508
  batch 379 loss: 0.24598775025565264
  batch 380 loss: 0.24595330757530112
  batch 381 loss: 0.2458720759065758
  batch 382 loss: 0.2458826442351516
  batch 383 loss: 0.24574679237432953
  batch 384 loss: 0.24570746902221194
  batch 385 loss: 0.2456666059308238
  batch 386 loss: 0.24565096323045424
  batch 387 loss: 0.2456059348059563
  batch 388 loss: 0.2454785194209556
  batch 389 loss: 0.24544833922294235
  batch 390 loss: 0.2454638726818256
  batch 391 loss: 0.24535945236987775
  batch 392 loss: 0.24546248583617258
  batch 393 loss: 0.2454796216945915
  batch 394 loss: 0.24544587285082958
  batch 395 loss: 0.2454570305875585
  batch 396 loss: 0.24550943283571136
  batch 397 loss: 0.24558333279353844
  batch 398 loss: 0.24566136798517188
  batch 399 loss: 0.24563655152655484
  batch 400 loss: 0.2457886788994074
  batch 401 loss: 0.24584017192336388
  batch 402 loss: 0.24589299799791023
  batch 403 loss: 0.24587680159076566
  batch 404 loss: 0.24586088070184878
  batch 405 loss: 0.2458708902935923
  batch 406 loss: 0.24574323398579517
  batch 407 loss: 0.24580191654274447
  batch 408 loss: 0.24571403630954378
  batch 409 loss: 0.245814740329908
  batch 410 loss: 0.2456658373882131
  batch 411 loss: 0.24566078001130237
  batch 412 loss: 0.24575020376484372
  batch 413 loss: 0.24570240975147875
  batch 414 loss: 0.24575592602652627
  batch 415 loss: 0.24575191535145405
  batch 416 loss: 0.24578407349494788
  batch 417 loss: 0.24575725919622884
  batch 418 loss: 0.24576669154269842
  batch 419 loss: 0.24575679786592225
  batch 420 loss: 0.24575824159241857
  batch 421 loss: 0.2458531716019977
  batch 422 loss: 0.24580475620890116
  batch 423 loss: 0.24576424883048867
  batch 424 loss: 0.24581849455552282
  batch 425 loss: 0.24573760572601766
  batch 426 loss: 0.24569505767923006
  batch 427 loss: 0.24567899097463844
  batch 428 loss: 0.24569500805200817
  batch 429 loss: 0.2455321155436389
  batch 430 loss: 0.24552715167749758
  batch 431 loss: 0.24555933603416186
  batch 432 loss: 0.24560518207511417
  batch 433 loss: 0.24558539926042183
  batch 434 loss: 0.24550656909080146
  batch 435 loss: 0.245547251009393
  batch 436 loss: 0.24552206401157817
  batch 437 loss: 0.24544846493264902
  batch 438 loss: 0.24537634128304922
  batch 439 loss: 0.2453070957052409
  batch 440 loss: 0.24536565027453683
  batch 441 loss: 0.24538191700197942
  batch 442 loss: 0.2453941977941073
  batch 443 loss: 0.24537007968124364
  batch 444 loss: 0.24544007092848555
  batch 445 loss: 0.24540636224693127
  batch 446 loss: 0.24539813826971524
  batch 447 loss: 0.24537628722404206
  batch 448 loss: 0.24546886993838207
  batch 449 loss: 0.24539178948359924
  batch 450 loss: 0.24537524220016269
  batch 451 loss: 0.2453380335130607
  batch 452 loss: 0.24524711481237835
  batch 453 loss: 0.24522837269516706
  batch 454 loss: 0.24522111873400895
  batch 455 loss: 0.24519724744361834
  batch 456 loss: 0.2451312093292935
  batch 457 loss: 0.24510566358754096
  batch 458 loss: 0.24507350129721986
  batch 459 loss: 0.24501455339860811
  batch 460 loss: 0.244970713523419
  batch 461 loss: 0.24498132255873814
  batch 462 loss: 0.2450196925889362
  batch 463 loss: 0.24502377878255
  batch 464 loss: 0.2449355621918522
  batch 465 loss: 0.2449321021315872
  batch 466 loss: 0.24485386210961405
  batch 467 loss: 0.24500901764159028
  batch 468 loss: 0.24498002912498948
  batch 469 loss: 0.2449127977400192
  batch 470 loss: 0.2449574128427404
  batch 471 loss: 0.24498523189636806
  batch 472 loss: 0.24481192524781673
LOSS train 0.24481192524781673 valid 0.22646170854568481
LOSS train 0.24481192524781673 valid 0.22161667793989182
LOSS train 0.24481192524781673 valid 0.2292687644561132
LOSS train 0.24481192524781673 valid 0.21435773745179176
LOSS train 0.24481192524781673 valid 0.21441373527050017
LOSS train 0.24481192524781673 valid 0.21939032276471457
LOSS train 0.24481192524781673 valid 0.21432994306087494
LOSS train 0.24481192524781673 valid 0.21190509386360645
LOSS train 0.24481192524781673 valid 0.21080870926380157
LOSS train 0.24481192524781673 valid 0.20891666561365127
LOSS train 0.24481192524781673 valid 0.20805985954674808
LOSS train 0.24481192524781673 valid 0.21129440640409788
LOSS train 0.24481192524781673 valid 0.21181413875176355
LOSS train 0.24481192524781673 valid 0.21058176777192525
LOSS train 0.24481192524781673 valid 0.21046768526236218
LOSS train 0.24481192524781673 valid 0.21356033813208342
LOSS train 0.24481192524781673 valid 0.2146717783282785
LOSS train 0.24481192524781673 valid 0.21480770740244123
LOSS train 0.24481192524781673 valid 0.21729027440673426
LOSS train 0.24481192524781673 valid 0.21640455722808838
LOSS train 0.24481192524781673 valid 0.2177578139872778
LOSS train 0.24481192524781673 valid 0.21758448806675998
LOSS train 0.24481192524781673 valid 0.21542317452638046
LOSS train 0.24481192524781673 valid 0.21580403670668602
LOSS train 0.24481192524781673 valid 0.21577155232429504
LOSS train 0.24481192524781673 valid 0.21538723432100737
LOSS train 0.24481192524781673 valid 0.2149253448954335
LOSS train 0.24481192524781673 valid 0.2151023388973304
LOSS train 0.24481192524781673 valid 0.21391843824551024
LOSS train 0.24481192524781673 valid 0.21350405663251876
LOSS train 0.24481192524781673 valid 0.21376934214945761
LOSS train 0.24481192524781673 valid 0.21359872305765748
LOSS train 0.24481192524781673 valid 0.21247092882792154
LOSS train 0.24481192524781673 valid 0.21211383973850922
LOSS train 0.24481192524781673 valid 0.21254770713193075
LOSS train 0.24481192524781673 valid 0.21311330257190597
LOSS train 0.24481192524781673 valid 0.2134986801727398
LOSS train 0.24481192524781673 valid 0.21365600118511602
LOSS train 0.24481192524781673 valid 0.21468726641092545
LOSS train 0.24481192524781673 valid 0.2151904206722975
LOSS train 0.24481192524781673 valid 0.21489795533622183
LOSS train 0.24481192524781673 valid 0.21629145599546887
LOSS train 0.24481192524781673 valid 0.21672125853771387
LOSS train 0.24481192524781673 valid 0.21636315367438577
LOSS train 0.24481192524781673 valid 0.21588084002335867
LOSS train 0.24481192524781673 valid 0.21558669403843259
LOSS train 0.24481192524781673 valid 0.2154462347005276
LOSS train 0.24481192524781673 valid 0.21696252344797054
LOSS train 0.24481192524781673 valid 0.21621050609617817
LOSS train 0.24481192524781673 valid 0.21673520863056184
LOSS train 0.24481192524781673 valid 0.2162816784545487
LOSS train 0.24481192524781673 valid 0.2159917182647265
LOSS train 0.24481192524781673 valid 0.217250812166142
LOSS train 0.24481192524781673 valid 0.21734274951396165
LOSS train 0.24481192524781673 valid 0.21722032048485496
LOSS train 0.24481192524781673 valid 0.21732303580003126
LOSS train 0.24481192524781673 valid 0.21677344383900626
LOSS train 0.24481192524781673 valid 0.21749214339872885
LOSS train 0.24481192524781673 valid 0.2172165932291645
LOSS train 0.24481192524781673 valid 0.21708164513111114
LOSS train 0.24481192524781673 valid 0.21708482772600454
LOSS train 0.24481192524781673 valid 0.21670746683113037
LOSS train 0.24481192524781673 valid 0.21651156459535872
LOSS train 0.24481192524781673 valid 0.21705494774505496
LOSS train 0.24481192524781673 valid 0.21600672006607055
LOSS train 0.24481192524781673 valid 0.21580493675939966
LOSS train 0.24481192524781673 valid 0.21647217602872137
LOSS train 0.24481192524781673 valid 0.21603019272579865
LOSS train 0.24481192524781673 valid 0.21678945119830145
LOSS train 0.24481192524781673 valid 0.2172742622239249
LOSS train 0.24481192524781673 valid 0.21774979334482006
LOSS train 0.24481192524781673 valid 0.21840710358487236
LOSS train 0.24481192524781673 valid 0.21896279837987195
LOSS train 0.24481192524781673 valid 0.21854952138823433
LOSS train 0.24481192524781673 valid 0.2182513741652171
LOSS train 0.24481192524781673 valid 0.21843906805703514
LOSS train 0.24481192524781673 valid 0.2184143124462722
LOSS train 0.24481192524781673 valid 0.2181933036981485
LOSS train 0.24481192524781673 valid 0.2179819117241268
LOSS train 0.24481192524781673 valid 0.21770079098641873
LOSS train 0.24481192524781673 valid 0.21791968007146575
LOSS train 0.24481192524781673 valid 0.2177166411789452
LOSS train 0.24481192524781673 valid 0.217956086777779
LOSS train 0.24481192524781673 valid 0.2177047213273389
LOSS train 0.24481192524781673 valid 0.21855828919831444
LOSS train 0.24481192524781673 valid 0.21852683847726778
LOSS train 0.24481192524781673 valid 0.21827010793247442
LOSS train 0.24481192524781673 valid 0.2186093982309103
LOSS train 0.24481192524781673 valid 0.21899044429988004
LOSS train 0.24481192524781673 valid 0.21932560205459595
LOSS train 0.24481192524781673 valid 0.2193022337588635
LOSS train 0.24481192524781673 valid 0.21933770195945448
LOSS train 0.24481192524781673 valid 0.21916842716996388
LOSS train 0.24481192524781673 valid 0.2194615182407359
LOSS train 0.24481192524781673 valid 0.21975913926174767
LOSS train 0.24481192524781673 valid 0.2198581307505568
LOSS train 0.24481192524781673 valid 0.21989970047449328
LOSS train 0.24481192524781673 valid 0.22031603054124482
LOSS train 0.24481192524781673 valid 0.22059497538239065
LOSS train 0.24481192524781673 valid 0.22066984727978706
LOSS train 0.24481192524781673 valid 0.22075138720545437
LOSS train 0.24481192524781673 valid 0.22109056001200394
LOSS train 0.24481192524781673 valid 0.22096017364738058
LOSS train 0.24481192524781673 valid 0.22082343339346922
LOSS train 0.24481192524781673 valid 0.22109668240660713
LOSS train 0.24481192524781673 valid 0.22121355851303856
LOSS train 0.24481192524781673 valid 0.22089005672486028
LOSS train 0.24481192524781673 valid 0.22104604845797574
LOSS train 0.24481192524781673 valid 0.22060325919488155
LOSS train 0.24481192524781673 valid 0.2205384320833466
LOSS train 0.24481192524781673 valid 0.22071305964444135
LOSS train 0.24481192524781673 valid 0.22098664992621966
LOSS train 0.24481192524781673 valid 0.22069634483978812
LOSS train 0.24481192524781673 valid 0.22091891002236752
LOSS train 0.24481192524781673 valid 0.22154899965161862
LOSS train 0.24481192524781673 valid 0.22128560817961035
LOSS train 0.24481192524781673 valid 0.22197363557469132
LOSS train 0.24481192524781673 valid 0.22178738425343725
LOSS train 0.24481192524781673 valid 0.22161119362386336
LOSS train 0.24481192524781673 valid 0.22129066276053588
LOSS train 0.24481192524781673 valid 0.2210535966167765
LOSS train 0.24481192524781673 valid 0.22121839054295275
LOSS train 0.24481192524781673 valid 0.22123894592126211
LOSS train 0.24481192524781673 valid 0.22152890108766093
LOSS train 0.24481192524781673 valid 0.22149770057201384
LOSS train 0.24481192524781673 valid 0.2217164338817672
LOSS train 0.24481192524781673 valid 0.221502200238348
LOSS train 0.24481192524781673 valid 0.22155279631260782
LOSS train 0.24481192524781673 valid 0.22127284733361977
LOSS train 0.24481192524781673 valid 0.2210035217496065
LOSS train 0.24481192524781673 valid 0.22099989209011311
LOSS train 0.24481192524781673 valid 0.22099398257154407
LOSS train 0.24481192524781673 valid 0.22101810081560808
LOSS train 0.24481192524781673 valid 0.22124720153523914
LOSS train 0.24481192524781673 valid 0.2213762033868719
LOSS train 0.24481192524781673 valid 0.22132377501796274
LOSS train 0.24481192524781673 valid 0.2213389096903975
LOSS train 0.24481192524781673 valid 0.22116558880045795
LOSS train 0.24481192524781673 valid 0.22101481001582934
LOSS train 0.24481192524781673 valid 0.22093591062085968
LOSS train 0.24481192524781673 valid 0.2210410383153469
LOSS train 0.24481192524781673 valid 0.22132457603870984
LOSS train 0.24481192524781673 valid 0.2214229085020252
LOSS train 0.24481192524781673 valid 0.22151178887320888
LOSS train 0.24481192524781673 valid 0.22148224978611386
LOSS train 0.24481192524781673 valid 0.22135293983841595
LOSS train 0.24481192524781673 valid 0.22123961961593758
LOSS train 0.24481192524781673 valid 0.22120580087239677
LOSS train 0.24481192524781673 valid 0.2212780360007446
LOSS train 0.24481192524781673 valid 0.22121709605058035
LOSS train 0.24481192524781673 valid 0.22115542280752928
LOSS train 0.24481192524781673 valid 0.22114455807758004
LOSS train 0.24481192524781673 valid 0.22092153151440463
LOSS train 0.24481192524781673 valid 0.2209915625197547
LOSS train 0.24481192524781673 valid 0.2211080462701859
LOSS train 0.24481192524781673 valid 0.22111332263702002
LOSS train 0.24481192524781673 valid 0.22109707307284046
LOSS train 0.24481192524781673 valid 0.2212398437005055
LOSS train 0.24481192524781673 valid 0.22133768495148834
LOSS train 0.24481192524781673 valid 0.22154641887173057
LOSS train 0.24481192524781673 valid 0.22151739275233345
LOSS train 0.24481192524781673 valid 0.22164383668590476
LOSS train 0.24481192524781673 valid 0.22142819868274993
LOSS train 0.24481192524781673 valid 0.22145892888671015
LOSS train 0.24481192524781673 valid 0.22161139114336534
LOSS train 0.24481192524781673 valid 0.22154875216354808
LOSS train 0.24481192524781673 valid 0.2215302947930947
LOSS train 0.24481192524781673 valid 0.22140439608622164
LOSS train 0.24481192524781673 valid 0.22115290482368696
LOSS train 0.24481192524781673 valid 0.22124777997241302
LOSS train 0.24481192524781673 valid 0.22118364309358318
LOSS train 0.24481192524781673 valid 0.22119856123314346
LOSS train 0.24481192524781673 valid 0.22103354653526594
LOSS train 0.24481192524781673 valid 0.2213911128969028
LOSS train 0.24481192524781673 valid 0.22132842140538353
LOSS train 0.24481192524781673 valid 0.22123052340678193
LOSS train 0.24481192524781673 valid 0.22133056112265181
LOSS train 0.24481192524781673 valid 0.22144779805721862
LOSS train 0.24481192524781673 valid 0.22166239890639342
LOSS train 0.24481192524781673 valid 0.22146085641450353
LOSS train 0.24481192524781673 valid 0.22145948266785448
LOSS train 0.24481192524781673 valid 0.22130509539619908
LOSS train 0.24481192524781673 valid 0.22139169129191852
LOSS train 0.24481192524781673 valid 0.22123739892697852
LOSS train 0.24481192524781673 valid 0.22106537432283968
LOSS train 0.24481192524781673 valid 0.22111174095702427
LOSS train 0.24481192524781673 valid 0.22104669772686167
LOSS train 0.24481192524781673 valid 0.2211166116944019
LOSS train 0.24481192524781673 valid 0.22091299303309628
LOSS train 0.24481192524781673 valid 0.22092053811801107
LOSS train 0.24481192524781673 valid 0.22106250153162094
LOSS train 0.24481192524781673 valid 0.22110106330364943
LOSS train 0.24481192524781673 valid 0.2208830864009462
LOSS train 0.24481192524781673 valid 0.22074196272596872
LOSS train 0.24481192524781673 valid 0.22065194386702316
LOSS train 0.24481192524781673 valid 0.2207497217673428
LOSS train 0.24481192524781673 valid 0.2206876751432564
LOSS train 0.24481192524781673 valid 0.2209011876221859
LOSS train 0.24481192524781673 valid 0.22087357421616213
LOSS train 0.24481192524781673 valid 0.22102659165859223
LOSS train 0.24481192524781673 valid 0.2208138512289939
LOSS train 0.24481192524781673 valid 0.22077088965342775
LOSS train 0.24481192524781673 valid 0.22085123461455547
LOSS train 0.24481192524781673 valid 0.22070267562772714
LOSS train 0.24481192524781673 valid 0.22062490444357802
LOSS train 0.24481192524781673 valid 0.22063097408384952
LOSS train 0.24481192524781673 valid 0.22062645233483705
LOSS train 0.24481192524781673 valid 0.22050735289947346
LOSS train 0.24481192524781673 valid 0.2205609776899575
LOSS train 0.24481192524781673 valid 0.2207225692414102
LOSS train 0.24481192524781673 valid 0.220953411329979
LOSS train 0.24481192524781673 valid 0.2207529808412183
LOSS train 0.24481192524781673 valid 0.22070517107634477
LOSS train 0.24481192524781673 valid 0.2206587457907534
LOSS train 0.24481192524781673 valid 0.22039532834707304
LOSS train 0.24481192524781673 valid 0.2201691427854476
LOSS train 0.24481192524781673 valid 0.2199422957435731
LOSS train 0.24481192524781673 valid 0.21992649452401958
LOSS train 0.24481192524781673 valid 0.21987719071782344
LOSS train 0.24481192524781673 valid 0.21992881210012868
LOSS train 0.24481192524781673 valid 0.21989950180323414
LOSS train 0.24481192524781673 valid 0.21998374989709338
LOSS train 0.24481192524781673 valid 0.22011276164129712
LOSS train 0.24481192524781673 valid 0.22000670665875077
LOSS train 0.24481192524781673 valid 0.2198726001713011
LOSS train 0.24481192524781673 valid 0.21973980343447322
LOSS train 0.24481192524781673 valid 0.21974088809563724
LOSS train 0.24481192524781673 valid 0.21972721592898956
LOSS train 0.24481192524781673 valid 0.2196060991703683
LOSS train 0.24481192524781673 valid 0.21963163504134053
LOSS train 0.24481192524781673 valid 0.21954135119398951
LOSS train 0.24481192524781673 valid 0.21968900521510634
LOSS train 0.24481192524781673 valid 0.22004109729768892
LOSS train 0.24481192524781673 valid 0.2199810153653479
LOSS train 0.24481192524781673 valid 0.22002676738069413
LOSS train 0.24481192524781673 valid 0.21992821612600552
LOSS train 0.24481192524781673 valid 0.22002734193570503
LOSS train 0.24481192524781673 valid 0.22002862631773748
LOSS train 0.24481192524781673 valid 0.21999645563598458
LOSS train 0.24481192524781673 valid 0.21963119494418304
LOSS train 0.24481192524781673 valid 0.21952213989995822
LOSS train 0.24481192524781673 valid 0.21945409347449452
LOSS train 0.24481192524781673 valid 0.2192617955276505
LOSS train 0.24481192524781673 valid 0.2192463005175356
LOSS train 0.24481192524781673 valid 0.21936034535875126
LOSS train 0.24481192524781673 valid 0.21940560366322354
LOSS train 0.24481192524781673 valid 0.21943005304104887
LOSS train 0.24481192524781673 valid 0.21938240371884837
LOSS train 0.24481192524781673 valid 0.2192022397096856
LOSS train 0.24481192524781673 valid 0.21925788062810897
LOSS train 0.24481192524781673 valid 0.2192921498382234
LOSS train 0.24481192524781673 valid 0.21917746535369328
LOSS train 0.24481192524781673 valid 0.2189978106101982
LOSS train 0.24481192524781673 valid 0.21912446553547552
LOSS train 0.24481192524781673 valid 0.21904195950311772
LOSS train 0.24481192524781673 valid 0.2188409722293727
LOSS train 0.24481192524781673 valid 0.21886473221537667
LOSS train 0.24481192524781673 valid 0.21905105416626894
LOSS train 0.24481192524781673 valid 0.21893706152567993
LOSS train 0.24481192524781673 valid 0.2187714275259238
LOSS train 0.24481192524781673 valid 0.2188141087348434
LOSS train 0.24481192524781673 valid 0.21897721182526522
LOSS train 0.24481192524781673 valid 0.21882913910390758
LOSS train 0.24481192524781673 valid 0.2189107123536594
LOSS train 0.24481192524781673 valid 0.21882113465723002
LOSS train 0.24481192524781673 valid 0.21887471454035967
LOSS train 0.24481192524781673 valid 0.2188187867737888
LOSS train 0.24481192524781673 valid 0.2188910977609122
LOSS train 0.24481192524781673 valid 0.2190190380833849
LOSS train 0.24481192524781673 valid 0.21887089985388297
LOSS train 0.24481192524781673 valid 0.21888313131798678
LOSS train 0.24481192524781673 valid 0.21877724244533217
LOSS train 0.24481192524781673 valid 0.2187056574738506
LOSS train 0.24481192524781673 valid 0.21859827844330865
LOSS train 0.24481192524781673 valid 0.2186453018947081
LOSS train 0.24481192524781673 valid 0.21873170245384824
LOSS train 0.24481192524781673 valid 0.21874164679635733
LOSS train 0.24481192524781673 valid 0.2187629013503198
LOSS train 0.24481192524781673 valid 0.21871692129147097
LOSS train 0.24481192524781673 valid 0.21859559996851852
LOSS train 0.24481192524781673 valid 0.2184361005699083
LOSS train 0.24481192524781673 valid 0.21838139695055941
LOSS train 0.24481192524781673 valid 0.218354906978961
LOSS train 0.24481192524781673 valid 0.21834203537920832
LOSS train 0.24481192524781673 valid 0.21836908830885302
LOSS train 0.24481192524781673 valid 0.21837327967990527
LOSS train 0.24481192524781673 valid 0.2183777328567638
LOSS train 0.24481192524781673 valid 0.21835803876941404
LOSS train 0.24481192524781673 valid 0.2182698718507397
LOSS train 0.24481192524781673 valid 0.218125127975283
LOSS train 0.24481192524781673 valid 0.21810866252253555
LOSS train 0.24481192524781673 valid 0.21809339727440927
LOSS train 0.24481192524781673 valid 0.21808995698092334
LOSS train 0.24481192524781673 valid 0.21811932974121198
LOSS train 0.24481192524781673 valid 0.21813471701185583
LOSS train 0.24481192524781673 valid 0.2180865149240236
LOSS train 0.24481192524781673 valid 0.21802812523713416
LOSS train 0.24481192524781673 valid 0.21808468005401177
LOSS train 0.24481192524781673 valid 0.21806911152342093
LOSS train 0.24481192524781673 valid 0.2180228939652443
LOSS train 0.24481192524781673 valid 0.21791136576685793
LOSS train 0.24481192524781673 valid 0.2178948259610214
LOSS train 0.24481192524781673 valid 0.21782191737060105
LOSS train 0.24481192524781673 valid 0.21784576376605974
LOSS train 0.24481192524781673 valid 0.21779291126571718
LOSS train 0.24481192524781673 valid 0.21805073647132886
LOSS train 0.24481192524781673 valid 0.2180380766380105
LOSS train 0.24481192524781673 valid 0.21808854485680532
LOSS train 0.24481192524781673 valid 0.21811240245995012
LOSS train 0.24481192524781673 valid 0.21813931441114795
LOSS train 0.24481192524781673 valid 0.21815701408784874
LOSS train 0.24481192524781673 valid 0.21816007554149017
LOSS train 0.24481192524781673 valid 0.2182232028188797
LOSS train 0.24481192524781673 valid 0.2181799658544504
LOSS train 0.24481192524781673 valid 0.21810705964527433
LOSS train 0.24481192524781673 valid 0.21805591282399395
LOSS train 0.24481192524781673 valid 0.2180867265169553
LOSS train 0.24481192524781673 valid 0.21808032199459257
LOSS train 0.24481192524781673 valid 0.21804491115214308
LOSS train 0.24481192524781673 valid 0.21791909150779248
LOSS train 0.24481192524781673 valid 0.2180300820096631
LOSS train 0.24481192524781673 valid 0.2180392850444924
LOSS train 0.24481192524781673 valid 0.2179530571119704
LOSS train 0.24481192524781673 valid 0.21797290757114504
LOSS train 0.24481192524781673 valid 0.21782810986042023
LOSS train 0.24481192524781673 valid 0.21789059673715955
LOSS train 0.24481192524781673 valid 0.21781035902303295
LOSS train 0.24481192524781673 valid 0.21778734069226718
LOSS train 0.24481192524781673 valid 0.21778172082451702
LOSS train 0.24481192524781673 valid 0.21776463682904387
LOSS train 0.24481192524781673 valid 0.2176882428345003
LOSS train 0.24481192524781673 valid 0.21767128344221287
LOSS train 0.24481192524781673 valid 0.2178171963931562
LOSS train 0.24481192524781673 valid 0.21773454538005554
LOSS train 0.24481192524781673 valid 0.21760495576395916
LOSS train 0.24481192524781673 valid 0.21752108642387957
LOSS train 0.24481192524781673 valid 0.21766788165364137
LOSS train 0.24481192524781673 valid 0.21767855109371378
LOSS train 0.24481192524781673 valid 0.21758666309283548
LOSS train 0.24481192524781673 valid 0.2177008502623614
LOSS train 0.24481192524781673 valid 0.21762906613587635
LOSS train 0.24481192524781673 valid 0.21758820543512267
LOSS train 0.24481192524781673 valid 0.2174837080973578
LOSS train 0.24481192524781673 valid 0.2175759098730808
LOSS train 0.24481192524781673 valid 0.21767822426298392
LOSS train 0.24481192524781673 valid 0.21761447738188539
LOSS train 0.24481192524781673 valid 0.21751680021155465
LOSS train 0.24481192524781673 valid 0.2176174011466832
LOSS train 0.24481192524781673 valid 0.2175676982775117
LOSS train 0.24481192524781673 valid 0.21758120681558338
LOSS train 0.24481192524781673 valid 0.21764862087377457
LOSS train 0.24481192524781673 valid 0.2176907479509034
LOSS train 0.24481192524781673 valid 0.21771374180195352
LOSS train 0.24481192524781673 valid 0.21775397170061445
LOSS train 0.24481192524781673 valid 0.21760587553742905
LOSS train 0.24481192524781673 valid 0.2176458491117097
LOSS train 0.24481192524781673 valid 0.21768579446301073
LOSS train 0.24481192524781673 valid 0.21763961691430161
LOSS train 0.24481192524781673 valid 0.2175525365018579
LOSS train 0.24481192524781673 valid 0.21771806383298503
LOSS train 0.24481192524781673 valid 0.21763354139479904
LOSS train 0.24481192524781673 valid 0.2176837939666121
LOSS train 0.24481192524781673 valid 0.21775863551568064
LOSS train 0.24481192524781673 valid 0.21767027356794902
LOSS train 0.24481192524781673 valid 0.21778989728182963
LOSS train 0.24481192524781673 valid 0.21778503931610962
LOSS train 0.24481192524781673 valid 0.2177274078130722
LOSS train 0.24481192524781673 valid 0.2176155996630373
LOSS train 0.24481192524781673 valid 0.21757507138459017
EPOCH 17:
  batch 1 loss: 0.2567894756793976
  batch 2 loss: 0.26859839260578156
  batch 3 loss: 0.2536979814370473
  batch 4 loss: 0.25203273445367813
  batch 5 loss: 0.2585907161235809
  batch 6 loss: 0.25637974341710407
  batch 7 loss: 0.2557275891304016
  batch 8 loss: 0.2548874355852604
  batch 9 loss: 0.25694484843148124
  batch 10 loss: 0.2516824007034302
  batch 11 loss: 0.2521269673650915
  batch 12 loss: 0.2517893972496192
  batch 13 loss: 0.24977405140033135
  batch 14 loss: 0.24908517194645746
  batch 15 loss: 0.24952301681041716
  batch 16 loss: 0.2496525002643466
  batch 17 loss: 0.25079214835868163
  batch 18 loss: 0.24756201439433628
  batch 19 loss: 0.24651183109534414
  batch 20 loss: 0.24495791792869567
  batch 21 loss: 0.24779220280193148
  batch 22 loss: 0.24834516102617438
  batch 23 loss: 0.2481356820334559
  batch 24 loss: 0.24965122093756995
  batch 25 loss: 0.2504748249053955
  batch 26 loss: 0.2498498885677411
  batch 27 loss: 0.24999771349959904
  batch 28 loss: 0.2510822292949472
  batch 29 loss: 0.2515551905179846
  batch 30 loss: 0.25231192658344903
  batch 31 loss: 0.25194889979977764
  batch 32 loss: 0.25322796404361725
  batch 33 loss: 0.2556389281243989
  batch 34 loss: 0.2560421692974427
  batch 35 loss: 0.257422433580671
  batch 36 loss: 0.2568553454346127
  batch 37 loss: 0.2571988363523741
  batch 38 loss: 0.2583267900504564
  batch 39 loss: 0.2591140339007744
  batch 40 loss: 0.2587782897055149
  batch 41 loss: 0.2585843547088344
  batch 42 loss: 0.2583444728737786
  batch 43 loss: 0.25894504300383636
  batch 44 loss: 0.2598177208141847
  batch 45 loss: 0.25903704199526045
  batch 46 loss: 0.2577394363672837
  batch 47 loss: 0.25803547019654133
  batch 48 loss: 0.2577624376863241
  batch 49 loss: 0.25813013011095476
  batch 50 loss: 0.25942125618457795
  batch 51 loss: 0.2599560881362242
  batch 52 loss: 0.2600783115396133
  batch 53 loss: 0.259682925804606
  batch 54 loss: 0.2597113141307124
  batch 55 loss: 0.2599468550898812
  batch 56 loss: 0.2600568559552942
  batch 57 loss: 0.2593076574174981
  batch 58 loss: 0.2590113341294486
  batch 59 loss: 0.25913858388440086
  batch 60 loss: 0.25945075526833533
  batch 61 loss: 0.26003456824138516
  batch 62 loss: 0.2602285923015687
  batch 63 loss: 0.25920938191905857
  batch 64 loss: 0.2586516330484301
  batch 65 loss: 0.2582918279446088
  batch 66 loss: 0.25816909901120444
  batch 67 loss: 0.25799632717424364
  batch 68 loss: 0.2580336825374295
  batch 69 loss: 0.2579986101043397
  batch 70 loss: 0.25797150624649867
  batch 71 loss: 0.2579916616980459
  batch 72 loss: 0.257929766136739
  batch 73 loss: 0.2580377963716037
  batch 74 loss: 0.257817249644447
  batch 75 loss: 0.2577588524421056
  batch 76 loss: 0.2579396314134723
  batch 77 loss: 0.25767588286430804
  batch 78 loss: 0.25773213746456
  batch 79 loss: 0.25764009186738657
  batch 80 loss: 0.2574636394158006
  batch 81 loss: 0.25717898706595105
  batch 82 loss: 0.2573900293649697
  batch 83 loss: 0.2568004154656307
  batch 84 loss: 0.2560221015342644
  batch 85 loss: 0.2565298688762328
  batch 86 loss: 0.2562898222443669
  batch 87 loss: 0.25601971457744466
  batch 88 loss: 0.2555283240296624
  batch 89 loss: 0.25523613460278244
  batch 90 loss: 0.2552407514717844
  batch 91 loss: 0.2549325003401264
  batch 92 loss: 0.2545597608322683
  batch 93 loss: 0.2540094729072304
  batch 94 loss: 0.2536571170104311
  batch 95 loss: 0.2535583031804938
  batch 96 loss: 0.25359677833815414
  batch 97 loss: 0.25402845396209006
  batch 98 loss: 0.25380639336547073
  batch 99 loss: 0.2533563495886446
  batch 100 loss: 0.25360246777534484
  batch 101 loss: 0.25354856813308035
  batch 102 loss: 0.2535031862995204
  batch 103 loss: 0.253373544627023
  batch 104 loss: 0.25358722344613993
  batch 105 loss: 0.253386432216281
  batch 106 loss: 0.2533484359957137
  batch 107 loss: 0.2533139976385598
  batch 108 loss: 0.25291232488773485
  batch 109 loss: 0.25301465856919597
  batch 110 loss: 0.25306545983661305
  batch 111 loss: 0.25298970830333123
  batch 112 loss: 0.25280500549290863
  batch 113 loss: 0.2526810394715419
  batch 114 loss: 0.2525583368382956
  batch 115 loss: 0.25225222214408544
  batch 116 loss: 0.25202706571796846
  batch 117 loss: 0.25191275329671353
  batch 118 loss: 0.2518923310405117
  batch 119 loss: 0.25205014082564026
  batch 120 loss: 0.2519107344249884
  batch 121 loss: 0.25175462983363917
  batch 122 loss: 0.25161063768824593
  batch 123 loss: 0.2512696866339784
  batch 124 loss: 0.25126165908671194
  batch 125 loss: 0.2508074709177017
  batch 126 loss: 0.2508424349010937
  batch 127 loss: 0.25101014326407217
  batch 128 loss: 0.2509645096724853
  batch 129 loss: 0.25116626086623167
  batch 130 loss: 0.25102625420460334
  batch 131 loss: 0.2508719822832646
  batch 132 loss: 0.2509425711451155
  batch 133 loss: 0.2507442589989282
  batch 134 loss: 0.2505441194594796
  batch 135 loss: 0.2503129115811101
  batch 136 loss: 0.2505444868960801
  batch 137 loss: 0.25080135986752755
  batch 138 loss: 0.2507188796349194
  batch 139 loss: 0.25067515992861
  batch 140 loss: 0.25096411098326954
  batch 141 loss: 0.25070570081683763
  batch 142 loss: 0.2503559774076435
  batch 143 loss: 0.25045550765691105
  batch 144 loss: 0.25036081413014066
  batch 145 loss: 0.2503029529390664
  batch 146 loss: 0.2504367583418546
  batch 147 loss: 0.25036369385767954
  batch 148 loss: 0.2502955105659124
  batch 149 loss: 0.24986260809354335
  batch 150 loss: 0.2499077840646108
  batch 151 loss: 0.24954847803968466
  batch 152 loss: 0.24935147636815122
  batch 153 loss: 0.24956848477226456
  batch 154 loss: 0.24943848825120307
  batch 155 loss: 0.2494449750069649
  batch 156 loss: 0.24936974115478686
  batch 157 loss: 0.24928100701350314
  batch 158 loss: 0.24937453130377998
  batch 159 loss: 0.24966981444718703
  batch 160 loss: 0.2496502467431128
  batch 161 loss: 0.24938208068379705
  batch 162 loss: 0.24930328167515037
  batch 163 loss: 0.24918542994311982
  batch 164 loss: 0.2493112623328116
  batch 165 loss: 0.24932428526155875
  batch 166 loss: 0.24960146395556898
  batch 167 loss: 0.24950922267165726
  batch 168 loss: 0.2492407633080369
  batch 169 loss: 0.2491788833275349
  batch 170 loss: 0.24914349282489104
  batch 171 loss: 0.24915897811365406
  batch 172 loss: 0.2491178372225096
  batch 173 loss: 0.24903272307677077
  batch 174 loss: 0.24913564564167767
  batch 175 loss: 0.2491979193687439
  batch 176 loss: 0.24948832003230398
  batch 177 loss: 0.2497316602259706
  batch 178 loss: 0.24960006371642765
  batch 179 loss: 0.24960427510671776
  batch 180 loss: 0.24956343124310176
  batch 181 loss: 0.24953383561326653
  batch 182 loss: 0.2495588862470218
  batch 183 loss: 0.2495019354455458
  batch 184 loss: 0.24944498252285563
  batch 185 loss: 0.24918383529057375
  batch 186 loss: 0.24916130584734741
  batch 187 loss: 0.24928946920575942
  batch 188 loss: 0.24922524780986158
  batch 189 loss: 0.24918266287241034
  batch 190 loss: 0.24892443794953195
  batch 191 loss: 0.24894058298690155
  batch 192 loss: 0.248734205806007
  batch 193 loss: 0.2487006228359252
  batch 194 loss: 0.24857269573150223
  batch 195 loss: 0.24867417025260435
  batch 196 loss: 0.24852302936570986
  batch 197 loss: 0.248574372399882
  batch 198 loss: 0.2485159332252512
  batch 199 loss: 0.2485945578945342
  batch 200 loss: 0.24854926332831384
  batch 201 loss: 0.24839779868054745
  batch 202 loss: 0.24851939982116814
  batch 203 loss: 0.24849927410703573
  batch 204 loss: 0.24861111112085044
  batch 205 loss: 0.2485296293729689
  batch 206 loss: 0.2486253796705922
  batch 207 loss: 0.24863546388448723
  batch 208 loss: 0.24842182768938634
  batch 209 loss: 0.24846413097027956
  batch 210 loss: 0.24850176125764847
  batch 211 loss: 0.2484816877056637
  batch 212 loss: 0.24849188405106654
  batch 213 loss: 0.24833485379185474
  batch 214 loss: 0.24826381487824092
  batch 215 loss: 0.24820820266424223
  batch 216 loss: 0.24808889282522378
  batch 217 loss: 0.24794670197820884
  batch 218 loss: 0.24777493379805066
  batch 219 loss: 0.247499869906739
  batch 220 loss: 0.24770938896320083
  batch 221 loss: 0.2476038857283096
  batch 222 loss: 0.24757607402028264
  batch 223 loss: 0.2474696628181389
  batch 224 loss: 0.2474572703109256
  batch 225 loss: 0.24750369290510812
  batch 226 loss: 0.24752051343696307
  batch 227 loss: 0.24756136490647487
  batch 228 loss: 0.24758638814091682
  batch 229 loss: 0.24759103279707212
  batch 230 loss: 0.24755383418953936
  batch 231 loss: 0.247533456845717
  batch 232 loss: 0.24750222034495453
  batch 233 loss: 0.24757497684126759
  batch 234 loss: 0.2475128992755189
  batch 235 loss: 0.24739552831396144
  batch 236 loss: 0.247383813926224
  batch 237 loss: 0.24722732861333757
  batch 238 loss: 0.24711397690933293
  batch 239 loss: 0.24710682059431674
  batch 240 loss: 0.24722539174060026
  batch 241 loss: 0.24704600617104053
  batch 242 loss: 0.2469147356950547
  batch 243 loss: 0.24688887050367678
  batch 244 loss: 0.24675604120874015
  batch 245 loss: 0.24666658019532964
  batch 246 loss: 0.24660549349174266
  batch 247 loss: 0.24653659730787703
  batch 248 loss: 0.24665502482844937
  batch 249 loss: 0.24662378897149878
  batch 250 loss: 0.24666595458984375
  batch 251 loss: 0.24651368027664275
  batch 252 loss: 0.24657762559160354
  batch 253 loss: 0.2464761914825251
  batch 254 loss: 0.24643722293883796
  batch 255 loss: 0.24629274384648192
  batch 256 loss: 0.2462983060395345
  batch 257 loss: 0.24630761819127006
  batch 258 loss: 0.24631474941044815
  batch 259 loss: 0.24614902162874067
  batch 260 loss: 0.2461680718912528
  batch 261 loss: 0.24623907520168128
  batch 262 loss: 0.2461475935259848
  batch 263 loss: 0.2460461843603011
  batch 264 loss: 0.24604600205114394
  batch 265 loss: 0.24594147767660754
  batch 266 loss: 0.24578361599741125
  batch 267 loss: 0.24558178187532817
  batch 268 loss: 0.2454453533265128
  batch 269 loss: 0.2455069737363482
  batch 270 loss: 0.24541416587652984
  batch 271 loss: 0.24535555026847938
  batch 272 loss: 0.2452210959266214
  batch 273 loss: 0.24504711356136824
  batch 274 loss: 0.2448636044333451
  batch 275 loss: 0.2448296582698822
  batch 276 loss: 0.24478616101154382
  batch 277 loss: 0.2446973683171324
  batch 278 loss: 0.24453606831727268
  batch 279 loss: 0.2445360210634047
  batch 280 loss: 0.24448144510388375
  batch 281 loss: 0.24444787941369298
  batch 282 loss: 0.24430248157141057
  batch 283 loss: 0.24428025182902602
  batch 284 loss: 0.244233023690086
  batch 285 loss: 0.24419291244264235
  batch 286 loss: 0.24419840133481927
  batch 287 loss: 0.2440883909264508
  batch 288 loss: 0.24409187538549304
  batch 289 loss: 0.24396732392814333
  batch 290 loss: 0.24386745645054456
  batch 291 loss: 0.24382321500696266
  batch 292 loss: 0.24369068469290864
  batch 293 loss: 0.24368584964665943
  batch 294 loss: 0.2434046695731124
  batch 295 loss: 0.24342436068138834
  batch 296 loss: 0.2435017371600544
  batch 297 loss: 0.24349165434387798
  batch 298 loss: 0.24346103324186083
  batch 299 loss: 0.24346813410022186
  batch 300 loss: 0.2433941346903642
  batch 301 loss: 0.24338744630251216
  batch 302 loss: 0.24351679671876478
  batch 303 loss: 0.24346318836062655
  batch 304 loss: 0.24340819498818173
  batch 305 loss: 0.24336803951224342
  batch 306 loss: 0.2434469686906322
  batch 307 loss: 0.24342201533651509
  batch 308 loss: 0.24346530210081632
  batch 309 loss: 0.24346930955605986
  batch 310 loss: 0.24348361035508495
  batch 311 loss: 0.2434058439501612
  batch 312 loss: 0.24348069832493097
  batch 313 loss: 0.24349423495534891
  batch 314 loss: 0.24344533980841848
  batch 315 loss: 0.2433725081265919
  batch 316 loss: 0.2433428312121313
  batch 317 loss: 0.24346002894057084
  batch 318 loss: 0.24319671028814976
  batch 319 loss: 0.24297189525675997
  batch 320 loss: 0.24310111040249466
  batch 321 loss: 0.2431578698373658
  batch 322 loss: 0.2430757522490454
  batch 323 loss: 0.24303161009177335
  batch 324 loss: 0.24309594082979508
  batch 325 loss: 0.243208758189128
  batch 326 loss: 0.24332990163674384
  batch 327 loss: 0.24326027132320113
  batch 328 loss: 0.24324411326428738
  batch 329 loss: 0.2433192995362731
  batch 330 loss: 0.24332413709524905
  batch 331 loss: 0.24331463387185354
  batch 332 loss: 0.24344974949237813
  batch 333 loss: 0.24349578431955687
  batch 334 loss: 0.24380242989627188
  batch 335 loss: 0.2438748406385308
  batch 336 loss: 0.24391813146039135
  batch 337 loss: 0.24395075878156045
  batch 338 loss: 0.24389661817684682
  batch 339 loss: 0.24385869428295653
  batch 340 loss: 0.24372246177757487
  batch 341 loss: 0.24368146158033802
  batch 342 loss: 0.24367813264940216
  batch 343 loss: 0.2437971664847855
  batch 344 loss: 0.24390285414491975
  batch 345 loss: 0.24406534100788227
  batch 346 loss: 0.24409613465470387
  batch 347 loss: 0.24397540096762543
  batch 348 loss: 0.2439906119700821
  batch 349 loss: 0.24407957801675387
  batch 350 loss: 0.2442324520434652
  batch 351 loss: 0.24421881793061553
  batch 352 loss: 0.24411156972531567
  batch 353 loss: 0.24408863873218342
  batch 354 loss: 0.24405535495718994
  batch 355 loss: 0.24395548713878845
  batch 356 loss: 0.24401415235708268
  batch 357 loss: 0.24383657623906763
  batch 358 loss: 0.243692516739475
  batch 359 loss: 0.24360199969957133
  batch 360 loss: 0.24355399240222242
  batch 361 loss: 0.24351585229182837
  batch 362 loss: 0.24350578971824594
  batch 363 loss: 0.24353315921361782
  batch 364 loss: 0.2434467979512372
  batch 365 loss: 0.24345979951832392
  batch 366 loss: 0.24345688200224944
  batch 367 loss: 0.24349816354646345
  batch 368 loss: 0.24346188710921485
  batch 369 loss: 0.2433879746573404
  batch 370 loss: 0.2433944433927536
  batch 371 loss: 0.24346192900061286
  batch 372 loss: 0.24352216087682274
  batch 373 loss: 0.243544443443058
  batch 374 loss: 0.24348833846217172
  batch 375 loss: 0.24357313474019368
  batch 376 loss: 0.24371673507576294
  batch 377 loss: 0.2437008624446803
  batch 378 loss: 0.24370023415044503
  batch 379 loss: 0.24371105790295514
  batch 380 loss: 0.24365995032222648
  batch 381 loss: 0.24358631431899985
  batch 382 loss: 0.243611187366915
  batch 383 loss: 0.24345870078699397
  batch 384 loss: 0.24342181040750197
  batch 385 loss: 0.24335682721107038
  batch 386 loss: 0.2433090417650697
  batch 387 loss: 0.2432533541083028
  batch 388 loss: 0.2431176186913682
  batch 389 loss: 0.24307769150078143
  batch 390 loss: 0.2430410800453944
  batch 391 loss: 0.24298002443197744
  batch 392 loss: 0.2430246602743864
  batch 393 loss: 0.24306500576076315
  batch 394 loss: 0.24304730104764705
  batch 395 loss: 0.24305844506885432
  batch 396 loss: 0.2430781377761653
  batch 397 loss: 0.2431368674304083
  batch 398 loss: 0.24325904650754065
  batch 399 loss: 0.24322033021832468
  batch 400 loss: 0.24333368193358182
  batch 401 loss: 0.2434005370535458
  batch 402 loss: 0.24342265276618263
  batch 403 loss: 0.24339436867248923
  batch 404 loss: 0.24338076761600994
  batch 405 loss: 0.243409659501947
  batch 406 loss: 0.24330526560985397
  batch 407 loss: 0.24336199641813516
  batch 408 loss: 0.24326938167944842
  batch 409 loss: 0.24329631416751005
  batch 410 loss: 0.2431509799346691
  batch 411 loss: 0.24313795196748997
  batch 412 loss: 0.24325609539897697
  batch 413 loss: 0.24324214498153898
  batch 414 loss: 0.24331372152060127
  batch 415 loss: 0.24327481651162527
  batch 416 loss: 0.2432874019544285
  batch 417 loss: 0.24323362140632648
  batch 418 loss: 0.2432209421216586
  batch 419 loss: 0.243215744563674
  batch 420 loss: 0.24321677656400773
  batch 421 loss: 0.2433303671041747
  batch 422 loss: 0.24326096422158147
  batch 423 loss: 0.2432365534082936
  batch 424 loss: 0.24328599782344304
  batch 425 loss: 0.24321517919792848
  batch 426 loss: 0.24317503034928595
  batch 427 loss: 0.24314859177943415
  batch 428 loss: 0.24317412109714803
  batch 429 loss: 0.2430066882799833
  batch 430 loss: 0.24300040585357088
  batch 431 loss: 0.24303173559032848
  batch 432 loss: 0.24308526415929751
  batch 433 loss: 0.2430745505364332
  batch 434 loss: 0.24301444539963374
  batch 435 loss: 0.2431047296729581
  batch 436 loss: 0.24310937294856125
  batch 437 loss: 0.24302833170994467
  batch 438 loss: 0.2430004493490746
  batch 439 loss: 0.24293400788497274
  batch 440 loss: 0.24301007962362334
  batch 441 loss: 0.2430344070233996
  batch 442 loss: 0.24302810352986753
  batch 443 loss: 0.24303766314789618
  batch 444 loss: 0.24305894813156342
  batch 445 loss: 0.24302957898445343
  batch 446 loss: 0.24301182157923823
  batch 447 loss: 0.24296166006083988
  batch 448 loss: 0.24305071236033524
  batch 449 loss: 0.24297761435896356
  batch 450 loss: 0.242946700586213
  batch 451 loss: 0.2429388776058104
  batch 452 loss: 0.24285973661241278
  batch 453 loss: 0.24285427656921066
  batch 454 loss: 0.24284816654768285
  batch 455 loss: 0.24281586352940443
  batch 456 loss: 0.24275387317072927
  batch 457 loss: 0.2427192387421752
  batch 458 loss: 0.2426870089170714
  batch 459 loss: 0.24264521916317783
  batch 460 loss: 0.24256173064527303
  batch 461 loss: 0.24254997504507383
  batch 462 loss: 0.24256412343029335
  batch 463 loss: 0.24254942905825622
  batch 464 loss: 0.24247631745348716
  batch 465 loss: 0.24247797278947728
  batch 466 loss: 0.242394825035922
  batch 467 loss: 0.2425791905829942
  batch 468 loss: 0.24249889227187532
  batch 469 loss: 0.2424193165068433
  batch 470 loss: 0.24246696519090774
  batch 471 loss: 0.242485809161658
  batch 472 loss: 0.24230470271560095
LOSS train 0.24230470271560095 valid 0.21901696920394897
LOSS train 0.24230470271560095 valid 0.2075292244553566
LOSS train 0.24230470271560095 valid 0.2153495748837789
LOSS train 0.24230470271560095 valid 0.19931265711784363
LOSS train 0.24230470271560095 valid 0.19740183651447296
LOSS train 0.24230470271560095 valid 0.20087421188751856
LOSS train 0.24230470271560095 valid 0.1965352658714567
LOSS train 0.24230470271560095 valid 0.19428339414298534
LOSS train 0.24230470271560095 valid 0.19329845077461666
LOSS train 0.24230470271560095 valid 0.19139862656593323
LOSS train 0.24230470271560095 valid 0.19093818420713599
LOSS train 0.24230470271560095 valid 0.19349868347247443
LOSS train 0.24230470271560095 valid 0.19348522676871374
LOSS train 0.24230470271560095 valid 0.19237671686070307
LOSS train 0.24230470271560095 valid 0.19241519471009572
LOSS train 0.24230470271560095 valid 0.19528941251337528
LOSS train 0.24230470271560095 valid 0.1958248992176617
LOSS train 0.24230470271560095 valid 0.1954500956667794
LOSS train 0.24230470271560095 valid 0.19793164259509036
LOSS train 0.24230470271560095 valid 0.1974618837237358
LOSS train 0.24230470271560095 valid 0.1985345184803009
LOSS train 0.24230470271560095 valid 0.1981230154633522
LOSS train 0.24230470271560095 valid 0.19632023443346439
LOSS train 0.24230470271560095 valid 0.1960504005352656
LOSS train 0.24230470271560095 valid 0.19613437175750734
LOSS train 0.24230470271560095 valid 0.19541427091910288
LOSS train 0.24230470271560095 valid 0.19517305493354797
LOSS train 0.24230470271560095 valid 0.19568049322281564
LOSS train 0.24230470271560095 valid 0.19457587651137648
LOSS train 0.24230470271560095 valid 0.1940282235542933
LOSS train 0.24230470271560095 valid 0.1941679177745696
LOSS train 0.24230470271560095 valid 0.19400168024003506
LOSS train 0.24230470271560095 valid 0.19298148832537912
LOSS train 0.24230470271560095 valid 0.19270635527722976
LOSS train 0.24230470271560095 valid 0.19317298403808048
LOSS train 0.24230470271560095 valid 0.19374804405702484
LOSS train 0.24230470271560095 valid 0.19414639432688016
LOSS train 0.24230470271560095 valid 0.19420691344298815
LOSS train 0.24230470271560095 valid 0.19508365255135757
LOSS train 0.24230470271560095 valid 0.1955056067556143
LOSS train 0.24230470271560095 valid 0.19527329686211375
LOSS train 0.24230470271560095 valid 0.19656982379300253
LOSS train 0.24230470271560095 valid 0.19697801630164302
LOSS train 0.24230470271560095 valid 0.19678233157504688
LOSS train 0.24230470271560095 valid 0.1963662733634313
LOSS train 0.24230470271560095 valid 0.19619755284941715
LOSS train 0.24230470271560095 valid 0.19577675804178765
LOSS train 0.24230470271560095 valid 0.19749467447400093
LOSS train 0.24230470271560095 valid 0.19678630299714148
LOSS train 0.24230470271560095 valid 0.19746757954359054
LOSS train 0.24230470271560095 valid 0.1971168900821723
LOSS train 0.24230470271560095 valid 0.19695736725743002
LOSS train 0.24230470271560095 valid 0.19813884513558083
LOSS train 0.24230470271560095 valid 0.19814343584908378
LOSS train 0.24230470271560095 valid 0.1980091555552049
LOSS train 0.24230470271560095 valid 0.1980671060404607
LOSS train 0.24230470271560095 valid 0.19749191011253156
LOSS train 0.24230470271560095 valid 0.19811121734051867
LOSS train 0.24230470271560095 valid 0.19770778159973984
LOSS train 0.24230470271560095 valid 0.19744849701722464
LOSS train 0.24230470271560095 valid 0.1974557677253348
LOSS train 0.24230470271560095 valid 0.19730709877706343
LOSS train 0.24230470271560095 valid 0.1969548019151839
LOSS train 0.24230470271560095 valid 0.19753309665247798
LOSS train 0.24230470271560095 valid 0.19645492617900556
LOSS train 0.24230470271560095 valid 0.19629920161131656
LOSS train 0.24230470271560095 valid 0.19693597506231336
LOSS train 0.24230470271560095 valid 0.19638874951530905
LOSS train 0.24230470271560095 valid 0.19712349210960278
LOSS train 0.24230470271560095 valid 0.19770421854087283
LOSS train 0.24230470271560095 valid 0.19810395551399446
LOSS train 0.24230470271560095 valid 0.1986981433712774
LOSS train 0.24230470271560095 valid 0.1991670652203364
LOSS train 0.24230470271560095 valid 0.1988832269971435
LOSS train 0.24230470271560095 valid 0.19862694621086122
LOSS train 0.24230470271560095 valid 0.1988424913663613
LOSS train 0.24230470271560095 valid 0.19875644708608653
LOSS train 0.24230470271560095 valid 0.19864803075026244
LOSS train 0.24230470271560095 valid 0.1984238798105264
LOSS train 0.24230470271560095 valid 0.1981666289269924
LOSS train 0.24230470271560095 valid 0.19839588911445052
LOSS train 0.24230470271560095 valid 0.19819015773331247
LOSS train 0.24230470271560095 valid 0.19842589763273677
LOSS train 0.24230470271560095 valid 0.19813895491617067
LOSS train 0.24230470271560095 valid 0.19889232849373537
LOSS train 0.24230470271560095 valid 0.19891775694004324
LOSS train 0.24230470271560095 valid 0.19866815894499593
LOSS train 0.24230470271560095 valid 0.19895698130130768
LOSS train 0.24230470271560095 valid 0.19933257143149216
LOSS train 0.24230470271560095 valid 0.19968334254291323
LOSS train 0.24230470271560095 valid 0.1996526533103251
LOSS train 0.24230470271560095 valid 0.19972311510987903
LOSS train 0.24230470271560095 valid 0.19955267060187556
LOSS train 0.24230470271560095 valid 0.19968049085520684
LOSS train 0.24230470271560095 valid 0.1998680072395425
LOSS train 0.24230470271560095 valid 0.20000428070003787
LOSS train 0.24230470271560095 valid 0.20006406168962262
LOSS train 0.24230470271560095 valid 0.20056334700511427
LOSS train 0.24230470271560095 valid 0.2007378071847588
LOSS train 0.24230470271560095 valid 0.20081125438213349
LOSS train 0.24230470271560095 valid 0.20089636403735323
LOSS train 0.24230470271560095 valid 0.2012211543087866
LOSS train 0.24230470271560095 valid 0.20115503509646482
LOSS train 0.24230470271560095 valid 0.2010773393110587
LOSS train 0.24230470271560095 valid 0.20134361074084328
LOSS train 0.24230470271560095 valid 0.2014546810456042
LOSS train 0.24230470271560095 valid 0.20113206104697468
LOSS train 0.24230470271560095 valid 0.2012380615428642
LOSS train 0.24230470271560095 valid 0.2008548392615187
LOSS train 0.24230470271560095 valid 0.2007947249846025
LOSS train 0.24230470271560095 valid 0.20093638107583328
LOSS train 0.24230470271560095 valid 0.20119014395666973
LOSS train 0.24230470271560095 valid 0.20094762399660804
LOSS train 0.24230470271560095 valid 0.20107479066702358
LOSS train 0.24230470271560095 valid 0.20171422608520673
LOSS train 0.24230470271560095 valid 0.20144028460671162
LOSS train 0.24230470271560095 valid 0.20207991877682188
LOSS train 0.24230470271560095 valid 0.20188723896014488
LOSS train 0.24230470271560095 valid 0.2017037311271459
LOSS train 0.24230470271560095 valid 0.20133206335206827
LOSS train 0.24230470271560095 valid 0.20109212644829239
LOSS train 0.24230470271560095 valid 0.2012876909775812
LOSS train 0.24230470271560095 valid 0.20130271761398005
LOSS train 0.24230470271560095 valid 0.20158756079692994
LOSS train 0.24230470271560095 valid 0.20152622306346893
LOSS train 0.24230470271560095 valid 0.20172774235880564
LOSS train 0.24230470271560095 valid 0.20148756149716265
LOSS train 0.24230470271560095 valid 0.2014692040393129
LOSS train 0.24230470271560095 valid 0.2011347994092823
LOSS train 0.24230470271560095 valid 0.20088407534819383
LOSS train 0.24230470271560095 valid 0.2008910454411543
LOSS train 0.24230470271560095 valid 0.20084836503321474
LOSS train 0.24230470271560095 valid 0.20089643140484517
LOSS train 0.24230470271560095 valid 0.2011521276253373
LOSS train 0.24230470271560095 valid 0.20128606337088126
LOSS train 0.24230470271560095 valid 0.20125725580488935
LOSS train 0.24230470271560095 valid 0.20128218731740966
LOSS train 0.24230470271560095 valid 0.2011120886258457
LOSS train 0.24230470271560095 valid 0.20097761499367173
LOSS train 0.24230470271560095 valid 0.2008972065789359
LOSS train 0.24230470271560095 valid 0.20095082430552083
LOSS train 0.24230470271560095 valid 0.20119125102187546
LOSS train 0.24230470271560095 valid 0.20128341112937126
LOSS train 0.24230470271560095 valid 0.20134988954911628
LOSS train 0.24230470271560095 valid 0.20133000273129037
LOSS train 0.24230470271560095 valid 0.20119819657443322
LOSS train 0.24230470271560095 valid 0.20108636889327952
LOSS train 0.24230470271560095 valid 0.20100888811253212
LOSS train 0.24230470271560095 valid 0.20107565020154786
LOSS train 0.24230470271560095 valid 0.2010435491800308
LOSS train 0.24230470271560095 valid 0.2009691550636923
LOSS train 0.24230470271560095 valid 0.20097978099396355
LOSS train 0.24230470271560095 valid 0.20079711488648957
LOSS train 0.24230470271560095 valid 0.2009040719115889
LOSS train 0.24230470271560095 valid 0.20095226197473465
LOSS train 0.24230470271560095 valid 0.2009250976336308
LOSS train 0.24230470271560095 valid 0.2008829205089314
LOSS train 0.24230470271560095 valid 0.20101126594633995
LOSS train 0.24230470271560095 valid 0.20109520327744992
LOSS train 0.24230470271560095 valid 0.20126457344740628
LOSS train 0.24230470271560095 valid 0.20119241333526114
LOSS train 0.24230470271560095 valid 0.2013189908530977
LOSS train 0.24230470271560095 valid 0.20111093985522452
LOSS train 0.24230470271560095 valid 0.20112067760854233
LOSS train 0.24230470271560095 valid 0.20124306281407675
LOSS train 0.24230470271560095 valid 0.2011722944766642
LOSS train 0.24230470271560095 valid 0.20116451382637024
LOSS train 0.24230470271560095 valid 0.20106885289507254
LOSS train 0.24230470271560095 valid 0.20084166394535607
LOSS train 0.24230470271560095 valid 0.20091960587922264
LOSS train 0.24230470271560095 valid 0.2008538706260815
LOSS train 0.24230470271560095 valid 0.2008752362845942
LOSS train 0.24230470271560095 valid 0.20073136179088857
LOSS train 0.24230470271560095 valid 0.20109088008088627
LOSS train 0.24230470271560095 valid 0.20104182268892015
LOSS train 0.24230470271560095 valid 0.20094450283795595
LOSS train 0.24230470271560095 valid 0.20098920334867165
LOSS train 0.24230470271560095 valid 0.20112778120831157
LOSS train 0.24230470271560095 valid 0.20130616043533026
LOSS train 0.24230470271560095 valid 0.2011766028073099
LOSS train 0.24230470271560095 valid 0.20121130627163208
LOSS train 0.24230470271560095 valid 0.20108019364076657
LOSS train 0.24230470271560095 valid 0.20113497337356942
LOSS train 0.24230470271560095 valid 0.20100000654549702
LOSS train 0.24230470271560095 valid 0.20082783054661107
LOSS train 0.24230470271560095 valid 0.20091977266855138
LOSS train 0.24230470271560095 valid 0.20082816975639467
LOSS train 0.24230470271560095 valid 0.2008426972526185
LOSS train 0.24230470271560095 valid 0.20063501998545633
LOSS train 0.24230470271560095 valid 0.20061219624782864
LOSS train 0.24230470271560095 valid 0.2007339613911993
LOSS train 0.24230470271560095 valid 0.20075682542907694
LOSS train 0.24230470271560095 valid 0.2005715346243715
LOSS train 0.24230470271560095 valid 0.20045667172399992
LOSS train 0.24230470271560095 valid 0.20040721366038688
LOSS train 0.24230470271560095 valid 0.2004638851753303
LOSS train 0.24230470271560095 valid 0.2004384546715596
LOSS train 0.24230470271560095 valid 0.20066748115450445
LOSS train 0.24230470271560095 valid 0.20062956655744332
LOSS train 0.24230470271560095 valid 0.20075234077870846
LOSS train 0.24230470271560095 valid 0.2005392359708672
LOSS train 0.24230470271560095 valid 0.2004996548489769
LOSS train 0.24230470271560095 valid 0.20055100057512668
LOSS train 0.24230470271560095 valid 0.20039810086874402
LOSS train 0.24230470271560095 valid 0.20033150424317617
LOSS train 0.24230470271560095 valid 0.20035760010619766
LOSS train 0.24230470271560095 valid 0.20034075719146913
LOSS train 0.24230470271560095 valid 0.20023081723887187
LOSS train 0.24230470271560095 valid 0.2002755197469127
LOSS train 0.24230470271560095 valid 0.2003943418463071
LOSS train 0.24230470271560095 valid 0.20061283329086846
LOSS train 0.24230470271560095 valid 0.20041291682787662
LOSS train 0.24230470271560095 valid 0.20036079136418625
LOSS train 0.24230470271560095 valid 0.20032715978466462
LOSS train 0.24230470271560095 valid 0.20002465677815814
LOSS train 0.24230470271560095 valid 0.19984048646357325
LOSS train 0.24230470271560095 valid 0.19959313844755497
LOSS train 0.24230470271560095 valid 0.1995514740637683
LOSS train 0.24230470271560095 valid 0.19951046935227362
LOSS train 0.24230470271560095 valid 0.19957200701941144
LOSS train 0.24230470271560095 valid 0.1995359394210496
LOSS train 0.24230470271560095 valid 0.19962300521296425
LOSS train 0.24230470271560095 valid 0.19975623728983072
LOSS train 0.24230470271560095 valid 0.19962549868172833
LOSS train 0.24230470271560095 valid 0.19950328250726065
LOSS train 0.24230470271560095 valid 0.19937041138125733
LOSS train 0.24230470271560095 valid 0.1994231198852808
LOSS train 0.24230470271560095 valid 0.19939447847897546
LOSS train 0.24230470271560095 valid 0.19930681996210173
LOSS train 0.24230470271560095 valid 0.19933361443488495
LOSS train 0.24230470271560095 valid 0.19927990804244947
LOSS train 0.24230470271560095 valid 0.19937543815066075
LOSS train 0.24230470271560095 valid 0.1996631328128438
LOSS train 0.24230470271560095 valid 0.19959538232566965
LOSS train 0.24230470271560095 valid 0.1996350664407649
LOSS train 0.24230470271560095 valid 0.19950143033165044
LOSS train 0.24230470271560095 valid 0.19960473779636093
LOSS train 0.24230470271560095 valid 0.1995939852184608
LOSS train 0.24230470271560095 valid 0.1995511512626664
LOSS train 0.24230470271560095 valid 0.199192371374617
LOSS train 0.24230470271560095 valid 0.1990660343983856
LOSS train 0.24230470271560095 valid 0.19901647120098437
LOSS train 0.24230470271560095 valid 0.1988392498942069
LOSS train 0.24230470271560095 valid 0.19884593964966593
LOSS train 0.24230470271560095 valid 0.19898122777136004
LOSS train 0.24230470271560095 valid 0.19902863211142338
LOSS train 0.24230470271560095 valid 0.19904427607411798
LOSS train 0.24230470271560095 valid 0.19901642837231198
LOSS train 0.24230470271560095 valid 0.19882859974860187
LOSS train 0.24230470271560095 valid 0.1989254285991192
LOSS train 0.24230470271560095 valid 0.19896603417230319
LOSS train 0.24230470271560095 valid 0.19884602675243976
LOSS train 0.24230470271560095 valid 0.19867949832687265
LOSS train 0.24230470271560095 valid 0.1987937522509436
LOSS train 0.24230470271560095 valid 0.19872952147441753
LOSS train 0.24230470271560095 valid 0.19853523958590813
LOSS train 0.24230470271560095 valid 0.19853187595939822
LOSS train 0.24230470271560095 valid 0.19868461894550066
LOSS train 0.24230470271560095 valid 0.19858512630448838
LOSS train 0.24230470271560095 valid 0.19840952200958362
LOSS train 0.24230470271560095 valid 0.19846888174841687
LOSS train 0.24230470271560095 valid 0.19864378081819484
LOSS train 0.24230470271560095 valid 0.1985064708404668
LOSS train 0.24230470271560095 valid 0.19858654935590245
LOSS train 0.24230470271560095 valid 0.1985117682027367
LOSS train 0.24230470271560095 valid 0.19854489890368363
LOSS train 0.24230470271560095 valid 0.19846921676441973
LOSS train 0.24230470271560095 valid 0.19853849372645813
LOSS train 0.24230470271560095 valid 0.19868032864146074
LOSS train 0.24230470271560095 valid 0.19852068487692762
LOSS train 0.24230470271560095 valid 0.1985245963689146
LOSS train 0.24230470271560095 valid 0.19843590207507505
LOSS train 0.24230470271560095 valid 0.19836071059926524
LOSS train 0.24230470271560095 valid 0.19825679936656987
LOSS train 0.24230470271560095 valid 0.1983148484609344
LOSS train 0.24230470271560095 valid 0.19838028927536114
LOSS train 0.24230470271560095 valid 0.19840201108787034
LOSS train 0.24230470271560095 valid 0.198393889763158
LOSS train 0.24230470271560095 valid 0.1983387918081335
LOSS train 0.24230470271560095 valid 0.1982222720714552
LOSS train 0.24230470271560095 valid 0.19808303336762023
LOSS train 0.24230470271560095 valid 0.19804781297525614
LOSS train 0.24230470271560095 valid 0.19804578485105567
LOSS train 0.24230470271560095 valid 0.1980368069727236
LOSS train 0.24230470271560095 valid 0.19807777245316588
LOSS train 0.24230470271560095 valid 0.1980630132545541
LOSS train 0.24230470271560095 valid 0.1980485589298637
LOSS train 0.24230470271560095 valid 0.19802375176611045
LOSS train 0.24230470271560095 valid 0.19795109772455322
LOSS train 0.24230470271560095 valid 0.19781991287038245
LOSS train 0.24230470271560095 valid 0.19783784016403547
LOSS train 0.24230470271560095 valid 0.197828554969332
LOSS train 0.24230470271560095 valid 0.19782939576146547
LOSS train 0.24230470271560095 valid 0.19785850158049947
LOSS train 0.24230470271560095 valid 0.19785893900414644
LOSS train 0.24230470271560095 valid 0.19782816767189149
LOSS train 0.24230470271560095 valid 0.19775762950832193
LOSS train 0.24230470271560095 valid 0.19782523393330959
LOSS train 0.24230470271560095 valid 0.19780845957555898
LOSS train 0.24230470271560095 valid 0.19776619754731656
LOSS train 0.24230470271560095 valid 0.19766584788743047
LOSS train 0.24230470271560095 valid 0.19767743001217083
LOSS train 0.24230470271560095 valid 0.19758296413685228
LOSS train 0.24230470271560095 valid 0.19760019109143237
LOSS train 0.24230470271560095 valid 0.1975541589934318
LOSS train 0.24230470271560095 valid 0.19784379153956774
LOSS train 0.24230470271560095 valid 0.19780977883342812
LOSS train 0.24230470271560095 valid 0.19783746284227094
LOSS train 0.24230470271560095 valid 0.19784296226829387
LOSS train 0.24230470271560095 valid 0.19783513807962017
LOSS train 0.24230470271560095 valid 0.19783821809809307
LOSS train 0.24230470271560095 valid 0.1978273082476778
LOSS train 0.24230470271560095 valid 0.197908603797515
LOSS train 0.24230470271560095 valid 0.19788564112820442
LOSS train 0.24230470271560095 valid 0.19782994306749768
LOSS train 0.24230470271560095 valid 0.1977715095434385
LOSS train 0.24230470271560095 valid 0.1978100452466342
LOSS train 0.24230470271560095 valid 0.19777704801105853
LOSS train 0.24230470271560095 valid 0.19775791524626243
LOSS train 0.24230470271560095 valid 0.19765268189366908
LOSS train 0.24230470271560095 valid 0.1977693272110458
LOSS train 0.24230470271560095 valid 0.1977970188032396
LOSS train 0.24230470271560095 valid 0.19772531159060658
LOSS train 0.24230470271560095 valid 0.197741512692085
LOSS train 0.24230470271560095 valid 0.19759870815735597
LOSS train 0.24230470271560095 valid 0.19765109296865258
LOSS train 0.24230470271560095 valid 0.19755118522531032
LOSS train 0.24230470271560095 valid 0.19752735297036608
LOSS train 0.24230470271560095 valid 0.1975211778536756
LOSS train 0.24230470271560095 valid 0.19752524289669413
LOSS train 0.24230470271560095 valid 0.19745758061052449
LOSS train 0.24230470271560095 valid 0.1974515073136034
LOSS train 0.24230470271560095 valid 0.19758438520334862
LOSS train 0.24230470271560095 valid 0.19749566789961862
LOSS train 0.24230470271560095 valid 0.19738334375086114
LOSS train 0.24230470271560095 valid 0.19731345043207207
LOSS train 0.24230470271560095 valid 0.1974527626920171
LOSS train 0.24230470271560095 valid 0.19747157569026805
LOSS train 0.24230470271560095 valid 0.1973843231006006
LOSS train 0.24230470271560095 valid 0.19750922517303157
LOSS train 0.24230470271560095 valid 0.19744964190854356
LOSS train 0.24230470271560095 valid 0.19742390345673114
LOSS train 0.24230470271560095 valid 0.19732249463004203
LOSS train 0.24230470271560095 valid 0.1974051183869326
LOSS train 0.24230470271560095 valid 0.1975280959321105
LOSS train 0.24230470271560095 valid 0.19747088550229294
LOSS train 0.24230470271560095 valid 0.19739694916978692
LOSS train 0.24230470271560095 valid 0.19750325208337144
LOSS train 0.24230470271560095 valid 0.19746177933875333
LOSS train 0.24230470271560095 valid 0.1974836766081197
LOSS train 0.24230470271560095 valid 0.19755622763664293
LOSS train 0.24230470271560095 valid 0.19759605285203594
LOSS train 0.24230470271560095 valid 0.19762451007497886
LOSS train 0.24230470271560095 valid 0.19764751480039905
LOSS train 0.24230470271560095 valid 0.1975180824667635
LOSS train 0.24230470271560095 valid 0.19755838687918828
LOSS train 0.24230470271560095 valid 0.19760177413872979
LOSS train 0.24230470271560095 valid 0.1975628603978197
LOSS train 0.24230470271560095 valid 0.19747996780616658
LOSS train 0.24230470271560095 valid 0.19763735990143486
LOSS train 0.24230470271560095 valid 0.19756258667093235
LOSS train 0.24230470271560095 valid 0.19762927621026724
LOSS train 0.24230470271560095 valid 0.19771003142286595
LOSS train 0.24230470271560095 valid 0.19763225085705846
LOSS train 0.24230470271560095 valid 0.19771537562347438
LOSS train 0.24230470271560095 valid 0.19769785954245453
LOSS train 0.24230470271560095 valid 0.19764102201614459
LOSS train 0.24230470271560095 valid 0.19753276724773255
LOSS train 0.24230470271560095 valid 0.19748141770036562
EPOCH 18:
  batch 1 loss: 0.25368982553482056
  batch 2 loss: 0.26692672073841095
  batch 3 loss: 0.2549391835927963
  batch 4 loss: 0.25253186002373695
  batch 5 loss: 0.25545425713062286
  batch 6 loss: 0.25106660773356754
  batch 7 loss: 0.24854151053088053
  batch 8 loss: 0.24431918561458588
  batch 9 loss: 0.2473041746351454
  batch 10 loss: 0.24333686679601668
  batch 11 loss: 0.24525100399147381
  batch 12 loss: 0.24354065457979837
  batch 13 loss: 0.2420576042853869
  batch 14 loss: 0.24101474561861583
  batch 15 loss: 0.24172087411085766
  batch 16 loss: 0.24117422848939896
  batch 17 loss: 0.24224718353327582
  batch 18 loss: 0.24007094734244877
  batch 19 loss: 0.2392988659833607
  batch 20 loss: 0.23787940070033073
  batch 21 loss: 0.2407933168468021
  batch 22 loss: 0.2406736449761824
  batch 23 loss: 0.24000740180844846
  batch 24 loss: 0.24104862287640572
  batch 25 loss: 0.24193437933921813
  batch 26 loss: 0.24155097340161985
  batch 27 loss: 0.24167370630635154
  batch 28 loss: 0.24301129739199365
  batch 29 loss: 0.24361931917996243
  batch 30 loss: 0.24498435507218044
  batch 31 loss: 0.24469177905590303
  batch 32 loss: 0.24589719763025641
  batch 33 loss: 0.24829245200662903
  batch 34 loss: 0.24910703169948914
  batch 35 loss: 0.2503810750586646
  batch 36 loss: 0.25039298459887505
  batch 37 loss: 0.2506346513290663
  batch 38 loss: 0.2513840861226383
  batch 39 loss: 0.25235788371318424
  batch 40 loss: 0.2521243091672659
  batch 41 loss: 0.25206572798694055
  batch 42 loss: 0.25201012868256795
  batch 43 loss: 0.25252918691136117
  batch 44 loss: 0.25308245996182616
  batch 45 loss: 0.2524653149975671
  batch 46 loss: 0.2511044637016628
  batch 47 loss: 0.25038732271245184
  batch 48 loss: 0.25009028644611436
  batch 49 loss: 0.25053586552337725
  batch 50 loss: 0.25191242605447767
  batch 51 loss: 0.2521413754598767
  batch 52 loss: 0.2523003521446998
  batch 53 loss: 0.25197164039566833
  batch 54 loss: 0.2519483055781435
  batch 55 loss: 0.2522256837649779
  batch 56 loss: 0.2522059154829809
  batch 57 loss: 0.2514915050644624
  batch 58 loss: 0.2512168984474807
  batch 59 loss: 0.25141276015063463
  batch 60 loss: 0.2520338915288448
  batch 61 loss: 0.25257415532088673
  batch 62 loss: 0.2527398695388148
  batch 63 loss: 0.251934933047446
  batch 64 loss: 0.2512176258023828
  batch 65 loss: 0.25083618141137637
  batch 66 loss: 0.2506904272419034
  batch 67 loss: 0.25067359787314686
  batch 68 loss: 0.25092605369932514
  batch 69 loss: 0.2509728195010752
  batch 70 loss: 0.2510643550327846
  batch 71 loss: 0.2513032622740302
  batch 72 loss: 0.25132326698965496
  batch 73 loss: 0.2514803270771079
  batch 74 loss: 0.25141900576449727
  batch 75 loss: 0.2515720168749491
  batch 76 loss: 0.2516107943497206
  batch 77 loss: 0.25130347752725923
  batch 78 loss: 0.25123447943956423
  batch 79 loss: 0.25110134916215004
  batch 80 loss: 0.2508671011775732
  batch 81 loss: 0.25071290061797624
  batch 82 loss: 0.25086624665958124
  batch 83 loss: 0.2504545426871403
  batch 84 loss: 0.2497216218284198
  batch 85 loss: 0.250162432123633
  batch 86 loss: 0.24988006124662798
  batch 87 loss: 0.24948084114611835
  batch 88 loss: 0.24923984875733202
  batch 89 loss: 0.24887297461541852
  batch 90 loss: 0.24890880551603106
  batch 91 loss: 0.24880502020919715
  batch 92 loss: 0.24852480512598288
  batch 93 loss: 0.24804836511611938
  batch 94 loss: 0.24767422359040442
  batch 95 loss: 0.24771908145201835
  batch 96 loss: 0.2476648110896349
  batch 97 loss: 0.24813969510117756
  batch 98 loss: 0.24796028465640788
  batch 99 loss: 0.2475556219467009
  batch 100 loss: 0.24780744254589082
  batch 101 loss: 0.24766919238142449
  batch 102 loss: 0.24772939451184928
  batch 103 loss: 0.24748426140512078
  batch 104 loss: 0.24775982748430508
  batch 105 loss: 0.24763891540822527
  batch 106 loss: 0.24774363144951048
  batch 107 loss: 0.24770387561521798
  batch 108 loss: 0.24731015772731216
  batch 109 loss: 0.2473954607587342
  batch 110 loss: 0.24741056913679296
  batch 111 loss: 0.24731975941507667
  batch 112 loss: 0.24719384152974402
  batch 113 loss: 0.24725126244325554
  batch 114 loss: 0.24724415506710085
  batch 115 loss: 0.24695068662581238
  batch 116 loss: 0.24677945031174298
  batch 117 loss: 0.24667162887561014
  batch 118 loss: 0.24667326588246782
  batch 119 loss: 0.24674326895164841
  batch 120 loss: 0.24654340830941995
  batch 121 loss: 0.24645359223046578
  batch 122 loss: 0.24622686182866332
  batch 123 loss: 0.24590773383776346
  batch 124 loss: 0.24597502139306837
  batch 125 loss: 0.24555475449562073
  batch 126 loss: 0.2456356899605857
  batch 127 loss: 0.24577711346581227
  batch 128 loss: 0.2458258594851941
  batch 129 loss: 0.24605915597242903
  batch 130 loss: 0.24591071777618848
  batch 131 loss: 0.24587558050647038
  batch 132 loss: 0.2459830796402512
  batch 133 loss: 0.24582130590775855
  batch 134 loss: 0.24563653077652206
  batch 135 loss: 0.2454636307778182
  batch 136 loss: 0.24570750740959363
  batch 137 loss: 0.24605296421660125
  batch 138 loss: 0.24598748895569125
  batch 139 loss: 0.2460166683299936
  batch 140 loss: 0.24644457995891572
  batch 141 loss: 0.24614142568398875
  batch 142 loss: 0.2458275015085516
  batch 143 loss: 0.24609048466582398
  batch 144 loss: 0.2460638890042901
  batch 145 loss: 0.24596534011692836
  batch 146 loss: 0.2461434177135768
  batch 147 loss: 0.24614150619425743
  batch 148 loss: 0.24617553129792213
  batch 149 loss: 0.24574443457910677
  batch 150 loss: 0.24577692687511443
  batch 151 loss: 0.24545717239379883
  batch 152 loss: 0.2452416194504813
  batch 153 loss: 0.24536832563238206
  batch 154 loss: 0.24523284953910035
  batch 155 loss: 0.24528786309303777
  batch 156 loss: 0.24521063019831976
  batch 157 loss: 0.24504095211530189
  batch 158 loss: 0.2451319884084448
  batch 159 loss: 0.24543947328186635
  batch 160 loss: 0.2455286172218621
  batch 161 loss: 0.2452771083353469
  batch 162 loss: 0.2452350080381205
  batch 163 loss: 0.24500106183663467
  batch 164 loss: 0.24519350025348546
  batch 165 loss: 0.24533941953471214
  batch 166 loss: 0.2453948111239686
  batch 167 loss: 0.24530304888051427
  batch 168 loss: 0.2450925890533697
  batch 169 loss: 0.2450331418648274
  batch 170 loss: 0.24504417908542298
  batch 171 loss: 0.2451696872536899
  batch 172 loss: 0.24511369950203007
  batch 173 loss: 0.24504004504983826
  batch 174 loss: 0.24514150987753922
  batch 175 loss: 0.24523356616497038
  batch 176 loss: 0.24545952381396835
  batch 177 loss: 0.24577781465430718
  batch 178 loss: 0.24557588266187838
  batch 179 loss: 0.24548324434783872
  batch 180 loss: 0.245431075245142
  batch 181 loss: 0.24536414617332963
  batch 182 loss: 0.2453684382713758
  batch 183 loss: 0.24532331127286608
  batch 184 loss: 0.2452577638399342
  batch 185 loss: 0.2450313062281222
  batch 186 loss: 0.24504015586709463
  batch 187 loss: 0.24521524574667375
  batch 188 loss: 0.24516976497908857
  batch 189 loss: 0.24514340021938244
  batch 190 loss: 0.24498012450180556
  batch 191 loss: 0.24494817119618362
  batch 192 loss: 0.24479786128116152
  batch 193 loss: 0.24479412129197095
  batch 194 loss: 0.24467624478118935
  batch 195 loss: 0.24477254243997426
  batch 196 loss: 0.2446339192743204
  batch 197 loss: 0.24470067447817265
  batch 198 loss: 0.24465032433620607
  batch 199 loss: 0.24470074287611038
  batch 200 loss: 0.24464577674865723
  batch 201 loss: 0.24446400095574297
  batch 202 loss: 0.24465446513478117
  batch 203 loss: 0.24462144832892957
  batch 204 loss: 0.24472880816342785
  batch 205 loss: 0.2446616588569269
  batch 206 loss: 0.24467487018374565
  batch 207 loss: 0.24466400201193952
  batch 208 loss: 0.24454234762547108
  batch 209 loss: 0.24459641565831655
  batch 210 loss: 0.2446081349537486
  batch 211 loss: 0.24456605286959787
  batch 212 loss: 0.24464163985454812
  batch 213 loss: 0.2445334410163718
  batch 214 loss: 0.24444374456026843
  batch 215 loss: 0.24438861358997432
  batch 216 loss: 0.24435904311637083
  batch 217 loss: 0.2443085284689055
  batch 218 loss: 0.244118958916686
  batch 219 loss: 0.24386210052390075
  batch 220 loss: 0.2440231363881718
  batch 221 loss: 0.24395767629685985
  batch 222 loss: 0.2439846954904161
  batch 223 loss: 0.2438606959287361
  batch 224 loss: 0.24379091496978486
  batch 225 loss: 0.24383709642622206
  batch 226 loss: 0.2439142393063655
  batch 227 loss: 0.2439362761864053
  batch 228 loss: 0.24396713542049392
  batch 229 loss: 0.2439704171044337
  batch 230 loss: 0.24400988771863605
  batch 231 loss: 0.24399578081064927
  batch 232 loss: 0.24396226722104797
  batch 233 loss: 0.24393379828960599
  batch 234 loss: 0.24387363554575506
  batch 235 loss: 0.24369447935134805
  batch 236 loss: 0.24366849427253512
  batch 237 loss: 0.24351924148541462
  batch 238 loss: 0.24339701817566609
  batch 239 loss: 0.24337132312513296
  batch 240 loss: 0.24349041587362688
  batch 241 loss: 0.24333729418728856
  batch 242 loss: 0.24321353626399
  batch 243 loss: 0.2431667577092049
  batch 244 loss: 0.2430581630619823
  batch 245 loss: 0.24290572696802568
  batch 246 loss: 0.24280400525748244
  batch 247 loss: 0.24274574654546344
  batch 248 loss: 0.24286417825327766
  batch 249 loss: 0.2428244828100664
  batch 250 loss: 0.24279803442955017
  batch 251 loss: 0.24261590974976818
  batch 252 loss: 0.24271357018086645
  batch 253 loss: 0.2425916960588086
  batch 254 loss: 0.24256519292752574
  batch 255 loss: 0.2424337487594754
  batch 256 loss: 0.24239797645714134
  batch 257 loss: 0.24238485045702077
  batch 258 loss: 0.24232963079860967
  batch 259 loss: 0.24216395524477866
  batch 260 loss: 0.24219348636957316
  batch 261 loss: 0.24226868323896122
  batch 262 loss: 0.24217959100963504
  batch 263 loss: 0.24208043306046112
  batch 264 loss: 0.2421271951135361
  batch 265 loss: 0.24200701702315852
  batch 266 loss: 0.24183577299118042
  batch 267 loss: 0.24171544963054442
  batch 268 loss: 0.2415443221802142
  batch 269 loss: 0.24154369372417492
  batch 270 loss: 0.24145391175040493
  batch 271 loss: 0.2414082469095603
  batch 272 loss: 0.24129835675087044
  batch 273 loss: 0.2411396998516369
  batch 274 loss: 0.24096998860583688
  batch 275 loss: 0.240904369300062
  batch 276 loss: 0.24084576926585557
  batch 277 loss: 0.24083089554137702
  batch 278 loss: 0.24069444390295222
  batch 279 loss: 0.2407151587761431
  batch 280 loss: 0.24064027502068452
  batch 281 loss: 0.2406417599140113
  batch 282 loss: 0.24048157427327851
  batch 283 loss: 0.24043736630530746
  batch 284 loss: 0.24035653576884472
  batch 285 loss: 0.24033511819546682
  batch 286 loss: 0.2403203932034386
  batch 287 loss: 0.2402015492475822
  batch 288 loss: 0.24022707234447202
  batch 289 loss: 0.24009419786888836
  batch 290 loss: 0.23996444894322033
  batch 291 loss: 0.23995748740905748
  batch 292 loss: 0.23985479297499135
  batch 293 loss: 0.23984032813396064
  batch 294 loss: 0.23955227530935183
  batch 295 loss: 0.23951427426378605
  batch 296 loss: 0.2395470298927378
  batch 297 loss: 0.23957925879393363
  batch 298 loss: 0.23960383106398103
  batch 299 loss: 0.23961249093546916
  batch 300 loss: 0.23956802139679592
  batch 301 loss: 0.23957650289187005
  batch 302 loss: 0.23969748359642282
  batch 303 loss: 0.23970841358203698
  batch 304 loss: 0.23968868516385555
  batch 305 loss: 0.23966177091246746
  batch 306 loss: 0.23980491491509418
  batch 307 loss: 0.2398914157470585
  batch 308 loss: 0.2399360423835067
  batch 309 loss: 0.23989659596029608
  batch 310 loss: 0.2400501549243927
  batch 311 loss: 0.24006863920634966
  batch 312 loss: 0.24009222007141665
  batch 313 loss: 0.2401106069072748
  batch 314 loss: 0.2401494883997425
  batch 315 loss: 0.24017156045588237
  batch 316 loss: 0.24027712034839618
  batch 317 loss: 0.24034122944066202
  batch 318 loss: 0.24025188803485353
  batch 319 loss: 0.2402754996357293
  batch 320 loss: 0.24033219064585865
  batch 321 loss: 0.24046990530698842
  batch 322 loss: 0.24053606830583596
  batch 323 loss: 0.24047317348951155
  batch 324 loss: 0.24061085306751875
  batch 325 loss: 0.24094527881879074
  batch 326 loss: 0.24108519953635574
  batch 327 loss: 0.24103057512085008
  batch 328 loss: 0.24101462864839449
  batch 329 loss: 0.24114655157534184
  batch 330 loss: 0.2411811912150094
  batch 331 loss: 0.24121849076805518
  batch 332 loss: 0.2413897540824241
  batch 333 loss: 0.241477449600761
  batch 334 loss: 0.24179351173653574
  batch 335 loss: 0.24191604412313716
  batch 336 loss: 0.2420221682904022
  batch 337 loss: 0.2421476964338597
  batch 338 loss: 0.2422542525730895
  batch 339 loss: 0.24238692127322026
  batch 340 loss: 0.2423484772005502
  batch 341 loss: 0.24230618066976498
  batch 342 loss: 0.2422917889137017
  batch 343 loss: 0.24253910393304798
  batch 344 loss: 0.2426750596266153
  batch 345 loss: 0.24276912061200626
  batch 346 loss: 0.24281850702673025
  batch 347 loss: 0.24274886405948948
  batch 348 loss: 0.24281418610407018
  batch 349 loss: 0.2428774126194268
  batch 350 loss: 0.2430762351836477
  batch 351 loss: 0.2431425504450105
  batch 352 loss: 0.24313846518370238
  batch 353 loss: 0.24311535579624663
  batch 354 loss: 0.2430812603673019
  batch 355 loss: 0.24306299938282497
  batch 356 loss: 0.24321578359335996
  batch 357 loss: 0.24309665127461696
  batch 358 loss: 0.24302572607161613
  batch 359 loss: 0.2429795554089347
  batch 360 loss: 0.243046130405532
  batch 361 loss: 0.24299242308265284
  batch 362 loss: 0.24299212211732707
  batch 363 loss: 0.243038085127665
  batch 364 loss: 0.2429790225821537
  batch 365 loss: 0.2430408677826189
  batch 366 loss: 0.24306237925597227
  batch 367 loss: 0.24311223577413638
  batch 368 loss: 0.24309287139255067
  batch 369 loss: 0.24305610948300296
  batch 370 loss: 0.24305171366478945
  batch 371 loss: 0.24310878437323713
  batch 372 loss: 0.24316063983946717
  batch 373 loss: 0.24315939028684957
  batch 374 loss: 0.24306509954565986
  batch 375 loss: 0.2432467030286789
  batch 376 loss: 0.24343142908145773
  batch 377 loss: 0.24342253681677406
  batch 378 loss: 0.24343115915224034
  batch 379 loss: 0.24342509321770128
  batch 380 loss: 0.24340689139146554
  batch 381 loss: 0.2433736754527555
  batch 382 loss: 0.24343340465535668
  batch 383 loss: 0.24333065608151588
  batch 384 loss: 0.24332602671347558
  batch 385 loss: 0.243259841010168
  batch 386 loss: 0.24321196120174438
  batch 387 loss: 0.24314698919615388
  batch 388 loss: 0.24300448550391443
  batch 389 loss: 0.2430061954995354
  batch 390 loss: 0.242978464334439
  batch 391 loss: 0.24296070646751872
  batch 392 loss: 0.2430432979397628
  batch 393 loss: 0.24306574668593078
  batch 394 loss: 0.24303759267638783
  batch 395 loss: 0.24305255794072453
  batch 396 loss: 0.24310037294271017
  batch 397 loss: 0.24315212291644261
  batch 398 loss: 0.2432436608714075
  batch 399 loss: 0.24321465739480833
  batch 400 loss: 0.24335201758891345
  batch 401 loss: 0.2434143301777709
  batch 402 loss: 0.24344203109616663
  batch 403 loss: 0.24342595835773287
  batch 404 loss: 0.24339662942260798
  batch 405 loss: 0.2433973690977803
  batch 406 loss: 0.24327542302496916
  batch 407 loss: 0.24333068212215087
  batch 408 loss: 0.24321391780440713
  batch 409 loss: 0.24322633350536702
  batch 410 loss: 0.24306016560734772
  batch 411 loss: 0.2430122166219419
  batch 412 loss: 0.2431370580514658
  batch 413 loss: 0.2431004735805798
  batch 414 loss: 0.24311558920260212
  batch 415 loss: 0.243067072996174
  batch 416 loss: 0.2430941637677069
  batch 417 loss: 0.24305611109133246
  batch 418 loss: 0.24305327821291234
  batch 419 loss: 0.24306176275655023
  batch 420 loss: 0.24307175509276843
  batch 421 loss: 0.24317467145427105
  batch 422 loss: 0.24311357132772698
  batch 423 loss: 0.243086335823891
  batch 424 loss: 0.2431123940238975
  batch 425 loss: 0.24303374062566196
  batch 426 loss: 0.24298028389053167
  batch 427 loss: 0.24297046221670557
  batch 428 loss: 0.24300177743501752
  batch 429 loss: 0.24285551061580232
  batch 430 loss: 0.24284085506616637
  batch 431 loss: 0.24283330318822663
  batch 432 loss: 0.2428567128049003
  batch 433 loss: 0.24285262699215296
  batch 434 loss: 0.24274451584310575
  batch 435 loss: 0.24283215691303384
  batch 436 loss: 0.24283473391871935
  batch 437 loss: 0.2427261981865907
  batch 438 loss: 0.2426635592359386
  batch 439 loss: 0.24258416202046604
  batch 440 loss: 0.24265413484112783
  batch 441 loss: 0.24263659832564072
  batch 442 loss: 0.24262018886087167
  batch 443 loss: 0.24261711786751283
  batch 444 loss: 0.24264261814522314
  batch 445 loss: 0.2425851773010211
  batch 446 loss: 0.2425373778281725
  batch 447 loss: 0.2424707796082934
  batch 448 loss: 0.24258842512166925
  batch 449 loss: 0.2425217875393568
  batch 450 loss: 0.2424748424688975
  batch 451 loss: 0.2424077318275054
  batch 452 loss: 0.24233141904119895
  batch 453 loss: 0.2423092695261469
  batch 454 loss: 0.2422880326204888
  batch 455 loss: 0.2422667661865989
  batch 456 loss: 0.24218524201658734
  batch 457 loss: 0.24214027342702418
  batch 458 loss: 0.24211725917966084
  batch 459 loss: 0.24206499917704555
  batch 460 loss: 0.2420233132722585
  batch 461 loss: 0.2420121310783829
  batch 462 loss: 0.2420439665645232
  batch 463 loss: 0.24203571493759526
  batch 464 loss: 0.2419652540943232
  batch 465 loss: 0.2419544074484097
  batch 466 loss: 0.24192008488040115
  batch 467 loss: 0.2420168382771776
  batch 468 loss: 0.24193526954095587
  batch 469 loss: 0.2418441638382259
  batch 470 loss: 0.24186974161482871
  batch 471 loss: 0.24186460325798917
  batch 472 loss: 0.24166707419869254
LOSS train 0.24166707419869254 valid 0.21659600734710693
LOSS train 0.24166707419869254 valid 0.20963677763938904
LOSS train 0.24166707419869254 valid 0.21997779111067453
LOSS train 0.24166707419869254 valid 0.20604759454727173
LOSS train 0.24166707419869254 valid 0.20258229672908784
LOSS train 0.24166707419869254 valid 0.20708583543697992
LOSS train 0.24166707419869254 valid 0.20151343090193613
LOSS train 0.24166707419869254 valid 0.1994699276983738
LOSS train 0.24166707419869254 valid 0.1986366973982917
LOSS train 0.24166707419869254 valid 0.19688142240047454
LOSS train 0.24166707419869254 valid 0.1961807053197514
LOSS train 0.24166707419869254 valid 0.19851307943463326
LOSS train 0.24166707419869254 valid 0.19840053640879118
LOSS train 0.24166707419869254 valid 0.19704540073871613
LOSS train 0.24166707419869254 valid 0.19668839772542318
LOSS train 0.24166707419869254 valid 0.20018863305449486
LOSS train 0.24166707419869254 valid 0.20026218277566574
LOSS train 0.24166707419869254 valid 0.20002741449409062
LOSS train 0.24166707419869254 valid 0.20224639851796
LOSS train 0.24166707419869254 valid 0.20157092660665513
LOSS train 0.24166707419869254 valid 0.2030049172185716
LOSS train 0.24166707419869254 valid 0.20256334136832843
LOSS train 0.24166707419869254 valid 0.20048342321230017
LOSS train 0.24166707419869254 valid 0.20048873809476694
LOSS train 0.24166707419869254 valid 0.2007643187046051
LOSS train 0.24166707419869254 valid 0.20026065294559187
LOSS train 0.24166707419869254 valid 0.20021247918959018
LOSS train 0.24166707419869254 valid 0.20081115514039993
LOSS train 0.24166707419869254 valid 0.19955903445852213
LOSS train 0.24166707419869254 valid 0.19899756560722986
LOSS train 0.24166707419869254 valid 0.19949730125165754
LOSS train 0.24166707419869254 valid 0.1995409862138331
LOSS train 0.24166707419869254 valid 0.198424703244007
LOSS train 0.24166707419869254 valid 0.19825426182326147
LOSS train 0.24166707419869254 valid 0.19863352690424238
LOSS train 0.24166707419869254 valid 0.19923078848256004
LOSS train 0.24166707419869254 valid 0.19971508472352414
LOSS train 0.24166707419869254 valid 0.1995363427620185
LOSS train 0.24166707419869254 valid 0.2005391258459825
LOSS train 0.24166707419869254 valid 0.2009744480252266
LOSS train 0.24166707419869254 valid 0.20079429193240841
LOSS train 0.24166707419869254 valid 0.20207093230315618
LOSS train 0.24166707419869254 valid 0.2025021882944329
LOSS train 0.24166707419869254 valid 0.20227013460614465
LOSS train 0.24166707419869254 valid 0.20175903307067022
LOSS train 0.24166707419869254 valid 0.201459601521492
LOSS train 0.24166707419869254 valid 0.2011708325528084
LOSS train 0.24166707419869254 valid 0.20277160095671812
LOSS train 0.24166707419869254 valid 0.20204447939687845
LOSS train 0.24166707419869254 valid 0.20262923687696457
LOSS train 0.24166707419869254 valid 0.20211667319138846
LOSS train 0.24166707419869254 valid 0.20187057526065752
LOSS train 0.24166707419869254 valid 0.20290517947583828
LOSS train 0.24166707419869254 valid 0.20297549775353185
LOSS train 0.24166707419869254 valid 0.20283377468585967
LOSS train 0.24166707419869254 valid 0.2030930378075157
LOSS train 0.24166707419869254 valid 0.2026250257826688
LOSS train 0.24166707419869254 valid 0.2031151994035162
LOSS train 0.24166707419869254 valid 0.20288084598921113
LOSS train 0.24166707419869254 valid 0.20276510044932367
LOSS train 0.24166707419869254 valid 0.2028132992201164
LOSS train 0.24166707419869254 valid 0.2025501744881753
LOSS train 0.24166707419869254 valid 0.2023270499138605
LOSS train 0.24166707419869254 valid 0.20292319636791945
LOSS train 0.24166707419869254 valid 0.20185169669298025
LOSS train 0.24166707419869254 valid 0.201729109341448
LOSS train 0.24166707419869254 valid 0.20241371064043756
LOSS train 0.24166707419869254 valid 0.20189722211045377
LOSS train 0.24166707419869254 valid 0.20246125498543616
LOSS train 0.24166707419869254 valid 0.20294361753123147
LOSS train 0.24166707419869254 valid 0.20338643226825015
LOSS train 0.24166707419869254 valid 0.20400557501448524
LOSS train 0.24166707419869254 valid 0.20456991987685635
LOSS train 0.24166707419869254 valid 0.20430852190868273
LOSS train 0.24166707419869254 valid 0.20398303071657817
LOSS train 0.24166707419869254 valid 0.20423279016425736
LOSS train 0.24166707419869254 valid 0.20424385097893802
LOSS train 0.24166707419869254 valid 0.20414007359590286
LOSS train 0.24166707419869254 valid 0.20404168623912183
LOSS train 0.24166707419869254 valid 0.20368119943886995
LOSS train 0.24166707419869254 valid 0.20395298872464968
LOSS train 0.24166707419869254 valid 0.20377035766113094
LOSS train 0.24166707419869254 valid 0.20410466391638102
LOSS train 0.24166707419869254 valid 0.20389023334497497
LOSS train 0.24166707419869254 valid 0.20469130260102888
LOSS train 0.24166707419869254 valid 0.20469113855167878
LOSS train 0.24166707419869254 valid 0.2044171288438227
LOSS train 0.24166707419869254 valid 0.20466156456280837
LOSS train 0.24166707419869254 valid 0.20507511647229784
LOSS train 0.24166707419869254 valid 0.20549832350677913
LOSS train 0.24166707419869254 valid 0.20552532784231417
LOSS train 0.24166707419869254 valid 0.20559092465302217
LOSS train 0.24166707419869254 valid 0.20537153639460123
LOSS train 0.24166707419869254 valid 0.205592117569548
LOSS train 0.24166707419869254 valid 0.20579368848549692
LOSS train 0.24166707419869254 valid 0.20595463598147035
LOSS train 0.24166707419869254 valid 0.20609306490298399
LOSS train 0.24166707419869254 valid 0.20656989210722398
LOSS train 0.24166707419869254 valid 0.20679353930131353
LOSS train 0.24166707419869254 valid 0.20690474092960356
LOSS train 0.24166707419869254 valid 0.2068926132846587
LOSS train 0.24166707419869254 valid 0.20724172860968346
LOSS train 0.24166707419869254 valid 0.20707646417386324
LOSS train 0.24166707419869254 valid 0.2068858050669615
LOSS train 0.24166707419869254 valid 0.20712160822891054
LOSS train 0.24166707419869254 valid 0.2072332023449664
LOSS train 0.24166707419869254 valid 0.206929769471427
LOSS train 0.24166707419869254 valid 0.20705432155066067
LOSS train 0.24166707419869254 valid 0.2066716003582018
LOSS train 0.24166707419869254 valid 0.20660311742262408
LOSS train 0.24166707419869254 valid 0.2066952831841804
LOSS train 0.24166707419869254 valid 0.2069434608731951
LOSS train 0.24166707419869254 valid 0.206776483923988
LOSS train 0.24166707419869254 valid 0.20688934584981516
LOSS train 0.24166707419869254 valid 0.20754802784194118
LOSS train 0.24166707419869254 valid 0.20725537682401723
LOSS train 0.24166707419869254 valid 0.20796742143794003
LOSS train 0.24166707419869254 valid 0.20781777685476563
LOSS train 0.24166707419869254 valid 0.207556158679874
LOSS train 0.24166707419869254 valid 0.20716008208692074
LOSS train 0.24166707419869254 valid 0.20691441319698145
LOSS train 0.24166707419869254 valid 0.20705516770726345
LOSS train 0.24166707419869254 valid 0.20708442449085113
LOSS train 0.24166707419869254 valid 0.20738641977790864
LOSS train 0.24166707419869254 valid 0.20733401966094972
LOSS train 0.24166707419869254 valid 0.20748611232118
LOSS train 0.24166707419869254 valid 0.20727200454144967
LOSS train 0.24166707419869254 valid 0.20729843468870968
LOSS train 0.24166707419869254 valid 0.20696749763433323
LOSS train 0.24166707419869254 valid 0.20675334804333173
LOSS train 0.24166707419869254 valid 0.20679135870842533
LOSS train 0.24166707419869254 valid 0.20665445506121172
LOSS train 0.24166707419869254 valid 0.20667583108844614
LOSS train 0.24166707419869254 valid 0.20687417863909877
LOSS train 0.24166707419869254 valid 0.20694064729743533
LOSS train 0.24166707419869254 valid 0.20685229494291194
LOSS train 0.24166707419869254 valid 0.2068752881384244
LOSS train 0.24166707419869254 valid 0.20676375957934753
LOSS train 0.24166707419869254 valid 0.20661315150398143
LOSS train 0.24166707419869254 valid 0.20653996978487288
LOSS train 0.24166707419869254 valid 0.20667422462439705
LOSS train 0.24166707419869254 valid 0.20693111619059468
LOSS train 0.24166707419869254 valid 0.2070492930137194
LOSS train 0.24166707419869254 valid 0.20710116241955095
LOSS train 0.24166707419869254 valid 0.20708014009327724
LOSS train 0.24166707419869254 valid 0.2069885137758843
LOSS train 0.24166707419869254 valid 0.20685636054496376
LOSS train 0.24166707419869254 valid 0.20685560920753995
LOSS train 0.24166707419869254 valid 0.2069160276211348
LOSS train 0.24166707419869254 valid 0.20694303145011267
LOSS train 0.24166707419869254 valid 0.2068457527468536
LOSS train 0.24166707419869254 valid 0.2068105445880639
LOSS train 0.24166707419869254 valid 0.20657832199841544
LOSS train 0.24166707419869254 valid 0.2066770758141171
LOSS train 0.24166707419869254 valid 0.20676516525207028
LOSS train 0.24166707419869254 valid 0.20673947829084519
LOSS train 0.24166707419869254 valid 0.20669779171989222
LOSS train 0.24166707419869254 valid 0.20686113145909732
LOSS train 0.24166707419869254 valid 0.2069491020538522
LOSS train 0.24166707419869254 valid 0.2070590834133327
LOSS train 0.24166707419869254 valid 0.20705396131328915
LOSS train 0.24166707419869254 valid 0.20716081237719383
LOSS train 0.24166707419869254 valid 0.20695410687133578
LOSS train 0.24166707419869254 valid 0.2069907075748211
LOSS train 0.24166707419869254 valid 0.20711270001801577
LOSS train 0.24166707419869254 valid 0.2070421008280961
LOSS train 0.24166707419869254 valid 0.20702676564276576
LOSS train 0.24166707419869254 valid 0.20693568024961723
LOSS train 0.24166707419869254 valid 0.20672385447476743
LOSS train 0.24166707419869254 valid 0.20679388072560814
LOSS train 0.24166707419869254 valid 0.20671699087173617
LOSS train 0.24166707419869254 valid 0.20672726172000863
LOSS train 0.24166707419869254 valid 0.2065709342674024
LOSS train 0.24166707419869254 valid 0.20696596518672747
LOSS train 0.24166707419869254 valid 0.20693077087402345
LOSS train 0.24166707419869254 valid 0.2068494057113474
LOSS train 0.24166707419869254 valid 0.20691835804510925
LOSS train 0.24166707419869254 valid 0.2070875054832255
LOSS train 0.24166707419869254 valid 0.20726565325726343
LOSS train 0.24166707419869254 valid 0.20712312211592993
LOSS train 0.24166707419869254 valid 0.20713164282767155
LOSS train 0.24166707419869254 valid 0.20699126297955986
LOSS train 0.24166707419869254 valid 0.2070973429197822
LOSS train 0.24166707419869254 valid 0.2069707935594994
LOSS train 0.24166707419869254 valid 0.206770203161884
LOSS train 0.24166707419869254 valid 0.20687945299251106
LOSS train 0.24166707419869254 valid 0.20682567549261818
LOSS train 0.24166707419869254 valid 0.20690516905581696
LOSS train 0.24166707419869254 valid 0.2067091483130026
LOSS train 0.24166707419869254 valid 0.20673390626907348
LOSS train 0.24166707419869254 valid 0.2068449428880402
LOSS train 0.24166707419869254 valid 0.20686907111667097
LOSS train 0.24166707419869254 valid 0.20666247382374006
LOSS train 0.24166707419869254 valid 0.206521582895333
LOSS train 0.24166707419869254 valid 0.20644581463092412
LOSS train 0.24166707419869254 valid 0.206529827978538
LOSS train 0.24166707419869254 valid 0.20650968583404716
LOSS train 0.24166707419869254 valid 0.20671142705462195
LOSS train 0.24166707419869254 valid 0.20668109525088688
LOSS train 0.24166707419869254 valid 0.20679195441305637
LOSS train 0.24166707419869254 valid 0.20659367918078578
LOSS train 0.24166707419869254 valid 0.20654701146453913
LOSS train 0.24166707419869254 valid 0.20661733743592436
LOSS train 0.24166707419869254 valid 0.20644350620169266
LOSS train 0.24166707419869254 valid 0.20640416959436927
LOSS train 0.24166707419869254 valid 0.20642313592642256
LOSS train 0.24166707419869254 valid 0.2063678847850809
LOSS train 0.24166707419869254 valid 0.20628279712624276
LOSS train 0.24166707419869254 valid 0.20631010338450162
LOSS train 0.24166707419869254 valid 0.2064192309266045
LOSS train 0.24166707419869254 valid 0.20665102985232928
LOSS train 0.24166707419869254 valid 0.2065022440170342
LOSS train 0.24166707419869254 valid 0.20647100964342485
LOSS train 0.24166707419869254 valid 0.20641999225193094
LOSS train 0.24166707419869254 valid 0.2061762399451677
LOSS train 0.24166707419869254 valid 0.20600448797146478
LOSS train 0.24166707419869254 valid 0.20576912865111355
LOSS train 0.24166707419869254 valid 0.2057752174521805
LOSS train 0.24166707419869254 valid 0.20575093141157333
LOSS train 0.24166707419869254 valid 0.20583643899722534
LOSS train 0.24166707419869254 valid 0.20579064890270318
LOSS train 0.24166707419869254 valid 0.20589255440879511
LOSS train 0.24166707419869254 valid 0.2059983544285522
LOSS train 0.24166707419869254 valid 0.20588306390813418
LOSS train 0.24166707419869254 valid 0.2057453986008962
LOSS train 0.24166707419869254 valid 0.20563249566913705
LOSS train 0.24166707419869254 valid 0.2056433878806194
LOSS train 0.24166707419869254 valid 0.20562500918382093
LOSS train 0.24166707419869254 valid 0.20555123758368096
LOSS train 0.24166707419869254 valid 0.20557340066070143
LOSS train 0.24166707419869254 valid 0.20550018187725183
LOSS train 0.24166707419869254 valid 0.2055767454078485
LOSS train 0.24166707419869254 valid 0.20590466420793738
LOSS train 0.24166707419869254 valid 0.20584371151068273
LOSS train 0.24166707419869254 valid 0.2059111272400998
LOSS train 0.24166707419869254 valid 0.20579268398173786
LOSS train 0.24166707419869254 valid 0.205862328661645
LOSS train 0.24166707419869254 valid 0.20587893368817178
LOSS train 0.24166707419869254 valid 0.20584010791080268
LOSS train 0.24166707419869254 valid 0.20548569705958167
LOSS train 0.24166707419869254 valid 0.20538847293720205
LOSS train 0.24166707419869254 valid 0.20536603843254492
LOSS train 0.24166707419869254 valid 0.20516890695188272
LOSS train 0.24166707419869254 valid 0.2051816185417234
LOSS train 0.24166707419869254 valid 0.20530826732212185
LOSS train 0.24166707419869254 valid 0.2053770306936609
LOSS train 0.24166707419869254 valid 0.20541132048436023
LOSS train 0.24166707419869254 valid 0.20536474790424109
LOSS train 0.24166707419869254 valid 0.2051630092852087
LOSS train 0.24166707419869254 valid 0.20521827402710915
LOSS train 0.24166707419869254 valid 0.20525697215026117
LOSS train 0.24166707419869254 valid 0.2051226348571834
LOSS train 0.24166707419869254 valid 0.20493458815125137
LOSS train 0.24166707419869254 valid 0.20508627819381361
LOSS train 0.24166707419869254 valid 0.2050298100885223
LOSS train 0.24166707419869254 valid 0.2048307205841411
LOSS train 0.24166707419869254 valid 0.20480271330255478
LOSS train 0.24166707419869254 valid 0.20496040158147036
LOSS train 0.24166707419869254 valid 0.20485709167470342
LOSS train 0.24166707419869254 valid 0.2047005229557936
LOSS train 0.24166707419869254 valid 0.20477276951988083
LOSS train 0.24166707419869254 valid 0.20492559082749237
LOSS train 0.24166707419869254 valid 0.20478832356263477
LOSS train 0.24166707419869254 valid 0.20485251789178813
LOSS train 0.24166707419869254 valid 0.20477142477372906
LOSS train 0.24166707419869254 valid 0.20481359339634278
LOSS train 0.24166707419869254 valid 0.20473783290229933
LOSS train 0.24166707419869254 valid 0.20480731666199306
LOSS train 0.24166707419869254 valid 0.20496565117379548
LOSS train 0.24166707419869254 valid 0.20480337780382898
LOSS train 0.24166707419869254 valid 0.20482071692543277
LOSS train 0.24166707419869254 valid 0.20470237471711109
LOSS train 0.24166707419869254 valid 0.2046322464233353
LOSS train 0.24166707419869254 valid 0.20452557811445563
LOSS train 0.24166707419869254 valid 0.2045734463767572
LOSS train 0.24166707419869254 valid 0.20464231854007728
LOSS train 0.24166707419869254 valid 0.20466751449267356
LOSS train 0.24166707419869254 valid 0.20468191799821614
LOSS train 0.24166707419869254 valid 0.20463422536315884
LOSS train 0.24166707419869254 valid 0.2045220387567367
LOSS train 0.24166707419869254 valid 0.2043782626129554
LOSS train 0.24166707419869254 valid 0.20432578331084117
LOSS train 0.24166707419869254 valid 0.20431867334004425
LOSS train 0.24166707419869254 valid 0.20427192461637544
LOSS train 0.24166707419869254 valid 0.2043102984627088
LOSS train 0.24166707419869254 valid 0.20427745354029683
LOSS train 0.24166707419869254 valid 0.2042239178815785
LOSS train 0.24166707419869254 valid 0.20418731099925935
LOSS train 0.24166707419869254 valid 0.20410632744276813
LOSS train 0.24166707419869254 valid 0.203980012212334
LOSS train 0.24166707419869254 valid 0.20398615349301769
LOSS train 0.24166707419869254 valid 0.20394304376861003
LOSS train 0.24166707419869254 valid 0.2039444373075059
LOSS train 0.24166707419869254 valid 0.20395595988347417
LOSS train 0.24166707419869254 valid 0.20393978516429156
LOSS train 0.24166707419869254 valid 0.203905788149584
LOSS train 0.24166707419869254 valid 0.20383361852429932
LOSS train 0.24166707419869254 valid 0.20392518118023872
LOSS train 0.24166707419869254 valid 0.20390582874566815
LOSS train 0.24166707419869254 valid 0.2038731918980678
LOSS train 0.24166707419869254 valid 0.20376020216961635
LOSS train 0.24166707419869254 valid 0.20376893722557074
LOSS train 0.24166707419869254 valid 0.2036618866985387
LOSS train 0.24166707419869254 valid 0.2036684382167694
LOSS train 0.24166707419869254 valid 0.20362193044580398
LOSS train 0.24166707419869254 valid 0.2038648116578853
LOSS train 0.24166707419869254 valid 0.20382856383482875
LOSS train 0.24166707419869254 valid 0.2038792267973934
LOSS train 0.24166707419869254 valid 0.20389177167975014
LOSS train 0.24166707419869254 valid 0.20389343939961926
LOSS train 0.24166707419869254 valid 0.2038953066399243
LOSS train 0.24166707419869254 valid 0.20389177251416138
LOSS train 0.24166707419869254 valid 0.20401559205957875
LOSS train 0.24166707419869254 valid 0.20399055627595847
LOSS train 0.24166707419869254 valid 0.20393305655036653
LOSS train 0.24166707419869254 valid 0.20387825801308396
LOSS train 0.24166707419869254 valid 0.2039105035482145
LOSS train 0.24166707419869254 valid 0.2039058086866478
LOSS train 0.24166707419869254 valid 0.20386854152787814
LOSS train 0.24166707419869254 valid 0.20372944998089224
LOSS train 0.24166707419869254 valid 0.2038727336537058
LOSS train 0.24166707419869254 valid 0.20388556810917322
LOSS train 0.24166707419869254 valid 0.20378174079184191
LOSS train 0.24166707419869254 valid 0.20379517303296812
LOSS train 0.24166707419869254 valid 0.20367341369390488
LOSS train 0.24166707419869254 valid 0.20372405824211479
LOSS train 0.24166707419869254 valid 0.20361842426958435
LOSS train 0.24166707419869254 valid 0.20357116432179037
LOSS train 0.24166707419869254 valid 0.2035613442490891
LOSS train 0.24166707419869254 valid 0.20357883005882754
LOSS train 0.24166707419869254 valid 0.203508185723577
LOSS train 0.24166707419869254 valid 0.20347098869282798
LOSS train 0.24166707419869254 valid 0.20362073091325816
LOSS train 0.24166707419869254 valid 0.2035548059035561
LOSS train 0.24166707419869254 valid 0.20342623380137914
LOSS train 0.24166707419869254 valid 0.20335851363571628
LOSS train 0.24166707419869254 valid 0.2034667734089874
LOSS train 0.24166707419869254 valid 0.20346343206495224
LOSS train 0.24166707419869254 valid 0.20332872307142325
LOSS train 0.24166707419869254 valid 0.20342260114848615
LOSS train 0.24166707419869254 valid 0.2033443019842822
LOSS train 0.24166707419869254 valid 0.2033279051688331
LOSS train 0.24166707419869254 valid 0.20322039460562408
LOSS train 0.24166707419869254 valid 0.20332869413027235
LOSS train 0.24166707419869254 valid 0.20342895757892857
LOSS train 0.24166707419869254 valid 0.20336795613341938
LOSS train 0.24166707419869254 valid 0.20326830174033167
LOSS train 0.24166707419869254 valid 0.2033809359003415
LOSS train 0.24166707419869254 valid 0.20334370154620582
LOSS train 0.24166707419869254 valid 0.203364587277174
LOSS train 0.24166707419869254 valid 0.2034406852391031
LOSS train 0.24166707419869254 valid 0.2034928281579844
LOSS train 0.24166707419869254 valid 0.20352577464523125
LOSS train 0.24166707419869254 valid 0.2035602619292709
LOSS train 0.24166707419869254 valid 0.20344233305101664
LOSS train 0.24166707419869254 valid 0.20349758112112457
LOSS train 0.24166707419869254 valid 0.2035415085549114
LOSS train 0.24166707419869254 valid 0.2034998130490327
LOSS train 0.24166707419869254 valid 0.2034325641800434
LOSS train 0.24166707419869254 valid 0.2035792171748148
LOSS train 0.24166707419869254 valid 0.20349050016036654
LOSS train 0.24166707419869254 valid 0.20354538746383968
LOSS train 0.24166707419869254 valid 0.20361346078230988
LOSS train 0.24166707419869254 valid 0.20353216557139223
LOSS train 0.24166707419869254 valid 0.2036219243113309
LOSS train 0.24166707419869254 valid 0.20360559361472808
LOSS train 0.24166707419869254 valid 0.2035491113482444
LOSS train 0.24166707419869254 valid 0.20342042274854105
LOSS train 0.24166707419869254 valid 0.20340219687236358
EPOCH 19:
  batch 1 loss: 0.25557851791381836
  batch 2 loss: 0.26966357231140137
  batch 3 loss: 0.25766361753145856
  batch 4 loss: 0.2568081095814705
  batch 5 loss: 0.2578020989894867
  batch 6 loss: 0.25433309376239777
  batch 7 loss: 0.2511478194168636
  batch 8 loss: 0.2484111227095127
  batch 9 loss: 0.2518601848019494
  batch 10 loss: 0.2478707179427147
  batch 11 loss: 0.24730496650392358
  batch 12 loss: 0.24579668790102005
  batch 13 loss: 0.2425215611091027
  batch 14 loss: 0.24246512459857122
  batch 15 loss: 0.24356505771478018
  batch 16 loss: 0.24304654262959957
  batch 17 loss: 0.24509240073316238
  batch 18 loss: 0.24369227720631492
  batch 19 loss: 0.2422160819957131
  batch 20 loss: 0.2406145878136158
  batch 21 loss: 0.24461740042482102
  batch 22 loss: 0.24561967971650037
  batch 23 loss: 0.24495846680972888
  batch 24 loss: 0.24605796486139297
  batch 25 loss: 0.24750829219818116
  batch 26 loss: 0.24752943160442206
  batch 27 loss: 0.2472406200788639
  batch 28 loss: 0.2483099289238453
  batch 29 loss: 0.24881260302560082
  batch 30 loss: 0.24998917132616044
  batch 31 loss: 0.24978269636631012
  batch 32 loss: 0.25061889784410596
  batch 33 loss: 0.25271445377306506
  batch 34 loss: 0.25354181186241265
  batch 35 loss: 0.2555762448481151
  batch 36 loss: 0.25537751822008026
  batch 37 loss: 0.2553970793614516
  batch 38 loss: 0.255935280338714
  batch 39 loss: 0.2569651607519541
  batch 40 loss: 0.2569143492728472
  batch 41 loss: 0.2566333373145359
  batch 42 loss: 0.256370814428443
  batch 43 loss: 0.25714548555917516
  batch 44 loss: 0.2586238238621842
  batch 45 loss: 0.2584320237239202
  batch 46 loss: 0.25718208726333536
  batch 47 loss: 0.25698009038225134
  batch 48 loss: 0.257871356792748
  batch 49 loss: 0.25946932514103094
  batch 50 loss: 0.26010084837675096
  batch 51 loss: 0.2600316484184826
  batch 52 loss: 0.26023165165231776
  batch 53 loss: 0.26034580508493027
  batch 54 loss: 0.26036425017648274
  batch 55 loss: 0.2608861801299182
  batch 56 loss: 0.261389802343079
  batch 57 loss: 0.2611902761354781
  batch 58 loss: 0.2612765829624801
  batch 59 loss: 0.261850044131279
  batch 60 loss: 0.2622341257830461
  batch 61 loss: 0.26236604886953946
  batch 62 loss: 0.2625918905100515
  batch 63 loss: 0.26135527638215866
  batch 64 loss: 0.2610223889350891
  batch 65 loss: 0.2606680196065169
  batch 66 loss: 0.2605550374948617
  batch 67 loss: 0.26043981714035147
  batch 68 loss: 0.2606751116759637
  batch 69 loss: 0.26064995708672895
  batch 70 loss: 0.2604181319475174
  batch 71 loss: 0.260533782378049
  batch 72 loss: 0.26038446173899704
  batch 73 loss: 0.2604023016070666
  batch 74 loss: 0.2602374209342776
  batch 75 loss: 0.26020287613073984
  batch 76 loss: 0.26028976726688835
  batch 77 loss: 0.26001432131637225
  batch 78 loss: 0.2599824955448126
  batch 79 loss: 0.25962408226502093
  batch 80 loss: 0.25927579645067456
  batch 81 loss: 0.25924225077952867
  batch 82 loss: 0.2594084036422939
  batch 83 loss: 0.25865550806005316
  batch 84 loss: 0.2578079335036732
  batch 85 loss: 0.25831615749527426
  batch 86 loss: 0.25812423645063887
  batch 87 loss: 0.2576503774215435
  batch 88 loss: 0.2572197544981133
  batch 89 loss: 0.25675116596596964
  batch 90 loss: 0.2567695067988502
  batch 91 loss: 0.2564364193560003
  batch 92 loss: 0.25607226319287135
  batch 93 loss: 0.2554066328592198
  batch 94 loss: 0.255015345012888
  batch 95 loss: 0.25490679364455376
  batch 96 loss: 0.2547579347155988
  batch 97 loss: 0.255116488362096
  batch 98 loss: 0.2547576357515491
  batch 99 loss: 0.2543353194540197
  batch 100 loss: 0.2545105817914009
  batch 101 loss: 0.25435273260763375
  batch 102 loss: 0.2544696507792847
  batch 103 loss: 0.2541418036500227
  batch 104 loss: 0.2543471706314729
  batch 105 loss: 0.2541217891942887
  batch 106 loss: 0.2541538429147792
  batch 107 loss: 0.2542062371133644
  batch 108 loss: 0.2537224303793024
  batch 109 loss: 0.2537898019366308
  batch 110 loss: 0.25370314771478825
  batch 111 loss: 0.2536407086226317
  batch 112 loss: 0.2533194161951542
  batch 113 loss: 0.2531232941994625
  batch 114 loss: 0.25311924125018875
  batch 115 loss: 0.25285229605177173
  batch 116 loss: 0.25253879330281553
  batch 117 loss: 0.25241289536158246
  batch 118 loss: 0.25246563831628377
  batch 119 loss: 0.25251130071006905
  batch 120 loss: 0.2522621323664983
  batch 121 loss: 0.2521531569563653
  batch 122 loss: 0.25191146697177264
  batch 123 loss: 0.2515180983194491
  batch 124 loss: 0.2515502641758611
  batch 125 loss: 0.25103843069076537
  batch 126 loss: 0.25102681430086254
  batch 127 loss: 0.25114209480642335
  batch 128 loss: 0.25104557059239596
  batch 129 loss: 0.25118177309054734
  batch 130 loss: 0.25104652436879965
  batch 131 loss: 0.2508331202823697
  batch 132 loss: 0.2508411281036608
  batch 133 loss: 0.2506314703172311
  batch 134 loss: 0.2504165989916716
  batch 135 loss: 0.2501516819000244
  batch 136 loss: 0.25031768990790143
  batch 137 loss: 0.2504916715360906
  batch 138 loss: 0.25037679378537164
  batch 139 loss: 0.250253924362951
  batch 140 loss: 0.25066294755254476
  batch 141 loss: 0.2503568269048177
  batch 142 loss: 0.24997810179918584
  batch 143 loss: 0.2501084243917799
  batch 144 loss: 0.24999595971571076
  batch 145 loss: 0.24986498797761983
  batch 146 loss: 0.24990477739539865
  batch 147 loss: 0.2497847008259118
  batch 148 loss: 0.24963896097363653
  batch 149 loss: 0.24928297512483277
  batch 150 loss: 0.24927807907263438
  batch 151 loss: 0.24884249526538596
  batch 152 loss: 0.24864886965798705
  batch 153 loss: 0.24868341335673738
  batch 154 loss: 0.248341339845936
  batch 155 loss: 0.24831727291307143
  batch 156 loss: 0.2482140518915959
  batch 157 loss: 0.24813656414010724
  batch 158 loss: 0.248115017067028
  batch 159 loss: 0.24827899854138213
  batch 160 loss: 0.24833863638341427
  batch 161 loss: 0.24820041304789714
  batch 162 loss: 0.24826786713099774
  batch 163 loss: 0.24803303300968707
  batch 164 loss: 0.2482375715200494
  batch 165 loss: 0.24840381886019852
  batch 166 loss: 0.24849564267928342
  batch 167 loss: 0.24839229908532012
  batch 168 loss: 0.24813276130173886
  batch 169 loss: 0.24812794860298112
  batch 170 loss: 0.24803904338794597
  batch 171 loss: 0.2482232957729819
  batch 172 loss: 0.2481693595128004
  batch 173 loss: 0.24811709090808912
  batch 174 loss: 0.24815452158793636
  batch 175 loss: 0.24823405291352954
  batch 176 loss: 0.24840610804544253
  batch 177 loss: 0.24853991628703426
  batch 178 loss: 0.24840352625659343
  batch 179 loss: 0.24833635960877273
  batch 180 loss: 0.24825087131725418
  batch 181 loss: 0.24824341258949997
  batch 182 loss: 0.24813381803559734
  batch 183 loss: 0.24802033342624621
  batch 184 loss: 0.2480591416520917
  batch 185 loss: 0.24782437086105347
  batch 186 loss: 0.24776286439549539
  batch 187 loss: 0.24792416760309494
  batch 188 loss: 0.24782330756808849
  batch 189 loss: 0.24777657810657744
  batch 190 loss: 0.24756479231934797
  batch 191 loss: 0.24754733916040492
  batch 192 loss: 0.24736736443204185
  batch 193 loss: 0.24734482091943216
  batch 194 loss: 0.24719999308131405
  batch 195 loss: 0.24729865201008625
  batch 196 loss: 0.24723685327537206
  batch 197 loss: 0.24732436133822813
  batch 198 loss: 0.2472068216614049
  batch 199 loss: 0.24722135733419925
  batch 200 loss: 0.2472310245782137
  batch 201 loss: 0.24711012617865605
  batch 202 loss: 0.24721847297531543
  batch 203 loss: 0.24720528090528668
  batch 204 loss: 0.24745764817092933
  batch 205 loss: 0.24744002586457786
  batch 206 loss: 0.2474505988862908
  batch 207 loss: 0.24738342743276973
  batch 208 loss: 0.24726750379284987
  batch 209 loss: 0.24734297715590903
  batch 210 loss: 0.24738414677835646
  batch 211 loss: 0.24731664903356002
  batch 212 loss: 0.24733760221949164
  batch 213 loss: 0.2472929425642524
  batch 214 loss: 0.24718199593720036
  batch 215 loss: 0.24716898722704067
  batch 216 loss: 0.24713734889196026
  batch 217 loss: 0.24715302609902923
  batch 218 loss: 0.24705088193263483
  batch 219 loss: 0.2466512799399084
  batch 220 loss: 0.24684962447394024
  batch 221 loss: 0.24679665860816904
  batch 222 loss: 0.24674278303041114
  batch 223 loss: 0.24652353483732506
  batch 224 loss: 0.24656824548063533
  batch 225 loss: 0.2465999553600947
  batch 226 loss: 0.24660115682445796
  batch 227 loss: 0.2465782886297167
  batch 228 loss: 0.24656143117892115
  batch 229 loss: 0.24665866395271502
  batch 230 loss: 0.2466872750417046
  batch 231 loss: 0.24669225713907383
  batch 232 loss: 0.2466387374904649
  batch 233 loss: 0.24668780199447926
  batch 234 loss: 0.2466706543778762
  batch 235 loss: 0.2465820370836461
  batch 236 loss: 0.24653486414986142
  batch 237 loss: 0.24636394615414775
  batch 238 loss: 0.24635890583280756
  batch 239 loss: 0.24635089890228654
  batch 240 loss: 0.24648147349556287
  batch 241 loss: 0.24640025965158374
  batch 242 loss: 0.24631245748316946
  batch 243 loss: 0.2462596508823795
  batch 244 loss: 0.2461879195370635
  batch 245 loss: 0.24621724334298348
  batch 246 loss: 0.2461878544067949
  batch 247 loss: 0.24624549262678092
  batch 248 loss: 0.2462970080274728
  batch 249 loss: 0.24638227974793997
  batch 250 loss: 0.24639486062526703
  batch 251 loss: 0.24629791805231238
  batch 252 loss: 0.24626424530195812
  batch 253 loss: 0.24616841918866153
  batch 254 loss: 0.24610936055033225
  batch 255 loss: 0.2459950192301881
  batch 256 loss: 0.2458786609931849
  batch 257 loss: 0.24592110230533065
  batch 258 loss: 0.24602019145738246
  batch 259 loss: 0.24587476190222737
  batch 260 loss: 0.245865482206528
  batch 261 loss: 0.2458732091033139
  batch 262 loss: 0.2457689115678081
  batch 263 loss: 0.2456602626993629
  batch 264 loss: 0.2457067027800914
  batch 265 loss: 0.24558691826631437
  batch 266 loss: 0.24541846231410377
  batch 267 loss: 0.24530407345473543
  batch 268 loss: 0.24522233170582286
  batch 269 loss: 0.24524881920628389
  batch 270 loss: 0.24515159146653281
  batch 271 loss: 0.24515131253377978
  batch 272 loss: 0.24503508380011602
  batch 273 loss: 0.24487611012799398
  batch 274 loss: 0.24467257198191036
  batch 275 loss: 0.24464165470816873
  batch 276 loss: 0.24459106404928194
  batch 277 loss: 0.24450850981667585
  batch 278 loss: 0.2443780506686341
  batch 279 loss: 0.24443721578967187
  batch 280 loss: 0.24440990909934043
  batch 281 loss: 0.24438909698423542
  batch 282 loss: 0.24421770743867185
  batch 283 loss: 0.2441546551540968
  batch 284 loss: 0.24408063413181774
  batch 285 loss: 0.24398629858828427
  batch 286 loss: 0.24394631917243237
  batch 287 loss: 0.2437836516401909
  batch 288 loss: 0.24377041273853844
  batch 289 loss: 0.24363862302889047
  batch 290 loss: 0.24349664850481625
  batch 291 loss: 0.2434686067997385
  batch 292 loss: 0.2433380973461556
  batch 293 loss: 0.2432662761760653
  batch 294 loss: 0.24296301770575193
  batch 295 loss: 0.24292051494121553
  batch 296 loss: 0.24294507287039951
  batch 297 loss: 0.2428476430149592
  batch 298 loss: 0.24281760145153775
  batch 299 loss: 0.24285113617129947
  batch 300 loss: 0.24269522349039713
  batch 301 loss: 0.24269044602234102
  batch 302 loss: 0.24283584819132129
  batch 303 loss: 0.2427501112911174
  batch 304 loss: 0.24273608762182688
  batch 305 loss: 0.2427326049472465
  batch 306 loss: 0.2427554621902946
  batch 307 loss: 0.24277957337104536
  batch 308 loss: 0.24294088127744662
  batch 309 loss: 0.2428735337886224
  batch 310 loss: 0.24291102996756953
  batch 311 loss: 0.24293093373729485
  batch 312 loss: 0.24295014415222865
  batch 313 loss: 0.2428977109087161
  batch 314 loss: 0.2428284445955495
  batch 315 loss: 0.24285834109972393
  batch 316 loss: 0.24305999580817886
  batch 317 loss: 0.2431532594871822
  batch 318 loss: 0.24294571259861472
  batch 319 loss: 0.24302550496352504
  batch 320 loss: 0.24309541434049606
  batch 321 loss: 0.24312185284876006
  batch 322 loss: 0.24319330070699965
  batch 323 loss: 0.24324095073868246
  batch 324 loss: 0.24327860873790436
  batch 325 loss: 0.2433848143540896
  batch 326 loss: 0.24366560770324402
  batch 327 loss: 0.24374171973003889
  batch 328 loss: 0.24373568094721654
  batch 329 loss: 0.24373563603246104
  batch 330 loss: 0.24374066722212415
  batch 331 loss: 0.24375077762452496
  batch 332 loss: 0.24387956666479627
  batch 333 loss: 0.24390369112248178
  batch 334 loss: 0.24419665207227548
  batch 335 loss: 0.24433774374314207
  batch 336 loss: 0.24449792570833648
  batch 337 loss: 0.24454098900100243
  batch 338 loss: 0.24461216012225348
  batch 339 loss: 0.24475835958237493
  batch 340 loss: 0.24475073831922867
  batch 341 loss: 0.24469675774798016
  batch 342 loss: 0.24460621565928933
  batch 343 loss: 0.24475718999916887
  batch 344 loss: 0.24487353588432767
  batch 345 loss: 0.24493536253770193
  batch 346 loss: 0.24493704103148742
  batch 347 loss: 0.2448050328651148
  batch 348 loss: 0.2448175615020867
  batch 349 loss: 0.24485065820053178
  batch 350 loss: 0.24510058586086544
  batch 351 loss: 0.2451310361976977
  batch 352 loss: 0.24505594410848888
  batch 353 loss: 0.24503076908102117
  batch 354 loss: 0.244974081588667
  batch 355 loss: 0.24491142416504066
  batch 356 loss: 0.24499571536866466
  batch 357 loss: 0.2448618063489262
  batch 358 loss: 0.2447878185597212
  batch 359 loss: 0.24471431360437346
  batch 360 loss: 0.2446723142431842
  batch 361 loss: 0.2446588982835701
  batch 362 loss: 0.24463673322898907
  batch 363 loss: 0.24464474918100132
  batch 364 loss: 0.244559550514588
  batch 365 loss: 0.24466228248321847
  batch 366 loss: 0.24465901278407196
  batch 367 loss: 0.24474348667856782
  batch 368 loss: 0.24473814224905294
  batch 369 loss: 0.2447393598756816
  batch 370 loss: 0.24477514028549194
  batch 371 loss: 0.24484227674026696
  batch 372 loss: 0.24492807013373222
  batch 373 loss: 0.24494446674236983
  batch 374 loss: 0.2448309107339956
  batch 375 loss: 0.24498345561822255
  batch 376 loss: 0.2451934765707305
  batch 377 loss: 0.24522390633584334
  batch 378 loss: 0.24524598269077835
  batch 379 loss: 0.2452362108985478
  batch 380 loss: 0.2452206889265462
  batch 381 loss: 0.2451816095611242
  batch 382 loss: 0.24524561951609805
  batch 383 loss: 0.24516953761365956
  batch 384 loss: 0.24521835571310172
  batch 385 loss: 0.2451861251871307
  batch 386 loss: 0.2451679378825148
  batch 387 loss: 0.24512725175197117
  batch 388 loss: 0.24501058474643944
  batch 389 loss: 0.24497852886244081
  batch 390 loss: 0.24496398583436624
  batch 391 loss: 0.24495746747916922
  batch 392 loss: 0.2450701055326024
  batch 393 loss: 0.2451216947789714
  batch 394 loss: 0.24514158312136752
  batch 395 loss: 0.2451262139066865
  batch 396 loss: 0.24517805678675872
  batch 397 loss: 0.24523868488724945
  batch 398 loss: 0.245337808626381
  batch 399 loss: 0.2453092869212454
  batch 400 loss: 0.24545483388006686
  batch 401 loss: 0.24551580523017635
  batch 402 loss: 0.24553253653630688
  batch 403 loss: 0.2455669864827291
  batch 404 loss: 0.2456062338729896
  batch 405 loss: 0.24563463116869513
  batch 406 loss: 0.24554091229520994
  batch 407 loss: 0.24559817391765792
  batch 408 loss: 0.2454941977618956
  batch 409 loss: 0.2454350225645639
  batch 410 loss: 0.24527553120037404
  batch 411 loss: 0.2452544597523636
  batch 412 loss: 0.24533499502441258
  batch 413 loss: 0.245380892303319
  batch 414 loss: 0.24543204212534256
  batch 415 loss: 0.24537562020571835
  batch 416 loss: 0.24537971103563905
  batch 417 loss: 0.245370420298988
  batch 418 loss: 0.24534113191816795
  batch 419 loss: 0.24536448624651871
  batch 420 loss: 0.24538042694330214
  batch 421 loss: 0.2455060198867689
  batch 422 loss: 0.24549269912790914
  batch 423 loss: 0.245495357834701
  batch 424 loss: 0.2455243542790413
  batch 425 loss: 0.2454793899199542
  batch 426 loss: 0.24541577130136355
  batch 427 loss: 0.24537743816470653
  batch 428 loss: 0.24541786831812323
  batch 429 loss: 0.24528986903337333
  batch 430 loss: 0.24529902675124102
  batch 431 loss: 0.2453084780279279
  batch 432 loss: 0.24538169132062682
  batch 433 loss: 0.24532301947646548
  batch 434 loss: 0.24520345190153695
  batch 435 loss: 0.24526717258595873
  batch 436 loss: 0.24523329194806037
  batch 437 loss: 0.24516958533626398
  batch 438 loss: 0.24512537943037677
  batch 439 loss: 0.245049133353733
  batch 440 loss: 0.24509412182325666
  batch 441 loss: 0.24508277629238137
  batch 442 loss: 0.24508057342530376
  batch 443 loss: 0.24506130659822417
  batch 444 loss: 0.24506671357530732
  batch 445 loss: 0.24502021224980944
  batch 446 loss: 0.24498019136922777
  batch 447 loss: 0.24496313229503247
  batch 448 loss: 0.24504655074062093
  batch 449 loss: 0.24496938659911166
  batch 450 loss: 0.2449460628297594
  batch 451 loss: 0.2448529704513148
  batch 452 loss: 0.24479460597565744
  batch 453 loss: 0.24477647942291453
  batch 454 loss: 0.2447936306769102
  batch 455 loss: 0.24477229799543107
  batch 456 loss: 0.24469743639622865
  batch 457 loss: 0.2446622589289229
  batch 458 loss: 0.24462502563624403
  batch 459 loss: 0.24461236492221392
  batch 460 loss: 0.24454877587116283
  batch 461 loss: 0.24455402665438208
  batch 462 loss: 0.24457087080716056
  batch 463 loss: 0.24456675611355908
  batch 464 loss: 0.24446132891522399
  batch 465 loss: 0.2444080969659231
  batch 466 loss: 0.24431120233346465
  batch 467 loss: 0.2444348109674658
  batch 468 loss: 0.24432265284097093
  batch 469 loss: 0.24421481556221367
  batch 470 loss: 0.244239087371116
  batch 471 loss: 0.24426457342828156
  batch 472 loss: 0.2440395142125376
LOSS train 0.2440395142125376 valid 0.22181853652000427
LOSS train 0.2440395142125376 valid 0.20856234431266785
LOSS train 0.2440395142125376 valid 0.21635513504346213
LOSS train 0.2440395142125376 valid 0.2016686648130417
LOSS train 0.2440395142125376 valid 0.1992560863494873
LOSS train 0.2440395142125376 valid 0.20204655826091766
LOSS train 0.2440395142125376 valid 0.19730667557035172
LOSS train 0.2440395142125376 valid 0.19442634284496307
LOSS train 0.2440395142125376 valid 0.19302176104651558
LOSS train 0.2440395142125376 valid 0.19106689244508743
LOSS train 0.2440395142125376 valid 0.1906010793014006
LOSS train 0.2440395142125376 valid 0.19298329701026282
LOSS train 0.2440395142125376 valid 0.19319022848055914
LOSS train 0.2440395142125376 valid 0.19199809857777186
LOSS train 0.2440395142125376 valid 0.19180761377016703
LOSS train 0.2440395142125376 valid 0.1953505091369152
LOSS train 0.2440395142125376 valid 0.19566733170958125
LOSS train 0.2440395142125376 valid 0.19501030776235792
LOSS train 0.2440395142125376 valid 0.19733573888477526
LOSS train 0.2440395142125376 valid 0.1968896545469761
LOSS train 0.2440395142125376 valid 0.19904762364569165
LOSS train 0.2440395142125376 valid 0.19867025722156872
LOSS train 0.2440395142125376 valid 0.1961868353511976
LOSS train 0.2440395142125376 valid 0.1965490560978651
LOSS train 0.2440395142125376 valid 0.19618523597717286
LOSS train 0.2440395142125376 valid 0.1956487103150441
LOSS train 0.2440395142125376 valid 0.1952578475077947
LOSS train 0.2440395142125376 valid 0.1960197633930615
LOSS train 0.2440395142125376 valid 0.19486111145595025
LOSS train 0.2440395142125376 valid 0.1940507267912229
LOSS train 0.2440395142125376 valid 0.19482947885990143
LOSS train 0.2440395142125376 valid 0.19464252516627312
LOSS train 0.2440395142125376 valid 0.1932116864305554
LOSS train 0.2440395142125376 valid 0.192853161955581
LOSS train 0.2440395142125376 valid 0.19328169098922185
LOSS train 0.2440395142125376 valid 0.19394208780593342
LOSS train 0.2440395142125376 valid 0.1944004470432127
LOSS train 0.2440395142125376 valid 0.1942433503113295
LOSS train 0.2440395142125376 valid 0.19518192341694465
LOSS train 0.2440395142125376 valid 0.1955531846731901
LOSS train 0.2440395142125376 valid 0.19545344463208827
LOSS train 0.2440395142125376 valid 0.19662768925939286
LOSS train 0.2440395142125376 valid 0.19709172567655875
LOSS train 0.2440395142125376 valid 0.19685882770202376
LOSS train 0.2440395142125376 valid 0.19645745356877645
LOSS train 0.2440395142125376 valid 0.19631471420111862
LOSS train 0.2440395142125376 valid 0.19607598445516952
LOSS train 0.2440395142125376 valid 0.19755868085970482
LOSS train 0.2440395142125376 valid 0.19691119723174036
LOSS train 0.2440395142125376 valid 0.19771991848945616
LOSS train 0.2440395142125376 valid 0.1973253184673833
LOSS train 0.2440395142125376 valid 0.1971738705268273
LOSS train 0.2440395142125376 valid 0.1983181541820742
LOSS train 0.2440395142125376 valid 0.1983622999654876
LOSS train 0.2440395142125376 valid 0.1982502818107605
LOSS train 0.2440395142125376 valid 0.19847895896860532
LOSS train 0.2440395142125376 valid 0.19794181302974098
LOSS train 0.2440395142125376 valid 0.19841782055024443
LOSS train 0.2440395142125376 valid 0.19813478371854557
LOSS train 0.2440395142125376 valid 0.1979327661295732
LOSS train 0.2440395142125376 valid 0.1980177230033718
LOSS train 0.2440395142125376 valid 0.1977508976574867
LOSS train 0.2440395142125376 valid 0.19741623245534443
LOSS train 0.2440395142125376 valid 0.1978920737747103
LOSS train 0.2440395142125376 valid 0.19679412589623377
LOSS train 0.2440395142125376 valid 0.19672507070230716
LOSS train 0.2440395142125376 valid 0.19756061394712818
LOSS train 0.2440395142125376 valid 0.1970064554582624
LOSS train 0.2440395142125376 valid 0.19758732681689056
LOSS train 0.2440395142125376 valid 0.1981350126011031
LOSS train 0.2440395142125376 valid 0.1985357363878841
LOSS train 0.2440395142125376 valid 0.19918889117737612
LOSS train 0.2440395142125376 valid 0.19978231028334736
LOSS train 0.2440395142125376 valid 0.1994908305036055
LOSS train 0.2440395142125376 valid 0.19920372744401296
LOSS train 0.2440395142125376 valid 0.1993747149643145
LOSS train 0.2440395142125376 valid 0.1995525990987753
LOSS train 0.2440395142125376 valid 0.1995071073373159
LOSS train 0.2440395142125376 valid 0.19937144540533236
LOSS train 0.2440395142125376 valid 0.19903288893401622
LOSS train 0.2440395142125376 valid 0.19930579743267576
LOSS train 0.2440395142125376 valid 0.19917428457155462
LOSS train 0.2440395142125376 valid 0.19951254488473916
LOSS train 0.2440395142125376 valid 0.19923037485707373
LOSS train 0.2440395142125376 valid 0.20002501414102666
LOSS train 0.2440395142125376 valid 0.20001388705053993
LOSS train 0.2440395142125376 valid 0.19977043808876782
LOSS train 0.2440395142125376 valid 0.1999175770377571
LOSS train 0.2440395142125376 valid 0.20029502655013223
LOSS train 0.2440395142125376 valid 0.20065619597832363
LOSS train 0.2440395142125376 valid 0.20062231313396287
LOSS train 0.2440395142125376 valid 0.20081905146007953
LOSS train 0.2440395142125376 valid 0.200578069013934
LOSS train 0.2440395142125376 valid 0.20085751899379364
LOSS train 0.2440395142125376 valid 0.2010300918629295
LOSS train 0.2440395142125376 valid 0.2011242980758349
LOSS train 0.2440395142125376 valid 0.20122485446561242
LOSS train 0.2440395142125376 valid 0.2016425091697245
LOSS train 0.2440395142125376 valid 0.2018228546537534
LOSS train 0.2440395142125376 valid 0.2018912887573242
LOSS train 0.2440395142125376 valid 0.20186686161721107
LOSS train 0.2440395142125376 valid 0.2021993185375251
LOSS train 0.2440395142125376 valid 0.20201921954895685
LOSS train 0.2440395142125376 valid 0.20183289423584938
LOSS train 0.2440395142125376 valid 0.20207841722738176
LOSS train 0.2440395142125376 valid 0.20218991952122384
LOSS train 0.2440395142125376 valid 0.2019046685684507
LOSS train 0.2440395142125376 valid 0.20206824897064102
LOSS train 0.2440395142125376 valid 0.20164510922147594
LOSS train 0.2440395142125376 valid 0.20157891281626442
LOSS train 0.2440395142125376 valid 0.20159661206039223
LOSS train 0.2440395142125376 valid 0.2019117662150945
LOSS train 0.2440395142125376 valid 0.20170121039964456
LOSS train 0.2440395142125376 valid 0.20184334929574999
LOSS train 0.2440395142125376 valid 0.20254456634106843
LOSS train 0.2440395142125376 valid 0.20216095717302685
LOSS train 0.2440395142125376 valid 0.20290044956227654
LOSS train 0.2440395142125376 valid 0.20272694982714573
LOSS train 0.2440395142125376 valid 0.20251375987750142
LOSS train 0.2440395142125376 valid 0.2020960090061029
LOSS train 0.2440395142125376 valid 0.20185662806034088
LOSS train 0.2440395142125376 valid 0.2019951120018959
LOSS train 0.2440395142125376 valid 0.20204707834778762
LOSS train 0.2440395142125376 valid 0.20234076871025947
LOSS train 0.2440395142125376 valid 0.2022459876537323
LOSS train 0.2440395142125376 valid 0.2023916676167458
LOSS train 0.2440395142125376 valid 0.2022222813189499
LOSS train 0.2440395142125376 valid 0.20221232378389686
LOSS train 0.2440395142125376 valid 0.2019231166950492
LOSS train 0.2440395142125376 valid 0.201684458897664
LOSS train 0.2440395142125376 valid 0.2017033596075218
LOSS train 0.2440395142125376 valid 0.20163199666774634
LOSS train 0.2440395142125376 valid 0.2016692994008387
LOSS train 0.2440395142125376 valid 0.20187246699386568
LOSS train 0.2440395142125376 valid 0.20194968437706984
LOSS train 0.2440395142125376 valid 0.20193767087424502
LOSS train 0.2440395142125376 valid 0.20193315792257768
LOSS train 0.2440395142125376 valid 0.2018026827254157
LOSS train 0.2440395142125376 valid 0.20162658189698088
LOSS train 0.2440395142125376 valid 0.20153956753867014
LOSS train 0.2440395142125376 valid 0.20166332766096642
LOSS train 0.2440395142125376 valid 0.20185316574405615
LOSS train 0.2440395142125376 valid 0.20192641738828246
LOSS train 0.2440395142125376 valid 0.2019995397163762
LOSS train 0.2440395142125376 valid 0.20191144079997622
LOSS train 0.2440395142125376 valid 0.20181806018091228
LOSS train 0.2440395142125376 valid 0.201650633698418
LOSS train 0.2440395142125376 valid 0.20161961643276988
LOSS train 0.2440395142125376 valid 0.20169352585037284
LOSS train 0.2440395142125376 valid 0.20166905403137206
LOSS train 0.2440395142125376 valid 0.20154737202536982
LOSS train 0.2440395142125376 valid 0.20146001659725843
LOSS train 0.2440395142125376 valid 0.2012349540501638
LOSS train 0.2440395142125376 valid 0.20131297661112502
LOSS train 0.2440395142125376 valid 0.2013978410151697
LOSS train 0.2440395142125376 valid 0.20132920738214102
LOSS train 0.2440395142125376 valid 0.2012936888606685
LOSS train 0.2440395142125376 valid 0.20142990567638905
LOSS train 0.2440395142125376 valid 0.20153294616150405
LOSS train 0.2440395142125376 valid 0.2016659826040268
LOSS train 0.2440395142125376 valid 0.2016010632426102
LOSS train 0.2440395142125376 valid 0.2017020145316183
LOSS train 0.2440395142125376 valid 0.20154938650277496
LOSS train 0.2440395142125376 valid 0.20160957789275705
LOSS train 0.2440395142125376 valid 0.2017477658661929
LOSS train 0.2440395142125376 valid 0.20165690779685974
LOSS train 0.2440395142125376 valid 0.20164786555809888
LOSS train 0.2440395142125376 valid 0.20157231922660554
LOSS train 0.2440395142125376 valid 0.20133962319094753
LOSS train 0.2440395142125376 valid 0.2014310071573538
LOSS train 0.2440395142125376 valid 0.2013967033715276
LOSS train 0.2440395142125376 valid 0.20144484486690786
LOSS train 0.2440395142125376 valid 0.20126175690937592
LOSS train 0.2440395142125376 valid 0.20167618955688915
LOSS train 0.2440395142125376 valid 0.20163106671401432
LOSS train 0.2440395142125376 valid 0.20151007404042917
LOSS train 0.2440395142125376 valid 0.20156517927929507
LOSS train 0.2440395142125376 valid 0.20167345670836695
LOSS train 0.2440395142125376 valid 0.20183241825196996
LOSS train 0.2440395142125376 valid 0.20165919901596174
LOSS train 0.2440395142125376 valid 0.20167247942798044
LOSS train 0.2440395142125376 valid 0.20149160978885797
LOSS train 0.2440395142125376 valid 0.2015839881910001
LOSS train 0.2440395142125376 valid 0.20145587785088498
LOSS train 0.2440395142125376 valid 0.20128167368270256
LOSS train 0.2440395142125376 valid 0.2013601994802875
LOSS train 0.2440395142125376 valid 0.20130818643990686
LOSS train 0.2440395142125376 valid 0.20132306638233205
LOSS train 0.2440395142125376 valid 0.2011457706885363
LOSS train 0.2440395142125376 valid 0.20110387096279544
LOSS train 0.2440395142125376 valid 0.2012066044576505
LOSS train 0.2440395142125376 valid 0.20126414927653968
LOSS train 0.2440395142125376 valid 0.2010538699250147
LOSS train 0.2440395142125376 valid 0.20089789848659456
LOSS train 0.2440395142125376 valid 0.20083133937456668
LOSS train 0.2440395142125376 valid 0.20088689166064166
LOSS train 0.2440395142125376 valid 0.20088097392605042
LOSS train 0.2440395142125376 valid 0.20113175008634124
LOSS train 0.2440395142125376 valid 0.20110112975290673
LOSS train 0.2440395142125376 valid 0.20118557386100291
LOSS train 0.2440395142125376 valid 0.20101654329406682
LOSS train 0.2440395142125376 valid 0.20099449998671465
LOSS train 0.2440395142125376 valid 0.20105249665934463
LOSS train 0.2440395142125376 valid 0.2008794084483502
LOSS train 0.2440395142125376 valid 0.20082469495331368
LOSS train 0.2440395142125376 valid 0.20085103723030645
LOSS train 0.2440395142125376 valid 0.20083861528099448
LOSS train 0.2440395142125376 valid 0.20072877450058094
LOSS train 0.2440395142125376 valid 0.2007553238047367
LOSS train 0.2440395142125376 valid 0.20090813892228263
LOSS train 0.2440395142125376 valid 0.20112806615106302
LOSS train 0.2440395142125376 valid 0.20095831422873264
LOSS train 0.2440395142125376 valid 0.2009481367230975
LOSS train 0.2440395142125376 valid 0.20089398930284463
LOSS train 0.2440395142125376 valid 0.200680433664211
LOSS train 0.2440395142125376 valid 0.20050100637254892
LOSS train 0.2440395142125376 valid 0.2002368103111944
LOSS train 0.2440395142125376 valid 0.20023806249603218
LOSS train 0.2440395142125376 valid 0.20023969965710486
LOSS train 0.2440395142125376 valid 0.20034106942740354
LOSS train 0.2440395142125376 valid 0.20030150006259728
LOSS train 0.2440395142125376 valid 0.2003516921320477
LOSS train 0.2440395142125376 valid 0.2004652485719176
LOSS train 0.2440395142125376 valid 0.20032441316704666
LOSS train 0.2440395142125376 valid 0.20017578740914663
LOSS train 0.2440395142125376 valid 0.20003555108488133
LOSS train 0.2440395142125376 valid 0.20004869039100698
LOSS train 0.2440395142125376 valid 0.20002642999354162
LOSS train 0.2440395142125376 valid 0.19993463041480453
LOSS train 0.2440395142125376 valid 0.19997453832108042
LOSS train 0.2440395142125376 valid 0.19989598184436946
LOSS train 0.2440395142125376 valid 0.20000336068714486
LOSS train 0.2440395142125376 valid 0.20031120727246413
LOSS train 0.2440395142125376 valid 0.20022193457071596
LOSS train 0.2440395142125376 valid 0.2002911980481858
LOSS train 0.2440395142125376 valid 0.2001347630205801
LOSS train 0.2440395142125376 valid 0.200216503460196
LOSS train 0.2440395142125376 valid 0.2002157050521434
LOSS train 0.2440395142125376 valid 0.20019744748101573
LOSS train 0.2440395142125376 valid 0.19983123609175285
LOSS train 0.2440395142125376 valid 0.1997214600876654
LOSS train 0.2440395142125376 valid 0.19972198967598687
LOSS train 0.2440395142125376 valid 0.19952607878441672
LOSS train 0.2440395142125376 valid 0.19954852576627105
LOSS train 0.2440395142125376 valid 0.19968054878468416
LOSS train 0.2440395142125376 valid 0.19976547542141704
LOSS train 0.2440395142125376 valid 0.19980682438684377
LOSS train 0.2440395142125376 valid 0.1997882200465087
LOSS train 0.2440395142125376 valid 0.19958514920678963
LOSS train 0.2440395142125376 valid 0.1996437258720398
LOSS train 0.2440395142125376 valid 0.19966723386984897
LOSS train 0.2440395142125376 valid 0.1995216686101187
LOSS train 0.2440395142125376 valid 0.19935055241989996
LOSS train 0.2440395142125376 valid 0.19951721890939503
LOSS train 0.2440395142125376 valid 0.19944074416861815
LOSS train 0.2440395142125376 valid 0.19923341082176194
LOSS train 0.2440395142125376 valid 0.1991922438492571
LOSS train 0.2440395142125376 valid 0.1993484666296678
LOSS train 0.2440395142125376 valid 0.19924258852097057
LOSS train 0.2440395142125376 valid 0.1991117178247525
LOSS train 0.2440395142125376 valid 0.19919680812578092
LOSS train 0.2440395142125376 valid 0.1993677031447869
LOSS train 0.2440395142125376 valid 0.19919689486915168
LOSS train 0.2440395142125376 valid 0.19924350641667843
LOSS train 0.2440395142125376 valid 0.19917206123190107
LOSS train 0.2440395142125376 valid 0.19923360460906997
LOSS train 0.2440395142125376 valid 0.1991468816437525
LOSS train 0.2440395142125376 valid 0.19923481856709097
LOSS train 0.2440395142125376 valid 0.19938570283159449
LOSS train 0.2440395142125376 valid 0.1992111309810921
LOSS train 0.2440395142125376 valid 0.19922939124362496
LOSS train 0.2440395142125376 valid 0.19914661358822794
LOSS train 0.2440395142125376 valid 0.199068824559341
LOSS train 0.2440395142125376 valid 0.19897357048126904
LOSS train 0.2440395142125376 valid 0.19901800312779167
LOSS train 0.2440395142125376 valid 0.19908433207783147
LOSS train 0.2440395142125376 valid 0.19908573050791606
LOSS train 0.2440395142125376 valid 0.19909105203348956
LOSS train 0.2440395142125376 valid 0.199027693399819
LOSS train 0.2440395142125376 valid 0.19889480663197381
LOSS train 0.2440395142125376 valid 0.1987420650351514
LOSS train 0.2440395142125376 valid 0.19867584944194092
LOSS train 0.2440395142125376 valid 0.19869641459451548
LOSS train 0.2440395142125376 valid 0.19865904947821522
LOSS train 0.2440395142125376 valid 0.19870763964820326
LOSS train 0.2440395142125376 valid 0.19868201512348402
LOSS train 0.2440395142125376 valid 0.19863825589937614
LOSS train 0.2440395142125376 valid 0.19860169870985878
LOSS train 0.2440395142125376 valid 0.1985180532127928
LOSS train 0.2440395142125376 valid 0.1984035683089289
LOSS train 0.2440395142125376 valid 0.19840318118174052
LOSS train 0.2440395142125376 valid 0.19835471581310443
LOSS train 0.2440395142125376 valid 0.19834841080899937
LOSS train 0.2440395142125376 valid 0.19836212208076398
LOSS train 0.2440395142125376 valid 0.1983566920636064
LOSS train 0.2440395142125376 valid 0.19833736489149364
LOSS train 0.2440395142125376 valid 0.19827081804925745
LOSS train 0.2440395142125376 valid 0.19830153082441163
LOSS train 0.2440395142125376 valid 0.19829185132597602
LOSS train 0.2440395142125376 valid 0.19825528979301452
LOSS train 0.2440395142125376 valid 0.19815420876705767
LOSS train 0.2440395142125376 valid 0.19814419114826531
LOSS train 0.2440395142125376 valid 0.198031613633971
LOSS train 0.2440395142125376 valid 0.19804619801671883
LOSS train 0.2440395142125376 valid 0.19799948857455957
LOSS train 0.2440395142125376 valid 0.19826829000041377
LOSS train 0.2440395142125376 valid 0.19824746687948122
LOSS train 0.2440395142125376 valid 0.19828851375873988
LOSS train 0.2440395142125376 valid 0.1982802280909035
LOSS train 0.2440395142125376 valid 0.19827279905157705
LOSS train 0.2440395142125376 valid 0.19828301002166662
LOSS train 0.2440395142125376 valid 0.19827276769165808
LOSS train 0.2440395142125376 valid 0.19835598814411284
LOSS train 0.2440395142125376 valid 0.19834575664465595
LOSS train 0.2440395142125376 valid 0.1982831932722576
LOSS train 0.2440395142125376 valid 0.1982560528418686
LOSS train 0.2440395142125376 valid 0.1982736276339281
LOSS train 0.2440395142125376 valid 0.1982484408332117
LOSS train 0.2440395142125376 valid 0.19817811815903105
LOSS train 0.2440395142125376 valid 0.1980429499875754
LOSS train 0.2440395142125376 valid 0.1981930633850187
LOSS train 0.2440395142125376 valid 0.19820644243718674
LOSS train 0.2440395142125376 valid 0.19809438306665272
LOSS train 0.2440395142125376 valid 0.19811009137350835
LOSS train 0.2440395142125376 valid 0.19796211866232066
LOSS train 0.2440395142125376 valid 0.19803374530347578
LOSS train 0.2440395142125376 valid 0.19795369450287717
LOSS train 0.2440395142125376 valid 0.19792148520852007
LOSS train 0.2440395142125376 valid 0.19790933451029305
LOSS train 0.2440395142125376 valid 0.19792705503377048
LOSS train 0.2440395142125376 valid 0.19785547116912022
LOSS train 0.2440395142125376 valid 0.19783135162419582
LOSS train 0.2440395142125376 valid 0.19797786958403774
LOSS train 0.2440395142125376 valid 0.19791359606081854
LOSS train 0.2440395142125376 valid 0.19779433972799956
LOSS train 0.2440395142125376 valid 0.19771358273213818
LOSS train 0.2440395142125376 valid 0.19784945037315438
LOSS train 0.2440395142125376 valid 0.1978626665631695
LOSS train 0.2440395142125376 valid 0.19773380739674806
LOSS train 0.2440395142125376 valid 0.19782323052777964
LOSS train 0.2440395142125376 valid 0.19774588627899148
LOSS train 0.2440395142125376 valid 0.19772994871202268
LOSS train 0.2440395142125376 valid 0.1976277248244244
LOSS train 0.2440395142125376 valid 0.19771750570209914
LOSS train 0.2440395142125376 valid 0.1978159034597701
LOSS train 0.2440395142125376 valid 0.1977630289476042
LOSS train 0.2440395142125376 valid 0.19766274983498136
LOSS train 0.2440395142125376 valid 0.1977704680342784
LOSS train 0.2440395142125376 valid 0.19770905157727292
LOSS train 0.2440395142125376 valid 0.19772086871521813
LOSS train 0.2440395142125376 valid 0.19779650289618392
LOSS train 0.2440395142125376 valid 0.1978329365903681
LOSS train 0.2440395142125376 valid 0.19788038612425157
LOSS train 0.2440395142125376 valid 0.19791139216072814
LOSS train 0.2440395142125376 valid 0.19776026157426163
LOSS train 0.2440395142125376 valid 0.19779671770468188
LOSS train 0.2440395142125376 valid 0.19786095460589861
LOSS train 0.2440395142125376 valid 0.1978088060820569
LOSS train 0.2440395142125376 valid 0.1977335246575576
LOSS train 0.2440395142125376 valid 0.1978882467167245
LOSS train 0.2440395142125376 valid 0.19780042804674428
LOSS train 0.2440395142125376 valid 0.19786315468958068
LOSS train 0.2440395142125376 valid 0.19795049074267554
LOSS train 0.2440395142125376 valid 0.1978918659899916
LOSS train 0.2440395142125376 valid 0.197962480987588
LOSS train 0.2440395142125376 valid 0.19797254907466022
LOSS train 0.2440395142125376 valid 0.19790198919883542
LOSS train 0.2440395142125376 valid 0.19776333487876083
LOSS train 0.2440395142125376 valid 0.19771966459305307
EPOCH 20:
  batch 1 loss: 0.2364262044429779
  batch 2 loss: 0.2556479424238205
  batch 3 loss: 0.24573728938897452
  batch 4 loss: 0.24602237716317177
  batch 5 loss: 0.24798592627048494
  batch 6 loss: 0.24607315907875696
  batch 7 loss: 0.24329717244420732
  batch 8 loss: 0.24033185467123985
  batch 9 loss: 0.24370420310232374
  batch 10 loss: 0.24058762192726135
  batch 11 loss: 0.241612740538337
  batch 12 loss: 0.24036709715922674
  batch 13 loss: 0.23671904779397523
  batch 14 loss: 0.23614011066300528
  batch 15 loss: 0.23822486400604248
  batch 16 loss: 0.23804372362792492
  batch 17 loss: 0.23950194435961106
  batch 18 loss: 0.2384378355410364
  batch 19 loss: 0.23796398153430537
  batch 20 loss: 0.23581983149051666
  batch 21 loss: 0.23874765918368385
  batch 22 loss: 0.2401753067970276
  batch 23 loss: 0.24152464322421863
  batch 24 loss: 0.2415267707159122
  batch 25 loss: 0.24295208871364593
  batch 26 loss: 0.24377987533807755
  batch 27 loss: 0.24383962154388428
  batch 28 loss: 0.2448601946234703
  batch 29 loss: 0.2444288838526298
  batch 30 loss: 0.2451958402991295
  batch 31 loss: 0.24540084648516872
  batch 32 loss: 0.2464064876548946
  batch 33 loss: 0.2481598858580445
  batch 34 loss: 0.24812855045585072
  batch 35 loss: 0.25001458057335446
  batch 36 loss: 0.2501747951739364
  batch 37 loss: 0.25074869515122594
  batch 38 loss: 0.25034101817168686
  batch 39 loss: 0.25102335864152664
  batch 40 loss: 0.25103475637733935
  batch 41 loss: 0.2509189295332606
  batch 42 loss: 0.25052676704667864
  batch 43 loss: 0.25084135386832923
  batch 44 loss: 0.251708196984096
  batch 45 loss: 0.2517353292968538
  batch 46 loss: 0.25052980819474097
  batch 47 loss: 0.24981600838772794
  batch 48 loss: 0.24995064456015825
  batch 49 loss: 0.25136141539836415
  batch 50 loss: 0.25234944313764573
  batch 51 loss: 0.2517726535306257
  batch 52 loss: 0.2518220552458213
  batch 53 loss: 0.25165539558203714
  batch 54 loss: 0.2515598022275501
  batch 55 loss: 0.25183627063577824
  batch 56 loss: 0.2523162822638239
  batch 57 loss: 0.2518921034377918
  batch 58 loss: 0.25195260140402564
  batch 59 loss: 0.25224023400726964
  batch 60 loss: 0.2526668851574262
  batch 61 loss: 0.2530762225878043
  batch 62 loss: 0.2532367177547947
  batch 63 loss: 0.25196520723993815
  batch 64 loss: 0.25162082677707076
  batch 65 loss: 0.2512756627339583
  batch 66 loss: 0.25121702727946366
  batch 67 loss: 0.25123770179143595
  batch 68 loss: 0.25128643981674137
  batch 69 loss: 0.2514458499524904
  batch 70 loss: 0.2514546515686171
  batch 71 loss: 0.25154580049951314
  batch 72 loss: 0.2514538922243648
  batch 73 loss: 0.25151961592778765
  batch 74 loss: 0.25154569946430827
  batch 75 loss: 0.2515074775616328
  batch 76 loss: 0.2516174726188183
  batch 77 loss: 0.2513893358893209
  batch 78 loss: 0.2513484102793229
  batch 79 loss: 0.2511777845741827
  batch 80 loss: 0.25099414940923453
  batch 81 loss: 0.25094237132572833
  batch 82 loss: 0.25115962889863225
  batch 83 loss: 0.2506983978561608
  batch 84 loss: 0.24998569914272853
  batch 85 loss: 0.25060944872743945
  batch 86 loss: 0.25036442938239073
  batch 87 loss: 0.2498630497647428
  batch 88 loss: 0.24946613118729807
  batch 89 loss: 0.2490754323394111
  batch 90 loss: 0.24930523153808382
  batch 91 loss: 0.24887722223014622
  batch 92 loss: 0.2485191493254641
  batch 93 loss: 0.24787146574066532
  batch 94 loss: 0.24765823163250658
  batch 95 loss: 0.24756175718809428
  batch 96 loss: 0.24738351504007974
  batch 97 loss: 0.2477449836190214
  batch 98 loss: 0.2475187570160749
  batch 99 loss: 0.24720729526245233
  batch 100 loss: 0.24737693294882773
  batch 101 loss: 0.2472040209144649
  batch 102 loss: 0.24748757613055847
  batch 103 loss: 0.2471578232582333
  batch 104 loss: 0.24733725261802858
  batch 105 loss: 0.2471415312517257
  batch 106 loss: 0.24718742438082425
  batch 107 loss: 0.24731057297403566
  batch 108 loss: 0.2469143027232753
  batch 109 loss: 0.24712857508331265
  batch 110 loss: 0.2471211782910607
  batch 111 loss: 0.24704559227904757
  batch 112 loss: 0.24692706191646202
  batch 113 loss: 0.24668619775139125
  batch 114 loss: 0.24663810850235454
  batch 115 loss: 0.2463711019443429
  batch 116 loss: 0.2461358581123681
  batch 117 loss: 0.2458824210951471
  batch 118 loss: 0.24587210179385494
  batch 119 loss: 0.24595785065859305
  batch 120 loss: 0.24578819821278255
  batch 121 loss: 0.2457185323573341
  batch 122 loss: 0.2455137484386319
  batch 123 loss: 0.24513967347338916
  batch 124 loss: 0.24510985312442626
  batch 125 loss: 0.24470735907554628
  batch 126 loss: 0.2446591734649643
  batch 127 loss: 0.2447957461039851
  batch 128 loss: 0.24481084931176156
  batch 129 loss: 0.2450725714596667
  batch 130 loss: 0.24502206295728685
  batch 131 loss: 0.24490429699875926
  batch 132 loss: 0.24508173966949637
  batch 133 loss: 0.24510121323112258
  batch 134 loss: 0.24505037825499007
  batch 135 loss: 0.2448592440949546
  batch 136 loss: 0.24502770547919414
  batch 137 loss: 0.24544744802652485
  batch 138 loss: 0.24548674612373544
  batch 139 loss: 0.2453364044856682
  batch 140 loss: 0.2455891284559454
  batch 141 loss: 0.24529454186030314
  batch 142 loss: 0.24490210183069738
  batch 143 loss: 0.2452826414491747
  batch 144 loss: 0.24527400939000976
  batch 145 loss: 0.2451974115495024
  batch 146 loss: 0.24533277719396435
  batch 147 loss: 0.24537935680678102
  batch 148 loss: 0.24520307426919807
  batch 149 loss: 0.24480659159637938
  batch 150 loss: 0.24476420611143113
  batch 151 loss: 0.24439404253533345
  batch 152 loss: 0.24411753957208834
  batch 153 loss: 0.2442027147299324
  batch 154 loss: 0.2439618416420825
  batch 155 loss: 0.24389519614558067
  batch 156 loss: 0.24384190361851302
  batch 157 loss: 0.243697231837139
  batch 158 loss: 0.24364185927412177
  batch 159 loss: 0.24387144283303674
  batch 160 loss: 0.24373913994058966
  batch 161 loss: 0.24347185533239235
  batch 162 loss: 0.24343301622587957
  batch 163 loss: 0.24324943673756957
  batch 164 loss: 0.24336865171790123
  batch 165 loss: 0.24346700897722534
  batch 166 loss: 0.2437087853091309
  batch 167 loss: 0.24370914572727181
  batch 168 loss: 0.2433984446383658
  batch 169 loss: 0.24333477628654276
  batch 170 loss: 0.24330285770051618
  batch 171 loss: 0.24341280575384172
  batch 172 loss: 0.2433418000853339
  batch 173 loss: 0.24334896876977358
  batch 174 loss: 0.24339636226837663
  batch 175 loss: 0.24344879107815878
  batch 176 loss: 0.24358839516273953
  batch 177 loss: 0.24378234038581956
  batch 178 loss: 0.24371129918969078
  batch 179 loss: 0.2437231007900984
  batch 180 loss: 0.24369321192304294
  batch 181 loss: 0.24367793497130358
  batch 182 loss: 0.24368404957291845
  batch 183 loss: 0.24356793852451722
  batch 184 loss: 0.24355201585137326
  batch 185 loss: 0.24330581378292393
  batch 186 loss: 0.24327248246759497
  batch 187 loss: 0.2433451236409937
  batch 188 loss: 0.2432180234092347
  batch 189 loss: 0.24329386769779146
  batch 190 loss: 0.24304946919805126
  batch 191 loss: 0.24296909266429423
  batch 192 loss: 0.2427441474671165
  batch 193 loss: 0.24272691732552384
  batch 194 loss: 0.24252345996726418
  batch 195 loss: 0.24262708815244527
  batch 196 loss: 0.24250556893494665
  batch 197 loss: 0.2426819563819672
  batch 198 loss: 0.24256253468267847
  batch 199 loss: 0.24257005444124116
  batch 200 loss: 0.2424802965670824
  batch 201 loss: 0.2424334790279616
  batch 202 loss: 0.24258493831252106
  batch 203 loss: 0.24256090797814242
  batch 204 loss: 0.24286150479433583
  batch 205 loss: 0.24291248394221795
  batch 206 loss: 0.24301004207249985
  batch 207 loss: 0.2429834141391487
  batch 208 loss: 0.24282736868525928
  batch 209 loss: 0.24292773314925473
  batch 210 loss: 0.24300390602577301
  batch 211 loss: 0.24293885870849916
  batch 212 loss: 0.24291805367705957
  batch 213 loss: 0.24285843275802235
  batch 214 loss: 0.24276523723780552
  batch 215 loss: 0.24281576206517774
  batch 216 loss: 0.2427117927344861
  batch 217 loss: 0.24268811577201438
  batch 218 loss: 0.24262723791489907
  batch 219 loss: 0.24231922966704522
  batch 220 loss: 0.2423985253680836
  batch 221 loss: 0.24235008292877835
  batch 222 loss: 0.2424491241842777
  batch 223 loss: 0.24232880443735508
  batch 224 loss: 0.24225325057549135
  batch 225 loss: 0.24232107241948445
  batch 226 loss: 0.24245608881511518
  batch 227 loss: 0.24241732730476867
  batch 228 loss: 0.24243446968887983
  batch 229 loss: 0.2424318992154567
  batch 230 loss: 0.24243704112975495
  batch 231 loss: 0.24253275093836185
  batch 232 loss: 0.24255083738986788
  batch 233 loss: 0.2425467281126669
  batch 234 loss: 0.24243716417978972
  batch 235 loss: 0.24238383218329004
  batch 236 loss: 0.24235008587524043
  batch 237 loss: 0.24221163327935374
  batch 238 loss: 0.2422347399993103
  batch 239 loss: 0.2422195078189403
  batch 240 loss: 0.24233043752610683
  batch 241 loss: 0.2421772643861929
  batch 242 loss: 0.24238984095902483
  batch 243 loss: 0.24248365849379158
  batch 244 loss: 0.2423925514592499
  batch 245 loss: 0.2423538734718245
  batch 246 loss: 0.24242725059753512
  batch 247 loss: 0.24256190778272838
  batch 248 loss: 0.2426584686242765
  batch 249 loss: 0.24272839898565207
  batch 250 loss: 0.24276518094539642
  batch 251 loss: 0.24268720254480125
  batch 252 loss: 0.24266401832065884
  batch 253 loss: 0.24263075836326764
  batch 254 loss: 0.2425726670918502
  batch 255 loss: 0.24244661646730759
  batch 256 loss: 0.24236462672706693
  batch 257 loss: 0.2423801810592993
  batch 258 loss: 0.24239563329737315
  batch 259 loss: 0.24228830813901306
  batch 260 loss: 0.24225608465763238
  batch 261 loss: 0.2422881570584016
  batch 262 loss: 0.24222800988732404
  batch 263 loss: 0.24212158564832273
  batch 264 loss: 0.24222885952754455
  batch 265 loss: 0.24207922894999667
  batch 266 loss: 0.24192671388163603
  batch 267 loss: 0.24180383673321443
  batch 268 loss: 0.24172793189758685
  batch 269 loss: 0.24174425249649245
  batch 270 loss: 0.24165492477240386
  batch 271 loss: 0.24161513897545664
  batch 272 loss: 0.2415092926581993
  batch 273 loss: 0.2413513665889209
  batch 274 loss: 0.24116535691449242
  batch 275 loss: 0.2410615774718198
  batch 276 loss: 0.2410027169878932
  batch 277 loss: 0.2410923598690584
  batch 278 loss: 0.2409324991188461
  batch 279 loss: 0.24091777087967028
  batch 280 loss: 0.24087980884526458
  batch 281 loss: 0.24077928278369837
  batch 282 loss: 0.24065268092544367
  batch 283 loss: 0.24060645804809597
  batch 284 loss: 0.2405706559058646
  batch 285 loss: 0.24056549804252492
  batch 286 loss: 0.24052060364218025
  batch 287 loss: 0.2403890520126562
  batch 288 loss: 0.24036572185448474
  batch 289 loss: 0.24030236172841082
  batch 290 loss: 0.24019171205060236
  batch 291 loss: 0.24020334289655654
  batch 292 loss: 0.24008835540854767
  batch 293 loss: 0.24009568455276228
  batch 294 loss: 0.23986862772176054
  batch 295 loss: 0.23985243944798484
  batch 296 loss: 0.2399159126185082
  batch 297 loss: 0.2398407989078098
  batch 298 loss: 0.23976798700606264
  batch 299 loss: 0.2397609309707597
  batch 300 loss: 0.23969045877456666
  batch 301 loss: 0.239651466029823
  batch 302 loss: 0.23980191488139677
  batch 303 loss: 0.23972317768205512
  batch 304 loss: 0.23972874620047055
  batch 305 loss: 0.23970664451356793
  batch 306 loss: 0.23978321726610458
  batch 307 loss: 0.23972549486043787
  batch 308 loss: 0.23975508679430207
  batch 309 loss: 0.23971432312406768
  batch 310 loss: 0.23977760711023885
  batch 311 loss: 0.23965272685914177
  batch 312 loss: 0.2396051078939285
  batch 313 loss: 0.23958604009197162
  batch 314 loss: 0.23954744214654727
  batch 315 loss: 0.2395468168788486
  batch 316 loss: 0.23963534313289425
  batch 317 loss: 0.2397160540431832
  batch 318 loss: 0.2395365149626192
  batch 319 loss: 0.23950815359626818
  batch 320 loss: 0.23951419861987233
  batch 321 loss: 0.23958968661284522
  batch 322 loss: 0.23961741639220197
  batch 323 loss: 0.2395603957098704
  batch 324 loss: 0.2396299783370377
  batch 325 loss: 0.23984773612939395
  batch 326 loss: 0.2400570578096103
  batch 327 loss: 0.24010719586767554
  batch 328 loss: 0.2400748913698807
  batch 329 loss: 0.24009247830754718
  batch 330 loss: 0.24009568162939765
  batch 331 loss: 0.24010301406109802
  batch 332 loss: 0.24025970791657286
  batch 333 loss: 0.24029006404024703
  batch 334 loss: 0.24059656454239062
  batch 335 loss: 0.24074601837948187
  batch 336 loss: 0.24092157127424366
  batch 337 loss: 0.24100209356946126
  batch 338 loss: 0.24103293285743724
  batch 339 loss: 0.2411057676097988
  batch 340 loss: 0.2410709215875934
  batch 341 loss: 0.24102943065054605
  batch 342 loss: 0.24094049532336798
  batch 343 loss: 0.24106669126203387
  batch 344 loss: 0.24115263016591237
  batch 345 loss: 0.2412085488654565
  batch 346 loss: 0.2412576788681091
  batch 347 loss: 0.24115279428553513
  batch 348 loss: 0.24115213199422278
  batch 349 loss: 0.24120186200968516
  batch 350 loss: 0.2414325504217829
  batch 351 loss: 0.24145416512108936
  batch 352 loss: 0.24139308438382365
  batch 353 loss: 0.24137672900140456
  batch 354 loss: 0.24130520940163716
  batch 355 loss: 0.24119424891303962
  batch 356 loss: 0.24133730022592492
  batch 357 loss: 0.24116780606805444
  batch 358 loss: 0.2410628654520605
  batch 359 loss: 0.24100883886176563
  batch 360 loss: 0.24094180915918614
  batch 361 loss: 0.24089571486879915
  batch 362 loss: 0.24086413989409558
  batch 363 loss: 0.24088864172128935
  batch 364 loss: 0.2407711548628388
  batch 365 loss: 0.2408286019547345
  batch 366 loss: 0.2408237672242962
  batch 367 loss: 0.24087298736585258
  batch 368 loss: 0.24083832865985838
  batch 369 loss: 0.24080303129626485
  batch 370 loss: 0.24082733195375752
  batch 371 loss: 0.2408224116920461
  batch 372 loss: 0.24089933499213187
  batch 373 loss: 0.2408870841718551
  batch 374 loss: 0.24075592325173598
  batch 375 loss: 0.2408480159044266
  batch 376 loss: 0.24102729705578468
  batch 377 loss: 0.24099187963995441
  batch 378 loss: 0.24103762181820693
  batch 379 loss: 0.24103411678273948
  batch 380 loss: 0.24098771457609378
  batch 381 loss: 0.24093417162344524
  batch 382 loss: 0.24095790595284308
  batch 383 loss: 0.24084455280640107
  batch 384 loss: 0.24086031907548508
  batch 385 loss: 0.2408089552219812
  batch 386 loss: 0.24075380573773014
  batch 387 loss: 0.24071419658740978
  batch 388 loss: 0.2405948597054506
  batch 389 loss: 0.24052983955276655
  batch 390 loss: 0.24057024468978247
  batch 391 loss: 0.24055181889582777
  batch 392 loss: 0.24060427724402778
  batch 393 loss: 0.24069448744371041
  batch 394 loss: 0.24073934645822206
  batch 395 loss: 0.24072103311743917
  batch 396 loss: 0.24074358786597397
  batch 397 loss: 0.24079940894088456
  batch 398 loss: 0.24091577447538998
  batch 399 loss: 0.24091185648040964
  batch 400 loss: 0.24100204966962338
  batch 401 loss: 0.24103878144908725
  batch 402 loss: 0.24106567719979072
  batch 403 loss: 0.24107495061221254
  batch 404 loss: 0.24106926806639917
  batch 405 loss: 0.2410791121147297
  batch 406 loss: 0.24097253885028397
  batch 407 loss: 0.24104148497306158
  batch 408 loss: 0.24093779578220612
  batch 409 loss: 0.2409004423321022
  batch 410 loss: 0.24077521704319046
  batch 411 loss: 0.24077185020394568
  batch 412 loss: 0.24079985016034644
  batch 413 loss: 0.24079284165731066
  batch 414 loss: 0.24081635345583377
  batch 415 loss: 0.24077897412949298
  batch 416 loss: 0.24077840625809935
  batch 417 loss: 0.24075932822210325
  batch 418 loss: 0.24072759537890767
  batch 419 loss: 0.2407247336283504
  batch 420 loss: 0.2407104067504406
  batch 421 loss: 0.2408098959653791
  batch 422 loss: 0.24082493435997535
  batch 423 loss: 0.24083097958959304
  batch 424 loss: 0.2408527322816399
  batch 425 loss: 0.24081592693048365
  batch 426 loss: 0.2407934911234278
  batch 427 loss: 0.24079470779633355
  batch 428 loss: 0.24084965174443254
  batch 429 loss: 0.24074396609148502
  batch 430 loss: 0.24074815040410952
  batch 431 loss: 0.2407768038198732
  batch 432 loss: 0.24082901094246795
  batch 433 loss: 0.24076620191679815
  batch 434 loss: 0.2406599018980281
  batch 435 loss: 0.2407527890698663
  batch 436 loss: 0.2407235060946657
  batch 437 loss: 0.24066453701571414
  batch 438 loss: 0.24063875400312415
  batch 439 loss: 0.24054424379844058
  batch 440 loss: 0.24057103578339922
  batch 441 loss: 0.240544729120607
  batch 442 loss: 0.2405147121637655
  batch 443 loss: 0.24048923080446488
  batch 444 loss: 0.2404593079610988
  batch 445 loss: 0.2403935837946581
  batch 446 loss: 0.24042312356522266
  batch 447 loss: 0.24040081860368417
  batch 448 loss: 0.2405013906037701
  batch 449 loss: 0.24041705573587482
  batch 450 loss: 0.24041105098194546
  batch 451 loss: 0.24033929156490547
  batch 452 loss: 0.24028009064165892
  batch 453 loss: 0.24025289428155153
  batch 454 loss: 0.24021952353122475
  batch 455 loss: 0.24019394182896875
  batch 456 loss: 0.24010901568098025
  batch 457 loss: 0.24007570179468693
  batch 458 loss: 0.24005660544836885
  batch 459 loss: 0.24004406483604498
  batch 460 loss: 0.2399819442111513
  batch 461 loss: 0.23999583837525704
  batch 462 loss: 0.2400389016964735
  batch 463 loss: 0.24003949182079884
  batch 464 loss: 0.2399766752175216
  batch 465 loss: 0.2399611295551382
  batch 466 loss: 0.2399270015852646
  batch 467 loss: 0.24008695171783007
  batch 468 loss: 0.2400034448912001
  batch 469 loss: 0.239879550932567
  batch 470 loss: 0.23989106378656752
  batch 471 loss: 0.2399095756549491
  batch 472 loss: 0.2396902238666001
LOSS train 0.2396902238666001 valid 0.22036467492580414
LOSS train 0.2396902238666001 valid 0.21231766790151596
LOSS train 0.2396902238666001 valid 0.22105415165424347
LOSS train 0.2396902238666001 valid 0.2100597508251667
LOSS train 0.2396902238666001 valid 0.20679961442947387
LOSS train 0.2396902238666001 valid 0.21104401846726736
LOSS train 0.2396902238666001 valid 0.20633630454540253
LOSS train 0.2396902238666001 valid 0.2035067118704319
LOSS train 0.2396902238666001 valid 0.20291052924262154
LOSS train 0.2396902238666001 valid 0.20051222443580627
LOSS train 0.2396902238666001 valid 0.2002313421531157
LOSS train 0.2396902238666001 valid 0.20269092917442322
LOSS train 0.2396902238666001 valid 0.20189622388436243
LOSS train 0.2396902238666001 valid 0.20068053901195526
LOSS train 0.2396902238666001 valid 0.2010320524374644
LOSS train 0.2396902238666001 valid 0.2048634346574545
LOSS train 0.2396902238666001 valid 0.20497059033197515
LOSS train 0.2396902238666001 valid 0.20460663239161173
LOSS train 0.2396902238666001 valid 0.20730004185124448
LOSS train 0.2396902238666001 valid 0.20640020966529846
LOSS train 0.2396902238666001 valid 0.20910256533395677
LOSS train 0.2396902238666001 valid 0.2086537155238065
LOSS train 0.2396902238666001 valid 0.20670241765353992
LOSS train 0.2396902238666001 valid 0.20668432613213858
LOSS train 0.2396902238666001 valid 0.2064868664741516
LOSS train 0.2396902238666001 valid 0.2057364548628147
LOSS train 0.2396902238666001 valid 0.20564545304686935
LOSS train 0.2396902238666001 valid 0.20636543099369323
LOSS train 0.2396902238666001 valid 0.2054378806517042
LOSS train 0.2396902238666001 valid 0.2050464227795601
LOSS train 0.2396902238666001 valid 0.2054762917180215
LOSS train 0.2396902238666001 valid 0.20552187971770763
LOSS train 0.2396902238666001 valid 0.20415306317083765
LOSS train 0.2396902238666001 valid 0.20389508193030076
LOSS train 0.2396902238666001 valid 0.2045206559555871
LOSS train 0.2396902238666001 valid 0.2053430953787433
LOSS train 0.2396902238666001 valid 0.20563187993861534
LOSS train 0.2396902238666001 valid 0.20562410629109332
LOSS train 0.2396902238666001 valid 0.20667489560750815
LOSS train 0.2396902238666001 valid 0.20686009973287584
LOSS train 0.2396902238666001 valid 0.20663263703264842
LOSS train 0.2396902238666001 valid 0.20796913227864675
LOSS train 0.2396902238666001 valid 0.20859296654545983
LOSS train 0.2396902238666001 valid 0.2081157470291311
LOSS train 0.2396902238666001 valid 0.20762539439731173
LOSS train 0.2396902238666001 valid 0.20735068522069766
LOSS train 0.2396902238666001 valid 0.20712855743600966
LOSS train 0.2396902238666001 valid 0.20867983903735876
LOSS train 0.2396902238666001 valid 0.20810013553317713
LOSS train 0.2396902238666001 valid 0.20882426530122758
LOSS train 0.2396902238666001 valid 0.20832397511192396
LOSS train 0.2396902238666001 valid 0.20813886506053117
LOSS train 0.2396902238666001 valid 0.20942533381705014
LOSS train 0.2396902238666001 valid 0.20945586291728197
LOSS train 0.2396902238666001 valid 0.20928356512026353
LOSS train 0.2396902238666001 valid 0.2095898168959788
LOSS train 0.2396902238666001 valid 0.20913035827770568
LOSS train 0.2396902238666001 valid 0.20948187570119725
LOSS train 0.2396902238666001 valid 0.20936028179475816
LOSS train 0.2396902238666001 valid 0.20914781739314398
LOSS train 0.2396902238666001 valid 0.2091865285498197
LOSS train 0.2396902238666001 valid 0.20905383291744417
LOSS train 0.2396902238666001 valid 0.20875042345788744
LOSS train 0.2396902238666001 valid 0.20916639897041023
LOSS train 0.2396902238666001 valid 0.2080327740082374
LOSS train 0.2396902238666001 valid 0.20777946733164065
LOSS train 0.2396902238666001 valid 0.20849472751368336
LOSS train 0.2396902238666001 valid 0.20789904169300022
LOSS train 0.2396902238666001 valid 0.20848310879175214
LOSS train 0.2396902238666001 valid 0.20893928217036384
LOSS train 0.2396902238666001 valid 0.20920278110974272
LOSS train 0.2396902238666001 valid 0.20979028857416576
LOSS train 0.2396902238666001 valid 0.2105502678106909
LOSS train 0.2396902238666001 valid 0.21021351822324702
LOSS train 0.2396902238666001 valid 0.20985505163669585
LOSS train 0.2396902238666001 valid 0.21007294599947177
LOSS train 0.2396902238666001 valid 0.2102435709207089
LOSS train 0.2396902238666001 valid 0.21010441142014968
LOSS train 0.2396902238666001 valid 0.2100467080179649
LOSS train 0.2396902238666001 valid 0.20980533622205258
LOSS train 0.2396902238666001 valid 0.21006741291946834
LOSS train 0.2396902238666001 valid 0.21003452488561955
LOSS train 0.2396902238666001 valid 0.210345768246306
LOSS train 0.2396902238666001 valid 0.21012232257496744
LOSS train 0.2396902238666001 valid 0.21088921269949745
LOSS train 0.2396902238666001 valid 0.21081703520098397
LOSS train 0.2396902238666001 valid 0.21051742838717055
LOSS train 0.2396902238666001 valid 0.21080835603854872
LOSS train 0.2396902238666001 valid 0.2111888795756222
LOSS train 0.2396902238666001 valid 0.2117362956206004
LOSS train 0.2396902238666001 valid 0.2117420474251548
LOSS train 0.2396902238666001 valid 0.21193206601816675
LOSS train 0.2396902238666001 valid 0.21172627021548568
LOSS train 0.2396902238666001 valid 0.21201957778093664
LOSS train 0.2396902238666001 valid 0.2121596789673755
LOSS train 0.2396902238666001 valid 0.21229733309398094
LOSS train 0.2396902238666001 valid 0.2123865257833422
LOSS train 0.2396902238666001 valid 0.21283775476776823
LOSS train 0.2396902238666001 valid 0.21310571558547742
LOSS train 0.2396902238666001 valid 0.2131586940586567
LOSS train 0.2396902238666001 valid 0.2131750074648621
LOSS train 0.2396902238666001 valid 0.21354883193385368
LOSS train 0.2396902238666001 valid 0.2133886207943981
LOSS train 0.2396902238666001 valid 0.21316587050946859
LOSS train 0.2396902238666001 valid 0.2134437606448219
LOSS train 0.2396902238666001 valid 0.21359817655581348
LOSS train 0.2396902238666001 valid 0.2133431330183956
LOSS train 0.2396902238666001 valid 0.2134941086449005
LOSS train 0.2396902238666001 valid 0.2131408120787472
LOSS train 0.2396902238666001 valid 0.21313835829496383
LOSS train 0.2396902238666001 valid 0.2131430469117723
LOSS train 0.2396902238666001 valid 0.2134679885847228
LOSS train 0.2396902238666001 valid 0.21318808514459997
LOSS train 0.2396902238666001 valid 0.2133633766257972
LOSS train 0.2396902238666001 valid 0.2140790211117786
LOSS train 0.2396902238666001 valid 0.21371969426500387
LOSS train 0.2396902238666001 valid 0.21433604553214505
LOSS train 0.2396902238666001 valid 0.2141951355388609
LOSS train 0.2396902238666001 valid 0.21391263378768408
LOSS train 0.2396902238666001 valid 0.21350850785772005
LOSS train 0.2396902238666001 valid 0.21326494857299427
LOSS train 0.2396902238666001 valid 0.2134737101246099
LOSS train 0.2396902238666001 valid 0.2134429801528047
LOSS train 0.2396902238666001 valid 0.2136698236628886
LOSS train 0.2396902238666001 valid 0.21358082962036132
LOSS train 0.2396902238666001 valid 0.2137242837084664
LOSS train 0.2396902238666001 valid 0.21358949004665134
LOSS train 0.2396902238666001 valid 0.21353922574780881
LOSS train 0.2396902238666001 valid 0.2132493857034417
LOSS train 0.2396902238666001 valid 0.21298971118835303
LOSS train 0.2396902238666001 valid 0.21310043607959311
LOSS train 0.2396902238666001 valid 0.21307503578789305
LOSS train 0.2396902238666001 valid 0.21306244334332028
LOSS train 0.2396902238666001 valid 0.2132741593158067
LOSS train 0.2396902238666001 valid 0.21329989477440162
LOSS train 0.2396902238666001 valid 0.21321362684316494
LOSS train 0.2396902238666001 valid 0.21315964349429972
LOSS train 0.2396902238666001 valid 0.21304902164400488
LOSS train 0.2396902238666001 valid 0.21287537049904143
LOSS train 0.2396902238666001 valid 0.21275095577750888
LOSS train 0.2396902238666001 valid 0.2127585252548786
LOSS train 0.2396902238666001 valid 0.21293777982953568
LOSS train 0.2396902238666001 valid 0.21300419769087037
LOSS train 0.2396902238666001 valid 0.2130901642764608
LOSS train 0.2396902238666001 valid 0.21304207618894247
LOSS train 0.2396902238666001 valid 0.2128911964493255
LOSS train 0.2396902238666001 valid 0.2127986824025913
LOSS train 0.2396902238666001 valid 0.21273339791475115
LOSS train 0.2396902238666001 valid 0.2127649885096006
LOSS train 0.2396902238666001 valid 0.21279633949200313
LOSS train 0.2396902238666001 valid 0.21268165526011132
LOSS train 0.2396902238666001 valid 0.21261659117513582
LOSS train 0.2396902238666001 valid 0.21243706608519836
LOSS train 0.2396902238666001 valid 0.2126223093697003
LOSS train 0.2396902238666001 valid 0.2127040300638445
LOSS train 0.2396902238666001 valid 0.21259800602610296
LOSS train 0.2396902238666001 valid 0.2125282278106471
LOSS train 0.2396902238666001 valid 0.21268697384792037
LOSS train 0.2396902238666001 valid 0.21277654958221148
LOSS train 0.2396902238666001 valid 0.2129165031015873
LOSS train 0.2396902238666001 valid 0.2128381326516963
LOSS train 0.2396902238666001 valid 0.21295044791919213
LOSS train 0.2396902238666001 valid 0.2127536889051367
LOSS train 0.2396902238666001 valid 0.21285523083515284
LOSS train 0.2396902238666001 valid 0.2129665729674426
LOSS train 0.2396902238666001 valid 0.21291789215013204
LOSS train 0.2396902238666001 valid 0.21288495042366895
LOSS train 0.2396902238666001 valid 0.21274018722275892
LOSS train 0.2396902238666001 valid 0.21255147695188692
LOSS train 0.2396902238666001 valid 0.21267130979720283
LOSS train 0.2396902238666001 valid 0.21263644273518123
LOSS train 0.2396902238666001 valid 0.2127018081414145
LOSS train 0.2396902238666001 valid 0.2125429160505361
LOSS train 0.2396902238666001 valid 0.21294706246291084
LOSS train 0.2396902238666001 valid 0.21291318825313024
LOSS train 0.2396902238666001 valid 0.2128541168164123
LOSS train 0.2396902238666001 valid 0.21290935289725071
LOSS train 0.2396902238666001 valid 0.21301653939351606
LOSS train 0.2396902238666001 valid 0.2131619028871952
LOSS train 0.2396902238666001 valid 0.2129719405538506
LOSS train 0.2396902238666001 valid 0.21296540174365702
LOSS train 0.2396902238666001 valid 0.21273595430366285
LOSS train 0.2396902238666001 valid 0.21288315821540812
LOSS train 0.2396902238666001 valid 0.212778118479511
LOSS train 0.2396902238666001 valid 0.21262132614045529
LOSS train 0.2396902238666001 valid 0.2127465275667047
LOSS train 0.2396902238666001 valid 0.21268929628764882
LOSS train 0.2396902238666001 valid 0.21278075509248895
LOSS train 0.2396902238666001 valid 0.21255432621196466
LOSS train 0.2396902238666001 valid 0.21254515200853347
LOSS train 0.2396902238666001 valid 0.21265834445104548
LOSS train 0.2396902238666001 valid 0.21270994058189294
LOSS train 0.2396902238666001 valid 0.21246852999832963
LOSS train 0.2396902238666001 valid 0.21230452192812851
LOSS train 0.2396902238666001 valid 0.21222720520618635
LOSS train 0.2396902238666001 valid 0.21231762874795465
LOSS train 0.2396902238666001 valid 0.2123382416019585
LOSS train 0.2396902238666001 valid 0.21255996454544743
LOSS train 0.2396902238666001 valid 0.21251352273639124
LOSS train 0.2396902238666001 valid 0.21263911254704
LOSS train 0.2396902238666001 valid 0.212441525945616
LOSS train 0.2396902238666001 valid 0.2124177463131376
LOSS train 0.2396902238666001 valid 0.21251153909220485
LOSS train 0.2396902238666001 valid 0.21230566655011737
LOSS train 0.2396902238666001 valid 0.21222052494200264
LOSS train 0.2396902238666001 valid 0.2122423637260511
LOSS train 0.2396902238666001 valid 0.21217677940205099
LOSS train 0.2396902238666001 valid 0.21203906397120312
LOSS train 0.2396902238666001 valid 0.21206876492956608
LOSS train 0.2396902238666001 valid 0.21223200985363552
LOSS train 0.2396902238666001 valid 0.2124371104895786
LOSS train 0.2396902238666001 valid 0.2122780298287014
LOSS train 0.2396902238666001 valid 0.21225534337507168
LOSS train 0.2396902238666001 valid 0.2121911766790898
LOSS train 0.2396902238666001 valid 0.21196998981542364
LOSS train 0.2396902238666001 valid 0.2118014453185929
LOSS train 0.2396902238666001 valid 0.21158017472188045
LOSS train 0.2396902238666001 valid 0.21160316002478294
LOSS train 0.2396902238666001 valid 0.2115767937680902
LOSS train 0.2396902238666001 valid 0.21167364411733366
LOSS train 0.2396902238666001 valid 0.21161034757195554
LOSS train 0.2396902238666001 valid 0.21169576968427176
LOSS train 0.2396902238666001 valid 0.211823302333665
LOSS train 0.2396902238666001 valid 0.21170007324378406
LOSS train 0.2396902238666001 valid 0.21155797945128546
LOSS train 0.2396902238666001 valid 0.2114615817513086
LOSS train 0.2396902238666001 valid 0.21145572995824435
LOSS train 0.2396902238666001 valid 0.2114511928276012
LOSS train 0.2396902238666001 valid 0.2113438172501768
LOSS train 0.2396902238666001 valid 0.2113918511763863
LOSS train 0.2396902238666001 valid 0.21133661212085128
LOSS train 0.2396902238666001 valid 0.21144128937659593
LOSS train 0.2396902238666001 valid 0.21174679498304114
LOSS train 0.2396902238666001 valid 0.21164071508961865
LOSS train 0.2396902238666001 valid 0.21172252551038215
LOSS train 0.2396902238666001 valid 0.2116084886809527
LOSS train 0.2396902238666001 valid 0.21169147543011838
LOSS train 0.2396902238666001 valid 0.21165719246413528
LOSS train 0.2396902238666001 valid 0.2116366015692635
LOSS train 0.2396902238666001 valid 0.21126783754055697
LOSS train 0.2396902238666001 valid 0.21112714012257786
LOSS train 0.2396902238666001 valid 0.2111129040006271
LOSS train 0.2396902238666001 valid 0.21096407444266135
LOSS train 0.2396902238666001 valid 0.2109914182334161
LOSS train 0.2396902238666001 valid 0.21108454943311458
LOSS train 0.2396902238666001 valid 0.21118144451723836
LOSS train 0.2396902238666001 valid 0.21120760571739453
LOSS train 0.2396902238666001 valid 0.21119810800038039
LOSS train 0.2396902238666001 valid 0.21097852430310116
LOSS train 0.2396902238666001 valid 0.21106150355935097
LOSS train 0.2396902238666001 valid 0.21112196755955417
LOSS train 0.2396902238666001 valid 0.21099418286411536
LOSS train 0.2396902238666001 valid 0.21079822918289734
LOSS train 0.2396902238666001 valid 0.21092895990398924
LOSS train 0.2396902238666001 valid 0.21087629184419032
LOSS train 0.2396902238666001 valid 0.2106544381531421
LOSS train 0.2396902238666001 valid 0.21060723446106633
LOSS train 0.2396902238666001 valid 0.21082674564663753
LOSS train 0.2396902238666001 valid 0.21069592332402712
LOSS train 0.2396902238666001 valid 0.21054031622524444
LOSS train 0.2396902238666001 valid 0.21063888521144217
LOSS train 0.2396902238666001 valid 0.21083200435715777
LOSS train 0.2396902238666001 valid 0.21065219684358785
LOSS train 0.2396902238666001 valid 0.2106876839234522
LOSS train 0.2396902238666001 valid 0.2105928692896411
LOSS train 0.2396902238666001 valid 0.21066004729696683
LOSS train 0.2396902238666001 valid 0.21057774949721184
LOSS train 0.2396902238666001 valid 0.21069742024723273
LOSS train 0.2396902238666001 valid 0.21083967900076764
LOSS train 0.2396902238666001 valid 0.21067934513643935
LOSS train 0.2396902238666001 valid 0.21069566330896533
LOSS train 0.2396902238666001 valid 0.2105951601475039
LOSS train 0.2396902238666001 valid 0.210517171729397
LOSS train 0.2396902238666001 valid 0.21040018715888914
LOSS train 0.2396902238666001 valid 0.21044665041294966
LOSS train 0.2396902238666001 valid 0.21047524716435134
LOSS train 0.2396902238666001 valid 0.21051301452119428
LOSS train 0.2396902238666001 valid 0.2105378404849296
LOSS train 0.2396902238666001 valid 0.21046346204362035
LOSS train 0.2396902238666001 valid 0.21033177319914104
LOSS train 0.2396902238666001 valid 0.21018806054816977
LOSS train 0.2396902238666001 valid 0.2101014104559489
LOSS train 0.2396902238666001 valid 0.2101371518135492
LOSS train 0.2396902238666001 valid 0.21006944736229702
LOSS train 0.2396902238666001 valid 0.21012158856580132
LOSS train 0.2396902238666001 valid 0.21011920442635362
LOSS train 0.2396902238666001 valid 0.2100806998718491
LOSS train 0.2396902238666001 valid 0.2100467821324451
LOSS train 0.2396902238666001 valid 0.20995222209955994
LOSS train 0.2396902238666001 valid 0.20982048262296052
LOSS train 0.2396902238666001 valid 0.2098334295694361
LOSS train 0.2396902238666001 valid 0.20978144196512766
LOSS train 0.2396902238666001 valid 0.20977064272954601
LOSS train 0.2396902238666001 valid 0.20979013722263226
LOSS train 0.2396902238666001 valid 0.20979621089616066
LOSS train 0.2396902238666001 valid 0.20976221256864233
LOSS train 0.2396902238666001 valid 0.20968540742842837
LOSS train 0.2396902238666001 valid 0.20974390991282144
LOSS train 0.2396902238666001 valid 0.2097321864603754
LOSS train 0.2396902238666001 valid 0.20972473599016667
LOSS train 0.2396902238666001 valid 0.20963121467947562
LOSS train 0.2396902238666001 valid 0.2095999310397549
LOSS train 0.2396902238666001 valid 0.2094935109168783
LOSS train 0.2396902238666001 valid 0.2095043284965581
LOSS train 0.2396902238666001 valid 0.2094625966715031
LOSS train 0.2396902238666001 valid 0.20971043283740678
LOSS train 0.2396902238666001 valid 0.209672100939075
LOSS train 0.2396902238666001 valid 0.2097391380456748
LOSS train 0.2396902238666001 valid 0.2097372628965424
LOSS train 0.2396902238666001 valid 0.20969478076984804
LOSS train 0.2396902238666001 valid 0.2096935281847451
LOSS train 0.2396902238666001 valid 0.2096723571467476
LOSS train 0.2396902238666001 valid 0.2097844824004478
LOSS train 0.2396902238666001 valid 0.20977976887374167
LOSS train 0.2396902238666001 valid 0.20973673687567787
LOSS train 0.2396902238666001 valid 0.2097069645371241
LOSS train 0.2396902238666001 valid 0.20972886430070228
LOSS train 0.2396902238666001 valid 0.20970115663026864
LOSS train 0.2396902238666001 valid 0.20964007100621734
LOSS train 0.2396902238666001 valid 0.20954242979642004
LOSS train 0.2396902238666001 valid 0.20969272400267028
LOSS train 0.2396902238666001 valid 0.20970277212884114
LOSS train 0.2396902238666001 valid 0.2095796833328049
LOSS train 0.2396902238666001 valid 0.20956415436977957
LOSS train 0.2396902238666001 valid 0.20942018093971107
LOSS train 0.2396902238666001 valid 0.20947654268767205
LOSS train 0.2396902238666001 valid 0.20940056595871573
LOSS train 0.2396902238666001 valid 0.2093713114310692
LOSS train 0.2396902238666001 valid 0.20935230113362108
LOSS train 0.2396902238666001 valid 0.20936411662083684
LOSS train 0.2396902238666001 valid 0.209277878242497
LOSS train 0.2396902238666001 valid 0.2092899831168982
LOSS train 0.2396902238666001 valid 0.20941738343543118
LOSS train 0.2396902238666001 valid 0.209331472945249
LOSS train 0.2396902238666001 valid 0.2092133840295806
LOSS train 0.2396902238666001 valid 0.20913996203758178
LOSS train 0.2396902238666001 valid 0.2092918188750567
LOSS train 0.2396902238666001 valid 0.2092829511759902
LOSS train 0.2396902238666001 valid 0.2091950824522691
LOSS train 0.2396902238666001 valid 0.20929931873346078
LOSS train 0.2396902238666001 valid 0.20919476630838035
LOSS train 0.2396902238666001 valid 0.2091682077282005
LOSS train 0.2396902238666001 valid 0.2090538027031081
LOSS train 0.2396902238666001 valid 0.2091771682377818
LOSS train 0.2396902238666001 valid 0.2092942828933398
LOSS train 0.2396902238666001 valid 0.209262704035278
LOSS train 0.2396902238666001 valid 0.20916178047399356
LOSS train 0.2396902238666001 valid 0.2092792651905068
LOSS train 0.2396902238666001 valid 0.2092100721930706
LOSS train 0.2396902238666001 valid 0.2092028869688511
LOSS train 0.2396902238666001 valid 0.20930132688025804
LOSS train 0.2396902238666001 valid 0.20933397598987954
LOSS train 0.2396902238666001 valid 0.20937696489750832
LOSS train 0.2396902238666001 valid 0.20938625024621096
LOSS train 0.2396902238666001 valid 0.20922462195158004
LOSS train 0.2396902238666001 valid 0.2092713544333584
LOSS train 0.2396902238666001 valid 0.2093264957895132
LOSS train 0.2396902238666001 valid 0.20928620574111378
LOSS train 0.2396902238666001 valid 0.20923271955802913
LOSS train 0.2396902238666001 valid 0.2093899169522855
LOSS train 0.2396902238666001 valid 0.20930854404484467
LOSS train 0.2396902238666001 valid 0.20933520750693194
LOSS train 0.2396902238666001 valid 0.209409407327162
LOSS train 0.2396902238666001 valid 0.2093300396719804
LOSS train 0.2396902238666001 valid 0.20940804912211144
LOSS train 0.2396902238666001 valid 0.20941164928788697
LOSS train 0.2396902238666001 valid 0.20934654084067253
LOSS train 0.2396902238666001 valid 0.209218209427174
LOSS train 0.2396902238666001 valid 0.20917286368485713
EPOCH 21:
  batch 1 loss: 0.24429991841316223
  batch 2 loss: 0.25755302608013153
  batch 3 loss: 0.25017839173475903
  batch 4 loss: 0.24862748011946678
  batch 5 loss: 0.25189838707447054
  batch 6 loss: 0.2510120744506518
  batch 7 loss: 0.2461985924414226
  batch 8 loss: 0.24308075197041035
  batch 9 loss: 0.24640261630217233
  batch 10 loss: 0.24343693852424622
  batch 11 loss: 0.24325936219908975
  batch 12 loss: 0.24226673444112143
  batch 13 loss: 0.2382467847604018
  batch 14 loss: 0.23880568040268763
  batch 15 loss: 0.23921698530515034
  batch 16 loss: 0.23954555112868547
  batch 17 loss: 0.2416698117466534
  batch 18 loss: 0.24047793447971344
  batch 19 loss: 0.2396479994058609
  batch 20 loss: 0.2372661478817463
  batch 21 loss: 0.24113694046224868
  batch 22 loss: 0.24185649982907556
  batch 23 loss: 0.24365552028884058
  batch 24 loss: 0.24323064523438612
  batch 25 loss: 0.24398155272006988
  batch 26 loss: 0.24459007783577993
  batch 27 loss: 0.24491752794495336
  batch 28 loss: 0.2465228315974985
  batch 29 loss: 0.24727367481281018
  batch 30 loss: 0.24813170780738195
  batch 31 loss: 0.24904649824865402
  batch 32 loss: 0.24977849191054702
  batch 33 loss: 0.25148310399416723
  batch 34 loss: 0.2512227029484861
  batch 35 loss: 0.25267735847405026
  batch 36 loss: 0.2529795662396484
  batch 37 loss: 0.25397520975486654
  batch 38 loss: 0.2532209001089397
  batch 39 loss: 0.253516544898351
  batch 40 loss: 0.25284249186515806
  batch 41 loss: 0.25305320286169286
  batch 42 loss: 0.2525500594860032
  batch 43 loss: 0.25233808055866597
  batch 44 loss: 0.2525951506739313
  batch 45 loss: 0.2520938730902142
  batch 46 loss: 0.2509862433957017
  batch 47 loss: 0.24993611047876643
  batch 48 loss: 0.24946663745989403
  batch 49 loss: 0.2501380580420397
  batch 50 loss: 0.25174201518297196
  batch 51 loss: 0.2510289251804352
  batch 52 loss: 0.25115480102025545
  batch 53 loss: 0.25091830598858167
  batch 54 loss: 0.2506367898097745
  batch 55 loss: 0.250912669301033
  batch 56 loss: 0.2514084718589272
  batch 57 loss: 0.2511370984608667
  batch 58 loss: 0.25117089599370956
  batch 59 loss: 0.2513036922378055
  batch 60 loss: 0.2519080422818661
  batch 61 loss: 0.2521276007421681
  batch 62 loss: 0.25255787156281934
  batch 63 loss: 0.2513684065095962
  batch 64 loss: 0.25084070092998445
  batch 65 loss: 0.25033280964081106
  batch 66 loss: 0.2500254222840974
  batch 67 loss: 0.25014279048834276
  batch 68 loss: 0.25032467964817495
  batch 69 loss: 0.25064486826675525
  batch 70 loss: 0.250627752499921
  batch 71 loss: 0.25063102744834526
  batch 72 loss: 0.25051460104684037
  batch 73 loss: 0.2506526628993962
  batch 74 loss: 0.25056155307872874
  batch 75 loss: 0.25062359889348346
  batch 76 loss: 0.25088312869009216
  batch 77 loss: 0.25067623714347936
  batch 78 loss: 0.25068306579039645
  batch 79 loss: 0.250577166487899
  batch 80 loss: 0.25042626429349185
  batch 81 loss: 0.25039456214433836
  batch 82 loss: 0.25049634968362205
  batch 83 loss: 0.2500899385616004
  batch 84 loss: 0.2495119431543918
  batch 85 loss: 0.24999139642014223
  batch 86 loss: 0.24965292700501376
  batch 87 loss: 0.24919806506441927
  batch 88 loss: 0.24881482124328613
  batch 89 loss: 0.24833570320284767
  batch 90 loss: 0.24837532225582334
  batch 91 loss: 0.24806593784264155
  batch 92 loss: 0.2478044857473477
  batch 93 loss: 0.24711430104829932
  batch 94 loss: 0.24696093417228537
  batch 95 loss: 0.24693354728974795
  batch 96 loss: 0.2468499472985665
  batch 97 loss: 0.24734745013345147
  batch 98 loss: 0.24703527987003326
  batch 99 loss: 0.24672099180293805
  batch 100 loss: 0.2468423680961132
  batch 101 loss: 0.24660482146952412
  batch 102 loss: 0.2466592470220491
  batch 103 loss: 0.24633875937716473
  batch 104 loss: 0.24658660060511187
  batch 105 loss: 0.24623319237005142
  batch 106 loss: 0.24615412402265477
  batch 107 loss: 0.24594931680465412
  batch 108 loss: 0.245553152428733
  batch 109 loss: 0.2457210774268579
  batch 110 loss: 0.2457821621136232
  batch 111 loss: 0.2457808517926448
  batch 112 loss: 0.24576540756970644
  batch 113 loss: 0.24559740458442048
  batch 114 loss: 0.24548772212706113
  batch 115 loss: 0.24509683484616487
  batch 116 loss: 0.2448275332050077
  batch 117 loss: 0.2448181256524518
  batch 118 loss: 0.24478775136551614
  batch 119 loss: 0.2448732231845375
  batch 120 loss: 0.2448507429411014
  batch 121 loss: 0.24485824903673378
  batch 122 loss: 0.24457672592557844
  batch 123 loss: 0.24420814119218812
  batch 124 loss: 0.24425670756928383
  batch 125 loss: 0.24421461498737335
  batch 126 loss: 0.2441955953836441
  batch 127 loss: 0.24426237875082363
  batch 128 loss: 0.24433728423900902
  batch 129 loss: 0.2447484128696974
  batch 130 loss: 0.24488210999048673
  batch 131 loss: 0.24473637171829019
  batch 132 loss: 0.24481818723407658
  batch 133 loss: 0.24477136807334154
  batch 134 loss: 0.2448986139315278
  batch 135 loss: 0.2447606592266648
  batch 136 loss: 0.2449721266679904
  batch 137 loss: 0.24519748844369485
  batch 138 loss: 0.2451931884971218
  batch 139 loss: 0.2448828335074212
  batch 140 loss: 0.24526671969464847
  batch 141 loss: 0.24486390358590066
  batch 142 loss: 0.24447416786996412
  batch 143 loss: 0.2447386549694555
  batch 144 loss: 0.2446842970740464
  batch 145 loss: 0.24459456575327906
  batch 146 loss: 0.24462088411801483
  batch 147 loss: 0.24468207724240362
  batch 148 loss: 0.2445973981473897
  batch 149 loss: 0.24421180134651646
  batch 150 loss: 0.244079330265522
  batch 151 loss: 0.24382011651598065
  batch 152 loss: 0.24365021082523622
  batch 153 loss: 0.24354158419799182
  batch 154 loss: 0.24330807825574627
  batch 155 loss: 0.24334258754407206
  batch 156 loss: 0.24334020893543196
  batch 157 loss: 0.2432800392816021
  batch 158 loss: 0.24323637255384953
  batch 159 loss: 0.24346036468661805
  batch 160 loss: 0.24338512122631073
  batch 161 loss: 0.24316135456102975
  batch 162 loss: 0.2431825038827496
  batch 163 loss: 0.24297932978788037
  batch 164 loss: 0.2431171320197059
  batch 165 loss: 0.2433253692858147
  batch 166 loss: 0.2436395589127598
  batch 167 loss: 0.24353958229104916
  batch 168 loss: 0.24317237725924878
  batch 169 loss: 0.24312511077646673
  batch 170 loss: 0.24311711069415598
  batch 171 loss: 0.2432419002404687
  batch 172 loss: 0.24321932969398277
  batch 173 loss: 0.2432545221954412
  batch 174 loss: 0.24329291275520434
  batch 175 loss: 0.24335391598088402
  batch 176 loss: 0.24355432535098356
  batch 177 loss: 0.24373374273211268
  batch 178 loss: 0.24368850459878363
  batch 179 loss: 0.24371448159217834
  batch 180 loss: 0.24363873683744006
  batch 181 loss: 0.24358507315756867
  batch 182 loss: 0.24353651342156168
  batch 183 loss: 0.24354572203315672
  batch 184 loss: 0.24347804810689844
  batch 185 loss: 0.2432442629659498
  batch 186 loss: 0.2432077153716036
  batch 187 loss: 0.24326398155905984
  batch 188 loss: 0.243148572742939
  batch 189 loss: 0.2431429037972102
  batch 190 loss: 0.24288020275141065
  batch 191 loss: 0.24287977814674377
  batch 192 loss: 0.2426001949546238
  batch 193 loss: 0.2425734156771645
  batch 194 loss: 0.24241965025970616
  batch 195 loss: 0.242550513224724
  batch 196 loss: 0.24238906344588923
  batch 197 loss: 0.24250116793032225
  batch 198 loss: 0.2423874768945906
  batch 199 loss: 0.2424062363166905
  batch 200 loss: 0.24231191277503966
  batch 201 loss: 0.24212092264967772
  batch 202 loss: 0.24224729526160968
  batch 203 loss: 0.24224782606651044
  batch 204 loss: 0.24230839209813698
  batch 205 loss: 0.24228648892263085
  batch 206 loss: 0.2423558388520213
  batch 207 loss: 0.24227265329752568
  batch 208 loss: 0.2421071191963095
  batch 209 loss: 0.24208198188309465
  batch 210 loss: 0.2420523715161142
  batch 211 loss: 0.24193022672033987
  batch 212 loss: 0.24192504767539366
  batch 213 loss: 0.2418068440307474
  batch 214 loss: 0.24171723787472627
  batch 215 loss: 0.24167904659759168
  batch 216 loss: 0.2415132256983607
  batch 217 loss: 0.241416624133488
  batch 218 loss: 0.2412573715005446
  batch 219 loss: 0.24091002494777175
  batch 220 loss: 0.24089819423177025
  batch 221 loss: 0.2407998816190262
  batch 222 loss: 0.2408006189911215
  batch 223 loss: 0.24072075097282905
  batch 224 loss: 0.24063431218798673
  batch 225 loss: 0.24060443341732024
  batch 226 loss: 0.24069456828642735
  batch 227 loss: 0.24063887765491587
  batch 228 loss: 0.24064027040935398
  batch 229 loss: 0.24063631293555013
  batch 230 loss: 0.2406191545336143
  batch 231 loss: 0.24075830363349998
  batch 232 loss: 0.24079044612831083
  batch 233 loss: 0.24079776750357879
  batch 234 loss: 0.24068158769454712
  batch 235 loss: 0.2405662017299774
  batch 236 loss: 0.24057899567030244
  batch 237 loss: 0.2404110473796788
  batch 238 loss: 0.24035083865668594
  batch 239 loss: 0.24035216637236304
  batch 240 loss: 0.24046464102963608
  batch 241 loss: 0.24028329334813034
  batch 242 loss: 0.24023684686865687
  batch 243 loss: 0.24019869499736363
  batch 244 loss: 0.24003709541236767
  batch 245 loss: 0.23991185408465715
  batch 246 loss: 0.2398464576136775
  batch 247 loss: 0.2399275441039429
  batch 248 loss: 0.24002691688797168
  batch 249 loss: 0.2400357714259481
  batch 250 loss: 0.2400444083213806
  batch 251 loss: 0.23987195857492577
  batch 252 loss: 0.23983614590196384
  batch 253 loss: 0.23979249278547266
  batch 254 loss: 0.23973497138248653
  batch 255 loss: 0.23956367957825755
  batch 256 loss: 0.23946470906957984
  batch 257 loss: 0.23943949429905367
  batch 258 loss: 0.23942053872485494
  batch 259 loss: 0.23927451883043563
  batch 260 loss: 0.23924588836156405
  batch 261 loss: 0.2392269391780612
  batch 262 loss: 0.2391531451512839
  batch 263 loss: 0.23902610012333655
  batch 264 loss: 0.23907436080502742
  batch 265 loss: 0.23895662297617715
  batch 266 loss: 0.2387566567587673
  batch 267 loss: 0.23861305583059117
  batch 268 loss: 0.23850895536702071
  batch 269 loss: 0.23850880723903614
  batch 270 loss: 0.23840802946576364
  batch 271 loss: 0.23837035262056822
  batch 272 loss: 0.23831390699042992
  batch 273 loss: 0.23813801097782542
  batch 274 loss: 0.23803122091467363
  batch 275 loss: 0.2379060288992795
  batch 276 loss: 0.2378346729969633
  batch 277 loss: 0.23783537121456022
  batch 278 loss: 0.23764657904561476
  batch 279 loss: 0.23771376950552814
  batch 280 loss: 0.23767040898757322
  batch 281 loss: 0.23760342772957269
  batch 282 loss: 0.23741410271073063
  batch 283 loss: 0.23738207402161912
  batch 284 loss: 0.23736748088833312
  batch 285 loss: 0.2373569758837683
  batch 286 loss: 0.2373034095951727
  batch 287 loss: 0.23719419881649548
  batch 288 loss: 0.23718925058427784
  batch 289 loss: 0.23709118010469787
  batch 290 loss: 0.23699080825879656
  batch 291 loss: 0.23698800623007246
  batch 292 loss: 0.23689301163978774
  batch 293 loss: 0.2368712626117895
  batch 294 loss: 0.2365756635459102
  batch 295 loss: 0.23654415430658954
  batch 296 loss: 0.23656159146009265
  batch 297 loss: 0.2365173341750296
  batch 298 loss: 0.23646059857318866
  batch 299 loss: 0.23648575446677447
  batch 300 loss: 0.23636551529169084
  batch 301 loss: 0.23635104278789407
  batch 302 loss: 0.2365429141860924
  batch 303 loss: 0.23650785862612644
  batch 304 loss: 0.2365253159874364
  batch 305 loss: 0.23649216948962604
  batch 306 loss: 0.2365624135226206
  batch 307 loss: 0.23658915829969152
  batch 308 loss: 0.2366121414226371
  batch 309 loss: 0.23662117128426202
  batch 310 loss: 0.23668666794415444
  batch 311 loss: 0.23665538856645874
  batch 312 loss: 0.23665010141065487
  batch 313 loss: 0.23667250140406454
  batch 314 loss: 0.23662511226098248
  batch 315 loss: 0.2365418176329325
  batch 316 loss: 0.23664408860893188
  batch 317 loss: 0.2367237280313901
  batch 318 loss: 0.2365668291846911
  batch 319 loss: 0.23656647467874808
  batch 320 loss: 0.2366032470948994
  batch 321 loss: 0.23670698855524866
  batch 322 loss: 0.23672531290639262
  batch 323 loss: 0.23663840228374527
  batch 324 loss: 0.2366561116995635
  batch 325 loss: 0.23680750186626728
  batch 326 loss: 0.2370059356733334
  batch 327 loss: 0.23707330135759594
  batch 328 loss: 0.23703530589800056
  batch 329 loss: 0.23701122063452712
  batch 330 loss: 0.23698870544180725
  batch 331 loss: 0.236960531901737
  batch 332 loss: 0.23703734684421354
  batch 333 loss: 0.23706230883662766
  batch 334 loss: 0.23733602575734705
  batch 335 loss: 0.237473969183751
  batch 336 loss: 0.23758484303419078
  batch 337 loss: 0.23765889619330624
  batch 338 loss: 0.23769799214319365
  batch 339 loss: 0.2377641648054123
  batch 340 loss: 0.23769844012225375
  batch 341 loss: 0.2376459863615875
  batch 342 loss: 0.23758111798275283
  batch 343 loss: 0.23769237392150278
  batch 344 loss: 0.23774825928862706
  batch 345 loss: 0.23783563738283903
  batch 346 loss: 0.23786016273705257
  batch 347 loss: 0.23776666407798142
  batch 348 loss: 0.23773647035504208
  batch 349 loss: 0.2377528470959568
  batch 350 loss: 0.23795798646552221
  batch 351 loss: 0.23800094043597197
  batch 352 loss: 0.2379546303471381
  batch 353 loss: 0.2379141780877586
  batch 354 loss: 0.2378207128600212
  batch 355 loss: 0.2377200076697578
  batch 356 loss: 0.23785654769352313
  batch 357 loss: 0.23767079738508753
  batch 358 loss: 0.2375778306546158
  batch 359 loss: 0.23754437454563662
  batch 360 loss: 0.2374606353127294
  batch 361 loss: 0.2374003273628425
  batch 362 loss: 0.2373882097433944
  batch 363 loss: 0.23741354528537467
  batch 364 loss: 0.23730578764781848
  batch 365 loss: 0.23738098038385994
  batch 366 loss: 0.23736028743734777
  batch 367 loss: 0.23742078334011887
  batch 368 loss: 0.2374031874958588
  batch 369 loss: 0.23735975548826904
  batch 370 loss: 0.23733776743347582
  batch 371 loss: 0.23731739461582627
  batch 372 loss: 0.23734337660253688
  batch 373 loss: 0.23735384527224318
  batch 374 loss: 0.23722777997746186
  batch 375 loss: 0.23733738493919374
  batch 376 loss: 0.23748880252242088
  batch 377 loss: 0.23745416847875328
  batch 378 loss: 0.23746809300291474
  batch 379 loss: 0.237432315709723
  batch 380 loss: 0.23739698172399873
  batch 381 loss: 0.2373053817142026
  batch 382 loss: 0.23731296244248046
  batch 383 loss: 0.2371882105309409
  batch 384 loss: 0.23722552818556628
  batch 385 loss: 0.2371660658678451
  batch 386 loss: 0.23715452031459217
  batch 387 loss: 0.23711010197644394
  batch 388 loss: 0.2369877152221719
  batch 389 loss: 0.23693895316982025
  batch 390 loss: 0.23696580089819738
  batch 391 loss: 0.23687116184350474
  batch 392 loss: 0.23696836277994573
  batch 393 loss: 0.23699797560998806
  batch 394 loss: 0.23704669231236888
  batch 395 loss: 0.23706824783282943
  batch 396 loss: 0.2370385725916636
  batch 397 loss: 0.23709298865470838
  batch 398 loss: 0.23721042870726416
  batch 399 loss: 0.23720892296548476
  batch 400 loss: 0.23729641754180192
  batch 401 loss: 0.23730625215908535
  batch 402 loss: 0.2373454601165667
  batch 403 loss: 0.23734669842140255
  batch 404 loss: 0.2373661730475355
  batch 405 loss: 0.23742366193989176
  batch 406 loss: 0.23734816134416412
  batch 407 loss: 0.2373976824983625
  batch 408 loss: 0.23731418936422058
  batch 409 loss: 0.2372619757078096
  batch 410 loss: 0.23713049677813924
  batch 411 loss: 0.2371000232052629
  batch 412 loss: 0.23714293698662692
  batch 413 loss: 0.23714209403887787
  batch 414 loss: 0.23719427155123818
  batch 415 loss: 0.2371809122074081
  batch 416 loss: 0.2371243559397184
  batch 417 loss: 0.2371047542249556
  batch 418 loss: 0.23705932180704684
  batch 419 loss: 0.23708820474603012
  batch 420 loss: 0.23708671597497805
  batch 421 loss: 0.23715217149597448
  batch 422 loss: 0.23711205291522058
  batch 423 loss: 0.23714807862085654
  batch 424 loss: 0.23721116514138454
  batch 425 loss: 0.23718250057276558
  batch 426 loss: 0.2371527734496784
  batch 427 loss: 0.23716503866802055
  batch 428 loss: 0.23721962627545695
  batch 429 loss: 0.23710228131089733
  batch 430 loss: 0.23714582213135652
  batch 431 loss: 0.23718621462514394
  batch 432 loss: 0.23719446023029309
  batch 433 loss: 0.2371562327028147
  batch 434 loss: 0.23703200713418046
  batch 435 loss: 0.23710053806332337
  batch 436 loss: 0.23706306660667473
  batch 437 loss: 0.23698309459457137
  batch 438 loss: 0.23692823374924593
  batch 439 loss: 0.23685944168605674
  batch 440 loss: 0.23690894009037453
  batch 441 loss: 0.23690773053369285
  batch 442 loss: 0.23690495457989058
  batch 443 loss: 0.2368799740318251
  batch 444 loss: 0.23688389213235528
  batch 445 loss: 0.23682701185847935
  batch 446 loss: 0.23684472460383257
  batch 447 loss: 0.23681458004102343
  batch 448 loss: 0.23691475916919963
  batch 449 loss: 0.2368227602511578
  batch 450 loss: 0.2368429962462849
  batch 451 loss: 0.23679467179954977
  batch 452 loss: 0.2367602935209211
  batch 453 loss: 0.23670032597403914
  batch 454 loss: 0.23669478290669194
  batch 455 loss: 0.2366757000540639
  batch 456 loss: 0.23657087092859702
  batch 457 loss: 0.236535444040528
  batch 458 loss: 0.23652909709116257
  batch 459 loss: 0.2365001993241653
  batch 460 loss: 0.2364695469322412
  batch 461 loss: 0.23651281225448575
  batch 462 loss: 0.23654228306951977
  batch 463 loss: 0.23653318934126497
  batch 464 loss: 0.23644532041688418
  batch 465 loss: 0.23644411778578195
  batch 466 loss: 0.2363986672569754
  batch 467 loss: 0.2364892128384343
  batch 468 loss: 0.2363741358375957
  batch 469 loss: 0.23626627056583413
  batch 470 loss: 0.23629614989808265
  batch 471 loss: 0.23626999000202065
  batch 472 loss: 0.23605545935363081
LOSS train 0.23605545935363081 valid 0.23221814632415771
LOSS train 0.23605545935363081 valid 0.21662840247154236
LOSS train 0.23605545935363081 valid 0.22810712456703186
LOSS train 0.23605545935363081 valid 0.2132958620786667
LOSS train 0.23605545935363081 valid 0.21131432056427002
LOSS train 0.23605545935363081 valid 0.2162358413139979
LOSS train 0.23605545935363081 valid 0.21069796596254622
LOSS train 0.23605545935363081 valid 0.20721358247101307
LOSS train 0.23605545935363081 valid 0.20698554979430306
LOSS train 0.23605545935363081 valid 0.20424141138792037
LOSS train 0.23605545935363081 valid 0.2040098485621539
LOSS train 0.23605545935363081 valid 0.20689075191815695
LOSS train 0.23605545935363081 valid 0.20641449093818665
LOSS train 0.23605545935363081 valid 0.2050581755382674
LOSS train 0.23605545935363081 valid 0.2053842931985855
LOSS train 0.23605545935363081 valid 0.2088679289445281
LOSS train 0.23605545935363081 valid 0.20863946658723495
LOSS train 0.23605545935363081 valid 0.2078789687818951
LOSS train 0.23605545935363081 valid 0.2111897248970835
LOSS train 0.23605545935363081 valid 0.21065924018621446
LOSS train 0.23605545935363081 valid 0.2132360793295361
LOSS train 0.23605545935363081 valid 0.21240449154918845
LOSS train 0.23605545935363081 valid 0.21007566413153772
LOSS train 0.23605545935363081 valid 0.21022370892266432
LOSS train 0.23605545935363081 valid 0.2103743952512741
LOSS train 0.23605545935363081 valid 0.2097441410789123
LOSS train 0.23605545935363081 valid 0.20963483496948523
LOSS train 0.23605545935363081 valid 0.21033982134291104
LOSS train 0.23605545935363081 valid 0.2093063793305693
LOSS train 0.23605545935363081 valid 0.20919355849424998
LOSS train 0.23605545935363081 valid 0.20965740949876846
LOSS train 0.23605545935363081 valid 0.2095215986482799
LOSS train 0.23605545935363081 valid 0.20807812837037173
LOSS train 0.23605545935363081 valid 0.20748745913014693
LOSS train 0.23605545935363081 valid 0.2079065271786281
LOSS train 0.23605545935363081 valid 0.2086331629090839
LOSS train 0.23605545935363081 valid 0.20911248551832662
LOSS train 0.23605545935363081 valid 0.20922655141667315
LOSS train 0.23605545935363081 valid 0.21048242503251785
LOSS train 0.23605545935363081 valid 0.21063459813594818
LOSS train 0.23605545935363081 valid 0.21059192898796825
LOSS train 0.23605545935363081 valid 0.21188963169143313
LOSS train 0.23605545935363081 valid 0.21241254758003147
LOSS train 0.23605545935363081 valid 0.21213419227437538
LOSS train 0.23605545935363081 valid 0.2116062449084388
LOSS train 0.23605545935363081 valid 0.21100589406231177
LOSS train 0.23605545935363081 valid 0.21061902952955125
LOSS train 0.23605545935363081 valid 0.21229868909964958
LOSS train 0.23605545935363081 valid 0.2115420890705926
LOSS train 0.23605545935363081 valid 0.2123376289010048
LOSS train 0.23605545935363081 valid 0.21201498134463442
LOSS train 0.23605545935363081 valid 0.21161679407724968
LOSS train 0.23605545935363081 valid 0.21279773228573348
LOSS train 0.23605545935363081 valid 0.21283497909704843
LOSS train 0.23605545935363081 valid 0.21250882528044962
LOSS train 0.23605545935363081 valid 0.21286808060748236
LOSS train 0.23605545935363081 valid 0.21235033660604244
LOSS train 0.23605545935363081 valid 0.21271341139900274
LOSS train 0.23605545935363081 valid 0.21255531856569193
LOSS train 0.23605545935363081 valid 0.21243962546189626
LOSS train 0.23605545935363081 valid 0.21251697252031232
LOSS train 0.23605545935363081 valid 0.21243018849242118
LOSS train 0.23605545935363081 valid 0.21213974791859824
LOSS train 0.23605545935363081 valid 0.2127128776628524
LOSS train 0.23605545935363081 valid 0.21152009688890896
LOSS train 0.23605545935363081 valid 0.21132206871654047
LOSS train 0.23605545935363081 valid 0.21195155738004998
LOSS train 0.23605545935363081 valid 0.2113096319577273
LOSS train 0.23605545935363081 valid 0.2119902713575225
LOSS train 0.23605545935363081 valid 0.21259887942246028
LOSS train 0.23605545935363081 valid 0.21288557266685323
LOSS train 0.23605545935363081 valid 0.21351831923756334
LOSS train 0.23605545935363081 valid 0.21427043190557662
LOSS train 0.23605545935363081 valid 0.21391506231314428
LOSS train 0.23605545935363081 valid 0.21355757713317872
LOSS train 0.23605545935363081 valid 0.21378894934528753
LOSS train 0.23605545935363081 valid 0.2137746230348364
LOSS train 0.23605545935363081 valid 0.21370193935357606
LOSS train 0.23605545935363081 valid 0.2134812225269366
LOSS train 0.23605545935363081 valid 0.21308696288615464
LOSS train 0.23605545935363081 valid 0.21337169758331628
LOSS train 0.23605545935363081 valid 0.21317378649624383
LOSS train 0.23605545935363081 valid 0.21341207785060606
LOSS train 0.23605545935363081 valid 0.21305595125470841
LOSS train 0.23605545935363081 valid 0.21384803968317367
LOSS train 0.23605545935363081 valid 0.21380848194970642
LOSS train 0.23605545935363081 valid 0.21353528040578995
LOSS train 0.23605545935363081 valid 0.21384773691269485
LOSS train 0.23605545935363081 valid 0.21430149858587244
LOSS train 0.23605545935363081 valid 0.2148193198773596
LOSS train 0.23605545935363081 valid 0.21475443129356092
LOSS train 0.23605545935363081 valid 0.2149665833491346
LOSS train 0.23605545935363081 valid 0.21471908031612313
LOSS train 0.23605545935363081 valid 0.2150443730519173
LOSS train 0.23605545935363081 valid 0.21531782307122882
LOSS train 0.23605545935363081 valid 0.21554358629509807
LOSS train 0.23605545935363081 valid 0.2156509497423762
LOSS train 0.23605545935363081 valid 0.21616451274983736
LOSS train 0.23605545935363081 valid 0.21636989878283608
LOSS train 0.23605545935363081 valid 0.21645551979541777
LOSS train 0.23605545935363081 valid 0.21644585409967027
LOSS train 0.23605545935363081 valid 0.21685831862337449
LOSS train 0.23605545935363081 valid 0.21660107996278596
LOSS train 0.23605545935363081 valid 0.21637331336163557
LOSS train 0.23605545935363081 valid 0.21665795005503155
LOSS train 0.23605545935363081 valid 0.21681665769725475
LOSS train 0.23605545935363081 valid 0.2164881893686045
LOSS train 0.23605545935363081 valid 0.21659694874176272
LOSS train 0.23605545935363081 valid 0.21618119223949012
LOSS train 0.23605545935363081 valid 0.2161346510052681
LOSS train 0.23605545935363081 valid 0.2161503181801186
LOSS train 0.23605545935363081 valid 0.2164704403174775
LOSS train 0.23605545935363081 valid 0.2162122824023255
LOSS train 0.23605545935363081 valid 0.21634454855270552
LOSS train 0.23605545935363081 valid 0.21721625548342
LOSS train 0.23605545935363081 valid 0.21683736646483684
LOSS train 0.23605545935363081 valid 0.21753954619933397
LOSS train 0.23605545935363081 valid 0.21735291763887568
LOSS train 0.23605545935363081 valid 0.21706570184030452
LOSS train 0.23605545935363081 valid 0.21668034108976522
LOSS train 0.23605545935363081 valid 0.2164162935550548
LOSS train 0.23605545935363081 valid 0.21658930854230632
LOSS train 0.23605545935363081 valid 0.21660746670350794
LOSS train 0.23605545935363081 valid 0.21684666062074323
LOSS train 0.23605545935363081 valid 0.21678517246246337
LOSS train 0.23605545935363081 valid 0.21691292098590306
LOSS train 0.23605545935363081 valid 0.2167940313421835
LOSS train 0.23605545935363081 valid 0.21678804722614586
LOSS train 0.23605545935363081 valid 0.21642255459645
LOSS train 0.23605545935363081 valid 0.21613272462899868
LOSS train 0.23605545935363081 valid 0.21617971679181544
LOSS train 0.23605545935363081 valid 0.21614676680077205
LOSS train 0.23605545935363081 valid 0.21615641648159886
LOSS train 0.23605545935363081 valid 0.21632607136644535
LOSS train 0.23605545935363081 valid 0.21640238574257603
LOSS train 0.23605545935363081 valid 0.21632412295131123
LOSS train 0.23605545935363081 valid 0.21625315631828168
LOSS train 0.23605545935363081 valid 0.21615115013243497
LOSS train 0.23605545935363081 valid 0.2159620493436031
LOSS train 0.23605545935363081 valid 0.21581356823444367
LOSS train 0.23605545935363081 valid 0.21584605769062717
LOSS train 0.23605545935363081 valid 0.2161247837291637
LOSS train 0.23605545935363081 valid 0.2161552176608906
LOSS train 0.23605545935363081 valid 0.21617215406149626
LOSS train 0.23605545935363081 valid 0.21608397970939505
LOSS train 0.23605545935363081 valid 0.2160052649982988
LOSS train 0.23605545935363081 valid 0.21593580521693845
LOSS train 0.23605545935363081 valid 0.21589846194193169
LOSS train 0.23605545935363081 valid 0.2160262241459533
LOSS train 0.23605545935363081 valid 0.21607066055138907
LOSS train 0.23605545935363081 valid 0.21590403432877647
LOSS train 0.23605545935363081 valid 0.21581081820553855
LOSS train 0.23605545935363081 valid 0.21559714978816463
LOSS train 0.23605545935363081 valid 0.21569810904465714
LOSS train 0.23605545935363081 valid 0.21578434686506948
LOSS train 0.23605545935363081 valid 0.2157655967733799
LOSS train 0.23605545935363081 valid 0.2156790681895177
LOSS train 0.23605545935363081 valid 0.21585624665021896
LOSS train 0.23605545935363081 valid 0.21592417687365095
LOSS train 0.23605545935363081 valid 0.21610847981646658
LOSS train 0.23605545935363081 valid 0.21604085394314357
LOSS train 0.23605545935363081 valid 0.21610338268456636
LOSS train 0.23605545935363081 valid 0.21589785320627178
LOSS train 0.23605545935363081 valid 0.21603048720010898
LOSS train 0.23605545935363081 valid 0.21614529273726724
LOSS train 0.23605545935363081 valid 0.21607203754674958
LOSS train 0.23605545935363081 valid 0.21603923188355154
LOSS train 0.23605545935363081 valid 0.21587843261659145
LOSS train 0.23605545935363081 valid 0.21571756476128595
LOSS train 0.23605545935363081 valid 0.21587787726346183
LOSS train 0.23605545935363081 valid 0.2158154023495334
LOSS train 0.23605545935363081 valid 0.21586256413612254
LOSS train 0.23605545935363081 valid 0.21569781220717238
LOSS train 0.23605545935363081 valid 0.21611839909663147
LOSS train 0.23605545935363081 valid 0.21609446917261396
LOSS train 0.23605545935363081 valid 0.2160112402317199
LOSS train 0.23605545935363081 valid 0.21606686947035925
LOSS train 0.23605545935363081 valid 0.21620316033282977
LOSS train 0.23605545935363081 valid 0.21640234086766588
LOSS train 0.23605545935363081 valid 0.21626049793428845
LOSS train 0.23605545935363081 valid 0.21623018960267798
LOSS train 0.23605545935363081 valid 0.21598571628987134
LOSS train 0.23605545935363081 valid 0.21609859306955598
LOSS train 0.23605545935363081 valid 0.21602349096666212
LOSS train 0.23605545935363081 valid 0.21588332145600705
LOSS train 0.23605545935363081 valid 0.21602830979772794
LOSS train 0.23605545935363081 valid 0.21594495601195066
LOSS train 0.23605545935363081 valid 0.21607474435834176
LOSS train 0.23605545935363081 valid 0.2158741845340325
LOSS train 0.23605545935363081 valid 0.21586994787580088
LOSS train 0.23605545935363081 valid 0.2159607231773007
LOSS train 0.23605545935363081 valid 0.21597395938200256
LOSS train 0.23605545935363081 valid 0.215724699469428
LOSS train 0.23605545935363081 valid 0.2155258799951101
LOSS train 0.23605545935363081 valid 0.21542441707391005
LOSS train 0.23605545935363081 valid 0.21552706905165497
LOSS train 0.23605545935363081 valid 0.2155444783910277
LOSS train 0.23605545935363081 valid 0.2157429476277997
LOSS train 0.23605545935363081 valid 0.21569568308154544
LOSS train 0.23605545935363081 valid 0.2158079221099615
LOSS train 0.23605545935363081 valid 0.21561856612340727
LOSS train 0.23605545935363081 valid 0.21559299137627724
LOSS train 0.23605545935363081 valid 0.21566922911282244
LOSS train 0.23605545935363081 valid 0.21541733338552363
LOSS train 0.23605545935363081 valid 0.21529403903135438
LOSS train 0.23605545935363081 valid 0.21529055791861804
LOSS train 0.23605545935363081 valid 0.21522362184697302
LOSS train 0.23605545935363081 valid 0.21508831473497245
LOSS train 0.23605545935363081 valid 0.21514844944317374
LOSS train 0.23605545935363081 valid 0.21531955777179626
LOSS train 0.23605545935363081 valid 0.21556870526329602
LOSS train 0.23605545935363081 valid 0.2154161253908895
LOSS train 0.23605545935363081 valid 0.21543136791723994
LOSS train 0.23605545935363081 valid 0.2153915088171157
LOSS train 0.23605545935363081 valid 0.21518546786419182
LOSS train 0.23605545935363081 valid 0.21501136847116328
LOSS train 0.23605545935363081 valid 0.2147677804467865
LOSS train 0.23605545935363081 valid 0.21479420309219885
LOSS train 0.23605545935363081 valid 0.2147600411280105
LOSS train 0.23605545935363081 valid 0.21485286260193046
LOSS train 0.23605545935363081 valid 0.21483673829568456
LOSS train 0.23605545935363081 valid 0.21494752376734674
LOSS train 0.23605545935363081 valid 0.21511822745137152
LOSS train 0.23605545935363081 valid 0.21500822682199733
LOSS train 0.23605545935363081 valid 0.21486936648686727
LOSS train 0.23605545935363081 valid 0.21472837795725966
LOSS train 0.23605545935363081 valid 0.21474224210835763
LOSS train 0.23605545935363081 valid 0.21472378859394475
LOSS train 0.23605545935363081 valid 0.21459830718269515
LOSS train 0.23605545935363081 valid 0.2146062038514925
LOSS train 0.23605545935363081 valid 0.21458194279051446
LOSS train 0.23605545935363081 valid 0.21469853625729166
LOSS train 0.23605545935363081 valid 0.21502016964388507
LOSS train 0.23605545935363081 valid 0.21493178695185572
LOSS train 0.23605545935363081 valid 0.2150174782631245
LOSS train 0.23605545935363081 valid 0.2148666674176515
LOSS train 0.23605545935363081 valid 0.21493887593223074
LOSS train 0.23605545935363081 valid 0.21491343765950002
LOSS train 0.23605545935363081 valid 0.2149030768846368
LOSS train 0.23605545935363081 valid 0.21452017885943253
LOSS train 0.23605545935363081 valid 0.21437498395373714
LOSS train 0.23605545935363081 valid 0.21435655031568748
LOSS train 0.23605545935363081 valid 0.21416610332182895
LOSS train 0.23605545935363081 valid 0.21419240497663372
LOSS train 0.23605545935363081 valid 0.21430188380942053
LOSS train 0.23605545935363081 valid 0.21444599500031974
LOSS train 0.23605545935363081 valid 0.21449977346518745
LOSS train 0.23605545935363081 valid 0.21449794283797663
LOSS train 0.23605545935363081 valid 0.2142745376710432
LOSS train 0.23605545935363081 valid 0.21438553565740584
LOSS train 0.23605545935363081 valid 0.2144420632565639
LOSS train 0.23605545935363081 valid 0.21433521038483058
LOSS train 0.23605545935363081 valid 0.2141072556906538
LOSS train 0.23605545935363081 valid 0.21427050209420873
LOSS train 0.23605545935363081 valid 0.21422073759284674
LOSS train 0.23605545935363081 valid 0.2139717762474902
LOSS train 0.23605545935363081 valid 0.21389511917583673
LOSS train 0.23605545935363081 valid 0.21409754876711573
LOSS train 0.23605545935363081 valid 0.21395101959180648
LOSS train 0.23605545935363081 valid 0.2138086365392575
LOSS train 0.23605545935363081 valid 0.21389108759233322
LOSS train 0.23605545935363081 valid 0.21411490622367568
LOSS train 0.23605545935363081 valid 0.21392230995481004
LOSS train 0.23605545935363081 valid 0.2139805219503063
LOSS train 0.23605545935363081 valid 0.21391268198220234
LOSS train 0.23605545935363081 valid 0.21398268701662695
LOSS train 0.23605545935363081 valid 0.21386691509337907
LOSS train 0.23605545935363081 valid 0.21396532626961595
LOSS train 0.23605545935363081 valid 0.21412701926931574
LOSS train 0.23605545935363081 valid 0.2139703721911819
LOSS train 0.23605545935363081 valid 0.2139333031824154
LOSS train 0.23605545935363081 valid 0.21380043615970543
LOSS train 0.23605545935363081 valid 0.2137150220272742
LOSS train 0.23605545935363081 valid 0.21362204463595022
LOSS train 0.23605545935363081 valid 0.2136662818626924
LOSS train 0.23605545935363081 valid 0.21369197759507358
LOSS train 0.23605545935363081 valid 0.21374916331001997
LOSS train 0.23605545935363081 valid 0.2137787448202106
LOSS train 0.23605545935363081 valid 0.21372353592653856
LOSS train 0.23605545935363081 valid 0.2136004919984511
LOSS train 0.23605545935363081 valid 0.2134325168629134
LOSS train 0.23605545935363081 valid 0.21333438263717272
LOSS train 0.23605545935363081 valid 0.21335181751857799
LOSS train 0.23605545935363081 valid 0.21329936804905744
LOSS train 0.23605545935363081 valid 0.21334238925523924
LOSS train 0.23605545935363081 valid 0.21331697165757627
LOSS train 0.23605545935363081 valid 0.21326366523832394
LOSS train 0.23605545935363081 valid 0.21321756713506249
LOSS train 0.23605545935363081 valid 0.21313598896392902
LOSS train 0.23605545935363081 valid 0.21301623819203214
LOSS train 0.23605545935363081 valid 0.21305452466420702
LOSS train 0.23605545935363081 valid 0.2130021285520841
LOSS train 0.23605545935363081 valid 0.21297433672301191
LOSS train 0.23605545935363081 valid 0.21294316689984327
LOSS train 0.23605545935363081 valid 0.2129336454100528
LOSS train 0.23605545935363081 valid 0.21288418930930061
LOSS train 0.23605545935363081 valid 0.2127844528718428
LOSS train 0.23605545935363081 valid 0.21287037072165702
LOSS train 0.23605545935363081 valid 0.21289314972517084
LOSS train 0.23605545935363081 valid 0.2128656240304311
LOSS train 0.23605545935363081 valid 0.2127612829703429
LOSS train 0.23605545935363081 valid 0.21275093983735469
LOSS train 0.23605545935363081 valid 0.2126342525379886
LOSS train 0.23605545935363081 valid 0.21265095560566374
LOSS train 0.23605545935363081 valid 0.21258483509548376
LOSS train 0.23605545935363081 valid 0.21286533607376945
LOSS train 0.23605545935363081 valid 0.21281717067238562
LOSS train 0.23605545935363081 valid 0.2128641816799517
LOSS train 0.23605545935363081 valid 0.21287214895861048
LOSS train 0.23605545935363081 valid 0.21283236761246957
LOSS train 0.23605545935363081 valid 0.21285076976014103
LOSS train 0.23605545935363081 valid 0.21281805171225315
LOSS train 0.23605545935363081 valid 0.21294041492116336
LOSS train 0.23605545935363081 valid 0.21291933953762054
LOSS train 0.23605545935363081 valid 0.21283964614073436
LOSS train 0.23605545935363081 valid 0.21280876570675947
LOSS train 0.23605545935363081 valid 0.21282390869378293
LOSS train 0.23605545935363081 valid 0.21279547218256775
LOSS train 0.23605545935363081 valid 0.2127354423454189
LOSS train 0.23605545935363081 valid 0.21264604292809963
LOSS train 0.23605545935363081 valid 0.21278625021099673
LOSS train 0.23605545935363081 valid 0.2127907113721652
LOSS train 0.23605545935363081 valid 0.21267656073112606
LOSS train 0.23605545935363081 valid 0.21267039549571495
LOSS train 0.23605545935363081 valid 0.2125265368131491
LOSS train 0.23605545935363081 valid 0.21257493672378225
LOSS train 0.23605545935363081 valid 0.21250083345338838
LOSS train 0.23605545935363081 valid 0.2124687400078628
LOSS train 0.23605545935363081 valid 0.21247557534816416
LOSS train 0.23605545935363081 valid 0.2125007413553469
LOSS train 0.23605545935363081 valid 0.21243970294193198
LOSS train 0.23605545935363081 valid 0.2124280993299312
LOSS train 0.23605545935363081 valid 0.21259545304395774
LOSS train 0.23605545935363081 valid 0.2125039569096651
LOSS train 0.23605545935363081 valid 0.21238535433562833
LOSS train 0.23605545935363081 valid 0.21231220205802293
LOSS train 0.23605545935363081 valid 0.21247505297116073
LOSS train 0.23605545935363081 valid 0.2124892083057285
LOSS train 0.23605545935363081 valid 0.21238294142713238
LOSS train 0.23605545935363081 valid 0.21252959918449907
LOSS train 0.23605545935363081 valid 0.21243824602222164
LOSS train 0.23605545935363081 valid 0.212436055666522
LOSS train 0.23605545935363081 valid 0.21232219496551824
LOSS train 0.23605545935363081 valid 0.21245198021101397
LOSS train 0.23605545935363081 valid 0.21255272708941197
LOSS train 0.23605545935363081 valid 0.21252584061181615
LOSS train 0.23605545935363081 valid 0.21242092897637777
LOSS train 0.23605545935363081 valid 0.2125271908168135
LOSS train 0.23605545935363081 valid 0.21245207614065242
LOSS train 0.23605545935363081 valid 0.21245990761688777
LOSS train 0.23605545935363081 valid 0.21256201665455798
LOSS train 0.23605545935363081 valid 0.21260623608461834
LOSS train 0.23605545935363081 valid 0.2126290217759589
LOSS train 0.23605545935363081 valid 0.2126199179618372
LOSS train 0.23605545935363081 valid 0.2124721067891994
LOSS train 0.23605545935363081 valid 0.21250148875157485
LOSS train 0.23605545935363081 valid 0.21256359217881488
LOSS train 0.23605545935363081 valid 0.21249533290636605
LOSS train 0.23605545935363081 valid 0.21240316817023297
LOSS train 0.23605545935363081 valid 0.2125837424563037
LOSS train 0.23605545935363081 valid 0.21249018580629556
LOSS train 0.23605545935363081 valid 0.21250897028498886
LOSS train 0.23605545935363081 valid 0.21257845166629338
LOSS train 0.23605545935363081 valid 0.21249983549773038
LOSS train 0.23605545935363081 valid 0.2125774707696209
LOSS train 0.23605545935363081 valid 0.21258433598801085
LOSS train 0.23605545935363081 valid 0.2125246075339798
LOSS train 0.23605545935363081 valid 0.2123815912105467
LOSS train 0.23605545935363081 valid 0.21233252292564567
EPOCH 22:
  batch 1 loss: 0.2281997799873352
  batch 2 loss: 0.25596868991851807
  batch 3 loss: 0.24595117568969727
  batch 4 loss: 0.2444499507546425
  batch 5 loss: 0.2483487010002136
  batch 6 loss: 0.24665540705124536
  batch 7 loss: 0.24302334231989725
  batch 8 loss: 0.23973476141691208
  batch 9 loss: 0.2433825367026859
  batch 10 loss: 0.24028732776641845
  batch 11 loss: 0.2407012622464787
  batch 12 loss: 0.2395211843152841
  batch 13 loss: 0.23597009250750908
  batch 14 loss: 0.23540217642273223
  batch 15 loss: 0.23635267317295075
  batch 16 loss: 0.23612353950738907
  batch 17 loss: 0.23754375471788294
  batch 18 loss: 0.23670796553293863
  batch 19 loss: 0.23559504590536418
  batch 20 loss: 0.23277938961982728
  batch 21 loss: 0.23598369104521616
  batch 22 loss: 0.23673888634551654
  batch 23 loss: 0.23770713806152344
  batch 24 loss: 0.23759982051948705
  batch 25 loss: 0.23828939139842986
  batch 26 loss: 0.23826825618743896
  batch 27 loss: 0.23812663996661151
  batch 28 loss: 0.23940701889140265
  batch 29 loss: 0.23924468971531967
  batch 30 loss: 0.2401002342502276
  batch 31 loss: 0.24048643294842012
  batch 32 loss: 0.24140934040769935
  batch 33 loss: 0.24285377291115848
  batch 34 loss: 0.2425411549561164
  batch 35 loss: 0.2440050563641957
  batch 36 loss: 0.24408195374740493
  batch 37 loss: 0.2445295134911666
  batch 38 loss: 0.24401129664559112
  batch 39 loss: 0.24431675710739234
  batch 40 loss: 0.2437126472592354
  batch 41 loss: 0.2434911335386881
  batch 42 loss: 0.2430301649229867
  batch 43 loss: 0.24332722674968632
  batch 44 loss: 0.24372649125077508
  batch 45 loss: 0.2430018464724223
  batch 46 loss: 0.24146745580693948
  batch 47 loss: 0.24054673155571552
  batch 48 loss: 0.2404813701286912
  batch 49 loss: 0.24089543703867464
  batch 50 loss: 0.24205519646406173
  batch 51 loss: 0.2416740489356658
  batch 52 loss: 0.24161420561946356
  batch 53 loss: 0.24115850200068276
  batch 54 loss: 0.24106281305904742
  batch 55 loss: 0.24108020067214966
  batch 56 loss: 0.24132786904062545
  batch 57 loss: 0.24082020970813015
  batch 58 loss: 0.24066089530443324
  batch 59 loss: 0.2406926789021088
  batch 60 loss: 0.24145894174774488
  batch 61 loss: 0.2415645852929256
  batch 62 loss: 0.2419290367153383
  batch 63 loss: 0.24114681330938187
  batch 64 loss: 0.24049635347910225
  batch 65 loss: 0.23986081228806422
  batch 66 loss: 0.23948920360117248
  batch 67 loss: 0.23951041342607185
  batch 68 loss: 0.23947653327794635
  batch 69 loss: 0.23978148271208224
  batch 70 loss: 0.23992597737482615
  batch 71 loss: 0.24001787476976152
  batch 72 loss: 0.24006881192326546
  batch 73 loss: 0.240127392419397
  batch 74 loss: 0.2401530333870166
  batch 75 loss: 0.24019127567609153
  batch 76 loss: 0.24053660466482765
  batch 77 loss: 0.24047213792800903
  batch 78 loss: 0.24057387675230318
  batch 79 loss: 0.24038798371447792
  batch 80 loss: 0.24040202014148235
  batch 81 loss: 0.24047833920261005
  batch 82 loss: 0.24057477567254043
  batch 83 loss: 0.24011369390660023
  batch 84 loss: 0.2395638841248694
  batch 85 loss: 0.2402050274259904
  batch 86 loss: 0.2397258785921474
  batch 87 loss: 0.23938366445316667
  batch 88 loss: 0.23906119540333748
  batch 89 loss: 0.23870192987195563
  batch 90 loss: 0.23877889000707203
  batch 91 loss: 0.23851587123923249
  batch 92 loss: 0.23846792543063994
  batch 93 loss: 0.2379762565576902
  batch 94 loss: 0.23780531673989397
  batch 95 loss: 0.23773421802018818
  batch 96 loss: 0.23768769313270846
  batch 97 loss: 0.2383093571232766
  batch 98 loss: 0.23810884843067248
  batch 99 loss: 0.23765828121792187
  batch 100 loss: 0.23793065130710603
  batch 101 loss: 0.23786965484666353
  batch 102 loss: 0.23798714256754108
  batch 103 loss: 0.23769624748276275
  batch 104 loss: 0.23791101231024817
  batch 105 loss: 0.23757670266287667
  batch 106 loss: 0.23742935469128051
  batch 107 loss: 0.23746393147473024
  batch 108 loss: 0.23710320103499624
  batch 109 loss: 0.23740897963353252
  batch 110 loss: 0.23745566931637851
  batch 111 loss: 0.23761311549324174
  batch 112 loss: 0.23744248652032443
  batch 113 loss: 0.23730652443075603
  batch 114 loss: 0.23735496597854713
  batch 115 loss: 0.23698769030363664
  batch 116 loss: 0.23679117729951596
  batch 117 loss: 0.23659051381624663
  batch 118 loss: 0.23667719796047373
  batch 119 loss: 0.23675341954251297
  batch 120 loss: 0.23665952533483506
  batch 121 loss: 0.2365490065133276
  batch 122 loss: 0.23633697958754712
  batch 123 loss: 0.2360006041400801
  batch 124 loss: 0.23594422494211503
  batch 125 loss: 0.23563149762153626
  batch 126 loss: 0.23565457229103362
  batch 127 loss: 0.2358521670103073
  batch 128 loss: 0.235981359728612
  batch 129 loss: 0.23629330537577933
  batch 130 loss: 0.23629123167349741
  batch 131 loss: 0.23628653915783832
  batch 132 loss: 0.2364608564160087
  batch 133 loss: 0.23642241417017199
  batch 134 loss: 0.23632999976624303
  batch 135 loss: 0.23608512624546332
  batch 136 loss: 0.23629076592624187
  batch 137 loss: 0.23657673803994256
  batch 138 loss: 0.23661980745585068
  batch 139 loss: 0.23649670430224576
  batch 140 loss: 0.23683558148997172
  batch 141 loss: 0.23667403030480053
  batch 142 loss: 0.23642666449009533
  batch 143 loss: 0.236616925759749
  batch 144 loss: 0.23651383351534605
  batch 145 loss: 0.2365101414507833
  batch 146 loss: 0.2366762819559607
  batch 147 loss: 0.23666904146979456
  batch 148 loss: 0.23670817720326218
  batch 149 loss: 0.23645986006563943
  batch 150 loss: 0.23636202315489452
  batch 151 loss: 0.23597177606544747
  batch 152 loss: 0.23577653038266458
  batch 153 loss: 0.23572446610413345
  batch 154 loss: 0.2354336263878005
  batch 155 loss: 0.23536806789136702
  batch 156 loss: 0.23543052308452436
  batch 157 loss: 0.23548541687856053
  batch 158 loss: 0.235458512472201
  batch 159 loss: 0.23570847173906723
  batch 160 loss: 0.2358201378956437
  batch 161 loss: 0.2356591866068218
  batch 162 loss: 0.23575725259236346
  batch 163 loss: 0.23558324931955044
  batch 164 loss: 0.2357222175271046
  batch 165 loss: 0.23606606008428516
  batch 166 loss: 0.2364330116704286
  batch 167 loss: 0.23641879330138246
  batch 168 loss: 0.2362120451316947
  batch 169 loss: 0.23630750514346466
  batch 170 loss: 0.23645258142667658
  batch 171 loss: 0.23656813338486074
  batch 172 loss: 0.2365398944117302
  batch 173 loss: 0.23663758663083775
  batch 174 loss: 0.23683008089147764
  batch 175 loss: 0.23693414722170147
  batch 176 loss: 0.2370967355302789
  batch 177 loss: 0.23734494269230946
  batch 178 loss: 0.23727856559699842
  batch 179 loss: 0.23728139174051124
  batch 180 loss: 0.2372464877863725
  batch 181 loss: 0.23723436768542336
  batch 182 loss: 0.2372370339356936
  batch 183 loss: 0.23725601387479917
  batch 184 loss: 0.23731134394588677
  batch 185 loss: 0.23703751612353968
  batch 186 loss: 0.23698182892735287
  batch 187 loss: 0.23713245167133004
  batch 188 loss: 0.23707164372218417
  batch 189 loss: 0.23708389675806438
  batch 190 loss: 0.23689513833899248
  batch 191 loss: 0.23689327026224885
  batch 192 loss: 0.23664763624159
  batch 193 loss: 0.23663237474742949
  batch 194 loss: 0.23662627373159545
  batch 195 loss: 0.23674787298226968
  batch 196 loss: 0.23659915142521568
  batch 197 loss: 0.2366459339403259
  batch 198 loss: 0.23659770243396663
  batch 199 loss: 0.2365977458618394
  batch 200 loss: 0.2365378710627556
  batch 201 loss: 0.23635866378077228
  batch 202 loss: 0.23647806682799122
  batch 203 loss: 0.23653267140458958
  batch 204 loss: 0.23657331586468452
  batch 205 loss: 0.2364706650012877
  batch 206 loss: 0.23647828494171494
  batch 207 loss: 0.23648424583356736
  batch 208 loss: 0.23630334451221502
  batch 209 loss: 0.23629118858627154
  batch 210 loss: 0.23628660092751186
  batch 211 loss: 0.2361623071381266
  batch 212 loss: 0.2361652004831242
  batch 213 loss: 0.2360405940125246
  batch 214 loss: 0.23600587005091606
  batch 215 loss: 0.23597556324892266
  batch 216 loss: 0.2358865619947513
  batch 217 loss: 0.23583898531951114
  batch 218 loss: 0.2356471952495225
  batch 219 loss: 0.23527814590767637
  batch 220 loss: 0.23532340960069137
  batch 221 loss: 0.23526441949794735
  batch 222 loss: 0.23526439627817086
  batch 223 loss: 0.23511276450926946
  batch 224 loss: 0.23505424575081893
  batch 225 loss: 0.2350299452410804
  batch 226 loss: 0.2351399023184734
  batch 227 loss: 0.23507345549860714
  batch 228 loss: 0.23508964962603754
  batch 229 loss: 0.23509432629206295
  batch 230 loss: 0.23512641871752946
  batch 231 loss: 0.2353306793934339
  batch 232 loss: 0.235368635569667
  batch 233 loss: 0.2353817145839781
  batch 234 loss: 0.23530440510083467
  batch 235 loss: 0.2352262851405651
  batch 236 loss: 0.23524649788515042
  batch 237 loss: 0.23511089346831357
  batch 238 loss: 0.23508712098378093
  batch 239 loss: 0.23504079092746
  batch 240 loss: 0.23513436273982127
  batch 241 loss: 0.23501516186102792
  batch 242 loss: 0.23492448327462534
  batch 243 loss: 0.23486924772399934
  batch 244 loss: 0.2347581319388796
  batch 245 loss: 0.23468848065454131
  batch 246 loss: 0.23458685186820302
  batch 247 loss: 0.23460315674664037
  batch 248 loss: 0.234699378631288
  batch 249 loss: 0.23469491057606587
  batch 250 loss: 0.23471246367692947
  batch 251 loss: 0.23456378174255568
  batch 252 loss: 0.23450449996051334
  batch 253 loss: 0.23440059289159512
  batch 254 loss: 0.23438360958587467
  batch 255 loss: 0.2342689854257247
  batch 256 loss: 0.23417203756980598
  batch 257 loss: 0.2341661875350002
  batch 258 loss: 0.23414028805586717
  batch 259 loss: 0.23399248149634327
  batch 260 loss: 0.2340044642870243
  batch 261 loss: 0.23397266973252498
  batch 262 loss: 0.23391037639088302
  batch 263 loss: 0.23378793396650613
  batch 264 loss: 0.2338428795902115
  batch 265 loss: 0.2337562510989747
  batch 266 loss: 0.23357128015810386
  batch 267 loss: 0.23343539399823893
  batch 268 loss: 0.23334400283534135
  batch 269 loss: 0.23338221107716897
  batch 270 loss: 0.23329874989059235
  batch 271 loss: 0.23323301223151358
  batch 272 loss: 0.23316723891698263
  batch 273 loss: 0.23305177377475486
  batch 274 loss: 0.23296119843738794
  batch 275 loss: 0.23289341812784023
  batch 276 loss: 0.23283819098403488
  batch 277 loss: 0.23282317085601792
  batch 278 loss: 0.23264653696645077
  batch 279 loss: 0.2326164829901897
  batch 280 loss: 0.23257505797914096
  batch 281 loss: 0.2325348221958744
  batch 282 loss: 0.23240263633271482
  batch 283 loss: 0.23240644871767333
  batch 284 loss: 0.23244642576491328
  batch 285 loss: 0.23243540185585357
  batch 286 loss: 0.2323965152556246
  batch 287 loss: 0.2323548930343435
  batch 288 loss: 0.23238923509294787
  batch 289 loss: 0.23234949150093698
  batch 290 loss: 0.23228308856487273
  batch 291 loss: 0.23233959980027372
  batch 292 loss: 0.23235766150771756
  batch 293 loss: 0.23231293413622794
  batch 294 loss: 0.2320536406976836
  batch 295 loss: 0.23204950538732239
  batch 296 loss: 0.23204397812888428
  batch 297 loss: 0.23197699496240326
  batch 298 loss: 0.23194484872705984
  batch 299 loss: 0.23201629190540632
  batch 300 loss: 0.23191492130359015
  batch 301 loss: 0.23184804128253975
  batch 302 loss: 0.23206199064160024
  batch 303 loss: 0.23200607073582438
  batch 304 loss: 0.2319949001662041
  batch 305 loss: 0.2319658555945412
  batch 306 loss: 0.232066211649795
  batch 307 loss: 0.23211114409497974
  batch 308 loss: 0.23214932347272899
  batch 309 loss: 0.232147635234984
  batch 310 loss: 0.232251304339978
  batch 311 loss: 0.2321736213191146
  batch 312 loss: 0.23217055187202418
  batch 313 loss: 0.23214971785918592
  batch 314 loss: 0.23209411166845614
  batch 315 loss: 0.23203231630817292
  batch 316 loss: 0.232114797198697
  batch 317 loss: 0.23215560610362032
  batch 318 loss: 0.23196314556418723
  batch 319 loss: 0.2318955947145773
  batch 320 loss: 0.23186322110705077
  batch 321 loss: 0.23194823397839923
  batch 322 loss: 0.23195909995655095
  batch 323 loss: 0.23186425057358048
  batch 324 loss: 0.23186808617578614
  batch 325 loss: 0.23200481456059677
  batch 326 loss: 0.23219593769750713
  batch 327 loss: 0.23225818328354336
  batch 328 loss: 0.2322521665383403
  batch 329 loss: 0.23224450838058552
  batch 330 loss: 0.2322420953801184
  batch 331 loss: 0.23217550996026964
  batch 332 loss: 0.23222116686134453
  batch 333 loss: 0.23224158795388253
  batch 334 loss: 0.23249303098924146
  batch 335 loss: 0.23258675728271255
  batch 336 loss: 0.2327099907256308
  batch 337 loss: 0.23283870549159516
  batch 338 loss: 0.23286739335610315
  batch 339 loss: 0.2328839695031664
  batch 340 loss: 0.23278503146241694
  batch 341 loss: 0.23274357695558553
  batch 342 loss: 0.2327019945222732
  batch 343 loss: 0.23275051785106213
  batch 344 loss: 0.2327973209754672
  batch 345 loss: 0.23288368189680403
  batch 346 loss: 0.23291230335228705
  batch 347 loss: 0.23280958367527735
  batch 348 loss: 0.2327723355385764
  batch 349 loss: 0.23277406795830985
  batch 350 loss: 0.2329763552546501
  batch 351 loss: 0.23301552550235705
  batch 352 loss: 0.23295978008007462
  batch 353 loss: 0.23291676839755548
  batch 354 loss: 0.2328442993473872
  batch 355 loss: 0.23272744232500103
  batch 356 loss: 0.23285552290048492
  batch 357 loss: 0.23264940049634927
  batch 358 loss: 0.23254715147631128
  batch 359 loss: 0.2325163460790613
  batch 360 loss: 0.2324689223534531
  batch 361 loss: 0.23238468252720926
  batch 362 loss: 0.23238451568955215
  batch 363 loss: 0.23239100348850913
  batch 364 loss: 0.23227828716511253
  batch 365 loss: 0.23233444584559088
  batch 366 loss: 0.23234665369401214
  batch 367 loss: 0.2323958975255327
  batch 368 loss: 0.2324385231603747
  batch 369 loss: 0.2324062247424914
  batch 370 loss: 0.23240504377597088
  batch 371 loss: 0.23240450418220376
  batch 372 loss: 0.23243343790051757
  batch 373 loss: 0.2324516455625401
  batch 374 loss: 0.23233187481521922
  batch 375 loss: 0.23241008738676708
  batch 376 loss: 0.23252158679385135
  batch 377 loss: 0.23249822368830522
  batch 378 loss: 0.2325362093628399
  batch 379 loss: 0.2325178609281228
  batch 380 loss: 0.23247146053533804
  batch 381 loss: 0.23238901164275141
  batch 382 loss: 0.23239300922729583
  batch 383 loss: 0.23231135160898103
  batch 384 loss: 0.23234978997303793
  batch 385 loss: 0.23232301272355116
  batch 386 loss: 0.23230783761010887
  batch 387 loss: 0.23222740414222698
  batch 388 loss: 0.23213907181448543
  batch 389 loss: 0.2320885898061767
  batch 390 loss: 0.23205554836835618
  batch 391 loss: 0.2319741981185001
  batch 392 loss: 0.23202883478786265
  batch 393 loss: 0.23209170003732046
  batch 394 loss: 0.23216183789944286
  batch 395 loss: 0.23221998437295985
  batch 396 loss: 0.23221595912720217
  batch 397 loss: 0.23226878463621403
  batch 398 loss: 0.23239727337006946
  batch 399 loss: 0.23239874757620924
  batch 400 loss: 0.23248396821320058
  batch 401 loss: 0.23253163085911338
  batch 402 loss: 0.23254720555312597
  batch 403 loss: 0.23258015905361318
  batch 404 loss: 0.23259678475632525
  batch 405 loss: 0.23264839855241187
  batch 406 loss: 0.23257372341132515
  batch 407 loss: 0.23264691672981225
  batch 408 loss: 0.23257281426705567
  batch 409 loss: 0.23255666543918602
  batch 410 loss: 0.23241865980189022
  batch 411 loss: 0.232412840690636
  batch 412 loss: 0.23245639802472104
  batch 413 loss: 0.23246112382873785
  batch 414 loss: 0.23251012359984255
  batch 415 loss: 0.23243700229977987
  batch 416 loss: 0.23244289674151403
  batch 417 loss: 0.23240648002790318
  batch 418 loss: 0.2323630595820372
  batch 419 loss: 0.23234246703207065
  batch 420 loss: 0.2323302354131426
  batch 421 loss: 0.23239315051647377
  batch 422 loss: 0.23233838319354713
  batch 423 loss: 0.23237240282779045
  batch 424 loss: 0.23243677015152742
  batch 425 loss: 0.2323987553400152
  batch 426 loss: 0.23236719413962162
  batch 427 loss: 0.23236774634002802
  batch 428 loss: 0.23242604993631907
  batch 429 loss: 0.23232433078311263
  batch 430 loss: 0.23237402019805686
  batch 431 loss: 0.23239697626599579
  batch 432 loss: 0.23243497691496653
  batch 433 loss: 0.23239209064159877
  batch 434 loss: 0.23226132954595274
  batch 435 loss: 0.23231269511683234
  batch 436 loss: 0.23228566167414735
  batch 437 loss: 0.23217705625282
  batch 438 loss: 0.23212147252336485
  batch 439 loss: 0.23205313100087072
  batch 440 loss: 0.2321144781329415
  batch 441 loss: 0.2321133959590713
  batch 442 loss: 0.23209291160511214
  batch 443 loss: 0.232075229556243
  batch 444 loss: 0.2320782391539028
  batch 445 loss: 0.23201740116215824
  batch 446 loss: 0.23204028496159565
  batch 447 loss: 0.23201097614679828
  batch 448 loss: 0.232105513469183
  batch 449 loss: 0.2320275355487199
  batch 450 loss: 0.2320263186097145
  batch 451 loss: 0.23197195052836264
  batch 452 loss: 0.23190672609922106
  batch 453 loss: 0.23182891901360442
  batch 454 loss: 0.23180956925195745
  batch 455 loss: 0.23180243513741336
  batch 456 loss: 0.2317104686313031
  batch 457 loss: 0.23167496792466688
  batch 458 loss: 0.23165525268259007
  batch 459 loss: 0.2316166463435865
  batch 460 loss: 0.23160206601023675
  batch 461 loss: 0.23162771887479272
  batch 462 loss: 0.2316586681000598
  batch 463 loss: 0.23167724672455509
  batch 464 loss: 0.23162807052120052
  batch 465 loss: 0.2316337445410349
  batch 466 loss: 0.23154586377701533
  batch 467 loss: 0.23166251587791156
  batch 468 loss: 0.23156855490982023
  batch 469 loss: 0.23145674864874718
  batch 470 loss: 0.2314813916036423
  batch 471 loss: 0.23150403215500454
  batch 472 loss: 0.2312853043993651
LOSS train 0.2312853043993651 valid 0.22986246645450592
LOSS train 0.2312853043993651 valid 0.2129058688879013
LOSS train 0.2312853043993651 valid 0.22102240721384683
LOSS train 0.2312853043993651 valid 0.20564205944538116
LOSS train 0.2312853043993651 valid 0.2026711881160736
LOSS train 0.2312853043993651 valid 0.20682989060878754
LOSS train 0.2312853043993651 valid 0.20228766330650874
LOSS train 0.2312853043993651 valid 0.1989166960120201
LOSS train 0.2312853043993651 valid 0.19838598039415148
LOSS train 0.2312853043993651 valid 0.19608457535505294
LOSS train 0.2312853043993651 valid 0.19609709084033966
LOSS train 0.2312853043993651 valid 0.19874794657031694
LOSS train 0.2312853043993651 valid 0.19828536074895126
LOSS train 0.2312853043993651 valid 0.19690118730068207
LOSS train 0.2312853043993651 valid 0.19684505263964336
LOSS train 0.2312853043993651 valid 0.20058461092412472
LOSS train 0.2312853043993651 valid 0.20063423584489262
LOSS train 0.2312853043993651 valid 0.20031573871771494
LOSS train 0.2312853043993651 valid 0.20340391366105331
LOSS train 0.2312853043993651 valid 0.20276810675859452
LOSS train 0.2312853043993651 valid 0.2054310937722524
LOSS train 0.2312853043993651 valid 0.20486042851751501
LOSS train 0.2312853043993651 valid 0.20250463485717773
LOSS train 0.2312853043993651 valid 0.20256407683094343
LOSS train 0.2312853043993651 valid 0.2025520098209381
LOSS train 0.2312853043993651 valid 0.20184559661608475
LOSS train 0.2312853043993651 valid 0.20147823128435347
LOSS train 0.2312853043993651 valid 0.20233204481857164
LOSS train 0.2312853043993651 valid 0.20135898477044598
LOSS train 0.2312853043993651 valid 0.20117825716733934
LOSS train 0.2312853043993651 valid 0.2013714611530304
LOSS train 0.2312853043993651 valid 0.20118580618873239
LOSS train 0.2312853043993651 valid 0.19974351561430728
LOSS train 0.2312853043993651 valid 0.19921269399278305
LOSS train 0.2312853043993651 valid 0.19991732495171682
LOSS train 0.2312853043993651 valid 0.20070179965760973
LOSS train 0.2312853043993651 valid 0.20120833773870725
LOSS train 0.2312853043993651 valid 0.20120779503332942
LOSS train 0.2312853043993651 valid 0.20228907809807703
LOSS train 0.2312853043993651 valid 0.202588165178895
LOSS train 0.2312853043993651 valid 0.2024610180680345
LOSS train 0.2312853043993651 valid 0.2036717782417933
LOSS train 0.2312853043993651 valid 0.20429398084795752
LOSS train 0.2312853043993651 valid 0.2040122083642266
LOSS train 0.2312853043993651 valid 0.20350956718126934
LOSS train 0.2312853043993651 valid 0.20297146880108377
LOSS train 0.2312853043993651 valid 0.20242292449829427
LOSS train 0.2312853043993651 valid 0.20388578437268734
LOSS train 0.2312853043993651 valid 0.2031017383750604
LOSS train 0.2312853043993651 valid 0.20379512697458269
LOSS train 0.2312853043993651 valid 0.20346001287301382
LOSS train 0.2312853043993651 valid 0.20321674473010576
LOSS train 0.2312853043993651 valid 0.20438316127039352
LOSS train 0.2312853043993651 valid 0.2044352643467762
LOSS train 0.2312853043993651 valid 0.204074551300569
LOSS train 0.2312853043993651 valid 0.20436893243874824
LOSS train 0.2312853043993651 valid 0.20384163396400318
LOSS train 0.2312853043993651 valid 0.20426510120260305
LOSS train 0.2312853043993651 valid 0.20380109270750466
LOSS train 0.2312853043993651 valid 0.2036823369562626
LOSS train 0.2312853043993651 valid 0.20364573671192418
LOSS train 0.2312853043993651 valid 0.20353380831018572
LOSS train 0.2312853043993651 valid 0.20320778754022387
LOSS train 0.2312853043993651 valid 0.2037442964501679
LOSS train 0.2312853043993651 valid 0.20259295610281136
LOSS train 0.2312853043993651 valid 0.2023677293098334
LOSS train 0.2312853043993651 valid 0.20295552041993212
LOSS train 0.2312853043993651 valid 0.20233803330098882
LOSS train 0.2312853043993651 valid 0.2030606027962505
LOSS train 0.2312853043993651 valid 0.2036028204219682
LOSS train 0.2312853043993651 valid 0.20386316280969433
LOSS train 0.2312853043993651 valid 0.2045114773015181
LOSS train 0.2312853043993651 valid 0.20526308270349894
LOSS train 0.2312853043993651 valid 0.20500056042864517
LOSS train 0.2312853043993651 valid 0.20470532953739165
LOSS train 0.2312853043993651 valid 0.2050108580212844
LOSS train 0.2312853043993651 valid 0.20484185489741238
LOSS train 0.2312853043993651 valid 0.2047915687927833
LOSS train 0.2312853043993651 valid 0.2046791147204894
LOSS train 0.2312853043993651 valid 0.20440035741776227
LOSS train 0.2312853043993651 valid 0.20464539785444
LOSS train 0.2312853043993651 valid 0.20447688149969753
LOSS train 0.2312853043993651 valid 0.20470474660396576
LOSS train 0.2312853043993651 valid 0.20431879198267347
LOSS train 0.2312853043993651 valid 0.20501997751348158
LOSS train 0.2312853043993651 valid 0.20494872920734938
LOSS train 0.2312853043993651 valid 0.20466148545002116
LOSS train 0.2312853043993651 valid 0.20498024807734924
LOSS train 0.2312853043993651 valid 0.2052840811482976
LOSS train 0.2312853043993651 valid 0.20586206217606862
LOSS train 0.2312853043993651 valid 0.205804488324857
LOSS train 0.2312853043993651 valid 0.20590709749123323
LOSS train 0.2312853043993651 valid 0.20563867704201771
LOSS train 0.2312853043993651 valid 0.20593216936004924
LOSS train 0.2312853043993651 valid 0.20614829565349377
LOSS train 0.2312853043993651 valid 0.20637931115925312
LOSS train 0.2312853043993651 valid 0.2065117276206459
LOSS train 0.2312853043993651 valid 0.20702328426497324
LOSS train 0.2312853043993651 valid 0.20723330673545298
LOSS train 0.2312853043993651 valid 0.20731885969638825
LOSS train 0.2312853043993651 valid 0.20736186442398788
LOSS train 0.2312853043993651 valid 0.20781762649615607
LOSS train 0.2312853043993651 valid 0.20759840570028545
LOSS train 0.2312853043993651 valid 0.2074235723568843
LOSS train 0.2312853043993651 valid 0.20769138705162776
LOSS train 0.2312853043993651 valid 0.2078782726290091
LOSS train 0.2312853043993651 valid 0.20758832796154736
LOSS train 0.2312853043993651 valid 0.20774909509000955
LOSS train 0.2312853043993651 valid 0.20738466084003448
LOSS train 0.2312853043993651 valid 0.2073024336587299
LOSS train 0.2312853043993651 valid 0.20730153853828842
LOSS train 0.2312853043993651 valid 0.20754660639379705
LOSS train 0.2312853043993651 valid 0.20727973978603836
LOSS train 0.2312853043993651 valid 0.2073546126484871
LOSS train 0.2312853043993651 valid 0.2081938905560452
LOSS train 0.2312853043993651 valid 0.20790869935319342
LOSS train 0.2312853043993651 valid 0.2085169295215199
LOSS train 0.2312853043993651 valid 0.20832947906801255
LOSS train 0.2312853043993651 valid 0.20809741631275466
LOSS train 0.2312853043993651 valid 0.2076875690370798
LOSS train 0.2312853043993651 valid 0.20739870536918484
LOSS train 0.2312853043993651 valid 0.20763187843268036
LOSS train 0.2312853043993651 valid 0.2076487598138127
LOSS train 0.2312853043993651 valid 0.2078829831894367
LOSS train 0.2312853043993651 valid 0.20777866184711458
LOSS train 0.2312853043993651 valid 0.2079843914224988
LOSS train 0.2312853043993651 valid 0.2078733620211834
LOSS train 0.2312853043993651 valid 0.20780533063225448
LOSS train 0.2312853043993651 valid 0.20751136148622792
LOSS train 0.2312853043993651 valid 0.20726871524865811
LOSS train 0.2312853043993651 valid 0.20734083959619506
LOSS train 0.2312853043993651 valid 0.2072732997211543
LOSS train 0.2312853043993651 valid 0.20725815950479723
LOSS train 0.2312853043993651 valid 0.2074504793802304
LOSS train 0.2312853043993651 valid 0.20755308888576648
LOSS train 0.2312853043993651 valid 0.20749924496254502
LOSS train 0.2312853043993651 valid 0.20744378157775767
LOSS train 0.2312853043993651 valid 0.20739874762037527
LOSS train 0.2312853043993651 valid 0.20715685799825106
LOSS train 0.2312853043993651 valid 0.20705960893205233
LOSS train 0.2312853043993651 valid 0.2070588744278495
LOSS train 0.2312853043993651 valid 0.2073458603150408
LOSS train 0.2312853043993651 valid 0.20744940721905314
LOSS train 0.2312853043993651 valid 0.20754864584240648
LOSS train 0.2312853043993651 valid 0.2074971786860762
LOSS train 0.2312853043993651 valid 0.20743134852549802
LOSS train 0.2312853043993651 valid 0.20740680964220137
LOSS train 0.2312853043993651 valid 0.2073423309704742
LOSS train 0.2312853043993651 valid 0.20742623777997574
LOSS train 0.2312853043993651 valid 0.20745245218276978
LOSS train 0.2312853043993651 valid 0.20734105313455822
LOSS train 0.2312853043993651 valid 0.20729548709565088
LOSS train 0.2312853043993651 valid 0.20702373728253484
LOSS train 0.2312853043993651 valid 0.20720510726625269
LOSS train 0.2312853043993651 valid 0.20725859672792496
LOSS train 0.2312853043993651 valid 0.20717504649208143
LOSS train 0.2312853043993651 valid 0.207104564187633
LOSS train 0.2312853043993651 valid 0.2072979459845567
LOSS train 0.2312853043993651 valid 0.2073758769335237
LOSS train 0.2312853043993651 valid 0.20756065677851437
LOSS train 0.2312853043993651 valid 0.20750272699764796
LOSS train 0.2312853043993651 valid 0.20759795403774875
LOSS train 0.2312853043993651 valid 0.20741988517389706
LOSS train 0.2312853043993651 valid 0.207516361027956
LOSS train 0.2312853043993651 valid 0.20764257664030247
LOSS train 0.2312853043993651 valid 0.20757221466446496
LOSS train 0.2312853043993651 valid 0.20751699073585922
LOSS train 0.2312853043993651 valid 0.20733217309628213
LOSS train 0.2312853043993651 valid 0.20718477322505072
LOSS train 0.2312853043993651 valid 0.20729194844470306
LOSS train 0.2312853043993651 valid 0.20732279217731187
LOSS train 0.2312853043993651 valid 0.2073794604214125
LOSS train 0.2312853043993651 valid 0.20722579404797858
LOSS train 0.2312853043993651 valid 0.20762465740072317
LOSS train 0.2312853043993651 valid 0.20760469862392972
LOSS train 0.2312853043993651 valid 0.207566566595977
LOSS train 0.2312853043993651 valid 0.20760910263506033
LOSS train 0.2312853043993651 valid 0.20770510118663982
LOSS train 0.2312853043993651 valid 0.20787471516172312
LOSS train 0.2312853043993651 valid 0.20775388818648127
LOSS train 0.2312853043993651 valid 0.2077383206364858
LOSS train 0.2312853043993651 valid 0.20752411024583564
LOSS train 0.2312853043993651 valid 0.20764691614713826
LOSS train 0.2312853043993651 valid 0.2075621373951435
LOSS train 0.2312853043993651 valid 0.20745488509938523
LOSS train 0.2312853043993651 valid 0.20763921473295457
LOSS train 0.2312853043993651 valid 0.2075787875104078
LOSS train 0.2312853043993651 valid 0.2076166311001524
LOSS train 0.2312853043993651 valid 0.20740067493663264
LOSS train 0.2312853043993651 valid 0.20737638026475907
LOSS train 0.2312853043993651 valid 0.2075033961010229
LOSS train 0.2312853043993651 valid 0.20748498927180967
LOSS train 0.2312853043993651 valid 0.20726459639368897
LOSS train 0.2312853043993651 valid 0.20711737310456246
LOSS train 0.2312853043993651 valid 0.20702152244555644
LOSS train 0.2312853043993651 valid 0.20706819294362652
LOSS train 0.2312853043993651 valid 0.2070786579308776
LOSS train 0.2312853043993651 valid 0.20726774790973373
LOSS train 0.2312853043993651 valid 0.20726171472863336
LOSS train 0.2312853043993651 valid 0.20736066222190858
LOSS train 0.2312853043993651 valid 0.20716700966085366
LOSS train 0.2312853043993651 valid 0.20713061020515933
LOSS train 0.2312853043993651 valid 0.2072177348759374
LOSS train 0.2312853043993651 valid 0.2069894161586668
LOSS train 0.2312853043993651 valid 0.20688047990566347
LOSS train 0.2312853043993651 valid 0.2069199593032448
LOSS train 0.2312853043993651 valid 0.20684836876853077
LOSS train 0.2312853043993651 valid 0.2067027763965038
LOSS train 0.2312853043993651 valid 0.20674154376299186
LOSS train 0.2312853043993651 valid 0.20684995984747295
LOSS train 0.2312853043993651 valid 0.2070774186702701
LOSS train 0.2312853043993651 valid 0.20689932306138975
LOSS train 0.2312853043993651 valid 0.20689996328432236
LOSS train 0.2312853043993651 valid 0.20688179490443703
LOSS train 0.2312853043993651 valid 0.20670013115849606
LOSS train 0.2312853043993651 valid 0.20652605731178214
LOSS train 0.2312853043993651 valid 0.20631318391742795
LOSS train 0.2312853043993651 valid 0.2064082898678036
LOSS train 0.2312853043993651 valid 0.206443523367246
LOSS train 0.2312853043993651 valid 0.2065618103200739
LOSS train 0.2312853043993651 valid 0.20656436830084787
LOSS train 0.2312853043993651 valid 0.20670413011097694
LOSS train 0.2312853043993651 valid 0.20687339812383523
LOSS train 0.2312853043993651 valid 0.20674755477479526
LOSS train 0.2312853043993651 valid 0.2066322222020891
LOSS train 0.2312853043993651 valid 0.20651383935350232
LOSS train 0.2312853043993651 valid 0.20655336884149897
LOSS train 0.2312853043993651 valid 0.206503809740146
LOSS train 0.2312853043993651 valid 0.20634998264010818
LOSS train 0.2312853043993651 valid 0.20635814323373464
LOSS train 0.2312853043993651 valid 0.20634762588001432
LOSS train 0.2312853043993651 valid 0.20645246975894632
LOSS train 0.2312853043993651 valid 0.206792268886075
LOSS train 0.2312853043993651 valid 0.20676029556327397
LOSS train 0.2312853043993651 valid 0.2068262239719959
LOSS train 0.2312853043993651 valid 0.20664708928789122
LOSS train 0.2312853043993651 valid 0.2066951791445414
LOSS train 0.2312853043993651 valid 0.20668456684641479
LOSS train 0.2312853043993651 valid 0.2066524124170447
LOSS train 0.2312853043993651 valid 0.20627413857728244
LOSS train 0.2312853043993651 valid 0.20612531634534542
LOSS train 0.2312853043993651 valid 0.20612167333029519
LOSS train 0.2312853043993651 valid 0.20594198272061445
LOSS train 0.2312853043993651 valid 0.20594875813751926
LOSS train 0.2312853043993651 valid 0.20609962605700202
LOSS train 0.2312853043993651 valid 0.20623668331683168
LOSS train 0.2312853043993651 valid 0.2062809307444916
LOSS train 0.2312853043993651 valid 0.20627255359243962
LOSS train 0.2312853043993651 valid 0.20603914451168245
LOSS train 0.2312853043993651 valid 0.20611650931835174
LOSS train 0.2312853043993651 valid 0.20613990373820423
LOSS train 0.2312853043993651 valid 0.2060192015672487
LOSS train 0.2312853043993651 valid 0.20581104279506818
LOSS train 0.2312853043993651 valid 0.20598067841895923
LOSS train 0.2312853043993651 valid 0.20589869928126242
LOSS train 0.2312853043993651 valid 0.2056523217470385
LOSS train 0.2312853043993651 valid 0.20559696667621108
LOSS train 0.2312853043993651 valid 0.2057767203727434
LOSS train 0.2312853043993651 valid 0.205671386541547
LOSS train 0.2312853043993651 valid 0.20552405800956947
LOSS train 0.2312853043993651 valid 0.20567551923209224
LOSS train 0.2312853043993651 valid 0.20594613237007883
LOSS train 0.2312853043993651 valid 0.2057340794983019
LOSS train 0.2312853043993651 valid 0.20582886621581786
LOSS train 0.2312853043993651 valid 0.20577117766974107
LOSS train 0.2312853043993651 valid 0.2058135165195716
LOSS train 0.2312853043993651 valid 0.20569743528571469
LOSS train 0.2312853043993651 valid 0.20578309860247285
LOSS train 0.2312853043993651 valid 0.20592764668083546
LOSS train 0.2312853043993651 valid 0.20582446444917607
LOSS train 0.2312853043993651 valid 0.2058379458765262
LOSS train 0.2312853043993651 valid 0.2057158430928693
LOSS train 0.2312853043993651 valid 0.20560686077390397
LOSS train 0.2312853043993651 valid 0.205505856414781
LOSS train 0.2312853043993651 valid 0.2055240534110503
LOSS train 0.2312853043993651 valid 0.2055426291257575
LOSS train 0.2312853043993651 valid 0.20560420608477473
LOSS train 0.2312853043993651 valid 0.20564691808155114
LOSS train 0.2312853043993651 valid 0.20558916261973775
LOSS train 0.2312853043993651 valid 0.2054598874279431
LOSS train 0.2312853043993651 valid 0.20528384136347583
LOSS train 0.2312853043993651 valid 0.20523126238415426
LOSS train 0.2312853043993651 valid 0.20524350877574812
LOSS train 0.2312853043993651 valid 0.2051706588499143
LOSS train 0.2312853043993651 valid 0.20528091380470678
LOSS train 0.2312853043993651 valid 0.20523997026306767
LOSS train 0.2312853043993651 valid 0.2051874873218636
LOSS train 0.2312853043993651 valid 0.20513756961251298
LOSS train 0.2312853043993651 valid 0.2050363432783569
LOSS train 0.2312853043993651 valid 0.2049176382093594
LOSS train 0.2312853043993651 valid 0.20493098653059236
LOSS train 0.2312853043993651 valid 0.20489214830202598
LOSS train 0.2312853043993651 valid 0.20488277509961111
LOSS train 0.2312853043993651 valid 0.20485392810941552
LOSS train 0.2312853043993651 valid 0.20484129791542635
LOSS train 0.2312853043993651 valid 0.20482403422529633
LOSS train 0.2312853043993651 valid 0.20473488041447468
LOSS train 0.2312853043993651 valid 0.2048255827702932
LOSS train 0.2312853043993651 valid 0.20486472307838324
LOSS train 0.2312853043993651 valid 0.20486757134397823
LOSS train 0.2312853043993651 valid 0.2047515076359245
LOSS train 0.2312853043993651 valid 0.20473353007179224
LOSS train 0.2312853043993651 valid 0.2046154034216412
LOSS train 0.2312853043993651 valid 0.20463838707655668
LOSS train 0.2312853043993651 valid 0.20455476876165046
LOSS train 0.2312853043993651 valid 0.20482027043704115
LOSS train 0.2312853043993651 valid 0.20479434117043835
LOSS train 0.2312853043993651 valid 0.20484089038588785
LOSS train 0.2312853043993651 valid 0.20484276004979526
LOSS train 0.2312853043993651 valid 0.20482630931561993
LOSS train 0.2312853043993651 valid 0.20485090303842662
LOSS train 0.2312853043993651 valid 0.20480608309690768
LOSS train 0.2312853043993651 valid 0.20492890710457445
LOSS train 0.2312853043993651 valid 0.2049048889404649
LOSS train 0.2312853043993651 valid 0.20483267912788997
LOSS train 0.2312853043993651 valid 0.2048067066677009
LOSS train 0.2312853043993651 valid 0.2048328869068284
LOSS train 0.2312853043993651 valid 0.2048027381863234
LOSS train 0.2312853043993651 valid 0.20474996761094813
LOSS train 0.2312853043993651 valid 0.20468361536040902
LOSS train 0.2312853043993651 valid 0.20481491864098939
LOSS train 0.2312853043993651 valid 0.2048267813295311
LOSS train 0.2312853043993651 valid 0.20469523546496413
LOSS train 0.2312853043993651 valid 0.2046837502239663
LOSS train 0.2312853043993651 valid 0.20452262878417968
LOSS train 0.2312853043993651 valid 0.20456734701899662
LOSS train 0.2312853043993651 valid 0.2044841911450074
LOSS train 0.2312853043993651 valid 0.20445613417683578
LOSS train 0.2312853043993651 valid 0.20444188450426315
LOSS train 0.2312853043993651 valid 0.2044559434056282
LOSS train 0.2312853043993651 valid 0.20440107147319078
LOSS train 0.2312853043993651 valid 0.20440252578581672
LOSS train 0.2312853043993651 valid 0.2045630866790319
LOSS train 0.2312853043993651 valid 0.20448261030955228
LOSS train 0.2312853043993651 valid 0.20438767520349418
LOSS train 0.2312853043993651 valid 0.2042895925807811
LOSS train 0.2312853043993651 valid 0.20442621233024655
LOSS train 0.2312853043993651 valid 0.20441819238063147
LOSS train 0.2312853043993651 valid 0.20432193128936058
LOSS train 0.2312853043993651 valid 0.2044594262014417
LOSS train 0.2312853043993651 valid 0.20436962958305113
LOSS train 0.2312853043993651 valid 0.20436649512477786
LOSS train 0.2312853043993651 valid 0.2042725848213229
LOSS train 0.2312853043993651 valid 0.20437351380323254
LOSS train 0.2312853043993651 valid 0.20448795055997546
LOSS train 0.2312853043993651 valid 0.20444843569242885
LOSS train 0.2312853043993651 valid 0.20435695972841136
LOSS train 0.2312853043993651 valid 0.20446558433702622
LOSS train 0.2312853043993651 valid 0.20441144055828325
LOSS train 0.2312853043993651 valid 0.20443947055510112
LOSS train 0.2312853043993651 valid 0.2045103739894014
LOSS train 0.2312853043993651 valid 0.20455203124914656
LOSS train 0.2312853043993651 valid 0.20455840736543152
LOSS train 0.2312853043993651 valid 0.2045528929533258
LOSS train 0.2312853043993651 valid 0.20443003798874332
LOSS train 0.2312853043993651 valid 0.2044778305911616
LOSS train 0.2312853043993651 valid 0.20454406838457123
LOSS train 0.2312853043993651 valid 0.20447556849298532
LOSS train 0.2312853043993651 valid 0.20440287817487476
LOSS train 0.2312853043993651 valid 0.20456251096394326
LOSS train 0.2312853043993651 valid 0.2044653466533756
LOSS train 0.2312853043993651 valid 0.20449419772427385
LOSS train 0.2312853043993651 valid 0.20454663144983865
LOSS train 0.2312853043993651 valid 0.2044627427809186
LOSS train 0.2312853043993651 valid 0.20455929951308524
LOSS train 0.2312853043993651 valid 0.20455456012096562
LOSS train 0.2312853043993651 valid 0.20448529890837397
LOSS train 0.2312853043993651 valid 0.20432655703600333
LOSS train 0.2312853043993651 valid 0.20427739495022834
EPOCH 23:
  batch 1 loss: 0.2259380966424942
  batch 2 loss: 0.24567324668169022
  batch 3 loss: 0.2388643721739451
  batch 4 loss: 0.242761068046093
  batch 5 loss: 0.24475641846656798
  batch 6 loss: 0.2397020955880483
  batch 7 loss: 0.23889656790665217
  batch 8 loss: 0.23583960346877575
  batch 9 loss: 0.23842810756630367
  batch 10 loss: 0.2346788987517357
  batch 11 loss: 0.23468658869916742
  batch 12 loss: 0.2338411125044028
  batch 13 loss: 0.2299593434883998
  batch 14 loss: 0.2295646710055215
  batch 15 loss: 0.23026000758012136
  batch 16 loss: 0.22975054383277893
  batch 17 loss: 0.23137912680121028
  batch 18 loss: 0.23021827224228117
  batch 19 loss: 0.2294438285262961
  batch 20 loss: 0.22718838527798652
  batch 21 loss: 0.23103582646165574
  batch 22 loss: 0.23124499348076907
  batch 23 loss: 0.23155088398767554
  batch 24 loss: 0.23162710418303809
  batch 25 loss: 0.23288408279418946
  batch 26 loss: 0.23225095524237707
  batch 27 loss: 0.2318225789953161
  batch 28 loss: 0.23329440291438783
  batch 29 loss: 0.2326275315778009
  batch 30 loss: 0.2339283913373947
  batch 31 loss: 0.23384219744513113
  batch 32 loss: 0.2347886930219829
  batch 33 loss: 0.2367152829061855
  batch 34 loss: 0.23664984282325296
  batch 35 loss: 0.23843855602400643
  batch 36 loss: 0.23900174515114891
  batch 37 loss: 0.23950416735700658
  batch 38 loss: 0.2389879767832003
  batch 39 loss: 0.23903508904652718
  batch 40 loss: 0.23859986774623393
  batch 41 loss: 0.23860289229125511
  batch 42 loss: 0.2380529014127595
  batch 43 loss: 0.23818842546884403
  batch 44 loss: 0.2386101856827736
  batch 45 loss: 0.23810502853658463
  batch 46 loss: 0.2365097976897074
  batch 47 loss: 0.23557185587730814
  batch 48 loss: 0.2351860866571466
  batch 49 loss: 0.23566679778147717
  batch 50 loss: 0.23683057397603988
  batch 51 loss: 0.23618834334261277
  batch 52 loss: 0.2364047714150869
  batch 53 loss: 0.23611207446962032
  batch 54 loss: 0.23582829535007477
  batch 55 loss: 0.23595571707595478
  batch 56 loss: 0.23645395865397795
  batch 57 loss: 0.2359816816292311
  batch 58 loss: 0.23604030727312483
  batch 59 loss: 0.23627144300331504
  batch 60 loss: 0.2369174803296725
  batch 61 loss: 0.23721873564798324
  batch 62 loss: 0.23772178542229436
  batch 63 loss: 0.23693720310453384
  batch 64 loss: 0.23634415050037205
  batch 65 loss: 0.23592963975209455
  batch 66 loss: 0.2355647204500256
  batch 67 loss: 0.2352791328483553
  batch 68 loss: 0.2352446295759257
  batch 69 loss: 0.23560358951057214
  batch 70 loss: 0.2356065232838903
  batch 71 loss: 0.23585161965497783
  batch 72 loss: 0.23592984448704454
  batch 73 loss: 0.2359474197234193
  batch 74 loss: 0.23597584161404017
  batch 75 loss: 0.23608188331127167
  batch 76 loss: 0.23648028761932724
  batch 77 loss: 0.2364308154815203
  batch 78 loss: 0.2364965598934736
  batch 79 loss: 0.23651103750814365
  batch 80 loss: 0.2365459343418479
  batch 81 loss: 0.23663943362088852
  batch 82 loss: 0.2367793239834832
  batch 83 loss: 0.23624578944171767
  batch 84 loss: 0.23575067857191676
  batch 85 loss: 0.23636065879288842
  batch 86 loss: 0.23605532892221628
  batch 87 loss: 0.23567032865409193
  batch 88 loss: 0.23537057858299126
  batch 89 loss: 0.234846363911468
  batch 90 loss: 0.23511127432187398
  batch 91 loss: 0.2347758615409935
  batch 92 loss: 0.23456554082424744
  batch 93 loss: 0.23408814864132993
  batch 94 loss: 0.23392547730435717
  batch 95 loss: 0.23389652857654972
  batch 96 loss: 0.2337156025071939
  batch 97 loss: 0.23428050113707474
  batch 98 loss: 0.23417442748133016
  batch 99 loss: 0.23385261450753067
  batch 100 loss: 0.2340970455110073
  batch 101 loss: 0.23410743563482078
  batch 102 loss: 0.2343625356169308
  batch 103 loss: 0.23412159432485266
  batch 104 loss: 0.23423699074639723
  batch 105 loss: 0.23411444638456616
  batch 106 loss: 0.23403438307204336
  batch 107 loss: 0.23399899913885883
  batch 108 loss: 0.23364297024629735
  batch 109 loss: 0.23389687505337076
  batch 110 loss: 0.23393677852370523
  batch 111 loss: 0.23415809842917296
  batch 112 loss: 0.23408404777624778
  batch 113 loss: 0.23397825275902198
  batch 114 loss: 0.2340275250505983
  batch 115 loss: 0.23364279296087181
  batch 116 loss: 0.23351591208885455
  batch 117 loss: 0.23340904432484227
  batch 118 loss: 0.23353218981775187
  batch 119 loss: 0.2336703185774699
  batch 120 loss: 0.23365513160824775
  batch 121 loss: 0.2336062556209643
  batch 122 loss: 0.23346985229214684
  batch 123 loss: 0.23321038036327052
  batch 124 loss: 0.23313563593452977
  batch 125 loss: 0.23283664679527283
  batch 126 loss: 0.23283150886732434
  batch 127 loss: 0.23303526686871145
  batch 128 loss: 0.23304391978308558
  batch 129 loss: 0.2333050464936929
  batch 130 loss: 0.23340216084168508
  batch 131 loss: 0.23341800886711092
  batch 132 loss: 0.23350064738681822
  batch 133 loss: 0.23340198229578205
  batch 134 loss: 0.23336721278393446
  batch 135 loss: 0.2330916221495028
  batch 136 loss: 0.23343716627534697
  batch 137 loss: 0.2337171498441348
  batch 138 loss: 0.2337088602176611
  batch 139 loss: 0.23355795518099834
  batch 140 loss: 0.2338733407003539
  batch 141 loss: 0.23369477826652799
  batch 142 loss: 0.23339090191982162
  batch 143 loss: 0.23351209105311574
  batch 144 loss: 0.23352947872545984
  batch 145 loss: 0.23349039801235857
  batch 146 loss: 0.23355157214076552
  batch 147 loss: 0.2335184617918365
  batch 148 loss: 0.23348083790089633
  batch 149 loss: 0.23316517752288973
  batch 150 loss: 0.23313332627216976
  batch 151 loss: 0.23276504558443234
  batch 152 loss: 0.23250280546122476
  batch 153 loss: 0.23241338348077015
  batch 154 loss: 0.23214385068261778
  batch 155 loss: 0.2320529490709305
  batch 156 loss: 0.23201067000627518
  batch 157 loss: 0.23191931511566138
  batch 158 loss: 0.23190657483248772
  batch 159 loss: 0.23206367479555262
  batch 160 loss: 0.23198522636666893
  batch 161 loss: 0.2318179718455913
  batch 162 loss: 0.2318570151189227
  batch 163 loss: 0.23162750241580915
  batch 164 loss: 0.23169753664150472
  batch 165 loss: 0.2319348541173068
  batch 166 loss: 0.23223301014268255
  batch 167 loss: 0.23217212734465115
  batch 168 loss: 0.23189761400932357
  batch 169 loss: 0.2319224562341645
  batch 170 loss: 0.2319998700829113
  batch 171 loss: 0.23219267544690628
  batch 172 loss: 0.23209381562679313
  batch 173 loss: 0.232050115748637
  batch 174 loss: 0.23216957616052408
  batch 175 loss: 0.23224309640271323
  batch 176 loss: 0.23249870496378702
  batch 177 loss: 0.23269171221444837
  batch 178 loss: 0.23269314218438072
  batch 179 loss: 0.2326706444750951
  batch 180 loss: 0.23270610877209238
  batch 181 loss: 0.2327071401954356
  batch 182 loss: 0.23264640258563743
  batch 183 loss: 0.23268835718840197
  batch 184 loss: 0.23277513674743797
  batch 185 loss: 0.23252295690613825
  batch 186 loss: 0.23246896058641453
  batch 187 loss: 0.23260782013602435
  batch 188 loss: 0.2325204815319244
  batch 189 loss: 0.232541500733643
  batch 190 loss: 0.2323877084412073
  batch 191 loss: 0.2323531024743125
  batch 192 loss: 0.23214785289019346
  batch 193 loss: 0.232194004126781
  batch 194 loss: 0.2322479999556984
  batch 195 loss: 0.2324388134173858
  batch 196 loss: 0.2323013207285988
  batch 197 loss: 0.23232839063641988
  batch 198 loss: 0.23229571379194355
  batch 199 loss: 0.23243087619992356
  batch 200 loss: 0.23244274660944939
  batch 201 loss: 0.23231787460656902
  batch 202 loss: 0.23247610482544
  batch 203 loss: 0.23258403068399194
  batch 204 loss: 0.23267056756452018
  batch 205 loss: 0.232557379908678
  batch 206 loss: 0.23268960951601417
  batch 207 loss: 0.23283128346797924
  batch 208 loss: 0.23265769261007124
  batch 209 loss: 0.23267492887220884
  batch 210 loss: 0.23276926839635487
  batch 211 loss: 0.23280940166016892
  batch 212 loss: 0.2328903022099216
  batch 213 loss: 0.23284637445295361
  batch 214 loss: 0.23288522514505922
  batch 215 loss: 0.23286809969780056
  batch 216 loss: 0.23284364884926212
  batch 217 loss: 0.2327878419567363
  batch 218 loss: 0.2325845205592453
  batch 219 loss: 0.23223710856209062
  batch 220 loss: 0.23238223215395754
  batch 221 loss: 0.2323395223504278
  batch 222 loss: 0.23227557452680828
  batch 223 loss: 0.23215185787378406
  batch 224 loss: 0.23204814636015467
  batch 225 loss: 0.23203441553645665
  batch 226 loss: 0.2320544689366248
  batch 227 loss: 0.2320302441256687
  batch 228 loss: 0.23201637224931465
  batch 229 loss: 0.23205958664677548
  batch 230 loss: 0.23212648507045663
  batch 231 loss: 0.23227859104608561
  batch 232 loss: 0.23233904387673426
  batch 233 loss: 0.2324528410838909
  batch 234 loss: 0.23241277892365414
  batch 235 loss: 0.23232474885088333
  batch 236 loss: 0.23226885964809837
  batch 237 loss: 0.23211214587658266
  batch 238 loss: 0.2320400263581957
  batch 239 loss: 0.23199916602926773
  batch 240 loss: 0.23206442650407552
  batch 241 loss: 0.23196045615366387
  batch 242 loss: 0.23192327623524941
  batch 243 loss: 0.23185418092419582
  batch 244 loss: 0.2317582629621029
  batch 245 loss: 0.23167960832313617
  batch 246 loss: 0.23161786219211128
  batch 247 loss: 0.23169004090643122
  batch 248 loss: 0.23176753863451943
  batch 249 loss: 0.2318711016910622
  batch 250 loss: 0.23184113585948943
  batch 251 loss: 0.2317250902315535
  batch 252 loss: 0.2316554304626253
  batch 253 loss: 0.23160730938430832
  batch 254 loss: 0.23152717238101433
  batch 255 loss: 0.2314013232203091
  batch 256 loss: 0.23126494867028669
  batch 257 loss: 0.2312960093587289
  batch 258 loss: 0.2312642090426859
  batch 259 loss: 0.23119447768655063
  batch 260 loss: 0.2311934053324736
  batch 261 loss: 0.2311227226851087
  batch 262 loss: 0.2310612382552096
  batch 263 loss: 0.23091896031292672
  batch 264 loss: 0.23099973086606373
  batch 265 loss: 0.23084764036367525
  batch 266 loss: 0.2306662901563752
  batch 267 loss: 0.23045715840345019
  batch 268 loss: 0.23035692456942886
  batch 269 loss: 0.2304145637830394
  batch 270 loss: 0.2303072006062225
  batch 271 loss: 0.23027363095116352
  batch 272 loss: 0.2302147114978117
  batch 273 loss: 0.23008276784158016
  batch 274 loss: 0.2299775751511546
  batch 275 loss: 0.22991293468258597
  batch 276 loss: 0.22986744570991266
  batch 277 loss: 0.2299476166924845
  batch 278 loss: 0.2298286536293064
  batch 279 loss: 0.22981708713116183
  batch 280 loss: 0.22981571251792568
  batch 281 loss: 0.22973170738627477
  batch 282 loss: 0.2296042338753423
  batch 283 loss: 0.22956718688718844
  batch 284 loss: 0.22954258815923206
  batch 285 loss: 0.22954259352725848
  batch 286 loss: 0.22949454537430009
  batch 287 loss: 0.22936044560699928
  batch 288 loss: 0.22942775358549422
  batch 289 loss: 0.22933742017275735
  batch 290 loss: 0.22930755445669437
  batch 291 loss: 0.229334038864706
  batch 292 loss: 0.22929721121510413
  batch 293 loss: 0.2292914449049751
  batch 294 loss: 0.22901757928181668
  batch 295 loss: 0.22901670836796195
  batch 296 loss: 0.22902148906644937
  batch 297 loss: 0.2289394387091049
  batch 298 loss: 0.22883638919599905
  batch 299 loss: 0.22888533536805755
  batch 300 loss: 0.22878653665383658
  batch 301 loss: 0.22872530562537058
  batch 302 loss: 0.22892441852203269
  batch 303 loss: 0.22891682297876564
  batch 304 loss: 0.2289004858191076
  batch 305 loss: 0.22885185470346545
  batch 306 loss: 0.22895916088733798
  batch 307 loss: 0.22896475221124457
  batch 308 loss: 0.22901408166273848
  batch 309 loss: 0.2290271976811986
  batch 310 loss: 0.22912641686777915
  batch 311 loss: 0.22906906495523607
  batch 312 loss: 0.2290793795329638
  batch 313 loss: 0.22906250404283263
  batch 314 loss: 0.2289971694065507
  batch 315 loss: 0.2289559417300754
  batch 316 loss: 0.22900457387860818
  batch 317 loss: 0.22905575989360688
  batch 318 loss: 0.22889425343125122
  batch 319 loss: 0.2288224547167183
  batch 320 loss: 0.22880217540077866
  batch 321 loss: 0.22888959619300758
  batch 322 loss: 0.22879471676134916
  batch 323 loss: 0.22870253718489833
  batch 324 loss: 0.22875871352943372
  batch 325 loss: 0.22891253407184894
  batch 326 loss: 0.22906681913539675
  batch 327 loss: 0.22906016387523861
  batch 328 loss: 0.22901275022545967
  batch 329 loss: 0.22903818834154077
  batch 330 loss: 0.22904058243289138
  batch 331 loss: 0.22897268503094007
  batch 332 loss: 0.22906584158001175
  batch 333 loss: 0.2290924455042
  batch 334 loss: 0.22937106073438052
  batch 335 loss: 0.2294608967072928
  batch 336 loss: 0.22957136442086526
  batch 337 loss: 0.22968044704605634
  batch 338 loss: 0.2296488098579751
  batch 339 loss: 0.22967401384252362
  batch 340 loss: 0.22957368905053419
  batch 341 loss: 0.2295607646539414
  batch 342 loss: 0.22948816213865728
  batch 343 loss: 0.22955390763178513
  batch 344 loss: 0.22957324141333268
  batch 345 loss: 0.22966684137565502
  batch 346 loss: 0.22973227294194215
  batch 347 loss: 0.2296303456987359
  batch 348 loss: 0.22960466227826032
  batch 349 loss: 0.22961718732101527
  batch 350 loss: 0.2298231795004436
  batch 351 loss: 0.22986668980868794
  batch 352 loss: 0.22979116985913028
  batch 353 loss: 0.22974700752972882
  batch 354 loss: 0.2297067604327606
  batch 355 loss: 0.22961976238539522
  batch 356 loss: 0.22974691318159693
  batch 357 loss: 0.2295925667723354
  batch 358 loss: 0.22947872122073307
  batch 359 loss: 0.22945253675528557
  batch 360 loss: 0.22944243695173
  batch 361 loss: 0.22940965587082332
  batch 362 loss: 0.22942372679051773
  batch 363 loss: 0.22939851280743098
  batch 364 loss: 0.22931760029642137
  batch 365 loss: 0.2293659901374007
  batch 366 loss: 0.22939039774931194
  batch 367 loss: 0.22947070116243182
  batch 368 loss: 0.22948158005981342
  batch 369 loss: 0.22944819519351814
  batch 370 loss: 0.22944348522940197
  batch 371 loss: 0.22950294738349247
  batch 372 loss: 0.2295753335359917
  batch 373 loss: 0.2296025393875291
  batch 374 loss: 0.22950188620364603
  batch 375 loss: 0.22962538039684297
  batch 376 loss: 0.22974819860401305
  batch 377 loss: 0.22969202630083504
  batch 378 loss: 0.22968671246180458
  batch 379 loss: 0.22963229615958825
  batch 380 loss: 0.22967942557052562
  batch 381 loss: 0.2295994449240642
  batch 382 loss: 0.22959690611243871
  batch 383 loss: 0.22950196126111183
  batch 384 loss: 0.22952136114084473
  batch 385 loss: 0.22953180976502308
  batch 386 loss: 0.22950910518206463
  batch 387 loss: 0.22946699340984186
  batch 388 loss: 0.22934625128802566
  batch 389 loss: 0.2293290215470184
  batch 390 loss: 0.22929074378349842
  batch 391 loss: 0.22916265746669087
  batch 392 loss: 0.2292343699278272
  batch 393 loss: 0.2292354655553973
  batch 394 loss: 0.22926629016998457
  batch 395 loss: 0.22933884447888483
  batch 396 loss: 0.2293320478062437
  batch 397 loss: 0.22940199854091672
  batch 398 loss: 0.22954839445538258
  batch 399 loss: 0.22956764541174235
  batch 400 loss: 0.2296371901780367
  batch 401 loss: 0.22969473612278773
  batch 402 loss: 0.22972434547854892
  batch 403 loss: 0.22972879915616057
  batch 404 loss: 0.22974582197199953
  batch 405 loss: 0.2297803506071185
  batch 406 loss: 0.2297157845987475
  batch 407 loss: 0.22979674881887086
  batch 408 loss: 0.22972965408481805
  batch 409 loss: 0.22971285591790028
  batch 410 loss: 0.2295733935949279
  batch 411 loss: 0.22956643204619415
  batch 412 loss: 0.22960630652395267
  batch 413 loss: 0.22958856034509784
  batch 414 loss: 0.22961831510354933
  batch 415 loss: 0.2295533850968602
  batch 416 loss: 0.22951385851662892
  batch 417 loss: 0.2294702076082893
  batch 418 loss: 0.2294217234735854
  batch 419 loss: 0.22941319498640256
  batch 420 loss: 0.22937387600541115
  batch 421 loss: 0.22942171537819497
  batch 422 loss: 0.2293895450884132
  batch 423 loss: 0.22936655496451872
  batch 424 loss: 0.22943116390620763
  batch 425 loss: 0.22940887524801143
  batch 426 loss: 0.22935309128162445
  batch 427 loss: 0.22937340643562254
  batch 428 loss: 0.22940971820710976
  batch 429 loss: 0.2293131081582783
  batch 430 loss: 0.2293590347780738
  batch 431 loss: 0.22936530722404577
  batch 432 loss: 0.22946239569810806
  batch 433 loss: 0.22940801851468604
  batch 434 loss: 0.22930331078405206
  batch 435 loss: 0.22937386018791417
  batch 436 loss: 0.22935929436475858
  batch 437 loss: 0.2292467380961237
  batch 438 loss: 0.22919873063150606
  batch 439 loss: 0.2290981744579411
  batch 440 loss: 0.22913956123996865
  batch 441 loss: 0.22915303473975382
  batch 442 loss: 0.2291454094634876
  batch 443 loss: 0.22915883833210302
  batch 444 loss: 0.22915168989214812
  batch 445 loss: 0.229075540083178
  batch 446 loss: 0.229086320062122
  batch 447 loss: 0.22907873144752494
  batch 448 loss: 0.22916871305954242
  batch 449 loss: 0.22907401886841236
  batch 450 loss: 0.22908408068948322
  batch 451 loss: 0.22904545415663666
  batch 452 loss: 0.22898887219814074
  batch 453 loss: 0.22892231907012972
  batch 454 loss: 0.2289508005792874
  batch 455 loss: 0.2289398057775183
  batch 456 loss: 0.22885331818670557
  batch 457 loss: 0.22883137542126736
  batch 458 loss: 0.22882647558591251
  batch 459 loss: 0.22880828533167413
  batch 460 loss: 0.22877235943856447
  batch 461 loss: 0.22879946787807273
  batch 462 loss: 0.22883949735205927
  batch 463 loss: 0.22884427049633752
  batch 464 loss: 0.22879310096774635
  batch 465 loss: 0.22877105458449293
  batch 466 loss: 0.22868707654558026
  batch 467 loss: 0.22879667717333269
  batch 468 loss: 0.22871875333098265
  batch 469 loss: 0.22862062814520367
  batch 470 loss: 0.2287063922970853
  batch 471 loss: 0.2287142542833482
  batch 472 loss: 0.22852774055958805
LOSS train 0.22852774055958805 valid 0.22365497052669525
LOSS train 0.22852774055958805 valid 0.20719747990369797
LOSS train 0.22852774055958805 valid 0.21765696505705515
LOSS train 0.22852774055958805 valid 0.20290955528616905
LOSS train 0.22852774055958805 valid 0.20076200366020203
LOSS train 0.22852774055958805 valid 0.2037415107091268
LOSS train 0.22852774055958805 valid 0.1983681321144104
LOSS train 0.22852774055958805 valid 0.1961722243577242
LOSS train 0.22852774055958805 valid 0.1961578627427419
LOSS train 0.22852774055958805 valid 0.19370388090610505
LOSS train 0.22852774055958805 valid 0.19365129687569357
LOSS train 0.22852774055958805 valid 0.19631085420648256
LOSS train 0.22852774055958805 valid 0.19563543796539307
LOSS train 0.22852774055958805 valid 0.19296904227563313
LOSS train 0.22852774055958805 valid 0.19275608261426289
LOSS train 0.22852774055958805 valid 0.19609457161277533
LOSS train 0.22852774055958805 valid 0.19624213699032278
LOSS train 0.22852774055958805 valid 0.1961256174577607
LOSS train 0.22852774055958805 valid 0.19882199717195412
LOSS train 0.22852774055958805 valid 0.19814641401171684
LOSS train 0.22852774055958805 valid 0.20073724147819338
LOSS train 0.22852774055958805 valid 0.20022478699684143
LOSS train 0.22852774055958805 valid 0.19812618260798248
LOSS train 0.22852774055958805 valid 0.19792837587495646
LOSS train 0.22852774055958805 valid 0.19774558365345002
LOSS train 0.22852774055958805 valid 0.19690018949600366
LOSS train 0.22852774055958805 valid 0.1965623006776527
LOSS train 0.22852774055958805 valid 0.19725489616394043
LOSS train 0.22852774055958805 valid 0.19614289906518212
LOSS train 0.22852774055958805 valid 0.19584980060656865
LOSS train 0.22852774055958805 valid 0.1961997454204867
LOSS train 0.22852774055958805 valid 0.1961802556179464
LOSS train 0.22852774055958805 valid 0.19466366822069342
LOSS train 0.22852774055958805 valid 0.1942600055652506
LOSS train 0.22852774055958805 valid 0.19473726195948465
LOSS train 0.22852774055958805 valid 0.19573020976450708
LOSS train 0.22852774055958805 valid 0.19607004120543198
LOSS train 0.22852774055958805 valid 0.19608434328907415
LOSS train 0.22852774055958805 valid 0.19695044671877837
LOSS train 0.22852774055958805 valid 0.19711797460913658
LOSS train 0.22852774055958805 valid 0.19700365168292347
LOSS train 0.22852774055958805 valid 0.1980616113259679
LOSS train 0.22852774055958805 valid 0.19845790571944658
LOSS train 0.22852774055958805 valid 0.19831784916195003
LOSS train 0.22852774055958805 valid 0.19784716069698333
LOSS train 0.22852774055958805 valid 0.1972728899639586
LOSS train 0.22852774055958805 valid 0.1967684623408825
LOSS train 0.22852774055958805 valid 0.1981844532613953
LOSS train 0.22852774055958805 valid 0.19735308660536396
LOSS train 0.22852774055958805 valid 0.19806530058383942
LOSS train 0.22852774055958805 valid 0.1977915264227811
LOSS train 0.22852774055958805 valid 0.19732320136748827
LOSS train 0.22852774055958805 valid 0.19848437185557383
LOSS train 0.22852774055958805 valid 0.19855975056136096
LOSS train 0.22852774055958805 valid 0.19831696233966134
LOSS train 0.22852774055958805 valid 0.19854455521064146
LOSS train 0.22852774055958805 valid 0.19802275273883552
LOSS train 0.22852774055958805 valid 0.19852284316358895
LOSS train 0.22852774055958805 valid 0.19814873600410202
LOSS train 0.22852774055958805 valid 0.19815597981214522
LOSS train 0.22852774055958805 valid 0.1982592874374546
LOSS train 0.22852774055958805 valid 0.19824676888604317
LOSS train 0.22852774055958805 valid 0.19792532069342478
LOSS train 0.22852774055958805 valid 0.19835847918875515
LOSS train 0.22852774055958805 valid 0.1971432382097611
LOSS train 0.22852774055958805 valid 0.19688081413959013
LOSS train 0.22852774055958805 valid 0.19745393130761474
LOSS train 0.22852774055958805 valid 0.1969492848524276
LOSS train 0.22852774055958805 valid 0.1977048331628675
LOSS train 0.22852774055958805 valid 0.19829097911715507
LOSS train 0.22852774055958805 valid 0.19865436001982487
LOSS train 0.22852774055958805 valid 0.19921764493402508
LOSS train 0.22852774055958805 valid 0.19996571020312506
LOSS train 0.22852774055958805 valid 0.19968190638197436
LOSS train 0.22852774055958805 valid 0.19936322877804438
LOSS train 0.22852774055958805 valid 0.19958585039957574
LOSS train 0.22852774055958805 valid 0.19944529922364593
LOSS train 0.22852774055958805 valid 0.19947167170735505
LOSS train 0.22852774055958805 valid 0.199434438555301
LOSS train 0.22852774055958805 valid 0.19912304999306799
LOSS train 0.22852774055958805 valid 0.19944655242157572
LOSS train 0.22852774055958805 valid 0.19924650768317828
LOSS train 0.22852774055958805 valid 0.1994229810963194
LOSS train 0.22852774055958805 valid 0.199094787151331
LOSS train 0.22852774055958805 valid 0.19979499780079898
LOSS train 0.22852774055958805 valid 0.19972711169095927
LOSS train 0.22852774055958805 valid 0.19951174202664146
LOSS train 0.22852774055958805 valid 0.19985059318556028
LOSS train 0.22852774055958805 valid 0.2001341562090295
LOSS train 0.22852774055958805 valid 0.2005647026002407
LOSS train 0.22852774055958805 valid 0.20054123468778945
LOSS train 0.22852774055958805 valid 0.20066983455225176
LOSS train 0.22852774055958805 valid 0.2004425316568344
LOSS train 0.22852774055958805 valid 0.2007258712769823
LOSS train 0.22852774055958805 valid 0.2008729628042171
LOSS train 0.22852774055958805 valid 0.20109390009505054
LOSS train 0.22852774055958805 valid 0.20113461526091567
LOSS train 0.22852774055958805 valid 0.2016987590005203
LOSS train 0.22852774055958805 valid 0.20194996614949873
LOSS train 0.22852774055958805 valid 0.20199953086674213
LOSS train 0.22852774055958805 valid 0.20207644431012692
LOSS train 0.22852774055958805 valid 0.20245440662199377
LOSS train 0.22852774055958805 valid 0.20224481481082232
LOSS train 0.22852774055958805 valid 0.20205821218685463
LOSS train 0.22852774055958805 valid 0.20234015378214065
LOSS train 0.22852774055958805 valid 0.2025618760512685
LOSS train 0.22852774055958805 valid 0.20222938708334326
LOSS train 0.22852774055958805 valid 0.20231651425085687
LOSS train 0.22852774055958805 valid 0.20202731327452791
LOSS train 0.22852774055958805 valid 0.2018781483850696
LOSS train 0.22852774055958805 valid 0.20190556515176017
LOSS train 0.22852774055958805 valid 0.20214246632531285
LOSS train 0.22852774055958805 valid 0.2019083984525858
LOSS train 0.22852774055958805 valid 0.20205654393424066
LOSS train 0.22852774055958805 valid 0.20285360560469004
LOSS train 0.22852774055958805 valid 0.20259189689210777
LOSS train 0.22852774055958805 valid 0.20319562749220774
LOSS train 0.22852774055958805 valid 0.2029690024585037
LOSS train 0.22852774055958805 valid 0.2027640491097915
LOSS train 0.22852774055958805 valid 0.2023194981738925
LOSS train 0.22852774055958805 valid 0.2020660797057073
LOSS train 0.22852774055958805 valid 0.20230239611424383
LOSS train 0.22852774055958805 valid 0.20236388811977898
LOSS train 0.22852774055958805 valid 0.20256069801267115
LOSS train 0.22852774055958805 valid 0.20236698430776595
LOSS train 0.22852774055958805 valid 0.2025871155635705
LOSS train 0.22852774055958805 valid 0.20248029498368736
LOSS train 0.22852774055958805 valid 0.20245886250631884
LOSS train 0.22852774055958805 valid 0.20215338439673416
LOSS train 0.22852774055958805 valid 0.2018885188950942
LOSS train 0.22852774055958805 valid 0.20197803527116776
LOSS train 0.22852774055958805 valid 0.20186210226154688
LOSS train 0.22852774055958805 valid 0.2018289302188651
LOSS train 0.22852774055958805 valid 0.20194733659945316
LOSS train 0.22852774055958805 valid 0.2020605422280453
LOSS train 0.22852774055958805 valid 0.20198493609752724
LOSS train 0.22852774055958805 valid 0.2019234098983507
LOSS train 0.22852774055958805 valid 0.201891735682021
LOSS train 0.22852774055958805 valid 0.20161412157815137
LOSS train 0.22852774055958805 valid 0.20151707279895034
LOSS train 0.22852774055958805 valid 0.20150941805847994
LOSS train 0.22852774055958805 valid 0.20176115868167138
LOSS train 0.22852774055958805 valid 0.20181833004409616
LOSS train 0.22852774055958805 valid 0.20189845122189987
LOSS train 0.22852774055958805 valid 0.2018407328889288
LOSS train 0.22852774055958805 valid 0.201790445318369
LOSS train 0.22852774055958805 valid 0.2017706570272543
LOSS train 0.22852774055958805 valid 0.2017069797761537
LOSS train 0.22852774055958805 valid 0.20178199859473528
LOSS train 0.22852774055958805 valid 0.20178742681940395
LOSS train 0.22852774055958805 valid 0.20167907278071964
LOSS train 0.22852774055958805 valid 0.20162904365478376
LOSS train 0.22852774055958805 valid 0.201408794356717
LOSS train 0.22852774055958805 valid 0.201590179574567
LOSS train 0.22852774055958805 valid 0.20164985171248836
LOSS train 0.22852774055958805 valid 0.20163380368970907
LOSS train 0.22852774055958805 valid 0.20157065204564173
LOSS train 0.22852774055958805 valid 0.20174787990465948
LOSS train 0.22852774055958805 valid 0.20178341092366092
LOSS train 0.22852774055958805 valid 0.20199641878716648
LOSS train 0.22852774055958805 valid 0.20193579862391728
LOSS train 0.22852774055958805 valid 0.20202328563656335
LOSS train 0.22852774055958805 valid 0.2018598964883506
LOSS train 0.22852774055958805 valid 0.20194087245660583
LOSS train 0.22852774055958805 valid 0.2021098614190564
LOSS train 0.22852774055958805 valid 0.20201012792896075
LOSS train 0.22852774055958805 valid 0.20197050304648406
LOSS train 0.22852774055958805 valid 0.20178277520019383
LOSS train 0.22852774055958805 valid 0.2016087225996531
LOSS train 0.22852774055958805 valid 0.20173575619564338
LOSS train 0.22852774055958805 valid 0.2016958151208727
LOSS train 0.22852774055958805 valid 0.20174634140418019
LOSS train 0.22852774055958805 valid 0.20154965195180363
LOSS train 0.22852774055958805 valid 0.20196202667108898
LOSS train 0.22852774055958805 valid 0.2019897855605398
LOSS train 0.22852774055958805 valid 0.20196411178701304
LOSS train 0.22852774055958805 valid 0.2019860494018948
LOSS train 0.22852774055958805 valid 0.20205620755807738
LOSS train 0.22852774055958805 valid 0.2022114370705029
LOSS train 0.22852774055958805 valid 0.2020969991468721
LOSS train 0.22852774055958805 valid 0.20208053245564192
LOSS train 0.22852774055958805 valid 0.20189426344010855
LOSS train 0.22852774055958805 valid 0.2020409724832884
LOSS train 0.22852774055958805 valid 0.20192272374001535
LOSS train 0.22852774055958805 valid 0.20179866218083614
LOSS train 0.22852774055958805 valid 0.20196134088340625
LOSS train 0.22852774055958805 valid 0.20190671249666317
LOSS train 0.22852774055958805 valid 0.20199719939301622
LOSS train 0.22852774055958805 valid 0.20177861870754332
LOSS train 0.22852774055958805 valid 0.20177570208907128
LOSS train 0.22852774055958805 valid 0.20188521492387612
LOSS train 0.22852774055958805 valid 0.2018838960599775
LOSS train 0.22852774055958805 valid 0.20166955428883201
LOSS train 0.22852774055958805 valid 0.20153464173379632
LOSS train 0.22852774055958805 valid 0.20146282693514458
LOSS train 0.22852774055958805 valid 0.20151651760905373
LOSS train 0.22852774055958805 valid 0.2015048823305193
LOSS train 0.22852774055958805 valid 0.20165112084060005
LOSS train 0.22852774055958805 valid 0.20160560418463233
LOSS train 0.22852774055958805 valid 0.20174041029065848
LOSS train 0.22852774055958805 valid 0.20155764652870187
LOSS train 0.22852774055958805 valid 0.20151082223447242
LOSS train 0.22852774055958805 valid 0.20157865317437448
LOSS train 0.22852774055958805 valid 0.20131824585590877
LOSS train 0.22852774055958805 valid 0.20121045239814897
LOSS train 0.22852774055958805 valid 0.20126343247236558
LOSS train 0.22852774055958805 valid 0.20120489744892442
LOSS train 0.22852774055958805 valid 0.20104154045335376
LOSS train 0.22852774055958805 valid 0.2010879602514956
LOSS train 0.22852774055958805 valid 0.20119358418243272
LOSS train 0.22852774055958805 valid 0.20137011966010407
LOSS train 0.22852774055958805 valid 0.20119905236335295
LOSS train 0.22852774055958805 valid 0.20117309874911823
LOSS train 0.22852774055958805 valid 0.20121587809418964
LOSS train 0.22852774055958805 valid 0.20097916774278463
LOSS train 0.22852774055958805 valid 0.2007725802767608
LOSS train 0.22852774055958805 valid 0.20054716616868973
LOSS train 0.22852774055958805 valid 0.2005813532944666
LOSS train 0.22852774055958805 valid 0.20059710070831047
LOSS train 0.22852774055958805 valid 0.20072678134522653
LOSS train 0.22852774055958805 valid 0.20069800819611658
LOSS train 0.22852774055958805 valid 0.2008183680072024
LOSS train 0.22852774055958805 valid 0.2010019165317574
LOSS train 0.22852774055958805 valid 0.20087844196573965
LOSS train 0.22852774055958805 valid 0.20075662033425437
LOSS train 0.22852774055958805 valid 0.20065583852408206
LOSS train 0.22852774055958805 valid 0.2006644872776212
LOSS train 0.22852774055958805 valid 0.20060914346392741
LOSS train 0.22852774055958805 valid 0.20052622847551863
LOSS train 0.22852774055958805 valid 0.20049613438870595
LOSS train 0.22852774055958805 valid 0.2004829455828254
LOSS train 0.22852774055958805 valid 0.20057135309767107
LOSS train 0.22852774055958805 valid 0.2008923273293757
LOSS train 0.22852774055958805 valid 0.2008630971177521
LOSS train 0.22852774055958805 valid 0.20096366370612004
LOSS train 0.22852774055958805 valid 0.20080432235815768
LOSS train 0.22852774055958805 valid 0.2008495373879304
LOSS train 0.22852774055958805 valid 0.20079924620124473
LOSS train 0.22852774055958805 valid 0.20075982544087964
LOSS train 0.22852774055958805 valid 0.20039979104573527
LOSS train 0.22852774055958805 valid 0.2002319655539584
LOSS train 0.22852774055958805 valid 0.20019559842372728
LOSS train 0.22852774055958805 valid 0.20000806815707634
LOSS train 0.22852774055958805 valid 0.20002426368901965
LOSS train 0.22852774055958805 valid 0.20014381691509364
LOSS train 0.22852774055958805 valid 0.20027523690728638
LOSS train 0.22852774055958805 valid 0.20031737499994787
LOSS train 0.22852774055958805 valid 0.20035069979607098
LOSS train 0.22852774055958805 valid 0.20015861641570745
LOSS train 0.22852774055958805 valid 0.20022777238488199
LOSS train 0.22852774055958805 valid 0.20027627131022305
LOSS train 0.22852774055958805 valid 0.20014118027710726
LOSS train 0.22852774055958805 valid 0.19992979598257382
LOSS train 0.22852774055958805 valid 0.20010261510418156
LOSS train 0.22852774055958805 valid 0.2000272737706409
LOSS train 0.22852774055958805 valid 0.19978807543520816
LOSS train 0.22852774055958805 valid 0.19973852108540702
LOSS train 0.22852774055958805 valid 0.19992468820870385
LOSS train 0.22852774055958805 valid 0.1997846964761097
LOSS train 0.22852774055958805 valid 0.19964270554482938
LOSS train 0.22852774055958805 valid 0.1997478490417031
LOSS train 0.22852774055958805 valid 0.20002386056398616
LOSS train 0.22852774055958805 valid 0.19982760716867085
LOSS train 0.22852774055958805 valid 0.19992921673551653
LOSS train 0.22852774055958805 valid 0.19986045891946216
LOSS train 0.22852774055958805 valid 0.19993030512355325
LOSS train 0.22852774055958805 valid 0.19982038028119656
LOSS train 0.22852774055958805 valid 0.19991625353360354
LOSS train 0.22852774055958805 valid 0.20003482868347913
LOSS train 0.22852774055958805 valid 0.19991688355803489
LOSS train 0.22852774055958805 valid 0.19994309800247426
LOSS train 0.22852774055958805 valid 0.1998309443485649
LOSS train 0.22852774055958805 valid 0.19972286651750187
LOSS train 0.22852774055958805 valid 0.19960002603865887
LOSS train 0.22852774055958805 valid 0.19964539330114017
LOSS train 0.22852774055958805 valid 0.1996748538930779
LOSS train 0.22852774055958805 valid 0.19972973499810223
LOSS train 0.22852774055958805 valid 0.1997381008411054
LOSS train 0.22852774055958805 valid 0.19966375317731638
LOSS train 0.22852774055958805 valid 0.19953476821205446
LOSS train 0.22852774055958805 valid 0.1993488357849817
LOSS train 0.22852774055958805 valid 0.19925688199223357
LOSS train 0.22852774055958805 valid 0.19931651728207989
LOSS train 0.22852774055958805 valid 0.19924175699936672
LOSS train 0.22852774055958805 valid 0.19933922805807047
LOSS train 0.22852774055958805 valid 0.19930746476654407
LOSS train 0.22852774055958805 valid 0.1992627038824849
LOSS train 0.22852774055958805 valid 0.19916876992728147
LOSS train 0.22852774055958805 valid 0.19907400664264355
LOSS train 0.22852774055958805 valid 0.19897157937802118
LOSS train 0.22852774055958805 valid 0.19897404428302629
LOSS train 0.22852774055958805 valid 0.19892357823068965
LOSS train 0.22852774055958805 valid 0.19894822793710762
LOSS train 0.22852774055958805 valid 0.19895111903750978
LOSS train 0.22852774055958805 valid 0.19892872559317087
LOSS train 0.22852774055958805 valid 0.19892497786094207
LOSS train 0.22852774055958805 valid 0.19883457942542804
LOSS train 0.22852774055958805 valid 0.19895120957653795
LOSS train 0.22852774055958805 valid 0.19899374453419427
LOSS train 0.22852774055958805 valid 0.19900379113852978
LOSS train 0.22852774055958805 valid 0.19888294908988516
LOSS train 0.22852774055958805 valid 0.19885478388296057
LOSS train 0.22852774055958805 valid 0.19877758236116308
LOSS train 0.22852774055958805 valid 0.19879232128886015
LOSS train 0.22852774055958805 valid 0.19872245932700205
LOSS train 0.22852774055958805 valid 0.19898954440468278
LOSS train 0.22852774055958805 valid 0.19896739854777676
LOSS train 0.22852774055958805 valid 0.19897867959331383
LOSS train 0.22852774055958805 valid 0.1989955788532507
LOSS train 0.22852774055958805 valid 0.19896583290350053
LOSS train 0.22852774055958805 valid 0.19895817030376944
LOSS train 0.22852774055958805 valid 0.19892230269331962
LOSS train 0.22852774055958805 valid 0.19905107373151534
LOSS train 0.22852774055958805 valid 0.1990492001982631
LOSS train 0.22852774055958805 valid 0.1989761357979169
LOSS train 0.22852774055958805 valid 0.19897275689187682
LOSS train 0.22852774055958805 valid 0.19898525470344802
LOSS train 0.22852774055958805 valid 0.19896546758289607
LOSS train 0.22852774055958805 valid 0.19893445347824068
LOSS train 0.22852774055958805 valid 0.19884308625478297
LOSS train 0.22852774055958805 valid 0.1989535205414362
LOSS train 0.22852774055958805 valid 0.19894121797836345
LOSS train 0.22852774055958805 valid 0.19882375811552486
LOSS train 0.22852774055958805 valid 0.1987991537898779
LOSS train 0.22852774055958805 valid 0.19866435199975968
LOSS train 0.22852774055958805 valid 0.19868763181969432
LOSS train 0.22852774055958805 valid 0.19864597043950988
LOSS train 0.22852774055958805 valid 0.19861380426531158
LOSS train 0.22852774055958805 valid 0.19860230175648055
LOSS train 0.22852774055958805 valid 0.1986036945247289
LOSS train 0.22852774055958805 valid 0.19854851388949282
LOSS train 0.22852774055958805 valid 0.19854167725397162
LOSS train 0.22852774055958805 valid 0.19870924477552152
LOSS train 0.22852774055958805 valid 0.19862243798589277
LOSS train 0.22852774055958805 valid 0.19853133107744045
LOSS train 0.22852774055958805 valid 0.19843659117551787
LOSS train 0.22852774055958805 valid 0.19860417378143316
LOSS train 0.22852774055958805 valid 0.19859123152829486
LOSS train 0.22852774055958805 valid 0.19850229032359643
LOSS train 0.22852774055958805 valid 0.19861564598977566
LOSS train 0.22852774055958805 valid 0.19853752897369548
LOSS train 0.22852774055958805 valid 0.19851307085121583
LOSS train 0.22852774055958805 valid 0.19840269431515267
LOSS train 0.22852774055958805 valid 0.19849022029530863
LOSS train 0.22852774055958805 valid 0.19861642988263697
LOSS train 0.22852774055958805 valid 0.1985826390886927
LOSS train 0.22852774055958805 valid 0.198486605557138
LOSS train 0.22852774055958805 valid 0.19857283306961088
LOSS train 0.22852774055958805 valid 0.19852187091692813
LOSS train 0.22852774055958805 valid 0.19855054734008654
LOSS train 0.22852774055958805 valid 0.19862473942870087
LOSS train 0.22852774055958805 valid 0.1986808281349526
LOSS train 0.22852774055958805 valid 0.19869600137215498
LOSS train 0.22852774055958805 valid 0.19869079123024885
LOSS train 0.22852774055958805 valid 0.19855500407202142
LOSS train 0.22852774055958805 valid 0.19858502150837626
LOSS train 0.22852774055958805 valid 0.19864922968529852
LOSS train 0.22852774055958805 valid 0.19857868845652601
LOSS train 0.22852774055958805 valid 0.19851616931907978
LOSS train 0.22852774055958805 valid 0.1986879387870431
LOSS train 0.22852774055958805 valid 0.19859322701217064
LOSS train 0.22852774055958805 valid 0.1986067411361149
LOSS train 0.22852774055958805 valid 0.19865740850034166
LOSS train 0.22852774055958805 valid 0.19857341401115225
LOSS train 0.22852774055958805 valid 0.19865300122597446
LOSS train 0.22852774055958805 valid 0.19865096912113694
LOSS train 0.22852774055958805 valid 0.1985908933288395
LOSS train 0.22852774055958805 valid 0.19845853930177249
LOSS train 0.22852774055958805 valid 0.1984029879697616
EPOCH 24:
  batch 1 loss: 0.22127768397331238
  batch 2 loss: 0.24489431083202362
  batch 3 loss: 0.24228805800278982
  batch 4 loss: 0.2444263957440853
  batch 5 loss: 0.24478089809417725
  batch 6 loss: 0.23857097824414572
  batch 7 loss: 0.23819585357393538
  batch 8 loss: 0.23416291549801826
  batch 9 loss: 0.23578922781679365
  batch 10 loss: 0.23210911303758622
  batch 11 loss: 0.23228100484067743
  batch 12 loss: 0.23106200993061066
  batch 13 loss: 0.22633716464042664
  batch 14 loss: 0.22556467141423905
  batch 15 loss: 0.225722407301267
  batch 16 loss: 0.22444050386548042
  batch 17 loss: 0.22490636390798233
  batch 18 loss: 0.22391060574187172
  batch 19 loss: 0.22319633082339638
  batch 20 loss: 0.2204977825284004
  batch 21 loss: 0.2236025333404541
  batch 22 loss: 0.22455005212263626
  batch 23 loss: 0.22559337771457175
  batch 24 loss: 0.22507202997803688
  batch 25 loss: 0.22536784589290618
  batch 26 loss: 0.22495045283666024
  batch 27 loss: 0.22502302295631832
  batch 28 loss: 0.22654857326831138
  batch 29 loss: 0.2264928350160862
  batch 30 loss: 0.22800389577945074
  batch 31 loss: 0.2281646555469882
  batch 32 loss: 0.2289782864972949
  batch 33 loss: 0.23110561388911624
  batch 34 loss: 0.23114942540140712
  batch 35 loss: 0.23272927914346966
  batch 36 loss: 0.23318746892942321
  batch 37 loss: 0.23373701725457166
  batch 38 loss: 0.2337662910944537
  batch 39 loss: 0.23417548071115446
  batch 40 loss: 0.23360417932271957
  batch 41 loss: 0.23372975427929948
  batch 42 loss: 0.23316180918897902
  batch 43 loss: 0.23362699081731397
  batch 44 loss: 0.2340594232082367
  batch 45 loss: 0.23357213073306612
  batch 46 loss: 0.23232597501381583
  batch 47 loss: 0.23150744241602877
  batch 48 loss: 0.23110439535230398
  batch 49 loss: 0.2315672280228868
  batch 50 loss: 0.23281465977430343
  batch 51 loss: 0.23216265498423108
  batch 52 loss: 0.2322503110537162
  batch 53 loss: 0.2320113789360478
  batch 54 loss: 0.23192765480942196
  batch 55 loss: 0.23246244137937372
  batch 56 loss: 0.23285042441317014
  batch 57 loss: 0.23223778291752464
  batch 58 loss: 0.23209883760789346
  batch 59 loss: 0.2322646336535276
  batch 60 loss: 0.23310120776295662
  batch 61 loss: 0.23345336147019122
  batch 62 loss: 0.23367316444073954
  batch 63 loss: 0.23297321441627683
  batch 64 loss: 0.2323460285551846
  batch 65 loss: 0.2319009230687068
  batch 66 loss: 0.2314914713303248
  batch 67 loss: 0.23122539782702034
  batch 68 loss: 0.23129994334543452
  batch 69 loss: 0.23160102185995682
  batch 70 loss: 0.23151239901781082
  batch 71 loss: 0.23181813094817416
  batch 72 loss: 0.23183254214624563
  batch 73 loss: 0.23184407716744568
  batch 74 loss: 0.23189222691832362
  batch 75 loss: 0.23195663114388784
  batch 76 loss: 0.23228991168894267
  batch 77 loss: 0.23234671122067935
  batch 78 loss: 0.23251487945134824
  batch 79 loss: 0.2324871379363386
  batch 80 loss: 0.23269040025770665
  batch 81 loss: 0.2327304361043153
  batch 82 loss: 0.23282109600741688
  batch 83 loss: 0.23244884628129292
  batch 84 loss: 0.2318575479799793
  batch 85 loss: 0.2324429766220205
  batch 86 loss: 0.23234991625297902
  batch 87 loss: 0.23226781035291738
  batch 88 loss: 0.23193720982156016
  batch 89 loss: 0.23160010509276657
  batch 90 loss: 0.23185923993587493
  batch 91 loss: 0.23160218996006054
  batch 92 loss: 0.23124994110801947
  batch 93 loss: 0.23080544317922286
  batch 94 loss: 0.23056284100451369
  batch 95 loss: 0.23054597738542054
  batch 96 loss: 0.23045928698653975
  batch 97 loss: 0.23108006153524535
  batch 98 loss: 0.2309111586334754
  batch 99 loss: 0.23054038635408036
  batch 100 loss: 0.2308420205116272
  batch 101 loss: 0.23076461137521384
  batch 102 loss: 0.2310531108694918
  batch 103 loss: 0.2309697128615333
  batch 104 loss: 0.2312255258170458
  batch 105 loss: 0.2309814541112809
  batch 106 loss: 0.2309872938214608
  batch 107 loss: 0.23094923989238025
  batch 108 loss: 0.2304757804506355
  batch 109 loss: 0.23066611095852807
  batch 110 loss: 0.23084684596820312
  batch 111 loss: 0.231068064366375
  batch 112 loss: 0.23101525155029126
  batch 113 loss: 0.23092767345694315
  batch 114 loss: 0.23088655241748743
  batch 115 loss: 0.23064591936443163
  batch 116 loss: 0.23051638348863043
  batch 117 loss: 0.23031470714471278
  batch 118 loss: 0.23038755938158198
  batch 119 loss: 0.23051536909672393
  batch 120 loss: 0.2304129516084989
  batch 121 loss: 0.23029262221549168
  batch 122 loss: 0.23019922903326692
  batch 123 loss: 0.22983702678021375
  batch 124 loss: 0.2296759069206253
  batch 125 loss: 0.2293450219631195
  batch 126 loss: 0.22933310152046263
  batch 127 loss: 0.22964301165633313
  batch 128 loss: 0.22971701598726213
  batch 129 loss: 0.22996058112891146
  batch 130 loss: 0.23002048008717024
  batch 131 loss: 0.23008955742111642
  batch 132 loss: 0.23024451924544392
  batch 133 loss: 0.2302120744733882
  batch 134 loss: 0.23019012319507884
  batch 135 loss: 0.23009575759923018
  batch 136 loss: 0.23043596174787073
  batch 137 loss: 0.23068074457836849
  batch 138 loss: 0.23075257954390152
  batch 139 loss: 0.23062769232465208
  batch 140 loss: 0.23092565909028054
  batch 141 loss: 0.23064884543418884
  batch 142 loss: 0.2304992635904903
  batch 143 loss: 0.2305862184587892
  batch 144 loss: 0.23049922266768086
  batch 145 loss: 0.23059692074512614
  batch 146 loss: 0.23073637750867296
  batch 147 loss: 0.23077709699163632
  batch 148 loss: 0.23083356163791707
  batch 149 loss: 0.2305305035122289
  batch 150 loss: 0.23051396111647288
  batch 151 loss: 0.23021102020677353
  batch 152 loss: 0.2301193967853722
  batch 153 loss: 0.23009697187180614
  batch 154 loss: 0.2298687422623882
  batch 155 loss: 0.2298906381091764
  batch 156 loss: 0.22999185887284768
  batch 157 loss: 0.22990344009201996
  batch 158 loss: 0.2299260264144668
  batch 159 loss: 0.23013961699398808
  batch 160 loss: 0.23011718932539224
  batch 161 loss: 0.23004145892510502
  batch 162 loss: 0.23005302949452106
  batch 163 loss: 0.22991619620220793
  batch 164 loss: 0.23004226704559674
  batch 165 loss: 0.23024464132207811
  batch 166 loss: 0.23055667160864335
  batch 167 loss: 0.23050459907083454
  batch 168 loss: 0.23029933807750544
  batch 169 loss: 0.23024256254088948
  batch 170 loss: 0.23020151318872675
  batch 171 loss: 0.23028155952169185
  batch 172 loss: 0.23019478903260343
  batch 173 loss: 0.23017739474428872
  batch 174 loss: 0.23026974259436817
  batch 175 loss: 0.23032714792660305
  batch 176 loss: 0.23057975924827837
  batch 177 loss: 0.23079942899235226
  batch 178 loss: 0.23064451924200807
  batch 179 loss: 0.23064912572586338
  batch 180 loss: 0.23068980053067206
  batch 181 loss: 0.23063087092908047
  batch 182 loss: 0.23052730517727987
  batch 183 loss: 0.2305394464828929
  batch 184 loss: 0.23054545162164647
  batch 185 loss: 0.2302993576268892
  batch 186 loss: 0.2303198859576256
  batch 187 loss: 0.23055672374638644
  batch 188 loss: 0.23044672370591063
  batch 189 loss: 0.23055797468417535
  batch 190 loss: 0.23043580337574607
  batch 191 loss: 0.23045925920858434
  batch 192 loss: 0.23027152218855917
  batch 193 loss: 0.23029007349607242
  batch 194 loss: 0.2303329119362782
  batch 195 loss: 0.23050161814078307
  batch 196 loss: 0.23039404728582927
  batch 197 loss: 0.23044633305617396
  batch 198 loss: 0.2303821270664533
  batch 199 loss: 0.23054474120463558
  batch 200 loss: 0.23054915696382522
  batch 201 loss: 0.23039743749063407
  batch 202 loss: 0.2304578339404399
  batch 203 loss: 0.23051104228484806
  batch 204 loss: 0.23070059322259007
  batch 205 loss: 0.23065625225625386
  batch 206 loss: 0.2306473783931686
  batch 207 loss: 0.2306918111688273
  batch 208 loss: 0.2305104504697598
  batch 209 loss: 0.23050540286388124
  batch 210 loss: 0.2305123630024138
  batch 211 loss: 0.2304163416980002
  batch 212 loss: 0.23048606557103823
  batch 213 loss: 0.23042958953850706
  batch 214 loss: 0.2303237071104139
  batch 215 loss: 0.23028056635413058
  batch 216 loss: 0.23023368093978475
  batch 217 loss: 0.2303436775498676
  batch 218 loss: 0.23017811508627112
  batch 219 loss: 0.22976622230386082
  batch 220 loss: 0.22983855347741733
  batch 221 loss: 0.22972586966747613
  batch 222 loss: 0.2296926808786822
  batch 223 loss: 0.2295580111410586
  batch 224 loss: 0.22943537343027337
  batch 225 loss: 0.22940941989421845
  batch 226 loss: 0.2294095610227205
  batch 227 loss: 0.22942140810027523
  batch 228 loss: 0.22941088447706742
  batch 229 loss: 0.2294576918974714
  batch 230 loss: 0.2295246557697006
  batch 231 loss: 0.2296200292063998
  batch 232 loss: 0.22961004926212902
  batch 233 loss: 0.22962240036976694
  batch 234 loss: 0.22959884319804671
  batch 235 loss: 0.22950589422215806
  batch 236 loss: 0.229443702783625
  batch 237 loss: 0.22925967242144332
  batch 238 loss: 0.22922433305437825
  batch 239 loss: 0.22922316249693786
  batch 240 loss: 0.22926496745397648
  batch 241 loss: 0.22918798320026318
  batch 242 loss: 0.22918527511772047
  batch 243 loss: 0.22913171596242568
  batch 244 loss: 0.22904196259428244
  batch 245 loss: 0.22892608198584344
  batch 246 loss: 0.2288496457343179
  batch 247 loss: 0.22890826082422666
  batch 248 loss: 0.22898906012696604
  batch 249 loss: 0.22906234920264248
  batch 250 loss: 0.22903258889913558
  batch 251 loss: 0.22894170892191124
  batch 252 loss: 0.22892022452184133
  batch 253 loss: 0.2288507375085778
  batch 254 loss: 0.22881464953497638
  batch 255 loss: 0.22869687062852523
  batch 256 loss: 0.22862468875246122
  batch 257 loss: 0.22865011053094605
  batch 258 loss: 0.22860389607128248
  batch 259 loss: 0.22848018969348038
  batch 260 loss: 0.22845453190115783
  batch 261 loss: 0.22844764938528053
  batch 262 loss: 0.22835004534430176
  batch 263 loss: 0.22822555276377574
  batch 264 loss: 0.22829496702461533
  batch 265 loss: 0.22811017823669147
  batch 266 loss: 0.22793145578606686
  batch 267 loss: 0.22770985687493386
  batch 268 loss: 0.22760579340271095
  batch 269 loss: 0.22763308893792247
  batch 270 loss: 0.22757558094130623
  batch 271 loss: 0.2275974559168094
  batch 272 loss: 0.22748939758714506
  batch 273 loss: 0.22742392417493756
  batch 274 loss: 0.22736090073620316
  batch 275 loss: 0.22731569691137835
  batch 276 loss: 0.22720839250562846
  batch 277 loss: 0.22718574542431194
  batch 278 loss: 0.22703911364078522
  batch 279 loss: 0.22699598717005876
  batch 280 loss: 0.2269815631210804
  batch 281 loss: 0.22689649086193683
  batch 282 loss: 0.22675155467809516
  batch 283 loss: 0.22672947651505893
  batch 284 loss: 0.2267353096058671
  batch 285 loss: 0.22670433432386633
  batch 286 loss: 0.22663410492501893
  batch 287 loss: 0.2265258733611489
  batch 288 loss: 0.2265678205423885
  batch 289 loss: 0.22649097184821396
  batch 290 loss: 0.22644942926949468
  batch 291 loss: 0.2264752822233639
  batch 292 loss: 0.22648528218269348
  batch 293 loss: 0.22652321611451615
  batch 294 loss: 0.22630662982966623
  batch 295 loss: 0.2263309233269449
  batch 296 loss: 0.22637716449193052
  batch 297 loss: 0.22635311174272288
  batch 298 loss: 0.22626853424230678
  batch 299 loss: 0.2263082388950431
  batch 300 loss: 0.22619274705648423
  batch 301 loss: 0.22611597952652612
  batch 302 loss: 0.22631463635441484
  batch 303 loss: 0.22625167692455128
  batch 304 loss: 0.2262622303770561
  batch 305 loss: 0.22622036665189463
  batch 306 loss: 0.2263187098444677
  batch 307 loss: 0.22631739613870067
  batch 308 loss: 0.22633468243595842
  batch 309 loss: 0.22635519007841745
  batch 310 loss: 0.22647463559143005
  batch 311 loss: 0.22643054945100927
  batch 312 loss: 0.22641454068705058
  batch 313 loss: 0.22640032285509018
  batch 314 loss: 0.2264116704938518
  batch 315 loss: 0.22642066786213527
  batch 316 loss: 0.22644429488838475
  batch 317 loss: 0.22647936430639273
  batch 318 loss: 0.2262732652748156
  batch 319 loss: 0.2261527930868083
  batch 320 loss: 0.22609510053880513
  batch 321 loss: 0.22620571230616526
  batch 322 loss: 0.22620295793373393
  batch 323 loss: 0.22610214011218896
  batch 324 loss: 0.22610440013217337
  batch 325 loss: 0.22627554343296932
  batch 326 loss: 0.22640795164678726
  batch 327 loss: 0.22644204448122496
  batch 328 loss: 0.22639055958971743
  batch 329 loss: 0.2264094350004631
  batch 330 loss: 0.22642812864346937
  batch 331 loss: 0.22638314192568787
  batch 332 loss: 0.22651485690330886
  batch 333 loss: 0.22654555263641002
  batch 334 loss: 0.22682131206739448
  batch 335 loss: 0.22691274102944048
  batch 336 loss: 0.2269754166315709
  batch 337 loss: 0.22707443452199771
  batch 338 loss: 0.22702313646585984
  batch 339 loss: 0.22702506813151999
  batch 340 loss: 0.22690229433424333
  batch 341 loss: 0.22684736222116128
  batch 342 loss: 0.22675681175195683
  batch 343 loss: 0.2268056818243366
  batch 344 loss: 0.22679446802236314
  batch 345 loss: 0.22687458853790726
  batch 346 loss: 0.2268899876861214
  batch 347 loss: 0.22679177544989573
  batch 348 loss: 0.22678836477899003
  batch 349 loss: 0.22683229495939347
  batch 350 loss: 0.22706001894814629
  batch 351 loss: 0.22709160312967763
  batch 352 loss: 0.22703616947612978
  batch 353 loss: 0.22698407293041434
  batch 354 loss: 0.22691304548143668
  batch 355 loss: 0.22681637674989835
  batch 356 loss: 0.2269376859571157
  batch 357 loss: 0.2267936118343631
  batch 358 loss: 0.22668396873014598
  batch 359 loss: 0.2266600100857966
  batch 360 loss: 0.22662522908714083
  batch 361 loss: 0.22655596887470944
  batch 362 loss: 0.22657085979841032
  batch 363 loss: 0.22654709954728108
  batch 364 loss: 0.2264567662317019
  batch 365 loss: 0.22649814229305476
  batch 366 loss: 0.2264698446204102
  batch 367 loss: 0.22650555450195187
  batch 368 loss: 0.2265675174396323
  batch 369 loss: 0.2265339944420791
  batch 370 loss: 0.22654965084952278
  batch 371 loss: 0.22659472221473478
  batch 372 loss: 0.22662844560960288
  batch 373 loss: 0.2266365461470934
  batch 374 loss: 0.2265601391620177
  batch 375 loss: 0.22668240650494895
  batch 376 loss: 0.2268080067761401
  batch 377 loss: 0.22678233580342655
  batch 378 loss: 0.2268081948003441
  batch 379 loss: 0.22683862142638356
  batch 380 loss: 0.22680047213246948
  batch 381 loss: 0.22666624082824377
  batch 382 loss: 0.22666997427403615
  batch 383 loss: 0.22667638922484673
  batch 384 loss: 0.22671523162474236
  batch 385 loss: 0.22667539429354977
  batch 386 loss: 0.2266929651094224
  batch 387 loss: 0.22670617926798434
  batch 388 loss: 0.2266516343296803
  batch 389 loss: 0.22666042319147017
  batch 390 loss: 0.2266967276350046
  batch 391 loss: 0.2266784231452381
  batch 392 loss: 0.2267846233610596
  batch 393 loss: 0.22679036935322158
  batch 394 loss: 0.22688113678681668
  batch 395 loss: 0.22701595572731162
  batch 396 loss: 0.22702293669936632
  batch 397 loss: 0.2271253781895193
  batch 398 loss: 0.22721997472509067
  batch 399 loss: 0.2273006514648447
  batch 400 loss: 0.22740823224186898
  batch 401 loss: 0.22747365294251953
  batch 402 loss: 0.2275364861559512
  batch 403 loss: 0.22756779722629056
  batch 404 loss: 0.22759178310337633
  batch 405 loss: 0.2276283539003796
  batch 406 loss: 0.22753522016585168
  batch 407 loss: 0.22762396085906672
  batch 408 loss: 0.22755332017207847
  batch 409 loss: 0.2275481812819875
  batch 410 loss: 0.22746825865129144
  batch 411 loss: 0.2274299412603216
  batch 412 loss: 0.22745007363337916
  batch 413 loss: 0.22738352677723975
  batch 414 loss: 0.22740430982360518
  batch 415 loss: 0.2274273349578122
  batch 416 loss: 0.22740191296459392
  batch 417 loss: 0.22744053355652652
  batch 418 loss: 0.22741623597139377
  batch 419 loss: 0.2274574486267595
  batch 420 loss: 0.22747085165409814
  batch 421 loss: 0.22753762059143773
  batch 422 loss: 0.22751588452060076
  batch 423 loss: 0.22752061623630795
  batch 424 loss: 0.2275968580946045
  batch 425 loss: 0.22759759282364564
  batch 426 loss: 0.22757934845389335
  batch 427 loss: 0.2276445683048257
  batch 428 loss: 0.2277018338024059
  batch 429 loss: 0.22764412714884832
  batch 430 loss: 0.22768133906430976
  batch 431 loss: 0.22768039959904213
  batch 432 loss: 0.22776540358447367
  batch 433 loss: 0.22773612340520766
  batch 434 loss: 0.22764020516163744
  batch 435 loss: 0.22768417565987029
  batch 436 loss: 0.22766289101281298
  batch 437 loss: 0.22756284840183344
  batch 438 loss: 0.22752511606777095
  batch 439 loss: 0.22741946693433446
  batch 440 loss: 0.2274518766186454
  batch 441 loss: 0.2274604250514318
  batch 442 loss: 0.2274277132021356
  batch 443 loss: 0.2274429657523842
  batch 444 loss: 0.22744613090479696
  batch 445 loss: 0.22739081426282948
  batch 446 loss: 0.22740969653220455
  batch 447 loss: 0.2274016850280015
  batch 448 loss: 0.22748591162131301
  batch 449 loss: 0.2273964248348716
  batch 450 loss: 0.2273785859015253
  batch 451 loss: 0.2272999947126583
  batch 452 loss: 0.22721009912480294
  batch 453 loss: 0.22714972946685935
  batch 454 loss: 0.227141843132248
  batch 455 loss: 0.2271320607963499
  batch 456 loss: 0.22703927774962626
  batch 457 loss: 0.2270059149948721
  batch 458 loss: 0.22697928508816848
  batch 459 loss: 0.22696607363509716
  batch 460 loss: 0.22697309469399246
  batch 461 loss: 0.226978315383907
  batch 462 loss: 0.2270292356545791
  batch 463 loss: 0.22703189420519843
  batch 464 loss: 0.22698493054586238
  batch 465 loss: 0.22695964686973122
  batch 466 loss: 0.226859143203676
  batch 467 loss: 0.22698465803323006
  batch 468 loss: 0.22692413742725664
  batch 469 loss: 0.22684417553802036
  batch 470 loss: 0.22691197953325637
  batch 471 loss: 0.22698956000324014
  batch 472 loss: 0.2268150654321505
LOSS train 0.2268150654321505 valid 0.2162153124809265
LOSS train 0.2268150654321505 valid 0.2024085819721222
LOSS train 0.2268150654321505 valid 0.21802316109339395
LOSS train 0.2268150654321505 valid 0.2032005786895752
LOSS train 0.2268150654321505 valid 0.20085585117340088
LOSS train 0.2268150654321505 valid 0.20210246245066324
LOSS train 0.2268150654321505 valid 0.19639775582722255
LOSS train 0.2268150654321505 valid 0.1930146086961031
LOSS train 0.2268150654321505 valid 0.19283783601389992
LOSS train 0.2268150654321505 valid 0.19087885469198226
LOSS train 0.2268150654321505 valid 0.19158435680649497
LOSS train 0.2268150654321505 valid 0.19426645835240683
LOSS train 0.2268150654321505 valid 0.19434485068688026
LOSS train 0.2268150654321505 valid 0.19196018363748277
LOSS train 0.2268150654321505 valid 0.19205666184425355
LOSS train 0.2268150654321505 valid 0.19569982774555683
LOSS train 0.2268150654321505 valid 0.19545073368970087
LOSS train 0.2268150654321505 valid 0.19492815931638083
LOSS train 0.2268150654321505 valid 0.1975601127273158
LOSS train 0.2268150654321505 valid 0.19692282155156135
LOSS train 0.2268150654321505 valid 0.20030191044012705
LOSS train 0.2268150654321505 valid 0.19958874109116467
LOSS train 0.2268150654321505 valid 0.1976527461539144
LOSS train 0.2268150654321505 valid 0.1976698823273182
LOSS train 0.2268150654321505 valid 0.1977352225780487
LOSS train 0.2268150654321505 valid 0.19691555660504562
LOSS train 0.2268150654321505 valid 0.1965332058844743
LOSS train 0.2268150654321505 valid 0.19733120713915145
LOSS train 0.2268150654321505 valid 0.19617209711979175
LOSS train 0.2268150654321505 valid 0.19600380808115006
LOSS train 0.2268150654321505 valid 0.19607543128152047
LOSS train 0.2268150654321505 valid 0.19620341807603836
LOSS train 0.2268150654321505 valid 0.1947060642820416
LOSS train 0.2268150654321505 valid 0.19417969575699637
LOSS train 0.2268150654321505 valid 0.19478861306394848
LOSS train 0.2268150654321505 valid 0.19560310368736586
LOSS train 0.2268150654321505 valid 0.19618815223912936
LOSS train 0.2268150654321505 valid 0.19617091550638802
LOSS train 0.2268150654321505 valid 0.19697362719438014
LOSS train 0.2268150654321505 valid 0.19716681800782682
LOSS train 0.2268150654321505 valid 0.1972284222521433
LOSS train 0.2268150654321505 valid 0.19822644939025244
LOSS train 0.2268150654321505 valid 0.1988236879886583
LOSS train 0.2268150654321505 valid 0.19878082011233678
LOSS train 0.2268150654321505 valid 0.19822671678331164
LOSS train 0.2268150654321505 valid 0.1978531651522802
LOSS train 0.2268150654321505 valid 0.19715557929049146
LOSS train 0.2268150654321505 valid 0.19864300607393184
LOSS train 0.2268150654321505 valid 0.19778912043084904
LOSS train 0.2268150654321505 valid 0.19853086709976198
LOSS train 0.2268150654321505 valid 0.19817453973433552
LOSS train 0.2268150654321505 valid 0.19770528605351081
LOSS train 0.2268150654321505 valid 0.1988925590830029
LOSS train 0.2268150654321505 valid 0.19899658196502262
LOSS train 0.2268150654321505 valid 0.19872369170188903
LOSS train 0.2268150654321505 valid 0.19891256679381644
LOSS train 0.2268150654321505 valid 0.1985236905645906
LOSS train 0.2268150654321505 valid 0.19898134906744136
LOSS train 0.2268150654321505 valid 0.19854037211102954
LOSS train 0.2268150654321505 valid 0.19844872057437896
LOSS train 0.2268150654321505 valid 0.19846304664846326
LOSS train 0.2268150654321505 valid 0.1984075476084986
LOSS train 0.2268150654321505 valid 0.19815504882070753
LOSS train 0.2268150654321505 valid 0.19868847634643316
LOSS train 0.2268150654321505 valid 0.1975982441351964
LOSS train 0.2268150654321505 valid 0.19746515741854004
LOSS train 0.2268150654321505 valid 0.19804060459136963
LOSS train 0.2268150654321505 valid 0.19752744804410374
LOSS train 0.2268150654321505 valid 0.19825286493785138
LOSS train 0.2268150654321505 valid 0.19878511982304709
LOSS train 0.2268150654321505 valid 0.19920871962963696
LOSS train 0.2268150654321505 valid 0.1998270402352015
LOSS train 0.2268150654321505 valid 0.20051039592043995
LOSS train 0.2268150654321505 valid 0.20021755489948634
LOSS train 0.2268150654321505 valid 0.19989444732666015
LOSS train 0.2268150654321505 valid 0.20021229492206322
LOSS train 0.2268150654321505 valid 0.2000431391325864
LOSS train 0.2268150654321505 valid 0.20006639911578253
LOSS train 0.2268150654321505 valid 0.20007643744915346
LOSS train 0.2268150654321505 valid 0.19967583622783422
LOSS train 0.2268150654321505 valid 0.1999332856984786
LOSS train 0.2268150654321505 valid 0.19979853183031082
LOSS train 0.2268150654321505 valid 0.19994847835546517
LOSS train 0.2268150654321505 valid 0.1994970337975593
LOSS train 0.2268150654321505 valid 0.20022917530115913
LOSS train 0.2268150654321505 valid 0.20023998947337615
LOSS train 0.2268150654321505 valid 0.19992384554325848
LOSS train 0.2268150654321505 valid 0.2002687860618938
LOSS train 0.2268150654321505 valid 0.2006079572974966
LOSS train 0.2268150654321505 valid 0.20104107641511493
LOSS train 0.2268150654321505 valid 0.20105733403137752
LOSS train 0.2268150654321505 valid 0.20122359774034956
LOSS train 0.2268150654321505 valid 0.20096474453326194
LOSS train 0.2268150654321505 valid 0.20125879822893344
LOSS train 0.2268150654321505 valid 0.20148875681977524
LOSS train 0.2268150654321505 valid 0.201758132626613
LOSS train 0.2268150654321505 valid 0.20182236537490925
LOSS train 0.2268150654321505 valid 0.20222380361994918
LOSS train 0.2268150654321505 valid 0.20234554885613798
LOSS train 0.2268150654321505 valid 0.20245504304766654
LOSS train 0.2268150654321505 valid 0.2025112235310054
LOSS train 0.2268150654321505 valid 0.20285827695739037
LOSS train 0.2268150654321505 valid 0.20266075967584998
LOSS train 0.2268150654321505 valid 0.2025167979300022
LOSS train 0.2268150654321505 valid 0.20276603954178946
LOSS train 0.2268150654321505 valid 0.20294356444534264
LOSS train 0.2268150654321505 valid 0.20260752848932675
LOSS train 0.2268150654321505 valid 0.20281070650175767
LOSS train 0.2268150654321505 valid 0.20245935720041258
LOSS train 0.2268150654321505 valid 0.2023267843506553
LOSS train 0.2268150654321505 valid 0.20235750065730498
LOSS train 0.2268150654321505 valid 0.20264312478580646
LOSS train 0.2268150654321505 valid 0.20239336393048277
LOSS train 0.2268150654321505 valid 0.2024415216425009
LOSS train 0.2268150654321505 valid 0.20324060476344566
LOSS train 0.2268150654321505 valid 0.20303341900480204
LOSS train 0.2268150654321505 valid 0.2036492134261335
LOSS train 0.2268150654321505 valid 0.20343023728011017
LOSS train 0.2268150654321505 valid 0.20323866430450888
LOSS train 0.2268150654321505 valid 0.20286254982153576
LOSS train 0.2268150654321505 valid 0.2026378404010426
LOSS train 0.2268150654321505 valid 0.20286883242794726
LOSS train 0.2268150654321505 valid 0.20290406108871709
LOSS train 0.2268150654321505 valid 0.2030990608036518
LOSS train 0.2268150654321505 valid 0.202972372174263
LOSS train 0.2268150654321505 valid 0.20320459833693882
LOSS train 0.2268150654321505 valid 0.20311828192294112
LOSS train 0.2268150654321505 valid 0.20307024021167308
LOSS train 0.2268150654321505 valid 0.2028216778538948
LOSS train 0.2268150654321505 valid 0.20255940017791896
LOSS train 0.2268150654321505 valid 0.20267741780244666
LOSS train 0.2268150654321505 valid 0.20262793519280173
LOSS train 0.2268150654321505 valid 0.202655662383352
LOSS train 0.2268150654321505 valid 0.2028349850382378
LOSS train 0.2268150654321505 valid 0.2029120377920292
LOSS train 0.2268150654321505 valid 0.20280764240990667
LOSS train 0.2268150654321505 valid 0.2027714001218768
LOSS train 0.2268150654321505 valid 0.20277571570182193
LOSS train 0.2268150654321505 valid 0.20251826095066483
LOSS train 0.2268150654321505 valid 0.20243305510708265
LOSS train 0.2268150654321505 valid 0.20243763405803247
LOSS train 0.2268150654321505 valid 0.20274049410937536
LOSS train 0.2268150654321505 valid 0.20280273550457054
LOSS train 0.2268150654321505 valid 0.20291077086908949
LOSS train 0.2268150654321505 valid 0.20284577669768497
LOSS train 0.2268150654321505 valid 0.20284993242319316
LOSS train 0.2268150654321505 valid 0.2028797137088516
LOSS train 0.2268150654321505 valid 0.20279248542076833
LOSS train 0.2268150654321505 valid 0.20292974918480688
LOSS train 0.2268150654321505 valid 0.20296015044053395
LOSS train 0.2268150654321505 valid 0.20290035759376374
LOSS train 0.2268150654321505 valid 0.20290533992412843
LOSS train 0.2268150654321505 valid 0.202630286415418
LOSS train 0.2268150654321505 valid 0.202896588518248
LOSS train 0.2268150654321505 valid 0.20297533629402037
LOSS train 0.2268150654321505 valid 0.2030070216800922
LOSS train 0.2268150654321505 valid 0.20296120235494747
LOSS train 0.2268150654321505 valid 0.2032200163112411
LOSS train 0.2268150654321505 valid 0.20323615675827242
LOSS train 0.2268150654321505 valid 0.20350390700623394
LOSS train 0.2268150654321505 valid 0.2034332929931072
LOSS train 0.2268150654321505 valid 0.20348707053028506
LOSS train 0.2268150654321505 valid 0.20337219390035408
LOSS train 0.2268150654321505 valid 0.2034506600622724
LOSS train 0.2268150654321505 valid 0.20364006504868015
LOSS train 0.2268150654321505 valid 0.2035492592188249
LOSS train 0.2268150654321505 valid 0.20352837898417148
LOSS train 0.2268150654321505 valid 0.20332509146205016
LOSS train 0.2268150654321505 valid 0.2031629736254201
LOSS train 0.2268150654321505 valid 0.20333979033371982
LOSS train 0.2268150654321505 valid 0.2033060779522734
LOSS train 0.2268150654321505 valid 0.20335711339531942
LOSS train 0.2268150654321505 valid 0.20312240004884025
LOSS train 0.2268150654321505 valid 0.20352748610164928
LOSS train 0.2268150654321505 valid 0.20352349851812634
LOSS train 0.2268150654321505 valid 0.20345007374205373
LOSS train 0.2268150654321505 valid 0.20346536297919388
LOSS train 0.2268150654321505 valid 0.20356456910291415
LOSS train 0.2268150654321505 valid 0.20370177308607368
LOSS train 0.2268150654321505 valid 0.20359502244326805
LOSS train 0.2268150654321505 valid 0.20354471079881678
LOSS train 0.2268150654321505 valid 0.20334302879624314
LOSS train 0.2268150654321505 valid 0.20346413047912995
LOSS train 0.2268150654321505 valid 0.20336132034983323
LOSS train 0.2268150654321505 valid 0.20325891077518463
LOSS train 0.2268150654321505 valid 0.20347077239264724
LOSS train 0.2268150654321505 valid 0.20342128034581475
LOSS train 0.2268150654321505 valid 0.2035004217573937
LOSS train 0.2268150654321505 valid 0.20328781355625739
LOSS train 0.2268150654321505 valid 0.2032720148563385
LOSS train 0.2268150654321505 valid 0.2033425170089562
LOSS train 0.2268150654321505 valid 0.2033713711425662
LOSS train 0.2268150654321505 valid 0.20314552422632207
LOSS train 0.2268150654321505 valid 0.2030080225817936
LOSS train 0.2268150654321505 valid 0.20295140376457801
LOSS train 0.2268150654321505 valid 0.20299913384476487
LOSS train 0.2268150654321505 valid 0.2030051568437954
LOSS train 0.2268150654321505 valid 0.20315252169214112
LOSS train 0.2268150654321505 valid 0.20312178898696323
LOSS train 0.2268150654321505 valid 0.20325438633561135
LOSS train 0.2268150654321505 valid 0.2030449909209019
LOSS train 0.2268150654321505 valid 0.20301190188320556
LOSS train 0.2268150654321505 valid 0.2030906217732453
LOSS train 0.2268150654321505 valid 0.20280185648623636
LOSS train 0.2268150654321505 valid 0.2027058356418842
LOSS train 0.2268150654321505 valid 0.2027493549637424
LOSS train 0.2268150654321505 valid 0.2026935884198129
LOSS train 0.2268150654321505 valid 0.20255159242795065
LOSS train 0.2268150654321505 valid 0.20263585305670231
LOSS train 0.2268150654321505 valid 0.20276361817405336
LOSS train 0.2268150654321505 valid 0.20298990381272483
LOSS train 0.2268150654321505 valid 0.20283718217375143
LOSS train 0.2268150654321505 valid 0.20283419698336874
LOSS train 0.2268150654321505 valid 0.2028512956104546
LOSS train 0.2268150654321505 valid 0.20264018385909324
LOSS train 0.2268150654321505 valid 0.20243167380491892
LOSS train 0.2268150654321505 valid 0.20221546992728237
LOSS train 0.2268150654321505 valid 0.20223766565322876
LOSS train 0.2268150654321505 valid 0.20224341062922455
LOSS train 0.2268150654321505 valid 0.2023633377118544
LOSS train 0.2268150654321505 valid 0.2023308279152909
LOSS train 0.2268150654321505 valid 0.20246335353937234
LOSS train 0.2268150654321505 valid 0.20265430305570764
LOSS train 0.2268150654321505 valid 0.2025448146409222
LOSS train 0.2268150654321505 valid 0.20246496770117017
LOSS train 0.2268150654321505 valid 0.20232286389950102
LOSS train 0.2268150654321505 valid 0.20235603338821345
LOSS train 0.2268150654321505 valid 0.2022764714140641
LOSS train 0.2268150654321505 valid 0.20217303105316828
LOSS train 0.2268150654321505 valid 0.2021765559911728
LOSS train 0.2268150654321505 valid 0.20217672996706776
LOSS train 0.2268150654321505 valid 0.20226873669387965
LOSS train 0.2268150654321505 valid 0.20259532326025
LOSS train 0.2268150654321505 valid 0.2025648647776017
LOSS train 0.2268150654321505 valid 0.20268284570663533
LOSS train 0.2268150654321505 valid 0.2025211378300594
LOSS train 0.2268150654321505 valid 0.20257074256989524
LOSS train 0.2268150654321505 valid 0.20251346817787955
LOSS train 0.2268150654321505 valid 0.20249191612379322
LOSS train 0.2268150654321505 valid 0.20208710084358852
LOSS train 0.2268150654321505 valid 0.20191543323617753
LOSS train 0.2268150654321505 valid 0.20185278393020314
LOSS train 0.2268150654321505 valid 0.20164242392950096
LOSS train 0.2268150654321505 valid 0.20167555195874856
LOSS train 0.2268150654321505 valid 0.20182049499482524
LOSS train 0.2268150654321505 valid 0.20193760097026825
LOSS train 0.2268150654321505 valid 0.20198440925795058
LOSS train 0.2268150654321505 valid 0.20201218879270938
LOSS train 0.2268150654321505 valid 0.20180914171010136
LOSS train 0.2268150654321505 valid 0.20189670252799988
LOSS train 0.2268150654321505 valid 0.2019569272420321
LOSS train 0.2268150654321505 valid 0.20182840354622356
LOSS train 0.2268150654321505 valid 0.2016248714193525
LOSS train 0.2268150654321505 valid 0.20179198793773576
LOSS train 0.2268150654321505 valid 0.20171931362619586
LOSS train 0.2268150654321505 valid 0.20150022121379152
LOSS train 0.2268150654321505 valid 0.20146014892173647
LOSS train 0.2268150654321505 valid 0.20166764263958895
LOSS train 0.2268150654321505 valid 0.20156854022884
LOSS train 0.2268150654321505 valid 0.2014250669341821
LOSS train 0.2268150654321505 valid 0.20150061905155695
LOSS train 0.2268150654321505 valid 0.2017617943405195
LOSS train 0.2268150654321505 valid 0.2015606007421878
LOSS train 0.2268150654321505 valid 0.20164655002229143
LOSS train 0.2268150654321505 valid 0.20154538261440566
LOSS train 0.2268150654321505 valid 0.20162956982402874
LOSS train 0.2268150654321505 valid 0.20152091087026988
LOSS train 0.2268150654321505 valid 0.20161424165786201
LOSS train 0.2268150654321505 valid 0.20174014978249277
LOSS train 0.2268150654321505 valid 0.20163443011266213
LOSS train 0.2268150654321505 valid 0.20164967886416235
LOSS train 0.2268150654321505 valid 0.20152772930176818
LOSS train 0.2268150654321505 valid 0.20143999857998593
LOSS train 0.2268150654321505 valid 0.201334100787657
LOSS train 0.2268150654321505 valid 0.2013819307088852
LOSS train 0.2268150654321505 valid 0.2014189244288465
LOSS train 0.2268150654321505 valid 0.20146258885464513
LOSS train 0.2268150654321505 valid 0.20148916998141103
LOSS train 0.2268150654321505 valid 0.20140246555583025
LOSS train 0.2268150654321505 valid 0.20130287797323296
LOSS train 0.2268150654321505 valid 0.20111475550831426
LOSS train 0.2268150654321505 valid 0.20103816941697547
LOSS train 0.2268150654321505 valid 0.20105979009778255
LOSS train 0.2268150654321505 valid 0.20097045511217185
LOSS train 0.2268150654321505 valid 0.20108816560946013
LOSS train 0.2268150654321505 valid 0.2010519463386569
LOSS train 0.2268150654321505 valid 0.201023459278748
LOSS train 0.2268150654321505 valid 0.20094605659445128
LOSS train 0.2268150654321505 valid 0.20086019488766943
LOSS train 0.2268150654321505 valid 0.2007675631292935
LOSS train 0.2268150654321505 valid 0.2007474651451373
LOSS train 0.2268150654321505 valid 0.20070926365378786
LOSS train 0.2268150654321505 valid 0.2007084972012165
LOSS train 0.2268150654321505 valid 0.20070713905452872
LOSS train 0.2268150654321505 valid 0.20069363475856133
LOSS train 0.2268150654321505 valid 0.20068148241655245
LOSS train 0.2268150654321505 valid 0.2006016191509035
LOSS train 0.2268150654321505 valid 0.20068603824049033
LOSS train 0.2268150654321505 valid 0.20071440915200223
LOSS train 0.2268150654321505 valid 0.2007008276383082
LOSS train 0.2268150654321505 valid 0.2005821221393604
LOSS train 0.2268150654321505 valid 0.20055677524662965
LOSS train 0.2268150654321505 valid 0.20047962572118236
LOSS train 0.2268150654321505 valid 0.20049231440613144
LOSS train 0.2268150654321505 valid 0.2003942918093478
LOSS train 0.2268150654321505 valid 0.20067354846818775
LOSS train 0.2268150654321505 valid 0.20063486127395194
LOSS train 0.2268150654321505 valid 0.20064257172408043
LOSS train 0.2268150654321505 valid 0.20064851593431146
LOSS train 0.2268150654321505 valid 0.2006496014614259
LOSS train 0.2268150654321505 valid 0.20064503045519066
LOSS train 0.2268150654321505 valid 0.20058454434650067
LOSS train 0.2268150654321505 valid 0.20071868874584906
LOSS train 0.2268150654321505 valid 0.20069422335571543
LOSS train 0.2268150654321505 valid 0.20063400126638867
LOSS train 0.2268150654321505 valid 0.20063334651574305
LOSS train 0.2268150654321505 valid 0.20065958478285312
LOSS train 0.2268150654321505 valid 0.2005995343316276
LOSS train 0.2268150654321505 valid 0.20056195001243424
LOSS train 0.2268150654321505 valid 0.2004615219309926
LOSS train 0.2268150654321505 valid 0.20055987423639804
LOSS train 0.2268150654321505 valid 0.2005315169414378
LOSS train 0.2268150654321505 valid 0.2004078212352729
LOSS train 0.2268150654321505 valid 0.20040071295735276
LOSS train 0.2268150654321505 valid 0.20024831221653866
LOSS train 0.2268150654321505 valid 0.2002622880544399
LOSS train 0.2268150654321505 valid 0.2002034735515577
LOSS train 0.2268150654321505 valid 0.20018598096581494
LOSS train 0.2268150654321505 valid 0.20017818599305254
LOSS train 0.2268150654321505 valid 0.2001642319740671
LOSS train 0.2268150654321505 valid 0.20012524769025267
LOSS train 0.2268150654321505 valid 0.20011999812650394
LOSS train 0.2268150654321505 valid 0.20030773299055415
LOSS train 0.2268150654321505 valid 0.20022670416061036
LOSS train 0.2268150654321505 valid 0.20014170096881354
LOSS train 0.2268150654321505 valid 0.20004659123896135
LOSS train 0.2268150654321505 valid 0.2002178560113341
LOSS train 0.2268150654321505 valid 0.20018408465138554
LOSS train 0.2268150654321505 valid 0.20010812602563594
LOSS train 0.2268150654321505 valid 0.20020602678551394
LOSS train 0.2268150654321505 valid 0.2001441159555989
LOSS train 0.2268150654321505 valid 0.20010408467193794
LOSS train 0.2268150654321505 valid 0.20000887681721946
LOSS train 0.2268150654321505 valid 0.20010340989155825
LOSS train 0.2268150654321505 valid 0.20024380848027656
LOSS train 0.2268150654321505 valid 0.20020565295839585
LOSS train 0.2268150654321505 valid 0.2000846743841336
LOSS train 0.2268150654321505 valid 0.20015347222315855
LOSS train 0.2268150654321505 valid 0.2000911878395217
LOSS train 0.2268150654321505 valid 0.2001121368578502
LOSS train 0.2268150654321505 valid 0.20018260720108988
LOSS train 0.2268150654321505 valid 0.2002533728426153
LOSS train 0.2268150654321505 valid 0.200263886081936
LOSS train 0.2268150654321505 valid 0.20025641660569077
LOSS train 0.2268150654321505 valid 0.20011915883547823
LOSS train 0.2268150654321505 valid 0.20014129453495647
LOSS train 0.2268150654321505 valid 0.20018857704992055
LOSS train 0.2268150654321505 valid 0.2001294800855594
LOSS train 0.2268150654321505 valid 0.20007969430728212
LOSS train 0.2268150654321505 valid 0.2002919789403677
LOSS train 0.2268150654321505 valid 0.20020058107177968
LOSS train 0.2268150654321505 valid 0.20022842423856588
LOSS train 0.2268150654321505 valid 0.2002700905908238
LOSS train 0.2268150654321505 valid 0.20019884669518734
LOSS train 0.2268150654321505 valid 0.20025290908062296
LOSS train 0.2268150654321505 valid 0.20023289815487105
LOSS train 0.2268150654321505 valid 0.20017575596431295
LOSS train 0.2268150654321505 valid 0.20002864580601454
LOSS train 0.2268150654321505 valid 0.1999812849814976
EPOCH 25:
  batch 1 loss: 0.21312661468982697
  batch 2 loss: 0.25002699345350266
  batch 3 loss: 0.24131478369235992
  batch 4 loss: 0.24031642451882362
  batch 5 loss: 0.2426989942789078
  batch 6 loss: 0.23910739769538245
  batch 7 loss: 0.24036327643053873
  batch 8 loss: 0.23750421218574047
  batch 9 loss: 0.23925194972091252
  batch 10 loss: 0.2343908041715622
  batch 11 loss: 0.23391550372947345
  batch 12 loss: 0.23280334969361624
  batch 13 loss: 0.22838273071325743
  batch 14 loss: 0.22650856418269022
  batch 15 loss: 0.22667860786120098
  batch 16 loss: 0.22560701984912157
  batch 17 loss: 0.22549783131655524
  batch 18 loss: 0.22356604950295556
  batch 19 loss: 0.2232181280851364
  batch 20 loss: 0.22044553086161614
  batch 21 loss: 0.22331986469881876
  batch 22 loss: 0.22394820302724838
  batch 23 loss: 0.22476345689400382
  batch 24 loss: 0.22495579098661742
  batch 25 loss: 0.2257191550731659
  batch 26 loss: 0.2249983136470501
  batch 27 loss: 0.22514423617610224
  batch 28 loss: 0.2270933027778353
  batch 29 loss: 0.22718115818911586
  batch 30 loss: 0.22779916276534398
  batch 31 loss: 0.22772821351405112
  batch 32 loss: 0.2288587293587625
  batch 33 loss: 0.23073587860121872
  batch 34 loss: 0.23040084803805633
  batch 35 loss: 0.23222668681825911
  batch 36 loss: 0.23288659834199482
  batch 37 loss: 0.23371986037976034
  batch 38 loss: 0.23378625433695943
  batch 39 loss: 0.23420279224713644
  batch 40 loss: 0.23365280777215958
  batch 41 loss: 0.2335868372422893
  batch 42 loss: 0.23304773973567144
  batch 43 loss: 0.2334798300682112
  batch 44 loss: 0.23416962576183406
  batch 45 loss: 0.23378415803114574
  batch 46 loss: 0.232292505386083
  batch 47 loss: 0.2318521982177775
  batch 48 loss: 0.2316354907428225
  batch 49 loss: 0.23226406653316653
  batch 50 loss: 0.23360454112291337
  batch 51 loss: 0.2331452057057736
  batch 52 loss: 0.2330652836423654
  batch 53 loss: 0.2326141601463534
  batch 54 loss: 0.23271891585102789
  batch 55 loss: 0.2327545003457503
  batch 56 loss: 0.23311098931091173
  batch 57 loss: 0.23251191461295412
  batch 58 loss: 0.23250303468827543
  batch 59 loss: 0.23246592611579572
  batch 60 loss: 0.23293576265374819
  batch 61 loss: 0.23342697029231024
  batch 62 loss: 0.23384372144937515
  batch 63 loss: 0.23338458367756434
  batch 64 loss: 0.23281186004169285
  batch 65 loss: 0.23236104800150945
  batch 66 loss: 0.23227091517412302
  batch 67 loss: 0.2317128837553423
  batch 68 loss: 0.2317986477385549
  batch 69 loss: 0.23220309334388678
  batch 70 loss: 0.23210716822317667
  batch 71 loss: 0.23245822522841708
  batch 72 loss: 0.23261576063103145
  batch 73 loss: 0.232579330467198
  batch 74 loss: 0.23260405699949008
  batch 75 loss: 0.23271262645721436
  batch 76 loss: 0.233282002571382
  batch 77 loss: 0.23318723850436024
  batch 78 loss: 0.23318055826119888
  batch 79 loss: 0.23326614291607578
  batch 80 loss: 0.2335904011502862
  batch 81 loss: 0.23347870527226247
  batch 82 loss: 0.23366463856726158
  batch 83 loss: 0.2332170810326036
  batch 84 loss: 0.2326254862405005
  batch 85 loss: 0.2330288680160747
  batch 86 loss: 0.23288267001856205
  batch 87 loss: 0.23279743735817657
  batch 88 loss: 0.23251540958881378
  batch 89 loss: 0.23215495368068137
  batch 90 loss: 0.23223453594578636
  batch 91 loss: 0.23194644051593738
  batch 92 loss: 0.2320134960438894
  batch 93 loss: 0.23151058359171756
  batch 94 loss: 0.23130268064585138
  batch 95 loss: 0.23119569812950336
  batch 96 loss: 0.23107627825811505
  batch 97 loss: 0.23160092809151128
  batch 98 loss: 0.2313993126153946
  batch 99 loss: 0.23118150715876107
  batch 100 loss: 0.23138587296009064
  batch 101 loss: 0.23131305702251964
  batch 102 loss: 0.23153143171586243
  batch 103 loss: 0.23128157087321422
  batch 104 loss: 0.23139891400933266
  batch 105 loss: 0.23095025789170037
  batch 106 loss: 0.2309485853842969
  batch 107 loss: 0.23089950538684276
  batch 108 loss: 0.23041994842114272
  batch 109 loss: 0.23063240904326832
  batch 110 loss: 0.2307401486418464
  batch 111 loss: 0.23085406046729903
  batch 112 loss: 0.23085446868624007
  batch 113 loss: 0.2308297154650224
  batch 114 loss: 0.23081171852454804
  batch 115 loss: 0.23042220162308735
  batch 116 loss: 0.2301930350990131
  batch 117 loss: 0.23003923026924458
  batch 118 loss: 0.230114114360284
  batch 119 loss: 0.23022778632761048
  batch 120 loss: 0.23019695058465003
  batch 121 loss: 0.23006640953465926
  batch 122 loss: 0.22987813319339126
  batch 123 loss: 0.22952973345915476
  batch 124 loss: 0.22948880529692095
  batch 125 loss: 0.22906772327423094
  batch 126 loss: 0.22902541997886838
  batch 127 loss: 0.22922505989788086
  batch 128 loss: 0.22920395829714835
  batch 129 loss: 0.2294057857158572
  batch 130 loss: 0.22937796803621147
  batch 131 loss: 0.2293596049301497
  batch 132 loss: 0.22950040978012662
  batch 133 loss: 0.2294206428796725
  batch 134 loss: 0.22934701860840642
  batch 135 loss: 0.2291431427001953
  batch 136 loss: 0.2294270058765131
  batch 137 loss: 0.22968965486018328
  batch 138 loss: 0.22973358447569003
  batch 139 loss: 0.22951115925106214
  batch 140 loss: 0.2298070629792554
  batch 141 loss: 0.22953425907919592
  batch 142 loss: 0.22931138237177487
  batch 143 loss: 0.22941620418658623
  batch 144 loss: 0.2293802582555347
  batch 145 loss: 0.2293781542572482
  batch 146 loss: 0.22955225438696064
  batch 147 loss: 0.2296159994237277
  batch 148 loss: 0.229695518274565
  batch 149 loss: 0.2293738476782037
  batch 150 loss: 0.22935909241437913
  batch 151 loss: 0.2290337286050746
  batch 152 loss: 0.2288418609256807
  batch 153 loss: 0.22878065461816352
  batch 154 loss: 0.2286060077996997
  batch 155 loss: 0.22853044925197477
  batch 156 loss: 0.22865823560800308
  batch 157 loss: 0.22868021620307
  batch 158 loss: 0.22868671796367138
  batch 159 loss: 0.22884582957756594
  batch 160 loss: 0.22884009508416056
  batch 161 loss: 0.22868184810099396
  batch 162 loss: 0.22867883843036346
  batch 163 loss: 0.22853519552690119
  batch 164 loss: 0.2286633378485354
  batch 165 loss: 0.2289368692672614
  batch 166 loss: 0.22924377221659006
  batch 167 loss: 0.2291467810819249
  batch 168 loss: 0.2289456563691298
  batch 169 loss: 0.22890536107960538
  batch 170 loss: 0.22891874769154716
  batch 171 loss: 0.22898646683720816
  batch 172 loss: 0.22882480624803278
  batch 173 loss: 0.22869630993446174
  batch 174 loss: 0.22881947012468315
  batch 175 loss: 0.22893919042178562
  batch 176 loss: 0.2292384495112029
  batch 177 loss: 0.22948512047697595
  batch 178 loss: 0.22940382294440537
  batch 179 loss: 0.22940933379714049
  batch 180 loss: 0.22938090273075634
  batch 181 loss: 0.22935189207943762
  batch 182 loss: 0.22927966419157092
  batch 183 loss: 0.2293327336754304
  batch 184 loss: 0.22936098625802476
  batch 185 loss: 0.2291826476116438
  batch 186 loss: 0.22917359194127462
  batch 187 loss: 0.22943098811223545
  batch 188 loss: 0.22933532464060377
  batch 189 loss: 0.22942065498816272
  batch 190 loss: 0.22925478235671395
  batch 191 loss: 0.22929769442343587
  batch 192 loss: 0.22909921260240176
  batch 193 loss: 0.22905680313320356
  batch 194 loss: 0.22905747431147958
  batch 195 loss: 0.22925399671762417
  batch 196 loss: 0.2291102864000262
  batch 197 loss: 0.22917306559339998
  batch 198 loss: 0.2290969980184478
  batch 199 loss: 0.22922261665813887
  batch 200 loss: 0.2292096894979477
  batch 201 loss: 0.22911141597809484
  batch 202 loss: 0.22925138060409245
  batch 203 loss: 0.22931050468841796
  batch 204 loss: 0.2295083155965104
  batch 205 loss: 0.22951588107318413
  batch 206 loss: 0.22952737015427896
  batch 207 loss: 0.22961853379788605
  batch 208 loss: 0.22945955438682666
  batch 209 loss: 0.22945072683706239
  batch 210 loss: 0.22946793550536745
  batch 211 loss: 0.22935431581255383
  batch 212 loss: 0.22940298479120685
  batch 213 loss: 0.22928256915768547
  batch 214 loss: 0.22915528032267204
  batch 215 loss: 0.2291251563748648
  batch 216 loss: 0.22909936543416093
  batch 217 loss: 0.2290939828767205
  batch 218 loss: 0.2288810837022755
  batch 219 loss: 0.22855168125128636
  batch 220 loss: 0.22859219597144562
  batch 221 loss: 0.22846389120250807
  batch 222 loss: 0.2284858806041984
  batch 223 loss: 0.22837410660068017
  batch 224 loss: 0.22834694405485476
  batch 225 loss: 0.22837585720751022
  batch 226 loss: 0.22833142812009408
  batch 227 loss: 0.22834702807638613
  batch 228 loss: 0.22832380222123966
  batch 229 loss: 0.22846318319374817
  batch 230 loss: 0.228472090156182
  batch 231 loss: 0.22854765978726474
  batch 232 loss: 0.22855826666385964
  batch 233 loss: 0.22866012769963096
  batch 234 loss: 0.22863220181475338
  batch 235 loss: 0.2285347135143077
  batch 236 loss: 0.22845761155930616
  batch 237 loss: 0.22836510522707607
  batch 238 loss: 0.22834888783072224
  batch 239 loss: 0.22828465631566786
  batch 240 loss: 0.22830794497082632
  batch 241 loss: 0.22823189198970795
  batch 242 loss: 0.2280930284995678
  batch 243 loss: 0.22805924732008098
  batch 244 loss: 0.22797482882122524
  batch 245 loss: 0.22785888922457792
  batch 246 loss: 0.22773604512941548
  batch 247 loss: 0.22782866837766005
  batch 248 loss: 0.22795313867109437
  batch 249 loss: 0.22798430949569226
  batch 250 loss: 0.2279697809815407
  batch 251 loss: 0.2279052784243428
  batch 252 loss: 0.22787256426517927
  batch 253 loss: 0.22782822984009393
  batch 254 loss: 0.22786925756555843
  batch 255 loss: 0.22777836322784423
  batch 256 loss: 0.22776364663150162
  batch 257 loss: 0.2278676699803497
  batch 258 loss: 0.2278068886362305
  batch 259 loss: 0.22770132584691508
  batch 260 loss: 0.22770914441117873
  batch 261 loss: 0.2277469708659183
  batch 262 loss: 0.2276126720750605
  batch 263 loss: 0.22749180377889494
  batch 264 loss: 0.2275131962290316
  batch 265 loss: 0.22737881946113875
  batch 266 loss: 0.22727560968999577
  batch 267 loss: 0.22708320980438132
  batch 268 loss: 0.22695186539594805
  batch 269 loss: 0.2270186424920107
  batch 270 loss: 0.22695587614068277
  batch 271 loss: 0.2269341296265486
  batch 272 loss: 0.22683913249741583
  batch 273 loss: 0.22673012845682136
  batch 274 loss: 0.22660919882520272
  batch 275 loss: 0.22655899985270067
  batch 276 loss: 0.22647043598302896
  batch 277 loss: 0.22648493192471322
  batch 278 loss: 0.22635975989291993
  batch 279 loss: 0.2263438528049804
  batch 280 loss: 0.22635359008397374
  batch 281 loss: 0.22625737852049044
  batch 282 loss: 0.2260946515181386
  batch 283 loss: 0.2260226868487921
  batch 284 loss: 0.22602013582494898
  batch 285 loss: 0.2259877129604942
  batch 286 loss: 0.22588816378916893
  batch 287 loss: 0.22575223217442475
  batch 288 loss: 0.22574824886396527
  batch 289 loss: 0.2256616913426706
  batch 290 loss: 0.22564323102605754
  batch 291 loss: 0.22566986462914249
  batch 292 loss: 0.2256839346804031
  batch 293 loss: 0.22564340146328402
  batch 294 loss: 0.22540656330228662
  batch 295 loss: 0.225420645232928
  batch 296 loss: 0.22541900120071462
  batch 297 loss: 0.2253873748610718
  batch 298 loss: 0.2252977869954685
  batch 299 loss: 0.22538803238733157
  batch 300 loss: 0.22529131397604943
  batch 301 loss: 0.2252435633213417
  batch 302 loss: 0.22547603181458467
  batch 303 loss: 0.22543754185190296
  batch 304 loss: 0.22551332963140389
  batch 305 loss: 0.2254906436458963
  batch 306 loss: 0.2256105397067039
  batch 307 loss: 0.22569147713409574
  batch 308 loss: 0.2257388759549562
  batch 309 loss: 0.22576692251904498
  batch 310 loss: 0.22591242545074033
  batch 311 loss: 0.2259042965445871
  batch 312 loss: 0.2259161857267221
  batch 313 loss: 0.2259072273874435
  batch 314 loss: 0.22591815314664962
  batch 315 loss: 0.2258902212930104
  batch 316 loss: 0.22594522788554808
  batch 317 loss: 0.22598241199078245
  batch 318 loss: 0.22583204025180084
  batch 319 loss: 0.22572321904864057
  batch 320 loss: 0.22566018868237733
  batch 321 loss: 0.22573144710695262
  batch 322 loss: 0.22570522573793897
  batch 323 loss: 0.22560315373881312
  batch 324 loss: 0.22557868366992032
  batch 325 loss: 0.22574337509962228
  batch 326 loss: 0.22589908878496087
  batch 327 loss: 0.22596203542630608
  batch 328 loss: 0.22594333172026204
  batch 329 loss: 0.22595150832166064
  batch 330 loss: 0.22596170884190184
  batch 331 loss: 0.22595775510069108
  batch 332 loss: 0.22610801768051572
  batch 333 loss: 0.22610371602369142
  batch 334 loss: 0.22642992298581643
  batch 335 loss: 0.22658510408294735
  batch 336 loss: 0.22667941890124763
  batch 337 loss: 0.22679565731955564
  batch 338 loss: 0.2267908219843221
  batch 339 loss: 0.22682469221694632
  batch 340 loss: 0.2266977222088505
  batch 341 loss: 0.2266744581922408
  batch 342 loss: 0.2265982549441488
  batch 343 loss: 0.22664410605722543
  batch 344 loss: 0.22665307835437531
  batch 345 loss: 0.2267292813114498
  batch 346 loss: 0.22677540064202567
  batch 347 loss: 0.22666598585909312
  batch 348 loss: 0.22665664630717244
  batch 349 loss: 0.2266762798016939
  batch 350 loss: 0.22687288343906403
  batch 351 loss: 0.22689726893548612
  batch 352 loss: 0.22680849878286774
  batch 353 loss: 0.22677557288587263
  batch 354 loss: 0.226691710283864
  batch 355 loss: 0.2265731456414075
  batch 356 loss: 0.22675350584675757
  batch 357 loss: 0.22657333310244798
  batch 358 loss: 0.22645495940187124
  batch 359 loss: 0.22640520043691884
  batch 360 loss: 0.22632691065470378
  batch 361 loss: 0.22625330098778257
  batch 362 loss: 0.2262870171287442
  batch 363 loss: 0.2262820855563009
  batch 364 loss: 0.2261747481240021
  batch 365 loss: 0.226215738792942
  batch 366 loss: 0.22620037678486662
  batch 367 loss: 0.22622808769385885
  batch 368 loss: 0.2262887513750921
  batch 369 loss: 0.2262571988309302
  batch 370 loss: 0.22626046953169074
  batch 371 loss: 0.22625955505512474
  batch 372 loss: 0.22630904194328091
  batch 373 loss: 0.22632698435086668
  batch 374 loss: 0.2262260881177882
  batch 375 loss: 0.22634881854057312
  batch 376 loss: 0.22646725867339906
  batch 377 loss: 0.22644069759219648
  batch 378 loss: 0.22647184519856065
  batch 379 loss: 0.2264745881422214
  batch 380 loss: 0.22642535979026243
  batch 381 loss: 0.22632152104158726
  batch 382 loss: 0.22635868499884432
  batch 383 loss: 0.22629079965791876
  batch 384 loss: 0.2262881841355314
  batch 385 loss: 0.226199828881722
  batch 386 loss: 0.22618982539430182
  batch 387 loss: 0.22619136378592608
  batch 388 loss: 0.2261101419547784
  batch 389 loss: 0.22606613580855736
  batch 390 loss: 0.2260589171678592
  batch 391 loss: 0.22597822059145975
  batch 392 loss: 0.22601193862454017
  batch 393 loss: 0.22600304356209802
  batch 394 loss: 0.2260495606185821
  batch 395 loss: 0.22614009195490728
  batch 396 loss: 0.22612070807754392
  batch 397 loss: 0.22624980055865473
  batch 398 loss: 0.22631119390079124
  batch 399 loss: 0.22632869812928347
  batch 400 loss: 0.2264043704047799
  batch 401 loss: 0.22645855611399224
  batch 402 loss: 0.2265115591201616
  batch 403 loss: 0.22651452369636696
  batch 404 loss: 0.2264928932827298
  batch 405 loss: 0.22653925646970302
  batch 406 loss: 0.22643488043634763
  batch 407 loss: 0.22649356796056105
  batch 408 loss: 0.22642760143122254
  batch 409 loss: 0.22642600448906858
  batch 410 loss: 0.226313552769219
  batch 411 loss: 0.22628501637718684
  batch 412 loss: 0.2263061957958254
  batch 413 loss: 0.2262633323092149
  batch 414 loss: 0.22627594159997028
  batch 415 loss: 0.2262103184878108
  batch 416 loss: 0.22619616920844868
  batch 417 loss: 0.22614599303375904
  batch 418 loss: 0.2261167234923851
  batch 419 loss: 0.22615335707448264
  batch 420 loss: 0.22614304171431632
  batch 421 loss: 0.22622559731051928
  batch 422 loss: 0.22617921953517678
  batch 423 loss: 0.2261937462832629
  batch 424 loss: 0.22627912721825097
  batch 425 loss: 0.2262601173625273
  batch 426 loss: 0.22623648029257995
  batch 427 loss: 0.22629106145971553
  batch 428 loss: 0.22635083203421574
  batch 429 loss: 0.2262728740146388
  batch 430 loss: 0.22633735994959986
  batch 431 loss: 0.22633656586267556
  batch 432 loss: 0.22638661769667157
  batch 433 loss: 0.22635828605562655
  batch 434 loss: 0.22623769265990104
  batch 435 loss: 0.2262764700527849
  batch 436 loss: 0.22619961725052343
  batch 437 loss: 0.22611717105730175
  batch 438 loss: 0.22606815404543593
  batch 439 loss: 0.22597338320318278
  batch 440 loss: 0.2260166206143119
  batch 441 loss: 0.22597784077634617
  batch 442 loss: 0.22593255162373926
  batch 443 loss: 0.22591695973216547
  batch 444 loss: 0.22592932791323275
  batch 445 loss: 0.22587390217218506
  batch 446 loss: 0.22590870517118095
  batch 447 loss: 0.22588022796632992
  batch 448 loss: 0.2259896585185613
  batch 449 loss: 0.22590559209906444
  batch 450 loss: 0.2258799992667304
  batch 451 loss: 0.22582983732752154
  batch 452 loss: 0.22575345213434336
  batch 453 loss: 0.2257092386220991
  batch 454 loss: 0.22573058963609688
  batch 455 loss: 0.22571216143749573
  batch 456 loss: 0.225643362392459
  batch 457 loss: 0.22560540800282416
  batch 458 loss: 0.225579918602148
  batch 459 loss: 0.2255734010784195
  batch 460 loss: 0.22557085622911868
  batch 461 loss: 0.22558336373652915
  batch 462 loss: 0.22562782898609773
  batch 463 loss: 0.225652335529451
  batch 464 loss: 0.22561901611886148
  batch 465 loss: 0.22564162294710835
  batch 466 loss: 0.22556677361542574
  batch 467 loss: 0.22570061993164986
  batch 468 loss: 0.2256166190864184
  batch 469 loss: 0.2254941832663408
  batch 470 loss: 0.22556567750078566
  batch 471 loss: 0.2256342132126956
  batch 472 loss: 0.22546848395870903
LOSS train 0.22546848395870903 valid 0.22521819174289703
LOSS train 0.22546848395870903 valid 0.21463808417320251
LOSS train 0.22546848395870903 valid 0.22940727074941
LOSS train 0.22546848395870903 valid 0.21702974662184715
LOSS train 0.22546848395870903 valid 0.21217587888240813
LOSS train 0.22546848395870903 valid 0.21319532146056494
LOSS train 0.22546848395870903 valid 0.2078542092016765
LOSS train 0.22546848395870903 valid 0.20421379804611206
LOSS train 0.22546848395870903 valid 0.20417651699648964
LOSS train 0.22546848395870903 valid 0.20168643444776535
LOSS train 0.22546848395870903 valid 0.20110589536753568
LOSS train 0.22546848395870903 valid 0.20373651136954626
LOSS train 0.22546848395870903 valid 0.20330029267531174
LOSS train 0.22546848395870903 valid 0.20147835782596044
LOSS train 0.22546848395870903 valid 0.2018434812625249
LOSS train 0.22546848395870903 valid 0.20606973860412836
LOSS train 0.22546848395870903 valid 0.20609735040103688
LOSS train 0.22546848395870903 valid 0.20604155212640762
LOSS train 0.22546848395870903 valid 0.20952493187628293
LOSS train 0.22546848395870903 valid 0.20882954373955726
LOSS train 0.22546848395870903 valid 0.2118120172194072
LOSS train 0.22546848395870903 valid 0.21122920174490323
LOSS train 0.22546848395870903 valid 0.2093232267576715
LOSS train 0.22546848395870903 valid 0.20956368806461492
LOSS train 0.22546848395870903 valid 0.20959999680519104
LOSS train 0.22546848395870903 valid 0.20870170856897646
LOSS train 0.22546848395870903 valid 0.20860362494433368
LOSS train 0.22546848395870903 valid 0.20914460665413312
LOSS train 0.22546848395870903 valid 0.208246690959766
LOSS train 0.22546848395870903 valid 0.2079104041059812
LOSS train 0.22546848395870903 valid 0.2081871662409075
LOSS train 0.22546848395870903 valid 0.2082906891591847
LOSS train 0.22546848395870903 valid 0.20690332398270117
LOSS train 0.22546848395870903 valid 0.20675037012380712
LOSS train 0.22546848395870903 valid 0.20749245924609047
LOSS train 0.22546848395870903 valid 0.2083985884156492
LOSS train 0.22546848395870903 valid 0.20919017775638685
LOSS train 0.22546848395870903 valid 0.20930209206907371
LOSS train 0.22546848395870903 valid 0.2100855860954676
LOSS train 0.22546848395870903 valid 0.21032897308468818
LOSS train 0.22546848395870903 valid 0.21012284842933096
LOSS train 0.22546848395870903 valid 0.21132833617074148
LOSS train 0.22546848395870903 valid 0.21196180893931277
LOSS train 0.22546848395870903 valid 0.21177313917062499
LOSS train 0.22546848395870903 valid 0.21113887329896291
LOSS train 0.22546848395870903 valid 0.21073040949261707
LOSS train 0.22546848395870903 valid 0.21039706278354564
LOSS train 0.22546848395870903 valid 0.2117267350355784
LOSS train 0.22546848395870903 valid 0.2107469041128548
LOSS train 0.22546848395870903 valid 0.2114220067858696
LOSS train 0.22546848395870903 valid 0.2112514218863319
LOSS train 0.22546848395870903 valid 0.21097643128954446
LOSS train 0.22546848395870903 valid 0.21216947993017593
LOSS train 0.22546848395870903 valid 0.21243131712630944
LOSS train 0.22546848395870903 valid 0.2122144501317631
LOSS train 0.22546848395870903 valid 0.21230466424354485
LOSS train 0.22546848395870903 valid 0.2117882174880881
LOSS train 0.22546848395870903 valid 0.21224202755196342
LOSS train 0.22546848395870903 valid 0.2118764499486503
LOSS train 0.22546848395870903 valid 0.211702166001002
LOSS train 0.22546848395870903 valid 0.21170629341094221
LOSS train 0.22546848395870903 valid 0.2118357984769729
LOSS train 0.22546848395870903 valid 0.21138304069874778
LOSS train 0.22546848395870903 valid 0.2118198520038277
LOSS train 0.22546848395870903 valid 0.21074591141480667
LOSS train 0.22546848395870903 valid 0.21053760476184613
LOSS train 0.22546848395870903 valid 0.21108707919049619
LOSS train 0.22546848395870903 valid 0.2104794727090527
LOSS train 0.22546848395870903 valid 0.2111871957347013
LOSS train 0.22546848395870903 valid 0.21172195587839399
LOSS train 0.22546848395870903 valid 0.2123823300213881
LOSS train 0.22546848395870903 valid 0.21314282715320587
LOSS train 0.22546848395870903 valid 0.21379623511066176
LOSS train 0.22546848395870903 valid 0.21338267825745247
LOSS train 0.22546848395870903 valid 0.2130527619520823
LOSS train 0.22546848395870903 valid 0.21324530557582252
LOSS train 0.22546848395870903 valid 0.21308035277701043
LOSS train 0.22546848395870903 valid 0.21307437102764082
LOSS train 0.22546848395870903 valid 0.21304521172107022
LOSS train 0.22546848395870903 valid 0.21272214725613595
LOSS train 0.22546848395870903 valid 0.21305092322973557
LOSS train 0.22546848395870903 valid 0.21291294966529056
LOSS train 0.22546848395870903 valid 0.21315082620425396
LOSS train 0.22546848395870903 valid 0.21280860688005174
LOSS train 0.22546848395870903 valid 0.21344269654330086
LOSS train 0.22546848395870903 valid 0.2133739426038986
LOSS train 0.22546848395870903 valid 0.21300008601840886
LOSS train 0.22546848395870903 valid 0.21340801939368248
LOSS train 0.22546848395870903 valid 0.21377412790662786
LOSS train 0.22546848395870903 valid 0.21431006491184235
LOSS train 0.22546848395870903 valid 0.21422795541993864
LOSS train 0.22546848395870903 valid 0.21443649268020754
LOSS train 0.22546848395870903 valid 0.2142006154021909
LOSS train 0.22546848395870903 valid 0.21466944049647513
LOSS train 0.22546848395870903 valid 0.21485273571390856
LOSS train 0.22546848395870903 valid 0.21516160868729153
LOSS train 0.22546848395870903 valid 0.21518915446148706
LOSS train 0.22546848395870903 valid 0.21568434503005476
LOSS train 0.22546848395870903 valid 0.21585526324883855
LOSS train 0.22546848395870903 valid 0.21604154542088508
LOSS train 0.22546848395870903 valid 0.21606364919997678
LOSS train 0.22546848395870903 valid 0.21635428085631014
LOSS train 0.22546848395870903 valid 0.21614409245333624
LOSS train 0.22546848395870903 valid 0.21602056462031144
LOSS train 0.22546848395870903 valid 0.21634398712998346
LOSS train 0.22546848395870903 valid 0.21655994808336473
LOSS train 0.22546848395870903 valid 0.2162529167449363
LOSS train 0.22546848395870903 valid 0.21639201596931176
LOSS train 0.22546848395870903 valid 0.21609612451780827
LOSS train 0.22546848395870903 valid 0.2159834405237978
LOSS train 0.22546848395870903 valid 0.21607557999658156
LOSS train 0.22546848395870903 valid 0.21634073888084718
LOSS train 0.22546848395870903 valid 0.21610441843492795
LOSS train 0.22546848395870903 valid 0.21623653735507997
LOSS train 0.22546848395870903 valid 0.2170901797387911
LOSS train 0.22546848395870903 valid 0.2168024674314877
LOSS train 0.22546848395870903 valid 0.21747489757517463
LOSS train 0.22546848395870903 valid 0.2172984854649689
LOSS train 0.22546848395870903 valid 0.21706116487499044
LOSS train 0.22546848395870903 valid 0.21662340375284353
LOSS train 0.22546848395870903 valid 0.2163398017075436
LOSS train 0.22546848395870903 valid 0.2165485062315816
LOSS train 0.22546848395870903 valid 0.2166322936129764
LOSS train 0.22546848395870903 valid 0.21682352632764848
LOSS train 0.22546848395870903 valid 0.21671609032154082
LOSS train 0.22546848395870903 valid 0.21702070687971417
LOSS train 0.22546848395870903 valid 0.2168706954698863
LOSS train 0.22546848395870903 valid 0.21681799506768584
LOSS train 0.22546848395870903 valid 0.21654231338075888
LOSS train 0.22546848395870903 valid 0.2162277456659537
LOSS train 0.22546848395870903 valid 0.21628858285550853
LOSS train 0.22546848395870903 valid 0.21623511375351387
LOSS train 0.22546848395870903 valid 0.21621180308940716
LOSS train 0.22546848395870903 valid 0.21642772264000196
LOSS train 0.22546848395870903 valid 0.2164803327233703
LOSS train 0.22546848395870903 valid 0.2163733707412201
LOSS train 0.22546848395870903 valid 0.21637611558837613
LOSS train 0.22546848395870903 valid 0.21635799606641135
LOSS train 0.22546848395870903 valid 0.2161069272233428
LOSS train 0.22546848395870903 valid 0.21603896554027285
LOSS train 0.22546848395870903 valid 0.21604872088060312
LOSS train 0.22546848395870903 valid 0.21635365444169918
LOSS train 0.22546848395870903 valid 0.21646742220525142
LOSS train 0.22546848395870903 valid 0.2165853221797281
LOSS train 0.22546848395870903 valid 0.21649010099213697
LOSS train 0.22546848395870903 valid 0.21649799689854662
LOSS train 0.22546848395870903 valid 0.21654588541611522
LOSS train 0.22546848395870903 valid 0.21648319498509974
LOSS train 0.22546848395870903 valid 0.21665103363510746
LOSS train 0.22546848395870903 valid 0.21666876673698426
LOSS train 0.22546848395870903 valid 0.21652668329658886
LOSS train 0.22546848395870903 valid 0.2165673783931293
LOSS train 0.22546848395870903 valid 0.21634004873777526
LOSS train 0.22546848395870903 valid 0.2165924319392675
LOSS train 0.22546848395870903 valid 0.21669047638293235
LOSS train 0.22546848395870903 valid 0.21674839149300867
LOSS train 0.22546848395870903 valid 0.21667342077774607
LOSS train 0.22546848395870903 valid 0.21693299889941758
LOSS train 0.22546848395870903 valid 0.21701079400830298
LOSS train 0.22546848395870903 valid 0.2172517476603389
LOSS train 0.22546848395870903 valid 0.21717648272928985
LOSS train 0.22546848395870903 valid 0.21724163326952192
LOSS train 0.22546848395870903 valid 0.2170728605949074
LOSS train 0.22546848395870903 valid 0.21720657915603825
LOSS train 0.22546848395870903 valid 0.21738932223031016
LOSS train 0.22546848395870903 valid 0.21733581894133464
LOSS train 0.22546848395870903 valid 0.21733656918217323
LOSS train 0.22546848395870903 valid 0.21712300695833706
LOSS train 0.22546848395870903 valid 0.21697940523102438
LOSS train 0.22546848395870903 valid 0.21715090467649348
LOSS train 0.22546848395870903 valid 0.2171045241125843
LOSS train 0.22546848395870903 valid 0.2171771569653999
LOSS train 0.22546848395870903 valid 0.21696983863508082
LOSS train 0.22546848395870903 valid 0.21732769641040386
LOSS train 0.22546848395870903 valid 0.21727711507252284
LOSS train 0.22546848395870903 valid 0.21724237730218607
LOSS train 0.22546848395870903 valid 0.21730003173405169
LOSS train 0.22546848395870903 valid 0.21738668514436552
LOSS train 0.22546848395870903 valid 0.21752667651829108
LOSS train 0.22546848395870903 valid 0.2174251555568642
LOSS train 0.22546848395870903 valid 0.2173597338449889
LOSS train 0.22546848395870903 valid 0.21709490780319488
LOSS train 0.22546848395870903 valid 0.21722164855954426
LOSS train 0.22546848395870903 valid 0.21713785721879938
LOSS train 0.22546848395870903 valid 0.21700209556399164
LOSS train 0.22546848395870903 valid 0.21722029886579
LOSS train 0.22546848395870903 valid 0.21718186713794974
LOSS train 0.22546848395870903 valid 0.21729590165171217
LOSS train 0.22546848395870903 valid 0.21702639127849901
LOSS train 0.22546848395870903 valid 0.2170489255534975
LOSS train 0.22546848395870903 valid 0.2171811311656892
LOSS train 0.22546848395870903 valid 0.21717472498615584
LOSS train 0.22546848395870903 valid 0.21689313565202328
LOSS train 0.22546848395870903 valid 0.21677616538153482
LOSS train 0.22546848395870903 valid 0.21666524647138058
LOSS train 0.22546848395870903 valid 0.21667750430654506
LOSS train 0.22546848395870903 valid 0.21665272010764494
LOSS train 0.22546848395870903 valid 0.21681636632090867
LOSS train 0.22546848395870903 valid 0.21678891605767772
LOSS train 0.22546848395870903 valid 0.21695711709558962
LOSS train 0.22546848395870903 valid 0.21675637490417235
LOSS train 0.22546848395870903 valid 0.21675188736160203
LOSS train 0.22546848395870903 valid 0.21678741034028565
LOSS train 0.22546848395870903 valid 0.21647829182592093
LOSS train 0.22546848395870903 valid 0.21636833374093217
LOSS train 0.22546848395870903 valid 0.21642118721332365
LOSS train 0.22546848395870903 valid 0.2163596190692146
LOSS train 0.22546848395870903 valid 0.21617901468506226
LOSS train 0.22546848395870903 valid 0.2162617012216714
LOSS train 0.22546848395870903 valid 0.216391098570256
LOSS train 0.22546848395870903 valid 0.21663025904323252
LOSS train 0.22546848395870903 valid 0.2164786934571446
LOSS train 0.22546848395870903 valid 0.21648586453966132
LOSS train 0.22546848395870903 valid 0.21650470166562874
LOSS train 0.22546848395870903 valid 0.2163355803766916
LOSS train 0.22546848395870903 valid 0.2161645309102756
LOSS train 0.22546848395870903 valid 0.21596837174233205
LOSS train 0.22546848395870903 valid 0.21603870501212025
LOSS train 0.22546848395870903 valid 0.21606556717391429
LOSS train 0.22546848395870903 valid 0.21615743013945493
LOSS train 0.22546848395870903 valid 0.2161533926946545
LOSS train 0.22546848395870903 valid 0.21627466541689797
LOSS train 0.22546848395870903 valid 0.21646117602763154
LOSS train 0.22546848395870903 valid 0.21633767496262277
LOSS train 0.22546848395870903 valid 0.21624511579672495
LOSS train 0.22546848395870903 valid 0.21613030016949747
LOSS train 0.22546848395870903 valid 0.2161505427129468
LOSS train 0.22546848395870903 valid 0.21607840289933639
LOSS train 0.22546848395870903 valid 0.2159460172642787
LOSS train 0.22546848395870903 valid 0.21594369061615157
LOSS train 0.22546848395870903 valid 0.2159570124629256
LOSS train 0.22546848395870903 valid 0.21605140054277305
LOSS train 0.22546848395870903 valid 0.2164272669175152
LOSS train 0.22546848395870903 valid 0.2163978598566137
LOSS train 0.22546848395870903 valid 0.21649424953663604
LOSS train 0.22546848395870903 valid 0.21633455555065204
LOSS train 0.22546848395870903 valid 0.21638850949484587
LOSS train 0.22546848395870903 valid 0.2163388553161581
LOSS train 0.22546848395870903 valid 0.21630811024161065
LOSS train 0.22546848395870903 valid 0.21589692092190185
LOSS train 0.22546848395870903 valid 0.2157212830307078
LOSS train 0.22546848395870903 valid 0.21565778602745908
LOSS train 0.22546848395870903 valid 0.21549387466269757
LOSS train 0.22546848395870903 valid 0.2155197794075872
LOSS train 0.22546848395870903 valid 0.21566154725697576
LOSS train 0.22546848395870903 valid 0.21577367549989282
LOSS train 0.22546848395870903 valid 0.21581069889821505
LOSS train 0.22546848395870903 valid 0.21579794014894194
LOSS train 0.22546848395870903 valid 0.2155722210325869
LOSS train 0.22546848395870903 valid 0.2156706455349922
LOSS train 0.22546848395870903 valid 0.21573328004177822
LOSS train 0.22546848395870903 valid 0.21563052179084885
LOSS train 0.22546848395870903 valid 0.21542760930042493
LOSS train 0.22546848395870903 valid 0.21556577602709373
LOSS train 0.22546848395870903 valid 0.2154717867865282
LOSS train 0.22546848395870903 valid 0.21523315139347687
LOSS train 0.22546848395870903 valid 0.2151995716971646
LOSS train 0.22546848395870903 valid 0.21536200597535732
LOSS train 0.22546848395870903 valid 0.2152036131578983
LOSS train 0.22546848395870903 valid 0.21505146599732913
LOSS train 0.22546848395870903 valid 0.21515555822529556
LOSS train 0.22546848395870903 valid 0.21542426864154465
LOSS train 0.22546848395870903 valid 0.2152054229390032
LOSS train 0.22546848395870903 valid 0.2152710511828914
LOSS train 0.22546848395870903 valid 0.21520985111875354
LOSS train 0.22546848395870903 valid 0.21527335658333355
LOSS train 0.22546848395870903 valid 0.21519648476263112
LOSS train 0.22546848395870903 valid 0.2152731925137897
LOSS train 0.22546848395870903 valid 0.21539691168136313
LOSS train 0.22546848395870903 valid 0.2152809399145621
LOSS train 0.22546848395870903 valid 0.21534892339328118
LOSS train 0.22546848395870903 valid 0.21523134930826285
LOSS train 0.22546848395870903 valid 0.21510981629183004
LOSS train 0.22546848395870903 valid 0.2149876301741078
LOSS train 0.22546848395870903 valid 0.2150341010093689
LOSS train 0.22546848395870903 valid 0.21505193747040155
LOSS train 0.22546848395870903 valid 0.21508130743185105
LOSS train 0.22546848395870903 valid 0.21511442085607446
LOSS train 0.22546848395870903 valid 0.21500202874556237
LOSS train 0.22546848395870903 valid 0.21492641738482884
LOSS train 0.22546848395870903 valid 0.2147290874736589
LOSS train 0.22546848395870903 valid 0.2146448738186072
LOSS train 0.22546848395870903 valid 0.21469939450071895
LOSS train 0.22546848395870903 valid 0.21462018105765462
LOSS train 0.22546848395870903 valid 0.21475516338097422
LOSS train 0.22546848395870903 valid 0.21472997009962588
LOSS train 0.22546848395870903 valid 0.21469522836108657
LOSS train 0.22546848395870903 valid 0.21461902072446215
LOSS train 0.22546848395870903 valid 0.21450474337516773
LOSS train 0.22546848395870903 valid 0.21438649096365633
LOSS train 0.22546848395870903 valid 0.214353765869878
LOSS train 0.22546848395870903 valid 0.21431247150040653
LOSS train 0.22546848395870903 valid 0.21432889123215204
LOSS train 0.22546848395870903 valid 0.21436066646762444
LOSS train 0.22546848395870903 valid 0.21436526775360107
LOSS train 0.22546848395870903 valid 0.21432378993847886
LOSS train 0.22546848395870903 valid 0.21424455077760549
LOSS train 0.22546848395870903 valid 0.2143117728449354
LOSS train 0.22546848395870903 valid 0.21434977735364716
LOSS train 0.22546848395870903 valid 0.21434795474012694
LOSS train 0.22546848395870903 valid 0.21421866940699544
LOSS train 0.22546848395870903 valid 0.21419448632475557
LOSS train 0.22546848395870903 valid 0.21410322317195804
LOSS train 0.22546848395870903 valid 0.21410870081500002
LOSS train 0.22546848395870903 valid 0.21402238866344828
LOSS train 0.22546848395870903 valid 0.21434303100397384
LOSS train 0.22546848395870903 valid 0.21428150244372676
LOSS train 0.22546848395870903 valid 0.21431499371280918
LOSS train 0.22546848395870903 valid 0.21433237460245977
LOSS train 0.22546848395870903 valid 0.21428040573673865
LOSS train 0.22546848395870903 valid 0.21430915646806023
LOSS train 0.22546848395870903 valid 0.2142279642419173
LOSS train 0.22546848395870903 valid 0.2143555832461427
LOSS train 0.22546848395870903 valid 0.2143140769783099
LOSS train 0.22546848395870903 valid 0.21422951746554603
LOSS train 0.22546848395870903 valid 0.2142240129694154
LOSS train 0.22546848395870903 valid 0.2142483695448385
LOSS train 0.22546848395870903 valid 0.2142162930290654
LOSS train 0.22546848395870903 valid 0.21416009247863554
LOSS train 0.22546848395870903 valid 0.21408281456679107
LOSS train 0.22546848395870903 valid 0.21417963073075375
LOSS train 0.22546848395870903 valid 0.2141575692510753
LOSS train 0.22546848395870903 valid 0.21401499686964526
LOSS train 0.22546848395870903 valid 0.21396809328853347
LOSS train 0.22546848395870903 valid 0.2138068816295037
LOSS train 0.22546848395870903 valid 0.21382272856001475
LOSS train 0.22546848395870903 valid 0.21376512853559734
LOSS train 0.22546848395870903 valid 0.21373786931721175
LOSS train 0.22546848395870903 valid 0.2137366898335222
LOSS train 0.22546848395870903 valid 0.21372002832817308
LOSS train 0.22546848395870903 valid 0.21368439513570953
LOSS train 0.22546848395870903 valid 0.2136852712067495
LOSS train 0.22546848395870903 valid 0.21387489032637966
LOSS train 0.22546848395870903 valid 0.21378379725589008
LOSS train 0.22546848395870903 valid 0.21369433198402177
LOSS train 0.22546848395870903 valid 0.21356491815476192
LOSS train 0.22546848395870903 valid 0.21373553454699076
LOSS train 0.22546848395870903 valid 0.21370539615845538
LOSS train 0.22546848395870903 valid 0.21365364586005872
LOSS train 0.22546848395870903 valid 0.2137582491425907
LOSS train 0.22546848395870903 valid 0.21367222713060743
LOSS train 0.22546848395870903 valid 0.21364087576580326
LOSS train 0.22546848395870903 valid 0.21354056499442275
LOSS train 0.22546848395870903 valid 0.2136516173391841
LOSS train 0.22546848395870903 valid 0.2137774698112322
LOSS train 0.22546848395870903 valid 0.213726885143043
LOSS train 0.22546848395870903 valid 0.2136003022922219
LOSS train 0.22546848395870903 valid 0.21371189093795315
LOSS train 0.22546848395870903 valid 0.2136546787525658
LOSS train 0.22546848395870903 valid 0.2136807458315577
LOSS train 0.22546848395870903 valid 0.2137490280842849
LOSS train 0.22546848395870903 valid 0.21379293899305843
LOSS train 0.22546848395870903 valid 0.2138208784057466
LOSS train 0.22546848395870903 valid 0.21382645526006397
LOSS train 0.22546848395870903 valid 0.21369991718043743
LOSS train 0.22546848395870903 valid 0.21373376283752785
LOSS train 0.22546848395870903 valid 0.21375272166328269
LOSS train 0.22546848395870903 valid 0.21367445858663686
LOSS train 0.22546848395870903 valid 0.21361617577939312
LOSS train 0.22546848395870903 valid 0.2138239157282644
LOSS train 0.22546848395870903 valid 0.2137206532370681
LOSS train 0.22546848395870903 valid 0.21371058518715327
LOSS train 0.22546848395870903 valid 0.21377544039372898
LOSS train 0.22546848395870903 valid 0.2136822390769209
LOSS train 0.22546848395870903 valid 0.2137635584563425
LOSS train 0.22546848395870903 valid 0.21376251009969763
LOSS train 0.22546848395870903 valid 0.21370453478854748
LOSS train 0.22546848395870903 valid 0.21356755936437327
LOSS train 0.22546848395870903 valid 0.21352490370835714
EPOCH 26:
  batch 1 loss: 0.22300639748573303
  batch 2 loss: 0.2414422184228897
  batch 3 loss: 0.23756790657838187
  batch 4 loss: 0.238333348184824
  batch 5 loss: 0.23773111701011657
  batch 6 loss: 0.2360286737481753
  batch 7 loss: 0.23782288815293992
  batch 8 loss: 0.23642189241945744
  batch 9 loss: 0.24052408503161538
  batch 10 loss: 0.23764943033456803
  batch 11 loss: 0.23820561847903512
  batch 12 loss: 0.2370913289487362
  batch 13 loss: 0.23345021559641913
  batch 14 loss: 0.2323206663131714
  batch 15 loss: 0.23252587914466857
  batch 16 loss: 0.23165300954133272
  batch 17 loss: 0.2313282095334109
  batch 18 loss: 0.2295756083395746
  batch 19 loss: 0.2288968884631207
  batch 20 loss: 0.22786434665322303
  batch 21 loss: 0.23117718739168985
  batch 22 loss: 0.23112511973489414
  batch 23 loss: 0.23090350822262143
  batch 24 loss: 0.23044307778278986
  batch 25 loss: 0.23125197649002074
  batch 26 loss: 0.23027887080724424
  batch 27 loss: 0.23060297414108558
  batch 28 loss: 0.2318992359297616
  batch 29 loss: 0.23197160866753808
  batch 30 loss: 0.23269202460845312
  batch 31 loss: 0.23229733397883753
  batch 32 loss: 0.23322260659188032
  batch 33 loss: 0.235080698222825
  batch 34 loss: 0.23493139971705043
  batch 35 loss: 0.23634886826787677
  batch 36 loss: 0.23733763893445334
  batch 37 loss: 0.23788553959614522
  batch 38 loss: 0.23796914361025157
  batch 39 loss: 0.23817687844618773
  batch 40 loss: 0.2379174344241619
  batch 41 loss: 0.2375503611273882
  batch 42 loss: 0.23676949739456177
  batch 43 loss: 0.23710969021154005
  batch 44 loss: 0.23720907487652518
  batch 45 loss: 0.23678506679005093
  batch 46 loss: 0.23513016622999441
  batch 47 loss: 0.23427128347944706
  batch 48 loss: 0.23393528846402964
  batch 49 loss: 0.2346588172474686
  batch 50 loss: 0.23563785791397096
  batch 51 loss: 0.23549484067103443
  batch 52 loss: 0.23534456067360365
  batch 53 loss: 0.23490687867380539
  batch 54 loss: 0.2350240225593249
  batch 55 loss: 0.2348747277801687
  batch 56 loss: 0.23511710975851333
  batch 57 loss: 0.23462179486165968
  batch 58 loss: 0.23462574595007404
  batch 59 loss: 0.2347876745260368
  batch 60 loss: 0.23559291139245034
  batch 61 loss: 0.23604778017176956
  batch 62 loss: 0.23642080565614085
  batch 63 loss: 0.235954607999514
  batch 64 loss: 0.2355744014494121
  batch 65 loss: 0.23507504623669845
  batch 66 loss: 0.23466584267038287
  batch 67 loss: 0.2343131188136428
  batch 68 loss: 0.2341817144085379
  batch 69 loss: 0.23420429337715756
  batch 70 loss: 0.2344124566231455
  batch 71 loss: 0.23470701738982133
  batch 72 loss: 0.2348272841837671
  batch 73 loss: 0.2347971266263152
  batch 74 loss: 0.23471531573985074
  batch 75 loss: 0.23470003028710684
  batch 76 loss: 0.23508365158187716
  batch 77 loss: 0.23498481782999905
  batch 78 loss: 0.2348850348438972
  batch 79 loss: 0.2348074128356161
  batch 80 loss: 0.23499968685209752
  batch 81 loss: 0.2348876549505893
  batch 82 loss: 0.23500391550180388
  batch 83 loss: 0.23450406649744654
  batch 84 loss: 0.23386128353221075
  batch 85 loss: 0.23449628282995785
  batch 86 loss: 0.23440057351145632
  batch 87 loss: 0.23425019266961636
  batch 88 loss: 0.23386975343931804
  batch 89 loss: 0.2337958797645033
  batch 90 loss: 0.23379585378699833
  batch 91 loss: 0.233361256810335
  batch 92 loss: 0.23345849970760552
  batch 93 loss: 0.23330751670304165
  batch 94 loss: 0.23311149027753383
  batch 95 loss: 0.23305800588507403
  batch 96 loss: 0.23298678826540709
  batch 97 loss: 0.2336581352445268
  batch 98 loss: 0.23355627211989188
  batch 99 loss: 0.23318099117640292
  batch 100 loss: 0.23348284885287285
  batch 101 loss: 0.23346102503266666
  batch 102 loss: 0.2335130883490338
  batch 103 loss: 0.2332480504964162
  batch 104 loss: 0.23341245304506558
  batch 105 loss: 0.23291261423201787
  batch 106 loss: 0.23298923390091592
  batch 107 loss: 0.23294202565589797
  batch 108 loss: 0.2325937614120819
  batch 109 loss: 0.23290734157102919
  batch 110 loss: 0.23303793682293458
  batch 111 loss: 0.23332918536018682
  batch 112 loss: 0.23345024897051708
  batch 113 loss: 0.23331622924424905
  batch 114 loss: 0.2333177197397801
  batch 115 loss: 0.23302544199902078
  batch 116 loss: 0.23301134798033485
  batch 117 loss: 0.23314070090269431
  batch 118 loss: 0.23330509208016476
  batch 119 loss: 0.23349607641957387
  batch 120 loss: 0.23337086637814838
  batch 121 loss: 0.23334210446058226
  batch 122 loss: 0.23306399876954126
  batch 123 loss: 0.232696533203125
  batch 124 loss: 0.2327201049174032
  batch 125 loss: 0.2326405543088913
  batch 126 loss: 0.2325971657558093
  batch 127 loss: 0.23293850682382508
  batch 128 loss: 0.23299928102642298
  batch 129 loss: 0.23331890226334565
  batch 130 loss: 0.23330456683268913
  batch 131 loss: 0.23337979535110123
  batch 132 loss: 0.23357964510267432
  batch 133 loss: 0.23363764640083887
  batch 134 loss: 0.23362219000040596
  batch 135 loss: 0.23340158153463292
  batch 136 loss: 0.23371916067074328
  batch 137 loss: 0.23393966877547495
  batch 138 loss: 0.23396515554707983
  batch 139 loss: 0.23370286211264218
  batch 140 loss: 0.23412470359887397
  batch 141 loss: 0.23379293802782153
  batch 142 loss: 0.23351598279157154
  batch 143 loss: 0.2335463569506065
  batch 144 loss: 0.23348652902576658
  batch 145 loss: 0.23345639664551307
  batch 146 loss: 0.23358044599833555
  batch 147 loss: 0.23359615198608968
  batch 148 loss: 0.2335392559903699
  batch 149 loss: 0.23328129137122391
  batch 150 loss: 0.23322918166716894
  batch 151 loss: 0.23283285376251928
  batch 152 loss: 0.2326345200601377
  batch 153 loss: 0.23261752763604807
  batch 154 loss: 0.2323834045560329
  batch 155 loss: 0.2322347422761302
  batch 156 loss: 0.23222223717050675
  batch 157 loss: 0.23213855143944928
  batch 158 loss: 0.23222484515060351
  batch 159 loss: 0.2321942093424827
  batch 160 loss: 0.23211678629741073
  batch 161 loss: 0.23206145478331525
  batch 162 loss: 0.2321819692482183
  batch 163 loss: 0.23209258082088519
  batch 164 loss: 0.23224848190822253
  batch 165 loss: 0.23255207059961377
  batch 166 loss: 0.2328795789774642
  batch 167 loss: 0.23288050383150935
  batch 168 loss: 0.23250791758653663
  batch 169 loss: 0.23245065319820268
  batch 170 loss: 0.23239523899905823
  batch 171 loss: 0.23241324234775632
  batch 172 loss: 0.2323351297960725
  batch 173 loss: 0.232209509593903
  batch 174 loss: 0.23235127372645784
  batch 175 loss: 0.23237031672682082
  batch 176 loss: 0.23262426401065153
  batch 177 loss: 0.232877173750414
  batch 178 loss: 0.23275297160228986
  batch 179 loss: 0.23269932970987353
  batch 180 loss: 0.23277729973196984
  batch 181 loss: 0.2326899861597883
  batch 182 loss: 0.23257967895203893
  batch 183 loss: 0.2325583583995944
  batch 184 loss: 0.23250967369455358
  batch 185 loss: 0.23229812641401548
  batch 186 loss: 0.23225727813538685
  batch 187 loss: 0.23237884897280503
  batch 188 loss: 0.23223482944229815
  batch 189 loss: 0.23224137274045792
  batch 190 loss: 0.23199527287169508
  batch 191 loss: 0.23197194208337374
  batch 192 loss: 0.23173599566022554
  batch 193 loss: 0.23175279595382473
  batch 194 loss: 0.23165725640107676
  batch 195 loss: 0.23184162447085746
  batch 196 loss: 0.23164210536954355
  batch 197 loss: 0.23174098580319263
  batch 198 loss: 0.2316914311412609
  batch 199 loss: 0.23173300098234684
  batch 200 loss: 0.2316894295066595
  batch 201 loss: 0.23152569143926327
  batch 202 loss: 0.23164315861050444
  batch 203 loss: 0.23169789195354348
  batch 204 loss: 0.23178903093817188
  batch 205 loss: 0.23178521467418206
  batch 206 loss: 0.2317639877929271
  batch 207 loss: 0.2318160982379591
  batch 208 loss: 0.23159207876485127
  batch 209 loss: 0.23166167472253005
  batch 210 loss: 0.23167412139120555
  batch 211 loss: 0.23158158340725288
  batch 212 loss: 0.23163485435663528
  batch 213 loss: 0.2315664205332877
  batch 214 loss: 0.2314536826354321
  batch 215 loss: 0.2314035787138828
  batch 216 loss: 0.231264254561177
  batch 217 loss: 0.2312545963833409
  batch 218 loss: 0.23111767397014374
  batch 219 loss: 0.23075781061769077
  batch 220 loss: 0.23081048903140156
  batch 221 loss: 0.2306593732057114
  batch 222 loss: 0.23067768703441363
  batch 223 loss: 0.23049809446249309
  batch 224 loss: 0.23036428501031228
  batch 225 loss: 0.23026694152090285
  batch 226 loss: 0.23023541469489578
  batch 227 loss: 0.23026793827569433
  batch 228 loss: 0.23019038970794595
  batch 229 loss: 0.23029289501983527
  batch 230 loss: 0.23035550914380862
  batch 231 loss: 0.23048267213555126
  batch 232 loss: 0.23046131072373227
  batch 233 loss: 0.23049427709610165
  batch 234 loss: 0.2304480591008806
  batch 235 loss: 0.2304740127096785
  batch 236 loss: 0.23041749354136193
  batch 237 loss: 0.23029466388346273
  batch 238 loss: 0.2303169248234324
  batch 239 loss: 0.23033166399311322
  batch 240 loss: 0.23034879037489495
  batch 241 loss: 0.23021027074809886
  batch 242 loss: 0.2302776708090601
  batch 243 loss: 0.23029217479650865
  batch 244 loss: 0.23017494801859387
  batch 245 loss: 0.23008792850435997
  batch 246 loss: 0.2300629321394897
  batch 247 loss: 0.23014180706097528
  batch 248 loss: 0.2302347480529739
  batch 249 loss: 0.23033911217168632
  batch 250 loss: 0.23039873880147935
  batch 251 loss: 0.23036821744593966
  batch 252 loss: 0.23036562468087862
  batch 253 loss: 0.23036501359327038
  batch 254 loss: 0.23039119473592504
  batch 255 loss: 0.2303487034404979
  batch 256 loss: 0.23036891943775117
  batch 257 loss: 0.23050515173938024
  batch 258 loss: 0.2304582490708477
  batch 259 loss: 0.23033401997163028
  batch 260 loss: 0.23034817198148141
  batch 261 loss: 0.2303802462944126
  batch 262 loss: 0.2302836927635069
  batch 263 loss: 0.23020895250396584
  batch 264 loss: 0.23031730757969798
  batch 265 loss: 0.23022034078274134
  batch 266 loss: 0.2300990231727299
  batch 267 loss: 0.22994602519028196
  batch 268 loss: 0.22983754837690895
  batch 269 loss: 0.2298549926635501
  batch 270 loss: 0.22982338374411618
  batch 271 loss: 0.22979081716264746
  batch 272 loss: 0.229710415334386
  batch 273 loss: 0.2296370783131638
  batch 274 loss: 0.22957921588290348
  batch 275 loss: 0.22950002746148543
  batch 276 loss: 0.2294694405534993
  batch 277 loss: 0.229430967343413
  batch 278 loss: 0.22926491933117668
  batch 279 loss: 0.22929834785427244
  batch 280 loss: 0.22934214866587094
  batch 281 loss: 0.22924562769004034
  batch 282 loss: 0.22904116771322616
  batch 283 loss: 0.22906790894880733
  batch 284 loss: 0.22906507200128595
  batch 285 loss: 0.2290486054985147
  batch 286 loss: 0.2290163281393218
  batch 287 loss: 0.22898182693466493
  batch 288 loss: 0.22900744077439109
  batch 289 loss: 0.2288907707046885
  batch 290 loss: 0.22885084851034757
  batch 291 loss: 0.22890885450790838
  batch 292 loss: 0.22889865776651527
  batch 293 loss: 0.22892747941480968
  batch 294 loss: 0.228711692502304
  batch 295 loss: 0.22876044542102492
  batch 296 loss: 0.22874794150325092
  batch 297 loss: 0.2287374641056414
  batch 298 loss: 0.22872093284890155
  batch 299 loss: 0.22883888376397035
  batch 300 loss: 0.22869986896713576
  batch 301 loss: 0.22868371994986486
  batch 302 loss: 0.22882519045610303
  batch 303 loss: 0.2287965268880227
  batch 304 loss: 0.22877096360255228
  batch 305 loss: 0.22873585859283072
  batch 306 loss: 0.2288103980176589
  batch 307 loss: 0.2288643668257064
  batch 308 loss: 0.22896693627555648
  batch 309 loss: 0.22896256864456682
  batch 310 loss: 0.22905636376911595
  batch 311 loss: 0.2290216632115496
  batch 312 loss: 0.2290515662290347
  batch 313 loss: 0.2290512679483944
  batch 314 loss: 0.22908741933335164
  batch 315 loss: 0.22907176798298245
  batch 316 loss: 0.22917001831192005
  batch 317 loss: 0.22920824775372395
  batch 318 loss: 0.22899076141089014
  batch 319 loss: 0.22898338253968936
  batch 320 loss: 0.2289440046530217
  batch 321 loss: 0.22906853165767646
  batch 322 loss: 0.2290561800787908
  batch 323 loss: 0.22891786357751204
  batch 324 loss: 0.22889402389158436
  batch 325 loss: 0.22899607594196614
  batch 326 loss: 0.22909511095541388
  batch 327 loss: 0.22912495745066838
  batch 328 loss: 0.22912242186323897
  batch 329 loss: 0.22911834363517067
  batch 330 loss: 0.22911569927677963
  batch 331 loss: 0.229048811732102
  batch 332 loss: 0.2291541145777846
  batch 333 loss: 0.22914877019307991
  batch 334 loss: 0.22952141282622684
  batch 335 loss: 0.22967049661856978
  batch 336 loss: 0.22975250985473394
  batch 337 loss: 0.22987103590448815
  batch 338 loss: 0.2298742128899817
  batch 339 loss: 0.22987491525379958
  batch 340 loss: 0.22975993182729273
  batch 341 loss: 0.22978618697337042
  batch 342 loss: 0.22972527128911158
  batch 343 loss: 0.2297689436823564
  batch 344 loss: 0.22977339368053648
  batch 345 loss: 0.22987941179586494
  batch 346 loss: 0.2298785579859177
  batch 347 loss: 0.22975611291632528
  batch 348 loss: 0.22969425717989603
  batch 349 loss: 0.22974643427184796
  batch 350 loss: 0.22993146904877254
  batch 351 loss: 0.2299429597678008
  batch 352 loss: 0.2298671327192675
  batch 353 loss: 0.22978956051825125
  batch 354 loss: 0.22972515743162672
  batch 355 loss: 0.22960354446525305
  batch 356 loss: 0.22970189275533964
  batch 357 loss: 0.22953324299566552
  batch 358 loss: 0.22938846792588688
  batch 359 loss: 0.22931062776730254
  batch 360 loss: 0.22920338412125904
  batch 361 loss: 0.2290829589740061
  batch 362 loss: 0.2290538803813207
  batch 363 loss: 0.22905353434158093
  batch 364 loss: 0.22897320253017184
  batch 365 loss: 0.22900067106501698
  batch 366 loss: 0.22902432316150823
  batch 367 loss: 0.22917483657679702
  batch 368 loss: 0.2291879182195534
  batch 369 loss: 0.22916681204384903
  batch 370 loss: 0.22925302305737058
  batch 371 loss: 0.22934384784608516
  batch 372 loss: 0.22943544491965284
  batch 373 loss: 0.22946377997423945
  batch 374 loss: 0.2293431451295149
  batch 375 loss: 0.2294269321759542
  batch 376 loss: 0.22956850617489916
  batch 377 loss: 0.229523228713625
  batch 378 loss: 0.22953994982141665
  batch 379 loss: 0.22951637904373512
  batch 380 loss: 0.22948189793448698
  batch 381 loss: 0.22933874887431388
  batch 382 loss: 0.2293359050966058
  batch 383 loss: 0.2292120678073121
  batch 384 loss: 0.22922628050825247
  batch 385 loss: 0.22916027733257838
  batch 386 loss: 0.22914252789218192
  batch 387 loss: 0.22905486506393097
  batch 388 loss: 0.2289418658798503
  batch 389 loss: 0.22893511773994465
  batch 390 loss: 0.22889939970695056
  batch 391 loss: 0.228812762004945
  batch 392 loss: 0.22888868560596387
  batch 393 loss: 0.22887282786326857
  batch 394 loss: 0.2289346183844024
  batch 395 loss: 0.22898549780815464
  batch 396 loss: 0.22895546528426083
  batch 397 loss: 0.2290559926021009
  batch 398 loss: 0.22912066575869844
  batch 399 loss: 0.22913141533695067
  batch 400 loss: 0.229172606356442
  batch 401 loss: 0.2292301143865633
  batch 402 loss: 0.22927815971238102
  batch 403 loss: 0.22927286037736141
  batch 404 loss: 0.22925956226368943
  batch 405 loss: 0.22926383783787857
  batch 406 loss: 0.22912926454409002
  batch 407 loss: 0.22919399728792597
  batch 408 loss: 0.22911388565804444
  batch 409 loss: 0.22911047964632367
  batch 410 loss: 0.22902484641569418
  batch 411 loss: 0.2289843677459262
  batch 412 loss: 0.22897838359897577
  batch 413 loss: 0.2289146562681937
  batch 414 loss: 0.22888671077679898
  batch 415 loss: 0.22884463240583258
  batch 416 loss: 0.22880604512129837
  batch 417 loss: 0.22879347005050532
  batch 418 loss: 0.22878440801036415
  batch 419 loss: 0.22877345278610192
  batch 420 loss: 0.22877949029207229
  batch 421 loss: 0.22882749221670373
  batch 422 loss: 0.22876211308755015
  batch 423 loss: 0.22873141343571052
  batch 424 loss: 0.2287940826126427
  batch 425 loss: 0.22874720548882205
  batch 426 loss: 0.2287250442753935
  batch 427 loss: 0.22879055634455045
  batch 428 loss: 0.22884875824936082
  batch 429 loss: 0.22875913772688602
  batch 430 loss: 0.22883661274993142
  batch 431 loss: 0.22884784493673152
  batch 432 loss: 0.22882434556743614
  batch 433 loss: 0.22878665836661297
  batch 434 loss: 0.22867399299611693
  batch 435 loss: 0.22868341351377552
  batch 436 loss: 0.2286231558120579
  batch 437 loss: 0.2285476232829847
  batch 438 loss: 0.22850763188785614
  batch 439 loss: 0.2284339568948963
  batch 440 loss: 0.22850127359005537
  batch 441 loss: 0.22847557554439624
  batch 442 loss: 0.22844217633364966
  batch 443 loss: 0.22842797088703776
  batch 444 loss: 0.22842258419673722
  batch 445 loss: 0.2283702439471577
  batch 446 loss: 0.22837189568265137
  batch 447 loss: 0.22834247274020109
  batch 448 loss: 0.22846011528080062
  batch 449 loss: 0.22840037450758546
  batch 450 loss: 0.22836229658789106
  batch 451 loss: 0.2283141308375314
  batch 452 loss: 0.22823856935828132
  batch 453 loss: 0.22818865240541253
  batch 454 loss: 0.22823236465847965
  batch 455 loss: 0.2282257052240791
  batch 456 loss: 0.22812596796766707
  batch 457 loss: 0.22806612589687958
  batch 458 loss: 0.22803164576078608
  batch 459 loss: 0.22799974183241525
  batch 460 loss: 0.22801574248334636
  batch 461 loss: 0.22805145172778024
  batch 462 loss: 0.2281255265012448
  batch 463 loss: 0.22809969339334682
  batch 464 loss: 0.22806031215165196
  batch 465 loss: 0.2280853239118412
  batch 466 loss: 0.22801724942840731
  batch 467 loss: 0.22812126525847254
  batch 468 loss: 0.22804949827428556
  batch 469 loss: 0.22793177434249218
  batch 470 loss: 0.22794864218919836
  batch 471 loss: 0.22799125003713458
  batch 472 loss: 0.22783032722644886
LOSS train 0.22783032722644886 valid 0.26432886719703674
LOSS train 0.22783032722644886 valid 0.255110464990139
LOSS train 0.22783032722644886 valid 0.27499934534231824
LOSS train 0.22783032722644886 valid 0.2605862058699131
LOSS train 0.22783032722644886 valid 0.25634714066982267
LOSS train 0.22783032722644886 valid 0.2577745442589124
LOSS train 0.22783032722644886 valid 0.25175527802535463
LOSS train 0.22783032722644886 valid 0.24782953411340714
LOSS train 0.22783032722644886 valid 0.24791749980714586
LOSS train 0.22783032722644886 valid 0.24518483579158784
LOSS train 0.22783032722644886 valid 0.24555151029066605
LOSS train 0.22783032722644886 valid 0.24811253945032755
LOSS train 0.22783032722644886 valid 0.2483119506102342
LOSS train 0.22783032722644886 valid 0.24588471970387868
LOSS train 0.22783032722644886 valid 0.2461299886306127
LOSS train 0.22783032722644886 valid 0.25150644313544035
LOSS train 0.22783032722644886 valid 0.2501895909800249
LOSS train 0.22783032722644886 valid 0.24962799747784933
LOSS train 0.22783032722644886 valid 0.2533991101541017
LOSS train 0.22783032722644886 valid 0.25254388004541395
LOSS train 0.22783032722644886 valid 0.2563276503767286
LOSS train 0.22783032722644886 valid 0.25561571392146026
LOSS train 0.22783032722644886 valid 0.25361022288384644
LOSS train 0.22783032722644886 valid 0.2536208437134822
LOSS train 0.22783032722644886 valid 0.25343345940113066
LOSS train 0.22783032722644886 valid 0.252144360771546
LOSS train 0.22783032722644886 valid 0.25207362241215175
LOSS train 0.22783032722644886 valid 0.2523911701781409
LOSS train 0.22783032722644886 valid 0.25111479748939647
LOSS train 0.22783032722644886 valid 0.2511663948496183
LOSS train 0.22783032722644886 valid 0.25132960802124393
LOSS train 0.22783032722644886 valid 0.25163894472643733
LOSS train 0.22783032722644886 valid 0.24978841479980585
LOSS train 0.22783032722644886 valid 0.24945558300789664
LOSS train 0.22783032722644886 valid 0.2500365116766521
LOSS train 0.22783032722644886 valid 0.250980699641837
LOSS train 0.22783032722644886 valid 0.2515886486382098
LOSS train 0.22783032722644886 valid 0.25167186440605865
LOSS train 0.22783032722644886 valid 0.25261993400561505
LOSS train 0.22783032722644886 valid 0.2525522969663143
LOSS train 0.22783032722644886 valid 0.25226163537037083
LOSS train 0.22783032722644886 valid 0.2533608125079246
LOSS train 0.22783032722644886 valid 0.2539511742286904
LOSS train 0.22783032722644886 valid 0.2535796554928476
LOSS train 0.22783032722644886 valid 0.25280534889962936
LOSS train 0.22783032722644886 valid 0.25269952837539755
LOSS train 0.22783032722644886 valid 0.2523493085135805
LOSS train 0.22783032722644886 valid 0.2539337833101551
LOSS train 0.22783032722644886 valid 0.2526224967168302
LOSS train 0.22783032722644886 valid 0.2530295902490616
LOSS train 0.22783032722644886 valid 0.25285399193857233
LOSS train 0.22783032722644886 valid 0.25240912918861097
LOSS train 0.22783032722644886 valid 0.25371752538771
LOSS train 0.22783032722644886 valid 0.254010029413082
LOSS train 0.22783032722644886 valid 0.2538114322857423
LOSS train 0.22783032722644886 valid 0.25382639707199167
LOSS train 0.22783032722644886 valid 0.2534567522898055
LOSS train 0.22783032722644886 valid 0.25381715724180487
LOSS train 0.22783032722644886 valid 0.253491955540948
LOSS train 0.22783032722644886 valid 0.25336580326159797
LOSS train 0.22783032722644886 valid 0.25326536791246446
LOSS train 0.22783032722644886 valid 0.25347793799254204
LOSS train 0.22783032722644886 valid 0.2531071447190784
LOSS train 0.22783032722644886 valid 0.2535601882264018
LOSS train 0.22783032722644886 valid 0.2522773916904743
LOSS train 0.22783032722644886 valid 0.25192149276986264
LOSS train 0.22783032722644886 valid 0.2525966783512884
LOSS train 0.22783032722644886 valid 0.2517947495421943
LOSS train 0.22783032722644886 valid 0.2524548930966336
LOSS train 0.22783032722644886 valid 0.2529963989342962
LOSS train 0.22783032722644886 valid 0.25362503171806605
LOSS train 0.22783032722644886 valid 0.25445731667180854
LOSS train 0.22783032722644886 valid 0.2552717930241807
LOSS train 0.22783032722644886 valid 0.254806082192305
LOSS train 0.22783032722644886 valid 0.2543876928091049
LOSS train 0.22783032722644886 valid 0.2547138315674506
LOSS train 0.22783032722644886 valid 0.25453485742017823
LOSS train 0.22783032722644886 valid 0.25437410939962435
LOSS train 0.22783032722644886 valid 0.2544794154318073
LOSS train 0.22783032722644886 valid 0.25400751940906047
LOSS train 0.22783032722644886 valid 0.2544498995498375
LOSS train 0.22783032722644886 valid 0.2544203324288857
LOSS train 0.22783032722644886 valid 0.254771211061133
LOSS train 0.22783032722644886 valid 0.254309100231954
LOSS train 0.22783032722644886 valid 0.25508773695020115
LOSS train 0.22783032722644886 valid 0.25508818810069284
LOSS train 0.22783032722644886 valid 0.25474729877093744
LOSS train 0.22783032722644886 valid 0.25528523715382273
LOSS train 0.22783032722644886 valid 0.25567547206798297
LOSS train 0.22783032722644886 valid 0.25626828753285935
LOSS train 0.22783032722644886 valid 0.2562039443752268
LOSS train 0.22783032722644886 valid 0.2562558464705944
LOSS train 0.22783032722644886 valid 0.2560656932733392
LOSS train 0.22783032722644886 valid 0.25653942024454157
LOSS train 0.22783032722644886 valid 0.2566618580567209
LOSS train 0.22783032722644886 valid 0.25698507235695917
LOSS train 0.22783032722644886 valid 0.25707142746325623
LOSS train 0.22783032722644886 valid 0.2577512103075884
LOSS train 0.22783032722644886 valid 0.25784907316920735
LOSS train 0.22783032722644886 valid 0.25814358949661254
LOSS train 0.22783032722644886 valid 0.2581845049810882
LOSS train 0.22783032722644886 valid 0.25847262786883934
LOSS train 0.22783032722644886 valid 0.25821555702431687
LOSS train 0.22783032722644886 valid 0.2579995726163571
LOSS train 0.22783032722644886 valid 0.25842842317762826
LOSS train 0.22783032722644886 valid 0.25865001076797267
LOSS train 0.22783032722644886 valid 0.25834454693526865
LOSS train 0.22783032722644886 valid 0.2584484506536413
LOSS train 0.22783032722644886 valid 0.257969928033855
LOSS train 0.22783032722644886 valid 0.2579095080494881
LOSS train 0.22783032722644886 valid 0.25791414055201384
LOSS train 0.22783032722644886 valid 0.2581653580335634
LOSS train 0.22783032722644886 valid 0.25783354887920146
LOSS train 0.22783032722644886 valid 0.2579849263032277
LOSS train 0.22783032722644886 valid 0.25886616810508395
LOSS train 0.22783032722644886 valid 0.25849347702901937
LOSS train 0.22783032722644886 valid 0.2592343128899224
LOSS train 0.22783032722644886 valid 0.2590587375275159
LOSS train 0.22783032722644886 valid 0.2588555258111793
LOSS train 0.22783032722644886 valid 0.25836080896357694
LOSS train 0.22783032722644886 valid 0.258070089723453
LOSS train 0.22783032722644886 valid 0.2584044868584539
LOSS train 0.22783032722644886 valid 0.25839898390014
LOSS train 0.22783032722644886 valid 0.2585731825280574
LOSS train 0.22783032722644886 valid 0.25850498592853544
LOSS train 0.22783032722644886 valid 0.25888762575766394
LOSS train 0.22783032722644886 valid 0.25871727586261867
LOSS train 0.22783032722644886 valid 0.2586310120532289
LOSS train 0.22783032722644886 valid 0.2583063196304233
LOSS train 0.22783032722644886 valid 0.2580027618087255
LOSS train 0.22783032722644886 valid 0.25805074169890574
LOSS train 0.22783032722644886 valid 0.25791831371007545
LOSS train 0.22783032722644886 valid 0.2579615287538758
LOSS train 0.22783032722644886 valid 0.2582132795186185
LOSS train 0.22783032722644886 valid 0.2583366134652385
LOSS train 0.22783032722644886 valid 0.2581443692393163
LOSS train 0.22783032722644886 valid 0.25807839675541344
LOSS train 0.22783032722644886 valid 0.2580971573142038
LOSS train 0.22783032722644886 valid 0.2578191761490252
LOSS train 0.22783032722644886 valid 0.25771151930093766
LOSS train 0.22783032722644886 valid 0.25770627733663465
LOSS train 0.22783032722644886 valid 0.25809726832618174
LOSS train 0.22783032722644886 valid 0.25819663547135735
LOSS train 0.22783032722644886 valid 0.25828683231439853
LOSS train 0.22783032722644886 valid 0.25821324463548334
LOSS train 0.22783032722644886 valid 0.2581672670498286
LOSS train 0.22783032722644886 valid 0.2581642931010447
LOSS train 0.22783032722644886 valid 0.25798748353043116
LOSS train 0.22783032722644886 valid 0.2581023057835214
LOSS train 0.22783032722644886 valid 0.2580920584996541
LOSS train 0.22783032722644886 valid 0.25796808322139136
LOSS train 0.22783032722644886 valid 0.2579376363244496
LOSS train 0.22783032722644886 valid 0.2576683515232373
LOSS train 0.22783032722644886 valid 0.2580154469454443
LOSS train 0.22783032722644886 valid 0.25813688299348275
LOSS train 0.22783032722644886 valid 0.2581060159091766
LOSS train 0.22783032722644886 valid 0.2580842483005706
LOSS train 0.22783032722644886 valid 0.25825168390440034
LOSS train 0.22783032722644886 valid 0.25835887825339093
LOSS train 0.22783032722644886 valid 0.2585154674015939
LOSS train 0.22783032722644886 valid 0.25846091033138846
LOSS train 0.22783032722644886 valid 0.25849745643359645
LOSS train 0.22783032722644886 valid 0.25827381645609265
LOSS train 0.22783032722644886 valid 0.2583550746302779
LOSS train 0.22783032722644886 valid 0.2585034078720844
LOSS train 0.22783032722644886 valid 0.25841612539377556
LOSS train 0.22783032722644886 valid 0.2584012093658219
LOSS train 0.22783032722644886 valid 0.25813253356942106
LOSS train 0.22783032722644886 valid 0.2579749095193028
LOSS train 0.22783032722644886 valid 0.2581496719928349
LOSS train 0.22783032722644886 valid 0.2580473492717185
LOSS train 0.22783032722644886 valid 0.2581221873677054
LOSS train 0.22783032722644886 valid 0.2578304512480091
LOSS train 0.22783032722644886 valid 0.2582267511678838
LOSS train 0.22783032722644886 valid 0.2582323742764337
LOSS train 0.22783032722644886 valid 0.2582482154565779
LOSS train 0.22783032722644886 valid 0.2582440316508719
LOSS train 0.22783032722644886 valid 0.25829871660203074
LOSS train 0.22783032722644886 valid 0.25848951727651354
LOSS train 0.22783032722644886 valid 0.2582964892188708
LOSS train 0.22783032722644886 valid 0.2582441862281515
LOSS train 0.22783032722644886 valid 0.25794475648429366
LOSS train 0.22783032722644886 valid 0.2581284752960413
LOSS train 0.22783032722644886 valid 0.2579996194690466
LOSS train 0.22783032722644886 valid 0.25780042289076627
LOSS train 0.22783032722644886 valid 0.25798258273511804
LOSS train 0.22783032722644886 valid 0.2578887864548892
LOSS train 0.22783032722644886 valid 0.25800830902571376
LOSS train 0.22783032722644886 valid 0.25771287028436307
LOSS train 0.22783032722644886 valid 0.25777485880412554
LOSS train 0.22783032722644886 valid 0.25793305593323335
LOSS train 0.22783032722644886 valid 0.25793075243321556
LOSS train 0.22783032722644886 valid 0.25760524469027246
LOSS train 0.22783032722644886 valid 0.25744926307311994
LOSS train 0.22783032722644886 valid 0.257390608619421
LOSS train 0.22783032722644886 valid 0.25751166965584366
LOSS train 0.22783032722644886 valid 0.25745218924156904
LOSS train 0.22783032722644886 valid 0.25761176502764827
LOSS train 0.22783032722644886 valid 0.2575763397330615
LOSS train 0.22783032722644886 valid 0.2577568701654673
LOSS train 0.22783032722644886 valid 0.2575485386806934
LOSS train 0.22783032722644886 valid 0.2575656003733673
LOSS train 0.22783032722644886 valid 0.2576252934821133
LOSS train 0.22783032722644886 valid 0.2573224386455966
LOSS train 0.22783032722644886 valid 0.25718684937895797
LOSS train 0.22783032722644886 valid 0.25726242392387205
LOSS train 0.22783032722644886 valid 0.25718499871267786
LOSS train 0.22783032722644886 valid 0.2569315177030288
LOSS train 0.22783032722644886 valid 0.25702947720386193
LOSS train 0.22783032722644886 valid 0.25717558435031346
LOSS train 0.22783032722644886 valid 0.25737193085571036
LOSS train 0.22783032722644886 valid 0.25714159278937104
LOSS train 0.22783032722644886 valid 0.2571328752477404
LOSS train 0.22783032722644886 valid 0.2571770605918403
LOSS train 0.22783032722644886 valid 0.2569995542598325
LOSS train 0.22783032722644886 valid 0.2568562664781456
LOSS train 0.22783032722644886 valid 0.25661782103200115
LOSS train 0.22783032722644886 valid 0.256631123226717
LOSS train 0.22783032722644886 valid 0.2566873174007625
LOSS train 0.22783032722644886 valid 0.25681489584120837
LOSS train 0.22783032722644886 valid 0.2567759031488885
LOSS train 0.22783032722644886 valid 0.2569288575837204
LOSS train 0.22783032722644886 valid 0.2571052452507575
LOSS train 0.22783032722644886 valid 0.2570342186040112
LOSS train 0.22783032722644886 valid 0.25690714948707155
LOSS train 0.22783032722644886 valid 0.2567770698033603
LOSS train 0.22783032722644886 valid 0.25679233742442953
LOSS train 0.22783032722644886 valid 0.25673243292329606
LOSS train 0.22783032722644886 valid 0.2566279770895904
LOSS train 0.22783032722644886 valid 0.2566184387906738
LOSS train 0.22783032722644886 valid 0.2566615693367921
LOSS train 0.22783032722644886 valid 0.2568302220697033
LOSS train 0.22783032722644886 valid 0.2572013249085185
LOSS train 0.22783032722644886 valid 0.25716017676979047
LOSS train 0.22783032722644886 valid 0.2573248408576275
LOSS train 0.22783032722644886 valid 0.2571528627205703
LOSS train 0.22783032722644886 valid 0.2572653062232939
LOSS train 0.22783032722644886 valid 0.2571491370556735
LOSS train 0.22783032722644886 valid 0.2570622178051761
LOSS train 0.22783032722644886 valid 0.2565911581739783
LOSS train 0.22783032722644886 valid 0.2563738943506573
LOSS train 0.22783032722644886 valid 0.25631449568616455
LOSS train 0.22783032722644886 valid 0.2561141931348377
LOSS train 0.22783032722644886 valid 0.25615798150662517
LOSS train 0.22783032722644886 valid 0.2562789278979204
LOSS train 0.22783032722644886 valid 0.2563847044740266
LOSS train 0.22783032722644886 valid 0.2564347379724024
LOSS train 0.22783032722644886 valid 0.25647324702191737
LOSS train 0.22783032722644886 valid 0.2562409858986077
LOSS train 0.22783032722644886 valid 0.256360704600811
LOSS train 0.22783032722644886 valid 0.25641479821081653
LOSS train 0.22783032722644886 valid 0.2562943918719178
LOSS train 0.22783032722644886 valid 0.25602747965235956
LOSS train 0.22783032722644886 valid 0.2562229248482411
LOSS train 0.22783032722644886 valid 0.2561232434768303
LOSS train 0.22783032722644886 valid 0.2558246937696822
LOSS train 0.22783032722644886 valid 0.2558434280324075
LOSS train 0.22783032722644886 valid 0.2560183156945909
LOSS train 0.22783032722644886 valid 0.2558006964710228
LOSS train 0.22783032722644886 valid 0.25564753183951744
LOSS train 0.22783032722644886 valid 0.25574978820665584
LOSS train 0.22783032722644886 valid 0.2560289587455851
LOSS train 0.22783032722644886 valid 0.25579386071333865
LOSS train 0.22783032722644886 valid 0.2558845715653716
LOSS train 0.22783032722644886 valid 0.2557567460919326
LOSS train 0.22783032722644886 valid 0.2558407374006465
LOSS train 0.22783032722644886 valid 0.2557882834574703
LOSS train 0.22783032722644886 valid 0.25593458210576825
LOSS train 0.22783032722644886 valid 0.2560619212770108
LOSS train 0.22783032722644886 valid 0.25598877088891137
LOSS train 0.22783032722644886 valid 0.2560404657327821
LOSS train 0.22783032722644886 valid 0.25588368651840615
LOSS train 0.22783032722644886 valid 0.25573241814370556
LOSS train 0.22783032722644886 valid 0.25560002986096986
LOSS train 0.22783032722644886 valid 0.2556537507880818
LOSS train 0.22783032722644886 valid 0.2556983659016913
LOSS train 0.22783032722644886 valid 0.25581239760997926
LOSS train 0.22783032722644886 valid 0.25586334524823606
LOSS train 0.22783032722644886 valid 0.255749291202928
LOSS train 0.22783032722644886 valid 0.2556385176522391
LOSS train 0.22783032722644886 valid 0.25541302789784837
LOSS train 0.22783032722644886 valid 0.25529365865051323
LOSS train 0.22783032722644886 valid 0.2553660482062889
LOSS train 0.22783032722644886 valid 0.25530820224486606
LOSS train 0.22783032722644886 valid 0.25544629515263073
LOSS train 0.22783032722644886 valid 0.2554025236334834
LOSS train 0.22783032722644886 valid 0.2553658822895343
LOSS train 0.22783032722644886 valid 0.25527110256047714
LOSS train 0.22783032722644886 valid 0.25519571069202623
LOSS train 0.22783032722644886 valid 0.2551063167123959
LOSS train 0.22783032722644886 valid 0.255065960623964
LOSS train 0.22783032722644886 valid 0.25500446114025704
LOSS train 0.22783032722644886 valid 0.25500969507385035
LOSS train 0.22783032722644886 valid 0.2550278678536415
LOSS train 0.22783032722644886 valid 0.25501718505964444
LOSS train 0.22783032722644886 valid 0.2549607244295043
LOSS train 0.22783032722644886 valid 0.2549059590707323
LOSS train 0.22783032722644886 valid 0.2550222653670599
LOSS train 0.22783032722644886 valid 0.25507607109171887
LOSS train 0.22783032722644886 valid 0.25507074266672136
LOSS train 0.22783032722644886 valid 0.2549517230437047
LOSS train 0.22783032722644886 valid 0.2549076030487256
LOSS train 0.22783032722644886 valid 0.2548052937579234
LOSS train 0.22783032722644886 valid 0.25479586897908074
LOSS train 0.22783032722644886 valid 0.25467645682272366
LOSS train 0.22783032722644886 valid 0.254953239965283
LOSS train 0.22783032722644886 valid 0.25484021677838864
LOSS train 0.22783032722644886 valid 0.2548760984141331
LOSS train 0.22783032722644886 valid 0.25483687540281164
LOSS train 0.22783032722644886 valid 0.2547818124294281
LOSS train 0.22783032722644886 valid 0.254839289705853
LOSS train 0.22783032722644886 valid 0.25478160677429956
LOSS train 0.22783032722644886 valid 0.2549038461793345
LOSS train 0.22783032722644886 valid 0.2548715914036058
LOSS train 0.22783032722644886 valid 0.25478945100118244
LOSS train 0.22783032722644886 valid 0.25476847933251645
LOSS train 0.22783032722644886 valid 0.2547806584590617
LOSS train 0.22783032722644886 valid 0.25477126807724154
LOSS train 0.22783032722644886 valid 0.25475902986190163
LOSS train 0.22783032722644886 valid 0.25468843332491814
LOSS train 0.22783032722644886 valid 0.2548050931031087
LOSS train 0.22783032722644886 valid 0.25477824878433475
LOSS train 0.22783032722644886 valid 0.25462906348631476
LOSS train 0.22783032722644886 valid 0.25456988834488536
LOSS train 0.22783032722644886 valid 0.254408078789711
LOSS train 0.22783032722644886 valid 0.2543967814624675
LOSS train 0.22783032722644886 valid 0.2543220390147025
LOSS train 0.22783032722644886 valid 0.2543111758352053
LOSS train 0.22783032722644886 valid 0.2543169521361499
LOSS train 0.22783032722644886 valid 0.2543135871941393
LOSS train 0.22783032722644886 valid 0.25431111171346416
LOSS train 0.22783032722644886 valid 0.2543208604027708
LOSS train 0.22783032722644886 valid 0.254539164918679
LOSS train 0.22783032722644886 valid 0.254429242612716
LOSS train 0.22783032722644886 valid 0.25434895922888573
LOSS train 0.22783032722644886 valid 0.25424075880575747
LOSS train 0.22783032722644886 valid 0.25441670939547373
LOSS train 0.22783032722644886 valid 0.25439611130212186
LOSS train 0.22783032722644886 valid 0.2543651433908834
LOSS train 0.22783032722644886 valid 0.2544916516279473
LOSS train 0.22783032722644886 valid 0.2543565198735408
LOSS train 0.22783032722644886 valid 0.2543112043051692
LOSS train 0.22783032722644886 valid 0.25420275882277477
LOSS train 0.22783032722644886 valid 0.25432135351002216
LOSS train 0.22783032722644886 valid 0.2544461389814598
LOSS train 0.22783032722644886 valid 0.2543735874095404
LOSS train 0.22783032722644886 valid 0.254230015173082
LOSS train 0.22783032722644886 valid 0.25430571883060465
LOSS train 0.22783032722644886 valid 0.2542821665462587
LOSS train 0.22783032722644886 valid 0.25431288314717154
LOSS train 0.22783032722644886 valid 0.25443575529121604
LOSS train 0.22783032722644886 valid 0.25446320710365067
LOSS train 0.22783032722644886 valid 0.2545002785434804
LOSS train 0.22783032722644886 valid 0.25448836129432345
LOSS train 0.22783032722644886 valid 0.25432968597177047
LOSS train 0.22783032722644886 valid 0.25437958475746464
LOSS train 0.22783032722644886 valid 0.25439972190462906
LOSS train 0.22783032722644886 valid 0.2543193872331241
LOSS train 0.22783032722644886 valid 0.2542555064461025
LOSS train 0.22783032722644886 valid 0.2544682297027773
LOSS train 0.22783032722644886 valid 0.25434553957546846
LOSS train 0.22783032722644886 valid 0.2543262283611034
LOSS train 0.22783032722644886 valid 0.25440918996971174
LOSS train 0.22783032722644886 valid 0.2543320486342514
LOSS train 0.22783032722644886 valid 0.2544062376838841
LOSS train 0.22783032722644886 valid 0.2544160059403852
LOSS train 0.22783032722644886 valid 0.2543290246491211
LOSS train 0.22783032722644886 valid 0.2542091558646897
LOSS train 0.22783032722644886 valid 0.25419148923904916
EPOCH 27:
  batch 1 loss: 0.23563233017921448
  batch 2 loss: 0.2443186342716217
  batch 3 loss: 0.2421418478091558
  batch 4 loss: 0.24616459384560585
  batch 5 loss: 0.2482570081949234
  batch 6 loss: 0.24415580928325653
  batch 7 loss: 0.24225891062191554
  batch 8 loss: 0.23931705206632614
  batch 9 loss: 0.24226346280839708
  batch 10 loss: 0.2402331069111824
  batch 11 loss: 0.23996585336598483
  batch 12 loss: 0.23824196805556616
  batch 13 loss: 0.23575151769014505
  batch 14 loss: 0.23643250869853155
  batch 15 loss: 0.23743583858013154
  batch 16 loss: 0.2365440484136343
  batch 17 loss: 0.23621079238022075
  batch 18 loss: 0.233734173907174
  batch 19 loss: 0.23210476260436208
  batch 20 loss: 0.22920265123248101
  batch 21 loss: 0.23232810483092353
  batch 22 loss: 0.23207714544101196
  batch 23 loss: 0.23316638819549396
  batch 24 loss: 0.23298904113471508
  batch 25 loss: 0.23296062111854554
  batch 26 loss: 0.23181205357496554
  batch 27 loss: 0.23162411484453413
  batch 28 loss: 0.23300994985869952
  batch 29 loss: 0.2329122614243935
  batch 30 loss: 0.23368258525927862
  batch 31 loss: 0.23343883022185294
  batch 32 loss: 0.23422360233962536
  batch 33 loss: 0.23591840628421668
  batch 34 loss: 0.2355913611895898
  batch 35 loss: 0.23734902696950094
  batch 36 loss: 0.23811192189653715
  batch 37 loss: 0.23809554327178645
  batch 38 loss: 0.23833855398391424
  batch 39 loss: 0.23814106369629884
  batch 40 loss: 0.2379320964217186
  batch 41 loss: 0.2375732496744249
  batch 42 loss: 0.23689878341697512
  batch 43 loss: 0.23739204365153646
  batch 44 loss: 0.23743086545304817
  batch 45 loss: 0.23678413463963402
  batch 46 loss: 0.23513515261204346
  batch 47 loss: 0.23409919440746307
  batch 48 loss: 0.23384502374877533
  batch 49 loss: 0.23429797528957835
  batch 50 loss: 0.2351698997616768
  batch 51 loss: 0.23517670789185693
  batch 52 loss: 0.2351654183406096
  batch 53 loss: 0.23475626504646158
  batch 54 loss: 0.23447644765730258
  batch 55 loss: 0.23449697602878916
  batch 56 loss: 0.23486192098685674
  batch 57 loss: 0.23427048534677739
  batch 58 loss: 0.23416491829115768
  batch 59 loss: 0.23423667024757902
  batch 60 loss: 0.23481445660193762
  batch 61 loss: 0.2351911141247046
  batch 62 loss: 0.23561785682555167
  batch 63 loss: 0.23493662145402697
  batch 64 loss: 0.2345339427702129
  batch 65 loss: 0.23412138154873482
  batch 66 loss: 0.23354174693425497
  batch 67 loss: 0.23308056081408884
  batch 68 loss: 0.2329548257676994
  batch 69 loss: 0.23318667688231537
  batch 70 loss: 0.23317871604646956
  batch 71 loss: 0.23343116380798984
  batch 72 loss: 0.23340738688906035
  batch 73 loss: 0.23319717996740993
  batch 74 loss: 0.23305070480784854
  batch 75 loss: 0.23299367507298788
  batch 76 loss: 0.2334740859897513
  batch 77 loss: 0.23336529247946552
  batch 78 loss: 0.23331490999613053
  batch 79 loss: 0.23343193040618412
  batch 80 loss: 0.23361819945275783
  batch 81 loss: 0.233599998332836
  batch 82 loss: 0.23374669962539907
  batch 83 loss: 0.2332287928067058
  batch 84 loss: 0.232567044063693
  batch 85 loss: 0.23280106660197764
  batch 86 loss: 0.23247524472170097
  batch 87 loss: 0.2324543321269682
  batch 88 loss: 0.23202697949653323
  batch 89 loss: 0.23163899377490696
  batch 90 loss: 0.23183952536847857
  batch 91 loss: 0.23154251064573014
  batch 92 loss: 0.231260328837063
  batch 93 loss: 0.2307354211807251
  batch 94 loss: 0.2304394912529499
  batch 95 loss: 0.23037149498337192
  batch 96 loss: 0.23016391337538758
  batch 97 loss: 0.23075166237108485
  batch 98 loss: 0.23066362358477652
  batch 99 loss: 0.23038661374588204
  batch 100 loss: 0.23066758647561073
  batch 101 loss: 0.2305497901864571
  batch 102 loss: 0.23064070048869825
  batch 103 loss: 0.23040339451970407
  batch 104 loss: 0.23066508669692737
  batch 105 loss: 0.2302044187273298
  batch 106 loss: 0.23029534996680492
  batch 107 loss: 0.2302304517442935
  batch 108 loss: 0.22977779722876018
  batch 109 loss: 0.22993798307869412
  batch 110 loss: 0.23026986216956918
  batch 111 loss: 0.23050365939333634
  batch 112 loss: 0.23040214726435287
  batch 113 loss: 0.23030709033518765
  batch 114 loss: 0.23039117011061885
  batch 115 loss: 0.2300129683121391
  batch 116 loss: 0.22984614223241806
  batch 117 loss: 0.22971502354002407
  batch 118 loss: 0.22984856089292946
  batch 119 loss: 0.23005223674934452
  batch 120 loss: 0.22980346170564492
  batch 121 loss: 0.2296243876465096
  batch 122 loss: 0.2293955984174228
  batch 123 loss: 0.22907053079546952
  batch 124 loss: 0.2289810618085246
  batch 125 loss: 0.2285262668132782
  batch 126 loss: 0.2286406723516328
  batch 127 loss: 0.22889854976042048
  batch 128 loss: 0.22884807852096856
  batch 129 loss: 0.22914328247077706
  batch 130 loss: 0.22912543840133226
  batch 131 loss: 0.22918922584930448
  batch 132 loss: 0.22946269731178429
  batch 133 loss: 0.2293905227031923
  batch 134 loss: 0.2293591603859147
  batch 135 loss: 0.22923678203865333
  batch 136 loss: 0.2295774192932774
  batch 137 loss: 0.22983300990431849
  batch 138 loss: 0.22981069679709448
  batch 139 loss: 0.22953063078063854
  batch 140 loss: 0.22989064178296498
  batch 141 loss: 0.2296197494716509
  batch 142 loss: 0.22948515593585833
  batch 143 loss: 0.22959669945123312
  batch 144 loss: 0.22957148040748304
  batch 145 loss: 0.22954810888602817
  batch 146 loss: 0.22958441946196229
  batch 147 loss: 0.2296881391888573
  batch 148 loss: 0.2296730333284752
  batch 149 loss: 0.22932867675819654
  batch 150 loss: 0.22927516241868337
  batch 151 loss: 0.22892870591176268
  batch 152 loss: 0.2287419586393394
  batch 153 loss: 0.22873172475621592
  batch 154 loss: 0.2284440677854922
  batch 155 loss: 0.22830116796878078
  batch 156 loss: 0.22834471890177482
  batch 157 loss: 0.22817728891494168
  batch 158 loss: 0.2282087361510796
  batch 159 loss: 0.22821440468044402
  batch 160 loss: 0.22812253618612885
  batch 161 loss: 0.22789517313427066
  batch 162 loss: 0.22796907165536173
  batch 163 loss: 0.22780016128636577
  batch 164 loss: 0.22794176429146673
  batch 165 loss: 0.22824846858328038
  batch 166 loss: 0.22857070893767367
  batch 167 loss: 0.2286056101679088
  batch 168 loss: 0.22830008094509444
  batch 169 loss: 0.22821070245031774
  batch 170 loss: 0.22820188753745135
  batch 171 loss: 0.2283944708910602
  batch 172 loss: 0.2283425367502279
  batch 173 loss: 0.22822674299251136
  batch 174 loss: 0.22832672726148848
  batch 175 loss: 0.22844201913901738
  batch 176 loss: 0.22872516606003046
  batch 177 loss: 0.22888417255743748
  batch 178 loss: 0.22875910888562043
  batch 179 loss: 0.22876708251137973
  batch 180 loss: 0.22881591038571464
  batch 181 loss: 0.22882239043054
  batch 182 loss: 0.22873578236980752
  batch 183 loss: 0.22872897681317042
  batch 184 loss: 0.2287492275076068
  batch 185 loss: 0.22851867595234432
  batch 186 loss: 0.22849109832958509
  batch 187 loss: 0.2285999366465737
  batch 188 loss: 0.2284928729876559
  batch 189 loss: 0.22854082822484315
  batch 190 loss: 0.22838648094942696
  batch 191 loss: 0.22850216815921023
  batch 192 loss: 0.22826837461131314
  batch 193 loss: 0.22827681914512357
  batch 194 loss: 0.22820938739579977
  batch 195 loss: 0.22833270919628632
  batch 196 loss: 0.22809040546417236
  batch 197 loss: 0.2281673446222005
  batch 198 loss: 0.22815265875272076
  batch 199 loss: 0.22820567538091285
  batch 200 loss: 0.22818598732352258
  batch 201 loss: 0.22800581802183123
  batch 202 loss: 0.22812724983928226
  batch 203 loss: 0.2281915419941465
  batch 204 loss: 0.22843201743329272
  batch 205 loss: 0.2284766211015422
  batch 206 loss: 0.2284217656960765
  batch 207 loss: 0.2285052465211942
  batch 208 loss: 0.22830239253548476
  batch 209 loss: 0.22832207032367943
  batch 210 loss: 0.2283435868365424
  batch 211 loss: 0.2282181571445194
  batch 212 loss: 0.22825579731813017
  batch 213 loss: 0.2281353295968732
  batch 214 loss: 0.2280002395126307
  batch 215 loss: 0.22798611293005389
  batch 216 loss: 0.22792191334344722
  batch 217 loss: 0.2280262282916478
  batch 218 loss: 0.22782433921590858
  batch 219 loss: 0.22745794159908816
  batch 220 loss: 0.2274622103707357
  batch 221 loss: 0.22730870153839233
  batch 222 loss: 0.22734520960230012
  batch 223 loss: 0.22726224884056725
  batch 224 loss: 0.22714419370251043
  batch 225 loss: 0.2270216139819887
  batch 226 loss: 0.22693211769899435
  batch 227 loss: 0.22690735632627546
  batch 228 loss: 0.22694241928688266
  batch 229 loss: 0.22694334602512126
  batch 230 loss: 0.22699789197548575
  batch 231 loss: 0.22704420287113686
  batch 232 loss: 0.22703017422865177
  batch 233 loss: 0.22706612650416952
  batch 234 loss: 0.22700438680302384
  batch 235 loss: 0.22698363281310874
  batch 236 loss: 0.2269401634901257
  batch 237 loss: 0.2267285804205303
  batch 238 loss: 0.22675822700272089
  batch 239 loss: 0.22676658642840686
  batch 240 loss: 0.2268165751049916
  batch 241 loss: 0.22668725765345007
  batch 242 loss: 0.22655789529489093
  batch 243 loss: 0.22655537859402566
  batch 244 loss: 0.22642968686633422
  batch 245 loss: 0.22631238309704527
  batch 246 loss: 0.22626426743298042
  batch 247 loss: 0.22645405453708972
  batch 248 loss: 0.22656810788377638
  batch 249 loss: 0.22666065239283933
  batch 250 loss: 0.22671207135915755
  batch 251 loss: 0.22664594472167027
  batch 252 loss: 0.2266156944254088
  batch 253 loss: 0.2266133514788782
  batch 254 loss: 0.2266254717205453
  batch 255 loss: 0.2265657696653815
  batch 256 loss: 0.22659399936674163
  batch 257 loss: 0.22672950890973384
  batch 258 loss: 0.22667942808349004
  batch 259 loss: 0.22656620141392048
  batch 260 loss: 0.226579961925745
  batch 261 loss: 0.22658667590654674
  batch 262 loss: 0.2264683723790955
  batch 263 loss: 0.2263859787481366
  batch 264 loss: 0.22650235061618415
  batch 265 loss: 0.22643740272746896
  batch 266 loss: 0.22630939419780458
  batch 267 loss: 0.22610263961754487
  batch 268 loss: 0.2260168123601088
  batch 269 loss: 0.22610180356910237
  batch 270 loss: 0.22607698920700287
  batch 271 loss: 0.2260670081825714
  batch 272 loss: 0.22597620641703114
  batch 273 loss: 0.22592884918927272
  batch 274 loss: 0.2258756158016894
  batch 275 loss: 0.225828660401431
  batch 276 loss: 0.22577986758256305
  batch 277 loss: 0.22579313900711734
  batch 278 loss: 0.2256569982432633
  batch 279 loss: 0.22568713646635788
  batch 280 loss: 0.2256636690880571
  batch 281 loss: 0.2255444688941236
  batch 282 loss: 0.22532697275598
  batch 283 loss: 0.22533293637075189
  batch 284 loss: 0.22536321435595902
  batch 285 loss: 0.22536953917720862
  batch 286 loss: 0.22533309220017253
  batch 287 loss: 0.22524623811660327
  batch 288 loss: 0.22530257205168405
  batch 289 loss: 0.22518963436354403
  batch 290 loss: 0.22518704456501992
  batch 291 loss: 0.22523782512371482
  batch 292 loss: 0.22526351831955452
  batch 293 loss: 0.2252460960743777
  batch 294 loss: 0.225006541680722
  batch 295 loss: 0.22500915790008286
  batch 296 loss: 0.22495670571319154
  batch 297 loss: 0.2249457178894518
  batch 298 loss: 0.22488077119892877
  batch 299 loss: 0.22501753270626068
  batch 300 loss: 0.22491274838646252
  batch 301 loss: 0.2248263052135607
  batch 302 loss: 0.22502847716508323
  batch 303 loss: 0.22503681077618803
  batch 304 loss: 0.22500969023492776
  batch 305 loss: 0.22494278017614708
  batch 306 loss: 0.2251245676984195
  batch 307 loss: 0.2252385471173917
  batch 308 loss: 0.22535392443661567
  batch 309 loss: 0.22541997511795808
  batch 310 loss: 0.2255951641067382
  batch 311 loss: 0.22564118592685442
  batch 312 loss: 0.22569808187202
  batch 313 loss: 0.22574223025728718
  batch 314 loss: 0.2257833590458153
  batch 315 loss: 0.22575642655766198
  batch 316 loss: 0.22576454314815847
  batch 317 loss: 0.22585664567323138
  batch 318 loss: 0.22562927630899837
  batch 319 loss: 0.22550453754801736
  batch 320 loss: 0.22546547143720091
  batch 321 loss: 0.2255698594255982
  batch 322 loss: 0.22556830030975875
  batch 323 loss: 0.22548638131965423
  batch 324 loss: 0.22547113844825897
  batch 325 loss: 0.22559853750925798
  batch 326 loss: 0.22579418447302893
  batch 327 loss: 0.22587483096013375
  batch 328 loss: 0.225843752847939
  batch 329 loss: 0.2258033354775159
  batch 330 loss: 0.2258015578443354
  batch 331 loss: 0.2257579789691101
  batch 332 loss: 0.22585058988756443
  batch 333 loss: 0.22590721830412433
  batch 334 loss: 0.22624186756546627
  batch 335 loss: 0.22640534990759037
  batch 336 loss: 0.2264800581282803
  batch 337 loss: 0.22660504281520844
  batch 338 loss: 0.2266401714650837
  batch 339 loss: 0.22665220951787843
  batch 340 loss: 0.22662047099541216
  batch 341 loss: 0.22655694986368555
  batch 342 loss: 0.22649517202237893
  batch 343 loss: 0.22656309952193723
  batch 344 loss: 0.2265843610864046
  batch 345 loss: 0.22663755149081133
  batch 346 loss: 0.22666535031244245
  batch 347 loss: 0.22655902352044494
  batch 348 loss: 0.2265443843842923
  batch 349 loss: 0.22659358113235595
  batch 350 loss: 0.2267676060966083
  batch 351 loss: 0.22683720010468084
  batch 352 loss: 0.22674723998220128
  batch 353 loss: 0.22676196242695829
  batch 354 loss: 0.22672695051983924
  batch 355 loss: 0.2266585476801429
  batch 356 loss: 0.226850554933039
  batch 357 loss: 0.22673996786276499
  batch 358 loss: 0.22660723474272138
  batch 359 loss: 0.22664239811698042
  batch 360 loss: 0.22665665915442837
  batch 361 loss: 0.22654231028873834
  batch 362 loss: 0.22652311903007782
  batch 363 loss: 0.22654438790539408
  batch 364 loss: 0.22645919434316866
  batch 365 loss: 0.22648265700634213
  batch 366 loss: 0.22648264085008799
  batch 367 loss: 0.2264629346635751
  batch 368 loss: 0.2264724822876894
  batch 369 loss: 0.22642960110654028
  batch 370 loss: 0.2264626131669895
  batch 371 loss: 0.22653717774586535
  batch 372 loss: 0.2266028718922728
  batch 373 loss: 0.22660247534113978
  batch 374 loss: 0.22648043934514817
  batch 375 loss: 0.22656959553559622
  batch 376 loss: 0.22669659582699866
  batch 377 loss: 0.22666987531697402
  batch 378 loss: 0.22671192135445026
  batch 379 loss: 0.22667483399600027
  batch 380 loss: 0.22664654294126912
  batch 381 loss: 0.2265491087058085
  batch 382 loss: 0.2265313848541045
  batch 383 loss: 0.2264099579022385
  batch 384 loss: 0.2264285763958469
  batch 385 loss: 0.22637588664308772
  batch 386 loss: 0.22633134774439076
  batch 387 loss: 0.22624683872981896
  batch 388 loss: 0.22617250684763968
  batch 389 loss: 0.22616069696862778
  batch 390 loss: 0.22609516301980384
  batch 391 loss: 0.2260316552027412
  batch 392 loss: 0.2261233654785521
  batch 393 loss: 0.226080833310996
  batch 394 loss: 0.22614553877091045
  batch 395 loss: 0.22619788292088086
  batch 396 loss: 0.22618593527662634
  batch 397 loss: 0.22627045484423938
  batch 398 loss: 0.22635779537297973
  batch 399 loss: 0.22637678824719928
  batch 400 loss: 0.22647539768368005
  batch 401 loss: 0.22650136532926202
  batch 402 loss: 0.2265401019237528
  batch 403 loss: 0.22656887488684643
  batch 404 loss: 0.22657636091998307
  batch 405 loss: 0.22659077291135435
  batch 406 loss: 0.22649473403328158
  batch 407 loss: 0.2265397941084986
  batch 408 loss: 0.2264708265223924
  batch 409 loss: 0.22649379828739866
  batch 410 loss: 0.22639431440975608
  batch 411 loss: 0.22637321598773455
  batch 412 loss: 0.22637733820717312
  batch 413 loss: 0.22630019291116885
  batch 414 loss: 0.22625453618989474
  batch 415 loss: 0.22622398877718364
  batch 416 loss: 0.2262115146463307
  batch 417 loss: 0.22616847131035026
  batch 418 loss: 0.22619303062772067
  batch 419 loss: 0.22620636505699385
  batch 420 loss: 0.22618017189559483
  batch 421 loss: 0.22622060563388743
  batch 422 loss: 0.22615368946751147
  batch 423 loss: 0.22617066998571933
  batch 424 loss: 0.22624363968113684
  batch 425 loss: 0.22619422036058762
  batch 426 loss: 0.22615496975155502
  batch 427 loss: 0.22622924184631688
  batch 428 loss: 0.2263131964011727
  batch 429 loss: 0.22622518682535434
  batch 430 loss: 0.22629649098529372
  batch 431 loss: 0.22628477174278755
  batch 432 loss: 0.22630853837149012
  batch 433 loss: 0.22625876195711572
  batch 434 loss: 0.22612377307656722
  batch 435 loss: 0.2261093552085175
  batch 436 loss: 0.226041601030925
  batch 437 loss: 0.22595095801544407
  batch 438 loss: 0.22591572326340087
  batch 439 loss: 0.22582453591133847
  batch 440 loss: 0.22585602216422557
  batch 441 loss: 0.22581915797289806
  batch 442 loss: 0.22580172583393382
  batch 443 loss: 0.22580644824703983
  batch 444 loss: 0.22579360746585572
  batch 445 loss: 0.2257811445868417
  batch 446 loss: 0.2257681491329531
  batch 447 loss: 0.2257732942333691
  batch 448 loss: 0.22588634783668177
  batch 449 loss: 0.22581065736528494
  batch 450 loss: 0.22577223509550096
  batch 451 loss: 0.225750288684458
  batch 452 loss: 0.2256530286266213
  batch 453 loss: 0.22559428428840428
  batch 454 loss: 0.22566376071800745
  batch 455 loss: 0.22565990545592465
  batch 456 loss: 0.2255964670330286
  batch 457 loss: 0.2255685593223676
  batch 458 loss: 0.22556188718850956
  batch 459 loss: 0.22555363064627762
  batch 460 loss: 0.2255939667639525
  batch 461 loss: 0.22563066244642543
  batch 462 loss: 0.22566775935681868
  batch 463 loss: 0.2256626507697816
  batch 464 loss: 0.22562854778792324
  batch 465 loss: 0.22565057838475833
  batch 466 loss: 0.22557358552457948
  batch 467 loss: 0.22569093146467106
  batch 468 loss: 0.22560794715188506
  batch 469 loss: 0.22551236198400892
  batch 470 loss: 0.22552215285757754
  batch 471 loss: 0.225531916646188
  batch 472 loss: 0.22538036707851847
LOSS train 0.22538036707851847 valid 0.27378880977630615
LOSS train 0.22538036707851847 valid 0.2684340327978134
LOSS train 0.22538036707851847 valid 0.2890215416749318
LOSS train 0.22538036707851847 valid 0.2752910740673542
LOSS train 0.22538036707851847 valid 0.27405809462070463
LOSS train 0.22538036707851847 valid 0.27436641107002896
LOSS train 0.22538036707851847 valid 0.2666460211787905
LOSS train 0.22538036707851847 valid 0.26300669834017754
LOSS train 0.22538036707851847 valid 0.2636055847009023
LOSS train 0.22538036707851847 valid 0.2626820236444473
LOSS train 0.22538036707851847 valid 0.26298724792220374
LOSS train 0.22538036707851847 valid 0.2652728656927745
LOSS train 0.22538036707851847 valid 0.26545005578261155
LOSS train 0.22538036707851847 valid 0.263022885790893
LOSS train 0.22538036707851847 valid 0.26291174193223316
LOSS train 0.22538036707851847 valid 0.2678304994478822
LOSS train 0.22538036707851847 valid 0.2664860320441863
LOSS train 0.22538036707851847 valid 0.2669272977444861
LOSS train 0.22538036707851847 valid 0.27057210787346486
LOSS train 0.22538036707851847 valid 0.26953117102384566
LOSS train 0.22538036707851847 valid 0.27316260905492873
LOSS train 0.22538036707851847 valid 0.27255356311798096
LOSS train 0.22538036707851847 valid 0.2704109789236732
LOSS train 0.22538036707851847 valid 0.270710414275527
LOSS train 0.22538036707851847 valid 0.2702767497301102
LOSS train 0.22538036707851847 valid 0.26930402849729246
LOSS train 0.22538036707851847 valid 0.26946349773142075
LOSS train 0.22538036707851847 valid 0.2700859835105283
LOSS train 0.22538036707851847 valid 0.2688102321378116
LOSS train 0.22538036707851847 valid 0.26909478306770324
LOSS train 0.22538036707851847 valid 0.2697433204420151
LOSS train 0.22538036707851847 valid 0.2704214109107852
LOSS train 0.22538036707851847 valid 0.2687764912843704
LOSS train 0.22538036707851847 valid 0.2683075074763859
LOSS train 0.22538036707851847 valid 0.26894871209348953
LOSS train 0.22538036707851847 valid 0.2698247263001071
LOSS train 0.22538036707851847 valid 0.27037421311881094
LOSS train 0.22538036707851847 valid 0.27064672308532817
LOSS train 0.22538036707851847 valid 0.2715514772213422
LOSS train 0.22538036707851847 valid 0.27159394808113574
LOSS train 0.22538036707851847 valid 0.2714444874990277
LOSS train 0.22538036707851847 valid 0.2726359186427934
LOSS train 0.22538036707851847 valid 0.27353827378084494
LOSS train 0.22538036707851847 valid 0.2730861363763159
LOSS train 0.22538036707851847 valid 0.272456607553694
LOSS train 0.22538036707851847 valid 0.2721921866354735
LOSS train 0.22538036707851847 valid 0.2717453196961829
LOSS train 0.22538036707851847 valid 0.27333463045458
LOSS train 0.22538036707851847 valid 0.2720803265668908
LOSS train 0.22538036707851847 valid 0.2724835479259491
LOSS train 0.22538036707851847 valid 0.27230862016771357
LOSS train 0.22538036707851847 valid 0.2718062572754346
LOSS train 0.22538036707851847 valid 0.27326912193928127
LOSS train 0.22538036707851847 valid 0.27332642398498674
LOSS train 0.22538036707851847 valid 0.2731196430596438
LOSS train 0.22538036707851847 valid 0.2732279843517712
LOSS train 0.22538036707851847 valid 0.27251056751661135
LOSS train 0.22538036707851847 valid 0.2728560184096468
LOSS train 0.22538036707851847 valid 0.27259594479859883
LOSS train 0.22538036707851847 valid 0.27246235236525534
LOSS train 0.22538036707851847 valid 0.2722820168147322
LOSS train 0.22538036707851847 valid 0.2721638648260024
LOSS train 0.22538036707851847 valid 0.2716812538722205
LOSS train 0.22538036707851847 valid 0.2724494389258325
LOSS train 0.22538036707851847 valid 0.2709902045818476
LOSS train 0.22538036707851847 valid 0.2706786662791715
LOSS train 0.22538036707851847 valid 0.2713818605711211
LOSS train 0.22538036707851847 valid 0.2706986766527681
LOSS train 0.22538036707851847 valid 0.2714707873005798
LOSS train 0.22538036707851847 valid 0.27216732118810927
LOSS train 0.22538036707851847 valid 0.2728078927792294
LOSS train 0.22538036707851847 valid 0.27377066595686805
LOSS train 0.22538036707851847 valid 0.27457595238946886
LOSS train 0.22538036707851847 valid 0.27412472343122635
LOSS train 0.22538036707851847 valid 0.2736375705401103
LOSS train 0.22538036707851847 valid 0.27378284970396444
LOSS train 0.22538036707851847 valid 0.2736222283406691
LOSS train 0.22538036707851847 valid 0.27351027039381176
LOSS train 0.22538036707851847 valid 0.273568031531346
LOSS train 0.22538036707851847 valid 0.27312929183244705
LOSS train 0.22538036707851847 valid 0.27352510263890395
LOSS train 0.22538036707851847 valid 0.2736103382052445
LOSS train 0.22538036707851847 valid 0.273729246064841
LOSS train 0.22538036707851847 valid 0.2732435459537165
LOSS train 0.22538036707851847 valid 0.2742032536688973
LOSS train 0.22538036707851847 valid 0.2742722656491191
LOSS train 0.22538036707851847 valid 0.2739177491815611
LOSS train 0.22538036707851847 valid 0.2743553375317292
LOSS train 0.22538036707851847 valid 0.2749571724888984
LOSS train 0.22538036707851847 valid 0.27541664524210824
LOSS train 0.22538036707851847 valid 0.275312052487017
LOSS train 0.22538036707851847 valid 0.27534250315764675
LOSS train 0.22538036707851847 valid 0.2751165585171792
LOSS train 0.22538036707851847 valid 0.27560747446531947
LOSS train 0.22538036707851847 valid 0.2758648698267184
LOSS train 0.22538036707851847 valid 0.2762349780338506
LOSS train 0.22538036707851847 valid 0.27635809548736845
LOSS train 0.22538036707851847 valid 0.2768820589598344
LOSS train 0.22538036707851847 valid 0.2770218061979371
LOSS train 0.22538036707851847 valid 0.27733846291899683
LOSS train 0.22538036707851847 valid 0.2773952907559895
LOSS train 0.22538036707851847 valid 0.2778103887742641
LOSS train 0.22538036707851847 valid 0.2775207614725076
LOSS train 0.22538036707851847 valid 0.27724002258708846
LOSS train 0.22538036707851847 valid 0.2778146668559029
LOSS train 0.22538036707851847 valid 0.2780114658317476
LOSS train 0.22538036707851847 valid 0.2776923288251752
LOSS train 0.22538036707851847 valid 0.2778525658779674
LOSS train 0.22538036707851847 valid 0.2773532000703549
LOSS train 0.22538036707851847 valid 0.2772782163186507
LOSS train 0.22538036707851847 valid 0.2773253329702326
LOSS train 0.22538036707851847 valid 0.2775547730603388
LOSS train 0.22538036707851847 valid 0.2772562126670263
LOSS train 0.22538036707851847 valid 0.27738165986119656
LOSS train 0.22538036707851847 valid 0.27835972464602926
LOSS train 0.22538036707851847 valid 0.2781053348604975
LOSS train 0.22538036707851847 valid 0.2788462726733623
LOSS train 0.22538036707851847 valid 0.2787089639548528
LOSS train 0.22538036707851847 valid 0.27840129630405364
LOSS train 0.22538036707851847 valid 0.27793203877906003
LOSS train 0.22538036707851847 valid 0.2776830807205074
LOSS train 0.22538036707851847 valid 0.277942096600767
LOSS train 0.22538036707851847 valid 0.27793905841625804
LOSS train 0.22538036707851847 valid 0.27824814713770346
LOSS train 0.22538036707851847 valid 0.27810636591911314
LOSS train 0.22538036707851847 valid 0.2784001997538975
LOSS train 0.22538036707851847 valid 0.27823707062428393
LOSS train 0.22538036707851847 valid 0.2781562143936753
LOSS train 0.22538036707851847 valid 0.2777728687885196
LOSS train 0.22538036707851847 valid 0.27746929136606363
LOSS train 0.22538036707851847 valid 0.27754791843072146
LOSS train 0.22538036707851847 valid 0.27739210900935257
LOSS train 0.22538036707851847 valid 0.27741632336064387
LOSS train 0.22538036707851847 valid 0.2776743542792192
LOSS train 0.22538036707851847 valid 0.27787455519040427
LOSS train 0.22538036707851847 valid 0.2777024647768806
LOSS train 0.22538036707851847 valid 0.2776697323705158
LOSS train 0.22538036707851847 valid 0.27773354645224585
LOSS train 0.22538036707851847 valid 0.27747376659791245
LOSS train 0.22538036707851847 valid 0.2773755731327193
LOSS train 0.22538036707851847 valid 0.2773711455206499
LOSS train 0.22538036707851847 valid 0.2777711511917517
LOSS train 0.22538036707851847 valid 0.27795142289641855
LOSS train 0.22538036707851847 valid 0.2780221891072061
LOSS train 0.22538036707851847 valid 0.2779391693657842
LOSS train 0.22538036707851847 valid 0.27792436619327493
LOSS train 0.22538036707851847 valid 0.27788444745297336
LOSS train 0.22538036707851847 valid 0.27769826312322876
LOSS train 0.22538036707851847 valid 0.2778348958732298
LOSS train 0.22538036707851847 valid 0.2777595786253611
LOSS train 0.22538036707851847 valid 0.2776450122034313
LOSS train 0.22538036707851847 valid 0.27767828655870336
LOSS train 0.22538036707851847 valid 0.2774236200292126
LOSS train 0.22538036707851847 valid 0.2775771004426015
LOSS train 0.22538036707851847 valid 0.2776947944395004
LOSS train 0.22538036707851847 valid 0.2776886541873981
LOSS train 0.22538036707851847 valid 0.2775960579799239
LOSS train 0.22538036707851847 valid 0.2778520133299164
LOSS train 0.22538036707851847 valid 0.2779260459561018
LOSS train 0.22538036707851847 valid 0.2781043851748109
LOSS train 0.22538036707851847 valid 0.2780397736137698
LOSS train 0.22538036707851847 valid 0.27806153727902305
LOSS train 0.22538036707851847 valid 0.2778723420906652
LOSS train 0.22538036707851847 valid 0.2779935055026194
LOSS train 0.22538036707851847 valid 0.2781235815900745
LOSS train 0.22538036707851847 valid 0.2780614382172205
LOSS train 0.22538036707851847 valid 0.2780654155565593
LOSS train 0.22538036707851847 valid 0.2777797274646305
LOSS train 0.22538036707851847 valid 0.2775989283295073
LOSS train 0.22538036707851847 valid 0.2777994330315029
LOSS train 0.22538036707851847 valid 0.27780117774218843
LOSS train 0.22538036707851847 valid 0.2778538793845232
LOSS train 0.22538036707851847 valid 0.27763092681507157
LOSS train 0.22538036707851847 valid 0.2781484456754279
LOSS train 0.22538036707851847 valid 0.2781177758319037
LOSS train 0.22538036707851847 valid 0.2780813502825119
LOSS train 0.22538036707851847 valid 0.278022575092181
LOSS train 0.22538036707851847 valid 0.27812267061364787
LOSS train 0.22538036707851847 valid 0.2782925188374919
LOSS train 0.22538036707851847 valid 0.278111058473587
LOSS train 0.22538036707851847 valid 0.2780745133181303
LOSS train 0.22538036707851847 valid 0.2777990264879478
LOSS train 0.22538036707851847 valid 0.2779828502180798
LOSS train 0.22538036707851847 valid 0.2778732154680335
LOSS train 0.22538036707851847 valid 0.2776726059816979
LOSS train 0.22538036707851847 valid 0.2778764374313816
LOSS train 0.22538036707851847 valid 0.2777616479658188
LOSS train 0.22538036707851847 valid 0.2779350130957492
LOSS train 0.22538036707851847 valid 0.27757062167717667
LOSS train 0.22538036707851847 valid 0.2777307962116442
LOSS train 0.22538036707851847 valid 0.27788436053935145
LOSS train 0.22538036707851847 valid 0.2779220237086217
LOSS train 0.22538036707851847 valid 0.27759798164503563
LOSS train 0.22538036707851847 valid 0.2774101148561104
LOSS train 0.22538036707851847 valid 0.2773423194885254
LOSS train 0.22538036707851847 valid 0.2774891702800381
LOSS train 0.22538036707851847 valid 0.27744433495599
LOSS train 0.22538036707851847 valid 0.27763129800859127
LOSS train 0.22538036707851847 valid 0.277606263382351
LOSS train 0.22538036707851847 valid 0.27782071962952615
LOSS train 0.22538036707851847 valid 0.27760683630236344
LOSS train 0.22538036707851847 valid 0.2776369430346064
LOSS train 0.22538036707851847 valid 0.27769620811997964
LOSS train 0.22538036707851847 valid 0.2773737682431352
LOSS train 0.22538036707851847 valid 0.2771754312806013
LOSS train 0.22538036707851847 valid 0.27722927855635154
LOSS train 0.22538036707851847 valid 0.27710072473051467
LOSS train 0.22538036707851847 valid 0.276877752935084
LOSS train 0.22538036707851847 valid 0.2769765880261882
LOSS train 0.22538036707851847 valid 0.2771801334761438
LOSS train 0.22538036707851847 valid 0.27739649481400497
LOSS train 0.22538036707851847 valid 0.27720718282573625
LOSS train 0.22538036707851847 valid 0.2771685352907494
LOSS train 0.22538036707851847 valid 0.27719192652501795
LOSS train 0.22538036707851847 valid 0.27697445265082427
LOSS train 0.22538036707851847 valid 0.2767517926102435
LOSS train 0.22538036707851847 valid 0.2764538629538453
LOSS train 0.22538036707851847 valid 0.2764680810751171
LOSS train 0.22538036707851847 valid 0.27647403154743316
LOSS train 0.22538036707851847 valid 0.27656557153571737
LOSS train 0.22538036707851847 valid 0.27652507818122796
LOSS train 0.22538036707851847 valid 0.27664671328153695
LOSS train 0.22538036707851847 valid 0.2768631974410583
LOSS train 0.22538036707851847 valid 0.27675934987408773
LOSS train 0.22538036707851847 valid 0.2766380066341824
LOSS train 0.22538036707851847 valid 0.2765323958829441
LOSS train 0.22538036707851847 valid 0.27650838447037246
LOSS train 0.22538036707851847 valid 0.27643048436495293
LOSS train 0.22538036707851847 valid 0.2763135645327089
LOSS train 0.22538036707851847 valid 0.27633731261543604
LOSS train 0.22538036707851847 valid 0.27632165006744913
LOSS train 0.22538036707851847 valid 0.2764504740721193
LOSS train 0.22538036707851847 valid 0.2768511333434879
LOSS train 0.22538036707851847 valid 0.2767874278828629
LOSS train 0.22538036707851847 valid 0.2769601947449623
LOSS train 0.22538036707851847 valid 0.2767729840036166
LOSS train 0.22538036707851847 valid 0.2768048937059153
LOSS train 0.22538036707851847 valid 0.2767029194270863
LOSS train 0.22538036707851847 valid 0.27662938745949556
LOSS train 0.22538036707851847 valid 0.27618243483205634
LOSS train 0.22538036707851847 valid 0.2759994141664742
LOSS train 0.22538036707851847 valid 0.2759348526597023
LOSS train 0.22538036707851847 valid 0.27571142584453395
LOSS train 0.22538036707851847 valid 0.2757466051300041
LOSS train 0.22538036707851847 valid 0.2759196943774515
LOSS train 0.22538036707851847 valid 0.27605342447030834
LOSS train 0.22538036707851847 valid 0.27606550106394145
LOSS train 0.22538036707851847 valid 0.27607953542422864
LOSS train 0.22538036707851847 valid 0.27580222026650686
LOSS train 0.22538036707851847 valid 0.2759855075478554
LOSS train 0.22538036707851847 valid 0.27606686563843275
LOSS train 0.22538036707851847 valid 0.2759104160562394
LOSS train 0.22538036707851847 valid 0.2756457097916735
LOSS train 0.22538036707851847 valid 0.27576939644306664
LOSS train 0.22538036707851847 valid 0.27565330085801143
LOSS train 0.22538036707851847 valid 0.2753291664412245
LOSS train 0.22538036707851847 valid 0.27532484698388365
LOSS train 0.22538036707851847 valid 0.2755587487950806
LOSS train 0.22538036707851847 valid 0.2753416098452903
LOSS train 0.22538036707851847 valid 0.2751429051733934
LOSS train 0.22538036707851847 valid 0.27521380879184754
LOSS train 0.22538036707851847 valid 0.2754761341760177
LOSS train 0.22538036707851847 valid 0.2752295801054842
LOSS train 0.22538036707851847 valid 0.2753133328462189
LOSS train 0.22538036707851847 valid 0.27524313774873627
LOSS train 0.22538036707851847 valid 0.2753575155721571
LOSS train 0.22538036707851847 valid 0.2752774582708373
LOSS train 0.22538036707851847 valid 0.27541151692840593
LOSS train 0.22538036707851847 valid 0.27552739577887225
LOSS train 0.22538036707851847 valid 0.27545804850481176
LOSS train 0.22538036707851847 valid 0.2754889214720673
LOSS train 0.22538036707851847 valid 0.27532560224918756
LOSS train 0.22538036707851847 valid 0.2752085633766957
LOSS train 0.22538036707851847 valid 0.2750805942137746
LOSS train 0.22538036707851847 valid 0.27516083191741597
LOSS train 0.22538036707851847 valid 0.2752198147708955
LOSS train 0.22538036707851847 valid 0.2753255546415756
LOSS train 0.22538036707851847 valid 0.27537161985532843
LOSS train 0.22538036707851847 valid 0.27522210119872964
LOSS train 0.22538036707851847 valid 0.27510392602000916
LOSS train 0.22538036707851847 valid 0.27491236295140087
LOSS train 0.22538036707851847 valid 0.27483147245349615
LOSS train 0.22538036707851847 valid 0.2748782071334313
LOSS train 0.22538036707851847 valid 0.2748148641745809
LOSS train 0.22538036707851847 valid 0.2749073346455892
LOSS train 0.22538036707851847 valid 0.27485502610256624
LOSS train 0.22538036707851847 valid 0.2748540462309475
LOSS train 0.22538036707851847 valid 0.27478187676105237
LOSS train 0.22538036707851847 valid 0.274704325467245
LOSS train 0.22538036707851847 valid 0.2745979927737137
LOSS train 0.22538036707851847 valid 0.27456764369895775
LOSS train 0.22538036707851847 valid 0.2744937219644246
LOSS train 0.22538036707851847 valid 0.27446449128434114
LOSS train 0.22538036707851847 valid 0.27449880307223523
LOSS train 0.22538036707851847 valid 0.2744989959870355
LOSS train 0.22538036707851847 valid 0.27445995012247887
LOSS train 0.22538036707851847 valid 0.27439007273426763
LOSS train 0.22538036707851847 valid 0.2744645889373434
LOSS train 0.22538036707851847 valid 0.2745126515726581
LOSS train 0.22538036707851847 valid 0.27450187345345817
LOSS train 0.22538036707851847 valid 0.2743676320658966
LOSS train 0.22538036707851847 valid 0.2743326105818843
LOSS train 0.22538036707851847 valid 0.2742454493203179
LOSS train 0.22538036707851847 valid 0.27426948163070175
LOSS train 0.22538036707851847 valid 0.2741665084342488
LOSS train 0.22538036707851847 valid 0.2744542421176543
LOSS train 0.22538036707851847 valid 0.2743327073534459
LOSS train 0.22538036707851847 valid 0.27429031121072833
LOSS train 0.22538036707851847 valid 0.2742764310832934
LOSS train 0.22538036707851847 valid 0.27422923162098856
LOSS train 0.22538036707851847 valid 0.27426942852340713
LOSS train 0.22538036707851847 valid 0.27419816134258723
LOSS train 0.22538036707851847 valid 0.274299490232818
LOSS train 0.22538036707851847 valid 0.2742689377657927
LOSS train 0.22538036707851847 valid 0.2742246658556045
LOSS train 0.22538036707851847 valid 0.27423385186474536
LOSS train 0.22538036707851847 valid 0.2742237875984294
LOSS train 0.22538036707851847 valid 0.274193582060577
LOSS train 0.22538036707851847 valid 0.2741611239771858
LOSS train 0.22538036707851847 valid 0.27404577429406346
LOSS train 0.22538036707851847 valid 0.2741593864755096
LOSS train 0.22538036707851847 valid 0.2741582256203853
LOSS train 0.22538036707851847 valid 0.273992342753307
LOSS train 0.22538036707851847 valid 0.27393833961751723
LOSS train 0.22538036707851847 valid 0.2737866356281134
LOSS train 0.22538036707851847 valid 0.27379242111385965
LOSS train 0.22538036707851847 valid 0.273728970859758
LOSS train 0.22538036707851847 valid 0.2737401423566952
LOSS train 0.22538036707851847 valid 0.27375332060012414
LOSS train 0.22538036707851847 valid 0.2737766289801309
LOSS train 0.22538036707851847 valid 0.27373369137145964
LOSS train 0.22538036707851847 valid 0.2737062130437558
LOSS train 0.22538036707851847 valid 0.2739674656151293
LOSS train 0.22538036707851847 valid 0.27388733647719116
LOSS train 0.22538036707851847 valid 0.27381576170672234
LOSS train 0.22538036707851847 valid 0.27367691079243306
LOSS train 0.22538036707851847 valid 0.27389589929262326
LOSS train 0.22538036707851847 valid 0.27388290730277465
LOSS train 0.22538036707851847 valid 0.2737815823614773
LOSS train 0.22538036707851847 valid 0.27392645053127235
LOSS train 0.22538036707851847 valid 0.2737919270030914
LOSS train 0.22538036707851847 valid 0.2737228226853393
LOSS train 0.22538036707851847 valid 0.27354750732291194
LOSS train 0.22538036707851847 valid 0.2736907101474529
LOSS train 0.22538036707851847 valid 0.27385508366253064
LOSS train 0.22538036707851847 valid 0.2738150550278625
LOSS train 0.22538036707851847 valid 0.2736393514730745
LOSS train 0.22538036707851847 valid 0.2737313901213394
LOSS train 0.22538036707851847 valid 0.27368619482974954
LOSS train 0.22538036707851847 valid 0.27369757762977054
LOSS train 0.22538036707851847 valid 0.2738062737674115
LOSS train 0.22538036707851847 valid 0.27385628409683704
LOSS train 0.22538036707851847 valid 0.2739002172737554
LOSS train 0.22538036707851847 valid 0.27387115931780326
LOSS train 0.22538036707851847 valid 0.2737061795634283
LOSS train 0.22538036707851847 valid 0.27373856862776735
LOSS train 0.22538036707851847 valid 0.27371100805887655
LOSS train 0.22538036707851847 valid 0.2736066895323759
LOSS train 0.22538036707851847 valid 0.27355220664842544
LOSS train 0.22538036707851847 valid 0.2737746192349328
LOSS train 0.22538036707851847 valid 0.2736087095126551
LOSS train 0.22538036707851847 valid 0.27362195015447577
LOSS train 0.22538036707851847 valid 0.2737047800758325
LOSS train 0.22538036707851847 valid 0.27362985401363166
LOSS train 0.22538036707851847 valid 0.2737089850314676
LOSS train 0.22538036707851847 valid 0.2737138168733628
LOSS train 0.22538036707851847 valid 0.27364934749434366
LOSS train 0.22538036707851847 valid 0.27353099968446337
LOSS train 0.22538036707851847 valid 0.2734950841442356
EPOCH 28:
  batch 1 loss: 0.22206097841262817
  batch 2 loss: 0.24821747839450836
  batch 3 loss: 0.2417997270822525
  batch 4 loss: 0.24287736788392067
  batch 5 loss: 0.2429109275341034
  batch 6 loss: 0.2415346403916677
  batch 7 loss: 0.23972125564302718
  batch 8 loss: 0.23610844835639
  batch 9 loss: 0.23983080188433328
  batch 10 loss: 0.23870422393083573
  batch 11 loss: 0.2399828230792826
  batch 12 loss: 0.23917483538389206
  batch 13 loss: 0.2360148521570059
  batch 14 loss: 0.23508743941783905
  batch 15 loss: 0.23726054430007934
  batch 16 loss: 0.23804698325693607
  batch 17 loss: 0.23745089857017293
  batch 18 loss: 0.2351111521323522
  batch 19 loss: 0.23373340383956306
  batch 20 loss: 0.23161765635013581
  batch 21 loss: 0.2349320806208111
  batch 22 loss: 0.23584451323205774
  batch 23 loss: 0.2388580234154411
  batch 24 loss: 0.2390023171901703
  batch 25 loss: 0.23851353645324708
  batch 26 loss: 0.23808598919556692
  batch 27 loss: 0.23830541085313867
  batch 28 loss: 0.23960542040211813
  batch 29 loss: 0.23931184359665575
  batch 30 loss: 0.23995223691066106
  batch 31 loss: 0.24001772172989383
  batch 32 loss: 0.24094014335423708
  batch 33 loss: 0.2429033283031348
  batch 34 loss: 0.2426178862943369
  batch 35 loss: 0.24381275730473653
  batch 36 loss: 0.24432458356022835
  batch 37 loss: 0.24503590166568756
  batch 38 loss: 0.24475404855452085
  batch 39 loss: 0.24455085091101816
  batch 40 loss: 0.24424008578062056
  batch 41 loss: 0.24374899304494624
  batch 42 loss: 0.24282096468266987
  batch 43 loss: 0.24274928659893746
  batch 44 loss: 0.24233794449405235
  batch 45 loss: 0.24208444125122494
  batch 46 loss: 0.2404523760728214
  batch 47 loss: 0.23923939149430457
  batch 48 loss: 0.23863386114438376
  batch 49 loss: 0.2393578710604687
  batch 50 loss: 0.24029836535453797
  batch 51 loss: 0.239967837637546
  batch 52 loss: 0.23970466021161813
  batch 53 loss: 0.2394317627515433
  batch 54 loss: 0.2388709612466671
  batch 55 loss: 0.2385521043430675
  batch 56 loss: 0.23852579002933844
  batch 57 loss: 0.23795945158130244
  batch 58 loss: 0.2377154362098924
  batch 59 loss: 0.2375556566452576
  batch 60 loss: 0.23807906036575635
  batch 61 loss: 0.23835842555663625
  batch 62 loss: 0.23894573820214118
  batch 63 loss: 0.23808570939397056
  batch 64 loss: 0.2375322077423334
  batch 65 loss: 0.2370945570560602
  batch 66 loss: 0.23660907939527975
  batch 67 loss: 0.23607194090067452
  batch 68 loss: 0.23599810525774956
  batch 69 loss: 0.23611523189406464
  batch 70 loss: 0.23583423644304274
  batch 71 loss: 0.23641323182784335
  batch 72 loss: 0.23643202727867496
  batch 73 loss: 0.23610569536685944
  batch 74 loss: 0.23604675744836395
  batch 75 loss: 0.23588333229223887
  batch 76 loss: 0.23603906698132815
  batch 77 loss: 0.235823634963531
  batch 78 loss: 0.2357981990163143
  batch 79 loss: 0.23591660472411144
  batch 80 loss: 0.23621429093182086
  batch 81 loss: 0.23629283941822288
  batch 82 loss: 0.23647823420966543
  batch 83 loss: 0.2360335315566465
  batch 84 loss: 0.23547469540720894
  batch 85 loss: 0.2356639094212476
  batch 86 loss: 0.23534443125475285
  batch 87 loss: 0.23512668753492422
  batch 88 loss: 0.23465381732041185
  batch 89 loss: 0.23421933979130863
  batch 90 loss: 0.23421654154857
  batch 91 loss: 0.23396440734575083
  batch 92 loss: 0.2338003005994403
  batch 93 loss: 0.2332050830766719
  batch 94 loss: 0.23290674118919574
  batch 95 loss: 0.2326808196933646
  batch 96 loss: 0.23231806481877962
  batch 97 loss: 0.23291752295395762
  batch 98 loss: 0.23281045866255856
  batch 99 loss: 0.2324794183174769
  batch 100 loss: 0.23287393227219583
  batch 101 loss: 0.2328363746994793
  batch 102 loss: 0.23280945580963994
  batch 103 loss: 0.2325239640127108
  batch 104 loss: 0.23273509525908873
  batch 105 loss: 0.2322094874722617
  batch 106 loss: 0.2322304536992649
  batch 107 loss: 0.2321714597327687
  batch 108 loss: 0.23168082121345732
  batch 109 loss: 0.23188144816171138
  batch 110 loss: 0.2319908089258454
  batch 111 loss: 0.23224005707212397
  batch 112 loss: 0.2320694677265627
  batch 113 loss: 0.2317855400321758
  batch 114 loss: 0.23173786908910984
  batch 115 loss: 0.2313351070103438
  batch 116 loss: 0.23123165348480487
  batch 117 loss: 0.23106168503435248
  batch 118 loss: 0.23115886596299834
  batch 119 loss: 0.23140129346807464
  batch 120 loss: 0.23118330811460813
  batch 121 loss: 0.23107994292393202
  batch 122 loss: 0.23071443301732422
  batch 123 loss: 0.23033262474265526
  batch 124 loss: 0.23015303032532816
  batch 125 loss: 0.22975963151454926
  batch 126 loss: 0.2296937708816831
  batch 127 loss: 0.22997846166918598
  batch 128 loss: 0.22987298492807895
  batch 129 loss: 0.23016343664291294
  batch 130 loss: 0.23017213837458536
  batch 131 loss: 0.23018229963215253
  batch 132 loss: 0.2304435602643273
  batch 133 loss: 0.2303932669915651
  batch 134 loss: 0.2304205275071201
  batch 135 loss: 0.23025351221914644
  batch 136 loss: 0.23050500156686587
  batch 137 loss: 0.23067543051973746
  batch 138 loss: 0.23064011929259784
  batch 139 loss: 0.23037686633120338
  batch 140 loss: 0.23062196682606426
  batch 141 loss: 0.2303020608551959
  batch 142 loss: 0.23008638830252096
  batch 143 loss: 0.2301949852413231
  batch 144 loss: 0.2301087715766496
  batch 145 loss: 0.23011324909226646
  batch 146 loss: 0.2300735011900941
  batch 147 loss: 0.23017915029104064
  batch 148 loss: 0.23024361902797544
  batch 149 loss: 0.22986278557937417
  batch 150 loss: 0.22995024224122365
  batch 151 loss: 0.2295802410272573
  batch 152 loss: 0.22955230701910823
  batch 153 loss: 0.22953881888218175
  batch 154 loss: 0.22926461106383955
  batch 155 loss: 0.22922969950783637
  batch 156 loss: 0.22924801907860315
  batch 157 loss: 0.22927095517990695
  batch 158 loss: 0.22934870280419725
  batch 159 loss: 0.2294204960454185
  batch 160 loss: 0.2294831491075456
  batch 161 loss: 0.22918505439106723
  batch 162 loss: 0.2292095935087145
  batch 163 loss: 0.22910751347527183
  batch 164 loss: 0.22921953214014448
  batch 165 loss: 0.22952588787584594
  batch 166 loss: 0.22986589417041067
  batch 167 loss: 0.23000257020581982
  batch 168 loss: 0.22980858447651067
  batch 169 loss: 0.22984350794518488
  batch 170 loss: 0.22982684435213313
  batch 171 loss: 0.22999921467220574
  batch 172 loss: 0.22997621290905532
  batch 173 loss: 0.22997951791810162
  batch 174 loss: 0.23004970708112607
  batch 175 loss: 0.23014577048165458
  batch 176 loss: 0.23056964102116498
  batch 177 loss: 0.23094118769559482
  batch 178 loss: 0.23070256604572362
  batch 179 loss: 0.23070963359744856
  batch 180 loss: 0.23089848433931667
  batch 181 loss: 0.23086734231335024
  batch 182 loss: 0.230737998180992
  batch 183 loss: 0.2307060703716643
  batch 184 loss: 0.23074202085642712
  batch 185 loss: 0.23055382467604973
  batch 186 loss: 0.23063328766053723
  batch 187 loss: 0.23087683431604966
  batch 188 loss: 0.23082768623816205
  batch 189 loss: 0.23083745983857956
  batch 190 loss: 0.2307621095525591
  batch 191 loss: 0.23081518397593373
  batch 192 loss: 0.23059525061398745
  batch 193 loss: 0.2305476050920437
  batch 194 loss: 0.2304765791776254
  batch 195 loss: 0.23073424108517476
  batch 196 loss: 0.23061575983860055
  batch 197 loss: 0.23069192794373797
  batch 198 loss: 0.2307522071130348
  batch 199 loss: 0.23091924460089985
  batch 200 loss: 0.2309132520854473
  batch 201 loss: 0.23077658975302284
  batch 202 loss: 0.23085005264176
  batch 203 loss: 0.2309069913155927
  batch 204 loss: 0.231264135635951
  batch 205 loss: 0.23141571552288243
  batch 206 loss: 0.23146500539721795
  batch 207 loss: 0.23146160594795062
  batch 208 loss: 0.231268260986186
  batch 209 loss: 0.2314666548841878
  batch 210 loss: 0.23158052599146253
  batch 211 loss: 0.2314970724249338
  batch 212 loss: 0.23152269359748318
  batch 213 loss: 0.23152418280711196
  batch 214 loss: 0.23148229669466197
  batch 215 loss: 0.23148156868857006
  batch 216 loss: 0.2314043994303103
  batch 217 loss: 0.2314459257823531
  batch 218 loss: 0.2314397284607275
  batch 219 loss: 0.23116563954581953
  batch 220 loss: 0.23112478561022065
  batch 221 loss: 0.23096851923616762
  batch 222 loss: 0.231069417351538
  batch 223 loss: 0.231049747009983
  batch 224 loss: 0.2309238192891436
  batch 225 loss: 0.23082933478885226
  batch 226 loss: 0.2309046448991362
  batch 227 loss: 0.2310666879762112
  batch 228 loss: 0.23110769094343772
  batch 229 loss: 0.23112607256033535
  batch 230 loss: 0.23111221064691959
  batch 231 loss: 0.23132980837450398
  batch 232 loss: 0.23136455430809794
  batch 233 loss: 0.23132413766414822
  batch 234 loss: 0.231232738838746
  batch 235 loss: 0.2312369481680241
  batch 236 loss: 0.23123519109972454
  batch 237 loss: 0.23103422693562406
  batch 238 loss: 0.23101474319686408
  batch 239 loss: 0.23098952980969242
  batch 240 loss: 0.23110472057014703
  batch 241 loss: 0.23097031684089994
  batch 242 loss: 0.23081421562709098
  batch 243 loss: 0.23086234908780934
  batch 244 loss: 0.23086858552987458
  batch 245 loss: 0.23072754990081398
  batch 246 loss: 0.23063112886213674
  batch 247 loss: 0.23090982587955258
  batch 248 loss: 0.23115463080185075
  batch 249 loss: 0.2312712334844482
  batch 250 loss: 0.23115959799289704
  batch 251 loss: 0.23112047727839405
  batch 252 loss: 0.23110995885162128
  batch 253 loss: 0.23127332418096866
  batch 254 loss: 0.23133149813479326
  batch 255 loss: 0.23124566627483742
  batch 256 loss: 0.23123282607411966
  batch 257 loss: 0.2314854560543127
  batch 258 loss: 0.23154108705677728
  batch 259 loss: 0.23148332121979776
  batch 260 loss: 0.23143266347738412
  batch 261 loss: 0.2314652471706785
  batch 262 loss: 0.2315862950022894
  batch 263 loss: 0.23145617148042177
  batch 264 loss: 0.23151535241667068
  batch 265 loss: 0.23144790142212274
  batch 266 loss: 0.23141362103528546
  batch 267 loss: 0.2312348744507586
  batch 268 loss: 0.23114355214273752
  batch 269 loss: 0.23115957398175307
  batch 270 loss: 0.2310895245384287
  batch 271 loss: 0.2311287064283976
  batch 272 loss: 0.2310918968937853
  batch 273 loss: 0.2310193331141175
  batch 274 loss: 0.23091292098490862
  batch 275 loss: 0.23092009555209767
  batch 276 loss: 0.23084920736542647
  batch 277 loss: 0.23086779357509063
  batch 278 loss: 0.2307273685503349
  batch 279 loss: 0.23072079774726675
  batch 280 loss: 0.2306812573224306
  batch 281 loss: 0.2308493915719918
  batch 282 loss: 0.2306960202383657
  batch 283 loss: 0.23059559121148746
  batch 284 loss: 0.230622006606468
  batch 285 loss: 0.23075425264082458
  batch 286 loss: 0.23086766737115966
  batch 287 loss: 0.2307933294295434
  batch 288 loss: 0.23081110614455408
  batch 289 loss: 0.2307022113391685
  batch 290 loss: 0.23075356981877623
  batch 291 loss: 0.23083351521762377
  batch 292 loss: 0.23090238925324727
  batch 293 loss: 0.2309312732960177
  batch 294 loss: 0.23073283876894282
  batch 295 loss: 0.23086004464303034
  batch 296 loss: 0.2309625117057884
  batch 297 loss: 0.23086870072867333
  batch 298 loss: 0.23084350950365898
  batch 299 loss: 0.23096509382477573
  batch 300 loss: 0.2309012584388256
  batch 301 loss: 0.23085680707942607
  batch 302 loss: 0.23094795688690728
  batch 303 loss: 0.2309194458494879
  batch 304 loss: 0.23087126746969788
  batch 305 loss: 0.23088998100796684
  batch 306 loss: 0.23094479379311106
  batch 307 loss: 0.23092375535530066
  batch 308 loss: 0.23105908698075778
  batch 309 loss: 0.2310095271246333
  batch 310 loss: 0.23107953907981996
  batch 311 loss: 0.23096932676827409
  batch 312 loss: 0.23098473298626068
  batch 313 loss: 0.23099909656154463
  batch 314 loss: 0.23104068186062915
  batch 315 loss: 0.2310189066898255
  batch 316 loss: 0.23105638337474835
  batch 317 loss: 0.2310805784226968
  batch 318 loss: 0.2308940794201767
  batch 319 loss: 0.23076965494028828
  batch 320 loss: 0.2307027022819966
  batch 321 loss: 0.23087220432416672
  batch 322 loss: 0.23076991056618484
  batch 323 loss: 0.23065212650380268
  batch 324 loss: 0.23069639825894508
  batch 325 loss: 0.23085842563555792
  batch 326 loss: 0.23101281708369226
  batch 327 loss: 0.23108400554831968
  batch 328 loss: 0.23111307757293306
  batch 329 loss: 0.23105079252669153
  batch 330 loss: 0.23109317236777507
  batch 331 loss: 0.23104635929953296
  batch 332 loss: 0.2311248825526381
  batch 333 loss: 0.23112100541770636
  batch 334 loss: 0.2314600181615281
  batch 335 loss: 0.23158381367797282
  batch 336 loss: 0.23169605957255476
  batch 337 loss: 0.23182293675419838
  batch 338 loss: 0.23188595949898105
  batch 339 loss: 0.23193680031285524
  batch 340 loss: 0.23195281554670893
  batch 341 loss: 0.2318778138531268
  batch 342 loss: 0.23180548955648267
  batch 343 loss: 0.2319021430463902
  batch 344 loss: 0.2320025808849307
  batch 345 loss: 0.23203554736531298
  batch 346 loss: 0.23204530792773803
  batch 347 loss: 0.23193092210492072
  batch 348 loss: 0.23194967330187216
  batch 349 loss: 0.23196988537864904
  batch 350 loss: 0.23212715446949006
  batch 351 loss: 0.23222570040626744
  batch 352 loss: 0.23218117366460236
  batch 353 loss: 0.2321923343892138
  batch 354 loss: 0.2320929594380034
  batch 355 loss: 0.23200290996423908
  batch 356 loss: 0.2321624681772141
  batch 357 loss: 0.23204065437744312
  batch 358 loss: 0.23193518267996485
  batch 359 loss: 0.2318649521113106
  batch 360 loss: 0.2317871087541183
  batch 361 loss: 0.2317267858173048
  batch 362 loss: 0.23177490457480784
  batch 363 loss: 0.23172593650410656
  batch 364 loss: 0.23157259588549425
  batch 365 loss: 0.23165583884062832
  batch 366 loss: 0.23162814514721677
  batch 367 loss: 0.23168337292339886
  batch 368 loss: 0.23165495956883483
  batch 369 loss: 0.23161975401365337
  batch 370 loss: 0.23168944720480894
  batch 371 loss: 0.2317816777772338
  batch 372 loss: 0.2318552927544681
  batch 373 loss: 0.23187410803486772
  batch 374 loss: 0.2317750018468515
  batch 375 loss: 0.23187471020221712
  batch 376 loss: 0.23200132955420524
  batch 377 loss: 0.2319647899040809
  batch 378 loss: 0.23197395803909454
  batch 379 loss: 0.23191953472065738
  batch 380 loss: 0.23188472496051538
  batch 381 loss: 0.2317904158057816
  batch 382 loss: 0.23176917757507395
  batch 383 loss: 0.2316897559337143
  batch 384 loss: 0.23171635356266052
  batch 385 loss: 0.23164126691106077
  batch 386 loss: 0.2316346954110373
  batch 387 loss: 0.23157614122989567
  batch 388 loss: 0.23149483059484935
  batch 389 loss: 0.23147056862750212
  batch 390 loss: 0.23141180574893952
  batch 391 loss: 0.23134704913629595
  batch 392 loss: 0.23135464714497936
  batch 393 loss: 0.23138939321951102
  batch 394 loss: 0.2314202709095127
  batch 395 loss: 0.231479036732565
  batch 396 loss: 0.23148579402553915
  batch 397 loss: 0.2316031614013523
  batch 398 loss: 0.23166072769230933
  batch 399 loss: 0.23164017162376777
  batch 400 loss: 0.2317241083458066
  batch 401 loss: 0.23175555664553607
  batch 402 loss: 0.23179072997908093
  batch 403 loss: 0.23176938276876588
  batch 404 loss: 0.23172658663427476
  batch 405 loss: 0.23173000742623837
  batch 406 loss: 0.23162812137691846
  batch 407 loss: 0.23165866619099562
  batch 408 loss: 0.23154731763198094
  batch 409 loss: 0.23155682896489327
  batch 410 loss: 0.23148140587457797
  batch 411 loss: 0.23141201003624576
  batch 412 loss: 0.23142865792061518
  batch 413 loss: 0.2313429992149875
  batch 414 loss: 0.23130541672741156
  batch 415 loss: 0.23121528923511506
  batch 416 loss: 0.23116891394154385
  batch 417 loss: 0.23111789946933445
  batch 418 loss: 0.2310929142616012
  batch 419 loss: 0.2310611473703157
  batch 420 loss: 0.23106851687743551
  batch 421 loss: 0.2311087611735292
  batch 422 loss: 0.2310169419688636
  batch 423 loss: 0.2309917930170154
  batch 424 loss: 0.23103336322138895
  batch 425 loss: 0.2309878413817462
  batch 426 loss: 0.23099230960920944
  batch 427 loss: 0.23104098938248477
  batch 428 loss: 0.2310975983947794
  batch 429 loss: 0.23102772993085546
  batch 430 loss: 0.2311103874860808
  batch 431 loss: 0.23109568060827365
  batch 432 loss: 0.23106360752825383
  batch 433 loss: 0.23101022880842725
  batch 434 loss: 0.23090294957984978
  batch 435 loss: 0.23088372976615512
  batch 436 loss: 0.23079693296907144
  batch 437 loss: 0.2306776545958879
  batch 438 loss: 0.23059352728874172
  batch 439 loss: 0.23049153564052322
  batch 440 loss: 0.23051456605846232
  batch 441 loss: 0.23044529307185926
  batch 442 loss: 0.23045870183010447
  batch 443 loss: 0.23046633426276608
  batch 444 loss: 0.23051284092503624
  batch 445 loss: 0.2304583886366212
  batch 446 loss: 0.23045168827066506
  batch 447 loss: 0.23050180460916003
  batch 448 loss: 0.23058530891181103
  batch 449 loss: 0.23056263309013605
  batch 450 loss: 0.23051992697848214
  batch 451 loss: 0.23050098474697103
  batch 452 loss: 0.23051517720507309
  batch 453 loss: 0.23051076245886865
  batch 454 loss: 0.23048107191460773
  batch 455 loss: 0.23047811231770357
  batch 456 loss: 0.2304429382477936
  batch 457 loss: 0.2304002829792724
  batch 458 loss: 0.23037331710736303
  batch 459 loss: 0.23037670773488503
  batch 460 loss: 0.23047224454905677
  batch 461 loss: 0.23051691789042666
  batch 462 loss: 0.23055669258941303
  batch 463 loss: 0.23056518037041115
  batch 464 loss: 0.23060592279994283
  batch 465 loss: 0.230643428333344
  batch 466 loss: 0.23058144386873736
  batch 467 loss: 0.23062147219252535
  batch 468 loss: 0.2305403833524284
  batch 469 loss: 0.2304373205597721
  batch 470 loss: 0.2304659310490527
  batch 471 loss: 0.23046640532917784
  batch 472 loss: 0.23028114254949456
LOSS train 0.23028114254949456 valid 0.28689873218536377
LOSS train 0.23028114254949456 valid 0.2804381549358368
LOSS train 0.23028114254949456 valid 0.2979792356491089
LOSS train 0.23028114254949456 valid 0.2844047583639622
LOSS train 0.23028114254949456 valid 0.28256896436214446
LOSS train 0.23028114254949456 valid 0.28612691909074783
LOSS train 0.23028114254949456 valid 0.27967014270169394
LOSS train 0.23028114254949456 valid 0.27448138780891895
LOSS train 0.23028114254949456 valid 0.27524416479799485
LOSS train 0.23028114254949456 valid 0.2735603913664818
LOSS train 0.23028114254949456 valid 0.2744649066166444
LOSS train 0.23028114254949456 valid 0.2765721343457699
LOSS train 0.23028114254949456 valid 0.27645690739154816
LOSS train 0.23028114254949456 valid 0.27391797623464037
LOSS train 0.23028114254949456 valid 0.2738845815261205
LOSS train 0.23028114254949456 valid 0.2801412558183074
LOSS train 0.23028114254949456 valid 0.2797817964764202
LOSS train 0.23028114254949456 valid 0.2798810030023257
LOSS train 0.23028114254949456 valid 0.28374568333751277
LOSS train 0.23028114254949456 valid 0.28284267857670786
LOSS train 0.23028114254949456 valid 0.2865330782674608
LOSS train 0.23028114254949456 valid 0.2859018885276534
LOSS train 0.23028114254949456 valid 0.28377698102723
LOSS train 0.23028114254949456 valid 0.284102317566673
LOSS train 0.23028114254949456 valid 0.28348941743373873
LOSS train 0.23028114254949456 valid 0.28204496720662486
LOSS train 0.23028114254949456 valid 0.28206892642709946
LOSS train 0.23028114254949456 valid 0.2821130608873708
LOSS train 0.23028114254949456 valid 0.28071566439908124
LOSS train 0.23028114254949456 valid 0.2809613550702731
LOSS train 0.23028114254949456 valid 0.28156516869221965
LOSS train 0.23028114254949456 valid 0.28206893196329474
LOSS train 0.23028114254949456 valid 0.2806792435320941
LOSS train 0.23028114254949456 valid 0.28016943835160313
LOSS train 0.23028114254949456 valid 0.28084469650472915
LOSS train 0.23028114254949456 valid 0.28162956196400857
LOSS train 0.23028114254949456 valid 0.28244120808872014
LOSS train 0.23028114254949456 valid 0.28254879658159454
LOSS train 0.23028114254949456 valid 0.28338188811754567
LOSS train 0.23028114254949456 valid 0.28316826559603214
LOSS train 0.23028114254949456 valid 0.28308827884313537
LOSS train 0.23028114254949456 valid 0.28445002330201014
LOSS train 0.23028114254949456 valid 0.2853239778169366
LOSS train 0.23028114254949456 valid 0.2851057184690779
LOSS train 0.23028114254949456 valid 0.28439037104447684
LOSS train 0.23028114254949456 valid 0.2839588584459346
LOSS train 0.23028114254949456 valid 0.28333261260326875
LOSS train 0.23028114254949456 valid 0.28511552046984434
LOSS train 0.23028114254949456 valid 0.2836623617580959
LOSS train 0.23028114254949456 valid 0.2841484099626541
LOSS train 0.23028114254949456 valid 0.2837899636988546
LOSS train 0.23028114254949456 valid 0.2833530679345131
LOSS train 0.23028114254949456 valid 0.2850518024192666
LOSS train 0.23028114254949456 valid 0.2849487750618546
LOSS train 0.23028114254949456 valid 0.28466130928559735
LOSS train 0.23028114254949456 valid 0.28442087130887167
LOSS train 0.23028114254949456 valid 0.283932963484212
LOSS train 0.23028114254949456 valid 0.28454893210838583
LOSS train 0.23028114254949456 valid 0.28438546798997005
LOSS train 0.23028114254949456 valid 0.28410401393969853
LOSS train 0.23028114254949456 valid 0.28393296482133085
LOSS train 0.23028114254949456 valid 0.28408735029159055
LOSS train 0.23028114254949456 valid 0.28360425479828366
LOSS train 0.23028114254949456 valid 0.2841062396764755
LOSS train 0.23028114254949456 valid 0.2828124332886476
LOSS train 0.23028114254949456 valid 0.2826526040832202
LOSS train 0.23028114254949456 valid 0.28310914737964743
LOSS train 0.23028114254949456 valid 0.2823156208238181
LOSS train 0.23028114254949456 valid 0.2831212642832079
LOSS train 0.23028114254949456 valid 0.28372522188084465
LOSS train 0.23028114254949456 valid 0.2844189096924285
LOSS train 0.23028114254949456 valid 0.2851527724415064
LOSS train 0.23028114254949456 valid 0.28618054867607273
LOSS train 0.23028114254949456 valid 0.2856378214987549
LOSS train 0.23028114254949456 valid 0.2852463318904241
LOSS train 0.23028114254949456 valid 0.2853466215494432
LOSS train 0.23028114254949456 valid 0.28506426745420926
LOSS train 0.23028114254949456 valid 0.28506850947936374
LOSS train 0.23028114254949456 valid 0.2849522344296492
LOSS train 0.23028114254949456 valid 0.28450160250067713
LOSS train 0.23028114254949456 valid 0.2849079098230527
LOSS train 0.23028114254949456 valid 0.2849880565957325
LOSS train 0.23028114254949456 valid 0.2852258631981999
LOSS train 0.23028114254949456 valid 0.2847343012690544
LOSS train 0.23028114254949456 valid 0.28559287120314203
LOSS train 0.23028114254949456 valid 0.28563407410022823
LOSS train 0.23028114254949456 valid 0.2852550272283883
LOSS train 0.23028114254949456 valid 0.28557090732184326
LOSS train 0.23028114254949456 valid 0.28607017705949506
LOSS train 0.23028114254949456 valid 0.2868227673901452
LOSS train 0.23028114254949456 valid 0.2866560476166861
LOSS train 0.23028114254949456 valid 0.2866461490807326
LOSS train 0.23028114254949456 valid 0.2864071326230162
LOSS train 0.23028114254949456 valid 0.2869140239472085
LOSS train 0.23028114254949456 valid 0.2871025618753935
LOSS train 0.23028114254949456 valid 0.2875074887027343
LOSS train 0.23028114254949456 valid 0.28756330154605747
LOSS train 0.23028114254949456 valid 0.2881124977554594
LOSS train 0.23028114254949456 valid 0.2881471123358216
LOSS train 0.23028114254949456 valid 0.2885870158672333
LOSS train 0.23028114254949456 valid 0.28863250914186533
LOSS train 0.23028114254949456 valid 0.28900906589685704
LOSS train 0.23028114254949456 valid 0.2887237343973326
LOSS train 0.23028114254949456 valid 0.2884083608022103
LOSS train 0.23028114254949456 valid 0.28890265907560075
LOSS train 0.23028114254949456 valid 0.28902768554552544
LOSS train 0.23028114254949456 valid 0.28859344848962587
LOSS train 0.23028114254949456 valid 0.2886375691051836
LOSS train 0.23028114254949456 valid 0.28806192749137177
LOSS train 0.23028114254949456 valid 0.2879370274868878
LOSS train 0.23028114254949456 valid 0.2879911116114608
LOSS train 0.23028114254949456 valid 0.288375727300133
LOSS train 0.23028114254949456 valid 0.288026880373997
LOSS train 0.23028114254949456 valid 0.2879991748353891
LOSS train 0.23028114254949456 valid 0.28893797941829846
LOSS train 0.23028114254949456 valid 0.2886280047482458
LOSS train 0.23028114254949456 valid 0.28945937192338145
LOSS train 0.23028114254949456 valid 0.2893267455747572
LOSS train 0.23028114254949456 valid 0.2891318417396866
LOSS train 0.23028114254949456 valid 0.2887501655767361
LOSS train 0.23028114254949456 valid 0.2885089283886035
LOSS train 0.23028114254949456 valid 0.28891681464480573
LOSS train 0.23028114254949456 valid 0.2888348602909383
LOSS train 0.23028114254949456 valid 0.28898351675560396
LOSS train 0.23028114254949456 valid 0.28882802999019624
LOSS train 0.23028114254949456 valid 0.28922189831260653
LOSS train 0.23028114254949456 valid 0.2890091349524776
LOSS train 0.23028114254949456 valid 0.2889200452482328
LOSS train 0.23028114254949456 valid 0.2886114553656689
LOSS train 0.23028114254949456 valid 0.2881988776417879
LOSS train 0.23028114254949456 valid 0.28824691588187035
LOSS train 0.23028114254949456 valid 0.28802588028889714
LOSS train 0.23028114254949456 valid 0.288010242626183
LOSS train 0.23028114254949456 valid 0.2882587174648669
LOSS train 0.23028114254949456 valid 0.28841683500342896
LOSS train 0.23028114254949456 valid 0.28835158937555905
LOSS train 0.23028114254949456 valid 0.28833435910896665
LOSS train 0.23028114254949456 valid 0.2883027644045111
LOSS train 0.23028114254949456 valid 0.2881307418612267
LOSS train 0.23028114254949456 valid 0.2880428655871323
LOSS train 0.23028114254949456 valid 0.2879876601146468
LOSS train 0.23028114254949456 valid 0.2882236990500504
LOSS train 0.23028114254949456 valid 0.28841008329308115
LOSS train 0.23028114254949456 valid 0.28854617321242887
LOSS train 0.23028114254949456 valid 0.28842163240087443
LOSS train 0.23028114254949456 valid 0.2884150771041439
LOSS train 0.23028114254949456 valid 0.2884389465155245
LOSS train 0.23028114254949456 valid 0.2882101802608451
LOSS train 0.23028114254949456 valid 0.2883011075674287
LOSS train 0.23028114254949456 valid 0.2882121554017067
LOSS train 0.23028114254949456 valid 0.2881221497887807
LOSS train 0.23028114254949456 valid 0.28819253423104163
LOSS train 0.23028114254949456 valid 0.28788050960481554
LOSS train 0.23028114254949456 valid 0.2881940776458034
LOSS train 0.23028114254949456 valid 0.2883472955034625
LOSS train 0.23028114254949456 valid 0.28833204956773
LOSS train 0.23028114254949456 valid 0.28834644406084803
LOSS train 0.23028114254949456 valid 0.28854043527117257
LOSS train 0.23028114254949456 valid 0.28869248176895596
LOSS train 0.23028114254949456 valid 0.2888359564356506
LOSS train 0.23028114254949456 valid 0.2887629384394758
LOSS train 0.23028114254949456 valid 0.2888419239057435
LOSS train 0.23028114254949456 valid 0.2886557640290699
LOSS train 0.23028114254949456 valid 0.2887700515367636
LOSS train 0.23028114254949456 valid 0.2889664246277376
LOSS train 0.23028114254949456 valid 0.2888289714254529
LOSS train 0.23028114254949456 valid 0.2888129677422746
LOSS train 0.23028114254949456 valid 0.2885737071434657
LOSS train 0.23028114254949456 valid 0.2883803937914809
LOSS train 0.23028114254949456 valid 0.2885516417377135
LOSS train 0.23028114254949456 valid 0.2885760013123005
LOSS train 0.23028114254949456 valid 0.28862050192993743
LOSS train 0.23028114254949456 valid 0.2884422332220684
LOSS train 0.23028114254949456 valid 0.2889285498651965
LOSS train 0.23028114254949456 valid 0.2888695251941681
LOSS train 0.23028114254949456 valid 0.28891448744318704
LOSS train 0.23028114254949456 valid 0.2888791759808858
LOSS train 0.23028114254949456 valid 0.2890054275480549
LOSS train 0.23028114254949456 valid 0.2891741804570459
LOSS train 0.23028114254949456 valid 0.28906917737589943
LOSS train 0.23028114254949456 valid 0.28906568934245663
LOSS train 0.23028114254949456 valid 0.28876611361136806
LOSS train 0.23028114254949456 valid 0.28899258375167847
LOSS train 0.23028114254949456 valid 0.28889666821645654
LOSS train 0.23028114254949456 valid 0.28871171893300235
LOSS train 0.23028114254949456 valid 0.2888985340313245
LOSS train 0.23028114254949456 valid 0.2887207370072125
LOSS train 0.23028114254949456 valid 0.28889286629063016
LOSS train 0.23028114254949456 valid 0.2885257985699114
LOSS train 0.23028114254949456 valid 0.28863903762478577
LOSS train 0.23028114254949456 valid 0.2888049889765485
LOSS train 0.23028114254949456 valid 0.28880384758425254
LOSS train 0.23028114254949456 valid 0.2884787550398723
LOSS train 0.23028114254949456 valid 0.2882939643927456
LOSS train 0.23028114254949456 valid 0.2882228752741447
LOSS train 0.23028114254949456 valid 0.2883464659811283
LOSS train 0.23028114254949456 valid 0.2882893125569155
LOSS train 0.23028114254949456 valid 0.2885182077685992
LOSS train 0.23028114254949456 valid 0.2885089626713614
LOSS train 0.23028114254949456 valid 0.28873818688094616
LOSS train 0.23028114254949456 valid 0.2884692039655809
LOSS train 0.23028114254949456 valid 0.2885074941533627
LOSS train 0.23028114254949456 valid 0.28860614848841587
LOSS train 0.23028114254949456 valid 0.28831984146552925
LOSS train 0.23028114254949456 valid 0.2882171187458969
LOSS train 0.23028114254949456 valid 0.28826629985304714
LOSS train 0.23028114254949456 valid 0.2881783408243299
LOSS train 0.23028114254949456 valid 0.28786393713492614
LOSS train 0.23028114254949456 valid 0.2880112401891553
LOSS train 0.23028114254949456 valid 0.2881730352129255
LOSS train 0.23028114254949456 valid 0.2884242317122871
LOSS train 0.23028114254949456 valid 0.28824413272569765
LOSS train 0.23028114254949456 valid 0.2882115604452124
LOSS train 0.23028114254949456 valid 0.2881618533457551
LOSS train 0.23028114254949456 valid 0.2879503705473833
LOSS train 0.23028114254949456 valid 0.28773836977779865
LOSS train 0.23028114254949456 valid 0.287450163663807
LOSS train 0.23028114254949456 valid 0.2874881055787069
LOSS train 0.23028114254949456 valid 0.2875295301142349
LOSS train 0.23028114254949456 valid 0.28766575719822535
LOSS train 0.23028114254949456 valid 0.2876276145288847
LOSS train 0.23028114254949456 valid 0.28781238607726656
LOSS train 0.23028114254949456 valid 0.2880538397973963
LOSS train 0.23028114254949456 valid 0.2879481014263417
LOSS train 0.23028114254949456 valid 0.2878320207198461
LOSS train 0.23028114254949456 valid 0.28767063574189633
LOSS train 0.23028114254949456 valid 0.28765002083410773
LOSS train 0.23028114254949456 valid 0.28752734244131206
LOSS train 0.23028114254949456 valid 0.2874226129601616
LOSS train 0.23028114254949456 valid 0.287424983991229
LOSS train 0.23028114254949456 valid 0.28742267023691365
LOSS train 0.23028114254949456 valid 0.2875785570077855
LOSS train 0.23028114254949456 valid 0.2879296448481441
LOSS train 0.23028114254949456 valid 0.28788549911517364
LOSS train 0.23028114254949456 valid 0.2879921647462439
LOSS train 0.23028114254949456 valid 0.28777484381097856
LOSS train 0.23028114254949456 valid 0.28786524124286345
LOSS train 0.23028114254949456 valid 0.287824117836832
LOSS train 0.23028114254949456 valid 0.2877450658935882
LOSS train 0.23028114254949456 valid 0.2872469689697027
LOSS train 0.23028114254949456 valid 0.28697648378575985
LOSS train 0.23028114254949456 valid 0.28692255353878354
LOSS train 0.23028114254949456 valid 0.28671462999449837
LOSS train 0.23028114254949456 valid 0.286740065598097
LOSS train 0.23028114254949456 valid 0.2869087391970109
LOSS train 0.23028114254949456 valid 0.28702801199463324
LOSS train 0.23028114254949456 valid 0.287110947283656
LOSS train 0.23028114254949456 valid 0.2871418698660789
LOSS train 0.23028114254949456 valid 0.28688993852540673
LOSS train 0.23028114254949456 valid 0.28705379968881606
LOSS train 0.23028114254949456 valid 0.287189730075726
LOSS train 0.23028114254949456 valid 0.2870389794309934
LOSS train 0.23028114254949456 valid 0.28674545872352813
LOSS train 0.23028114254949456 valid 0.28690287881479487
LOSS train 0.23028114254949456 valid 0.28674283980154525
LOSS train 0.23028114254949456 valid 0.28644916106713936
LOSS train 0.23028114254949456 valid 0.286431524598181
LOSS train 0.23028114254949456 valid 0.28661988739126415
LOSS train 0.23028114254949456 valid 0.2864093710902115
LOSS train 0.23028114254949456 valid 0.28619431280172786
LOSS train 0.23028114254949456 valid 0.2862485303504257
LOSS train 0.23028114254949456 valid 0.28654104209583225
LOSS train 0.23028114254949456 valid 0.2862250595831599
LOSS train 0.23028114254949456 valid 0.28634700839492405
LOSS train 0.23028114254949456 valid 0.2862225639932561
LOSS train 0.23028114254949456 valid 0.2862742619967102
LOSS train 0.23028114254949456 valid 0.28620584007729305
LOSS train 0.23028114254949456 valid 0.28634476355874716
LOSS train 0.23028114254949456 valid 0.2864625768036647
LOSS train 0.23028114254949456 valid 0.28637193468985733
LOSS train 0.23028114254949456 valid 0.2863948917586865
LOSS train 0.23028114254949456 valid 0.2861906611634528
LOSS train 0.23028114254949456 valid 0.28603446805651805
LOSS train 0.23028114254949456 valid 0.285930764011658
LOSS train 0.23028114254949456 valid 0.28602267108180307
LOSS train 0.23028114254949456 valid 0.2860825146561947
LOSS train 0.23028114254949456 valid 0.28618058180335626
LOSS train 0.23028114254949456 valid 0.28621290468697924
LOSS train 0.23028114254949456 valid 0.286094567628317
LOSS train 0.23028114254949456 valid 0.28597811135862555
LOSS train 0.23028114254949456 valid 0.2857367189541406
LOSS train 0.23028114254949456 valid 0.28568982465047366
LOSS train 0.23028114254949456 valid 0.2857802867257553
LOSS train 0.23028114254949456 valid 0.2857710580590745
LOSS train 0.23028114254949456 valid 0.28586389265562356
LOSS train 0.23028114254949456 valid 0.285801347006451
LOSS train 0.23028114254949456 valid 0.2857085918092562
LOSS train 0.23028114254949456 valid 0.2855947455391288
LOSS train 0.23028114254949456 valid 0.28552826826547667
LOSS train 0.23028114254949456 valid 0.2854415304701904
LOSS train 0.23028114254949456 valid 0.285399588625046
LOSS train 0.23028114254949456 valid 0.28537694252517126
LOSS train 0.23028114254949456 valid 0.28535612430995644
LOSS train 0.23028114254949456 valid 0.2853712791285547
LOSS train 0.23028114254949456 valid 0.2853609341686055
LOSS train 0.23028114254949456 valid 0.28528744817988294
LOSS train 0.23028114254949456 valid 0.2853098406334116
LOSS train 0.23028114254949456 valid 0.28543917104701866
LOSS train 0.23028114254949456 valid 0.2854681650930424
LOSS train 0.23028114254949456 valid 0.2854068916042646
LOSS train 0.23028114254949456 valid 0.28530562983398816
LOSS train 0.23028114254949456 valid 0.28529975410328795
LOSS train 0.23028114254949456 valid 0.2852034870744145
LOSS train 0.23028114254949456 valid 0.2852299261446062
LOSS train 0.23028114254949456 valid 0.28510893431843304
LOSS train 0.23028114254949456 valid 0.28540290380809824
LOSS train 0.23028114254949456 valid 0.2852985189578432
LOSS train 0.23028114254949456 valid 0.2853654148419956
LOSS train 0.23028114254949456 valid 0.2853395541026754
LOSS train 0.23028114254949456 valid 0.2852843918627308
LOSS train 0.23028114254949456 valid 0.28533965516895343
LOSS train 0.23028114254949456 valid 0.2852294026181484
LOSS train 0.23028114254949456 valid 0.285323583041898
LOSS train 0.23028114254949456 valid 0.2852392873851357
LOSS train 0.23028114254949456 valid 0.28518291669232504
LOSS train 0.23028114254949456 valid 0.28518382719234575
LOSS train 0.23028114254949456 valid 0.2851954682110236
LOSS train 0.23028114254949456 valid 0.2851900285426176
LOSS train 0.23028114254949456 valid 0.28520105629692255
LOSS train 0.23028114254949456 valid 0.2850736584980041
LOSS train 0.23028114254949456 valid 0.2852012706787044
LOSS train 0.23028114254949456 valid 0.2852066369808238
LOSS train 0.23028114254949456 valid 0.2850275283719733
LOSS train 0.23028114254949456 valid 0.2849508416781455
LOSS train 0.23028114254949456 valid 0.28477793991565703
LOSS train 0.23028114254949456 valid 0.2847714126567168
LOSS train 0.23028114254949456 valid 0.28470476509416504
LOSS train 0.23028114254949456 valid 0.2847202011815658
LOSS train 0.23028114254949456 valid 0.2847348865132926
LOSS train 0.23028114254949456 valid 0.2847493536996119
LOSS train 0.23028114254949456 valid 0.28468424041285617
LOSS train 0.23028114254949456 valid 0.28467144229146374
LOSS train 0.23028114254949456 valid 0.2849595659935439
LOSS train 0.23028114254949456 valid 0.284862592325596
LOSS train 0.23028114254949456 valid 0.2847684428318223
LOSS train 0.23028114254949456 valid 0.28464233378569287
LOSS train 0.23028114254949456 valid 0.2848470465719523
LOSS train 0.23028114254949456 valid 0.2848467334840425
LOSS train 0.23028114254949456 valid 0.28476842358752336
LOSS train 0.23028114254949456 valid 0.28490860839100446
LOSS train 0.23028114254949456 valid 0.2848279345070512
LOSS train 0.23028114254949456 valid 0.2847752248683171
LOSS train 0.23028114254949456 valid 0.28456554902885817
LOSS train 0.23028114254949456 valid 0.2847211431971816
LOSS train 0.23028114254949456 valid 0.28489524748014367
LOSS train 0.23028114254949456 valid 0.2848429600627436
LOSS train 0.23028114254949456 valid 0.2846854187397861
LOSS train 0.23028114254949456 valid 0.28478605522849093
LOSS train 0.23028114254949456 valid 0.28475926914665967
LOSS train 0.23028114254949456 valid 0.28478981230940137
LOSS train 0.23028114254949456 valid 0.2849223450717763
LOSS train 0.23028114254949456 valid 0.2849681595848365
LOSS train 0.23028114254949456 valid 0.2850338896003053
LOSS train 0.23028114254949456 valid 0.28501032276005395
LOSS train 0.23028114254949456 valid 0.2848392144475185
LOSS train 0.23028114254949456 valid 0.2848628378902259
LOSS train 0.23028114254949456 valid 0.28482854479000347
LOSS train 0.23028114254949456 valid 0.2847178386493102
LOSS train 0.23028114254949456 valid 0.2846550115054696
LOSS train 0.23028114254949456 valid 0.28488774676289824
LOSS train 0.23028114254949456 valid 0.2847798480303994
LOSS train 0.23028114254949456 valid 0.2847902656671751
LOSS train 0.23028114254949456 valid 0.2848691873396067
LOSS train 0.23028114254949456 valid 0.28476625992046606
LOSS train 0.23028114254949456 valid 0.28487919928276373
LOSS train 0.23028114254949456 valid 0.28483234972901683
LOSS train 0.23028114254949456 valid 0.28473294414681377
LOSS train 0.23028114254949456 valid 0.28463470275797276
LOSS train 0.23028114254949456 valid 0.28463828091214344
EPOCH 29:
  batch 1 loss: 0.2530045509338379
  batch 2 loss: 0.2724355012178421
  batch 3 loss: 0.25939497351646423
  batch 4 loss: 0.25983402878046036
  batch 5 loss: 0.25574841499328616
  batch 6 loss: 0.26291849215825397
  batch 7 loss: 0.259289630821773
  batch 8 loss: 0.2541605941951275
  batch 9 loss: 0.2549966043896145
  batch 10 loss: 0.2529710978269577
  batch 11 loss: 0.25393232161348517
  batch 12 loss: 0.25312502930561703
  batch 13 loss: 0.2501404102032001
  batch 14 loss: 0.24722875441823686
  batch 15 loss: 0.24808318614959718
  batch 16 loss: 0.24760983139276505
  batch 17 loss: 0.24790945473839254
  batch 18 loss: 0.24537711093823114
  batch 19 loss: 0.24326570959467636
  batch 20 loss: 0.24044814854860305
  batch 21 loss: 0.2428122687907446
  batch 22 loss: 0.2442888387224891
  batch 23 loss: 0.24456247428189154
  batch 24 loss: 0.24561971550186476
  batch 25 loss: 0.24661850810050964
  batch 26 loss: 0.24560982905901396
  batch 27 loss: 0.24566935941025062
  batch 28 loss: 0.2477784210017749
  batch 29 loss: 0.24834387775125175
  batch 30 loss: 0.24927930831909179
  batch 31 loss: 0.24915301174886764
  batch 32 loss: 0.2502859062515199
  batch 33 loss: 0.2528496479446238
  batch 34 loss: 0.2529642130522167
  batch 35 loss: 0.25428293134484975
  batch 36 loss: 0.25455056586199337
  batch 37 loss: 0.2542806686581792
  batch 38 loss: 0.2536644614056537
  batch 39 loss: 0.25280493383224195
  batch 40 loss: 0.2517821755260229
  batch 41 loss: 0.25113287350026575
  batch 42 loss: 0.25024798406021936
  batch 43 loss: 0.25015880514022915
  batch 44 loss: 0.24986289272254164
  batch 45 loss: 0.2490316884385215
  batch 46 loss: 0.24729955876651016
  batch 47 loss: 0.24574453589764048
  batch 48 loss: 0.24495227138201395
  batch 49 loss: 0.24491439151520633
  batch 50 loss: 0.24536707907915115
  batch 51 loss: 0.2445888358588312
  batch 52 loss: 0.24428039961136305
  batch 53 loss: 0.2435914694138293
  batch 54 loss: 0.24306839842487266
  batch 55 loss: 0.24295050447637384
  batch 56 loss: 0.24329022956745966
  batch 57 loss: 0.24241318985035545
  batch 58 loss: 0.2419968843460083
  batch 59 loss: 0.24187113167875904
  batch 60 loss: 0.2422034591436386
  batch 61 loss: 0.24266761345941512
  batch 62 loss: 0.24304738304307383
  batch 63 loss: 0.2424068342125605
  batch 64 loss: 0.24190018349327147
  batch 65 loss: 0.24125405297829555
  batch 66 loss: 0.24064276073918198
  batch 67 loss: 0.23990214093407589
  batch 68 loss: 0.23941279071218827
  batch 69 loss: 0.23951994379361471
  batch 70 loss: 0.2392051107117108
  batch 71 loss: 0.23919708565087386
  batch 72 loss: 0.23914505975941816
  batch 73 loss: 0.23867907589429047
  batch 74 loss: 0.23855456146034035
  batch 75 loss: 0.23823714554309844
  batch 76 loss: 0.2382417529037124
  batch 77 loss: 0.23826946182684464
  batch 78 loss: 0.23797729496772474
  batch 79 loss: 0.2378973132824596
  batch 80 loss: 0.2380469772964716
  batch 81 loss: 0.23812627976323353
  batch 82 loss: 0.23823722742679643
  batch 83 loss: 0.23793295654187718
  batch 84 loss: 0.23738558891983258
  batch 85 loss: 0.23764763106318082
  batch 86 loss: 0.23748322645592135
  batch 87 loss: 0.2369984890195145
  batch 88 loss: 0.23654853450981053
  batch 89 loss: 0.23588732303528304
  batch 90 loss: 0.23630536049604417
  batch 91 loss: 0.23625862451045068
  batch 92 loss: 0.23612444637262303
  batch 93 loss: 0.23546047829171662
  batch 94 loss: 0.23517556298286357
  batch 95 loss: 0.2349075769123278
  batch 96 loss: 0.23471861612051725
  batch 97 loss: 0.23522507898586312
  batch 98 loss: 0.2349629228820606
  batch 99 loss: 0.23476156201025453
  batch 100 loss: 0.23515083134174347
  batch 101 loss: 0.2355033829070554
  batch 102 loss: 0.23564396433386148
  batch 103 loss: 0.23532105331282013
  batch 104 loss: 0.23562873985904914
  batch 105 loss: 0.23524093131224313
  batch 106 loss: 0.23507478299005977
  batch 107 loss: 0.23506623993967182
  batch 108 loss: 0.2345980927348137
  batch 109 loss: 0.23469764568390103
  batch 110 loss: 0.23489166471091183
  batch 111 loss: 0.23519374443604066
  batch 112 loss: 0.23499706826571906
  batch 113 loss: 0.23475707513568675
  batch 114 loss: 0.23476318086971315
  batch 115 loss: 0.23439599884592968
  batch 116 loss: 0.2342889797070931
  batch 117 loss: 0.23422513749354926
  batch 118 loss: 0.2343923508868379
  batch 119 loss: 0.2345170567766959
  batch 120 loss: 0.23435005508363246
  batch 121 loss: 0.23421074868726335
  batch 122 loss: 0.23396306814717466
  batch 123 loss: 0.23343674196460382
  batch 124 loss: 0.23323987316220038
  batch 125 loss: 0.23285537588596344
  batch 126 loss: 0.2329507050296617
  batch 127 loss: 0.23311812729816736
  batch 128 loss: 0.23306841764133424
  batch 129 loss: 0.2334146905083989
  batch 130 loss: 0.2334472533601981
  batch 131 loss: 0.23346364577762954
  batch 132 loss: 0.23371227887092214
  batch 133 loss: 0.2336799188664085
  batch 134 loss: 0.23353035670163028
  batch 135 loss: 0.23340517602584981
  batch 136 loss: 0.23364353651071296
  batch 137 loss: 0.2336498728416262
  batch 138 loss: 0.23365253803954608
  batch 139 loss: 0.23337240216972158
  batch 140 loss: 0.23371074848941395
  batch 141 loss: 0.23328092683714333
  batch 142 loss: 0.23299484574039217
  batch 143 loss: 0.23301629701277593
  batch 144 loss: 0.23290170449763536
  batch 145 loss: 0.232880670551596
  batch 146 loss: 0.23283480730367034
  batch 147 loss: 0.2328446275320183
  batch 148 loss: 0.23281078846067996
  batch 149 loss: 0.23245172872639341
  batch 150 loss: 0.23241710702578228
  batch 151 loss: 0.23199045786399716
  batch 152 loss: 0.2319181639897196
  batch 153 loss: 0.2319413021304249
  batch 154 loss: 0.23169540662270088
  batch 155 loss: 0.2315053302434183
  batch 156 loss: 0.2315139484902223
  batch 157 loss: 0.23147548564300416
  batch 158 loss: 0.23154551384946967
  batch 159 loss: 0.2314514436249463
  batch 160 loss: 0.2313702023588121
  batch 161 loss: 0.23117643548464922
  batch 162 loss: 0.23127733161787928
  batch 163 loss: 0.2311214606271931
  batch 164 loss: 0.2311815008884523
  batch 165 loss: 0.23145405794634963
  batch 166 loss: 0.23182407093335347
  batch 167 loss: 0.23202822022809239
  batch 168 loss: 0.2317402596097617
  batch 169 loss: 0.2317483528302266
  batch 170 loss: 0.23173981089802348
  batch 171 loss: 0.23183390794441713
  batch 172 loss: 0.23170342478294706
  batch 173 loss: 0.2317137202258744
  batch 174 loss: 0.23175246559682933
  batch 175 loss: 0.23176184373242514
  batch 176 loss: 0.23212382205846635
  batch 177 loss: 0.23260972737255742
  batch 178 loss: 0.23235072806644974
  batch 179 loss: 0.23239042572469018
  batch 180 loss: 0.232398555179437
  batch 181 loss: 0.2323664643323224
  batch 182 loss: 0.232297396839975
  batch 183 loss: 0.23227518642209266
  batch 184 loss: 0.2323014098826958
  batch 185 loss: 0.23203079660196563
  batch 186 loss: 0.2320918891859311
  batch 187 loss: 0.23223977141520558
  batch 188 loss: 0.23217593807172268
  batch 189 loss: 0.2321699055887404
  batch 190 loss: 0.23198048158695822
  batch 191 loss: 0.23202207661111943
  batch 192 loss: 0.23179296877545616
  batch 193 loss: 0.23175754309318225
  batch 194 loss: 0.23162650339996693
  batch 195 loss: 0.23176321784655254
  batch 196 loss: 0.2316143388802908
  batch 197 loss: 0.23176669401263222
  batch 198 loss: 0.23177563060413708
  batch 199 loss: 0.2318485386707076
  batch 200 loss: 0.23184852451086044
  batch 201 loss: 0.23171895853619076
  batch 202 loss: 0.23190689610667747
  batch 203 loss: 0.23200889005155986
  batch 204 loss: 0.23217154593736516
  batch 205 loss: 0.23228058226224851
  batch 206 loss: 0.23238721670745646
  batch 207 loss: 0.23242932176532377
  batch 208 loss: 0.2322257562325551
  batch 209 loss: 0.23236195450764524
  batch 210 loss: 0.23248339891433717
  batch 211 loss: 0.23239485509870175
  batch 212 loss: 0.23245134141366436
  batch 213 loss: 0.2324156081872367
  batch 214 loss: 0.23229100903339475
  batch 215 loss: 0.23226347156735352
  batch 216 loss: 0.2322934122963084
  batch 217 loss: 0.2323358594547219
  batch 218 loss: 0.2321622947897386
  batch 219 loss: 0.23192886096429607
  batch 220 loss: 0.2320374188775366
  batch 221 loss: 0.23186992378526144
  batch 222 loss: 0.23193440250716768
  batch 223 loss: 0.23193163835681607
  batch 224 loss: 0.23190421245193907
  batch 225 loss: 0.23185072229968176
  batch 226 loss: 0.23186222556154285
  batch 227 loss: 0.23183037012421612
  batch 228 loss: 0.23186270480877474
  batch 229 loss: 0.23182995220459185
  batch 230 loss: 0.2318750327695971
  batch 231 loss: 0.23188463337235637
  batch 232 loss: 0.23182968088778957
  batch 233 loss: 0.23180000145036264
  batch 234 loss: 0.23181568219875678
  batch 235 loss: 0.2316720188298124
  batch 236 loss: 0.23166230554550382
  batch 237 loss: 0.2314626322391164
  batch 238 loss: 0.23137534497415319
  batch 239 loss: 0.23129446517722876
  batch 240 loss: 0.2312309306735794
  batch 241 loss: 0.23108176491319887
  batch 242 loss: 0.23097898133776404
  batch 243 loss: 0.2309007352149045
  batch 244 loss: 0.2307352038314108
  batch 245 loss: 0.23057381839168314
  batch 246 loss: 0.2304477713456968
  batch 247 loss: 0.23048057253302834
  batch 248 loss: 0.23055164065332182
  batch 249 loss: 0.23066579439793247
  batch 250 loss: 0.23052838724851607
  batch 251 loss: 0.23038656433739985
  batch 252 loss: 0.2302805334329605
  batch 253 loss: 0.23033361578647327
  batch 254 loss: 0.23042043304349494
  batch 255 loss: 0.23039481856074986
  batch 256 loss: 0.23034303140593693
  batch 257 loss: 0.2305145830147925
  batch 258 loss: 0.23059234121280123
  batch 259 loss: 0.23066917550839972
  batch 260 loss: 0.230818214152868
  batch 261 loss: 0.2308649682793124
  batch 262 loss: 0.23077124180912062
  batch 263 loss: 0.23073284180671996
  batch 264 loss: 0.23086190014851815
  batch 265 loss: 0.23085225224494935
  batch 266 loss: 0.23071870588718502
  batch 267 loss: 0.23047420237394756
  batch 268 loss: 0.23040655405441327
  batch 269 loss: 0.23049095201004838
  batch 270 loss: 0.2303748309612274
  batch 271 loss: 0.23033253469150444
  batch 272 loss: 0.2302161174661973
  batch 273 loss: 0.23013013557636694
  batch 274 loss: 0.2300071349217944
  batch 275 loss: 0.22990062952041626
  batch 276 loss: 0.22978273071888564
  batch 277 loss: 0.22969947050624806
  batch 278 loss: 0.22958284954158523
  batch 279 loss: 0.2295938212170823
  batch 280 loss: 0.22956391519733838
  batch 281 loss: 0.2295155576115401
  batch 282 loss: 0.22930925830881646
  batch 283 loss: 0.2292205402265589
  batch 284 loss: 0.22918883228385953
  batch 285 loss: 0.22917098951967138
  batch 286 loss: 0.22914851529823316
  batch 287 loss: 0.2290873927314107
  batch 288 loss: 0.2290664523219069
  batch 289 loss: 0.22892621334861307
  batch 290 loss: 0.22880933007289622
  batch 291 loss: 0.22882853581528484
  batch 292 loss: 0.22884725468003586
  batch 293 loss: 0.22886930106036085
  batch 294 loss: 0.22864264199117415
  batch 295 loss: 0.2286481536545996
  batch 296 loss: 0.22870538076637564
  batch 297 loss: 0.2286052254313973
  batch 298 loss: 0.22852795951118404
  batch 299 loss: 0.22861138668945402
  batch 300 loss: 0.22852146113912264
  batch 301 loss: 0.2285198146520659
  batch 302 loss: 0.22864615522473064
  batch 303 loss: 0.2285764519334233
  batch 304 loss: 0.22850889080253087
  batch 305 loss: 0.22848069384449818
  batch 306 loss: 0.2284912743599586
  batch 307 loss: 0.22843240206327034
  batch 308 loss: 0.22847145636166846
  batch 309 loss: 0.228424475754349
  batch 310 loss: 0.2284809271654775
  batch 311 loss: 0.22835512373033443
  batch 312 loss: 0.2283502620859788
  batch 313 loss: 0.22831487479491736
  batch 314 loss: 0.22832345981506785
  batch 315 loss: 0.22822352766044557
  batch 316 loss: 0.22819344939876207
  batch 317 loss: 0.22819683759558465
  batch 318 loss: 0.22800574039325774
  batch 319 loss: 0.22781369108765104
  batch 320 loss: 0.22778474483639002
  batch 321 loss: 0.22784017170329704
  batch 322 loss: 0.2277574594254079
  batch 323 loss: 0.22763109474728352
  batch 324 loss: 0.22760490677606912
  batch 325 loss: 0.2276951908148252
  batch 326 loss: 0.22784864829361803
  batch 327 loss: 0.22792448882662922
  batch 328 loss: 0.22800221594005096
  batch 329 loss: 0.22794299017875752
  batch 330 loss: 0.22794533462235422
  batch 331 loss: 0.2278555481725589
  batch 332 loss: 0.22796792317047176
  batch 333 loss: 0.22798529692419298
  batch 334 loss: 0.22833054378896417
  batch 335 loss: 0.2284714215282184
  batch 336 loss: 0.22857552691407146
  batch 337 loss: 0.22873228377156865
  batch 338 loss: 0.22882326701160013
  batch 339 loss: 0.22887456452248722
  batch 340 loss: 0.22888653655262553
  batch 341 loss: 0.2288435091895442
  batch 342 loss: 0.22876626710619843
  batch 343 loss: 0.22884177818416507
  batch 344 loss: 0.22886657190704068
  batch 345 loss: 0.22890831698542055
  batch 346 loss: 0.2289596507917939
  batch 347 loss: 0.2288431396831353
  batch 348 loss: 0.22881382151410498
  batch 349 loss: 0.2288904880469713
  batch 350 loss: 0.22906428315809796
  batch 351 loss: 0.22906375411059443
  batch 352 loss: 0.2290366938845678
  batch 353 loss: 0.22901906828704705
  batch 354 loss: 0.22893585597222807
  batch 355 loss: 0.22883517767342043
  batch 356 loss: 0.22898300921314219
  batch 357 loss: 0.22886678563947438
  batch 358 loss: 0.22876411564023802
  batch 359 loss: 0.22871164247020043
  batch 360 loss: 0.2286084601448642
  batch 361 loss: 0.22856407701308706
  batch 362 loss: 0.2286443468697822
  batch 363 loss: 0.22865184168677685
  batch 364 loss: 0.22849623666523577
  batch 365 loss: 0.22861878190138568
  batch 366 loss: 0.2286753727357244
  batch 367 loss: 0.22886862347983533
  batch 368 loss: 0.22886642590974984
  batch 369 loss: 0.22886045273080427
  batch 370 loss: 0.22896286063903087
  batch 371 loss: 0.22913496473407488
  batch 372 loss: 0.22929516939386244
  batch 373 loss: 0.22936318970557512
  batch 374 loss: 0.22924221737340172
  batch 375 loss: 0.22928734334309897
  batch 376 loss: 0.22946863241018134
  batch 377 loss: 0.22949218117589976
  batch 378 loss: 0.22956325917962997
  batch 379 loss: 0.22958048549050705
  batch 380 loss: 0.22959478572010994
  batch 381 loss: 0.22953873571604883
  batch 382 loss: 0.22953183792960583
  batch 383 loss: 0.2294892064507263
  batch 384 loss: 0.2295801684182758
  batch 385 loss: 0.22955514738312016
  batch 386 loss: 0.22956487872285547
  batch 387 loss: 0.22950830127687724
  batch 388 loss: 0.2294499850687907
  batch 389 loss: 0.22945312010291916
  batch 390 loss: 0.22942767769862443
  batch 391 loss: 0.2293293015731265
  batch 392 loss: 0.22930930106311428
  batch 393 loss: 0.22949013159475254
  batch 394 loss: 0.22956063095385654
  batch 395 loss: 0.22962249072292182
  batch 396 loss: 0.22964160042730244
  batch 397 loss: 0.2297843706322557
  batch 398 loss: 0.2298694192689268
  batch 399 loss: 0.2298572015642821
  batch 400 loss: 0.22994465075433254
  batch 401 loss: 0.2300205046399276
  batch 402 loss: 0.2300845945800715
  batch 403 loss: 0.23013260894022863
  batch 404 loss: 0.23013805280817617
  batch 405 loss: 0.2301706149990176
  batch 406 loss: 0.23008388211134032
  batch 407 loss: 0.23013256680613947
  batch 408 loss: 0.23009988099482714
  batch 409 loss: 0.23009787292235637
  batch 410 loss: 0.2300232180008074
  batch 411 loss: 0.22996465829167054
  batch 412 loss: 0.22999023262736867
  batch 413 loss: 0.22986487650986734
  batch 414 loss: 0.22982344150111295
  batch 415 loss: 0.2297541131456214
  batch 416 loss: 0.229728843157108
  batch 417 loss: 0.22967781694672948
  batch 418 loss: 0.22963639516294287
  batch 419 loss: 0.22958441533877344
  batch 420 loss: 0.22960409511412894
  batch 421 loss: 0.22965774708150968
  batch 422 loss: 0.2295572134485177
  batch 423 loss: 0.22952126121549177
  batch 424 loss: 0.2295410755740584
  batch 425 loss: 0.22949336434111875
  batch 426 loss: 0.22946723249736525
  batch 427 loss: 0.22949300420228436
  batch 428 loss: 0.22952456361501017
  batch 429 loss: 0.22946267069636525
  batch 430 loss: 0.22952534757381263
  batch 431 loss: 0.22951590143610873
  batch 432 loss: 0.22947109256077697
  batch 433 loss: 0.22941113137758357
  batch 434 loss: 0.22930192329367186
  batch 435 loss: 0.22929064374545524
  batch 436 loss: 0.2292456057161913
  batch 437 loss: 0.22911120705664567
  batch 438 loss: 0.22906163862170695
  batch 439 loss: 0.22898821845005748
  batch 440 loss: 0.2290398020297289
  batch 441 loss: 0.228994142475312
  batch 442 loss: 0.22899847470931878
  batch 443 loss: 0.22900519585636345
  batch 444 loss: 0.22907851398125426
  batch 445 loss: 0.22899332525355093
  batch 446 loss: 0.22895848179745568
  batch 447 loss: 0.22897849183621288
  batch 448 loss: 0.2290593291566308
  batch 449 loss: 0.22904437392087185
  batch 450 loss: 0.22899971395730973
  batch 451 loss: 0.22896046642453602
  batch 452 loss: 0.22890676775601057
  batch 453 loss: 0.22891861568749872
  batch 454 loss: 0.22889618493518116
  batch 455 loss: 0.22889766909263945
  batch 456 loss: 0.22885746031738163
  batch 457 loss: 0.22884565816796842
  batch 458 loss: 0.22886272689010378
  batch 459 loss: 0.22887580869374452
  batch 460 loss: 0.22889358151866043
  batch 461 loss: 0.22891204476097918
  batch 462 loss: 0.22899143984823517
  batch 463 loss: 0.22898485387633222
  batch 464 loss: 0.22895829713550106
  batch 465 loss: 0.22899442049123908
  batch 466 loss: 0.22897218778537579
  batch 467 loss: 0.22903722183224476
  batch 468 loss: 0.22892494884948444
  batch 469 loss: 0.2288081898554556
  batch 470 loss: 0.2287709467905633
  batch 471 loss: 0.22876118091894057
  batch 472 loss: 0.2286077032212989
LOSS train 0.2286077032212989 valid 0.22780486941337585
LOSS train 0.2286077032212989 valid 0.21974574774503708
LOSS train 0.2286077032212989 valid 0.2338032772143682
LOSS train 0.2286077032212989 valid 0.21948929876089096
LOSS train 0.2286077032212989 valid 0.21646469533443452
LOSS train 0.2286077032212989 valid 0.21868283301591873
LOSS train 0.2286077032212989 valid 0.21211181793894088
LOSS train 0.2286077032212989 valid 0.20640098676085472
LOSS train 0.2286077032212989 valid 0.20660841796133253
LOSS train 0.2286077032212989 valid 0.20527533888816835
LOSS train 0.2286077032212989 valid 0.2063310444355011
LOSS train 0.2286077032212989 valid 0.2084168940782547
LOSS train 0.2286077032212989 valid 0.20772485778881952
LOSS train 0.2286077032212989 valid 0.20506392951522553
LOSS train 0.2286077032212989 valid 0.2056495577096939
LOSS train 0.2286077032212989 valid 0.21090512815862894
LOSS train 0.2286077032212989 valid 0.21002060788519242
LOSS train 0.2286077032212989 valid 0.20968475772274864
LOSS train 0.2286077032212989 valid 0.21296247996782003
LOSS train 0.2286077032212989 valid 0.2121783271431923
LOSS train 0.2286077032212989 valid 0.21543544814700172
LOSS train 0.2286077032212989 valid 0.21564248949289322
LOSS train 0.2286077032212989 valid 0.21367933439171832
LOSS train 0.2286077032212989 valid 0.2139902034153541
LOSS train 0.2286077032212989 valid 0.21365564286708832
LOSS train 0.2286077032212989 valid 0.2123138142319826
LOSS train 0.2286077032212989 valid 0.21213539055100195
LOSS train 0.2286077032212989 valid 0.21238170830266817
LOSS train 0.2286077032212989 valid 0.21114023903320575
LOSS train 0.2286077032212989 valid 0.21110589653253556
LOSS train 0.2286077032212989 valid 0.21177751691110672
LOSS train 0.2286077032212989 valid 0.21224321937188506
LOSS train 0.2286077032212989 valid 0.21099274131384763
LOSS train 0.2286077032212989 valid 0.21047403005992665
LOSS train 0.2286077032212989 valid 0.21084727900368827
LOSS train 0.2286077032212989 valid 0.21152079353729883
LOSS train 0.2286077032212989 valid 0.21232787378736445
LOSS train 0.2286077032212989 valid 0.2125536270047489
LOSS train 0.2286077032212989 valid 0.21341956655184427
LOSS train 0.2286077032212989 valid 0.21355560943484306
LOSS train 0.2286077032212989 valid 0.21383012867555384
LOSS train 0.2286077032212989 valid 0.21496746085938953
LOSS train 0.2286077032212989 valid 0.21543026420959208
LOSS train 0.2286077032212989 valid 0.21549235860055144
LOSS train 0.2286077032212989 valid 0.21502328879303403
LOSS train 0.2286077032212989 valid 0.2144609113102374
LOSS train 0.2286077032212989 valid 0.21400541796329173
LOSS train 0.2286077032212989 valid 0.21585445509602627
LOSS train 0.2286077032212989 valid 0.21488056833646735
LOSS train 0.2286077032212989 valid 0.21546681255102157
LOSS train 0.2286077032212989 valid 0.21509206791718802
LOSS train 0.2286077032212989 valid 0.21483209127416977
LOSS train 0.2286077032212989 valid 0.21638487954184693
LOSS train 0.2286077032212989 valid 0.21614377255792971
LOSS train 0.2286077032212989 valid 0.21600604382428257
LOSS train 0.2286077032212989 valid 0.21595149620303086
LOSS train 0.2286077032212989 valid 0.2152639305905292
LOSS train 0.2286077032212989 valid 0.2155204294570561
LOSS train 0.2286077032212989 valid 0.21528989606994695
LOSS train 0.2286077032212989 valid 0.2149843230843544
LOSS train 0.2286077032212989 valid 0.21492865657220123
LOSS train 0.2286077032212989 valid 0.21489026613773837
LOSS train 0.2286077032212989 valid 0.21443913356652336
LOSS train 0.2286077032212989 valid 0.21478442009538412
LOSS train 0.2286077032212989 valid 0.21375717222690582
LOSS train 0.2286077032212989 valid 0.21343260100393585
LOSS train 0.2286077032212989 valid 0.21392377944134955
LOSS train 0.2286077032212989 valid 0.21324809487251675
LOSS train 0.2286077032212989 valid 0.21414487798144852
LOSS train 0.2286077032212989 valid 0.21461208441427776
LOSS train 0.2286077032212989 valid 0.21514569097001787
LOSS train 0.2286077032212989 valid 0.215719700894422
LOSS train 0.2286077032212989 valid 0.21664910957421343
LOSS train 0.2286077032212989 valid 0.21614290189904137
LOSS train 0.2286077032212989 valid 0.21574874798456828
LOSS train 0.2286077032212989 valid 0.2157582189691694
LOSS train 0.2286077032212989 valid 0.21549142993889847
LOSS train 0.2286077032212989 valid 0.21563467784569815
LOSS train 0.2286077032212989 valid 0.21539169851737686
LOSS train 0.2286077032212989 valid 0.21494607906788588
LOSS train 0.2286077032212989 valid 0.21532579355033826
LOSS train 0.2286077032212989 valid 0.21533769605363287
LOSS train 0.2286077032212989 valid 0.21551426688590683
LOSS train 0.2286077032212989 valid 0.2150729795296987
LOSS train 0.2286077032212989 valid 0.21575852702645695
LOSS train 0.2286077032212989 valid 0.21588005853253742
LOSS train 0.2286077032212989 valid 0.21559311141227855
LOSS train 0.2286077032212989 valid 0.2157622022046284
LOSS train 0.2286077032212989 valid 0.21625115613589127
LOSS train 0.2286077032212989 valid 0.21697499106327692
LOSS train 0.2286077032212989 valid 0.21695273705236204
LOSS train 0.2286077032212989 valid 0.2168419421043085
LOSS train 0.2286077032212989 valid 0.21662566274084072
LOSS train 0.2286077032212989 valid 0.21702990427296212
LOSS train 0.2286077032212989 valid 0.21717941635533383
LOSS train 0.2286077032212989 valid 0.21752068571125469
LOSS train 0.2286077032212989 valid 0.21755599115312713
LOSS train 0.2286077032212989 valid 0.21801295116239663
LOSS train 0.2286077032212989 valid 0.21803634004159408
LOSS train 0.2286077032212989 valid 0.218348481208086
LOSS train 0.2286077032212989 valid 0.21845135107488917
LOSS train 0.2286077032212989 valid 0.21875921417685115
LOSS train 0.2286077032212989 valid 0.21858156682218163
LOSS train 0.2286077032212989 valid 0.21830965822132734
LOSS train 0.2286077032212989 valid 0.21871903638044993
LOSS train 0.2286077032212989 valid 0.2187715860189132
LOSS train 0.2286077032212989 valid 0.2184191854479157
LOSS train 0.2286077032212989 valid 0.21842292613453335
LOSS train 0.2286077032212989 valid 0.21798281781717177
LOSS train 0.2286077032212989 valid 0.2178732373497703
LOSS train 0.2286077032212989 valid 0.21787106453835428
LOSS train 0.2286077032212989 valid 0.2181431326482977
LOSS train 0.2286077032212989 valid 0.2177998430169789
LOSS train 0.2286077032212989 valid 0.21777601893010892
LOSS train 0.2286077032212989 valid 0.21865167060624
LOSS train 0.2286077032212989 valid 0.21833612011938258
LOSS train 0.2286077032212989 valid 0.21908529077330205
LOSS train 0.2286077032212989 valid 0.2189788589790716
LOSS train 0.2286077032212989 valid 0.21875443441026352
LOSS train 0.2286077032212989 valid 0.21832680366933346
LOSS train 0.2286077032212989 valid 0.21818879012726555
LOSS train 0.2286077032212989 valid 0.21847797063041907
LOSS train 0.2286077032212989 valid 0.21835828675487176
LOSS train 0.2286077032212989 valid 0.2184764823365596
LOSS train 0.2286077032212989 valid 0.2183432502746582
LOSS train 0.2286077032212989 valid 0.21867331200175816
LOSS train 0.2286077032212989 valid 0.21844118012218025
LOSS train 0.2286077032212989 valid 0.21842164092231542
LOSS train 0.2286077032212989 valid 0.21809070147285164
LOSS train 0.2286077032212989 valid 0.2177184181717726
LOSS train 0.2286077032212989 valid 0.2177517530117326
LOSS train 0.2286077032212989 valid 0.21759641351121844
LOSS train 0.2286077032212989 valid 0.2176636637825715
LOSS train 0.2286077032212989 valid 0.21790844813656451
LOSS train 0.2286077032212989 valid 0.21803208766160187
LOSS train 0.2286077032212989 valid 0.21804312013966196
LOSS train 0.2286077032212989 valid 0.21808183062685668
LOSS train 0.2286077032212989 valid 0.21802255036174387
LOSS train 0.2286077032212989 valid 0.21786400525689983
LOSS train 0.2286077032212989 valid 0.2177532016166619
LOSS train 0.2286077032212989 valid 0.2176984597182443
LOSS train 0.2286077032212989 valid 0.21796712300307314
LOSS train 0.2286077032212989 valid 0.21810464727711845
LOSS train 0.2286077032212989 valid 0.2182005299255252
LOSS train 0.2286077032212989 valid 0.21809068589374936
LOSS train 0.2286077032212989 valid 0.21805588656092342
LOSS train 0.2286077032212989 valid 0.21806163051906896
LOSS train 0.2286077032212989 valid 0.2179356948346705
LOSS train 0.2286077032212989 valid 0.21806610390643946
LOSS train 0.2286077032212989 valid 0.2179967737197876
LOSS train 0.2286077032212989 valid 0.21782126963533313
LOSS train 0.2286077032212989 valid 0.21784189578733945
LOSS train 0.2286077032212989 valid 0.21762806420622308
LOSS train 0.2286077032212989 valid 0.21795356476848776
LOSS train 0.2286077032212989 valid 0.21806610543881694
LOSS train 0.2286077032212989 valid 0.2180353173842797
LOSS train 0.2286077032212989 valid 0.21808142428565178
LOSS train 0.2286077032212989 valid 0.2182927062805695
LOSS train 0.2286077032212989 valid 0.2184747116370771
LOSS train 0.2286077032212989 valid 0.21865585604682564
LOSS train 0.2286077032212989 valid 0.21854877092453265
LOSS train 0.2286077032212989 valid 0.21854226990246478
LOSS train 0.2286077032212989 valid 0.21837698937925093
LOSS train 0.2286077032212989 valid 0.21851296686544652
LOSS train 0.2286077032212989 valid 0.21869080247301043
LOSS train 0.2286077032212989 valid 0.21857603127697864
LOSS train 0.2286077032212989 valid 0.21860332501505664
LOSS train 0.2286077032212989 valid 0.21840683867534003
LOSS train 0.2286077032212989 valid 0.21824450958409958
LOSS train 0.2286077032212989 valid 0.21837501490817351
LOSS train 0.2286077032212989 valid 0.21838529268551987
LOSS train 0.2286077032212989 valid 0.21844980623139892
LOSS train 0.2286077032212989 valid 0.21822128948793246
LOSS train 0.2286077032212989 valid 0.21858412241456152
LOSS train 0.2286077032212989 valid 0.21851018403257644
LOSS train 0.2286077032212989 valid 0.21851953864097595
LOSS train 0.2286077032212989 valid 0.21849726089986704
LOSS train 0.2286077032212989 valid 0.21856860716021462
LOSS train 0.2286077032212989 valid 0.2187461413484712
LOSS train 0.2286077032212989 valid 0.2186488409837087
LOSS train 0.2286077032212989 valid 0.2186523956653163
LOSS train 0.2286077032212989 valid 0.21840867334669764
LOSS train 0.2286077032212989 valid 0.2185418107470528
LOSS train 0.2286077032212989 valid 0.21839153377906137
LOSS train 0.2286077032212989 valid 0.21825380768324878
LOSS train 0.2286077032212989 valid 0.2183980114998356
LOSS train 0.2286077032212989 valid 0.21826455770010617
LOSS train 0.2286077032212989 valid 0.21844083998114505
LOSS train 0.2286077032212989 valid 0.21815521999326332
LOSS train 0.2286077032212989 valid 0.21824259624669426
LOSS train 0.2286077032212989 valid 0.21838678536614822
LOSS train 0.2286077032212989 valid 0.21826731824936965
LOSS train 0.2286077032212989 valid 0.2179394731095418
LOSS train 0.2286077032212989 valid 0.2177645200767468
LOSS train 0.2286077032212989 valid 0.2177247642706602
LOSS train 0.2286077032212989 valid 0.21777016654306527
LOSS train 0.2286077032212989 valid 0.2177299369258929
LOSS train 0.2286077032212989 valid 0.21793597146417154
LOSS train 0.2286077032212989 valid 0.21791321720609713
LOSS train 0.2286077032212989 valid 0.21807098113000392
LOSS train 0.2286077032212989 valid 0.21785487649749166
LOSS train 0.2286077032212989 valid 0.21784696757498354
LOSS train 0.2286077032212989 valid 0.21791944216037618
LOSS train 0.2286077032212989 valid 0.2176596953296194
LOSS train 0.2286077032212989 valid 0.21760651534650383
LOSS train 0.2286077032212989 valid 0.21764231567243927
LOSS train 0.2286077032212989 valid 0.21764132841197764
LOSS train 0.2286077032212989 valid 0.21741288262777603
LOSS train 0.2286077032212989 valid 0.2175049916551444
LOSS train 0.2286077032212989 valid 0.2176460183092526
LOSS train 0.2286077032212989 valid 0.21789227426052094
LOSS train 0.2286077032212989 valid 0.217703261535685
LOSS train 0.2286077032212989 valid 0.2176816443882078
LOSS train 0.2286077032212989 valid 0.2176467343886322
LOSS train 0.2286077032212989 valid 0.21742713790993357
LOSS train 0.2286077032212989 valid 0.2172409075277823
LOSS train 0.2286077032212989 valid 0.21698557671314012
LOSS train 0.2286077032212989 valid 0.2169922044791213
LOSS train 0.2286077032212989 valid 0.21702772181600197
LOSS train 0.2286077032212989 valid 0.21710814196955075
LOSS train 0.2286077032212989 valid 0.21711313724517822
LOSS train 0.2286077032212989 valid 0.21724733730425705
LOSS train 0.2286077032212989 valid 0.21745824058761512
LOSS train 0.2286077032212989 valid 0.21732140957776988
LOSS train 0.2286077032212989 valid 0.21719018161296844
LOSS train 0.2286077032212989 valid 0.21708319085625422
LOSS train 0.2286077032212989 valid 0.21706915341547406
LOSS train 0.2286077032212989 valid 0.2169764501353105
LOSS train 0.2286077032212989 valid 0.21694190988894632
LOSS train 0.2286077032212989 valid 0.21693813885035723
LOSS train 0.2286077032212989 valid 0.21695618989405693
LOSS train 0.2286077032212989 valid 0.21708077070271148
LOSS train 0.2286077032212989 valid 0.21739325522097394
LOSS train 0.2286077032212989 valid 0.21733961903896087
LOSS train 0.2286077032212989 valid 0.21740689835649857
LOSS train 0.2286077032212989 valid 0.21727373223688642
LOSS train 0.2286077032212989 valid 0.21737676978362763
LOSS train 0.2286077032212989 valid 0.21734251031855575
LOSS train 0.2286077032212989 valid 0.21727604160248984
LOSS train 0.2286077032212989 valid 0.21686519018063943
LOSS train 0.2286077032212989 valid 0.2167255404208211
LOSS train 0.2286077032212989 valid 0.21666810667711842
LOSS train 0.2286077032212989 valid 0.2164748975156266
LOSS train 0.2286077032212989 valid 0.21651081172902076
LOSS train 0.2286077032212989 valid 0.21660548466808943
LOSS train 0.2286077032212989 valid 0.2167260597149531
LOSS train 0.2286077032212989 valid 0.21680791362335808
LOSS train 0.2286077032212989 valid 0.2167978749400185
LOSS train 0.2286077032212989 valid 0.2165788302579558
LOSS train 0.2286077032212989 valid 0.2166650078892708
LOSS train 0.2286077032212989 valid 0.2167901480459122
LOSS train 0.2286077032212989 valid 0.21663535368584452
LOSS train 0.2286077032212989 valid 0.2163820327977418
LOSS train 0.2286077032212989 valid 0.21647356409491517
LOSS train 0.2286077032212989 valid 0.21633132117636064
LOSS train 0.2286077032212989 valid 0.21611488808412105
LOSS train 0.2286077032212989 valid 0.2161078109815427
LOSS train 0.2286077032212989 valid 0.2162354479348937
LOSS train 0.2286077032212989 valid 0.2160921889037239
LOSS train 0.2286077032212989 valid 0.21591815610344595
LOSS train 0.2286077032212989 valid 0.21600519017926578
LOSS train 0.2286077032212989 valid 0.21621420359566013
LOSS train 0.2286077032212989 valid 0.2159697845181585
LOSS train 0.2286077032212989 valid 0.21603015951360716
LOSS train 0.2286077032212989 valid 0.21592762048514383
LOSS train 0.2286077032212989 valid 0.21602566271348106
LOSS train 0.2286077032212989 valid 0.21592837443735716
LOSS train 0.2286077032212989 valid 0.21605413651733257
LOSS train 0.2286077032212989 valid 0.2161824324317138
LOSS train 0.2286077032212989 valid 0.2160723861168932
LOSS train 0.2286077032212989 valid 0.21610428090465025
LOSS train 0.2286077032212989 valid 0.2159474086125984
LOSS train 0.2286077032212989 valid 0.2157883467800888
LOSS train 0.2286077032212989 valid 0.21567108536506222
LOSS train 0.2286077032212989 valid 0.21577097871086814
LOSS train 0.2286077032212989 valid 0.21585327224886935
LOSS train 0.2286077032212989 valid 0.21591093722018093
LOSS train 0.2286077032212989 valid 0.21590746633654875
LOSS train 0.2286077032212989 valid 0.21578705241389598
LOSS train 0.2286077032212989 valid 0.21569303870201112
LOSS train 0.2286077032212989 valid 0.21550380944993572
LOSS train 0.2286077032212989 valid 0.21546823341161647
LOSS train 0.2286077032212989 valid 0.21554013439075687
LOSS train 0.2286077032212989 valid 0.21551171632509836
LOSS train 0.2286077032212989 valid 0.2155694441837177
LOSS train 0.2286077032212989 valid 0.2155176789818944
LOSS train 0.2286077032212989 valid 0.21542061029410944
LOSS train 0.2286077032212989 valid 0.21531201174689663
LOSS train 0.2286077032212989 valid 0.21523447412108054
LOSS train 0.2286077032212989 valid 0.21515967409158576
LOSS train 0.2286077032212989 valid 0.21513381544052532
LOSS train 0.2286077032212989 valid 0.21510434492606006
LOSS train 0.2286077032212989 valid 0.2150988982094026
LOSS train 0.2286077032212989 valid 0.21509847820413355
LOSS train 0.2286077032212989 valid 0.21506522232193057
LOSS train 0.2286077032212989 valid 0.21505060822174354
LOSS train 0.2286077032212989 valid 0.215034129242303
LOSS train 0.2286077032212989 valid 0.21513459716467248
LOSS train 0.2286077032212989 valid 0.21515385228455267
LOSS train 0.2286077032212989 valid 0.21509190772970518
LOSS train 0.2286077032212989 valid 0.2149979248890449
LOSS train 0.2286077032212989 valid 0.2150143722923386
LOSS train 0.2286077032212989 valid 0.21495501338058573
LOSS train 0.2286077032212989 valid 0.21497973270322146
LOSS train 0.2286077032212989 valid 0.21485093685447193
LOSS train 0.2286077032212989 valid 0.21513543643203437
LOSS train 0.2286077032212989 valid 0.215086982188085
LOSS train 0.2286077032212989 valid 0.2151056777734261
LOSS train 0.2286077032212989 valid 0.21509485882269913
LOSS train 0.2286077032212989 valid 0.2150583899790241
LOSS train 0.2286077032212989 valid 0.215071548602972
LOSS train 0.2286077032212989 valid 0.21498761932628277
LOSS train 0.2286077032212989 valid 0.21507713546196874
LOSS train 0.2286077032212989 valid 0.2150028448101062
LOSS train 0.2286077032212989 valid 0.21494349941374763
LOSS train 0.2286077032212989 valid 0.21496263127538223
LOSS train 0.2286077032212989 valid 0.21498528649378273
LOSS train 0.2286077032212989 valid 0.21498392330213162
LOSS train 0.2286077032212989 valid 0.21497307923333397
LOSS train 0.2286077032212989 valid 0.21483224323019384
LOSS train 0.2286077032212989 valid 0.214952943852386
LOSS train 0.2286077032212989 valid 0.21498973635227783
LOSS train 0.2286077032212989 valid 0.21485804113625742
LOSS train 0.2286077032212989 valid 0.2148078990019398
LOSS train 0.2286077032212989 valid 0.21467033775953145
LOSS train 0.2286077032212989 valid 0.2146732611524547
LOSS train 0.2286077032212989 valid 0.21460315936020025
LOSS train 0.2286077032212989 valid 0.21461323008122968
LOSS train 0.2286077032212989 valid 0.2146281922026608
LOSS train 0.2286077032212989 valid 0.21463970618717598
LOSS train 0.2286077032212989 valid 0.214602127761279
LOSS train 0.2286077032212989 valid 0.21457404526601354
LOSS train 0.2286077032212989 valid 0.21475932422700944
LOSS train 0.2286077032212989 valid 0.21465244675110914
LOSS train 0.2286077032212989 valid 0.21458824192410084
LOSS train 0.2286077032212989 valid 0.2144919919027459
LOSS train 0.2286077032212989 valid 0.2146771507287945
LOSS train 0.2286077032212989 valid 0.21465640038957257
LOSS train 0.2286077032212989 valid 0.21459958027597725
LOSS train 0.2286077032212989 valid 0.21472870838992736
LOSS train 0.2286077032212989 valid 0.2146522396645588
LOSS train 0.2286077032212989 valid 0.21463323227669062
LOSS train 0.2286077032212989 valid 0.21447525570934786
LOSS train 0.2286077032212989 valid 0.21458784391193889
LOSS train 0.2286077032212989 valid 0.2147533061279767
LOSS train 0.2286077032212989 valid 0.21469141254363033
LOSS train 0.2286077032212989 valid 0.21457639289348887
LOSS train 0.2286077032212989 valid 0.2147188201632308
LOSS train 0.2286077032212989 valid 0.21467839202771555
LOSS train 0.2286077032212989 valid 0.21470775876726422
LOSS train 0.2286077032212989 valid 0.21478126369989836
LOSS train 0.2286077032212989 valid 0.21479826937006277
LOSS train 0.2286077032212989 valid 0.2148680204034865
LOSS train 0.2286077032212989 valid 0.21483954181105402
LOSS train 0.2286077032212989 valid 0.21469762014671112
LOSS train 0.2286077032212989 valid 0.21471183684267356
LOSS train 0.2286077032212989 valid 0.21470195080051904
LOSS train 0.2286077032212989 valid 0.2146239125861802
LOSS train 0.2286077032212989 valid 0.2145766428254109
LOSS train 0.2286077032212989 valid 0.21481472667720583
LOSS train 0.2286077032212989 valid 0.21475721293043892
LOSS train 0.2286077032212989 valid 0.21475695847477044
LOSS train 0.2286077032212989 valid 0.2148095101282288
LOSS train 0.2286077032212989 valid 0.21470769154992733
LOSS train 0.2286077032212989 valid 0.2148129595877373
LOSS train 0.2286077032212989 valid 0.2147801617781321
LOSS train 0.2286077032212989 valid 0.21468936901651223
LOSS train 0.2286077032212989 valid 0.21458012626870818
LOSS train 0.2286077032212989 valid 0.21455847720305124
EPOCH 30:
  batch 1 loss: 0.2228427231311798
  batch 2 loss: 0.23901714384555817
  batch 3 loss: 0.23320287466049194
  batch 4 loss: 0.2369467429816723
  batch 5 loss: 0.23945155441761018
  batch 6 loss: 0.23521622270345688
  batch 7 loss: 0.23357723014695303
  batch 8 loss: 0.2315487489104271
  batch 9 loss: 0.23549535870552063
  batch 10 loss: 0.2317476287484169
  batch 11 loss: 0.23427106169137088
  batch 12 loss: 0.23446732511123022
  batch 13 loss: 0.23207797339329353
  batch 14 loss: 0.2300481359873499
  batch 15 loss: 0.23024047315120696
  batch 16 loss: 0.2291788524016738
  batch 17 loss: 0.22959711884751038
  batch 18 loss: 0.2270684896243943
  batch 19 loss: 0.22550713153261887
  batch 20 loss: 0.22350504025816917
  batch 21 loss: 0.22667021907511212
  batch 22 loss: 0.22777302089062604
  batch 23 loss: 0.22731611197409424
  batch 24 loss: 0.22654171908895174
  batch 25 loss: 0.2283503544330597
  batch 26 loss: 0.22849687991233972
  batch 27 loss: 0.22931153078873953
  batch 28 loss: 0.23092876321503095
  batch 29 loss: 0.23135739256595744
  batch 30 loss: 0.23313554922739665
  batch 31 loss: 0.23326703617649694
  batch 32 loss: 0.23434824869036674
  batch 33 loss: 0.23589439464337897
  batch 34 loss: 0.23639543179203482
  batch 35 loss: 0.23821163177490234
  batch 36 loss: 0.23953057328859964
  batch 37 loss: 0.23999093593777837
  batch 38 loss: 0.2401149135671164
  batch 39 loss: 0.23994279748354203
  batch 40 loss: 0.23892370015382766
  batch 41 loss: 0.23848007764758133
  batch 42 loss: 0.23792488092467898
  batch 43 loss: 0.23782807449961818
  batch 44 loss: 0.2376704676584764
  batch 45 loss: 0.23682790531052483
  batch 46 loss: 0.2354263326396113
  batch 47 loss: 0.23407755887254755
  batch 48 loss: 0.23340165118376413
  batch 49 loss: 0.23349108775051272
  batch 50 loss: 0.2342497029900551
  batch 51 loss: 0.23351258974449307
  batch 52 loss: 0.23299398072637045
  batch 53 loss: 0.2322782722284209
  batch 54 loss: 0.23167830760832187
  batch 55 loss: 0.23161519332365557
  batch 56 loss: 0.23204697615333966
  batch 57 loss: 0.23154324214709432
  batch 58 loss: 0.23109698732351436
  batch 59 loss: 0.2310569895020986
  batch 60 loss: 0.23164442752798398
  batch 61 loss: 0.23235859123409772
  batch 62 loss: 0.23306246654641244
  batch 63 loss: 0.23255965397471473
  batch 64 loss: 0.23205737164244056
  batch 65 loss: 0.2314598771241995
  batch 66 loss: 0.23081223847288074
  batch 67 loss: 0.23007473015963142
  batch 68 loss: 0.229853336863658
  batch 69 loss: 0.23014913823293603
  batch 70 loss: 0.23010834285191129
  batch 71 loss: 0.230447488351607
  batch 72 loss: 0.230632240159644
  batch 73 loss: 0.23030814895891163
  batch 74 loss: 0.23022073910043045
  batch 75 loss: 0.23002216279506682
  batch 76 loss: 0.23027423788842402
  batch 77 loss: 0.23033443480343013
  batch 78 loss: 0.2303047138146865
  batch 79 loss: 0.23027175627177274
  batch 80 loss: 0.23043432906270028
  batch 81 loss: 0.230712205539515
  batch 82 loss: 0.23096504662095046
  batch 83 loss: 0.2307487562837371
  batch 84 loss: 0.23030559133206094
  batch 85 loss: 0.2306746880797779
  batch 86 loss: 0.23047612746094548
  batch 87 loss: 0.2301587887193965
  batch 88 loss: 0.2296132587573745
  batch 89 loss: 0.22903153266799584
  batch 90 loss: 0.22924094448486965
  batch 91 loss: 0.2291373094687095
  batch 92 loss: 0.22886858864322954
  batch 93 loss: 0.22830738832232772
  batch 94 loss: 0.22806037486867703
  batch 95 loss: 0.2278551492251848
  batch 96 loss: 0.227656080853194
  batch 97 loss: 0.22821639478206635
  batch 98 loss: 0.22807613669001325
  batch 99 loss: 0.2278401961531302
  batch 100 loss: 0.22818734958767892
  batch 101 loss: 0.22846753511688497
  batch 102 loss: 0.2287614832029623
  batch 103 loss: 0.2285647583239287
  batch 104 loss: 0.2288204016020665
  batch 105 loss: 0.22845766090211414
  batch 106 loss: 0.2285061076564609
  batch 107 loss: 0.22854857077108365
  batch 108 loss: 0.22819359021054375
  batch 109 loss: 0.22834182127353247
  batch 110 loss: 0.2283213559876789
  batch 111 loss: 0.22857644120315174
  batch 112 loss: 0.2285656706829156
  batch 113 loss: 0.22843484606890552
  batch 114 loss: 0.22851646926842237
  batch 115 loss: 0.22810604170612667
  batch 116 loss: 0.22807105797631988
  batch 117 loss: 0.2280075706732579
  batch 118 loss: 0.22813859821881277
  batch 119 loss: 0.22833140970779067
  batch 120 loss: 0.2281573921442032
  batch 121 loss: 0.22822953563583784
  batch 122 loss: 0.2280919318560694
  batch 123 loss: 0.2277365457720873
  batch 124 loss: 0.22762503018302302
  batch 125 loss: 0.22728406059741973
  batch 126 loss: 0.227347057017069
  batch 127 loss: 0.22762653677482306
  batch 128 loss: 0.22751771146431565
  batch 129 loss: 0.22773537085961926
  batch 130 loss: 0.22788420227857736
  batch 131 loss: 0.22788778504797522
  batch 132 loss: 0.22814498220880827
  batch 133 loss: 0.22794725964392037
  batch 134 loss: 0.2278083397158936
  batch 135 loss: 0.22765557501051162
  batch 136 loss: 0.22806631225873442
  batch 137 loss: 0.2281356420177613
  batch 138 loss: 0.22804622859626578
  batch 139 loss: 0.22778993544818685
  batch 140 loss: 0.22813811408621923
  batch 141 loss: 0.2278494841240822
  batch 142 loss: 0.2277249658191708
  batch 143 loss: 0.22775552447859224
  batch 144 loss: 0.22767401383154923
  batch 145 loss: 0.2277477989936697
  batch 146 loss: 0.22781517499521986
  batch 147 loss: 0.22777127175509523
  batch 148 loss: 0.227699117584003
  batch 149 loss: 0.22741450449364298
  batch 150 loss: 0.22743061383565266
  batch 151 loss: 0.22706117061589728
  batch 152 loss: 0.2268392860301231
  batch 153 loss: 0.22687607838047874
  batch 154 loss: 0.22659692652039715
  batch 155 loss: 0.2263783222244632
  batch 156 loss: 0.2262582622277431
  batch 157 loss: 0.22610334548980568
  batch 158 loss: 0.22626716143722775
  batch 159 loss: 0.2263111152551459
  batch 160 loss: 0.22611226225271822
  batch 161 loss: 0.22587934219689101
  batch 162 loss: 0.2260654288125627
  batch 163 loss: 0.2259768804341006
  batch 164 loss: 0.22613597761203602
  batch 165 loss: 0.2262762986349337
  batch 166 loss: 0.2266181575426136
  batch 167 loss: 0.22681388749690826
  batch 168 loss: 0.2266189238677422
  batch 169 loss: 0.22661624293355548
  batch 170 loss: 0.22657174064832575
  batch 171 loss: 0.22663865303784086
  batch 172 loss: 0.22652823659916257
  batch 173 loss: 0.2266860110739063
  batch 174 loss: 0.2267258764169682
  batch 175 loss: 0.22676336416176388
  batch 176 loss: 0.2270427965135737
  batch 177 loss: 0.22747941820298212
  batch 178 loss: 0.22729958819874216
  batch 179 loss: 0.22736856291413973
  batch 180 loss: 0.22735698504580393
  batch 181 loss: 0.2272938662800341
  batch 182 loss: 0.227205671615653
  batch 183 loss: 0.22727986763083868
  batch 184 loss: 0.22745108766400296
  batch 185 loss: 0.22728572286464072
  batch 186 loss: 0.22730632959514535
  batch 187 loss: 0.2274852575146578
  batch 188 loss: 0.22753976364719106
  batch 189 loss: 0.22761439472909958
  batch 190 loss: 0.22741685564580716
  batch 191 loss: 0.22750723962696434
  batch 192 loss: 0.22737593584073088
  batch 193 loss: 0.22734357022868537
  batch 194 loss: 0.2272252244586797
  batch 195 loss: 0.2273562254814001
  batch 196 loss: 0.22720747007703296
  batch 197 loss: 0.22745902382480312
  batch 198 loss: 0.22749183372114645
  batch 199 loss: 0.2276666479643865
  batch 200 loss: 0.22761658817529679
  batch 201 loss: 0.22752417312629186
  batch 202 loss: 0.22769393299770826
  batch 203 loss: 0.22775190488751887
  batch 204 loss: 0.22779229633948384
  batch 205 loss: 0.22778934552902128
  batch 206 loss: 0.22782031374359593
  batch 207 loss: 0.2278124286212783
  batch 208 loss: 0.22766259112037146
  batch 209 loss: 0.22774623070607344
  batch 210 loss: 0.22783589519205547
  batch 211 loss: 0.227727499335863
  batch 212 loss: 0.22780095764769698
  batch 213 loss: 0.22771308224805645
  batch 214 loss: 0.2275790718392791
  batch 215 loss: 0.22753622531890869
  batch 216 loss: 0.22747864429321554
  batch 217 loss: 0.22749020832200204
  batch 218 loss: 0.2273133883782483
  batch 219 loss: 0.2269876176091634
  batch 220 loss: 0.22718920816074717
  batch 221 loss: 0.22702914004412172
  batch 222 loss: 0.22708027239318365
  batch 223 loss: 0.22696578509337165
  batch 224 loss: 0.22697356010654143
  batch 225 loss: 0.22699630743927426
  batch 226 loss: 0.22700895547075609
  batch 227 loss: 0.22697256282300152
  batch 228 loss: 0.22700547851752817
  batch 229 loss: 0.2270446766542035
  batch 230 loss: 0.2271120762695437
  batch 231 loss: 0.22710930939876672
  batch 232 loss: 0.22707744430879068
  batch 233 loss: 0.22702384391567737
  batch 234 loss: 0.22705705476622295
  batch 235 loss: 0.22696671067400181
  batch 236 loss: 0.22694314378550498
  batch 237 loss: 0.22671674688657126
  batch 238 loss: 0.2266313189468464
  batch 239 loss: 0.22661738303415946
  batch 240 loss: 0.22654093466699124
  batch 241 loss: 0.22634719454401261
  batch 242 loss: 0.22631707442693474
  batch 243 loss: 0.22630172244315286
  batch 244 loss: 0.22619457701679135
  batch 245 loss: 0.22602972485581224
  batch 246 loss: 0.22587401607656868
  batch 247 loss: 0.22586483064933344
  batch 248 loss: 0.2259043803017947
  batch 249 loss: 0.22594368912608748
  batch 250 loss: 0.22584049820899962
  batch 251 loss: 0.22570336126236326
  batch 252 loss: 0.22562336637860253
  batch 253 loss: 0.2255391167557758
  batch 254 loss: 0.22551133442582108
  batch 255 loss: 0.2254742842094571
  batch 256 loss: 0.22547130612656474
  batch 257 loss: 0.22554268754641835
  batch 258 loss: 0.22547495948482854
  batch 259 loss: 0.22536541251142053
  batch 260 loss: 0.22538218652972808
  batch 261 loss: 0.22548163262591964
  batch 262 loss: 0.2253433607006801
  batch 263 loss: 0.225186186598281
  batch 264 loss: 0.2252248361932509
  batch 265 loss: 0.22509830273547263
  batch 266 loss: 0.22501284584290998
  batch 267 loss: 0.22483005840680126
  batch 268 loss: 0.2246950560652498
  batch 269 loss: 0.22469717779345671
  batch 270 loss: 0.22459761323752225
  batch 271 loss: 0.22457980526769294
  batch 272 loss: 0.22443088363198674
  batch 273 loss: 0.2243188045946233
  batch 274 loss: 0.2241945248233141
  batch 275 loss: 0.22408168987794355
  batch 276 loss: 0.22398117314214291
  batch 277 loss: 0.22389320641863647
  batch 278 loss: 0.2237550767420007
  batch 279 loss: 0.22372817298844724
  batch 280 loss: 0.22366960437170097
  batch 281 loss: 0.22365767787147672
  batch 282 loss: 0.22345452301257046
  batch 283 loss: 0.22335207341205948
  batch 284 loss: 0.22335749387111462
  batch 285 loss: 0.22333370295532962
  batch 286 loss: 0.2232750656825679
  batch 287 loss: 0.2232024182633656
  batch 288 loss: 0.2232181111143695
  batch 289 loss: 0.22310721203942613
  batch 290 loss: 0.22301833958461367
  batch 291 loss: 0.22303823430308772
  batch 292 loss: 0.22310210095897112
  batch 293 loss: 0.2230915129693295
  batch 294 loss: 0.2228760159238666
  batch 295 loss: 0.2229409051143517
  batch 296 loss: 0.22299747783187274
  batch 297 loss: 0.22287438624234313
  batch 298 loss: 0.22283559592578234
  batch 299 loss: 0.2229424297909274
  batch 300 loss: 0.22283810287714004
  batch 301 loss: 0.2228312423557934
  batch 302 loss: 0.22293068799159385
  batch 303 loss: 0.22284222716348792
  batch 304 loss: 0.22280961069229402
  batch 305 loss: 0.2227534570166322
  batch 306 loss: 0.22277088711659113
  batch 307 loss: 0.22275479929066636
  batch 308 loss: 0.22282184878146494
  batch 309 loss: 0.22279867689007693
  batch 310 loss: 0.2228727119103555
  batch 311 loss: 0.2227849284264819
  batch 312 loss: 0.22275856571892896
  batch 313 loss: 0.22275175363682329
  batch 314 loss: 0.2228121167630147
  batch 315 loss: 0.2227060967494571
  batch 316 loss: 0.22266442036326928
  batch 317 loss: 0.22271953471451528
  batch 318 loss: 0.2225269267112954
  batch 319 loss: 0.22234580822312347
  batch 320 loss: 0.22230552290566266
  batch 321 loss: 0.22237827296940338
  batch 322 loss: 0.22232807126844892
  batch 323 loss: 0.22222063309821552
  batch 324 loss: 0.22220387875481887
  batch 325 loss: 0.2222880301108727
  batch 326 loss: 0.22239253601413564
  batch 327 loss: 0.22237685937947088
  batch 328 loss: 0.22241966848874964
  batch 329 loss: 0.2223604926131779
  batch 330 loss: 0.22238268680644757
  batch 331 loss: 0.22232795274869913
  batch 332 loss: 0.22238956311021943
  batch 333 loss: 0.22239124967350257
  batch 334 loss: 0.22273950113686258
  batch 335 loss: 0.22288768980930102
  batch 336 loss: 0.22296908049888553
  batch 337 loss: 0.2231185528662509
  batch 338 loss: 0.22318829900237935
  batch 339 loss: 0.2232471851736395
  batch 340 loss: 0.22327538430690766
  batch 341 loss: 0.22323131473882457
  batch 342 loss: 0.22317008412720865
  batch 343 loss: 0.22325522837068876
  batch 344 loss: 0.22327944722979567
  batch 345 loss: 0.22335700772810674
  batch 346 loss: 0.22335991065281663
  batch 347 loss: 0.223241069131351
  batch 348 loss: 0.22325869927974953
  batch 349 loss: 0.2232940550946916
  batch 350 loss: 0.22345327475241253
  batch 351 loss: 0.22345002929539423
  batch 352 loss: 0.22345222181386568
  batch 353 loss: 0.22342139410736203
  batch 354 loss: 0.22335112621050096
  batch 355 loss: 0.2232474901306797
  batch 356 loss: 0.2233738646413503
  batch 357 loss: 0.22326783664754124
  batch 358 loss: 0.22312349218396502
  batch 359 loss: 0.2230976675546269
  batch 360 loss: 0.22306427583098412
  batch 361 loss: 0.22297086044526826
  batch 362 loss: 0.22299209998622124
  batch 363 loss: 0.2229503123064015
  batch 364 loss: 0.2227854551440412
  batch 365 loss: 0.22285277778155183
  batch 366 loss: 0.22285931162495431
  batch 367 loss: 0.22306676407925766
  batch 368 loss: 0.22316943109035492
  batch 369 loss: 0.22314952568310062
  batch 370 loss: 0.22318493590967076
  batch 371 loss: 0.22324474284430398
  batch 372 loss: 0.223378824250352
  batch 373 loss: 0.22342518535280356
  batch 374 loss: 0.22331121914526997
  batch 375 loss: 0.22333868205547333
  batch 376 loss: 0.2234726526397974
  batch 377 loss: 0.22347481726651483
  batch 378 loss: 0.22354953558672042
  batch 379 loss: 0.22357804542481113
  batch 380 loss: 0.2234770589753201
  batch 381 loss: 0.22338816593951127
  batch 382 loss: 0.2234040591573216
  batch 383 loss: 0.22346903029683365
  batch 384 loss: 0.22357694284679988
  batch 385 loss: 0.22352384526234167
  batch 386 loss: 0.22355189081763974
  batch 387 loss: 0.22356444289209923
  batch 388 loss: 0.22357113217723737
  batch 389 loss: 0.22362780597798315
  batch 390 loss: 0.22368645400573045
  batch 391 loss: 0.22362990189543772
  batch 392 loss: 0.22367539131367692
  batch 393 loss: 0.22378703242798192
  batch 394 loss: 0.2238821110068844
  batch 395 loss: 0.2238944690061521
  batch 396 loss: 0.22390848654087145
  batch 397 loss: 0.22406979344353567
  batch 398 loss: 0.22429918157095885
  batch 399 loss: 0.22430850920223055
  batch 400 loss: 0.22437562987208368
  batch 401 loss: 0.22444631363685588
  batch 402 loss: 0.2245592626914456
  batch 403 loss: 0.22461380622818808
  batch 404 loss: 0.22467807945933674
  batch 405 loss: 0.22475878449133885
  batch 406 loss: 0.22468837603853253
  batch 407 loss: 0.2247529782592811
  batch 408 loss: 0.22472955611552678
  batch 409 loss: 0.22479514085430388
  batch 410 loss: 0.22472666865441857
  batch 411 loss: 0.2246747920765494
  batch 412 loss: 0.2247445725483223
  batch 413 loss: 0.22472676133272435
  batch 414 loss: 0.22472669169810658
  batch 415 loss: 0.2246638261051063
  batch 416 loss: 0.22469108526666576
  batch 417 loss: 0.22463822114667734
  batch 418 loss: 0.2246293942466307
  batch 419 loss: 0.22457740528771303
  batch 420 loss: 0.22461410044204622
  batch 421 loss: 0.224690602751639
  batch 422 loss: 0.2246253179359775
  batch 423 loss: 0.224572696232063
  batch 424 loss: 0.22460392890673764
  batch 425 loss: 0.22452989280223845
  batch 426 loss: 0.22452400421872384
  batch 427 loss: 0.22456910162815166
  batch 428 loss: 0.22461034402808297
  batch 429 loss: 0.2245387103590932
  batch 430 loss: 0.22461066349994305
  batch 431 loss: 0.22459858555666531
  batch 432 loss: 0.22456275226755273
  batch 433 loss: 0.22449116095659638
  batch 434 loss: 0.22440895616733533
  batch 435 loss: 0.22441636930251943
  batch 436 loss: 0.2244299062348287
  batch 437 loss: 0.22430209900749085
  batch 438 loss: 0.22424127926973447
  batch 439 loss: 0.2241122155091757
  batch 440 loss: 0.22418251918120818
  batch 441 loss: 0.22413064306285105
  batch 442 loss: 0.2240861304003189
  batch 443 loss: 0.22412127545671054
  batch 444 loss: 0.22419967717147088
  batch 445 loss: 0.22415996672732108
  batch 446 loss: 0.22413730086767086
  batch 447 loss: 0.22417921414577988
  batch 448 loss: 0.22430118333016122
  batch 449 loss: 0.22431312302305864
  batch 450 loss: 0.22426845477686988
  batch 451 loss: 0.2242432179511783
  batch 452 loss: 0.22421866052050507
  batch 453 loss: 0.2242822315848163
  batch 454 loss: 0.22425604027404658
  batch 455 loss: 0.22425926484904446
  batch 456 loss: 0.2242231158245551
  batch 457 loss: 0.2242420406714571
  batch 458 loss: 0.22428959246936322
  batch 459 loss: 0.2242791583530264
  batch 460 loss: 0.2242871420862882
  batch 461 loss: 0.22433249209552938
  batch 462 loss: 0.22440139665490105
  batch 463 loss: 0.22440663243987904
  batch 464 loss: 0.2243976112583588
  batch 465 loss: 0.2244485810559283
  batch 466 loss: 0.22439889491499748
  batch 467 loss: 0.22446754188685653
  batch 468 loss: 0.22434349701954767
  batch 469 loss: 0.22424476315726094
  batch 470 loss: 0.2242528830119904
  batch 471 loss: 0.22423922449287081
  batch 472 loss: 0.22409523344772347
LOSS train 0.22409523344772347 valid 0.2203463912010193
LOSS train 0.22409523344772347 valid 0.20994244515895844
LOSS train 0.22409523344772347 valid 0.2203624943892161
LOSS train 0.22409523344772347 valid 0.20628610625863075
LOSS train 0.22409523344772347 valid 0.20407911241054535
LOSS train 0.22409523344772347 valid 0.20616154124339423
LOSS train 0.22409523344772347 valid 0.20044109225273132
LOSS train 0.22409523344772347 valid 0.19521225430071354
LOSS train 0.22409523344772347 valid 0.19554120136631858
LOSS train 0.22409523344772347 valid 0.19448172003030778
LOSS train 0.22409523344772347 valid 0.19498831033706665
LOSS train 0.22409523344772347 valid 0.1969489318629106
LOSS train 0.22409523344772347 valid 0.19664242290533507
LOSS train 0.22409523344772347 valid 0.1938439149941717
LOSS train 0.22409523344772347 valid 0.19434368312358857
LOSS train 0.22409523344772347 valid 0.19923991430550814
LOSS train 0.22409523344772347 valid 0.19778194234651678
LOSS train 0.22409523344772347 valid 0.1972738661699825
LOSS train 0.22409523344772347 valid 0.20023172544805626
LOSS train 0.22409523344772347 valid 0.19964087903499603
LOSS train 0.22409523344772347 valid 0.20228151622272672
LOSS train 0.22409523344772347 valid 0.20239987088875336
LOSS train 0.22409523344772347 valid 0.2006326298350873
LOSS train 0.22409523344772347 valid 0.20052427301804224
LOSS train 0.22409523344772347 valid 0.20015472590923308
LOSS train 0.22409523344772347 valid 0.1984016757745009
LOSS train 0.22409523344772347 valid 0.19822845801159186
LOSS train 0.22409523344772347 valid 0.19845773439322198
LOSS train 0.22409523344772347 valid 0.19726407322390327
LOSS train 0.22409523344772347 valid 0.1971678227186203
LOSS train 0.22409523344772347 valid 0.19811376737010095
LOSS train 0.22409523344772347 valid 0.1983397463336587
LOSS train 0.22409523344772347 valid 0.19697024515180878
LOSS train 0.22409523344772347 valid 0.19661783339346156
LOSS train 0.22409523344772347 valid 0.19688370312963213
LOSS train 0.22409523344772347 valid 0.19756952838765252
LOSS train 0.22409523344772347 valid 0.1986646994545653
LOSS train 0.22409523344772347 valid 0.19895017970549433
LOSS train 0.22409523344772347 valid 0.19955647412018898
LOSS train 0.22409523344772347 valid 0.19967013895511626
LOSS train 0.22409523344772347 valid 0.19993981682672735
LOSS train 0.22409523344772347 valid 0.201122556413923
LOSS train 0.22409523344772347 valid 0.20191950714865395
LOSS train 0.22409523344772347 valid 0.20191522213545712
LOSS train 0.22409523344772347 valid 0.20161140627331203
LOSS train 0.22409523344772347 valid 0.20122814761555713
LOSS train 0.22409523344772347 valid 0.20082079127747962
LOSS train 0.22409523344772347 valid 0.2026194672410687
LOSS train 0.22409523344772347 valid 0.20161158515482533
LOSS train 0.22409523344772347 valid 0.20205170035362244
LOSS train 0.22409523344772347 valid 0.20171952510581298
LOSS train 0.22409523344772347 valid 0.20147343295124862
LOSS train 0.22409523344772347 valid 0.20291854947243096
LOSS train 0.22409523344772347 valid 0.20286693534365408
LOSS train 0.22409523344772347 valid 0.2027242836627093
LOSS train 0.22409523344772347 valid 0.20271694367485388
LOSS train 0.22409523344772347 valid 0.20210123820263043
LOSS train 0.22409523344772347 valid 0.20241591257267985
LOSS train 0.22409523344772347 valid 0.202054234631991
LOSS train 0.22409523344772347 valid 0.20177499105532964
LOSS train 0.22409523344772347 valid 0.20171718372673283
LOSS train 0.22409523344772347 valid 0.20158685239091997
LOSS train 0.22409523344772347 valid 0.20130621204300533
LOSS train 0.22409523344772347 valid 0.20168315526098013
LOSS train 0.22409523344772347 valid 0.2006781463439648
LOSS train 0.22409523344772347 valid 0.20034319884849316
LOSS train 0.22409523344772347 valid 0.200737627139732
LOSS train 0.22409523344772347 valid 0.19998801959788098
LOSS train 0.22409523344772347 valid 0.20076892190221426
LOSS train 0.22409523344772347 valid 0.20129751797233308
LOSS train 0.22409523344772347 valid 0.20195624358217482
LOSS train 0.22409523344772347 valid 0.20239537747369873
LOSS train 0.22409523344772347 valid 0.20332428976281047
LOSS train 0.22409523344772347 valid 0.20293875441357895
LOSS train 0.22409523344772347 valid 0.20255853096644083
LOSS train 0.22409523344772347 valid 0.20269956439733505
LOSS train 0.22409523344772347 valid 0.20240322026339444
LOSS train 0.22409523344772347 valid 0.20253445322696978
LOSS train 0.22409523344772347 valid 0.20250871192805375
LOSS train 0.22409523344772347 valid 0.20198995284736157
LOSS train 0.22409523344772347 valid 0.20225426407507907
LOSS train 0.22409523344772347 valid 0.20213647931814194
LOSS train 0.22409523344772347 valid 0.20245003484817872
LOSS train 0.22409523344772347 valid 0.20203621312975883
LOSS train 0.22409523344772347 valid 0.20280983710990233
LOSS train 0.22409523344772347 valid 0.2029776420704154
LOSS train 0.22409523344772347 valid 0.20269755625176702
LOSS train 0.22409523344772347 valid 0.20292808809740978
LOSS train 0.22409523344772347 valid 0.20327023977643988
LOSS train 0.22409523344772347 valid 0.20384903483920627
LOSS train 0.22409523344772347 valid 0.20388706372334406
LOSS train 0.22409523344772347 valid 0.2037364838887816
LOSS train 0.22409523344772347 valid 0.20362256547456148
LOSS train 0.22409523344772347 valid 0.20407895474357807
LOSS train 0.22409523344772347 valid 0.2042969912290573
LOSS train 0.22409523344772347 valid 0.204781545791775
LOSS train 0.22409523344772347 valid 0.20479878522071643
LOSS train 0.22409523344772347 valid 0.2051038298071647
LOSS train 0.22409523344772347 valid 0.2051059581113584
LOSS train 0.22409523344772347 valid 0.20534975215792656
LOSS train 0.22409523344772347 valid 0.2053640430221463
LOSS train 0.22409523344772347 valid 0.20569837604667626
LOSS train 0.22409523344772347 valid 0.2055441379836462
LOSS train 0.22409523344772347 valid 0.2052608924702956
LOSS train 0.22409523344772347 valid 0.20563879154977344
LOSS train 0.22409523344772347 valid 0.2056566690498928
LOSS train 0.22409523344772347 valid 0.20533037575605873
LOSS train 0.22409523344772347 valid 0.2053525726552363
LOSS train 0.22409523344772347 valid 0.20487880310334197
LOSS train 0.22409523344772347 valid 0.2047362507744269
LOSS train 0.22409523344772347 valid 0.20473454140865052
LOSS train 0.22409523344772347 valid 0.20506862032094172
LOSS train 0.22409523344772347 valid 0.2046491880859949
LOSS train 0.22409523344772347 valid 0.2045630399595227
LOSS train 0.22409523344772347 valid 0.2054539452428403
LOSS train 0.22409523344772347 valid 0.2051546666128882
LOSS train 0.22409523344772347 valid 0.2058595011376927
LOSS train 0.22409523344772347 valid 0.20568993106736974
LOSS train 0.22409523344772347 valid 0.2054981465349678
LOSS train 0.22409523344772347 valid 0.20507350551585357
LOSS train 0.22409523344772347 valid 0.20488374735698228
LOSS train 0.22409523344772347 valid 0.20512587687031167
LOSS train 0.22409523344772347 valid 0.2051215612791418
LOSS train 0.22409523344772347 valid 0.20529879737765558
LOSS train 0.22409523344772347 valid 0.2051662255525589
LOSS train 0.22409523344772347 valid 0.20551761310724986
LOSS train 0.22409523344772347 valid 0.2052755122344325
LOSS train 0.22409523344772347 valid 0.2052961594890803
LOSS train 0.22409523344772347 valid 0.2049099347850149
LOSS train 0.22409523344772347 valid 0.20448862107900473
LOSS train 0.22409523344772347 valid 0.2045832779116303
LOSS train 0.22409523344772347 valid 0.2044196052081657
LOSS train 0.22409523344772347 valid 0.20442959330135718
LOSS train 0.22409523344772347 valid 0.20462353138336495
LOSS train 0.22409523344772347 valid 0.20475977581960184
LOSS train 0.22409523344772347 valid 0.20472648871295593
LOSS train 0.22409523344772347 valid 0.2047334857230639
LOSS train 0.22409523344772347 valid 0.2046389668316081
LOSS train 0.22409523344772347 valid 0.2044422367065073
LOSS train 0.22409523344772347 valid 0.20434295034834316
LOSS train 0.22409523344772347 valid 0.20435698457220766
LOSS train 0.22409523344772347 valid 0.2046466480258485
LOSS train 0.22409523344772347 valid 0.2047626510575101
LOSS train 0.22409523344772347 valid 0.20487241188271177
LOSS train 0.22409523344772347 valid 0.20478247342438533
LOSS train 0.22409523344772347 valid 0.2047204104800747
LOSS train 0.22409523344772347 valid 0.20469444047431556
LOSS train 0.22409523344772347 valid 0.20458109918478373
LOSS train 0.22409523344772347 valid 0.20469714341147635
LOSS train 0.22409523344772347 valid 0.2046187866727511
LOSS train 0.22409523344772347 valid 0.20444953303463412
LOSS train 0.22409523344772347 valid 0.20451969114181243
LOSS train 0.22409523344772347 valid 0.20432304478938285
LOSS train 0.22409523344772347 valid 0.2045813643119552
LOSS train 0.22409523344772347 valid 0.20471367028451734
LOSS train 0.22409523344772347 valid 0.20469118186678642
LOSS train 0.22409523344772347 valid 0.20469694769686195
LOSS train 0.22409523344772347 valid 0.2049141203678107
LOSS train 0.22409523344772347 valid 0.20504549914186107
LOSS train 0.22409523344772347 valid 0.20518396012485027
LOSS train 0.22409523344772347 valid 0.20510704500704818
LOSS train 0.22409523344772347 valid 0.20508850264696427
LOSS train 0.22409523344772347 valid 0.20499318024497823
LOSS train 0.22409523344772347 valid 0.20507278774933116
LOSS train 0.22409523344772347 valid 0.20524469866897121
LOSS train 0.22409523344772347 valid 0.2051734339041882
LOSS train 0.22409523344772347 valid 0.20517509925865127
LOSS train 0.22409523344772347 valid 0.20493477068486668
LOSS train 0.22409523344772347 valid 0.20478629096019904
LOSS train 0.22409523344772347 valid 0.20497336299980387
LOSS train 0.22409523344772347 valid 0.20491631390058507
LOSS train 0.22409523344772347 valid 0.2050124888503274
LOSS train 0.22409523344772347 valid 0.2048171317646269
LOSS train 0.22409523344772347 valid 0.20513486485371643
LOSS train 0.22409523344772347 valid 0.20507476193564278
LOSS train 0.22409523344772347 valid 0.205095257779414
LOSS train 0.22409523344772347 valid 0.20505530744959405
LOSS train 0.22409523344772347 valid 0.2051409086986874
LOSS train 0.22409523344772347 valid 0.20529106617306864
LOSS train 0.22409523344772347 valid 0.20517220637864536
LOSS train 0.22409523344772347 valid 0.2052178293960529
LOSS train 0.22409523344772347 valid 0.20502625340288813
LOSS train 0.22409523344772347 valid 0.20513977655947535
LOSS train 0.22409523344772347 valid 0.20502201046632684
LOSS train 0.22409523344772347 valid 0.2049148934918481
LOSS train 0.22409523344772347 valid 0.20503780854645595
LOSS train 0.22409523344772347 valid 0.20488091339083278
LOSS train 0.22409523344772347 valid 0.20498671255847242
LOSS train 0.22409523344772347 valid 0.20472096876492576
LOSS train 0.22409523344772347 valid 0.2048300428610099
LOSS train 0.22409523344772347 valid 0.20493833416419505
LOSS train 0.22409523344772347 valid 0.2048452792223543
LOSS train 0.22409523344772347 valid 0.20448499045532603
LOSS train 0.22409523344772347 valid 0.20431382086166403
LOSS train 0.22409523344772347 valid 0.20432264850689816
LOSS train 0.22409523344772347 valid 0.20439341261374708
LOSS train 0.22409523344772347 valid 0.20437179097366817
LOSS train 0.22409523344772347 valid 0.20456677946177396
LOSS train 0.22409523344772347 valid 0.2045352807895622
LOSS train 0.22409523344772347 valid 0.20470018796622752
LOSS train 0.22409523344772347 valid 0.2045356356237658
LOSS train 0.22409523344772347 valid 0.2045450198768389
LOSS train 0.22409523344772347 valid 0.20466147010843155
LOSS train 0.22409523344772347 valid 0.2043650614134237
LOSS train 0.22409523344772347 valid 0.20432330079194977
LOSS train 0.22409523344772347 valid 0.20437834910976077
LOSS train 0.22409523344772347 valid 0.2043897391517381
LOSS train 0.22409523344772347 valid 0.20416855812072754
LOSS train 0.22409523344772347 valid 0.20422274230769946
LOSS train 0.22409523344772347 valid 0.2043835712330682
LOSS train 0.22409523344772347 valid 0.20460752627295906
LOSS train 0.22409523344772347 valid 0.20442136644192463
LOSS train 0.22409523344772347 valid 0.20442340288643546
LOSS train 0.22409523344772347 valid 0.20442467363916825
LOSS train 0.22409523344772347 valid 0.2042080199995706
LOSS train 0.22409523344772347 valid 0.20401742254142408
LOSS train 0.22409523344772347 valid 0.2037584491863778
LOSS train 0.22409523344772347 valid 0.20375621653751497
LOSS train 0.22409523344772347 valid 0.2038026750087738
LOSS train 0.22409523344772347 valid 0.20393105846914378
LOSS train 0.22409523344772347 valid 0.20393923477889186
LOSS train 0.22409523344772347 valid 0.20402038842439651
LOSS train 0.22409523344772347 valid 0.20423171091240083
LOSS train 0.22409523344772347 valid 0.20410739158147148
LOSS train 0.22409523344772347 valid 0.20398525920179156
LOSS train 0.22409523344772347 valid 0.20388436587774647
LOSS train 0.22409523344772347 valid 0.20386787467853612
LOSS train 0.22409523344772347 valid 0.2037728089643152
LOSS train 0.22409523344772347 valid 0.2037870056243963
LOSS train 0.22409523344772347 valid 0.20379410833120346
LOSS train 0.22409523344772347 valid 0.20379882850946285
LOSS train 0.22409523344772347 valid 0.20390021582615786
LOSS train 0.22409523344772347 valid 0.20422809024225488
LOSS train 0.22409523344772347 valid 0.2041793699320565
LOSS train 0.22409523344772347 valid 0.20424502003700176
LOSS train 0.22409523344772347 valid 0.2041007843563112
LOSS train 0.22409523344772347 valid 0.20420589250854299
LOSS train 0.22409523344772347 valid 0.20419400836489782
LOSS train 0.22409523344772347 valid 0.20412353179943613
LOSS train 0.22409523344772347 valid 0.20369159495458006
LOSS train 0.22409523344772347 valid 0.20360055865712184
LOSS train 0.22409523344772347 valid 0.20357646266660415
LOSS train 0.22409523344772347 valid 0.2033712560686555
LOSS train 0.22409523344772347 valid 0.20341165641658618
LOSS train 0.22409523344772347 valid 0.20352873908621924
LOSS train 0.22409523344772347 valid 0.20361850207777527
LOSS train 0.22409523344772347 valid 0.2037403148859136
LOSS train 0.22409523344772347 valid 0.20373455948767163
LOSS train 0.22409523344772347 valid 0.2035184064842611
LOSS train 0.22409523344772347 valid 0.20358134219050408
LOSS train 0.22409523344772347 valid 0.20367131578138625
LOSS train 0.22409523344772347 valid 0.2035532183649521
LOSS train 0.22409523344772347 valid 0.20329724343515668
LOSS train 0.22409523344772347 valid 0.20338555504604588
LOSS train 0.22409523344772347 valid 0.20324331226886488
LOSS train 0.22409523344772347 valid 0.20305585439200513
LOSS train 0.22409523344772347 valid 0.20304010451883658
LOSS train 0.22409523344772347 valid 0.2031876453191273
LOSS train 0.22409523344772347 valid 0.2030266216420299
LOSS train 0.22409523344772347 valid 0.2028666158994803
LOSS train 0.22409523344772347 valid 0.20293175314920617
LOSS train 0.22409523344772347 valid 0.20314885946061775
LOSS train 0.22409523344772347 valid 0.20291977625042767
LOSS train 0.22409523344772347 valid 0.20297418396468414
LOSS train 0.22409523344772347 valid 0.20287380699279173
LOSS train 0.22409523344772347 valid 0.20295708595698042
LOSS train 0.22409523344772347 valid 0.2028735037544247
LOSS train 0.22409523344772347 valid 0.20297346583831666
LOSS train 0.22409523344772347 valid 0.20308066386604842
LOSS train 0.22409523344772347 valid 0.20298853773209785
LOSS train 0.22409523344772347 valid 0.2030122521783593
LOSS train 0.22409523344772347 valid 0.20285149063805447
LOSS train 0.22409523344772347 valid 0.20271148663628233
LOSS train 0.22409523344772347 valid 0.20261310832235066
LOSS train 0.22409523344772347 valid 0.2027369766614654
LOSS train 0.22409523344772347 valid 0.20283661132165487
LOSS train 0.22409523344772347 valid 0.20285816181330044
LOSS train 0.22409523344772347 valid 0.2028759056769258
LOSS train 0.22409523344772347 valid 0.20274718386191193
LOSS train 0.22409523344772347 valid 0.20267019466097866
LOSS train 0.22409523344772347 valid 0.20248765042349962
LOSS train 0.22409523344772347 valid 0.20244743608942267
LOSS train 0.22409523344772347 valid 0.20252024266513413
LOSS train 0.22409523344772347 valid 0.20250563882291317
LOSS train 0.22409523344772347 valid 0.2025769466632291
LOSS train 0.22409523344772347 valid 0.20252352549166946
LOSS train 0.22409523344772347 valid 0.20243650106291322
LOSS train 0.22409523344772347 valid 0.2023390165478405
LOSS train 0.22409523344772347 valid 0.20227322268547895
LOSS train 0.22409523344772347 valid 0.20218807553936696
LOSS train 0.22409523344772347 valid 0.2021593640196774
LOSS train 0.22409523344772347 valid 0.20214919207541093
LOSS train 0.22409523344772347 valid 0.2021138138382508
LOSS train 0.22409523344772347 valid 0.2021353535962348
LOSS train 0.22409523344772347 valid 0.2021276173197617
LOSS train 0.22409523344772347 valid 0.2020891172610022
LOSS train 0.22409523344772347 valid 0.20207687703146276
LOSS train 0.22409523344772347 valid 0.2021405635114084
LOSS train 0.22409523344772347 valid 0.20215477664833484
LOSS train 0.22409523344772347 valid 0.2021085635572672
LOSS train 0.22409523344772347 valid 0.20202840540397207
LOSS train 0.22409523344772347 valid 0.2020450560314371
LOSS train 0.22409523344772347 valid 0.20199477591431966
LOSS train 0.22409523344772347 valid 0.20200538632803058
LOSS train 0.22409523344772347 valid 0.20188245155283663
LOSS train 0.22409523344772347 valid 0.20217856567572146
LOSS train 0.22409523344772347 valid 0.2021426653871707
LOSS train 0.22409523344772347 valid 0.20217029975315967
LOSS train 0.22409523344772347 valid 0.2021693809856103
LOSS train 0.22409523344772347 valid 0.20213852618490497
LOSS train 0.22409523344772347 valid 0.20215801801233046
LOSS train 0.22409523344772347 valid 0.20208911194155613
LOSS train 0.22409523344772347 valid 0.2021629782482839
LOSS train 0.22409523344772347 valid 0.20209021930387067
LOSS train 0.22409523344772347 valid 0.20205584823612183
LOSS train 0.22409523344772347 valid 0.20208772439273853
LOSS train 0.22409523344772347 valid 0.20211024979593625
LOSS train 0.22409523344772347 valid 0.2020725352256178
LOSS train 0.22409523344772347 valid 0.20206881534062957
LOSS train 0.22409523344772347 valid 0.20192277731839567
LOSS train 0.22409523344772347 valid 0.20205262848799846
LOSS train 0.22409523344772347 valid 0.20208562565793903
LOSS train 0.22409523344772347 valid 0.20196035128352074
LOSS train 0.22409523344772347 valid 0.20191450667694028
LOSS train 0.22409523344772347 valid 0.20177117235385456
LOSS train 0.22409523344772347 valid 0.20174603861259535
LOSS train 0.22409523344772347 valid 0.2016981447554145
LOSS train 0.22409523344772347 valid 0.20172201735308257
LOSS train 0.22409523344772347 valid 0.20172346926840606
LOSS train 0.22409523344772347 valid 0.20176611310153297
LOSS train 0.22409523344772347 valid 0.20171053354203522
LOSS train 0.22409523344772347 valid 0.2016814932990146
LOSS train 0.22409523344772347 valid 0.2018265831354144
LOSS train 0.22409523344772347 valid 0.20174262674893448
LOSS train 0.22409523344772347 valid 0.2016764579201812
LOSS train 0.22409523344772347 valid 0.2015847071694831
LOSS train 0.22409523344772347 valid 0.20172498481298412
LOSS train 0.22409523344772347 valid 0.20172481364487896
LOSS train 0.22409523344772347 valid 0.20165490486540977
LOSS train 0.22409523344772347 valid 0.20178138725459577
LOSS train 0.22409523344772347 valid 0.2017227048354764
LOSS train 0.22409523344772347 valid 0.20171361213974787
LOSS train 0.22409523344772347 valid 0.2015784482088798
LOSS train 0.22409523344772347 valid 0.20169226740768484
LOSS train 0.22409523344772347 valid 0.20188553689614586
LOSS train 0.22409523344772347 valid 0.20185133472429534
LOSS train 0.22409523344772347 valid 0.2017219855264903
LOSS train 0.22409523344772347 valid 0.20188258402049541
LOSS train 0.22409523344772347 valid 0.20186357693293033
LOSS train 0.22409523344772347 valid 0.20191347375512123
LOSS train 0.22409523344772347 valid 0.2019763489776527
LOSS train 0.22409523344772347 valid 0.20204089816913687
LOSS train 0.22409523344772347 valid 0.20208993535119482
LOSS train 0.22409523344772347 valid 0.2020696191759096
LOSS train 0.22409523344772347 valid 0.201918244382865
LOSS train 0.22409523344772347 valid 0.20194223232232453
LOSS train 0.22409523344772347 valid 0.20198892772782082
LOSS train 0.22409523344772347 valid 0.20190290338749992
LOSS train 0.22409523344772347 valid 0.20186115230607454
LOSS train 0.22409523344772347 valid 0.20209258080770573
LOSS train 0.22409523344772347 valid 0.20203166991637353
LOSS train 0.22409523344772347 valid 0.20203965208724717
LOSS train 0.22409523344772347 valid 0.20208284191109918
LOSS train 0.22409523344772347 valid 0.20197663274030764
LOSS train 0.22409523344772347 valid 0.2020818129590113
LOSS train 0.22409523344772347 valid 0.20207677274698116
LOSS train 0.22409523344772347 valid 0.201989817623384
LOSS train 0.22409523344772347 valid 0.20189293556968155
LOSS train 0.22409523344772347 valid 0.20186972688772492
Training bichrom
DEVICE = cpu
####################
Total Parameters = 606342
Total Trainable Parameters = 1157
bimodal_network(
  (base_model): bichrom_seq(
    (conv1d): Conv1d(4, 256, kernel_size=(24,), stride=(1,))
    (relu): ReLU()
    (batchNorm1d): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (maxPool1d): MaxPool1d(kernel_size=15, stride=15, padding=0, dilation=1, ceil_mode=True)
    (lstm): LSTM(256, 32, batch_first=True)
    (tanh): Tanh()
    (model_dense_repeat): Sequential(
      (0): Linear(in_features=32, out_features=512, bias=True)
      (1): ReLU()
      (2): Dropout(p=0.5, inplace=False)
      (3): Linear(in_features=512, out_features=512, bias=True)
      (4): ReLU()
      (5): Dropout(p=0.5, inplace=False)
      (6): Linear(in_features=512, out_features=512, bias=True)
      (7): ReLU()
      (8): Dropout(p=0.5, inplace=False)
    )
    (linear): Linear(in_features=512, out_features=1, bias=True)
    (sigmoid): Sigmoid()
  )
  (linear): Linear(in_features=512, out_features=1, bias=True)
  (tanh): Tanh()
  (model): bichrom_chrom(
    (_reshape): _reshape()
    (conv1d): Conv1d(12, 15, kernel_size=(1,), stride=(1,), padding=valid)
    (relu): ReLU()
    (lstm): LSTM(15, 5, batch_first=True)
    (relu2): ReLU()
    (linear): Linear(in_features=5, out_features=1, bias=True)
    (tanh): Tanh()
  )
  (linear2): Linear(in_features=2, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)
base_model.conv1d.weight False
base_model.conv1d.bias False
base_model.batchNorm1d.weight False
base_model.batchNorm1d.bias False
base_model.lstm.weight_ih_l0 False
base_model.lstm.weight_hh_l0 False
base_model.lstm.bias_ih_l0 False
base_model.lstm.bias_hh_l0 False
base_model.model_dense_repeat.0.weight False
base_model.model_dense_repeat.0.bias False
base_model.model_dense_repeat.3.weight False
base_model.model_dense_repeat.3.bias False
base_model.model_dense_repeat.6.weight False
base_model.model_dense_repeat.6.bias False
base_model.linear.weight False
base_model.linear.bias False
linear.weight True
linear.bias True
model.conv1d.weight True
model.conv1d.bias True
model.lstm.weight_ih_l0 True
model.lstm.weight_hh_l0 True
model.lstm.bias_ih_l0 True
model.lstm.bias_hh_l0 True
model.linear.weight True
model.linear.bias True
linear2.weight True
linear2.bias True
####################
Epochs = 30
EPOCH 1:
  batch 1 loss: 0.7508410811424255
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.7405425608158112
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.7344440420468649
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.7280238717794418
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.719550347328186
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.713010827700297
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.7108935032572065
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.707674577832222
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.7041931019888984
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.7014814257621765
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.6989440050992098
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.696246325969696
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.6939103786761944
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.6909491419792175
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.6874127388000488
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.6848278194665909
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.6810100674629211
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.6777443852689531
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.6742576266589918
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.6702662110328674
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.6663243089403424
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.662992230870507
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.6590110944664996
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.655197262763977
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.6514555454254151
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.6480622474963849
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.644392622841729
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.6406173131295613
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.6368677390032801
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.6330320497353872
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.6289703057658288
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.6248721331357956
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.6206977665424347
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.6168281646335826
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.6129764556884766
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.6091275753246413
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.6054530296776746
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.6012925811504063
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.5974056055912604
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.5935245908796787
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.5896730735534574
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.5858004377001808
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.5820780209330625
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.5784598765048113
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.5747507830460866
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.5712590632231339
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.5682041182162914
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.5647591159989437
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.5612759468506794
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.5580281299352646
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.5548024878782385
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.5512242557910773
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.5479871248299221
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.544826137246909
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.5415296321565455
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.5386918618210724
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.5358701039824569
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.5327793889004608
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.530125781136044
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.5274133120973905
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.5248797061990519
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.5221117898341148
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.5192195186539302
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.5166246714070439
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.5138476032477158
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.5111555457115173
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.5089688114265898
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.5062630570986691
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.5036119607047759
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.5011740033115659
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.49877723211973485
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.49647324573662543
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.4944966970241233
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.4923137426376343
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.4900850749015808
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.4882753875694777
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.4861292974515395
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.48402948601123613
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.4820889807954619
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.48010813966393473
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.4782065572562041
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.4761374625490933
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.4742770866457238
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.47231990098953247
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.47055419508148644
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.4685676787481752
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.46670822131222695
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.464895524084568
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.4631997896044442
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.46140248841709564
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.45954324092183796
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.4579827354654022
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.4561546563461263
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.45448978498895115
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.45250115990638734
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.45078489774217206
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.44941440193923476
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.4480590282046065
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.446562943735508
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.4449173772335053
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.4434638784663512
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.44191087811600926
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.44048559463139875
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.439336771861865
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.43767361101650054
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.43652680628704577
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.4351409267042285
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.4339132066126223
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.4325680798346843
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.4314211590723558
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.4297845817364014
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.4285596164741686
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.42701624215176676
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.42598581601653185
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.4247597590736721
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.4235349068867749
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.4223333778034927
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.42088574171066284
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.41986068206674915
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.4185873640080293
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.41747117953852186
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.41651754716380696
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.41528618335723877
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.4141695064402396
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.4133505988121033
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.41224730677074856
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.41125961715780845
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.41011737170629203
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.4089398340199345
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.40774322220912346
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.4066827629358714
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.4055853919549422
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.40471364905063373
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.4036737493614652
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.4027519053883023
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.40178025491973934
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.400752526782725
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.39971919897673786
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.39873754806655776
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.39774468605007446
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.3966957036907791
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.39568357400491205
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.39494482662294295
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.39382840651604867
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.39312843084335325
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.39211052125447415
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.39105422764408343
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.39016191178076975
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.3893365085925032
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.3881936236222585
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.3874164612482715
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.38650816326078613
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.385777301258511
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.38508753517231387
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.38412719955367425
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.38308598129795146
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.382124222672669
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.3813220827451235
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.3806180312011227
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.37995869098231194
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.3794746596065367
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.3787373572036072
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.3780377663534843
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.37733654741470407
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.3764504675612305
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.3757899759943227
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.37508230750075355
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.37423863873950075
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.373453344113728
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.37289254349820755
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.3722301109840995
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.37142043967926225
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.37058966800656623
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.36996888257991306
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.36941981383732386
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.36884313889525155
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.3683992982920954
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.3676914519975694
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.36711699028587874
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.36645504145158664
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.365726375448111
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.3651808202266693
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.36450963707569517
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.3638488374974417
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.3631948467041995
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.36261065248199686
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.36220242688681353
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.36166475467542386
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.361118082410444
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.36051341569737383
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.3599104355455069
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.35962785438944894
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.35906078685750614
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.3585491063668556
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.35800302288471125
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.35750096504177364
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.35680318680511514
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.35632385313510895
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.3558533497192153
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.355404092669487
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.35494791734870984
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.35436577932669383
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.3538660941452816
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.3533567066724394
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.3527121950213502
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.35226852692736005
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.3517271166550364
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.3510068108399327
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.3506360784814689
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.35023051833822616
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.34988564937883077
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.34950397690793256
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.3490377487571027
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.3484913172008835
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.34794990351033767
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.347403314102579
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.3468025328788889
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.3463824107696157
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.3460222873921808
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.3454602588984099
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.34491663557641644
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.3444233917975211
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.34400837825018193
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.34352958322103533
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.343186803261439
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.34265672059449476
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.3421228379667593
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.34150745680457667
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.3410900947048154
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.34073514951312023
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.34035528635049794
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.34000190296049776
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.33945162607109086
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.33905905574305445
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.33856614396927204
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.33817516665084885
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.33777269777617874
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.3372956077341272
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.3367137591584457
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.33629363055030503
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.33587897800805655
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.3354032851201444
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.3350985972479047
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.33465072048492117
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.33426648822365973
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.3337397727418721
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.33337498785030506
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.3329993977902397
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.3325126134008768
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.33214173674583436
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.3317631645744065
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.3314241325216634
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.3310926872868783
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.330737250409727
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.33036596716619004
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.33000742678996176
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.32956062094942606
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.3292206285535827
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.32899474108081067
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.32855110558179706
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.3281532162336554
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.3277310854151049
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.32727983491955603
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.3268643543124199
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.32657158206093984
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.32610683019896197
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.3258780347050799
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.32564998962985936
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.32519611561165424
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.3248433195882373
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.3245979095517049
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.3242362590835375
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.3238246644482071
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.323465457145315
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.3230122018402273
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.3227238737694595
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.32262623627478465
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.3221791641210481
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.3218877048475341
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.3215970894587891
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.32126222068304694
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.32095966356020444
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.3206597376522664
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.32040239036293094
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.3201696211308764
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.3198783414868208
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.31969399376405655
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.3192794025979108
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.3189443084814144
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.318709596991539
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.31852372108456195
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.3181444026835977
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.3179248019086623
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.31759033772815654
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.31730211660013363
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.3169764116205074
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.31666104169405673
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.3163548469443449
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.3160834283035336
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.315754762639602
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.31542909828531385
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.3151047598841964
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.3147888852916535
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.3144782547789969
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.31427992978056923
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.31390733622452793
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.31354146472600075
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.3132213038670552
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.31289181759442325
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.3124628834186062
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.3120867877336177
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.3117940378112671
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.31148667205065583
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.3111599020327732
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.3110452027547927
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.31079881506252893
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.3105224831623234
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.31018320321099563
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.30983118334534027
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.3095348786562681
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.3091914842711802
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.30895066196503845
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.3086627247532824
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.30832654854029784
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.3081113839149475
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.3078333127992285
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.30754995327841617
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.30731166581191666
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.3070948718619564
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.3067808428948576
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.30645991033477726
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.30627598146717233
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.30598688443322797
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.3057584019299753
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.3055579119653844
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.30540810454459416
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.3051115346945358
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.30482664990883607
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.30449265448217194
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.3043221769525724
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.3040601105983656
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.303835513255401
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.303550734420907
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.30326142902810904
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.303066127801287
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.30277411721517583
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.30261008909525033
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.30242636538613804
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.30218717067323647
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.30200638060058865
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.3017979094021001
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.3015188006684184
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.30131436998378114
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.3011125742042132
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.3007903772340694
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.3004960740885038
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.30020403473818
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.29993559470056824
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.2996290913863434
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.2993767995801237
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.29908335989036716
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.2988129016526496
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.2985719805249826
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.29832354883898743
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.2981120280615271
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.2979450052797469
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.2976467667467263
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.2974515331747091
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.2972456759149789
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.29709586513203545
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.2968457914748282
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.2966288820069323
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.2963926002742778
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.296146648851308
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.2959119616349538
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.2956489195373464
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.2955542980675988
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.29530128600105404
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.2951110128834254
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.2949549986343635
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.2946880836030004
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.294485240628582
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.2942737894954632
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.2940539449530964
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.29379220136574336
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.29362697756506617
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.2933225623627966
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.29317599001158146
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.2929028334301971
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.2927502514078067
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.29257233585695475
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.29238144023229884
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.29212887425914064
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.29189433618850513
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.2916798340746119
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.2915011257911571
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.29126649051529335
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.2910010487065842
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.29079526741253703
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.2905595051497221
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.29043229637746504
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.2902480042840711
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.2900154953337189
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.2897957794294499
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.2895761369187155
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.28936070886445164
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.28911496944158027
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.2888211020286761
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.2886197011016109
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.2884471654891968
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.288259005930882
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.28805819645668695
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.2877888721718338
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.2874974809622995
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.2872852651469679
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.28704873386483926
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.2868219366033586
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.2866322458075564
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.2864486380237953
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.28616884443021956
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.2859364884624572
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.2857778724900919
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.28554054985514205
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.28536214791941195
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.28516350167639115
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.28491350389282466
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.28469747584095045
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.2844586880686127
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.2842264981606068
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.28406088272499486
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.2838337411390933
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.2836545882746577
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.28347871998028173
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.2833072030873892
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.28311260822175566
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.28297486497994956
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.28278577382286296
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.2826265248397714
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.282365296621529
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.2821840323169123
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.28203234316405257
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.2818256478368966
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.28162464153013167
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.2814064433647169
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.28119305462649696
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.28099428691938855
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.2807939749969465
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.28059398575818967
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.2803886815540509
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.28017235918177497
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.27999146322321206
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.2797788584983982
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.2795691719118333
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.27932691331214315
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.2790897179435898
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.2788929583079982
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.278651172633505
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.27839625786225347
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.2781627266495316
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.27796330150702725
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.2777823039417412
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.27755155653019487
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.2773421580217312
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.27718323790307703
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.27697390314712317
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.276824822145726
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.27666420953814935
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.2764581858029223
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.27626645695298974
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.27601634558845073
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.2758728202487759
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.2756298254480806
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.2756298254480806 valid 0.226681649684906
LOSS train 0.2756298254480806 valid 0.22864283621311188
LOSS train 0.2756298254480806 valid 0.23613219459851584
LOSS train 0.2756298254480806 valid 0.2253619320690632
LOSS train 0.2756298254480806 valid 0.2308410495519638
LOSS train 0.2756298254480806 valid 0.2366937721769015
LOSS train 0.2756298254480806 valid 0.23285846837929317
LOSS train 0.2756298254480806 valid 0.2319980263710022
LOSS train 0.2756298254480806 valid 0.22902098298072815
LOSS train 0.2756298254480806 valid 0.22944561392068863
LOSS train 0.2756298254480806 valid 0.22855284403670917
LOSS train 0.2756298254480806 valid 0.2328734534482161
LOSS train 0.2756298254480806 valid 0.2323645307467534
LOSS train 0.2756298254480806 valid 0.2312565499118396
LOSS train 0.2756298254480806 valid 0.23080816864967346
LOSS train 0.2756298254480806 valid 0.23325459472835064
LOSS train 0.2756298254480806 valid 0.2347699018085704
LOSS train 0.2756298254480806 valid 0.23562976386811998
LOSS train 0.2756298254480806 valid 0.23726756321756462
LOSS train 0.2756298254480806 valid 0.2361036442220211
LOSS train 0.2756298254480806 valid 0.23779151198409854
LOSS train 0.2756298254480806 valid 0.23751891946250742
LOSS train 0.2756298254480806 valid 0.23603988924752112
LOSS train 0.2756298254480806 valid 0.23697232889632383
LOSS train 0.2756298254480806 valid 0.23647732615470887
LOSS train 0.2756298254480806 valid 0.23560790889538252
LOSS train 0.2756298254480806 valid 0.23551260486797052
LOSS train 0.2756298254480806 valid 0.23603817980204309
LOSS train 0.2756298254480806 valid 0.23446704550036068
LOSS train 0.2756298254480806 valid 0.23455991595983505
LOSS train 0.2756298254480806 valid 0.234715947701085
LOSS train 0.2756298254480806 valid 0.23518990026786923
LOSS train 0.2756298254480806 valid 0.2347914603623477
LOSS train 0.2756298254480806 valid 0.23421360727618723
LOSS train 0.2756298254480806 valid 0.2346830087048667
LOSS train 0.2756298254480806 valid 0.23522389349010256
LOSS train 0.2756298254480806 valid 0.23569143865559553
LOSS train 0.2756298254480806 valid 0.23557658924868233
LOSS train 0.2756298254480806 valid 0.23633578075812414
LOSS train 0.2756298254480806 valid 0.2360324364155531
LOSS train 0.2756298254480806 valid 0.2361314413024158
LOSS train 0.2756298254480806 valid 0.2375055636678423
LOSS train 0.2756298254480806 valid 0.23848047228746636
LOSS train 0.2756298254480806 valid 0.23779731345447627
LOSS train 0.2756298254480806 valid 0.23740026983949875
LOSS train 0.2756298254480806 valid 0.23663932992064435
LOSS train 0.2756298254480806 valid 0.23643689713579544
LOSS train 0.2756298254480806 valid 0.23770501464605331
LOSS train 0.2756298254480806 valid 0.23701380558159887
LOSS train 0.2756298254480806 valid 0.23702431172132493
LOSS train 0.2756298254480806 valid 0.23644173174512154
LOSS train 0.2756298254480806 valid 0.23641914129257202
LOSS train 0.2756298254480806 valid 0.23751907641032957
LOSS train 0.2756298254480806 valid 0.23771771843786593
LOSS train 0.2756298254480806 valid 0.2375630259513855
LOSS train 0.2756298254480806 valid 0.23739236806120192
LOSS train 0.2756298254480806 valid 0.23687186471202917
LOSS train 0.2756298254480806 valid 0.23724919403421468
LOSS train 0.2756298254480806 valid 0.23710000919083418
LOSS train 0.2756298254480806 valid 0.23676389927665392
LOSS train 0.2756298254480806 valid 0.23683251831375185
LOSS train 0.2756298254480806 valid 0.2365529448274643
LOSS train 0.2756298254480806 valid 0.23638244824750082
LOSS train 0.2756298254480806 valid 0.23636681819334626
LOSS train 0.2756298254480806 valid 0.23550394085737375
LOSS train 0.2756298254480806 valid 0.23552167776859168
LOSS train 0.2756298254480806 valid 0.23595807756950607
LOSS train 0.2756298254480806 valid 0.23529556437450297
LOSS train 0.2756298254480806 valid 0.23587976028953772
LOSS train 0.2756298254480806 valid 0.2361626582486289
LOSS train 0.2756298254480806 valid 0.2361476245060773
LOSS train 0.2756298254480806 valid 0.23668427020311356
LOSS train 0.2756298254480806 valid 0.23700737136684052
LOSS train 0.2756298254480806 valid 0.23676223468941612
LOSS train 0.2756298254480806 valid 0.23650770843029023
LOSS train 0.2756298254480806 valid 0.23635921842957797
LOSS train 0.2756298254480806 valid 0.23648076881835986
LOSS train 0.2756298254480806 valid 0.23636330415805182
LOSS train 0.2756298254480806 valid 0.23650102713440038
LOSS train 0.2756298254480806 valid 0.2360693344846368
LOSS train 0.2756298254480806 valid 0.23629282819636074
LOSS train 0.2756298254480806 valid 0.23641196929100083
LOSS train 0.2756298254480806 valid 0.23656442151012191
LOSS train 0.2756298254480806 valid 0.23642928348410697
LOSS train 0.2756298254480806 valid 0.23682379740125992
LOSS train 0.2756298254480806 valid 0.23673062390366265
LOSS train 0.2756298254480806 valid 0.2363810576926703
LOSS train 0.2756298254480806 valid 0.23649229210886089
LOSS train 0.2756298254480806 valid 0.2368350789118349
LOSS train 0.2756298254480806 valid 0.2368550572130415
LOSS train 0.2756298254480806 valid 0.23693038752445808
LOSS train 0.2756298254480806 valid 0.2369003010832745
LOSS train 0.2756298254480806 valid 0.23704271579301486
LOSS train 0.2756298254480806 valid 0.2370505026997404
LOSS train 0.2756298254480806 valid 0.23730223790595406
LOSS train 0.2756298254480806 valid 0.23781095740074912
LOSS train 0.2756298254480806 valid 0.2378762270679179
LOSS train 0.2756298254480806 valid 0.2382045514425453
LOSS train 0.2756298254480806 valid 0.23827717265095374
LOSS train 0.2756298254480806 valid 0.23848474636673928
LOSS train 0.2756298254480806 valid 0.23849981447847762
LOSS train 0.2756298254480806 valid 0.23881922735303057
LOSS train 0.2756298254480806 valid 0.23858458932163645
LOSS train 0.2756298254480806 valid 0.23839975922153547
LOSS train 0.2756298254480806 valid 0.2385564562820253
LOSS train 0.2756298254480806 valid 0.23844983091331878
LOSS train 0.2756298254480806 valid 0.23815148456074367
LOSS train 0.2756298254480806 valid 0.2382460942974797
LOSS train 0.2756298254480806 valid 0.23792920856300845
LOSS train 0.2756298254480806 valid 0.2379528823223981
LOSS train 0.2756298254480806 valid 0.23819862400089298
LOSS train 0.2756298254480806 valid 0.2384721857628652
LOSS train 0.2756298254480806 valid 0.23836105903693006
LOSS train 0.2756298254480806 valid 0.23844655211034574
LOSS train 0.2756298254480806 valid 0.23896563584389893
LOSS train 0.2756298254480806 valid 0.23870820603493986
LOSS train 0.2756298254480806 valid 0.2389937445648715
LOSS train 0.2756298254480806 valid 0.23884285102456304
LOSS train 0.2756298254480806 valid 0.23852618752407426
LOSS train 0.2756298254480806 valid 0.2383541570355495
LOSS train 0.2756298254480806 valid 0.2382602787707463
LOSS train 0.2756298254480806 valid 0.23854314108363917
LOSS train 0.2756298254480806 valid 0.23856103311224683
LOSS train 0.2756298254480806 valid 0.23870856831631354
LOSS train 0.2756298254480806 valid 0.23865596640110015
LOSS train 0.2756298254480806 valid 0.2388277549355749
LOSS train 0.2756298254480806 valid 0.23876313501455654
LOSS train 0.2756298254480806 valid 0.23871081811375916
LOSS train 0.2756298254480806 valid 0.23846095049566077
LOSS train 0.2756298254480806 valid 0.23819223917447604
LOSS train 0.2756298254480806 valid 0.23830142412476868
LOSS train 0.2756298254480806 valid 0.23837783074740207
LOSS train 0.2756298254480806 valid 0.23844525531718605
LOSS train 0.2756298254480806 valid 0.2387095287219802
LOSS train 0.2756298254480806 valid 0.23877031527183673
LOSS train 0.2756298254480806 valid 0.238776193383862
LOSS train 0.2756298254480806 valid 0.2387745866592783
LOSS train 0.2756298254480806 valid 0.2385587198794752
LOSS train 0.2756298254480806 valid 0.23841812402653179
LOSS train 0.2756298254480806 valid 0.23827229120901652
LOSS train 0.2756298254480806 valid 0.23833658234447452
LOSS train 0.2756298254480806 valid 0.2387085307232091
LOSS train 0.2756298254480806 valid 0.2387892818534291
LOSS train 0.2756298254480806 valid 0.2387664539532529
LOSS train 0.2756298254480806 valid 0.2386167892094316
LOSS train 0.2756298254480806 valid 0.23860901563543163
LOSS train 0.2756298254480806 valid 0.23859390231216845
LOSS train 0.2756298254480806 valid 0.2384254364548503
LOSS train 0.2756298254480806 valid 0.23849996094335646
LOSS train 0.2756298254480806 valid 0.23842485268910726
LOSS train 0.2756298254480806 valid 0.23832191102552097
LOSS train 0.2756298254480806 valid 0.23821932498953843
LOSS train 0.2756298254480806 valid 0.23798910406679888
LOSS train 0.2756298254480806 valid 0.2380313961343332
LOSS train 0.2756298254480806 valid 0.23799831290398873
LOSS train 0.2756298254480806 valid 0.2380784973502159
LOSS train 0.2756298254480806 valid 0.23783351490452032
LOSS train 0.2756298254480806 valid 0.2378238426356376
LOSS train 0.2756298254480806 valid 0.23808418860975303
LOSS train 0.2756298254480806 valid 0.23815193865448236
LOSS train 0.2756298254480806 valid 0.23805778015474355
LOSS train 0.2756298254480806 valid 0.23800902252579914
LOSS train 0.2756298254480806 valid 0.2377511881794666
LOSS train 0.2756298254480806 valid 0.23772899115957866
LOSS train 0.2756298254480806 valid 0.23782749699823785
LOSS train 0.2756298254480806 valid 0.23771935342306114
LOSS train 0.2756298254480806 valid 0.23770730804183526
LOSS train 0.2756298254480806 valid 0.23763723662566572
LOSS train 0.2756298254480806 valid 0.23748251257916175
LOSS train 0.2756298254480806 valid 0.23758034364265554
LOSS train 0.2756298254480806 valid 0.23747621006087252
LOSS train 0.2756298254480806 valid 0.23758645901499792
LOSS train 0.2756298254480806 valid 0.23747363178371694
LOSS train 0.2756298254480806 valid 0.23774555008644346
LOSS train 0.2756298254480806 valid 0.2375939177615302
LOSS train 0.2756298254480806 valid 0.23750284508886663
LOSS train 0.2756298254480806 valid 0.23749998018229748
LOSS train 0.2756298254480806 valid 0.23761802384357775
LOSS train 0.2756298254480806 valid 0.2377491687429684
LOSS train 0.2756298254480806 valid 0.2377824986146556
LOSS train 0.2756298254480806 valid 0.23773453314660004
LOSS train 0.2756298254480806 valid 0.2375676718714473
LOSS train 0.2756298254480806 valid 0.23769264794438263
LOSS train 0.2756298254480806 valid 0.23772994995765065
LOSS train 0.2756298254480806 valid 0.23768317071167197
LOSS train 0.2756298254480806 valid 0.23773486071055935
LOSS train 0.2756298254480806 valid 0.23762443048112533
LOSS train 0.2756298254480806 valid 0.23770919354989173
LOSS train 0.2756298254480806 valid 0.2375229250974756
LOSS train 0.2756298254480806 valid 0.23760082196248206
LOSS train 0.2756298254480806 valid 0.23769022961249525
LOSS train 0.2756298254480806 valid 0.2378244521872451
LOSS train 0.2756298254480806 valid 0.23760683996689752
LOSS train 0.2756298254480806 valid 0.23757578755162426
LOSS train 0.2756298254480806 valid 0.23750169888520853
LOSS train 0.2756298254480806 valid 0.2376988987837519
LOSS train 0.2756298254480806 valid 0.2376787914388676
LOSS train 0.2756298254480806 valid 0.23788914949905993
LOSS train 0.2756298254480806 valid 0.23789129028068715
LOSS train 0.2756298254480806 valid 0.23806326083838938
LOSS train 0.2756298254480806 valid 0.23787727840800785
LOSS train 0.2756298254480806 valid 0.2379059205846031
LOSS train 0.2756298254480806 valid 0.23794226561273848
LOSS train 0.2756298254480806 valid 0.23782206786906018
LOSS train 0.2756298254480806 valid 0.2377397673159111
LOSS train 0.2756298254480806 valid 0.23768735329792337
LOSS train 0.2756298254480806 valid 0.2376119249684799
LOSS train 0.2756298254480806 valid 0.23743470321194485
LOSS train 0.2756298254480806 valid 0.23751561514783703
LOSS train 0.2756298254480806 valid 0.23765197006009875
LOSS train 0.2756298254480806 valid 0.23788686011922303
LOSS train 0.2756298254480806 valid 0.2376729928238212
LOSS train 0.2756298254480806 valid 0.23756754489292
LOSS train 0.2756298254480806 valid 0.23743647859196795
LOSS train 0.2756298254480806 valid 0.23720851159372994
LOSS train 0.2756298254480806 valid 0.23695779778063297
LOSS train 0.2756298254480806 valid 0.23675435966885036
LOSS train 0.2756298254480806 valid 0.23680072229936583
LOSS train 0.2756298254480806 valid 0.23672000975369317
LOSS train 0.2756298254480806 valid 0.23675049421462147
LOSS train 0.2756298254480806 valid 0.23672349609400892
LOSS train 0.2756298254480806 valid 0.23679727248780363
LOSS train 0.2756298254480806 valid 0.23685468312336191
LOSS train 0.2756298254480806 valid 0.23681786822687304
LOSS train 0.2756298254480806 valid 0.23660533752706317
LOSS train 0.2756298254480806 valid 0.23646370941295033
LOSS train 0.2756298254480806 valid 0.236490565971656
LOSS train 0.2756298254480806 valid 0.2364882407873346
LOSS train 0.2756298254480806 valid 0.23641271817632117
LOSS train 0.2756298254480806 valid 0.2364915254323379
LOSS train 0.2756298254480806 valid 0.23642345817832203
LOSS train 0.2756298254480806 valid 0.23648888855401812
LOSS train 0.2756298254480806 valid 0.23680928765192563
LOSS train 0.2756298254480806 valid 0.23682237913211188
LOSS train 0.2756298254480806 valid 0.23685610414819513
LOSS train 0.2756298254480806 valid 0.23677463147599817
LOSS train 0.2756298254480806 valid 0.23676506841736
LOSS train 0.2756298254480806 valid 0.23668392534766877
LOSS train 0.2756298254480806 valid 0.23672725397423222
LOSS train 0.2756298254480806 valid 0.23633565697818995
LOSS train 0.2756298254480806 valid 0.2361697284388839
LOSS train 0.2756298254480806 valid 0.23606770861247353
LOSS train 0.2756298254480806 valid 0.23605484972274843
LOSS train 0.2756298254480806 valid 0.2360198070890591
LOSS train 0.2756298254480806 valid 0.23624042157007724
LOSS train 0.2756298254480806 valid 0.2363053851617061
LOSS train 0.2756298254480806 valid 0.23634715818683147
LOSS train 0.2756298254480806 valid 0.23634458633680497
LOSS train 0.2756298254480806 valid 0.2362432243474516
LOSS train 0.2756298254480806 valid 0.23631060534715653
LOSS train 0.2756298254480806 valid 0.23634304272463597
LOSS train 0.2756298254480806 valid 0.23636489101345576
LOSS train 0.2756298254480806 valid 0.23622112282415625
LOSS train 0.2756298254480806 valid 0.23631297163371964
LOSS train 0.2756298254480806 valid 0.23617097308822707
LOSS train 0.2756298254480806 valid 0.236003210011404
LOSS train 0.2756298254480806 valid 0.23597581209143775
LOSS train 0.2756298254480806 valid 0.23610062602647516
LOSS train 0.2756298254480806 valid 0.23606513365815504
LOSS train 0.2756298254480806 valid 0.2359840117394924
LOSS train 0.2756298254480806 valid 0.2359814016983427
LOSS train 0.2756298254480806 valid 0.23614358765478352
LOSS train 0.2756298254480806 valid 0.2360483068590382
LOSS train 0.2756298254480806 valid 0.2360569495481975
LOSS train 0.2756298254480806 valid 0.23605142685602296
LOSS train 0.2756298254480806 valid 0.23610115180114158
LOSS train 0.2756298254480806 valid 0.23603244734167605
LOSS train 0.2756298254480806 valid 0.23612047453869633
LOSS train 0.2756298254480806 valid 0.23614528686583708
LOSS train 0.2756298254480806 valid 0.23600870028690055
LOSS train 0.2756298254480806 valid 0.2359898435123732
LOSS train 0.2756298254480806 valid 0.23589938040822744
LOSS train 0.2756298254480806 valid 0.2357552593007629
LOSS train 0.2756298254480806 valid 0.23570000668511773
LOSS train 0.2756298254480806 valid 0.23576960585334084
LOSS train 0.2756298254480806 valid 0.23584055317484814
LOSS train 0.2756298254480806 valid 0.23582841477454355
LOSS train 0.2756298254480806 valid 0.23580290580824984
LOSS train 0.2756298254480806 valid 0.23578917307238426
LOSS train 0.2756298254480806 valid 0.2357014545372554
LOSS train 0.2756298254480806 valid 0.2356048092723317
LOSS train 0.2756298254480806 valid 0.23559746922964744
LOSS train 0.2756298254480806 valid 0.23557045144874722
LOSS train 0.2756298254480806 valid 0.23544987800045752
LOSS train 0.2756298254480806 valid 0.23540395111368415
LOSS train 0.2756298254480806 valid 0.235401297157461
LOSS train 0.2756298254480806 valid 0.23548258929302468
LOSS train 0.2756298254480806 valid 0.23544531998534998
LOSS train 0.2756298254480806 valid 0.23538410730634182
LOSS train 0.2756298254480806 valid 0.23528397144942448
LOSS train 0.2756298254480806 valid 0.23522689472563899
LOSS train 0.2756298254480806 valid 0.2351332538544315
LOSS train 0.2756298254480806 valid 0.23510544525885338
LOSS train 0.2756298254480806 valid 0.23514423179788654
LOSS train 0.2756298254480806 valid 0.23518736584711883
LOSS train 0.2756298254480806 valid 0.2351140278215344
LOSS train 0.2756298254480806 valid 0.23505811894983555
LOSS train 0.2756298254480806 valid 0.23507804408569463
LOSS train 0.2756298254480806 valid 0.23507592704981864
LOSS train 0.2756298254480806 valid 0.23506102512280147
LOSS train 0.2756298254480806 valid 0.23499010815176852
LOSS train 0.2756298254480806 valid 0.2349114360300121
LOSS train 0.2756298254480806 valid 0.23488773199001162
LOSS train 0.2756298254480806 valid 0.23487136120858945
LOSS train 0.2756298254480806 valid 0.23480475901580247
LOSS train 0.2756298254480806 valid 0.23497161926592097
LOSS train 0.2756298254480806 valid 0.2350051406443313
LOSS train 0.2756298254480806 valid 0.235081574794921
LOSS train 0.2756298254480806 valid 0.23505722584269192
LOSS train 0.2756298254480806 valid 0.23506126572047512
LOSS train 0.2756298254480806 valid 0.23513682819064408
LOSS train 0.2756298254480806 valid 0.2350723189421189
LOSS train 0.2756298254480806 valid 0.23505289409869015
LOSS train 0.2756298254480806 valid 0.23494462256029153
LOSS train 0.2756298254480806 valid 0.23485338049275534
LOSS train 0.2756298254480806 valid 0.2348135782853712
LOSS train 0.2756298254480806 valid 0.23486542057727788
LOSS train 0.2756298254480806 valid 0.23489918611334554
LOSS train 0.2756298254480806 valid 0.23488094786117816
LOSS train 0.2756298254480806 valid 0.23475054036825896
LOSS train 0.2756298254480806 valid 0.23478786610182945
LOSS train 0.2756298254480806 valid 0.23472987124638528
LOSS train 0.2756298254480806 valid 0.2346114266213987
LOSS train 0.2756298254480806 valid 0.23458938875500066
LOSS train 0.2756298254480806 valid 0.23443954756626717
LOSS train 0.2756298254480806 valid 0.23455347253318212
LOSS train 0.2756298254480806 valid 0.23445917538364364
LOSS train 0.2756298254480806 valid 0.23445862681582208
LOSS train 0.2756298254480806 valid 0.2344702527515794
LOSS train 0.2756298254480806 valid 0.234519214702375
LOSS train 0.2756298254480806 valid 0.23446237073023873
LOSS train 0.2756298254480806 valid 0.23444438416017108
LOSS train 0.2756298254480806 valid 0.23450180212477664
LOSS train 0.2756298254480806 valid 0.2344295115260307
LOSS train 0.2756298254480806 valid 0.2342779272972648
LOSS train 0.2756298254480806 valid 0.23421600904493106
LOSS train 0.2756298254480806 valid 0.23433101424831312
LOSS train 0.2756298254480806 valid 0.23428738796146664
LOSS train 0.2756298254480806 valid 0.23424949538742898
LOSS train 0.2756298254480806 valid 0.2343378428150626
LOSS train 0.2756298254480806 valid 0.23430864110894917
LOSS train 0.2756298254480806 valid 0.23429448632468955
LOSS train 0.2756298254480806 valid 0.23417477261915848
LOSS train 0.2756298254480806 valid 0.23424413479691328
LOSS train 0.2756298254480806 valid 0.23432131919307986
LOSS train 0.2756298254480806 valid 0.23428138939803736
LOSS train 0.2756298254480806 valid 0.23421946945211042
LOSS train 0.2756298254480806 valid 0.23418788778884658
LOSS train 0.2756298254480806 valid 0.2341789581884969
LOSS train 0.2756298254480806 valid 0.23417625252689633
LOSS train 0.2756298254480806 valid 0.23424733130850345
LOSS train 0.2756298254480806 valid 0.23429573627866127
LOSS train 0.2756298254480806 valid 0.23435409535936189
LOSS train 0.2756298254480806 valid 0.23435389785106572
LOSS train 0.2756298254480806 valid 0.2342494269491921
LOSS train 0.2756298254480806 valid 0.23425178460023377
LOSS train 0.2756298254480806 valid 0.234194009327421
LOSS train 0.2756298254480806 valid 0.23416817154964256
LOSS train 0.2756298254480806 valid 0.23410758035760737
LOSS train 0.2756298254480806 valid 0.23427582499053742
LOSS train 0.2756298254480806 valid 0.23417723740236912
LOSS train 0.2756298254480806 valid 0.23424046383707564
LOSS train 0.2756298254480806 valid 0.23424909914492248
LOSS train 0.2756298254480806 valid 0.23417705940185013
LOSS train 0.2756298254480806 valid 0.23425485601980392
LOSS train 0.2756298254480806 valid 0.23421654736246567
LOSS train 0.2756298254480806 valid 0.23420337650529047
LOSS train 0.2756298254480806 valid 0.23412885196992886
LOSS train 0.2756298254480806 valid 0.2341475673322755
EPOCH 2:
  batch 1 loss: 0.20198902487754822
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.18879669159650803
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.18296009798844656
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.1817486472427845
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.1805259555578232
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.18306728452444077
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.18456056714057922
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.1817338466644287
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.18229695161183676
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.1803160235285759
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.18137056583707983
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.1838849720855554
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.1827835039450572
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.18091412207909993
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.1802544116973877
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.18179847486317158
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.18203785314279444
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.1827440998620457
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.18316964177708878
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.18193135038018227
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.18126268401032403
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.18171910806135697
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.18002284544965494
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.1795712218930324
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.17977214932441712
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.18200449989392206
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.18267475234137642
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.18325442661132132
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.18423927035825005
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.1845961555838585
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.18406498864773782
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.18322197534143925
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.1833587641065771
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.1835811593953301
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.18363516671316965
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.18400305343998802
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.18436652421951294
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.18358358074175685
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.1828842892860755
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.18268361128866673
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.18216545152954938
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.18193545653706505
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.18183364355286888
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.18194036016410048
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.18170020050472682
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.18156962744567706
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.18205698690515884
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.18201994833846888
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.18194364558677284
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.18195302575826644
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.18171924907787174
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.18082376540853426
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.180558887292754
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.18030307828276246
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.17992271320386366
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.1800860853067466
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.18041966228108658
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.18002646621958962
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.1805217925269725
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.18080964758992196
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.18120896767397396
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.18118288512191466
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.18090636980912042
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.18092963052913547
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.18071343096402975
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.18053081112377573
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.18082671067607936
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.18058022593750672
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.1802079990722131
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.18010050037077496
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.1800539753806423
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.17988208971089786
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.18018971750997517
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.18036427671039426
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.18011999348799387
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.18032926752379067
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.18051731450991196
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.1804371892641752
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.1805899729834327
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.18060130644589661
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.1806840541554086
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.18056723166529726
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.18048629667385516
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.18038636357301757
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.18044629552785088
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.1801586622415587
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.18000652701005168
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.17983726276592774
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.17985118054941798
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.17972850932015313
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.17961338096922572
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.1796179603299369
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.1794415537708549
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.17939716022699437
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.17901962148515801
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.17896503706773123
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.17893135670534113
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.17908302253606367
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.1791718794841959
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.17891312271356583
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.1789475810704845
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.17884794374306998
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.17889242091225188
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.17918597677579293
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.17892462071918305
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.17897862280314825
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.1790137126624027
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.17899332029951942
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.17896029079725984
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.1790376913818446
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.17863774903722712
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.1786097338689225
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.1781957314341469
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.17836749409897285
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.17832084673902263
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.1783023014150817
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.1783263573789189
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.1780319029496888
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.17814403296518727
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.1779642262806495
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.17794224188840094
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.17801622545621434
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.17787647138281568
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.1778905020365792
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.17822237181663514
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.17814328001132088
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.17827651221451796
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.17812631139531732
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.17784372780674187
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.17772503793239594
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.17762536379217192
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.1774125876977588
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.17745805985497354
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.177353453947537
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.17728686818370112
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.17719603746252902
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.17713213314975265
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.1769763563854107
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.17697097939981832
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.17676503381558828
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.17660705914311375
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.17653780629937077
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.17667780493522858
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.17644517868757248
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.1765607749593669
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.1765139053945672
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.1762771477707389
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.17617134397497047
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.17613154889753202
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.1758306677142779
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.1758211956711005
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.17578806149724283
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.1758001642484291
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.17573844128614896
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.17551075508517602
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.17522345340022674
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.17510203751409129
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.17506084600581398
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.1749861089500991
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.17489014845341444
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.17501130311385446
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.17505070254390623
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.17502732136132526
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.1749272434631499
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.17475589947267012
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.17470823749002204
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.17471278570369334
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.174515880999111
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.17436259032706536
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.17446103955016418
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.17441790389735795
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.17423209149477092
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.17401120678193308
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.1739515533735012
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.1739956283569336
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.17399545267901637
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.17419680927769612
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.1741026963075895
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.17410086686384743
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.1740779155658351
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.1738868026443608
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.17387886525510432
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.1738283305220265
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.17371710041618865
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.17364825300268225
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.17357111810356057
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.1736225570268172
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.17357086326847684
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.1736077549596312
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.17356641300414738
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.173466384956974
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.1737178914093723
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.17370054635359217
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.1736956521407845
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.17368562649457883
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.17366237459438189
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.1734078512881613
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.17338144064250618
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.173363258341449
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.17352058202028275
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.17343974966018355
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.17337630032607826
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.173338857557386
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.17326053998926105
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.17315275356536958
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.17315142558326999
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.17301790646596807
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.17276791526147953
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.1727677622195066
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.17282572771821703
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.17284850246533398
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.17295133434938934
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.17301712230617453
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.1729625523368889
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.1728944692500802
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.1728251338418987
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.17272464958478778
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.1727184044248467
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.17271603466985433
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.17245533429086207
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.172408024413823
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.17233200931737014
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.17234466853029526
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.17226732891452098
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.17230731536944707
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.1721578964023991
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.17208479039600766
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.17191267709590888
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.17187535499251044
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.17188407172975334
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.1719268345278063
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.17197417304048251
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.17187346073563006
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.1718334299313207
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.1717253051222639
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.17164295308797037
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.17163419185937207
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.1716005051774638
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.1714097301643264
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.171344086745133
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.1713132369505914
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.17129960437574662
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.17129851494429044
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.17125908776995588
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.17124777627842766
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.17109269288376094
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.1711105625578749
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.17113817011516902
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.17104651126636558
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.17105084690451622
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.17101241965811567
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.17096671110226047
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.17096307505614203
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.1708725403084999
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.17089150737898023
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.17084175956551917
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.17071501606285341
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.17068275228835816
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.17079020130473213
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.170708518790511
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.17069960516873905
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.170639441128223
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.17046368470550036
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.1704115431022012
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.17039938161395632
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.17024504877906993
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.17028135315197684
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.17031847743956902
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.17018879016646665
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.1701929945912626
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.1701994078350683
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.1701044250444016
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.17000360216522392
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.16996363961022265
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.1698046471584927
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.16975906904300916
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.16985060847515665
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.16972175318238547
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.16970088359596056
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.16974423056734458
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.16968843704122666
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.1697291723644057
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.1696316178348376
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.16958400252944147
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.16969457354984785
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.16969813310078807
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.16972774898029785
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.16960671287961304
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.16950678013290912
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.1695164494216442
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.16953407824551528
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.16949141880318727
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.16952293061050538
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.16946821530558626
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.16943877695475595
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.16947556017721827
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.16941296996592672
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.1694223314473693
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.16936760321010316
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.16927894420921802
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.16921058200621525
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.16911877649805404
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.16909127987355682
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.16907162265852094
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.16914551504811304
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.16908088197408158
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.16898392046216257
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.16894372032544056
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.1689380247471402
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.1688145337806594
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.16871238744239714
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.16865744100262722
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.16866179208119456
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.16859017395574577
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.1687341968455012
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.16871476416252082
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.16870331818279033
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.16864547173275887
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.16857624990532766
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.16848865828942508
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.16843605540530332
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.1684819915973992
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.1684271671316203
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.16835865095533706
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.16838193010825378
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.16831696885952188
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.16831328779094445
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.1682945596871943
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.16830949211283658
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.1682478140475172
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.1681603792705925
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.16818391839154515
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.16814978642238154
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.16814038427379316
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.16809820010146098
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.16815776969971402
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.16809914088656003
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.16803535913839143
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.16796496169844913
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.16801147401771124
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.1679793194460729
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.16793404748303847
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.16790192778447627
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.167815882171136
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.1678549033360205
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.16779368631326394
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.16782599608132062
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.16782138648646316
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.16774567374612678
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.16776884434478623
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.16774411391243976
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.16767258076420563
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.16762612027672802
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.16770507940380586
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.16761213622882332
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.16756149370934856
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.16748154449279234
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.16742671341453186
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.16735687920393053
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.16731974256949292
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.1672833244267263
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.16725750619452961
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.16724605310308047
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.1672002033095111
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.16721510574833987
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.1672240527234768
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.1671168220100026
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.1671597037423888
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.16716168176756319
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.16725589316841719
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.16724482847432884
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.1672063544993439
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.16714039402976433
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.1671252888353432
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.16707422361771265
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.1670420413361268
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.1671090724653211
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.16707176705280308
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.16710405089059416
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.1671173435488814
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.16708448554665398
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.1670743411319106
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.16705504637551993
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.16702372745688385
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.16695117789816546
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.16696005779767284
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.1668606275158335
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.16688618603517713
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.16683098648921382
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.16690072293082872
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.16692593383133564
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.16692520923228288
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.16687671754878897
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.16682551436239693
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.16681277829257748
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.166813825398232
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.16677141709426788
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.1667160276679238
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.16671903041147051
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.16665696101263167
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.1667456056671844
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.1667556758803218
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.16669383199501867
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.16668629294058473
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.16664866431627745
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.16663427983084922
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.16658722880211743
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.1664708590354113
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.16647371822799623
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.166445155932409
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.16649076105106775
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.16647843757951722
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.16637640686358435
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.16627606062065575
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.16623561105814325
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.16619814312658632
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.16615844877074948
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.1661306126622492
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.16609732902704388
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.16603585925130618
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.16599757905668996
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.166000793754207
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.1659419078987541
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.16593924073396987
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.1658867490642211
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.16581293384374027
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.1658044398459115
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.16577010835024797
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.16571787267159194
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.16576866777830346
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.16571112457671464
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.16572594342546332
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.16571956714636735
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.16571432403567748
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.16571142861897917
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.16574875149158164
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.16571394993456878
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.165738086654171
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.165678279919885
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.16567321639846672
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.1656644922129962
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.16562308141818413
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.1656182123764374
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.1655871445598366
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.16553742021657109
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.16551910561296437
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.16549569688387364
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.16545086846287763
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.165425607913852
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.16536884688668782
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.16535001791633683
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.16530174356514374
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.16526922728315357
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.16517352087972972
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.16508116111323073
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.1650790904338161
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.1650249419542319
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.1649405495405978
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.16488337521654328
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.16485305702232797
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.1648805193005053
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.16482388313650054
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.1647836084016971
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.16479520665096312
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.16475828996589106
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.1647794290321835
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.16480469121348423
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.16477541856340364
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.1647484525402726
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.1646492561761369
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.16464426373220553
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.16455086948886766
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.16455086948886766 valid 0.1970348358154297
LOSS train 0.16455086948886766 valid 0.2005297988653183
LOSS train 0.16455086948886766 valid 0.20845184723536173
LOSS train 0.16455086948886766 valid 0.19797121733427048
LOSS train 0.16455086948886766 valid 0.2000488668680191
LOSS train 0.16455086948886766 valid 0.20467778543631235
LOSS train 0.16455086948886766 valid 0.20150693825313024
LOSS train 0.16455086948886766 valid 0.1995634939521551
LOSS train 0.16455086948886766 valid 0.19693551460901895
LOSS train 0.16455086948886766 valid 0.19730856418609619
LOSS train 0.16455086948886766 valid 0.19672365893017163
LOSS train 0.16455086948886766 valid 0.20116334781050682
LOSS train 0.16455086948886766 valid 0.20080350797909957
LOSS train 0.16455086948886766 valid 0.20018073171377182
LOSS train 0.16455086948886766 valid 0.19971242249011995
LOSS train 0.16455086948886766 valid 0.20161165576428175
LOSS train 0.16455086948886766 valid 0.20282486870008357
LOSS train 0.16455086948886766 valid 0.2030204782883326
LOSS train 0.16455086948886766 valid 0.2049921829449503
LOSS train 0.16455086948886766 valid 0.20412345752120017
LOSS train 0.16455086948886766 valid 0.20560492149421147
LOSS train 0.16455086948886766 valid 0.20494525879621506
LOSS train 0.16455086948886766 valid 0.20357223373392355
LOSS train 0.16455086948886766 valid 0.2045331628372272
LOSS train 0.16455086948886766 valid 0.20342159450054167
LOSS train 0.16455086948886766 valid 0.20259228406044152
LOSS train 0.16455086948886766 valid 0.20221649165506717
LOSS train 0.16455086948886766 valid 0.20251691660710744
LOSS train 0.16455086948886766 valid 0.20118802837256727
LOSS train 0.16455086948886766 valid 0.20130669822295508
LOSS train 0.16455086948886766 valid 0.20145039356523944
LOSS train 0.16455086948886766 valid 0.2017486086115241
LOSS train 0.16455086948886766 valid 0.2012117604414622
LOSS train 0.16455086948886766 valid 0.20040422765647664
LOSS train 0.16455086948886766 valid 0.20059031673840114
LOSS train 0.16455086948886766 valid 0.2009031582209799
LOSS train 0.16455086948886766 valid 0.20139503116543228
LOSS train 0.16455086948886766 valid 0.20132139050646833
LOSS train 0.16455086948886766 valid 0.20175604751476875
LOSS train 0.16455086948886766 valid 0.20142759308218955
LOSS train 0.16455086948886766 valid 0.20150931143179174
LOSS train 0.16455086948886766 valid 0.2026222206297375
LOSS train 0.16455086948886766 valid 0.20388151462688003
LOSS train 0.16455086948886766 valid 0.20318569683215834
LOSS train 0.16455086948886766 valid 0.20260296795103286
LOSS train 0.16455086948886766 valid 0.2015952736787174
LOSS train 0.16455086948886766 valid 0.20138937361696932
LOSS train 0.16455086948886766 valid 0.20279940714438757
LOSS train 0.16455086948886766 valid 0.20227310061454773
LOSS train 0.16455086948886766 valid 0.2023579952120781
LOSS train 0.16455086948886766 valid 0.20181859065504634
LOSS train 0.16455086948886766 valid 0.20172579815754524
LOSS train 0.16455086948886766 valid 0.20284919862477285
LOSS train 0.16455086948886766 valid 0.20300680619698983
LOSS train 0.16455086948886766 valid 0.20288355540145528
LOSS train 0.16455086948886766 valid 0.20280434004962444
LOSS train 0.16455086948886766 valid 0.20230025445160113
LOSS train 0.16455086948886766 valid 0.20269836896452412
LOSS train 0.16455086948886766 valid 0.20246995783458321
LOSS train 0.16455086948886766 valid 0.20208851173520087
LOSS train 0.16455086948886766 valid 0.20195097439601772
LOSS train 0.16455086948886766 valid 0.2015523811982524
LOSS train 0.16455086948886766 valid 0.20134163541453226
LOSS train 0.16455086948886766 valid 0.20138476020656526
LOSS train 0.16455086948886766 valid 0.20062873638593234
LOSS train 0.16455086948886766 valid 0.20061729803229822
LOSS train 0.16455086948886766 valid 0.20123556856788807
LOSS train 0.16455086948886766 valid 0.2005195924464394
LOSS train 0.16455086948886766 valid 0.20092250298762668
LOSS train 0.16455086948886766 valid 0.20112292085375105
LOSS train 0.16455086948886766 valid 0.20107346563272074
LOSS train 0.16455086948886766 valid 0.2015435318979952
LOSS train 0.16455086948886766 valid 0.2019054799863737
LOSS train 0.16455086948886766 valid 0.2016614510400875
LOSS train 0.16455086948886766 valid 0.20145551939805348
LOSS train 0.16455086948886766 valid 0.20127330975312935
LOSS train 0.16455086948886766 valid 0.20122575701831222
LOSS train 0.16455086948886766 valid 0.2010691639704582
LOSS train 0.16455086948886766 valid 0.20119100394128245
LOSS train 0.16455086948886766 valid 0.20067382864654065
LOSS train 0.16455086948886766 valid 0.2008235800413438
LOSS train 0.16455086948886766 valid 0.20097748680812558
LOSS train 0.16455086948886766 valid 0.20106138935290188
LOSS train 0.16455086948886766 valid 0.20086540476906867
LOSS train 0.16455086948886766 valid 0.2012324722374187
LOSS train 0.16455086948886766 valid 0.20117125296315483
LOSS train 0.16455086948886766 valid 0.20091607447328239
LOSS train 0.16455086948886766 valid 0.20091565884649754
LOSS train 0.16455086948886766 valid 0.2011493623591541
LOSS train 0.16455086948886766 valid 0.20114680429299672
LOSS train 0.16455086948886766 valid 0.20116163069730278
LOSS train 0.16455086948886766 valid 0.20110515488878541
LOSS train 0.16455086948886766 valid 0.20136640116732607
LOSS train 0.16455086948886766 valid 0.2014548788045315
LOSS train 0.16455086948886766 valid 0.2018847799614856
LOSS train 0.16455086948886766 valid 0.20267395318175355
LOSS train 0.16455086948886766 valid 0.20278665408031227
LOSS train 0.16455086948886766 valid 0.20326681754418782
LOSS train 0.16455086948886766 valid 0.20320225348978332
LOSS train 0.16455086948886766 valid 0.20329560786485673
LOSS train 0.16455086948886766 valid 0.20317749443030594
LOSS train 0.16455086948886766 valid 0.2034448370629666
LOSS train 0.16455086948886766 valid 0.20309852830414632
LOSS train 0.16455086948886766 valid 0.20293257371164286
LOSS train 0.16455086948886766 valid 0.20309890835058123
LOSS train 0.16455086948886766 valid 0.2029472748344799
LOSS train 0.16455086948886766 valid 0.2026963096077197
LOSS train 0.16455086948886766 valid 0.2028004798900198
LOSS train 0.16455086948886766 valid 0.20248775826681645
LOSS train 0.16455086948886766 valid 0.20238606415011667
LOSS train 0.16455086948886766 valid 0.20257588281287803
LOSS train 0.16455086948886766 valid 0.20278680909957206
LOSS train 0.16455086948886766 valid 0.2026784409463933
LOSS train 0.16455086948886766 valid 0.2027617538707298
LOSS train 0.16455086948886766 valid 0.2032333431036576
LOSS train 0.16455086948886766 valid 0.2030190467063723
LOSS train 0.16455086948886766 valid 0.20322226790281442
LOSS train 0.16455086948886766 valid 0.2030720777683339
LOSS train 0.16455086948886766 valid 0.20279844743864878
LOSS train 0.16455086948886766 valid 0.20285782975455124
LOSS train 0.16455086948886766 valid 0.20273623141375455
LOSS train 0.16455086948886766 valid 0.20305556229880597
LOSS train 0.16455086948886766 valid 0.20305860224293498
LOSS train 0.16455086948886766 valid 0.2033059003372346
LOSS train 0.16455086948886766 valid 0.20325069952011107
LOSS train 0.16455086948886766 valid 0.2033752447792462
LOSS train 0.16455086948886766 valid 0.20327973189785725
LOSS train 0.16455086948886766 valid 0.20326904219109565
LOSS train 0.16455086948886766 valid 0.20309447214122892
LOSS train 0.16455086948886766 valid 0.2028509590488214
LOSS train 0.16455086948886766 valid 0.20312359587836812
LOSS train 0.16455086948886766 valid 0.2033514603972435
LOSS train 0.16455086948886766 valid 0.2034250178507396
LOSS train 0.16455086948886766 valid 0.20363640840818634
LOSS train 0.16455086948886766 valid 0.20363339792799068
LOSS train 0.16455086948886766 valid 0.2037530352964121
LOSS train 0.16455086948886766 valid 0.20371928669675424
LOSS train 0.16455086948886766 valid 0.20344985708378363
LOSS train 0.16455086948886766 valid 0.20331616249444673
LOSS train 0.16455086948886766 valid 0.20309799185820987
LOSS train 0.16455086948886766 valid 0.2032567320774633
LOSS train 0.16455086948886766 valid 0.2036153074931091
LOSS train 0.16455086948886766 valid 0.20373462369808784
LOSS train 0.16455086948886766 valid 0.2035961062130001
LOSS train 0.16455086948886766 valid 0.20345291094533327
LOSS train 0.16455086948886766 valid 0.20348940636605434
LOSS train 0.16455086948886766 valid 0.20350035102594466
LOSS train 0.16455086948886766 valid 0.20331033953541033
LOSS train 0.16455086948886766 valid 0.20348389386730706
LOSS train 0.16455086948886766 valid 0.203408149878184
LOSS train 0.16455086948886766 valid 0.20332549186731805
LOSS train 0.16455086948886766 valid 0.2032838564758238
LOSS train 0.16455086948886766 valid 0.20303037538637522
LOSS train 0.16455086948886766 valid 0.20299263762963282
LOSS train 0.16455086948886766 valid 0.202887517502231
LOSS train 0.16455086948886766 valid 0.20294378420863396
LOSS train 0.16455086948886766 valid 0.2027200304778518
LOSS train 0.16455086948886766 valid 0.2026493552176258
LOSS train 0.16455086948886766 valid 0.20294470138519816
LOSS train 0.16455086948886766 valid 0.2029592261649668
LOSS train 0.16455086948886766 valid 0.20290831703206766
LOSS train 0.16455086948886766 valid 0.20281302919726313
LOSS train 0.16455086948886766 valid 0.20259308952129693
LOSS train 0.16455086948886766 valid 0.20256241865274383
LOSS train 0.16455086948886766 valid 0.20272855198744572
LOSS train 0.16455086948886766 valid 0.2025873455477048
LOSS train 0.16455086948886766 valid 0.20265481679025524
LOSS train 0.16455086948886766 valid 0.20252334157980623
LOSS train 0.16455086948886766 valid 0.20236160628188996
LOSS train 0.16455086948886766 valid 0.2023475855588913
LOSS train 0.16455086948886766 valid 0.20223475330405766
LOSS train 0.16455086948886766 valid 0.20242049033905185
LOSS train 0.16455086948886766 valid 0.20238614797247628
LOSS train 0.16455086948886766 valid 0.20263201417936677
LOSS train 0.16455086948886766 valid 0.20247113100119998
LOSS train 0.16455086948886766 valid 0.2023927690461278
LOSS train 0.16455086948886766 valid 0.20239781516756716
LOSS train 0.16455086948886766 valid 0.20245513177654717
LOSS train 0.16455086948886766 valid 0.2025136720368316
LOSS train 0.16455086948886766 valid 0.2024952740305
LOSS train 0.16455086948886766 valid 0.20244556179692075
LOSS train 0.16455086948886766 valid 0.20237066662246053
LOSS train 0.16455086948886766 valid 0.20252185945953827
LOSS train 0.16455086948886766 valid 0.20257604883416838
LOSS train 0.16455086948886766 valid 0.2025293123077702
LOSS train 0.16455086948886766 valid 0.20256266694876454
LOSS train 0.16455086948886766 valid 0.20246696448262363
LOSS train 0.16455086948886766 valid 0.20253095958144107
LOSS train 0.16455086948886766 valid 0.20230597218193075
LOSS train 0.16455086948886766 valid 0.2023590009463461
LOSS train 0.16455086948886766 valid 0.20240922030354044
LOSS train 0.16455086948886766 valid 0.2025431472963343
LOSS train 0.16455086948886766 valid 0.20230252816887098
LOSS train 0.16455086948886766 valid 0.2022992095996424
LOSS train 0.16455086948886766 valid 0.20219380947259757
LOSS train 0.16455086948886766 valid 0.20238649457388994
LOSS train 0.16455086948886766 valid 0.20243059242437333
LOSS train 0.16455086948886766 valid 0.20263315503946458
LOSS train 0.16455086948886766 valid 0.20264104720036588
LOSS train 0.16455086948886766 valid 0.20286811351776124
LOSS train 0.16455086948886766 valid 0.2027610587836498
LOSS train 0.16455086948886766 valid 0.2028215399915629
LOSS train 0.16455086948886766 valid 0.20282648445056578
LOSS train 0.16455086948886766 valid 0.20267046104167022
LOSS train 0.16455086948886766 valid 0.20251211952872392
LOSS train 0.16455086948886766 valid 0.2024794462525729
LOSS train 0.16455086948886766 valid 0.2024270495498814
LOSS train 0.16455086948886766 valid 0.2022772796738606
LOSS train 0.16455086948886766 valid 0.20231668537028097
LOSS train 0.16455086948886766 valid 0.20247754248834793
LOSS train 0.16455086948886766 valid 0.20270452616621532
LOSS train 0.16455086948886766 valid 0.20248364938317603
LOSS train 0.16455086948886766 valid 0.20240125794645766
LOSS train 0.16455086948886766 valid 0.20232229317738631
LOSS train 0.16455086948886766 valid 0.20208391554133837
LOSS train 0.16455086948886766 valid 0.20184076667107917
LOSS train 0.16455086948886766 valid 0.20172311908089072
LOSS train 0.16455086948886766 valid 0.20177730863247442
LOSS train 0.16455086948886766 valid 0.20167001278977417
LOSS train 0.16455086948886766 valid 0.20165766728195278
LOSS train 0.16455086948886766 valid 0.20164694782026213
LOSS train 0.16455086948886766 valid 0.20177027328057331
LOSS train 0.16455086948886766 valid 0.20177593600055027
LOSS train 0.16455086948886766 valid 0.20169003301167063
LOSS train 0.16455086948886766 valid 0.2014426914188597
LOSS train 0.16455086948886766 valid 0.2012528719100277
LOSS train 0.16455086948886766 valid 0.2013089526198509
LOSS train 0.16455086948886766 valid 0.20132423760859589
LOSS train 0.16455086948886766 valid 0.20124334493839063
LOSS train 0.16455086948886766 valid 0.2013439985073131
LOSS train 0.16455086948886766 valid 0.2013067766314461
LOSS train 0.16455086948886766 valid 0.20144439186772395
LOSS train 0.16455086948886766 valid 0.2017531322691062
LOSS train 0.16455086948886766 valid 0.20175472226662514
LOSS train 0.16455086948886766 valid 0.20180262257444098
LOSS train 0.16455086948886766 valid 0.20168427295856556
LOSS train 0.16455086948886766 valid 0.20173187604181878
LOSS train 0.16455086948886766 valid 0.20165469240741568
LOSS train 0.16455086948886766 valid 0.2016421238498209
LOSS train 0.16455086948886766 valid 0.2012869605484108
LOSS train 0.16455086948886766 valid 0.2011748960094828
LOSS train 0.16455086948886766 valid 0.20111454835484835
LOSS train 0.16455086948886766 valid 0.2011039717881768
LOSS train 0.16455086948886766 valid 0.2010746842830396
LOSS train 0.16455086948886766 valid 0.20131427320290585
LOSS train 0.16455086948886766 valid 0.2014211579490968
LOSS train 0.16455086948886766 valid 0.20145995875722483
LOSS train 0.16455086948886766 valid 0.20142107477952395
LOSS train 0.16455086948886766 valid 0.20132389362437658
LOSS train 0.16455086948886766 valid 0.20137147054076196
LOSS train 0.16455086948886766 valid 0.20139941083363802
LOSS train 0.16455086948886766 valid 0.2013833432620953
LOSS train 0.16455086948886766 valid 0.2012612933050031
LOSS train 0.16455086948886766 valid 0.20133179376327145
LOSS train 0.16455086948886766 valid 0.20121147810828452
LOSS train 0.16455086948886766 valid 0.2010698475933168
LOSS train 0.16455086948886766 valid 0.20100220402970853
LOSS train 0.16455086948886766 valid 0.201150811486697
LOSS train 0.16455086948886766 valid 0.2011149752036485
LOSS train 0.16455086948886766 valid 0.20104306228458882
LOSS train 0.16455086948886766 valid 0.20106046943714792
LOSS train 0.16455086948886766 valid 0.20121000192433824
LOSS train 0.16455086948886766 valid 0.20112760203532845
LOSS train 0.16455086948886766 valid 0.20113200973719358
LOSS train 0.16455086948886766 valid 0.20113588281959857
LOSS train 0.16455086948886766 valid 0.20122303805292996
LOSS train 0.16455086948886766 valid 0.20114309563953778
LOSS train 0.16455086948886766 valid 0.2012481571300261
LOSS train 0.16455086948886766 valid 0.20128384141119882
LOSS train 0.16455086948886766 valid 0.20116349925045615
LOSS train 0.16455086948886766 valid 0.2011200000714112
LOSS train 0.16455086948886766 valid 0.2010199163273415
LOSS train 0.16455086948886766 valid 0.20088819157837073
LOSS train 0.16455086948886766 valid 0.20084865622385575
LOSS train 0.16455086948886766 valid 0.20095824043859134
LOSS train 0.16455086948886766 valid 0.2010406862296488
LOSS train 0.16455086948886766 valid 0.20095383026216865
LOSS train 0.16455086948886766 valid 0.20090868718010915
LOSS train 0.16455086948886766 valid 0.20087146751021825
LOSS train 0.16455086948886766 valid 0.20075772327503988
LOSS train 0.16455086948886766 valid 0.20065555466981969
LOSS train 0.16455086948886766 valid 0.2006364219841805
LOSS train 0.16455086948886766 valid 0.200560673518442
LOSS train 0.16455086948886766 valid 0.20043656557903322
LOSS train 0.16455086948886766 valid 0.2004142963833976
LOSS train 0.16455086948886766 valid 0.20039842676345285
LOSS train 0.16455086948886766 valid 0.20050990366042698
LOSS train 0.16455086948886766 valid 0.20044021435185438
LOSS train 0.16455086948886766 valid 0.20037093529136124
LOSS train 0.16455086948886766 valid 0.2002547844473658
LOSS train 0.16455086948886766 valid 0.20017974746903194
LOSS train 0.16455086948886766 valid 0.20011314762475557
LOSS train 0.16455086948886766 valid 0.20008610862819
LOSS train 0.16455086948886766 valid 0.20007735331143653
LOSS train 0.16455086948886766 valid 0.2001301777312311
LOSS train 0.16455086948886766 valid 0.2000703277267717
LOSS train 0.16455086948886766 valid 0.2000078486924621
LOSS train 0.16455086948886766 valid 0.20003007059829347
LOSS train 0.16455086948886766 valid 0.20001871786687686
LOSS train 0.16455086948886766 valid 0.20001491827269396
LOSS train 0.16455086948886766 valid 0.1999570057291129
LOSS train 0.16455086948886766 valid 0.19989722461396495
LOSS train 0.16455086948886766 valid 0.19989648916245295
LOSS train 0.16455086948886766 valid 0.19986582241070114
LOSS train 0.16455086948886766 valid 0.1997840630959292
LOSS train 0.16455086948886766 valid 0.19995474004570177
LOSS train 0.16455086948886766 valid 0.1999912329187999
LOSS train 0.16455086948886766 valid 0.2000470624680256
LOSS train 0.16455086948886766 valid 0.20000706239329186
LOSS train 0.16455086948886766 valid 0.19999935028533783
LOSS train 0.16455086948886766 valid 0.20005208562903848
LOSS train 0.16455086948886766 valid 0.19999423768753424
LOSS train 0.16455086948886766 valid 0.199917247001165
LOSS train 0.16455086948886766 valid 0.19980896693790795
LOSS train 0.16455086948886766 valid 0.19972668514838293
LOSS train 0.16455086948886766 valid 0.19966597994200036
LOSS train 0.16455086948886766 valid 0.19969481570581532
LOSS train 0.16455086948886766 valid 0.19973219624199207
LOSS train 0.16455086948886766 valid 0.19975004714204225
LOSS train 0.16455086948886766 valid 0.1996094509260729
LOSS train 0.16455086948886766 valid 0.1996698181994979
LOSS train 0.16455086948886766 valid 0.19957307737714014
LOSS train 0.16455086948886766 valid 0.1994684773379066
LOSS train 0.16455086948886766 valid 0.19945384131997457
LOSS train 0.16455086948886766 valid 0.19930822471013435
LOSS train 0.16455086948886766 valid 0.19942034792406427
LOSS train 0.16455086948886766 valid 0.1993117420635092
LOSS train 0.16455086948886766 valid 0.1993078039959073
LOSS train 0.16455086948886766 valid 0.19934953905438219
LOSS train 0.16455086948886766 valid 0.19941537910790155
LOSS train 0.16455086948886766 valid 0.19933167385820896
LOSS train 0.16455086948886766 valid 0.19930815856051015
LOSS train 0.16455086948886766 valid 0.19933385934468145
LOSS train 0.16455086948886766 valid 0.19925752953587178
LOSS train 0.16455086948886766 valid 0.1990962062086632
LOSS train 0.16455086948886766 valid 0.19903496384531968
LOSS train 0.16455086948886766 valid 0.19913110621162974
LOSS train 0.16455086948886766 valid 0.19912990184842483
LOSS train 0.16455086948886766 valid 0.19906267125961702
LOSS train 0.16455086948886766 valid 0.19915565846159178
LOSS train 0.16455086948886766 valid 0.19915907786301504
LOSS train 0.16455086948886766 valid 0.19915595478568857
LOSS train 0.16455086948886766 valid 0.19903640572861403
LOSS train 0.16455086948886766 valid 0.19911408222951862
LOSS train 0.16455086948886766 valid 0.19918004462252492
LOSS train 0.16455086948886766 valid 0.1991485736919621
LOSS train 0.16455086948886766 valid 0.19909924691709385
LOSS train 0.16455086948886766 valid 0.1990808282133149
LOSS train 0.16455086948886766 valid 0.19908100910975804
LOSS train 0.16455086948886766 valid 0.19910641514829228
LOSS train 0.16455086948886766 valid 0.19918287061473244
LOSS train 0.16455086948886766 valid 0.1992351214443757
LOSS train 0.16455086948886766 valid 0.19929843205990264
LOSS train 0.16455086948886766 valid 0.19929720998062925
LOSS train 0.16455086948886766 valid 0.19918925718102656
LOSS train 0.16455086948886766 valid 0.1991902683767375
LOSS train 0.16455086948886766 valid 0.1991358338659551
LOSS train 0.16455086948886766 valid 0.19914023983994675
LOSS train 0.16455086948886766 valid 0.19909616613703518
LOSS train 0.16455086948886766 valid 0.19931168131944205
LOSS train 0.16455086948886766 valid 0.19919459519706603
LOSS train 0.16455086948886766 valid 0.199291387253868
LOSS train 0.16455086948886766 valid 0.1993106323455976
LOSS train 0.16455086948886766 valid 0.19924490491521882
LOSS train 0.16455086948886766 valid 0.19928310393062357
LOSS train 0.16455086948886766 valid 0.199240057215176
LOSS train 0.16455086948886766 valid 0.19922259784930407
LOSS train 0.16455086948886766 valid 0.1991299978538376
LOSS train 0.16455086948886766 valid 0.19918973237114548
EPOCH 3:
  batch 1 loss: 0.17118369042873383
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.15214668959379196
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.1452305018901825
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.14370760694146156
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.14664742052555085
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.14846643805503845
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.15053115785121918
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.14695456810295582
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.1467070115937127
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.1443851351737976
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.14478024298494513
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.14741680398583412
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.14759990114432114
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.14558844161885126
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.1443958391745885
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.1461742864921689
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.14724153806181514
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.14822732905546823
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.14895163476467133
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.14841431081295015
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.14787262678146362
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.14834743805906989
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.14686336335928543
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.14644401147961617
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.14662419080734254
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.1487205956990902
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.14913024725737395
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.1495127049939973
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.15073856762770949
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.15074048588673275
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.15060452347801578
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.14959453791379929
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.1500240365664164
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.15030044667861042
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.15045182406902313
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.15121475441588295
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.15190217060011788
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.15126964410668925
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.15045961814048964
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.15036529377102853
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.1496957235220002
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.14973841962360201
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.14978808684404507
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.15001053498549896
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.14982848862806955
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.14976743673500809
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.14995236441175988
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.15012392836312452
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.15015987808607062
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.1502231240272522
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.14986611084610807
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.1490314622911123
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.14891323531573675
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.14859358855971583
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.14842613989656622
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.1487092785537243
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.14918574743103563
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.14886641168388828
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.14954797191134953
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.15009164661169053
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.15053747789781602
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.15049208965032332
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.15026811474845522
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.15037723584100604
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.15040160119533538
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.15023471573085495
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.1505523604243549
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.15052062053890788
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.15029917052690533
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.15020475600446972
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.15050549301463115
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.15030420819918314
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.15078023525133524
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.15114460523064072
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.15098559737205505
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.15102778414362356
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.15129941198732946
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.15133185264391777
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.15159035879599897
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.15166649948805572
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.15186377752710273
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.1517707405657303
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.1517913901662252
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.15179802991804622
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.15190924143089968
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.1515960703755534
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.1515274720972982
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.15153656459667467
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.1515913533695628
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.15156468037101958
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.15165147660197792
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.15158536965432373
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.15147424449202834
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.15148855777496986
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.15125107561287127
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.15123704013725123
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.15126160923967658
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.1513555274934185
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.15148879361875128
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.15130275428295137
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.15133227275149658
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.15135894847266815
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.15141365395008938
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.15167513790612036
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.15145414272944133
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.15149956309008147
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.15157663557574014
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.15163983045904725
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.15163091877731708
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.1517329056154598
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.1513798041520892
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.15131307293527893
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.15093086673095163
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.15110917324036882
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.1510946847822355
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.15112917690441527
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.1511769723943156
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.15094939404625005
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.1510819824052458
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.1509270935008923
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.15096441145278205
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.1510170635874154
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.15089282817472288
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.15095291743355413
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.1512915859222412
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.15117362922146207
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.1513621736699202
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.1512784380465746
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.1510106928704321
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.15099734520682923
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.150948342191809
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.1506963720488729
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.15074555144498222
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.15068124440401348
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.1506522106351676
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.15060097532456412
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.1505572228644886
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.15042831225023753
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.1505169753226445
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.15038220227829047
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.15036268431878258
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.15032400920147626
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.15050646324049344
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.15029950362319747
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.15047011370288915
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.15044729161548287
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.15030153709001282
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.15025731634247946
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.15028521433572642
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.14996920322378476
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.15001446891896772
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.15011377147349872
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.1501812473718637
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.150150859172081
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.14996097558929075
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.14976143359373778
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.14968619130219624
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.1497098999687388
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.14966371561746178
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.14963070126250386
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.1497618509745746
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.14982988510602785
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.1498720767490703
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.14982460421033023
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.1496641692790118
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.14958922268755465
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.1496492986550588
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.14948047196403855
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.1493274705297143
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.14954300005646312
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.1495411818661885
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.14938343671518703
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.14923264099166572
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.14917195866676583
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.14928279029471533
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.14933541887016458
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.14956804667994128
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.14951336404748178
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.1495690757419144
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.14958896757000023
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.14938654133134124
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.14947277475353127
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.14952096623963998
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.14944375278023275
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.14943124309584901
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.1493481796194789
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.14943059557740063
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.1493947729864653
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.14941621650621373
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.14942648351976745
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.14941572998674751
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.149710926033246
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.14976590412422783
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.1498060259084726
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.1498388559008256
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.1498518255824337
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.14960969084410497
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.1496115160560367
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.14959558664853848
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.14975046932697297
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.1496640686223756
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.14961908469990928
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.14964247630734748
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.14958384318971166
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.14953661837228915
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.14961052150691598
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.14948914120882606
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.14928302004073674
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.14929287499217897
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.14940485585303534
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.14943315802027263
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.14956937027427386
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.14965193344393807
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.1496644554154895
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.14962426056695538
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.14959178885651958
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.14952058389714237
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.1495644265632017
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.1496238930312466
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.1493814082308249
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.1493624638917759
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.1493012219130456
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.1493931958627273
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.149365192877927
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.14944930526945327
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.1493503054422615
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.1493042167850528
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.14918826432212404
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.14919358560221685
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.14924312988701074
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.1492814254902658
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.14934630881481128
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.14929357685884181
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.14927647195947477
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.14917882151426154
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.14913903729264008
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.14914936591175537
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.1491692482861651
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.1489771674679413
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.14896407360211014
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.1489472326463189
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.1489425879433628
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.1489457929882493
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.14895584321290742
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.1489806218718996
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.14882894244983913
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.1488855872981944
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.14896291347160454
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.14892441683624644
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.14896603611111642
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.14896437024096568
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.14892273156770638
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.14893318503623895
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.1488358948350422
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.14888205893483816
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.14885299469460733
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.14873926083517447
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.14874739139232523
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.1488637438899762
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.1488189502690847
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.1488739881536056
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.14887987950262221
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.14873025413361793
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.14872789566376896
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.1487235905708007
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.1486122185798516
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.14866964814814734
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.148724821187667
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.1486225248103248
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.14865614195112828
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.1486572955092381
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.1485777533174876
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.14849897081052865
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.14848797064084207
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.14832083591006018
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.148318644532043
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.14841685826059714
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.14829128868073868
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.14828656960986422
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.14835630123104368
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.1483077998687364
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.14835090781991364
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.14828285564382168
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.14827112407541612
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.14841919839382173
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.14845103374519547
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.14852200760035564
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.14845300222643548
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.14838741039116077
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.14843290818148647
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.14845577414912456
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.14843443041182544
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.14853025342213821
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.14850051280389837
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.1485284914404659
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.1486126057098846
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.14858680858154488
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.14862754330939096
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.14859955052667637
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.14852210144201913
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.14849451916954445
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.14842100392114246
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.14842268018045834
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.14841041052223822
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.1484956177043133
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.14846630973948372
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.14839212339165156
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.14838224262386174
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.1484037892814593
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.14832908362150193
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.14826498690908746
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.1482211263038409
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.14826403329738033
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.14823466809881722
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.14841366008160606
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.14842780773775488
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.14843597636801964
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.14839500038878722
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.1483690225779076
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.1483056120108813
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.14828689837381476
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.14836710691452026
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.14833030312238463
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.1483014713006991
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.1483583029416891
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.1483307364711001
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.14836736219374047
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.1483669784828657
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.1483923438653395
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.148333640396595
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.14824088266698976
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.14827764162187834
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.1482752633390126
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.14829762612363537
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.14824428066833695
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.1483282652417464
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.14829700701049245
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.14824774801819282
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.14821677997840188
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.14826777654097362
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.14827243693151082
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.14823870842306935
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.1482436127156975
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.14816911490417498
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.14823930416850076
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.14818417227078726
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.1482321073540006
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.1482474858091138
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.14818925414778783
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.1482468377479485
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.14825024587391447
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.14821259726092897
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.1481965843366496
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.14832329082876275
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.14825316770395763
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.1482365500767914
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.14817520869629724
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.14812698129918322
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.14809527468133438
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.14807184688333008
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.14806889581630764
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.14806995324749314
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.14807862037223232
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.14804897782812407
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.14806575540402164
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.14811000307319594
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.1480363236501691
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.14809288272796117
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.1481031973147134
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.14822634823016217
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.14824385983201693
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.14821501995526976
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.14817798095717827
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.1481643785488478
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.1481356075803439
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.14810704243706263
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.14822884265639738
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.14823083581511307
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.14829184195532963
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.1483133657394271
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.14827805642146138
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.14828148962081414
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.14828391110539124
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.14828946905133003
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.1482411617388973
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.14825321616209233
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.14819818009316768
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.1482346951001391
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.14820712266590416
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.14827376051973074
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.14831701749959564
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.14833722539170055
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.1483210843369251
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.14828264896715354
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.14829714164326463
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.14832198719559897
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.1482994091187977
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.1482774068475069
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.14830435530508967
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.1482605836354196
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.1483765871930598
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.1484292578600829
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.1483860843567635
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.14840381963196958
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.1483972508782222
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.14840481479840326
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.14838421782272745
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.14830355592729413
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.14832534079956833
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.14831046845491339
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.1483786370958725
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.1483872168091605
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.1483090739092873
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.14823486466986546
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.14820076358246517
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.1481924652874183
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.1481530551108525
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.14814586293968288
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.14814023846018
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.14809846244752406
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.14808473201588043
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.1481083213612084
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.14807182037872627
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.1480857718138481
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.1480626292614376
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.1480125081965901
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.14799034565826769
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.147984567872971
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.14796509687508735
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.14804477090405863
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.14800144253156025
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.14804276063417396
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.1480517165593407
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.14804770095351105
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.14806402221150783
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.14813427796262665
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.1481294572796647
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.148166335489788
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.148131275723491
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.14814252333546227
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.14816145647883144
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.14814207802328588
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.14816295311830383
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.14814714350746022
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.14811075883969832
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.1481276926137674
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.14811899936372536
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.1480936983300905
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.14807922449502223
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.14804936774902874
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.14806994559561334
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.14803820546815352
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.1479978777353074
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.147921468373306
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.14784118151599235
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.14785136800390064
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.1478185032043989
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.14776638534131528
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.1477283704579526
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.1477130896695282
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.14777311049159333
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.14774309124523427
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.1477223908283283
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.14776025946926455
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.14774161715020415
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.1477744583472162
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.14781465529757343
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.14781442867257658
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.1478027075783276
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.14771633176727497
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.14771797642459789
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.14764315844908105
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.14764315844908105 valid 0.19302386045455933
LOSS train 0.14764315844908105 valid 0.20189450681209564
LOSS train 0.14764315844908105 valid 0.20714535812536874
LOSS train 0.14764315844908105 valid 0.19790640100836754
LOSS train 0.14764315844908105 valid 0.19956490099430085
LOSS train 0.14764315844908105 valid 0.20429830004771551
LOSS train 0.14764315844908105 valid 0.20178558358124324
LOSS train 0.14764315844908105 valid 0.19954876974225044
LOSS train 0.14764315844908105 valid 0.19625603821542528
LOSS train 0.14764315844908105 valid 0.19650054574012757
LOSS train 0.14764315844908105 valid 0.1961237517270175
LOSS train 0.14764315844908105 valid 0.20050250242153803
LOSS train 0.14764315844908105 valid 0.20035130358659303
LOSS train 0.14764315844908105 valid 0.199662840792111
LOSS train 0.14764315844908105 valid 0.19950151145458223
LOSS train 0.14764315844908105 valid 0.20107835624367
LOSS train 0.14764315844908105 valid 0.20298988065298865
LOSS train 0.14764315844908105 valid 0.20341109153297213
LOSS train 0.14764315844908105 valid 0.205428241899139
LOSS train 0.14764315844908105 valid 0.20445391163229942
LOSS train 0.14764315844908105 valid 0.20571038339819228
LOSS train 0.14764315844908105 valid 0.20486570420590314
LOSS train 0.14764315844908105 valid 0.20362110435962677
LOSS train 0.14764315844908105 valid 0.20445243703822294
LOSS train 0.14764315844908105 valid 0.20321379363536834
LOSS train 0.14764315844908105 valid 0.20231778747760332
LOSS train 0.14764315844908105 valid 0.20191181147540058
LOSS train 0.14764315844908105 valid 0.20219070145062037
LOSS train 0.14764315844908105 valid 0.20066419500729132
LOSS train 0.14764315844908105 valid 0.20081529865662257
LOSS train 0.14764315844908105 valid 0.2011013127142383
LOSS train 0.14764315844908105 valid 0.20151223568245769
LOSS train 0.14764315844908105 valid 0.2006855078718879
LOSS train 0.14764315844908105 valid 0.19975616739076726
LOSS train 0.14764315844908105 valid 0.19998466968536377
LOSS train 0.14764315844908105 valid 0.2002296948598491
LOSS train 0.14764315844908105 valid 0.2006802228656975
LOSS train 0.14764315844908105 valid 0.20081209508996262
LOSS train 0.14764315844908105 valid 0.20128909823222038
LOSS train 0.14764315844908105 valid 0.20078801177442074
LOSS train 0.14764315844908105 valid 0.20093686006418088
LOSS train 0.14764315844908105 valid 0.2020235469653493
LOSS train 0.14764315844908105 valid 0.20330999306468076
LOSS train 0.14764315844908105 valid 0.2027268379249356
LOSS train 0.14764315844908105 valid 0.20219120350148942
LOSS train 0.14764315844908105 valid 0.2011293976203255
LOSS train 0.14764315844908105 valid 0.20087558126195948
LOSS train 0.14764315844908105 valid 0.20206356700509787
LOSS train 0.14764315844908105 valid 0.20160837441074606
LOSS train 0.14764315844908105 valid 0.2016591787338257
LOSS train 0.14764315844908105 valid 0.2010355892719007
LOSS train 0.14764315844908105 valid 0.20092080075007218
LOSS train 0.14764315844908105 valid 0.20198717375971237
LOSS train 0.14764315844908105 valid 0.20224857882217126
LOSS train 0.14764315844908105 valid 0.2021536111831665
LOSS train 0.14764315844908105 valid 0.20200485603085586
LOSS train 0.14764315844908105 valid 0.20143374722254903
LOSS train 0.14764315844908105 valid 0.2018306869371184
LOSS train 0.14764315844908105 valid 0.20152649182384297
LOSS train 0.14764315844908105 valid 0.20118681167562802
LOSS train 0.14764315844908105 valid 0.2010743864735619
LOSS train 0.14764315844908105 valid 0.20068486491518636
LOSS train 0.14764315844908105 valid 0.20043465542414832
LOSS train 0.14764315844908105 valid 0.20055801724083722
LOSS train 0.14764315844908105 valid 0.19982354663885557
LOSS train 0.14764315844908105 valid 0.19982949599172128
LOSS train 0.14764315844908105 valid 0.2004473465147303
LOSS train 0.14764315844908105 valid 0.19979870209799094
LOSS train 0.14764315844908105 valid 0.2001861625391504
LOSS train 0.14764315844908105 valid 0.2003556130187852
LOSS train 0.14764315844908105 valid 0.20023613790391195
LOSS train 0.14764315844908105 valid 0.2007123176008463
LOSS train 0.14764315844908105 valid 0.20113321587647479
LOSS train 0.14764315844908105 valid 0.2008482835985519
LOSS train 0.14764315844908105 valid 0.20072414696216584
LOSS train 0.14764315844908105 valid 0.20051556982492147
LOSS train 0.14764315844908105 valid 0.20049292003953612
LOSS train 0.14764315844908105 valid 0.20028474086370224
LOSS train 0.14764315844908105 valid 0.20044638857811312
LOSS train 0.14764315844908105 valid 0.19997344873845577
LOSS train 0.14764315844908105 valid 0.20015337308983744
LOSS train 0.14764315844908105 valid 0.20034641663475736
LOSS train 0.14764315844908105 valid 0.20045475679707814
LOSS train 0.14764315844908105 valid 0.2002075587709745
LOSS train 0.14764315844908105 valid 0.20052983305033514
LOSS train 0.14764315844908105 valid 0.20049154100029967
LOSS train 0.14764315844908105 valid 0.2002368750928462
LOSS train 0.14764315844908105 valid 0.20013923553580587
LOSS train 0.14764315844908105 valid 0.20033972501085046
LOSS train 0.14764315844908105 valid 0.20028538919157451
LOSS train 0.14764315844908105 valid 0.20035427104640793
LOSS train 0.14764315844908105 valid 0.20027098756121553
LOSS train 0.14764315844908105 valid 0.20055476343759926
LOSS train 0.14764315844908105 valid 0.20064911848687111
LOSS train 0.14764315844908105 valid 0.20112308310834984
LOSS train 0.14764315844908105 valid 0.2020036083025237
LOSS train 0.14764315844908105 valid 0.20212446460404346
LOSS train 0.14764315844908105 valid 0.20269982075812865
LOSS train 0.14764315844908105 valid 0.20257104180677973
LOSS train 0.14764315844908105 valid 0.20268216893076896
LOSS train 0.14764315844908105 valid 0.20253202658478575
LOSS train 0.14764315844908105 valid 0.20284271722330766
LOSS train 0.14764315844908105 valid 0.202479833393421
LOSS train 0.14764315844908105 valid 0.20232689681534582
LOSS train 0.14764315844908105 valid 0.2024677065156755
LOSS train 0.14764315844908105 valid 0.20231917430207413
LOSS train 0.14764315844908105 valid 0.20207263877458662
LOSS train 0.14764315844908105 valid 0.20220131095912722
LOSS train 0.14764315844908105 valid 0.20193006973200983
LOSS train 0.14764315844908105 valid 0.20176385614005002
LOSS train 0.14764315844908105 valid 0.20196770292681618
LOSS train 0.14764315844908105 valid 0.20218755238290345
LOSS train 0.14764315844908105 valid 0.20202941928817109
LOSS train 0.14764315844908105 valid 0.20211281117640043
LOSS train 0.14764315844908105 valid 0.20263811868170034
LOSS train 0.14764315844908105 valid 0.20240387140676894
LOSS train 0.14764315844908105 valid 0.2026446552103401
LOSS train 0.14764315844908105 valid 0.20239848832962876
LOSS train 0.14764315844908105 valid 0.20215777940108998
LOSS train 0.14764315844908105 valid 0.20228055119514465
LOSS train 0.14764315844908105 valid 0.20216041470854734
LOSS train 0.14764315844908105 valid 0.2024921363250154
LOSS train 0.14764315844908105 valid 0.20251917778476466
LOSS train 0.14764315844908105 valid 0.20281212969172385
LOSS train 0.14764315844908105 valid 0.20277350676059722
LOSS train 0.14764315844908105 valid 0.2028621318084853
LOSS train 0.14764315844908105 valid 0.20271774574996917
LOSS train 0.14764315844908105 valid 0.20274713716935366
LOSS train 0.14764315844908105 valid 0.20255567653234616
LOSS train 0.14764315844908105 valid 0.20233796823483247
LOSS train 0.14764315844908105 valid 0.2026216284236835
LOSS train 0.14764315844908105 valid 0.20288027145645834
LOSS train 0.14764315844908105 valid 0.2029180368758682
LOSS train 0.14764315844908105 valid 0.2031076276925073
LOSS train 0.14764315844908105 valid 0.2031137634206701
LOSS train 0.14764315844908105 valid 0.2032875166658093
LOSS train 0.14764315844908105 valid 0.20324643097654746
LOSS train 0.14764315844908105 valid 0.20293115165786466
LOSS train 0.14764315844908105 valid 0.20284497298353868
LOSS train 0.14764315844908105 valid 0.2026018576962607
LOSS train 0.14764315844908105 valid 0.20282711236612172
LOSS train 0.14764315844908105 valid 0.20314404641238737
LOSS train 0.14764315844908105 valid 0.20328717579791597
LOSS train 0.14764315844908105 valid 0.2031022890781363
LOSS train 0.14764315844908105 valid 0.20293389887645327
LOSS train 0.14764315844908105 valid 0.20293227123887572
LOSS train 0.14764315844908105 valid 0.20293581070137673
LOSS train 0.14764315844908105 valid 0.20274744474807302
LOSS train 0.14764315844908105 valid 0.2029383785572628
LOSS train 0.14764315844908105 valid 0.2028722110390663
LOSS train 0.14764315844908105 valid 0.20278624400792533
LOSS train 0.14764315844908105 valid 0.20275128221041278
LOSS train 0.14764315844908105 valid 0.20247513289545097
LOSS train 0.14764315844908105 valid 0.20243594340689772
LOSS train 0.14764315844908105 valid 0.20236153419940703
LOSS train 0.14764315844908105 valid 0.20238977546493211
LOSS train 0.14764315844908105 valid 0.202109367129909
LOSS train 0.14764315844908105 valid 0.20201293435655063
LOSS train 0.14764315844908105 valid 0.2023432971733921
LOSS train 0.14764315844908105 valid 0.20235899975523353
LOSS train 0.14764315844908105 valid 0.2023219529145993
LOSS train 0.14764315844908105 valid 0.20220894007771104
LOSS train 0.14764315844908105 valid 0.20200947411586903
LOSS train 0.14764315844908105 valid 0.201932337316798
LOSS train 0.14764315844908105 valid 0.20211035618276307
LOSS train 0.14764315844908105 valid 0.20196619207959576
LOSS train 0.14764315844908105 valid 0.20203687875213738
LOSS train 0.14764315844908105 valid 0.20190761885827496
LOSS train 0.14764315844908105 valid 0.20180835676263775
LOSS train 0.14764315844908105 valid 0.20178741190363378
LOSS train 0.14764315844908105 valid 0.2016729245408934
LOSS train 0.14764315844908105 valid 0.201887130910574
LOSS train 0.14764315844908105 valid 0.20184504098630365
LOSS train 0.14764315844908105 valid 0.20208229903859654
LOSS train 0.14764315844908105 valid 0.20188642059053694
LOSS train 0.14764315844908105 valid 0.20182411177930507
LOSS train 0.14764315844908105 valid 0.201827135968343
LOSS train 0.14764315844908105 valid 0.20187345808476545
LOSS train 0.14764315844908105 valid 0.2018902072859876
LOSS train 0.14764315844908105 valid 0.20186628128091494
LOSS train 0.14764315844908105 valid 0.20180132203009907
LOSS train 0.14764315844908105 valid 0.20172729818048057
LOSS train 0.14764315844908105 valid 0.20193630204500396
LOSS train 0.14764315844908105 valid 0.20200294717822387
LOSS train 0.14764315844908105 valid 0.201937670723812
LOSS train 0.14764315844908105 valid 0.20199991690535699
LOSS train 0.14764315844908105 valid 0.20189358891650316
LOSS train 0.14764315844908105 valid 0.2019491104527991
LOSS train 0.14764315844908105 valid 0.2016760900064751
LOSS train 0.14764315844908105 valid 0.20172973800646632
LOSS train 0.14764315844908105 valid 0.20177832979182297
LOSS train 0.14764315844908105 valid 0.20188402462129793
LOSS train 0.14764315844908105 valid 0.20161558510108316
LOSS train 0.14764315844908105 valid 0.20163077884113667
LOSS train 0.14764315844908105 valid 0.20149792188253157
LOSS train 0.14764315844908105 valid 0.20170668725456511
LOSS train 0.14764315844908105 valid 0.20179052287854518
LOSS train 0.14764315844908105 valid 0.20199823131163916
LOSS train 0.14764315844908105 valid 0.20200376706806258
LOSS train 0.14764315844908105 valid 0.202299250587821
LOSS train 0.14764315844908105 valid 0.20218962275270205
LOSS train 0.14764315844908105 valid 0.20226269421896131
LOSS train 0.14764315844908105 valid 0.20225087322037796
LOSS train 0.14764315844908105 valid 0.20206548733746305
LOSS train 0.14764315844908105 valid 0.2018921590432888
LOSS train 0.14764315844908105 valid 0.20184485209219663
LOSS train 0.14764315844908105 valid 0.20180091009911708
LOSS train 0.14764315844908105 valid 0.20168016829456276
LOSS train 0.14764315844908105 valid 0.2017299160004803
LOSS train 0.14764315844908105 valid 0.20192499515556153
LOSS train 0.14764315844908105 valid 0.20216633965618802
LOSS train 0.14764315844908105 valid 0.20194275065694214
LOSS train 0.14764315844908105 valid 0.20186546675755945
LOSS train 0.14764315844908105 valid 0.20176424684925615
LOSS train 0.14764315844908105 valid 0.20152151875717694
LOSS train 0.14764315844908105 valid 0.20125251159899765
LOSS train 0.14764315844908105 valid 0.20116723812944878
LOSS train 0.14764315844908105 valid 0.20120475201978597
LOSS train 0.14764315844908105 valid 0.20110888255241255
LOSS train 0.14764315844908105 valid 0.2010841153562069
LOSS train 0.14764315844908105 valid 0.20109890050747814
LOSS train 0.14764315844908105 valid 0.20122974580741143
LOSS train 0.14764315844908105 valid 0.20122053604489484
LOSS train 0.14764315844908105 valid 0.20112790326986993
LOSS train 0.14764315844908105 valid 0.20086823569403756
LOSS train 0.14764315844908105 valid 0.2006631762580534
LOSS train 0.14764315844908105 valid 0.2007089218510405
LOSS train 0.14764315844908105 valid 0.20073413476347923
LOSS train 0.14764315844908105 valid 0.2006495166163257
LOSS train 0.14764315844908105 valid 0.2007640069593554
LOSS train 0.14764315844908105 valid 0.2007196487544419
LOSS train 0.14764315844908105 valid 0.2008909147618146
LOSS train 0.14764315844908105 valid 0.20116221789638372
LOSS train 0.14764315844908105 valid 0.20120301970049867
LOSS train 0.14764315844908105 valid 0.20125207425432
LOSS train 0.14764315844908105 valid 0.20112962711413027
LOSS train 0.14764315844908105 valid 0.2011996076957083
LOSS train 0.14764315844908105 valid 0.20112334680156546
LOSS train 0.14764315844908105 valid 0.20108986536828044
LOSS train 0.14764315844908105 valid 0.20071147956574956
LOSS train 0.14764315844908105 valid 0.20059920423876695
LOSS train 0.14764315844908105 valid 0.20054276777077312
LOSS train 0.14764315844908105 valid 0.20053136069710853
LOSS train 0.14764315844908105 valid 0.20051036955269633
LOSS train 0.14764315844908105 valid 0.20074696586448318
LOSS train 0.14764315844908105 valid 0.2008580179294435
LOSS train 0.14764315844908105 valid 0.2009087891774139
LOSS train 0.14764315844908105 valid 0.20083559487736033
LOSS train 0.14764315844908105 valid 0.200744786685011
LOSS train 0.14764315844908105 valid 0.2007887042462826
LOSS train 0.14764315844908105 valid 0.20082851439951901
LOSS train 0.14764315844908105 valid 0.20082587438325086
LOSS train 0.14764315844908105 valid 0.20068776280630246
LOSS train 0.14764315844908105 valid 0.20074263851590982
LOSS train 0.14764315844908105 valid 0.20063930489853316
LOSS train 0.14764315844908105 valid 0.2004956183664035
LOSS train 0.14764315844908105 valid 0.20038298198569146
LOSS train 0.14764315844908105 valid 0.20055161251924758
LOSS train 0.14764315844908105 valid 0.2005343569234071
LOSS train 0.14764315844908105 valid 0.20049314736746826
LOSS train 0.14764315844908105 valid 0.20048847300682032
LOSS train 0.14764315844908105 valid 0.20062984671187764
LOSS train 0.14764315844908105 valid 0.20054211393848573
LOSS train 0.14764315844908105 valid 0.20053451369996322
LOSS train 0.14764315844908105 valid 0.20058154691502733
LOSS train 0.14764315844908105 valid 0.20068077656223363
LOSS train 0.14764315844908105 valid 0.20059661843468635
LOSS train 0.14764315844908105 valid 0.20071511896354938
LOSS train 0.14764315844908105 valid 0.20073834319207748
LOSS train 0.14764315844908105 valid 0.20060093085523004
LOSS train 0.14764315844908105 valid 0.20054113075851954
LOSS train 0.14764315844908105 valid 0.2004493772161796
LOSS train 0.14764315844908105 valid 0.2003240520646284
LOSS train 0.14764315844908105 valid 0.2002839713364187
LOSS train 0.14764315844908105 valid 0.2003846032240174
LOSS train 0.14764315844908105 valid 0.2004790869473979
LOSS train 0.14764315844908105 valid 0.20038382953792702
LOSS train 0.14764315844908105 valid 0.2003472237546238
LOSS train 0.14764315844908105 valid 0.20031452416625928
LOSS train 0.14764315844908105 valid 0.20021674060927971
LOSS train 0.14764315844908105 valid 0.20011290125681414
LOSS train 0.14764315844908105 valid 0.20008738896419817
LOSS train 0.14764315844908105 valid 0.19999930986561962
LOSS train 0.14764315844908105 valid 0.19986638843908278
LOSS train 0.14764315844908105 valid 0.1998719719679732
LOSS train 0.14764315844908105 valid 0.1998536308797506
LOSS train 0.14764315844908105 valid 0.19997362379724556
LOSS train 0.14764315844908105 valid 0.19989558617170486
LOSS train 0.14764315844908105 valid 0.1998337524005286
LOSS train 0.14764315844908105 valid 0.1997026376940053
LOSS train 0.14764315844908105 valid 0.1996269597579114
LOSS train 0.14764315844908105 valid 0.19959125777527895
LOSS train 0.14764315844908105 valid 0.19957997065050204
LOSS train 0.14764315844908105 valid 0.19956507749196623
LOSS train 0.14764315844908105 valid 0.1996241554617882
LOSS train 0.14764315844908105 valid 0.1995505676227244
LOSS train 0.14764315844908105 valid 0.1994914080107252
LOSS train 0.14764315844908105 valid 0.19949848222352515
LOSS train 0.14764315844908105 valid 0.19951001230789267
LOSS train 0.14764315844908105 valid 0.19951444379985334
LOSS train 0.14764315844908105 valid 0.19945071348023177
LOSS train 0.14764315844908105 valid 0.19938487532419086
LOSS train 0.14764315844908105 valid 0.19940000981485884
LOSS train 0.14764315844908105 valid 0.19935685202577397
LOSS train 0.14764315844908105 valid 0.19926600834873856
LOSS train 0.14764315844908105 valid 0.19943838948810022
LOSS train 0.14764315844908105 valid 0.1994895738722447
LOSS train 0.14764315844908105 valid 0.19955071335198818
LOSS train 0.14764315844908105 valid 0.19949947806334417
LOSS train 0.14764315844908105 valid 0.1994922936683701
LOSS train 0.14764315844908105 valid 0.19954065952174532
LOSS train 0.14764315844908105 valid 0.19948647684680346
LOSS train 0.14764315844908105 valid 0.1993793509781551
LOSS train 0.14764315844908105 valid 0.19927361429591847
LOSS train 0.14764315844908105 valid 0.19919135615466133
LOSS train 0.14764315844908105 valid 0.19913677901878388
LOSS train 0.14764315844908105 valid 0.19916516601193215
LOSS train 0.14764315844908105 valid 0.19922349965141253
LOSS train 0.14764315844908105 valid 0.19921334202293317
LOSS train 0.14764315844908105 valid 0.19907161996234207
LOSS train 0.14764315844908105 valid 0.1991279181151004
LOSS train 0.14764315844908105 valid 0.1990151165851525
LOSS train 0.14764315844908105 valid 0.1989311545210726
LOSS train 0.14764315844908105 valid 0.19891329355721857
LOSS train 0.14764315844908105 valid 0.19875928821472022
LOSS train 0.14764315844908105 valid 0.19887971962613563
LOSS train 0.14764315844908105 valid 0.19878241062073168
LOSS train 0.14764315844908105 valid 0.1987853427985456
LOSS train 0.14764315844908105 valid 0.19883234878169728
LOSS train 0.14764315844908105 valid 0.19890073187875026
LOSS train 0.14764315844908105 valid 0.19882397153316309
LOSS train 0.14764315844908105 valid 0.1987974312695035
LOSS train 0.14764315844908105 valid 0.19880580756667857
LOSS train 0.14764315844908105 valid 0.1987195932445769
LOSS train 0.14764315844908105 valid 0.1985569267353015
LOSS train 0.14764315844908105 valid 0.19849124699387521
LOSS train 0.14764315844908105 valid 0.198596599967791
LOSS train 0.14764315844908105 valid 0.19860081266633858
LOSS train 0.14764315844908105 valid 0.1985245134055087
LOSS train 0.14764315844908105 valid 0.19862476063563544
LOSS train 0.14764315844908105 valid 0.1986242373886346
LOSS train 0.14764315844908105 valid 0.1986274660200055
LOSS train 0.14764315844908105 valid 0.1985060014590925
LOSS train 0.14764315844908105 valid 0.19859823162212623
LOSS train 0.14764315844908105 valid 0.1986578989935958
LOSS train 0.14764315844908105 valid 0.19862828582439118
LOSS train 0.14764315844908105 valid 0.19858532854133107
LOSS train 0.14764315844908105 valid 0.19856264512857486
LOSS train 0.14764315844908105 valid 0.19856759378510425
LOSS train 0.14764315844908105 valid 0.1986023056294237
LOSS train 0.14764315844908105 valid 0.198677995075018
LOSS train 0.14764315844908105 valid 0.19874067886055194
LOSS train 0.14764315844908105 valid 0.19881022662430917
LOSS train 0.14764315844908105 valid 0.1987990747695252
LOSS train 0.14764315844908105 valid 0.19868599388381125
LOSS train 0.14764315844908105 valid 0.19868510838053868
LOSS train 0.14764315844908105 valid 0.19861600013757622
LOSS train 0.14764315844908105 valid 0.19862403868380205
LOSS train 0.14764315844908105 valid 0.19856694417494586
LOSS train 0.14764315844908105 valid 0.198801025437812
LOSS train 0.14764315844908105 valid 0.19869872426144633
LOSS train 0.14764315844908105 valid 0.1987905906750023
LOSS train 0.14764315844908105 valid 0.19879961968207163
LOSS train 0.14764315844908105 valid 0.19871155064102713
LOSS train 0.14764315844908105 valid 0.19874276447377792
LOSS train 0.14764315844908105 valid 0.19870840764192285
LOSS train 0.14764315844908105 valid 0.19869227786561125
LOSS train 0.14764315844908105 valid 0.1986043594737092
LOSS train 0.14764315844908105 valid 0.1986701290418462
EPOCH 4:
  batch 1 loss: 0.16076816618442535
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.1418999582529068
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.13448085635900497
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.13424448482692242
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.137956403195858
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.13908922051390013
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.14137631548302515
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.1381208524107933
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.1381557368569904
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.13605285808444023
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.1370549980889667
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.14018246221045652
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.1401287792966916
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.13860827524747169
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.13751727094252905
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.1399126616306603
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.14074262406896143
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.14181394999225935
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.14246167517022082
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.14176772125065326
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.14114552487929663
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.14135275544090706
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.13955088218917017
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.1389186658586065
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.13889310330152513
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.1412018477343596
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.14182203428612816
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.14200229543660367
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.1430263578377921
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.1428140175839265
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.14268086105585098
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.14168546069413424
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.14210280234163458
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.1426399117883514
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.14267261794635228
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.14360774846540558
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.1444372156181851
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.14372639789393074
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.1429149030874937
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.14294431656599044
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.14219967766505917
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.14218776176373163
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.1422603514998458
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.14239808307452637
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.14218710561593373
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.14208038831534592
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.1421092688403231
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.1423623856777946
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.14253562871290712
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.14253011882305144
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.14214662856915417
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.14123919534568602
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.14109937945064507
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.1409429037184627
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.14091504311019723
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.14123814646154642
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.14181062095520788
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.1414839500754044
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.14211205216282505
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.14276312701404095
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.1432701675374
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.1432145520323707
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.14304124942374608
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.14308192988391966
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.14304074129232994
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.1428018445995721
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.14307874385545502
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.14298989919616895
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.1427523961317712
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.14266141088945525
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.1428769254978274
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.1426510333807932
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.14307342561548703
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.14341418515588786
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.14326368977626164
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.14338612134911513
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.14366768310209374
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.14366480182760802
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.14400258594298665
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.1440572346560657
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.14425379378192218
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.1442442135658206
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.14424093381827136
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.14422529749572277
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.1443732069695697
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.14404080383652865
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.14402900302204594
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.14405416688797149
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.1441456386714839
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.14410215144356092
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.14421134785963938
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.14412583451232183
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.14402632952056904
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.14400189361990767
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.14378858153757296
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.14374340255744755
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.1437135030774726
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.14385734125971794
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.14395834392670429
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.14374928034842013
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.1437859689392666
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.14378631648187543
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.14383329342580536
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.1440780388477903
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.14387337664763133
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.14391209888008405
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.14402627666419912
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.1440762060659903
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.14408965600193094
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.14420181133530358
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.1438696296499656
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.1438168221419411
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.14342046898286953
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.14364901047788167
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.14367614537477494
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.14375701964158435
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.14387840752163503
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.14364346773442577
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.14380594223988155
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.14364759990324577
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.14373122543589142
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.1438380190949948
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.1437000024730597
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.14378580553156714
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.14411210244894027
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.14401790671168813
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.14419856274456488
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.14413014537421986
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.14386939730986145
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.14387926121170705
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.14387789499441175
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.14363766856717342
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.1436620470052375
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.14357968686676736
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.1436026969441661
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.14356097305083976
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.14353659150809267
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.14345447945853937
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.14354570327902869
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.1434248421873365
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.14344188253930273
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.1434250811875706
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.14358702781317118
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.14334304641104406
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.14351322219289583
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.14351148627800484
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.14331977849914915
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.14326624189679688
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.14325779936457642
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.14296018496155738
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.14301718371790767
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.14311871290402978
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.14324595779180527
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.14319223334843462
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.14300713803498977
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.14284515571899903
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.1427759029872858
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.14282350741986988
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.14274972433564048
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.14272842640057207
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.14285448813660545
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.14289461582163235
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.14295302706262086
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.14289633684405467
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.1427439851291252
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.14268646434129004
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.14273145402263024
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.14257619334828286
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.14246121432477907
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.14267641897587216
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.14269911192836818
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.14254082388482814
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.14240529805938631
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.14234852482532634
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.14247506175722394
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.14253486769104545
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.14280179589144928
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.14277414224121007
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.1428350262635247
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.14285301574402384
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.14268584718197092
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.1427937072630112
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.14285348001562181
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.14279566636390012
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.14277486112472174
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.14276594736723489
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.14282882807248418
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.14277841210206774
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.14283506434272836
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.14280669771526988
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.14281816690848137
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.14315349814326814
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.14319779561745688
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.14324537630087322
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.14326821989738026
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.1432599624124717
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.14302436269933197
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.14301142122859906
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.14300027291229622
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.14316541898995638
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.14305217805045162
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.1430079449005056
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.14306605130140418
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.14299958765360654
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.14296369737968212
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.1430197732468832
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.14289830326314135
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.14271053277815765
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.1426944565117074
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.14284363779283704
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.14284804060843317
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.14298201584309903
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.14306272804177422
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.14306765328222346
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.1430659608092419
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.14305676926893215
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.14301246878463553
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.14305327730167897
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.14313498819799728
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.1428846323016015
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.14289518816573588
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.14282968616834632
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.14291834874671672
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.14288588190850401
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.14297453843884997
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.14288159838951794
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.1428276733351699
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.14272166108875944
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.14272512319827185
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.14277849391750666
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.14282534906874486
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.14290977218027773
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.14285461559827747
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.1428280959908779
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.14275086471374998
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.14272026723976863
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.14273288154400854
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.14278103035538137
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.14259992022394635
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.1425875221689542
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.1425618662873739
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.14257793892021023
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.1425873951166255
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.1426001843370375
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.14263635983272474
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.14248990128195382
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.14256949390959645
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.1426724549382925
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.14262441369185008
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.14267126387357712
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.14268690763241743
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.14263378976592941
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.14264787427285913
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.14258452622205253
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.14263484448778863
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.14260332006961107
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.1425093720859127
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.14251921580050342
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.14262937078374693
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.14258902239111754
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.14264753866241353
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.1426468451173251
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.14250623706742385
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.14248671662062407
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.1424970253739717
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.1423805245947569
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.14244015406021912
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.14250986636685792
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.14241444319486618
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.14243049171787722
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.14243820251903852
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.14236813638468876
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.14229310077406984
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.14228727913250888
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.14212535421956668
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.14210667550239875
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.14219588732568797
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.14206629932569942
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.14205133509037743
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.14211183243564196
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.1420735337662103
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.14213573509919727
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.14206815076590426
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.14204145238642962
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.14220159770103924
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.14224291743932072
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.14229912645725423
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.1422281373395688
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.14214650631775905
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.14220082287130684
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.14221771020455048
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.14221519737006866
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.14229229357055428
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.14227664490946296
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.14228766378709826
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.14239577722509164
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.14237496462534574
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.14243592086854398
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.14240457027851539
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.1423115518440803
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.14228695133496758
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.14222002979244616
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.14222936737950487
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.14222163170281993
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.14232273431586437
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.14230210726070247
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.14222811371185104
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.14223172564011116
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.1422749952590967
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.14221726953983307
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.14215696677326006
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.14211118464859632
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.14215654401352612
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.14213020065028198
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.1423104633414556
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.14231853012608575
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.14231944220487253
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.1422922487060229
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.14228438144567243
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.14222457366995514
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.14219589081137352
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.14228898421022462
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.14226399814196786
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.14222773711806463
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.14229355660768656
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.1422794153171083
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.1423292523677196
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.14232230481759803
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.14235830393062174
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.14233140778360945
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.1422398964173844
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.14228464465543447
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.14229396312265424
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.1423182575824018
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.14228300158657245
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.14238130474197014
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.1423554075577846
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.1423139453992336
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.14228060142656343
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.1423363032148165
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.14234800829041389
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.14231164099877341
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.14234045447656782
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.14227883957413046
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.1423391928491385
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.1422813417594557
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.14232149114698087
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.1423270632994586
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.14226455510431169
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.14233176152620997
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.14232750574493
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.1423056518278
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.14228960907577456
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.1424182385921815
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.1423461178868589
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.1423504277435916
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.1423013175235075
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.142256053507994
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.14223334346807104
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.14220808177358574
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.14222286406316256
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.14221735920201348
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.14223717990492987
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.14222304338773528
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.14224083754297806
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.14229076701760943
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.14221855145384246
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.1422777666669825
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.1422844861096483
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.14242378167203953
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.14244536412854722
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.14242581346182412
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.14238977300417646
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.14237764477729797
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.1423400184313456
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.14231472414858798
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.14244114396110455
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.14245398957577962
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.14254744307661435
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.14257562097750212
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.14255802744016874
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.1425726159782934
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.1425791078930424
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.14257820967274407
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.14252407657248634
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.14253081201812146
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.14247246415809142
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.14253340225628355
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.14251445860130316
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.1425938406624855
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.142646113034252
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.14267795610868808
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.14267512065839524
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.1426401101052761
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.14266578824836998
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.14268641627271367
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.1426648581846235
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.1426476596775067
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.14267669713362716
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.1426342104561627
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.14274668866336496
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.14278831045648352
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.14274290605233267
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.14276174907710884
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.14276923745134731
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.14277852258732165
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.142760650586584
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.14267075695462672
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.14268920519605535
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.14268271990302134
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.1427722994029667
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.1427731714527873
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.14269429508407236
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.14260733687287366
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.1425663474633033
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.14256340277811083
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.14252402140296622
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.14251572333953597
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.1425128063644885
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.14248159831123694
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.14248339589375497
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.14251148670699923
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.14247157801620786
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.14249238222486005
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.1424767001411494
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.14242245318114477
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.1424095310811695
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.14241317483796695
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.1423884472681648
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.14246994827029316
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.14243269601097794
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.14247885741362418
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.14249615262823898
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.14249592492451316
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.14252646354765727
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.14260134321355492
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.14259324574020416
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.14262960713407766
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.1425964837227704
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.14260760432278569
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.14262833868632782
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.1426176792727067
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.14265608705126137
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.14263607425665534
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.14259794988993849
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.14261623396801307
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.14259666259456802
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.14257553740338022
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.14256129549911664
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.14253906360930868
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.14255738580504437
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.14251484018049937
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.14247812552759978
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.1423983774619743
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.14232402565060082
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.14233766283774585
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.14231919586919328
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.14227334294451896
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.14224084473188667
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.14222415052354337
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.14227619193445834
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.1422453394796683
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.14221958772603174
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.14226472787237887
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.14224847489467232
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.14228501812007294
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.14233655482530594
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.14233930623875216
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.1423304494160579
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.14224653958957245
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.1422471077420656
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.14217020317091275
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.14217020317091275 valid 0.18915019929409027
LOSS train 0.14217020317091275 valid 0.1980733498930931
LOSS train 0.14217020317091275 valid 0.2022817780574163
LOSS train 0.14217020317091275 valid 0.19239047542214394
LOSS train 0.14217020317091275 valid 0.19277069568634034
LOSS train 0.14217020317091275 valid 0.19777139276266098
LOSS train 0.14217020317091275 valid 0.19576056940214975
LOSS train 0.14217020317091275 valid 0.19396433979272842
LOSS train 0.14217020317091275 valid 0.1910983274380366
LOSS train 0.14217020317091275 valid 0.19139174669981002
LOSS train 0.14217020317091275 valid 0.19128120622851633
LOSS train 0.14217020317091275 valid 0.1956063968439897
LOSS train 0.14217020317091275 valid 0.19545754331808823
LOSS train 0.14217020317091275 valid 0.1946553321821349
LOSS train 0.14217020317091275 valid 0.1945928543806076
LOSS train 0.14217020317091275 valid 0.19624304864555597
LOSS train 0.14217020317091275 valid 0.19834759130197413
LOSS train 0.14217020317091275 valid 0.19882039642996258
LOSS train 0.14217020317091275 valid 0.20086495578289032
LOSS train 0.14217020317091275 valid 0.19997320622205733
LOSS train 0.14217020317091275 valid 0.20112872975213186
LOSS train 0.14217020317091275 valid 0.2002353539521044
LOSS train 0.14217020317091275 valid 0.1991071150354717
LOSS train 0.14217020317091275 valid 0.19987857776383558
LOSS train 0.14217020317091275 valid 0.19866108775138855
LOSS train 0.14217020317091275 valid 0.19782238797499582
LOSS train 0.14217020317091275 valid 0.19751976320037135
LOSS train 0.14217020317091275 valid 0.19776684099010058
LOSS train 0.14217020317091275 valid 0.19611359413327842
LOSS train 0.14217020317091275 valid 0.1962537149588267
LOSS train 0.14217020317091275 valid 0.19665872281597507
LOSS train 0.14217020317091275 valid 0.19705192418769002
LOSS train 0.14217020317091275 valid 0.1961701399449146
LOSS train 0.14217020317091275 valid 0.1952849863206639
LOSS train 0.14217020317091275 valid 0.19543503863470896
LOSS train 0.14217020317091275 valid 0.1956898888780011
LOSS train 0.14217020317091275 valid 0.1961492279896865
LOSS train 0.14217020317091275 valid 0.19639308123212113
LOSS train 0.14217020317091275 valid 0.19681546856195498
LOSS train 0.14217020317091275 valid 0.1962629295885563
LOSS train 0.14217020317091275 valid 0.1962679261114539
LOSS train 0.14217020317091275 valid 0.1972249902430035
LOSS train 0.14217020317091275 valid 0.19842035756554716
LOSS train 0.14217020317091275 valid 0.1978533389893445
LOSS train 0.14217020317091275 valid 0.1972822454240587
LOSS train 0.14217020317091275 valid 0.1962354047142941
LOSS train 0.14217020317091275 valid 0.19598902857049982
LOSS train 0.14217020317091275 valid 0.1972108824799458
LOSS train 0.14217020317091275 valid 0.19676561379919247
LOSS train 0.14217020317091275 valid 0.1968705940246582
LOSS train 0.14217020317091275 valid 0.19629102039570903
LOSS train 0.14217020317091275 valid 0.1961480129796725
LOSS train 0.14217020317091275 valid 0.1973157876505042
LOSS train 0.14217020317091275 valid 0.1974583116394502
LOSS train 0.14217020317091275 valid 0.1973328495567495
LOSS train 0.14217020317091275 valid 0.19718309864401817
LOSS train 0.14217020317091275 valid 0.19665115156717466
LOSS train 0.14217020317091275 valid 0.19705796678518428
LOSS train 0.14217020317091275 valid 0.19671387818910308
LOSS train 0.14217020317091275 valid 0.1963592750330766
LOSS train 0.14217020317091275 valid 0.19620619054700508
LOSS train 0.14217020317091275 valid 0.19582942536761683
LOSS train 0.14217020317091275 valid 0.1955912820403538
LOSS train 0.14217020317091275 valid 0.1957351635210216
LOSS train 0.14217020317091275 valid 0.19502491354942322
LOSS train 0.14217020317091275 valid 0.1950095733909896
LOSS train 0.14217020317091275 valid 0.1956070852813436
LOSS train 0.14217020317091275 valid 0.19494828099713607
LOSS train 0.14217020317091275 valid 0.19537771374419116
LOSS train 0.14217020317091275 valid 0.19556803213698523
LOSS train 0.14217020317091275 valid 0.19541039664140888
LOSS train 0.14217020317091275 valid 0.19591032920612228
LOSS train 0.14217020317091275 valid 0.19635331896069932
LOSS train 0.14217020317091275 valid 0.19608605854414604
LOSS train 0.14217020317091275 valid 0.1959723327557246
LOSS train 0.14217020317091275 valid 0.19584588431998304
LOSS train 0.14217020317091275 valid 0.1957872761534406
LOSS train 0.14217020317091275 valid 0.1955528492346788
LOSS train 0.14217020317091275 valid 0.19568554151661788
LOSS train 0.14217020317091275 valid 0.19519249722361565
LOSS train 0.14217020317091275 valid 0.19536692363980376
LOSS train 0.14217020317091275 valid 0.19556609359456273
LOSS train 0.14217020317091275 valid 0.19567623860146627
LOSS train 0.14217020317091275 valid 0.19539873142327582
LOSS train 0.14217020317091275 valid 0.19573945455691394
LOSS train 0.14217020317091275 valid 0.19566904164330903
LOSS train 0.14217020317091275 valid 0.19543283492669292
LOSS train 0.14217020317091275 valid 0.1953388423743573
LOSS train 0.14217020317091275 valid 0.19556767401400577
LOSS train 0.14217020317091275 valid 0.19554259263806872
LOSS train 0.14217020317091275 valid 0.19561901393827502
LOSS train 0.14217020317091275 valid 0.19552378793773445
LOSS train 0.14217020317091275 valid 0.19577368693326108
LOSS train 0.14217020317091275 valid 0.19590453454788695
LOSS train 0.14217020317091275 valid 0.1964365287830955
LOSS train 0.14217020317091275 valid 0.19727487303316593
LOSS train 0.14217020317091275 valid 0.19742559557108535
LOSS train 0.14217020317091275 valid 0.19804231974543357
LOSS train 0.14217020317091275 valid 0.19791902526460511
LOSS train 0.14217020317091275 valid 0.19805204927921294
LOSS train 0.14217020317091275 valid 0.1979116213203657
LOSS train 0.14217020317091275 valid 0.19825183103481928
LOSS train 0.14217020317091275 valid 0.19792822538648994
LOSS train 0.14217020317091275 valid 0.19780538976192474
LOSS train 0.14217020317091275 valid 0.19792776079404922
LOSS train 0.14217020317091275 valid 0.1977758931944955
LOSS train 0.14217020317091275 valid 0.19753112929446676
LOSS train 0.14217020317091275 valid 0.19765614731996148
LOSS train 0.14217020317091275 valid 0.19739572115994375
LOSS train 0.14217020317091275 valid 0.19720279303464022
LOSS train 0.14217020317091275 valid 0.1974144007440086
LOSS train 0.14217020317091275 valid 0.19760407001844474
LOSS train 0.14217020317091275 valid 0.19744886985394808
LOSS train 0.14217020317091275 valid 0.19753986008857427
LOSS train 0.14217020317091275 valid 0.19804670279440673
LOSS train 0.14217020317091275 valid 0.19781578422106547
LOSS train 0.14217020317091275 valid 0.19809072356448215
LOSS train 0.14217020317091275 valid 0.19786038578061735
LOSS train 0.14217020317091275 valid 0.1976273463052862
LOSS train 0.14217020317091275 valid 0.19775411734978357
LOSS train 0.14217020317091275 valid 0.19761553579125524
LOSS train 0.14217020317091275 valid 0.19791700581058128
LOSS train 0.14217020317091275 valid 0.19797835941237163
LOSS train 0.14217020317091275 valid 0.19827910344446859
LOSS train 0.14217020317091275 valid 0.19823763501644134
LOSS train 0.14217020317091275 valid 0.19832602923824674
LOSS train 0.14217020317091275 valid 0.19817838483438718
LOSS train 0.14217020317091275 valid 0.1982599652837962
LOSS train 0.14217020317091275 valid 0.198073861441871
LOSS train 0.14217020317091275 valid 0.19784855876977628
LOSS train 0.14217020317091275 valid 0.1980779756798999
LOSS train 0.14217020317091275 valid 0.1983788532741142
LOSS train 0.14217020317091275 valid 0.19842067394489632
LOSS train 0.14217020317091275 valid 0.19862365633694093
LOSS train 0.14217020317091275 valid 0.19866781764560276
LOSS train 0.14217020317091275 valid 0.19885539000525193
LOSS train 0.14217020317091275 valid 0.1988198154599127
LOSS train 0.14217020317091275 valid 0.19851276084132816
LOSS train 0.14217020317091275 valid 0.19841941237020835
LOSS train 0.14217020317091275 valid 0.1981503491955144
LOSS train 0.14217020317091275 valid 0.19837570095315893
LOSS train 0.14217020317091275 valid 0.19865647514521237
LOSS train 0.14217020317091275 valid 0.19880597741453798
LOSS train 0.14217020317091275 valid 0.19859732997914156
LOSS train 0.14217020317091275 valid 0.19844476190106622
LOSS train 0.14217020317091275 valid 0.1984053225549933
LOSS train 0.14217020317091275 valid 0.19841312885689896
LOSS train 0.14217020317091275 valid 0.19823585561401136
LOSS train 0.14217020317091275 valid 0.1984380885658648
LOSS train 0.14217020317091275 valid 0.19834857036670048
LOSS train 0.14217020317091275 valid 0.19825254222809874
LOSS train 0.14217020317091275 valid 0.19823134778753707
LOSS train 0.14217020317091275 valid 0.19795231715916028
LOSS train 0.14217020317091275 valid 0.19790605329847955
LOSS train 0.14217020317091275 valid 0.1978280079941596
LOSS train 0.14217020317091275 valid 0.19784944342114988
LOSS train 0.14217020317091275 valid 0.1975598247947207
LOSS train 0.14217020317091275 valid 0.19746453767713112
LOSS train 0.14217020317091275 valid 0.19779590943699363
LOSS train 0.14217020317091275 valid 0.19780346620827913
LOSS train 0.14217020317091275 valid 0.1977610455721802
LOSS train 0.14217020317091275 valid 0.19767250037855572
LOSS train 0.14217020317091275 valid 0.19749416886297472
LOSS train 0.14217020317091275 valid 0.1974420721029363
LOSS train 0.14217020317091275 valid 0.19762577351295588
LOSS train 0.14217020317091275 valid 0.19748095952602754
LOSS train 0.14217020317091275 valid 0.1975543982611445
LOSS train 0.14217020317091275 valid 0.1973913831725007
LOSS train 0.14217020317091275 valid 0.19728716493710963
LOSS train 0.14217020317091275 valid 0.1972659691291697
LOSS train 0.14217020317091275 valid 0.19713439378473493
LOSS train 0.14217020317091275 valid 0.19736219171521274
LOSS train 0.14217020317091275 valid 0.1973275105560446
LOSS train 0.14217020317091275 valid 0.19756569405054225
LOSS train 0.14217020317091275 valid 0.1973790873800005
LOSS train 0.14217020317091275 valid 0.19732963463122194
LOSS train 0.14217020317091275 valid 0.19732532182992515
LOSS train 0.14217020317091275 valid 0.19735719020781892
LOSS train 0.14217020317091275 valid 0.19738112598158128
LOSS train 0.14217020317091275 valid 0.19734780473841562
LOSS train 0.14217020317091275 valid 0.19728545087147814
LOSS train 0.14217020317091275 valid 0.19721945309704478
LOSS train 0.14217020317091275 valid 0.19744207176680123
LOSS train 0.14217020317091275 valid 0.19751057360807192
LOSS train 0.14217020317091275 valid 0.19747440452511247
LOSS train 0.14217020317091275 valid 0.19756900198677535
LOSS train 0.14217020317091275 valid 0.19745977597440628
LOSS train 0.14217020317091275 valid 0.19749233752806136
LOSS train 0.14217020317091275 valid 0.19721970750541284
LOSS train 0.14217020317091275 valid 0.1972452757389922
LOSS train 0.14217020317091275 valid 0.1972710217168818
LOSS train 0.14217020317091275 valid 0.19735093244041005
LOSS train 0.14217020317091275 valid 0.19707875810756584
LOSS train 0.14217020317091275 valid 0.19710980609212955
LOSS train 0.14217020317091275 valid 0.196983534632585
LOSS train 0.14217020317091275 valid 0.19718087616623664
LOSS train 0.14217020317091275 valid 0.19726089399478158
LOSS train 0.14217020317091275 valid 0.19747501092426706
LOSS train 0.14217020317091275 valid 0.19747852350599202
LOSS train 0.14217020317091275 valid 0.19778684720396997
LOSS train 0.14217020317091275 valid 0.19768690432778638
LOSS train 0.14217020317091275 valid 0.19776275222844417
LOSS train 0.14217020317091275 valid 0.19776079535777932
LOSS train 0.14217020317091275 valid 0.197568632881431
LOSS train 0.14217020317091275 valid 0.19742344705069936
LOSS train 0.14217020317091275 valid 0.19735288851469465
LOSS train 0.14217020317091275 valid 0.19732838595546962
LOSS train 0.14217020317091275 valid 0.19720443825309092
LOSS train 0.14217020317091275 valid 0.1972489172334306
LOSS train 0.14217020317091275 valid 0.19744263057197844
LOSS train 0.14217020317091275 valid 0.1976962905218251
LOSS train 0.14217020317091275 valid 0.19747140371012237
LOSS train 0.14217020317091275 valid 0.19737140617460153
LOSS train 0.14217020317091275 valid 0.19727739406244776
LOSS train 0.14217020317091275 valid 0.19701965194801951
LOSS train 0.14217020317091275 valid 0.19675285265677506
LOSS train 0.14217020317091275 valid 0.19666858216584554
LOSS train 0.14217020317091275 valid 0.19671100289176363
LOSS train 0.14217020317091275 valid 0.1966385839328374
LOSS train 0.14217020317091275 valid 0.19661047580567273
LOSS train 0.14217020317091275 valid 0.19662859665052923
LOSS train 0.14217020317091275 valid 0.19676382812830778
LOSS train 0.14217020317091275 valid 0.1967593956020381
LOSS train 0.14217020317091275 valid 0.19667384141523922
LOSS train 0.14217020317091275 valid 0.19641315884060329
LOSS train 0.14217020317091275 valid 0.19621665386763293
LOSS train 0.14217020317091275 valid 0.19625995377086858
LOSS train 0.14217020317091275 valid 0.19630257628465952
LOSS train 0.14217020317091275 valid 0.1962160639887814
LOSS train 0.14217020317091275 valid 0.19634623812592547
LOSS train 0.14217020317091275 valid 0.1962973727034284
LOSS train 0.14217020317091275 valid 0.1964864517594206
LOSS train 0.14217020317091275 valid 0.19674154949802186
LOSS train 0.14217020317091275 valid 0.1967769772069067
LOSS train 0.14217020317091275 valid 0.19682811352800816
LOSS train 0.14217020317091275 valid 0.1966934859500093
LOSS train 0.14217020317091275 valid 0.1967810298068614
LOSS train 0.14217020317091275 valid 0.19671023092350037
LOSS train 0.14217020317091275 valid 0.1966697934032983
LOSS train 0.14217020317091275 valid 0.19629937230298916
LOSS train 0.14217020317091275 valid 0.1961913278241375
LOSS train 0.14217020317091275 valid 0.19614548907299673
LOSS train 0.14217020317091275 valid 0.19613270222404858
LOSS train 0.14217020317091275 valid 0.19611897093595052
LOSS train 0.14217020317091275 valid 0.19633734426936325
LOSS train 0.14217020317091275 valid 0.19644728720915028
LOSS train 0.14217020317091275 valid 0.19647266429204208
LOSS train 0.14217020317091275 valid 0.1963973551868431
LOSS train 0.14217020317091275 valid 0.1963058240322703
LOSS train 0.14217020317091275 valid 0.19635516542196274
LOSS train 0.14217020317091275 valid 0.1963887168947919
LOSS train 0.14217020317091275 valid 0.19638129471549912
LOSS train 0.14217020317091275 valid 0.19624446862535513
LOSS train 0.14217020317091275 valid 0.19629644215341627
LOSS train 0.14217020317091275 valid 0.19619649280520046
LOSS train 0.14217020317091275 valid 0.19604675192385912
LOSS train 0.14217020317091275 valid 0.19592838435785315
LOSS train 0.14217020317091275 valid 0.19608214439809785
LOSS train 0.14217020317091275 valid 0.19606150013599616
LOSS train 0.14217020317091275 valid 0.19602837533904957
LOSS train 0.14217020317091275 valid 0.1960262794946802
LOSS train 0.14217020317091275 valid 0.19617773660721669
LOSS train 0.14217020317091275 valid 0.19608599196822019
LOSS train 0.14217020317091275 valid 0.19606921330771662
LOSS train 0.14217020317091275 valid 0.19613608746033795
LOSS train 0.14217020317091275 valid 0.19624065495747373
LOSS train 0.14217020317091275 valid 0.19614722018831232
LOSS train 0.14217020317091275 valid 0.19625216711368135
LOSS train 0.14217020317091275 valid 0.19625905139517164
LOSS train 0.14217020317091275 valid 0.19611253479012736
LOSS train 0.14217020317091275 valid 0.1960403515074086
LOSS train 0.14217020317091275 valid 0.1959573783199577
LOSS train 0.14217020317091275 valid 0.19582232658243004
LOSS train 0.14217020317091275 valid 0.19577457260911482
LOSS train 0.14217020317091275 valid 0.19587541141293266
LOSS train 0.14217020317091275 valid 0.19598593832789987
LOSS train 0.14217020317091275 valid 0.19588413861469242
LOSS train 0.14217020317091275 valid 0.1958522818071379
LOSS train 0.14217020317091275 valid 0.19580890040671098
LOSS train 0.14217020317091275 valid 0.19572190503988948
LOSS train 0.14217020317091275 valid 0.19560851575641022
LOSS train 0.14217020317091275 valid 0.19557025574200543
LOSS train 0.14217020317091275 valid 0.1954810036997913
LOSS train 0.14217020317091275 valid 0.19535080332991103
LOSS train 0.14217020317091275 valid 0.19537797314033173
LOSS train 0.14217020317091275 valid 0.19534548648170658
LOSS train 0.14217020317091275 valid 0.1954713883300276
LOSS train 0.14217020317091275 valid 0.19539925813054046
LOSS train 0.14217020317091275 valid 0.1953397947508571
LOSS train 0.14217020317091275 valid 0.19519875383582608
LOSS train 0.14217020317091275 valid 0.1951292241664277
LOSS train 0.14217020317091275 valid 0.1950999058476866
LOSS train 0.14217020317091275 valid 0.19507942295318578
LOSS train 0.14217020317091275 valid 0.19506566069361303
LOSS train 0.14217020317091275 valid 0.19511409573635813
LOSS train 0.14217020317091275 valid 0.19503108160318555
LOSS train 0.14217020317091275 valid 0.1949689922107992
LOSS train 0.14217020317091275 valid 0.19497217507970413
LOSS train 0.14217020317091275 valid 0.19499727714819254
LOSS train 0.14217020317091275 valid 0.1950151332716147
LOSS train 0.14217020317091275 valid 0.19495260071912873
LOSS train 0.14217020317091275 valid 0.19487838639526178
LOSS train 0.14217020317091275 valid 0.19489177476258168
LOSS train 0.14217020317091275 valid 0.1948523645926463
LOSS train 0.14217020317091275 valid 0.19475500256311698
LOSS train 0.14217020317091275 valid 0.19494239260363422
LOSS train 0.14217020317091275 valid 0.19499070034741578
LOSS train 0.14217020317091275 valid 0.1950479724093691
LOSS train 0.14217020317091275 valid 0.19499245502995055
LOSS train 0.14217020317091275 valid 0.19498281286608787
LOSS train 0.14217020317091275 valid 0.19503599558612542
LOSS train 0.14217020317091275 valid 0.19498406188228193
LOSS train 0.14217020317091275 valid 0.19486134887312928
LOSS train 0.14217020317091275 valid 0.1947494778948225
LOSS train 0.14217020317091275 valid 0.19467670056555006
LOSS train 0.14217020317091275 valid 0.19461958581888222
LOSS train 0.14217020317091275 valid 0.1946496626462094
LOSS train 0.14217020317091275 valid 0.19469890855955627
LOSS train 0.14217020317091275 valid 0.19468403306313817
LOSS train 0.14217020317091275 valid 0.19454165236093104
LOSS train 0.14217020317091275 valid 0.19459540037165549
LOSS train 0.14217020317091275 valid 0.19447606237408538
LOSS train 0.14217020317091275 valid 0.19439923943196288
LOSS train 0.14217020317091275 valid 0.19436780188554598
LOSS train 0.14217020317091275 valid 0.19421405443778406
LOSS train 0.14217020317091275 valid 0.19433217472825315
LOSS train 0.14217020317091275 valid 0.19423730808113693
LOSS train 0.14217020317091275 valid 0.1942494252651203
LOSS train 0.14217020317091275 valid 0.19428894014344028
LOSS train 0.14217020317091275 valid 0.19434730261564254
LOSS train 0.14217020317091275 valid 0.19426317395940648
LOSS train 0.14217020317091275 valid 0.1942430724311306
LOSS train 0.14217020317091275 valid 0.19424674841197762
LOSS train 0.14217020317091275 valid 0.1941621051041666
LOSS train 0.14217020317091275 valid 0.19399976805964514
LOSS train 0.14217020317091275 valid 0.1939251586528761
LOSS train 0.14217020317091275 valid 0.19403180798366446
LOSS train 0.14217020317091275 valid 0.19403303308776143
LOSS train 0.14217020317091275 valid 0.1939555015360008
LOSS train 0.14217020317091275 valid 0.19406888910076198
LOSS train 0.14217020317091275 valid 0.19406062395167142
LOSS train 0.14217020317091275 valid 0.19405822473311285
LOSS train 0.14217020317091275 valid 0.1939358233797307
LOSS train 0.14217020317091275 valid 0.19404875820632592
LOSS train 0.14217020317091275 valid 0.1941135197877884
LOSS train 0.14217020317091275 valid 0.19407969187794394
LOSS train 0.14217020317091275 valid 0.1940288404136982
LOSS train 0.14217020317091275 valid 0.194016901669146
LOSS train 0.14217020317091275 valid 0.19402964780357984
LOSS train 0.14217020317091275 valid 0.19406603949410575
LOSS train 0.14217020317091275 valid 0.19414349222964372
LOSS train 0.14217020317091275 valid 0.19420610550283032
LOSS train 0.14217020317091275 valid 0.19427140154176664
LOSS train 0.14217020317091275 valid 0.1942609521628773
LOSS train 0.14217020317091275 valid 0.19414134982606054
LOSS train 0.14217020317091275 valid 0.1941396678180507
LOSS train 0.14217020317091275 valid 0.19407571242803953
LOSS train 0.14217020317091275 valid 0.19407774383129353
LOSS train 0.14217020317091275 valid 0.19401590231401342
LOSS train 0.14217020317091275 valid 0.19424713626503945
LOSS train 0.14217020317091275 valid 0.1941442425231194
LOSS train 0.14217020317091275 valid 0.19423790016720965
LOSS train 0.14217020317091275 valid 0.1942549203381394
LOSS train 0.14217020317091275 valid 0.19416548966706454
LOSS train 0.14217020317091275 valid 0.19419524183828538
LOSS train 0.14217020317091275 valid 0.19416219785890945
LOSS train 0.14217020317091275 valid 0.19414219974821856
LOSS train 0.14217020317091275 valid 0.19405084803862416
LOSS train 0.14217020317091275 valid 0.1941152560678601
EPOCH 5:
  batch 1 loss: 0.1563258022069931
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.13682182505726814
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.12985404829184213
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.12864597514271736
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.13369129598140717
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.13478261729081473
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.13710208662918635
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.13395515643060207
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.13407109512223137
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.1318395033478737
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.13268134133382278
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.13601167127490044
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.13585765659809113
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.13418123072811536
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.13320157825946807
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.1355285383760929
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.13617298182319193
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.13723667297098371
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.13821152166316383
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.13733073510229588
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.1368769212138085
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.13708675889806313
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.13527256315169128
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.13477929371098676
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.13490740239620208
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.13720147197063154
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.13771732206697818
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.13800007849931717
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.13915770012756873
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.13886494239171346
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.13876240243834834
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.13766712858341634
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.13815878263928674
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.13858767007203662
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.13867092324154717
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.13963470421731472
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.14066361072095665
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.13997547152011017
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.13926466898276255
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.13944884557276965
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.13865337593526375
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.13859437494760468
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.13872405106938163
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.13894835131412203
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.13859563238090938
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.13856641656678656
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.13852808101380126
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.13877380142609277
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.13899150673224
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.13892812967300416
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.1386013904622957
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.13775577740027353
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.1376453882680749
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.13747432441623123
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.13737277605316855
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.13771255979580538
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.13835565719688148
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.1380790782899692
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.13879658206034515
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.1394716719786326
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.13990890100354053
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.13986839113696928
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.13971124755011666
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.13966882275417447
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.1396470709488942
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.13943098891865124
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.13973113205005874
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.13970976454370163
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.13948146113450977
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.13947055637836456
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.13966513582518403
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.13943813223805693
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.13987692843561303
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.14017084220776688
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.1399681289990743
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.14003335213974902
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.14032552033275753
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.14034889829464448
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.1406521259606639
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.14071985092014075
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.14087177481916216
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.1408853845261946
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.14086512036352272
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.14089106165227436
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.14114515728810254
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.14078933511709057
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.1407859509532479
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.14082293102348392
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.1409444905231508
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.140899645537138
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.14101881370112135
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.14093347776519216
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.14085930129212718
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.14087164473343403
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.14062763585856086
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.14059776603244245
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.14052652828779416
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.1406709620995181
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.1407935158020318
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.14054727524518967
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.14056766903636478
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.14056977600443596
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.14057720604452115
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.14085194382529992
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.14064116740510577
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.14069644987302007
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.1408063438590442
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.14083117143147522
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.14082706623941388
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.14094845727086067
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.1405840915468362
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.14048105870772684
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.14009889729518807
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.14032823063041033
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.14033721458652745
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.1404542575493969
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.1405751975810426
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.14035297987067094
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.1405225885139794
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.14033300038427116
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.14041164572820192
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.14045480950201145
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.1403382830624658
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.14044309377429948
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.14080262523889542
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.14072125245417869
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.14091326665925227
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.14085972000611946
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.14059605934592181
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.1405716206591863
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.14053807134619195
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.14024965216716131
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.14027262663930878
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.1401906718513859
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.14021939558011515
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.14017042210873434
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.14016764377155444
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.14009627959002618
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.14020431920778836
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.14008888628865993
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.14012252116668308
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.1401197307441436
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.14029837180892904
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.1400266618260907
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.140191671303634
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.14020376371806614
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.14000903363941478
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.13997878385959445
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.13997513005797496
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.13966188569863638
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.13974430022255474
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.13986447394678467
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.1399877572955649
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.13992974384651555
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.13975641717833856
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.13961364935414913
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.13955974735461982
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.13962099554983876
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.13951921434897296
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.13951232330873609
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.13959268142717965
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.13962832893486377
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.13966558398278944
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.13959847154413782
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.13943535364938506
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.13933915209518857
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.1394028749740766
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.13923822959796303
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.1391095119644199
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.13930129128343918
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.13933759640183366
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.13918127877594427
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.13902363926172256
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.13898398774279946
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.13911127750362667
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.13914611745117741
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.13941362049788406
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.13937677522556166
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.13944180447129564
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.13944613954259288
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.1392651109323317
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.13937127823521803
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.13944058995592137
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.13936085363283104
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.13934960377377434
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.1393607391384981
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.1394261020947905
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.1393886676573373
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.13944484675845142
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.13942457464964766
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.139427472510575
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.1397541514985884
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.1397837287326551
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.13983425094755655
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.13985982136084482
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.1398549662636859
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.13961602040202484
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.13961089684656172
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.13959485842804215
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.13977046575397253
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.13965341170777135
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.13961579978908642
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.13966449988739832
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.13960264721775756
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.13956438546500555
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.13963628484352122
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.13949980284424796
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.13931818066451412
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.13929993395457427
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.1394795669331437
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.13949581878303915
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.139633701059897
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.13971637913458784
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.1397355007025126
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.13971758010082466
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.139701289538708
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.13964944902790308
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.13971437975217443
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.13978891064451165
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.13953614275563847
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.139546722375969
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.13947491156491074
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.13959191348654273
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.13956862797827593
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.13965332213375303
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.13954943148172008
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.1395016606683773
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.13939586438630758
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.13939030253731008
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.13941988640505334
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.1394829359147456
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.13957678732173195
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.13951094707578038
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.13948801398659363
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.13941446438114694
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.1393823916313507
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.1393964233717838
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.139459602503466
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.13929424098221327
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.13930397490039467
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.13928407798540543
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.13928594443300538
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.13928716218888515
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.1393041333153111
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.13932577855124764
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.13917610158280627
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.13925289920708428
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.13935572798213652
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.13931053224577003
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.13935507744550704
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.13937364245553416
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.1393301941690937
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.13931886262808862
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.13924042440540207
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.13927000912965512
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.13923211517976597
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.1391365190076457
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.13914181303608325
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.13924759609128518
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.13920991483789225
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.13928159203565899
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.13928099073299016
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.13913103816287145
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.1391121025733424
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.13912173304355369
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.13900964726742945
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.13907462852389624
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.13913436659347655
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.13903602675212803
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.1390626378081463
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.13907083440090898
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.13900120304349592
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.13891895038959307
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.13892208415008808
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.13874229349873282
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.13871073123553526
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.13879443812671552
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.13867110308768937
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.13866977949082637
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.1387358499424798
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.13869949252579986
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.1387471211083392
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.13867104674817818
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.13863516793074743
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.13881047603331115
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.13884312049909073
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.13889974926822277
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.13883938265240026
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.1387555278760339
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.1388137988232333
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.13883791702617074
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.13883996497176282
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.1389284910246374
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.13891355766832422
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.13893929813372888
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.13905011920409427
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.13904091683200714
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.13908458213578134
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.13904879860156355
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.13894942834973334
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.13891719177711842
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.13885173406328585
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.13886987555636823
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.13886549507610893
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.1389615394785756
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.1389489321196391
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.13886931460921073
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.1388824704986114
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.13891768604999222
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.13885755784088566
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.13879654769728805
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.13873144425451756
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.1387706929788041
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.13875832138167823
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.13894633612935506
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.13894549771388875
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.13895667562913444
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.13893921242195106
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.1389247846547339
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.1388718250906095
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.13884293979666315
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.13895186960049297
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.13893569648358106
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.13891135360815643
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.1389616110920906
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.13893231980365478
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.1389900172750155
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.13896695613043336
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.13899802792488986
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.13895609706188694
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.1388636194957347
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.1389023519783135
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.13891904946562048
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.1389350582382636
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.1388630663058651
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.13896382216452843
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.1389215457908124
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.1388826151350899
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.13886856503103334
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.13895150885424193
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.13897224174828823
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.1389315426567493
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.1389666115620741
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.13890383988185678
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.13896884242261665
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.1389075781224091
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.1389614073962231
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.13896729514516634
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.13891207527123756
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.13897741500820432
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.13897796497385725
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.13894623827019875
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.13891999841411795
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.13905401304785142
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.13896799360362577
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.1389694531144721
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.13892097702296843
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.13888126574248574
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.1388673218262893
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.13883700797127352
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.1388432692705429
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.1388278074603713
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.13883966474329473
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.1388323532601634
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.13884459235896804
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.13888270167705138
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.13882623269788577
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.1388742989131614
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.13888042448416635
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.1389993092699631
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.13902099946679453
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.13901093760405175
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.13899055428942791
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.1389780873084132
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.138953730960687
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.13892421152125647
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.1390344603505944
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.1390480868833721
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.13914501094959655
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.13918115035875847
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.13915528042188155
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.13917324487217433
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.1391825712730928
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.13918374108228213
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.1391336434266784
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.13913285305694595
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.13906104556580848
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.13911906286075557
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.1390901406925258
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.1391720049465314
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.1392228466356197
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.13924692911381015
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.13925347272448868
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.13921912950534507
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.13924837868802156
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.13927263599077258
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.13925224266739877
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.13923651659533606
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.13926057698657937
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.13922092908993364
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.139346531891912
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.13939578618057333
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.13935820959859688
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.1393741947405114
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.13937460594339135
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.13940193505930196
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.13939485497571327
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.13931678279357798
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.139332841507963
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.13931565702688403
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.13940225237042364
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.1394019624923618
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.1393230834538365
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.13924304208749735
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.13920577434172113
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.13920796411828354
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.13916925957305826
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.13915839338559283
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.13916061989305128
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.13912549096913565
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.13912524413892605
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.13915354984490227
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.13912043183266976
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.13914595684915218
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.13913228943067438
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.13908966669174427
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.13907516114326496
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.1390914601909223
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.13906903870594808
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.1391502007842064
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.13911503363236474
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.1391538846547957
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.1391785179907951
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.13918512166919797
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.13921666111069164
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.13927640605273597
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.13925940841777373
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.13930549183392635
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.13927304653073228
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.13929819291965528
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.1393194892306447
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.13931863057127905
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.1393541872635921
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.13933669460249376
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.13929638864284152
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.13932694122195244
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.13930979806104762
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.13929610155589348
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.13928540401110936
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.13926216428478558
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.1392907549001425
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.13925690967095108
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.13922108920336296
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.1391426018287432
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.1390630263384882
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.1390886434370227
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.13907001702282049
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.13903514425957567
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.13899807073581713
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.13898924939010454
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.13904521325017263
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.13902275386826818
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.13900659846435612
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.13904356009102073
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.13903791712176417
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.13907751984606484
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.1391364216995954
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.13914181298425055
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.13912673002239992
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.1390431210557197
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.13905215742671564
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.13898511212003434
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.13898511212003434 valid 0.1904398649930954
LOSS train 0.13898511212003434 valid 0.19967709481716156
LOSS train 0.13898511212003434 valid 0.20253599683443704
LOSS train 0.13898511212003434 valid 0.19206825271248817
LOSS train 0.13898511212003434 valid 0.19159031808376312
LOSS train 0.13898511212003434 valid 0.1962806930144628
LOSS train 0.13898511212003434 valid 0.1948871612548828
LOSS train 0.13898511212003434 valid 0.19378879107534885
LOSS train 0.13898511212003434 valid 0.19134363532066345
LOSS train 0.13898511212003434 valid 0.19165805876255035
LOSS train 0.13898511212003434 valid 0.19168602065606552
LOSS train 0.13898511212003434 valid 0.19576730330785116
LOSS train 0.13898511212003434 valid 0.1956903602068241
LOSS train 0.13898511212003434 valid 0.19494216782706125
LOSS train 0.13898511212003434 valid 0.19486408233642577
LOSS train 0.13898511212003434 valid 0.19659072253853083
LOSS train 0.13898511212003434 valid 0.19857045131571152
LOSS train 0.13898511212003434 valid 0.19879374321964052
LOSS train 0.13898511212003434 valid 0.20070113869089828
LOSS train 0.13898511212003434 valid 0.1998433403670788
LOSS train 0.13898511212003434 valid 0.20088903322106316
LOSS train 0.13898511212003434 valid 0.20011024922132492
LOSS train 0.13898511212003434 valid 0.19906099335007046
LOSS train 0.13898511212003434 valid 0.19986666863163313
LOSS train 0.13898511212003434 valid 0.19884704172611237
LOSS train 0.13898511212003434 valid 0.19800471514463425
LOSS train 0.13898511212003434 valid 0.19771880132180672
LOSS train 0.13898511212003434 valid 0.19795540400913783
LOSS train 0.13898511212003434 valid 0.19626575605622654
LOSS train 0.13898511212003434 valid 0.19634900540113448
LOSS train 0.13898511212003434 valid 0.19684437542192398
LOSS train 0.13898511212003434 valid 0.1972254212014377
LOSS train 0.13898511212003434 valid 0.19630182466723703
LOSS train 0.13898511212003434 valid 0.1954674672554521
LOSS train 0.13898511212003434 valid 0.19557960075991496
LOSS train 0.13898511212003434 valid 0.1958506686819924
LOSS train 0.13898511212003434 valid 0.19626206844239621
LOSS train 0.13898511212003434 valid 0.19660017090408424
LOSS train 0.13898511212003434 valid 0.19698996956531817
LOSS train 0.13898511212003434 valid 0.1964358977973461
LOSS train 0.13898511212003434 valid 0.19636380236323286
LOSS train 0.13898511212003434 valid 0.1972884667061624
LOSS train 0.13898511212003434 valid 0.19839378602282945
LOSS train 0.13898511212003434 valid 0.19787960770455273
LOSS train 0.13898511212003434 valid 0.19724801613224877
LOSS train 0.13898511212003434 valid 0.19623442771642105
LOSS train 0.13898511212003434 valid 0.1960200752983702
LOSS train 0.13898511212003434 valid 0.19732179337491593
LOSS train 0.13898511212003434 valid 0.1969750718194611
LOSS train 0.13898511212003434 valid 0.19708766758441926
LOSS train 0.13898511212003434 valid 0.19658524294694266
LOSS train 0.13898511212003434 valid 0.19635718860305273
LOSS train 0.13898511212003434 valid 0.19756804462873712
LOSS train 0.13898511212003434 valid 0.19764401763677597
LOSS train 0.13898511212003434 valid 0.19748675877397712
LOSS train 0.13898511212003434 valid 0.19738300144672394
LOSS train 0.13898511212003434 valid 0.19687602760498984
LOSS train 0.13898511212003434 valid 0.19733946821812925
LOSS train 0.13898511212003434 valid 0.197007427023629
LOSS train 0.13898511212003434 valid 0.1966361592213313
LOSS train 0.13898511212003434 valid 0.1964585253449737
LOSS train 0.13898511212003434 valid 0.19612319839577522
LOSS train 0.13898511212003434 valid 0.19587375554773542
LOSS train 0.13898511212003434 valid 0.19600830622948706
LOSS train 0.13898511212003434 valid 0.19533240107389596
LOSS train 0.13898511212003434 valid 0.1952973920287508
LOSS train 0.13898511212003434 valid 0.19583280162135167
LOSS train 0.13898511212003434 valid 0.19517079149098956
LOSS train 0.13898511212003434 valid 0.19564726408840954
LOSS train 0.13898511212003434 valid 0.19583916004214968
LOSS train 0.13898511212003434 valid 0.1956846556613143
LOSS train 0.13898511212003434 valid 0.19614401045772764
LOSS train 0.13898511212003434 valid 0.19659072650622014
LOSS train 0.13898511212003434 valid 0.19635472366133253
LOSS train 0.13898511212003434 valid 0.1962466871738434
LOSS train 0.13898511212003434 valid 0.19614959841496066
LOSS train 0.13898511212003434 valid 0.196074066804601
LOSS train 0.13898511212003434 valid 0.19580067082857475
LOSS train 0.13898511212003434 valid 0.19589585631708556
LOSS train 0.13898511212003434 valid 0.19540580194443463
LOSS train 0.13898511212003434 valid 0.19559327448591773
LOSS train 0.13898511212003434 valid 0.19581967410517903
LOSS train 0.13898511212003434 valid 0.19591531319072447
LOSS train 0.13898511212003434 valid 0.19560901306214787
LOSS train 0.13898511212003434 valid 0.1959103079403148
LOSS train 0.13898511212003434 valid 0.19583705414173214
LOSS train 0.13898511212003434 valid 0.19560643944932127
LOSS train 0.13898511212003434 valid 0.19553332623432984
LOSS train 0.13898511212003434 valid 0.19574209260806608
LOSS train 0.13898511212003434 valid 0.1957132516635789
LOSS train 0.13898511212003434 valid 0.1958002690430526
LOSS train 0.13898511212003434 valid 0.1956963922990405
LOSS train 0.13898511212003434 valid 0.19592272994979734
LOSS train 0.13898511212003434 valid 0.19608053977185108
LOSS train 0.13898511212003434 valid 0.1966360707032053
LOSS train 0.13898511212003434 valid 0.197446020009617
LOSS train 0.13898511212003434 valid 0.19760651241258248
LOSS train 0.13898511212003434 valid 0.19822580762663666
LOSS train 0.13898511212003434 valid 0.19813684170896356
LOSS train 0.13898511212003434 valid 0.1982848784327507
LOSS train 0.13898511212003434 valid 0.1981530382786647
LOSS train 0.13898511212003434 valid 0.19851682276702395
LOSS train 0.13898511212003434 valid 0.19820059589969302
LOSS train 0.13898511212003434 valid 0.19808606459544256
LOSS train 0.13898511212003434 valid 0.19817996606940314
LOSS train 0.13898511212003434 valid 0.1980142135102794
LOSS train 0.13898511212003434 valid 0.19776014691201327
LOSS train 0.13898511212003434 valid 0.1978830539241985
LOSS train 0.13898511212003434 valid 0.1976238639529692
LOSS train 0.13898511212003434 valid 0.19741690348495136
LOSS train 0.13898511212003434 valid 0.19763068626592825
LOSS train 0.13898511212003434 valid 0.19781651001955783
LOSS train 0.13898511212003434 valid 0.1976244938848293
LOSS train 0.13898511212003434 valid 0.1977137791221602
LOSS train 0.13898511212003434 valid 0.19819807060386824
LOSS train 0.13898511212003434 valid 0.197955375858422
LOSS train 0.13898511212003434 valid 0.1982414602723896
LOSS train 0.13898511212003434 valid 0.19803450547032436
LOSS train 0.13898511212003434 valid 0.19780784382038757
LOSS train 0.13898511212003434 valid 0.19792473862568538
LOSS train 0.13898511212003434 valid 0.19776599490938107
LOSS train 0.13898511212003434 valid 0.19802872006033287
LOSS train 0.13898511212003434 valid 0.19810752013349922
LOSS train 0.13898511212003434 valid 0.19840381438693694
LOSS train 0.13898511212003434 valid 0.1983554298877716
LOSS train 0.13898511212003434 valid 0.19842316092006743
LOSS train 0.13898511212003434 valid 0.19826487187794814
LOSS train 0.13898511212003434 valid 0.1983229898614809
LOSS train 0.13898511212003434 valid 0.19815261026685552
LOSS train 0.13898511212003434 valid 0.1979470431804657
LOSS train 0.13898511212003434 valid 0.19812877601339618
LOSS train 0.13898511212003434 valid 0.19843533352920503
LOSS train 0.13898511212003434 valid 0.19847008697968677
LOSS train 0.13898511212003434 valid 0.19868069097622118
LOSS train 0.13898511212003434 valid 0.1987449981548168
LOSS train 0.13898511212003434 valid 0.19893363403046832
LOSS train 0.13898511212003434 valid 0.19890228251036066
LOSS train 0.13898511212003434 valid 0.1986023359324621
LOSS train 0.13898511212003434 valid 0.19850728912748022
LOSS train 0.13898511212003434 valid 0.19822799552764211
LOSS train 0.13898511212003434 valid 0.19845355233402118
LOSS train 0.13898511212003434 valid 0.1987048831414169
LOSS train 0.13898511212003434 valid 0.1988671308422422
LOSS train 0.13898511212003434 valid 0.1986462367284629
LOSS train 0.13898511212003434 valid 0.19850005949365682
LOSS train 0.13898511212003434 valid 0.19843187703661722
LOSS train 0.13898511212003434 valid 0.1984216323515185
LOSS train 0.13898511212003434 valid 0.19824005653326576
LOSS train 0.13898511212003434 valid 0.1984359592999388
LOSS train 0.13898511212003434 valid 0.19832845737536747
LOSS train 0.13898511212003434 valid 0.1982341722542087
LOSS train 0.13898511212003434 valid 0.19821988124596446
LOSS train 0.13898511212003434 valid 0.19794881265926983
LOSS train 0.13898511212003434 valid 0.1978902520684453
LOSS train 0.13898511212003434 valid 0.19780725067661656
LOSS train 0.13898511212003434 valid 0.19781411267243898
LOSS train 0.13898511212003434 valid 0.1975158954122264
LOSS train 0.13898511212003434 valid 0.19742515155031712
LOSS train 0.13898511212003434 valid 0.1977566401913481
LOSS train 0.13898511212003434 valid 0.19775746539235114
LOSS train 0.13898511212003434 valid 0.19771868508795035
LOSS train 0.13898511212003434 valid 0.19764116204079288
LOSS train 0.13898511212003434 valid 0.19749383079859376
LOSS train 0.13898511212003434 valid 0.19744548501401413
LOSS train 0.13898511212003434 valid 0.1976353123332515
LOSS train 0.13898511212003434 valid 0.1974825433578836
LOSS train 0.13898511212003434 valid 0.19756437418703549
LOSS train 0.13898511212003434 valid 0.19739136490083875
LOSS train 0.13898511212003434 valid 0.19726062367653704
LOSS train 0.13898511212003434 valid 0.1972440761678359
LOSS train 0.13898511212003434 valid 0.1970957794733215
LOSS train 0.13898511212003434 valid 0.197324687225181
LOSS train 0.13898511212003434 valid 0.1972913515533326
LOSS train 0.13898511212003434 valid 0.1975434197262786
LOSS train 0.13898511212003434 valid 0.19736236470086235
LOSS train 0.13898511212003434 valid 0.19731174358590084
LOSS train 0.13898511212003434 valid 0.1972979114385648
LOSS train 0.13898511212003434 valid 0.19731577820657345
LOSS train 0.13898511212003434 valid 0.19734287120443483
LOSS train 0.13898511212003434 valid 0.19730079165763326
LOSS train 0.13898511212003434 valid 0.19724749744926368
LOSS train 0.13898511212003434 valid 0.19719451125506515
LOSS train 0.13898511212003434 valid 0.1974321387178911
LOSS train 0.13898511212003434 valid 0.19751039860041245
LOSS train 0.13898511212003434 valid 0.19748205059283488
LOSS train 0.13898511212003434 valid 0.1976086257766652
LOSS train 0.13898511212003434 valid 0.19749199993470135
LOSS train 0.13898511212003434 valid 0.19751777040197493
LOSS train 0.13898511212003434 valid 0.19723028583186014
LOSS train 0.13898511212003434 valid 0.19724112302064895
LOSS train 0.13898511212003434 valid 0.19726911233981867
LOSS train 0.13898511212003434 valid 0.1973382105740408
LOSS train 0.13898511212003434 valid 0.19706871518816974
LOSS train 0.13898511212003434 valid 0.19711295246463462
LOSS train 0.13898511212003434 valid 0.19699201194139626
LOSS train 0.13898511212003434 valid 0.19717667510314862
LOSS train 0.13898511212003434 valid 0.1972489701158504
LOSS train 0.13898511212003434 valid 0.19744552792322756
LOSS train 0.13898511212003434 valid 0.19745157561709534
LOSS train 0.13898511212003434 valid 0.19776274710893632
LOSS train 0.13898511212003434 valid 0.19766356695943804
LOSS train 0.13898511212003434 valid 0.19773586773046173
LOSS train 0.13898511212003434 valid 0.19775610097817012
LOSS train 0.13898511212003434 valid 0.19756452702716285
LOSS train 0.13898511212003434 valid 0.19743960361655166
LOSS train 0.13898511212003434 valid 0.1973706039699536
LOSS train 0.13898511212003434 valid 0.19734897561695264
LOSS train 0.13898511212003434 valid 0.19721874472900078
LOSS train 0.13898511212003434 valid 0.19725150205016706
LOSS train 0.13898511212003434 valid 0.19742983884754636
LOSS train 0.13898511212003434 valid 0.19767750714909973
LOSS train 0.13898511212003434 valid 0.1974531499282369
LOSS train 0.13898511212003434 valid 0.1973356671736274
LOSS train 0.13898511212003434 valid 0.19726961635262052
LOSS train 0.13898511212003434 valid 0.19700853145399758
LOSS train 0.13898511212003434 valid 0.19675055166913402
LOSS train 0.13898511212003434 valid 0.19667194014595402
LOSS train 0.13898511212003434 valid 0.19672024749014355
LOSS train 0.13898511212003434 valid 0.19667487281914717
LOSS train 0.13898511212003434 valid 0.19665489833463323
LOSS train 0.13898511212003434 valid 0.19666849016064433
LOSS train 0.13898511212003434 valid 0.19681508194755865
LOSS train 0.13898511212003434 valid 0.1968109075798582
LOSS train 0.13898511212003434 valid 0.1967408886578466
LOSS train 0.13898511212003434 valid 0.19648024174902173
LOSS train 0.13898511212003434 valid 0.19629169479671832
LOSS train 0.13898511212003434 valid 0.19631919238535844
LOSS train 0.13898511212003434 valid 0.19636680126974457
LOSS train 0.13898511212003434 valid 0.1962809506071707
LOSS train 0.13898511212003434 valid 0.19640756482663363
LOSS train 0.13898511212003434 valid 0.19636140976633346
LOSS train 0.13898511212003434 valid 0.19656883976582823
LOSS train 0.13898511212003434 valid 0.19680201572410028
LOSS train 0.13898511212003434 valid 0.19682216013853365
LOSS train 0.13898511212003434 valid 0.196875543860679
LOSS train 0.13898511212003434 valid 0.19672475584735305
LOSS train 0.13898511212003434 valid 0.19681165213323343
LOSS train 0.13898511212003434 valid 0.19674204308696153
LOSS train 0.13898511212003434 valid 0.1967008769138089
LOSS train 0.13898511212003434 valid 0.19633911276857058
LOSS train 0.13898511212003434 valid 0.196231862074112
LOSS train 0.13898511212003434 valid 0.19619852987941633
LOSS train 0.13898511212003434 valid 0.19619120424421727
LOSS train 0.13898511212003434 valid 0.1961903852395347
LOSS train 0.13898511212003434 valid 0.19639347663947515
LOSS train 0.13898511212003434 valid 0.19650389950692168
LOSS train 0.13898511212003434 valid 0.19651414749593388
LOSS train 0.13898511212003434 valid 0.19646627341787662
LOSS train 0.13898511212003434 valid 0.19637375663083242
LOSS train 0.13898511212003434 valid 0.19642266833782196
LOSS train 0.13898511212003434 valid 0.19645535649056453
LOSS train 0.13898511212003434 valid 0.19644272942391652
LOSS train 0.13898511212003434 valid 0.1963074164310463
LOSS train 0.13898511212003434 valid 0.19635994139853424
LOSS train 0.13898511212003434 valid 0.19627162738173615
LOSS train 0.13898511212003434 valid 0.19612257316475734
LOSS train 0.13898511212003434 valid 0.19600340805165034
LOSS train 0.13898511212003434 valid 0.19615180657584538
LOSS train 0.13898511212003434 valid 0.19613111266528319
LOSS train 0.13898511212003434 valid 0.19610201309506708
LOSS train 0.13898511212003434 valid 0.196096775460974
LOSS train 0.13898511212003434 valid 0.1962547414289176
LOSS train 0.13898511212003434 valid 0.19615189000227606
LOSS train 0.13898511212003434 valid 0.19613370649290807
LOSS train 0.13898511212003434 valid 0.1962115445789301
LOSS train 0.13898511212003434 valid 0.19632148949947573
LOSS train 0.13898511212003434 valid 0.19622953057512363
LOSS train 0.13898511212003434 valid 0.196326769721597
LOSS train 0.13898511212003434 valid 0.19632705392447547
LOSS train 0.13898511212003434 valid 0.19617487948249887
LOSS train 0.13898511212003434 valid 0.19608998864999116
LOSS train 0.13898511212003434 valid 0.1960089196834494
LOSS train 0.13898511212003434 valid 0.19587168785241935
LOSS train 0.13898511212003434 valid 0.195813900961058
LOSS train 0.13898511212003434 valid 0.1959149231152101
LOSS train 0.13898511212003434 valid 0.19603517252034036
LOSS train 0.13898511212003434 valid 0.19593034125192071
LOSS train 0.13898511212003434 valid 0.19589700657043527
LOSS train 0.13898511212003434 valid 0.19584918582952152
LOSS train 0.13898511212003434 valid 0.19576700505401406
LOSS train 0.13898511212003434 valid 0.19565525014629567
LOSS train 0.13898511212003434 valid 0.1956097034486473
LOSS train 0.13898511212003434 valid 0.1955253336012574
LOSS train 0.13898511212003434 valid 0.19539825329688235
LOSS train 0.13898511212003434 valid 0.19544043289987664
LOSS train 0.13898511212003434 valid 0.1954053637239483
LOSS train 0.13898511212003434 valid 0.19553787585750274
LOSS train 0.13898511212003434 valid 0.19546680871604216
LOSS train 0.13898511212003434 valid 0.195412498495983
LOSS train 0.13898511212003434 valid 0.19526501976210495
LOSS train 0.13898511212003434 valid 0.1952057550965306
LOSS train 0.13898511212003434 valid 0.19518251380283538
LOSS train 0.13898511212003434 valid 0.19514850585533897
LOSS train 0.13898511212003434 valid 0.19514836553408174
LOSS train 0.13898511212003434 valid 0.19519536202236756
LOSS train 0.13898511212003434 valid 0.19510594941675663
LOSS train 0.13898511212003434 valid 0.1950436029269639
LOSS train 0.13898511212003434 valid 0.19504159098903603
LOSS train 0.13898511212003434 valid 0.19507821726559796
LOSS train 0.13898511212003434 valid 0.195105551580588
LOSS train 0.13898511212003434 valid 0.19503499463547108
LOSS train 0.13898511212003434 valid 0.19495960665459663
LOSS train 0.13898511212003434 valid 0.19497536344103295
LOSS train 0.13898511212003434 valid 0.19494564419514254
LOSS train 0.13898511212003434 valid 0.19484458013636166
LOSS train 0.13898511212003434 valid 0.19504297193553713
LOSS train 0.13898511212003434 valid 0.19509070063258616
LOSS train 0.13898511212003434 valid 0.19513720743261376
LOSS train 0.13898511212003434 valid 0.19508086318529924
LOSS train 0.13898511212003434 valid 0.19506925104125852
LOSS train 0.13898511212003434 valid 0.19511504845028904
LOSS train 0.13898511212003434 valid 0.1950610869874557
LOSS train 0.13898511212003434 valid 0.19493081989570166
LOSS train 0.13898511212003434 valid 0.19482081060766415
LOSS train 0.13898511212003434 valid 0.1947494168603231
LOSS train 0.13898511212003434 valid 0.19470049813389778
LOSS train 0.13898511212003434 valid 0.19473072862963572
LOSS train 0.13898511212003434 valid 0.1947851135108456
LOSS train 0.13898511212003434 valid 0.19476493889448412
LOSS train 0.13898511212003434 valid 0.19462451310828327
LOSS train 0.13898511212003434 valid 0.1946702068468492
LOSS train 0.13898511212003434 valid 0.19455216130854922
LOSS train 0.13898511212003434 valid 0.19448031088331535
LOSS train 0.13898511212003434 valid 0.19444344114558196
LOSS train 0.13898511212003434 valid 0.19428819046570706
LOSS train 0.13898511212003434 valid 0.19439757313647885
LOSS train 0.13898511212003434 valid 0.19429983882182236
LOSS train 0.13898511212003434 valid 0.19431732140663194
LOSS train 0.13898511212003434 valid 0.19435841750953697
LOSS train 0.13898511212003434 valid 0.19441431959470112
LOSS train 0.13898511212003434 valid 0.19432293027189204
LOSS train 0.13898511212003434 valid 0.19430461180317832
LOSS train 0.13898511212003434 valid 0.19430444441042147
LOSS train 0.13898511212003434 valid 0.19422512389942556
LOSS train 0.13898511212003434 valid 0.19407253287621398
LOSS train 0.13898511212003434 valid 0.19398885867780163
LOSS train 0.13898511212003434 valid 0.19409483083100984
LOSS train 0.13898511212003434 valid 0.19409315993447276
LOSS train 0.13898511212003434 valid 0.19401908975786866
LOSS train 0.13898511212003434 valid 0.1941353795282981
LOSS train 0.13898511212003434 valid 0.19412501147590425
LOSS train 0.13898511212003434 valid 0.19411791664989372
LOSS train 0.13898511212003434 valid 0.1940005615831464
LOSS train 0.13898511212003434 valid 0.19412816068980582
LOSS train 0.13898511212003434 valid 0.1941942213670067
LOSS train 0.13898511212003434 valid 0.19415370125129733
LOSS train 0.13898511212003434 valid 0.19410103898742356
LOSS train 0.13898511212003434 valid 0.19409866990714236
LOSS train 0.13898511212003434 valid 0.19410785430652705
LOSS train 0.13898511212003434 valid 0.1941548771943365
LOSS train 0.13898511212003434 valid 0.19423969450839224
LOSS train 0.13898511212003434 valid 0.19430011684413662
LOSS train 0.13898511212003434 valid 0.1943628203767555
LOSS train 0.13898511212003434 valid 0.194350856551006
LOSS train 0.13898511212003434 valid 0.19422668399105608
LOSS train 0.13898511212003434 valid 0.19422700281223554
LOSS train 0.13898511212003434 valid 0.1941673149581717
LOSS train 0.13898511212003434 valid 0.19416585631210712
LOSS train 0.13898511212003434 valid 0.1940966929233838
LOSS train 0.13898511212003434 valid 0.1943285796377394
LOSS train 0.13898511212003434 valid 0.19422821477674712
LOSS train 0.13898511212003434 valid 0.19432523908371424
LOSS train 0.13898511212003434 valid 0.19435093237841425
LOSS train 0.13898511212003434 valid 0.19426561900220074
LOSS train 0.13898511212003434 valid 0.19429847263310054
LOSS train 0.13898511212003434 valid 0.19426445642618534
LOSS train 0.13898511212003434 valid 0.19424089074784467
LOSS train 0.13898511212003434 valid 0.1941506956177561
LOSS train 0.13898511212003434 valid 0.1942069862431627
EPOCH 6:
  batch 1 loss: 0.15733923017978668
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.13550565391778946
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.12766139954328537
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.12689749710261822
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.131949083507061
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.13265102977554002
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.13460586220026016
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.13143854588270187
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.13147718211015066
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.12924129664897918
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.13026283139532263
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.13346158216396967
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.13357059886822334
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.1319685796541827
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.13073283831278484
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.1331384191289544
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.13392682373523712
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.13506978501876196
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.13607271094071238
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.13513692021369933
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.1347135944025857
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.1349151385101405
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.1330343284036802
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.13241224735975266
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.13227815806865692
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.1347275158533683
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.135210484818176
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.13551634762968337
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.13668192768919057
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.1365313912431399
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.13655959598479733
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.1354864309541881
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.13596617407871014
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.1364668905735016
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.1363754940884454
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.1373784016403887
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.13849657733698148
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.13785887235089353
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.13711206844219795
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.13728711046278477
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.1365019804457339
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.13644726130933987
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.13645122269558352
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.13679034720090302
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.136470778617594
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.13634276859786199
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.13640855807573238
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.13657632702961564
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.13669557580534292
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.13656689569354058
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.13611869061110066
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.13526913294425377
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.13515695339103914
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.13499064053650256
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.13500357297333804
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.13526458798774652
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.13597799184029563
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.13567790106452746
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.13632352347091092
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.13701085075736047
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.13737418588067665
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.1372754037860901
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.13714950614505345
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.13717937050387263
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.13716063705774453
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.13691270565896324
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.13718956364179724
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.1371098797330085
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.1368921846151352
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.13690679648092816
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.1370489366457496
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.1367793958634138
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.1372897782146114
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.13764186988811236
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.13746909379959107
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.1375738913683515
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.1378576722625014
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.1378679842903064
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.13818944519079185
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.13827917780727148
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.1385083939926124
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.13852944261417155
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.138556257069829
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.13858648247662045
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.13881818056106568
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.13845766197110332
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.138491944848806
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.13852731714194472
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.1386212319805381
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.13858752300341923
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.13870580821901887
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.1386192160780015
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.13860718921948506
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.1386373780192213
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.1384058330404131
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.13835836411453784
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.13829804488371328
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.1384361718534207
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.13856791147981026
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.13831492364406586
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.13829536175373758
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.1382947537828894
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.1383121647302387
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.13853828637645796
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.1383340373635292
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.13836298538547642
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.1384648035202071
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.13852356381162448
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.138522595235514
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.13864446180787954
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.13831097894423716
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.1382612381130457
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.1378674224282788
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.13809707492851375
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.13814853604720986
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.13829358339566608
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.13840196205255312
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.13815332791310245
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.13833879979969071
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.13817558605223895
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.13824470987743584
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.13827138314725923
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.13811722446263322
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.13821558498086467
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.13855621421337128
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.1384336365357278
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.1386329961104656
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.13861104799434543
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.13837533750275308
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.13835608259989665
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.13831015408949088
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.13803686331393142
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.138054751083815
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.13798870785690065
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.13801356029731257
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.1379499145619133
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.1379563812248028
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.1378779179700043
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.13800858931361343
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.13791251634912832
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.13794562564039906
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.13791669364756262
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.13807879456064917
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.13778607050577799
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.1379770246045343
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.13797762071433134
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.1377849190109441
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.13777561329707905
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.13777353644571047
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.1374710172911485
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.13752589058994458
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.137681983656397
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.13781262909859612
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.13774836164983836
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.13756049275398255
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.13740837702957484
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.13738172529799164
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.137433715211817
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.1373311553454999
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.13730098330415785
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.1373620898179386
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.1373758537257895
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.13743205546784254
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.1373678076467136
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.13720569750576309
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.13712253066969204
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.13722613378020818
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.13707799131848983
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.13696640910657906
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.13714466853176846
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.13717869939337
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.1370131165482277
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.1368542518143709
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.13679056036575088
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.13690278619527818
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.13695539352060718
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.13723703448550176
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.1371993813035863
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.1372582920353506
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.13725877159999478
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.13708699239387037
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.13719334506562778
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.1372453631029103
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.13716885747145052
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.1371633674647357
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.13717648186670836
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.13723000390006895
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.13718861238436497
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.13727519192077495
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.13728358722046802
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.13731208061360564
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.13764752090598145
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.13769108195996654
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.1377424668405474
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.1377751159362304
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.13776105482663428
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.13752571016852624
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.1375222716743898
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.13751242526961333
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.13769179996103048
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.13757737877950146
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.13753576104593748
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.13757242460556218
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.13750793174931816
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.13748454447926545
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.1375484878504739
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.13739593211867382
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.1372104104106816
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.137195777130184
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.1373626658249469
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.13737973885089866
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.13751106333198412
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.13755901623080033
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.13760064594517243
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.1375823387226393
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.13757477662767525
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.13754184087437968
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.13759832217469128
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.13768738137396502
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.13743837804279543
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.13745767805236497
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.1373948012520601
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.13748393598693368
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.13745939119585923
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.13754590815967985
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.1374540174363461
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.13739617421632297
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.1372940796182344
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.13731271181434523
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.13732337281107904
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.13737247558512214
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.13744482619623685
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.1373779699845887
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.13736132669270548
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.1372843620941994
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.13726223307519647
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.13728232194476991
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.1373388766929382
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.13717346685202053
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.13716538219402233
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.13714693968968766
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.13713726185816377
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.137141262859474
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.13714925236389286
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.13717788470034697
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.137031650640131
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.13710707993159893
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.13723854137764824
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.13719540930177312
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.1372470144033432
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.13726408013784552
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.13721273845386883
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.13721578306124615
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.13715085126166268
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.13720626506735298
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.13716381255653687
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.13707321473480663
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.1370788991393507
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.13720206211547595
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.13717166921840265
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.13724910062505824
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.13725793734192848
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.13711508819585516
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.1370897779071873
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.1370904912363808
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.13698442470758482
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.13706046331687813
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.137116832019233
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.1370298410436921
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.13705923016424532
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.13707516570592718
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.13700265152489438
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.1369239404633805
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.13693621776399822
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.13675722859122535
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.13672483393895454
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.13681185579041713
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.13668252667077155
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.13666011361024713
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.1367413467062371
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.136690384393485
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.1367464134773464
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.13668095080886208
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.13663675378955586
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.13681756545577134
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.13687193466650022
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.13693612798373458
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.13686484900406665
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.13677243945095366
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.13682204942251075
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.13683739913902743
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.13684796108162567
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.13692887056199357
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.13691256392975243
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.13692084434679
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.13704040369673356
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.13703058369031257
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.13706602156162262
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.13702222413342932
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.13692137447496255
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.13690505929266494
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.136824659065695
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.13684254417521727
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.13683445722256837
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.1369244981984623
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.1369022174793131
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.13682010888762117
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.13683517521561742
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.13687283241922416
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.13681340147891352
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.13674019442398064
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.1366755761540471
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.1367254468341605
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.13671136645090048
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.13691263182295693
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.13690406275040742
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.1369209648917902
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.13691751853381312
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.1369005682933667
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.13684438213240355
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.13679913937395607
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.13691356734181784
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.13689069885864347
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.13685871405458008
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.1369135936177694
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.13688871760401258
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.1369570404610138
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.13692854637870702
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.1369709262233737
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.13693670930735993
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.13683644067664882
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.13688386294497065
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.13688429304071376
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.13692275871952136
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.13687082531292047
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.1369690828335782
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.13692742026789606
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.13689243698702055
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.13688875904037537
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.13695867177756393
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.13699445384076614
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.13696936733628573
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.1369994967611121
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.13692832240011804
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.13699795344601506
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.13693902675988356
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.13699406003230588
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.13699886416909338
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.13695488989694754
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.13700430090938295
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.13700929759574412
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.13698307073421098
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.13695051641369677
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.13709169954566633
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.13701131902110408
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.13701733402656704
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.13696219849319352
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.136924492621888
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.13691202378870718
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.13688603995574844
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.13689554583828206
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.13689055647803933
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.136913619217137
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.13690319685981825
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.13691753365405618
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.13697373907748467
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.13690919154996117
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.13697503624564927
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.13697734148037144
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.13709960459051906
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.1371231926981651
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.1371107703895979
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.13708543573723403
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.13707761980473676
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.137051620443662
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.13702530108709285
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.13714630735805874
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.13717522070048346
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.1372761068998352
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.13730721587413236
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.13728271561657662
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.1373062465359403
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.13731375079864622
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.13731013292757174
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.13725907363287815
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.1372631406652804
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.13720697218441533
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.1372632524862732
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.13723725306497442
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.13731757287795726
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.13736343181804014
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.13739179196406384
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.13739535671305717
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.13735132598256702
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.13736684886337835
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.13738919107826672
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.13736854743221846
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.13734573633152636
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.1373836193653874
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.1373425101675093
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.13747325827578952
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.1375180379631211
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.1374682463132418
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.1374930013155583
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.13750665165759898
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.1375305413685996
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.13752775728995442
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.1374579141026034
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.13746726206870416
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.13746477997884518
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.13754756201212714
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.1375388092569356
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.1374587398969521
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.13737177629257746
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.13733407691659696
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.13734383223793253
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.1373037529184664
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.13729756712271837
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.13729918065318628
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.13726385524939924
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.1372697523080538
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.13728965852427258
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.1372596675235047
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.13729334641950872
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.13727705087731867
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.1372234263750309
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.13721842966118797
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.13723047171658445
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.1372193203314201
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.1373040154229763
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.1372636025058421
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.1373060129893323
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.1373208707701802
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.1373291968234948
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.137362125875621
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.13741604828301374
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.13739431526690926
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.1374488374788195
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.1374185629918798
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.13744111177934842
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.13747308795211538
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.13746451278010643
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.13750286691277613
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.13748296660681567
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.13744288205765606
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.13746304364014633
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.1374413186571742
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.13742962541125184
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.13741650442304484
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.13739661291241645
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.1374289918930462
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.13739326024279658
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.1373474116850373
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.13726431613845447
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.13717732100368857
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.1372068114415334
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.1371942267343565
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.13716019042584573
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.1371331401899749
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.13711932259409323
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.137172577179435
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.1371469147406615
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.13712286218582426
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.13716622625060124
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.13715960027710083
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.1372070602848806
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.13727601210744272
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.13727868631736845
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.13726924782368674
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.13719420448896733
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.1372031187618867
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.13714333116945068
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.13714333116945068 valid 0.18829581141471863
LOSS train 0.13714333116945068 valid 0.19836362451314926
LOSS train 0.13714333116945068 valid 0.20001927514870962
LOSS train 0.13714333116945068 valid 0.1894603967666626
LOSS train 0.13714333116945068 valid 0.18878643214702606
LOSS train 0.13714333116945068 valid 0.19333294530709585
LOSS train 0.13714333116945068 valid 0.19215703862054007
LOSS train 0.13714333116945068 valid 0.19120481610298157
LOSS train 0.13714333116945068 valid 0.18890492452515495
LOSS train 0.13714333116945068 valid 0.1892716705799103
LOSS train 0.13714333116945068 valid 0.1894553303718567
LOSS train 0.13714333116945068 valid 0.19352581103642783
LOSS train 0.13714333116945068 valid 0.19347735207814437
LOSS train 0.13714333116945068 valid 0.19246179823364531
LOSS train 0.13714333116945068 valid 0.1924545556306839
LOSS train 0.13714333116945068 valid 0.19430517498403788
LOSS train 0.13714333116945068 valid 0.19636365070062525
LOSS train 0.13714333116945068 valid 0.19662704318761826
LOSS train 0.13714333116945068 valid 0.1985539668484738
LOSS train 0.13714333116945068 valid 0.19770492911338805
LOSS train 0.13714333116945068 valid 0.19870199475969588
LOSS train 0.13714333116945068 valid 0.19791152328252792
LOSS train 0.13714333116945068 valid 0.19688002441240393
LOSS train 0.13714333116945068 valid 0.1976618462552627
LOSS train 0.13714333116945068 valid 0.19665365934371948
LOSS train 0.13714333116945068 valid 0.19584244375045484
LOSS train 0.13714333116945068 valid 0.19552976996810348
LOSS train 0.13714333116945068 valid 0.19578531490904943
LOSS train 0.13714333116945068 valid 0.19407852559254088
LOSS train 0.13714333116945068 valid 0.1941876620054245
LOSS train 0.13714333116945068 valid 0.19471018064406612
LOSS train 0.13714333116945068 valid 0.19506449671462178
LOSS train 0.13714333116945068 valid 0.19413756782358343
LOSS train 0.13714333116945068 valid 0.19334493007730036
LOSS train 0.13714333116945068 valid 0.19347928762435912
LOSS train 0.13714333116945068 valid 0.1937818895611498
LOSS train 0.13714333116945068 valid 0.19419727454314362
LOSS train 0.13714333116945068 valid 0.19452067503803655
LOSS train 0.13714333116945068 valid 0.19489927895558187
LOSS train 0.13714333116945068 valid 0.19436186291277407
LOSS train 0.13714333116945068 valid 0.19423648715019226
LOSS train 0.13714333116945068 valid 0.19512845895120076
LOSS train 0.13714333116945068 valid 0.19619362610717153
LOSS train 0.13714333116945068 valid 0.19566849009557205
LOSS train 0.13714333116945068 valid 0.1949840741025077
LOSS train 0.13714333116945068 valid 0.1939982849618663
LOSS train 0.13714333116945068 valid 0.19378394776202262
LOSS train 0.13714333116945068 valid 0.1950598700592915
LOSS train 0.13714333116945068 valid 0.1947494629694491
LOSS train 0.13714333116945068 valid 0.19492456912994385
LOSS train 0.13714333116945068 valid 0.194442673348913
LOSS train 0.13714333116945068 valid 0.19414865941955492
LOSS train 0.13714333116945068 valid 0.19541211482488885
LOSS train 0.13714333116945068 valid 0.1954441514832002
LOSS train 0.13714333116945068 valid 0.19527721838517623
LOSS train 0.13714333116945068 valid 0.19519988261163235
LOSS train 0.13714333116945068 valid 0.19468981655020462
LOSS train 0.13714333116945068 valid 0.19517440580088516
LOSS train 0.13714333116945068 valid 0.19481967187533944
LOSS train 0.13714333116945068 valid 0.19445270622769992
LOSS train 0.13714333116945068 valid 0.19427818841621525
LOSS train 0.13714333116945068 valid 0.19394899135635746
LOSS train 0.13714333116945068 valid 0.19370937442022657
LOSS train 0.13714333116945068 valid 0.1938434699550271
LOSS train 0.13714333116945068 valid 0.19320644369492165
LOSS train 0.13714333116945068 valid 0.19317021130612402
LOSS train 0.13714333116945068 valid 0.19368753059586483
LOSS train 0.13714333116945068 valid 0.19300463445046367
LOSS train 0.13714333116945068 valid 0.19346435022526892
LOSS train 0.13714333116945068 valid 0.19367088696786336
LOSS train 0.13714333116945068 valid 0.19349350035190582
LOSS train 0.13714333116945068 valid 0.19394722849958473
LOSS train 0.13714333116945068 valid 0.1944058751814986
LOSS train 0.13714333116945068 valid 0.19415629816216393
LOSS train 0.13714333116945068 valid 0.19404234250386557
LOSS train 0.13714333116945068 valid 0.19394377187678688
LOSS train 0.13714333116945068 valid 0.1938806307780278
LOSS train 0.13714333116945068 valid 0.19358814908907965
LOSS train 0.13714333116945068 valid 0.19367153735100467
LOSS train 0.13714333116945068 valid 0.19315780326724052
LOSS train 0.13714333116945068 valid 0.19338419095233636
LOSS train 0.13714333116945068 valid 0.19359877614713297
LOSS train 0.13714333116945068 valid 0.19367639971784797
LOSS train 0.13714333116945068 valid 0.1933393795930204
LOSS train 0.13714333116945068 valid 0.19360148731399984
LOSS train 0.13714333116945068 valid 0.1935416518948799
LOSS train 0.13714333116945068 valid 0.19332695315624104
LOSS train 0.13714333116945068 valid 0.1932392188093879
LOSS train 0.13714333116945068 valid 0.19346833731351273
LOSS train 0.13714333116945068 valid 0.1934446449081103
LOSS train 0.13714333116945068 valid 0.19353349195731864
LOSS train 0.13714333116945068 valid 0.19343519388981487
LOSS train 0.13714333116945068 valid 0.19365605006935777
LOSS train 0.13714333116945068 valid 0.1937957171429979
LOSS train 0.13714333116945068 valid 0.1943415582180023
LOSS train 0.13714333116945068 valid 0.19516853025803962
LOSS train 0.13714333116945068 valid 0.19533987742723877
LOSS train 0.13714333116945068 valid 0.19597372731992177
LOSS train 0.13714333116945068 valid 0.19587614725936542
LOSS train 0.13714333116945068 valid 0.1960019798576832
LOSS train 0.13714333116945068 valid 0.1958777773203236
LOSS train 0.13714333116945068 valid 0.19625917150109423
LOSS train 0.13714333116945068 valid 0.19594479967089531
LOSS train 0.13714333116945068 valid 0.1958328724767153
LOSS train 0.13714333116945068 valid 0.19594743180842628
LOSS train 0.13714333116945068 valid 0.1957535891319221
LOSS train 0.13714333116945068 valid 0.19551196972900461
LOSS train 0.13714333116945068 valid 0.19561912933433498
LOSS train 0.13714333116945068 valid 0.19536663878948315
LOSS train 0.13714333116945068 valid 0.19514273648912256
LOSS train 0.13714333116945068 valid 0.1953673915820079
LOSS train 0.13714333116945068 valid 0.1955582456929343
LOSS train 0.13714333116945068 valid 0.1953344645753371
LOSS train 0.13714333116945068 valid 0.19540659050669587
LOSS train 0.13714333116945068 valid 0.1958965442750765
LOSS train 0.13714333116945068 valid 0.19566314731692447
LOSS train 0.13714333116945068 valid 0.19596837601091108
LOSS train 0.13714333116945068 valid 0.19578236215195413
LOSS train 0.13714333116945068 valid 0.1955719298424841
LOSS train 0.13714333116945068 valid 0.19568897460897763
LOSS train 0.13714333116945068 valid 0.19553836810687356
LOSS train 0.13714333116945068 valid 0.19578258961927694
LOSS train 0.13714333116945068 valid 0.1958674139123622
LOSS train 0.13714333116945068 valid 0.19615254094523768
LOSS train 0.13714333116945068 valid 0.1960925884246826
LOSS train 0.13714333116945068 valid 0.19615151744986337
LOSS train 0.13714333116945068 valid 0.1959985624852143
LOSS train 0.13714333116945068 valid 0.19605885539203882
LOSS train 0.13714333116945068 valid 0.19590827349082443
LOSS train 0.13714333116945068 valid 0.19570937718336398
LOSS train 0.13714333116945068 valid 0.19585895981952434
LOSS train 0.13714333116945068 valid 0.19616586884314363
LOSS train 0.13714333116945068 valid 0.19619542326694145
LOSS train 0.13714333116945068 valid 0.19642670975247425
LOSS train 0.13714333116945068 valid 0.19645776097421294
LOSS train 0.13714333116945068 valid 0.19667234786731355
LOSS train 0.13714333116945068 valid 0.19662806989937803
LOSS train 0.13714333116945068 valid 0.19632614457952804
LOSS train 0.13714333116945068 valid 0.1962358999809773
LOSS train 0.13714333116945068 valid 0.19595504041228975
LOSS train 0.13714333116945068 valid 0.1961934821191409
LOSS train 0.13714333116945068 valid 0.19644113672031482
LOSS train 0.13714333116945068 valid 0.1966110887852582
LOSS train 0.13714333116945068 valid 0.19638468832191494
LOSS train 0.13714333116945068 valid 0.19623863316815474
LOSS train 0.13714333116945068 valid 0.1961548072222161
LOSS train 0.13714333116945068 valid 0.19614118428862826
LOSS train 0.13714333116945068 valid 0.1959596666935328
LOSS train 0.13714333116945068 valid 0.19615852922801202
LOSS train 0.13714333116945068 valid 0.1960503175854683
LOSS train 0.13714333116945068 valid 0.19595620845327313
LOSS train 0.13714333116945068 valid 0.19592945522775776
LOSS train 0.13714333116945068 valid 0.19566167443016774
LOSS train 0.13714333116945068 valid 0.1956041476556233
LOSS train 0.13714333116945068 valid 0.19552767757446535
LOSS train 0.13714333116945068 valid 0.195536645845725
LOSS train 0.13714333116945068 valid 0.19524362825663985
LOSS train 0.13714333116945068 valid 0.19515397097868256
LOSS train 0.13714333116945068 valid 0.19547972073719935
LOSS train 0.13714333116945068 valid 0.19547714442014694
LOSS train 0.13714333116945068 valid 0.1954419253775792
LOSS train 0.13714333116945068 valid 0.19538175498629795
LOSS train 0.13714333116945068 valid 0.1952345320966346
LOSS train 0.13714333116945068 valid 0.19518982046624508
LOSS train 0.13714333116945068 valid 0.19538675086064772
LOSS train 0.13714333116945068 valid 0.19522968464228044
LOSS train 0.13714333116945068 valid 0.19532360805722768
LOSS train 0.13714333116945068 valid 0.19515379101392769
LOSS train 0.13714333116945068 valid 0.19502895162303066
LOSS train 0.13714333116945068 valid 0.19501600142787484
LOSS train 0.13714333116945068 valid 0.19486154537451894
LOSS train 0.13714333116945068 valid 0.1950896353742411
LOSS train 0.13714333116945068 valid 0.19505965218723165
LOSS train 0.13714333116945068 valid 0.19531498815136394
LOSS train 0.13714333116945068 valid 0.1951366559096745
LOSS train 0.13714333116945068 valid 0.19507244838909668
LOSS train 0.13714333116945068 valid 0.19505840935612803
LOSS train 0.13714333116945068 valid 0.19506583362817764
LOSS train 0.13714333116945068 valid 0.19509391894553627
LOSS train 0.13714333116945068 valid 0.19505631311072244
LOSS train 0.13714333116945068 valid 0.195014795545715
LOSS train 0.13714333116945068 valid 0.1949743628501892
LOSS train 0.13714333116945068 valid 0.19522482352178605
LOSS train 0.13714333116945068 valid 0.19530844639824785
LOSS train 0.13714333116945068 valid 0.19527659222886368
LOSS train 0.13714333116945068 valid 0.19541640203165753
LOSS train 0.13714333116945068 valid 0.195301575217655
LOSS train 0.13714333116945068 valid 0.19532310748670964
LOSS train 0.13714333116945068 valid 0.19502897946922867
LOSS train 0.13714333116945068 valid 0.19503596119190517
LOSS train 0.13714333116945068 valid 0.19507761939345855
LOSS train 0.13714333116945068 valid 0.1951431620400399
LOSS train 0.13714333116945068 valid 0.19487828340555102
LOSS train 0.13714333116945068 valid 0.19491820820828074
LOSS train 0.13714333116945068 valid 0.19479899650964982
LOSS train 0.13714333116945068 valid 0.19498263307067812
LOSS train 0.13714333116945068 valid 0.19505906717728844
LOSS train 0.13714333116945068 valid 0.1952568692239848
LOSS train 0.13714333116945068 valid 0.1952668999008198
LOSS train 0.13714333116945068 valid 0.19558537393808365
LOSS train 0.13714333116945068 valid 0.19549317005558395
LOSS train 0.13714333116945068 valid 0.19556791962373374
LOSS train 0.13714333116945068 valid 0.19559331928274315
LOSS train 0.13714333116945068 valid 0.19539260681645543
LOSS train 0.13714333116945068 valid 0.19527422444122594
LOSS train 0.13714333116945068 valid 0.19520866118588495
LOSS train 0.13714333116945068 valid 0.19517202848109647
LOSS train 0.13714333116945068 valid 0.1950373393841661
LOSS train 0.13714333116945068 valid 0.19506567267424751
LOSS train 0.13714333116945068 valid 0.19524425651345934
LOSS train 0.13714333116945068 valid 0.19549624876105956
LOSS train 0.13714333116945068 valid 0.19527372232866738
LOSS train 0.13714333116945068 valid 0.19514276521026808
LOSS train 0.13714333116945068 valid 0.1950816156290402
LOSS train 0.13714333116945068 valid 0.1948232656301454
LOSS train 0.13714333116945068 valid 0.19457367504084552
LOSS train 0.13714333116945068 valid 0.19449402391910553
LOSS train 0.13714333116945068 valid 0.19455224535334
LOSS train 0.13714333116945068 valid 0.19451645309250104
LOSS train 0.13714333116945068 valid 0.1945056302980943
LOSS train 0.13714333116945068 valid 0.1945183803727724
LOSS train 0.13714333116945068 valid 0.19466889246895508
LOSS train 0.13714333116945068 valid 0.19465685859656653
LOSS train 0.13714333116945068 valid 0.19458244594612292
LOSS train 0.13714333116945068 valid 0.19432053691811033
LOSS train 0.13714333116945068 valid 0.19412288603793204
LOSS train 0.13714333116945068 valid 0.1941455247512473
LOSS train 0.13714333116945068 valid 0.1941889401590615
LOSS train 0.13714333116945068 valid 0.19410193623673969
LOSS train 0.13714333116945068 valid 0.1942246987119965
LOSS train 0.13714333116945068 valid 0.19418192683876334
LOSS train 0.13714333116945068 valid 0.19440796791479506
LOSS train 0.13714333116945068 valid 0.1946324072298574
LOSS train 0.13714333116945068 valid 0.19464560190581867
LOSS train 0.13714333116945068 valid 0.19469896941742998
LOSS train 0.13714333116945068 valid 0.19454763419294763
LOSS train 0.13714333116945068 valid 0.19463534414265227
LOSS train 0.13714333116945068 valid 0.194575695618361
LOSS train 0.13714333116945068 valid 0.1945399976799179
LOSS train 0.13714333116945068 valid 0.19418000128741067
LOSS train 0.13714333116945068 valid 0.1940746167326864
LOSS train 0.13714333116945068 valid 0.19404342866017799
LOSS train 0.13714333116945068 valid 0.19404741812022133
LOSS train 0.13714333116945068 valid 0.194047658750024
LOSS train 0.13714333116945068 valid 0.1942539629583456
LOSS train 0.13714333116945068 valid 0.19437208926168884
LOSS train 0.13714333116945068 valid 0.19437793853432545
LOSS train 0.13714333116945068 valid 0.19432587251668015
LOSS train 0.13714333116945068 valid 0.19421918340117098
LOSS train 0.13714333116945068 valid 0.19426664116978645
LOSS train 0.13714333116945068 valid 0.19430780609646642
LOSS train 0.13714333116945068 valid 0.1942972539968434
LOSS train 0.13714333116945068 valid 0.19416285865745053
LOSS train 0.13714333116945068 valid 0.19422155140539793
LOSS train 0.13714333116945068 valid 0.19413581306443495
LOSS train 0.13714333116945068 valid 0.1939896521216724
LOSS train 0.13714333116945068 valid 0.19387027931468495
LOSS train 0.13714333116945068 valid 0.19401493729200475
LOSS train 0.13714333116945068 valid 0.19398553120008308
LOSS train 0.13714333116945068 valid 0.19394761745173197
LOSS train 0.13714333116945068 valid 0.19394180415576445
LOSS train 0.13714333116945068 valid 0.19410149113253783
LOSS train 0.13714333116945068 valid 0.19398784781930112
LOSS train 0.13714333116945068 valid 0.1939696702033733
LOSS train 0.13714333116945068 valid 0.19405659962937516
LOSS train 0.13714333116945068 valid 0.19416923806967593
LOSS train 0.13714333116945068 valid 0.19408322661445382
LOSS train 0.13714333116945068 valid 0.19417535722144505
LOSS train 0.13714333116945068 valid 0.19418355354478367
LOSS train 0.13714333116945068 valid 0.19403154863803476
LOSS train 0.13714333116945068 valid 0.19394992019747456
LOSS train 0.13714333116945068 valid 0.19386743422707214
LOSS train 0.13714333116945068 valid 0.1937268288744675
LOSS train 0.13714333116945068 valid 0.1936602163108161
LOSS train 0.13714333116945068 valid 0.1937573133002628
LOSS train 0.13714333116945068 valid 0.19387990233582864
LOSS train 0.13714333116945068 valid 0.19377217640838038
LOSS train 0.13714333116945068 valid 0.1937389598100734
LOSS train 0.13714333116945068 valid 0.19369191590923562
LOSS train 0.13714333116945068 valid 0.19361667428165674
LOSS train 0.13714333116945068 valid 0.19350131447310973
LOSS train 0.13714333116945068 valid 0.19345396310619428
LOSS train 0.13714333116945068 valid 0.19337291523235003
LOSS train 0.13714333116945068 valid 0.19324773494941247
LOSS train 0.13714333116945068 valid 0.1932979766475527
LOSS train 0.13714333116945068 valid 0.1932642780556962
LOSS train 0.13714333116945068 valid 0.19340687421659974
LOSS train 0.13714333116945068 valid 0.1933318664992435
LOSS train 0.13714333116945068 valid 0.1932860883307292
LOSS train 0.13714333116945068 valid 0.19313897189909013
LOSS train 0.13714333116945068 valid 0.1930799840395803
LOSS train 0.13714333116945068 valid 0.19305658975794707
LOSS train 0.13714333116945068 valid 0.19301246114253184
LOSS train 0.13714333116945068 valid 0.19301107766575554
LOSS train 0.13714333116945068 valid 0.19305136807389178
LOSS train 0.13714333116945068 valid 0.19296165976069263
LOSS train 0.13714333116945068 valid 0.19290086800100828
LOSS train 0.13714333116945068 valid 0.19290383432635524
LOSS train 0.13714333116945068 valid 0.19293820192682304
LOSS train 0.13714333116945068 valid 0.1929719752818346
LOSS train 0.13714333116945068 valid 0.19289854443647536
LOSS train 0.13714333116945068 valid 0.1928265703385634
LOSS train 0.13714333116945068 valid 0.19284773736011865
LOSS train 0.13714333116945068 valid 0.19281889927132348
LOSS train 0.13714333116945068 valid 0.19271845956806277
LOSS train 0.13714333116945068 valid 0.19292834734702421
LOSS train 0.13714333116945068 valid 0.19296780886304496
LOSS train 0.13714333116945068 valid 0.1930031299155641
LOSS train 0.13714333116945068 valid 0.19294830103040128
LOSS train 0.13714333116945068 valid 0.1929397422700159
LOSS train 0.13714333116945068 valid 0.19298704608172848
LOSS train 0.13714333116945068 valid 0.19293956437076515
LOSS train 0.13714333116945068 valid 0.1928070023084601
LOSS train 0.13714333116945068 valid 0.19269719978521585
LOSS train 0.13714333116945068 valid 0.19262578503953087
LOSS train 0.13714333116945068 valid 0.19257344454055345
LOSS train 0.13714333116945068 valid 0.19260078633339248
LOSS train 0.13714333116945068 valid 0.19265279110591366
LOSS train 0.13714333116945068 valid 0.19263040218327113
LOSS train 0.13714333116945068 valid 0.19249160739127547
LOSS train 0.13714333116945068 valid 0.19253178681261443
LOSS train 0.13714333116945068 valid 0.19241302819078013
LOSS train 0.13714333116945068 valid 0.19233927788103328
LOSS train 0.13714333116945068 valid 0.192299579695603
LOSS train 0.13714333116945068 valid 0.1921461643393223
LOSS train 0.13714333116945068 valid 0.19224930977437393
LOSS train 0.13714333116945068 valid 0.19214813440855855
LOSS train 0.13714333116945068 valid 0.19216445213320052
LOSS train 0.13714333116945068 valid 0.1922094751500431
LOSS train 0.13714333116945068 valid 0.19226255346879814
LOSS train 0.13714333116945068 valid 0.19216909511661961
LOSS train 0.13714333116945068 valid 0.19214918408318457
LOSS train 0.13714333116945068 valid 0.1921494352343204
LOSS train 0.13714333116945068 valid 0.19207436977300102
LOSS train 0.13714333116945068 valid 0.1919267152005167
LOSS train 0.13714333116945068 valid 0.1918369584184672
LOSS train 0.13714333116945068 valid 0.1919402222118675
LOSS train 0.13714333116945068 valid 0.19193923189001677
LOSS train 0.13714333116945068 valid 0.19186715599270346
LOSS train 0.13714333116945068 valid 0.19198777853127788
LOSS train 0.13714333116945068 valid 0.19197271971845906
LOSS train 0.13714333116945068 valid 0.1919674322836929
LOSS train 0.13714333116945068 valid 0.19185087673431236
LOSS train 0.13714333116945068 valid 0.1919893129230585
LOSS train 0.13714333116945068 valid 0.192055203711641
LOSS train 0.13714333116945068 valid 0.19201106149595598
LOSS train 0.13714333116945068 valid 0.19195701853943833
LOSS train 0.13714333116945068 valid 0.19196385108790864
LOSS train 0.13714333116945068 valid 0.19197761677312306
LOSS train 0.13714333116945068 valid 0.1920316044134753
LOSS train 0.13714333116945068 valid 0.19211403497009197
LOSS train 0.13714333116945068 valid 0.19217863168821417
LOSS train 0.13714333116945068 valid 0.19224043336113858
LOSS train 0.13714333116945068 valid 0.19222685490540192
LOSS train 0.13714333116945068 valid 0.1921009157954807
LOSS train 0.13714333116945068 valid 0.19210234265565201
LOSS train 0.13714333116945068 valid 0.19204593545832888
LOSS train 0.13714333116945068 valid 0.19204195645018662
LOSS train 0.13714333116945068 valid 0.1919689006833645
LOSS train 0.13714333116945068 valid 0.19220268728418483
LOSS train 0.13714333116945068 valid 0.19210278329509117
LOSS train 0.13714333116945068 valid 0.1922038660811785
LOSS train 0.13714333116945068 valid 0.19223008288414353
LOSS train 0.13714333116945068 valid 0.19214450234322103
LOSS train 0.13714333116945068 valid 0.1921842902694663
LOSS train 0.13714333116945068 valid 0.19215596803143375
LOSS train 0.13714333116945068 valid 0.19212834124912684
LOSS train 0.13714333116945068 valid 0.1920387927726235
LOSS train 0.13714333116945068 valid 0.19209585072306112
EPOCH 7:
  batch 1 loss: 0.15519064664840698
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.13352181017398834
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.12668578326702118
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.12613825872540474
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.13063936531543732
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.13230640441179276
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.13445476761886052
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.1312672095373273
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.13121244062980017
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.12858146876096727
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.12958527695048938
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.13274516786138216
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.13275776344996232
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.13109710599694932
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.13018325368563335
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.13251600693911314
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.13320988066056194
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.13455757747093836
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.13567242653746353
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.1347899205982685
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.1344579862696784
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.13442315432158383
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.1324505196965259
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.13163415032128492
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.13143058001995087
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.13398733047338632
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.13447564288421912
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.1347152101142066
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.13587152803766317
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.1356343597173691
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.13547934159155814
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.13440745859406888
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.13499285348437048
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.13549567430334933
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.1353537591440337
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.1363667148268885
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.13759755141831734
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.13693062392504593
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.1361449943521084
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.13627563882619143
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.13545485513239372
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.13543113303326426
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.13551379307064898
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.13572754287584263
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.1353440824482176
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.1351698980383251
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.13516032347019682
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.13539706046382585
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.13556373666743843
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.1354768145084381
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.1351220334569613
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.13435545506385657
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.13428023429411762
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.1342329771982299
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.13423418646508997
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.1345113760658673
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.1351930608874873
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.1348892686695888
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.1355292004043773
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.13622141977151234
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.1367058343574649
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.1365548932263928
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.13633925952608622
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.13634983589872718
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.13629231590491075
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.13605223991202586
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.13627427627346408
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.1362265976255431
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.13595775396063708
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.13592577406338283
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.1360519251353304
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.1358323786407709
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.13631042695208773
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.13663101015058723
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.1364952717224757
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.13657510594317787
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.13684476853965163
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.13688843792829758
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.13714370112630386
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.13726439084857703
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.13740734112115555
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.13744634026434363
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.13749833261392203
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.13755521976522037
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.13778943419456482
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.13743105981239054
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.13744581756235538
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.13746842030774464
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.13755814262320487
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.13750774496131474
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.13761115565404788
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.13750248969249104
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.1374828908392178
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.13742836184324103
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.13719670843136939
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.1371599715979149
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.13709716230016394
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.1371893293547387
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.13732092240543076
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.13708314307034017
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.13712173825738452
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.13709231356487556
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.13711758862132006
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.13736788262254918
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.1371716083515258
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.1371640976307527
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.13723388021794436
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.13728803741159262
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.13731092378633833
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.13744933686473154
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.13712046436361364
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.13708277472427913
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.13666424992601428
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.136871482392675
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.1368964420064636
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.13700477232963876
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.13713322879157513
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.13688655431240293
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.13708500645491256
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.1369090536609292
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.13696800227746492
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.13698810552720164
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.13686096565267905
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.1369599555168421
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.13729786545038222
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.13715479475638223
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.1373358736122687
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.13730077643413097
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.1370479971632477
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.13706929156413444
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.13703123701892736
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.136777271544843
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.13680980746683322
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.13673057190295476
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.13679327804733205
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.136743471300339
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.13676611801786145
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.13670733706026839
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.13681329952941523
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.13670546486973761
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.13672943962803968
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.13669520123323925
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.13686293082220571
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.13655857239953345
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.1367163619604604
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.13673432074385147
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.1365423148282531
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.13653650854689045
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.1365275729522609
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.13619980841875076
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.13623310280161977
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.13640182837843895
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.13652023312702677
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.1364663148468191
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.1362709084826131
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.1361392069225892
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.13609590599681162
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.13615037958267368
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.13603067505846983
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.13600616683252156
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.13606810676200048
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.13609204703459032
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.13615999736486037
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.1360842904939157
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.13591611082806732
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.13581206366778856
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.13588888555051323
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.13573270854318426
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.1356331328227675
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.13581680896527626
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.13584425431071667
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.1356638657405626
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.1354952327519483
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.13541861362327104
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.13552694520780018
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.1355983951467682
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.13587397391681616
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.13585354584488976
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.13590615359265046
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.13593261891769037
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.13577093146947208
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.13588878852161732
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.13593533655496243
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.13584825983675924
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.13583269268274306
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.13582277910843973
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.13589500540239927
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.1358526953436593
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.13591949480078208
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.13592454371483703
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.13595935729629707
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.13631192893565944
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.1363599079955427
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.13641345474062508
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.13642378002405167
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.13640636635221998
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.1361683124137409
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.13616789075912852
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.13616974730437725
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.13634036835283042
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.13622303095771304
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.13617251544158057
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.1362272018355689
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.13616381972736002
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.136127982779247
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.13621584072853754
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.13607357672735113
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.13590105744795158
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.1358962927423596
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.13606571157773337
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.13607924880009692
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.13622381397575703
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.13625753583482733
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.13630633926558716
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.13628850017869196
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.13628099989835862
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.13624966302500344
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.13629722690910373
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.1363966867803983
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.13613908338275824
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.13615484461525446
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.13610774077273705
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.13619806781210708
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.13615695087771332
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.1362389396958881
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.13614704703862687
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.13609292315491495
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.13598619123692052
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.13598913914121394
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.13600763150531311
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.136072179359017
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.1361623016218173
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.13609165292185263
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.13607130562647796
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.1360015742005186
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.13597786511783883
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.13599262273387064
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.13606236270871483
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.13588356002230026
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.1358893562418719
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.1358587698261273
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.13586049991821456
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.13586550935673616
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.1358519507052957
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.1358813399258925
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.13573603795432462
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.1358078217759789
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.13595510072885983
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.13591981433840641
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.13598147836327554
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.13600853213157313
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.1359631512905397
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.13596573675220663
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.13589550944881176
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.1359559390766948
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.135914010432316
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.13582508993635845
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.13582676155276077
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.1359356890601541
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.13590076138766913
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.1359827348612734
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.13599371139443558
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.13585387153317266
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.1358457931853605
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.13584314804032163
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.13573074598509566
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.13578161100546518
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.1358410978495185
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.13576452292806598
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.1357936040946731
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.13581541320824536
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.13574544419808424
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.1356552303274036
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.135666591353225
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.13548199900171973
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.13544437065180662
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.13552128173921943
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.1353908296433284
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.1353828235659548
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.13546132115381104
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.13540000349177161
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.13545043074281504
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.13537891522418483
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.13533385675138151
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.13552043986947912
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.13556641616396137
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.13563206127296343
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.13555987743246886
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.13547479358717646
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.13554413390570674
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.13555129215479716
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.1355492757914001
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.13564814189798596
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.1356381169792746
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.13565414593381397
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.13579016556409565
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.1357667942340125
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.13582884810314882
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.13578717243611613
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.13568341592947641
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.13566424462486343
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.13559347035869068
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.13561141997477402
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.13560476413878955
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.1357034835170527
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.13568764642562742
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.1356064178753365
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.13563336814185242
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.13566973361768384
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.13560180724147827
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.13552309258861941
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.1354432943253181
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.13549276462759072
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.13548167401058658
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.1356982155451699
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.1356863123423691
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.13570857945864884
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.13570530347104343
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.13570112363671807
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.13564254860393704
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.1356026748676909
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.13572687162838368
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.1357105256003492
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.1356807836089973
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.1357465923520235
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.13572599050914583
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.13577764599786257
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.1357481200020851
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.1357820729490109
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.13574259743997544
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.13564983602286826
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.13569006393383065
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.1356962392772282
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.13572151394572088
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.13566001991727458
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.13577213679395972
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.13573552566395317
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.13569486000128753
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.13569379358340858
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.13578634362886935
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.13582461163445303
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.135797588013069
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.13583247932678758
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.13576582223610129
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.13584016555029413
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.13577970644900564
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.1358273895269512
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.13582508906122598
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.13578162453321468
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.13584002054163388
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.1358322986637765
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.13580974456007508
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.13577272591020162
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.13590861665411183
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.13581916024567375
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.13581201980371824
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.13575246850816475
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.13570919023129527
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.1356969690547016
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.13566934919605653
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.13568416516453935
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.13568173849039314
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.13569394100878193
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.13568154708124125
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.13569756364577437
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.1357572670883494
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.13569249978101222
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.13576015605307792
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.1357678264419884
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.1359033778913923
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.1359256039451396
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.1359206756316526
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.13589634630459563
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.1358917026636116
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.13586821240186692
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.13585043863016874
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.13597477389504486
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.13600506958743883
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.1361077533823519
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.13614655476259557
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.1361242860281874
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.13614246953298284
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.13614672354322813
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.13614333128013337
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.13609490531992602
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.13609853689920717
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.13604058013405912
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.13608803842024705
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.13605759063002995
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.13613899332972673
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.13620103550765217
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.1362165101442714
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.1362193874237495
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.13617833357701448
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.13619095590672917
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.13621899574016683
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.13620534008110802
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.13618692532930543
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.1362188518756912
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.13618140168488024
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.13629619242098565
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.13633776585854107
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.13630521291908496
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.13631266626612384
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.1363361284872632
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.13636173801542503
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.13636006105517287
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.1362876483768809
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.13630397519447401
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.1363027233903001
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.13638999967534465
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.13637789129053504
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.1362977305350523
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.1362099812730499
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.13617872954851173
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.13619676432930505
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.13615562650654242
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.13615443810606687
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.13616645400165658
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.13613511284901983
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.13613077605280344
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.1361480336548028
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.13612216365816462
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.1361592311130942
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.13613255900495191
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.13608334953306425
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.1360720724251287
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.1360880978161765
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.13607220151971827
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.13615890451988508
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.1361253221115214
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.13616831283326503
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.13619407219919816
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.1361961192730385
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.13623297704362322
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.13629049083235067
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.1362672513286909
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.1363192478611589
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.13629021390356072
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.13630720405754718
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.13633887011921053
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.13634348385946243
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.1363911881841063
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.13637890024026772
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.13633300955041072
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.13635089330274963
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.13632614572242868
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.13631068647373468
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.13629956581611144
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.13626770552661685
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.13630498384028475
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.13627286416898787
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.13622563222123035
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.13613452171570403
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.1360498226933427
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.13606839976681953
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.13605145431843055
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.13601870480192801
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.13599052434393524
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.1359791685057723
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.1360392891468557
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.13601158988295178
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.13598291466483287
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.1360234450870033
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.13602332046595952
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.13607973426324615
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.13614712492812114
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.1361446615276683
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.13613421716161375
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.13605426431019255
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.13605863781301838
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.13600056727369458
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.13600056727369458 valid 0.18716847896575928
LOSS train 0.13600056727369458 valid 0.19763851910829544
LOSS train 0.13600056727369458 valid 0.19831634561220804
LOSS train 0.13600056727369458 valid 0.18772775679826736
LOSS train 0.13600056727369458 valid 0.18679986298084258
LOSS train 0.13600056727369458 valid 0.1915444160501162
LOSS train 0.13600056727369458 valid 0.19058908309255326
LOSS train 0.13600056727369458 valid 0.18951652385294437
LOSS train 0.13600056727369458 valid 0.18724965386920506
LOSS train 0.13600056727369458 valid 0.18766382038593293
LOSS train 0.13600056727369458 valid 0.18796303461898456
LOSS train 0.13600056727369458 valid 0.1921306736767292
LOSS train 0.13600056727369458 valid 0.19206569171868837
LOSS train 0.13600056727369458 valid 0.19096692970820836
LOSS train 0.13600056727369458 valid 0.19100241561730702
LOSS train 0.13600056727369458 valid 0.19286016281694174
LOSS train 0.13600056727369458 valid 0.19496819727561054
LOSS train 0.13600056727369458 valid 0.1951795311437713
LOSS train 0.13600056727369458 valid 0.1971955707198695
LOSS train 0.13600056727369458 valid 0.19637364745140076
LOSS train 0.13600056727369458 valid 0.19738329024541945
LOSS train 0.13600056727369458 valid 0.19654508070512253
LOSS train 0.13600056727369458 valid 0.19552911623664523
LOSS train 0.13600056727369458 valid 0.19625055976212025
LOSS train 0.13600056727369458 valid 0.19522460877895356
LOSS train 0.13600056727369458 valid 0.19444755694040886
LOSS train 0.13600056727369458 valid 0.19412672022978464
LOSS train 0.13600056727369458 valid 0.1943828681749957
LOSS train 0.13600056727369458 valid 0.19269292981460176
LOSS train 0.13600056727369458 valid 0.19282070050636926
LOSS train 0.13600056727369458 valid 0.19335545984006697
LOSS train 0.13600056727369458 valid 0.1936897300183773
LOSS train 0.13600056727369458 valid 0.19275010173971002
LOSS train 0.13600056727369458 valid 0.19197128274861505
LOSS train 0.13600056727369458 valid 0.19211901937212264
LOSS train 0.13600056727369458 valid 0.1924228978653749
LOSS train 0.13600056727369458 valid 0.1928130472834046
LOSS train 0.13600056727369458 valid 0.19313569170864006
LOSS train 0.13600056727369458 valid 0.1935224643884561
LOSS train 0.13600056727369458 valid 0.19298681132495404
LOSS train 0.13600056727369458 valid 0.19286090026541455
LOSS train 0.13600056727369458 valid 0.19375007635071165
LOSS train 0.13600056727369458 valid 0.1948728218328121
LOSS train 0.13600056727369458 valid 0.19433177533474835
LOSS train 0.13600056727369458 valid 0.1936397330628501
LOSS train 0.13600056727369458 valid 0.1926442277820214
LOSS train 0.13600056727369458 valid 0.19240839335512608
LOSS train 0.13600056727369458 valid 0.19365994725376368
LOSS train 0.13600056727369458 valid 0.19338651548842994
LOSS train 0.13600056727369458 valid 0.19361433267593384
LOSS train 0.13600056727369458 valid 0.19313272278682858
LOSS train 0.13600056727369458 valid 0.19280306335825187
LOSS train 0.13600056727369458 valid 0.19410867651678482
LOSS train 0.13600056727369458 valid 0.19410360742498328
LOSS train 0.13600056727369458 valid 0.1939433515071869
LOSS train 0.13600056727369458 valid 0.1938617735036782
LOSS train 0.13600056727369458 valid 0.19335814055643583
LOSS train 0.13600056727369458 valid 0.19383894420903305
LOSS train 0.13600056727369458 valid 0.19348242121227718
LOSS train 0.13600056727369458 valid 0.19311092520753542
LOSS train 0.13600056727369458 valid 0.19293578555349444
LOSS train 0.13600056727369458 valid 0.19258853552802915
LOSS train 0.13600056727369458 valid 0.19236753526188077
LOSS train 0.13600056727369458 valid 0.1924882356543094
LOSS train 0.13600056727369458 valid 0.19186031314042898
LOSS train 0.13600056727369458 valid 0.19185083652987625
LOSS train 0.13600056727369458 valid 0.1923825838672581
LOSS train 0.13600056727369458 valid 0.1917045009048546
LOSS train 0.13600056727369458 valid 0.19209063247494076
LOSS train 0.13600056727369458 valid 0.19229521623679569
LOSS train 0.13600056727369458 valid 0.19211411098359335
LOSS train 0.13600056727369458 valid 0.19258994505637222
LOSS train 0.13600056727369458 valid 0.19305583610110086
LOSS train 0.13600056727369458 valid 0.19279910060199532
LOSS train 0.13600056727369458 valid 0.19269884943962098
LOSS train 0.13600056727369458 valid 0.1925540227246912
LOSS train 0.13600056727369458 valid 0.1924892245949089
LOSS train 0.13600056727369458 valid 0.19218827172731742
LOSS train 0.13600056727369458 valid 0.19228389953510672
LOSS train 0.13600056727369458 valid 0.19176920913159848
LOSS train 0.13600056727369458 valid 0.19201116484624367
LOSS train 0.13600056727369458 valid 0.19224500674300077
LOSS train 0.13600056727369458 valid 0.19229598774249296
LOSS train 0.13600056727369458 valid 0.19194311630867777
LOSS train 0.13600056727369458 valid 0.1921617330873714
LOSS train 0.13600056727369458 valid 0.19211604948653732
LOSS train 0.13600056727369458 valid 0.1919016831222622
LOSS train 0.13600056727369458 valid 0.191801882264289
LOSS train 0.13600056727369458 valid 0.19204416100898486
LOSS train 0.13600056727369458 valid 0.19201986889044445
LOSS train 0.13600056727369458 valid 0.19210950850130437
LOSS train 0.13600056727369458 valid 0.19202006915989128
LOSS train 0.13600056727369458 valid 0.1922555878277748
LOSS train 0.13600056727369458 valid 0.19239054223958482
LOSS train 0.13600056727369458 valid 0.19293339362269954
LOSS train 0.13600056727369458 valid 0.19380346639081836
LOSS train 0.13600056727369458 valid 0.19398929301610926
LOSS train 0.13600056727369458 valid 0.19463439969992152
LOSS train 0.13600056727369458 valid 0.1945107097577567
LOSS train 0.13600056727369458 valid 0.19463282972574233
LOSS train 0.13600056727369458 valid 0.19450067278772298
LOSS train 0.13600056727369458 valid 0.1948955465181201
LOSS train 0.13600056727369458 valid 0.19458145221460213
LOSS train 0.13600056727369458 valid 0.19448477488297683
LOSS train 0.13600056727369458 valid 0.1945983560312362
LOSS train 0.13600056727369458 valid 0.1943880520620436
LOSS train 0.13600056727369458 valid 0.19415950747293848
LOSS train 0.13600056727369458 valid 0.19425371868742836
LOSS train 0.13600056727369458 valid 0.1940055011062447
LOSS train 0.13600056727369458 valid 0.1937670966440981
LOSS train 0.13600056727369458 valid 0.1939971002641025
LOSS train 0.13600056727369458 valid 0.19419609236397914
LOSS train 0.13600056727369458 valid 0.19395329255973343
LOSS train 0.13600056727369458 valid 0.19402403920366054
LOSS train 0.13600056727369458 valid 0.1945118476515231
LOSS train 0.13600056727369458 valid 0.19428387695345387
LOSS train 0.13600056727369458 valid 0.19458416473661733
LOSS train 0.13600056727369458 valid 0.19440580399359686
LOSS train 0.13600056727369458 valid 0.19420840710151097
LOSS train 0.13600056727369458 valid 0.19433779940009116
LOSS train 0.13600056727369458 valid 0.19419194770253395
LOSS train 0.13600056727369458 valid 0.19443756268649806
LOSS train 0.13600056727369458 valid 0.19452458959284838
LOSS train 0.13600056727369458 valid 0.19480823917735007
LOSS train 0.13600056727369458 valid 0.1947408481836319
LOSS train 0.13600056727369458 valid 0.194799419078562
LOSS train 0.13600056727369458 valid 0.19463738411899625
LOSS train 0.13600056727369458 valid 0.19469902908895165
LOSS train 0.13600056727369458 valid 0.19456015862235726
LOSS train 0.13600056727369458 valid 0.19435517432597968
LOSS train 0.13600056727369458 valid 0.19450852320394443
LOSS train 0.13600056727369458 valid 0.19480478199142398
LOSS train 0.13600056727369458 valid 0.194825664050597
LOSS train 0.13600056727369458 valid 0.1950625475885263
LOSS train 0.13600056727369458 valid 0.19506855838828616
LOSS train 0.13600056727369458 valid 0.19530348435920827
LOSS train 0.13600056727369458 valid 0.19525358537687873
LOSS train 0.13600056727369458 valid 0.19493505930986957
LOSS train 0.13600056727369458 valid 0.19484766771038659
LOSS train 0.13600056727369458 valid 0.19456522443464824
LOSS train 0.13600056727369458 valid 0.1948241743635624
LOSS train 0.13600056727369458 valid 0.19507636234793865
LOSS train 0.13600056727369458 valid 0.19525490263542095
LOSS train 0.13600056727369458 valid 0.19500790939976773
LOSS train 0.13600056727369458 valid 0.1948674663387496
LOSS train 0.13600056727369458 valid 0.19478607096084177
LOSS train 0.13600056727369458 valid 0.19477146367232004
LOSS train 0.13600056727369458 valid 0.19458529443756953
LOSS train 0.13600056727369458 valid 0.19479148039881816
LOSS train 0.13600056727369458 valid 0.19468820651372273
LOSS train 0.13600056727369458 valid 0.19459869145162847
LOSS train 0.13600056727369458 valid 0.1945664541501748
LOSS train 0.13600056727369458 valid 0.1943002760020736
LOSS train 0.13600056727369458 valid 0.19424293567607928
LOSS train 0.13600056727369458 valid 0.19415965310988886
LOSS train 0.13600056727369458 valid 0.19417708691878197
LOSS train 0.13600056727369458 valid 0.19388514311070654
LOSS train 0.13600056727369458 valid 0.19379824692312675
LOSS train 0.13600056727369458 valid 0.1941287082696111
LOSS train 0.13600056727369458 valid 0.19411661177873613
LOSS train 0.13600056727369458 valid 0.19408836211106792
LOSS train 0.13600056727369458 valid 0.1940352076917519
LOSS train 0.13600056727369458 valid 0.19389051939446503
LOSS train 0.13600056727369458 valid 0.193837343556125
LOSS train 0.13600056727369458 valid 0.19403808270439957
LOSS train 0.13600056727369458 valid 0.1938772649470582
LOSS train 0.13600056727369458 valid 0.19398164731299805
LOSS train 0.13600056727369458 valid 0.1938084881930124
LOSS train 0.13600056727369458 valid 0.19369226835183137
LOSS train 0.13600056727369458 valid 0.1936799307079876
LOSS train 0.13600056727369458 valid 0.19352187956982886
LOSS train 0.13600056727369458 valid 0.19375653807507004
LOSS train 0.13600056727369458 valid 0.19373241325334317
LOSS train 0.13600056727369458 valid 0.1939867273315616
LOSS train 0.13600056727369458 valid 0.193808667744909
LOSS train 0.13600056727369458 valid 0.19373026210814714
LOSS train 0.13600056727369458 valid 0.19371208338077459
LOSS train 0.13600056727369458 valid 0.19371343764026513
LOSS train 0.13600056727369458 valid 0.193736073607839
LOSS train 0.13600056727369458 valid 0.19370153629117542
LOSS train 0.13600056727369458 valid 0.19365732429435897
LOSS train 0.13600056727369458 valid 0.19363170878572777
LOSS train 0.13600056727369458 valid 0.19389356811189912
LOSS train 0.13600056727369458 valid 0.1939786106510007
LOSS train 0.13600056727369458 valid 0.19393761584887634
LOSS train 0.13600056727369458 valid 0.19407113641500473
LOSS train 0.13600056727369458 valid 0.19396420038320164
LOSS train 0.13600056727369458 valid 0.19398852001796377
LOSS train 0.13600056727369458 valid 0.19367801520244154
LOSS train 0.13600056727369458 valid 0.19369124127061743
LOSS train 0.13600056727369458 valid 0.1937395910005919
LOSS train 0.13600056727369458 valid 0.1938060060298691
LOSS train 0.13600056727369458 valid 0.19354399204871814
LOSS train 0.13600056727369458 valid 0.19358199612074292
LOSS train 0.13600056727369458 valid 0.19346185746865394
LOSS train 0.13600056727369458 valid 0.19364053451893282
LOSS train 0.13600056727369458 valid 0.19372951598639415
LOSS train 0.13600056727369458 valid 0.19393048721431483
LOSS train 0.13600056727369458 valid 0.19394553696090852
LOSS train 0.13600056727369458 valid 0.1942649409174919
LOSS train 0.13600056727369458 valid 0.19417685018250005
LOSS train 0.13600056727369458 valid 0.19426647846651549
LOSS train 0.13600056727369458 valid 0.19430183454100133
LOSS train 0.13600056727369458 valid 0.1940981093428883
LOSS train 0.13600056727369458 valid 0.19397676100091235
LOSS train 0.13600056727369458 valid 0.19391531453838626
LOSS train 0.13600056727369458 valid 0.19387002087733596
LOSS train 0.13600056727369458 valid 0.19373146122178206
LOSS train 0.13600056727369458 valid 0.19375985340353405
LOSS train 0.13600056727369458 valid 0.19394664835362208
LOSS train 0.13600056727369458 valid 0.19420376504766998
LOSS train 0.13600056727369458 valid 0.1939758025109768
LOSS train 0.13600056727369458 valid 0.1938425813202567
LOSS train 0.13600056727369458 valid 0.19378524663570884
LOSS train 0.13600056727369458 valid 0.19353206760661545
LOSS train 0.13600056727369458 valid 0.19328427031912185
LOSS train 0.13600056727369458 valid 0.19319924389437046
LOSS train 0.13600056727369458 valid 0.19325152135223422
LOSS train 0.13600056727369458 valid 0.19322660735480862
LOSS train 0.13600056727369458 valid 0.1932147393849763
LOSS train 0.13600056727369458 valid 0.19322185683574072
LOSS train 0.13600056727369458 valid 0.19337894049313692
LOSS train 0.13600056727369458 valid 0.19336489297349357
LOSS train 0.13600056727369458 valid 0.19328761965568578
LOSS train 0.13600056727369458 valid 0.19302253471480477
LOSS train 0.13600056727369458 valid 0.1928211648379807
LOSS train 0.13600056727369458 valid 0.19284089178765923
LOSS train 0.13600056727369458 valid 0.19288690192134758
LOSS train 0.13600056727369458 valid 0.19280678782140323
LOSS train 0.13600056727369458 valid 0.1929243280187897
LOSS train 0.13600056727369458 valid 0.19287852252716625
LOSS train 0.13600056727369458 valid 0.19311243411282014
LOSS train 0.13600056727369458 valid 0.19333962124048895
LOSS train 0.13600056727369458 valid 0.19335376484017086
LOSS train 0.13600056727369458 valid 0.19341065985091188
LOSS train 0.13600056727369458 valid 0.1932577533742129
LOSS train 0.13600056727369458 valid 0.19334239132293668
LOSS train 0.13600056727369458 valid 0.1932893693447113
LOSS train 0.13600056727369458 valid 0.1932535791870939
LOSS train 0.13600056727369458 valid 0.1928900166725119
LOSS train 0.13600056727369458 valid 0.1927863371817403
LOSS train 0.13600056727369458 valid 0.19275159191740446
LOSS train 0.13600056727369458 valid 0.19275310016218036
LOSS train 0.13600056727369458 valid 0.19275222193510805
LOSS train 0.13600056727369458 valid 0.1929640504778648
LOSS train 0.13600056727369458 valid 0.19309015602357987
LOSS train 0.13600056727369458 valid 0.1930980056765591
LOSS train 0.13600056727369458 valid 0.1930411501877731
LOSS train 0.13600056727369458 valid 0.1929257467808015
LOSS train 0.13600056727369458 valid 0.1929702245593071
LOSS train 0.13600056727369458 valid 0.19301424415937932
LOSS train 0.13600056727369458 valid 0.19300875392934633
LOSS train 0.13600056727369458 valid 0.19287373137332706
LOSS train 0.13600056727369458 valid 0.1929392191016768
LOSS train 0.13600056727369458 valid 0.1928586124205122
LOSS train 0.13600056727369458 valid 0.1927210974972695
LOSS train 0.13600056727369458 valid 0.19259704759612623
LOSS train 0.13600056727369458 valid 0.192734798373178
LOSS train 0.13600056727369458 valid 0.1926958516642854
LOSS train 0.13600056727369458 valid 0.19265234871552542
LOSS train 0.13600056727369458 valid 0.19264787623937102
LOSS train 0.13600056727369458 valid 0.19280706038911835
LOSS train 0.13600056727369458 valid 0.19268842120814234
LOSS train 0.13600056727369458 valid 0.19267283470341653
LOSS train 0.13600056727369458 valid 0.19276919308698404
LOSS train 0.13600056727369458 valid 0.19288281289706552
LOSS train 0.13600056727369458 valid 0.19280243868684946
LOSS train 0.13600056727369458 valid 0.19289975390950245
LOSS train 0.13600056727369458 valid 0.19291426036437648
LOSS train 0.13600056727369458 valid 0.19276120331552293
LOSS train 0.13600056727369458 valid 0.19268055733059605
LOSS train 0.13600056727369458 valid 0.19259955016348293
LOSS train 0.13600056727369458 valid 0.19245422994479155
LOSS train 0.13600056727369458 valid 0.1923815731475823
LOSS train 0.13600056727369458 valid 0.1924763365767219
LOSS train 0.13600056727369458 valid 0.19259663228539453
LOSS train 0.13600056727369458 valid 0.19248368432375498
LOSS train 0.13600056727369458 valid 0.19245005876040286
LOSS train 0.13600056727369458 valid 0.19240433607904714
LOSS train 0.13600056727369458 valid 0.19232992040259497
LOSS train 0.13600056727369458 valid 0.1922124354864779
LOSS train 0.13600056727369458 valid 0.1921648200839124
LOSS train 0.13600056727369458 valid 0.19207961135955245
LOSS train 0.13600056727369458 valid 0.19194771317948758
LOSS train 0.13600056727369458 valid 0.19199928923657067
LOSS train 0.13600056727369458 valid 0.19196788904133377
LOSS train 0.13600056727369458 valid 0.19212066301693068
LOSS train 0.13600056727369458 valid 0.19204503054627114
LOSS train 0.13600056727369458 valid 0.19199982775330132
LOSS train 0.13600056727369458 valid 0.1918533376578627
LOSS train 0.13600056727369458 valid 0.19179330151720145
LOSS train 0.13600056727369458 valid 0.19177035073915574
LOSS train 0.13600056727369458 valid 0.19172824466594657
LOSS train 0.13600056727369458 valid 0.19172648360737327
LOSS train 0.13600056727369458 valid 0.19176578577292167
LOSS train 0.13600056727369458 valid 0.1916794242790422
LOSS train 0.13600056727369458 valid 0.19162185169028914
LOSS train 0.13600056727369458 valid 0.1916223033362587
LOSS train 0.13600056727369458 valid 0.19165132924864522
LOSS train 0.13600056727369458 valid 0.19169119596481324
LOSS train 0.13600056727369458 valid 0.1916148608111068
LOSS train 0.13600056727369458 valid 0.19154714649876223
LOSS train 0.13600056727369458 valid 0.19157103844995152
LOSS train 0.13600056727369458 valid 0.19154344920657182
LOSS train 0.13600056727369458 valid 0.19143873482454019
LOSS train 0.13600056727369458 valid 0.19165070452331717
LOSS train 0.13600056727369458 valid 0.19168488865760716
LOSS train 0.13600056727369458 valid 0.19171609908535883
LOSS train 0.13600056727369458 valid 0.19166118461144394
LOSS train 0.13600056727369458 valid 0.19165205229674617
LOSS train 0.13600056727369458 valid 0.1916976396677195
LOSS train 0.13600056727369458 valid 0.19165656567575076
LOSS train 0.13600056727369458 valid 0.19152102388512973
LOSS train 0.13600056727369458 valid 0.1914121599239149
LOSS train 0.13600056727369458 valid 0.19133903062532817
LOSS train 0.13600056727369458 valid 0.19128440414802939
LOSS train 0.13600056727369458 valid 0.19130904450777578
LOSS train 0.13600056727369458 valid 0.19136275336989816
LOSS train 0.13600056727369458 valid 0.1913355095166024
LOSS train 0.13600056727369458 valid 0.19119590423069893
LOSS train 0.13600056727369458 valid 0.19123292196762526
LOSS train 0.13600056727369458 valid 0.1911130139361257
LOSS train 0.13600056727369458 valid 0.1910354155966372
LOSS train 0.13600056727369458 valid 0.1909849792258975
LOSS train 0.13600056727369458 valid 0.1908315789241057
LOSS train 0.13600056727369458 valid 0.19093296498608736
LOSS train 0.13600056727369458 valid 0.1908285909016198
LOSS train 0.13600056727369458 valid 0.19084263811024224
LOSS train 0.13600056727369458 valid 0.1908920098401855
LOSS train 0.13600056727369458 valid 0.1909491403536363
LOSS train 0.13600056727369458 valid 0.19085368293469765
LOSS train 0.13600056727369458 valid 0.19083697182765927
LOSS train 0.13600056727369458 valid 0.19083927817888804
LOSS train 0.13600056727369458 valid 0.19076979410148667
LOSS train 0.13600056727369458 valid 0.1906243503983341
LOSS train 0.13600056727369458 valid 0.19053552849661737
LOSS train 0.13600056727369458 valid 0.19062958971682925
LOSS train 0.13600056727369458 valid 0.1906349877633992
LOSS train 0.13600056727369458 valid 0.1905615106444795
LOSS train 0.13600056727369458 valid 0.19068383004735498
LOSS train 0.13600056727369458 valid 0.19066784860801136
LOSS train 0.13600056727369458 valid 0.19066485897665136
LOSS train 0.13600056727369458 valid 0.19054946029672817
LOSS train 0.13600056727369458 valid 0.1906918487836455
LOSS train 0.13600056727369458 valid 0.19075309448484062
LOSS train 0.13600056727369458 valid 0.19070780419373098
LOSS train 0.13600056727369458 valid 0.19065186754942628
LOSS train 0.13600056727369458 valid 0.19066203398437337
LOSS train 0.13600056727369458 valid 0.19068420492646346
LOSS train 0.13600056727369458 valid 0.19074051669665745
LOSS train 0.13600056727369458 valid 0.1908213342102165
LOSS train 0.13600056727369458 valid 0.19088224550201136
LOSS train 0.13600056727369458 valid 0.19094675618933551
LOSS train 0.13600056727369458 valid 0.19093295759232032
LOSS train 0.13600056727369458 valid 0.19080604256038936
LOSS train 0.13600056727369458 valid 0.19080694555566552
LOSS train 0.13600056727369458 valid 0.19075073990501276
LOSS train 0.13600056727369458 valid 0.19074923237109317
LOSS train 0.13600056727369458 valid 0.1906740762978211
LOSS train 0.13600056727369458 valid 0.19091559867892
LOSS train 0.13600056727369458 valid 0.19081612257415898
LOSS train 0.13600056727369458 valid 0.19092478854221534
LOSS train 0.13600056727369458 valid 0.19094461325771553
LOSS train 0.13600056727369458 valid 0.19085573843056028
LOSS train 0.13600056727369458 valid 0.19089508746584802
LOSS train 0.13600056727369458 valid 0.19086970739025888
LOSS train 0.13600056727369458 valid 0.1908401407565343
LOSS train 0.13600056727369458 valid 0.19074621553654256
LOSS train 0.13600056727369458 valid 0.19080407723663298
EPOCH 8:
