Training seq
DEVICE = cpu
####################
Total Parameters = 605185
Total Trainable Parameters = 605185
bichrom_seq(
  (conv1d): Conv1d(4, 256, kernel_size=(24,), stride=(1,))
  (relu): ReLU()
  (batchNorm1d): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (maxPool1d): MaxPool1d(kernel_size=15, stride=15, padding=0, dilation=1, ceil_mode=True)
  (lstm): LSTM(256, 32, batch_first=True)
  (tanh): Tanh()
  (model_dense_repeat): Sequential(
    (0): Linear(in_features=32, out_features=512, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.5, inplace=False)
    (3): Linear(in_features=512, out_features=512, bias=True)
    (4): ReLU()
    (5): Dropout(p=0.5, inplace=False)
    (6): Linear(in_features=512, out_features=512, bias=True)
    (7): ReLU()
    (8): Dropout(p=0.5, inplace=False)
  )
  (linear): Linear(in_features=512, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)
####################
EPOCH 1:
  batch 1 loss: 0.6921278238296509
  batch 2 loss: 0.6925957798957825
  batch 3 loss: 0.6944204568862915
  batch 4 loss: 0.6938979625701904
  batch 5 loss: 0.694280469417572
  batch 6 loss: 0.6937068005402883
  batch 7 loss: 0.6934350218091693
  batch 8 loss: 0.6933959424495697
  batch 9 loss: 0.6935486793518066
  batch 10 loss: 0.693332839012146
  batch 11 loss: 0.6932127692482688
  batch 12 loss: 0.6932198057572047
  batch 13 loss: 0.6929837877933795
  batch 14 loss: 0.6924893856048584
  batch 15 loss: 0.69250009059906
  batch 16 loss: 0.6916772201657295
  batch 17 loss: 0.6911598619292764
  batch 18 loss: 0.6905631754133437
  batch 19 loss: 0.6899856015255577
  batch 20 loss: 0.6896305501461029
  batch 21 loss: 0.6892272773243132
  batch 22 loss: 0.6883825849403035
  batch 23 loss: 0.6876381195109823
  batch 24 loss: 0.686968651910623
  batch 25 loss: 0.6872558832168579
  batch 26 loss: 0.6861828565597534
  batch 27 loss: 0.6854279217896638
  batch 28 loss: 0.6841375998088292
  batch 29 loss: 0.6845429683553761
  batch 30 loss: 0.6836373011271158
  batch 31 loss: 0.6824331533524298
  batch 32 loss: 0.6815443467348814
  batch 33 loss: 0.6808498917203961
  batch 34 loss: 0.6802691045929404
  batch 35 loss: 0.6800409282956804
  batch 36 loss: 0.679172138373057
  batch 37 loss: 0.67808055233311
  batch 38 loss: 0.6778111646049901
  batch 39 loss: 0.6772439250579247
  batch 40 loss: 0.676499517261982
  batch 41 loss: 0.6753510469343604
  batch 42 loss: 0.6748799057233901
  batch 43 loss: 0.6748761091121408
  batch 44 loss: 0.6743131388317455
  batch 45 loss: 0.6741243905491299
  batch 46 loss: 0.673857964899229
  batch 47 loss: 0.6741411635216247
  batch 48 loss: 0.6735748661061128
  batch 49 loss: 0.6734405926295689
  batch 50 loss: 0.6728027629852295
  batch 51 loss: 0.6726619542813769
  batch 52 loss: 0.6723585999928988
  batch 53 loss: 0.6720710857859198
  batch 54 loss: 0.6713174967854111
  batch 55 loss: 0.670746036009355
  batch 56 loss: 0.6702925286122731
  batch 57 loss: 0.6703733320821795
  batch 58 loss: 0.6704145598000494
  batch 59 loss: 0.6696363657207812
  batch 60 loss: 0.6694716910521189
  batch 61 loss: 0.668972584067798
  batch 62 loss: 0.668968471788591
  batch 63 loss: 0.668797616920774
  batch 64 loss: 0.6684139538556337
  batch 65 loss: 0.6679872127679678
  batch 66 loss: 0.6678653612281337
  batch 67 loss: 0.6673875949276027
  batch 68 loss: 0.6668999238925821
  batch 69 loss: 0.6663836966390195
  batch 70 loss: 0.665723638023649
  batch 71 loss: 0.665049842545684
  batch 72 loss: 0.664621556798617
  batch 73 loss: 0.6641630244581667
  batch 74 loss: 0.6634981293936033
  batch 75 loss: 0.6631770896911621
  batch 76 loss: 0.6629611316480135
  batch 77 loss: 0.6626901208580315
  batch 78 loss: 0.6623856784441532
  batch 79 loss: 0.6621494195129298
  batch 80 loss: 0.6615308977663517
  batch 81 loss: 0.661049019407343
  batch 82 loss: 0.6609678515573827
  batch 83 loss: 0.6606390562402197
  batch 84 loss: 0.6605414769479206
  batch 85 loss: 0.6599967802272123
  batch 86 loss: 0.6598186998866326
  batch 87 loss: 0.6593845219447695
  batch 88 loss: 0.6588583974675699
  batch 89 loss: 0.6586461442240169
  batch 90 loss: 0.6580260323153602
  batch 91 loss: 0.6577238067165836
  batch 92 loss: 0.6571326430725015
  batch 93 loss: 0.6565275942125628
  batch 94 loss: 0.6559733989390921
  batch 95 loss: 0.6556351385618511
  batch 96 loss: 0.6551007628440857
  batch 97 loss: 0.6544194166193303
  batch 98 loss: 0.6540195370207027
  batch 99 loss: 0.6536143584684893
  batch 100 loss: 0.6530000686645507
  batch 101 loss: 0.6524150105986265
  batch 102 loss: 0.6518502965861676
  batch 103 loss: 0.6514511935919234
  batch 104 loss: 0.6509997970782794
  batch 105 loss: 0.6501787282171704
  batch 106 loss: 0.6497711859784037
  batch 107 loss: 0.6490482348147953
  batch 108 loss: 0.6484043691997174
  batch 109 loss: 0.6478218523734206
  batch 110 loss: 0.6474857953461733
  batch 111 loss: 0.6469493540557655
  batch 112 loss: 0.6461080914097173
  batch 113 loss: 0.64541859647869
  batch 114 loss: 0.6449439990938756
  batch 115 loss: 0.6439781437749448
  batch 116 loss: 0.6433102036344593
  batch 117 loss: 0.6427155712730864
  batch 118 loss: 0.6419791008456278
  batch 119 loss: 0.6410576936577549
  batch 120 loss: 0.6398497246205807
  batch 121 loss: 0.6389927605459512
  batch 122 loss: 0.6377960304256345
  batch 123 loss: 0.6366584230729235
  batch 124 loss: 0.6359338594540473
  batch 125 loss: 0.6348677461147308
  batch 126 loss: 0.6339294856029843
  batch 127 loss: 0.6332563694537156
  batch 128 loss: 0.6325295560527593
  batch 129 loss: 0.6315713134384895
  batch 130 loss: 0.6304935684570899
  batch 131 loss: 0.6297694899653661
  batch 132 loss: 0.628954903645949
  batch 133 loss: 0.6287143378329456
  batch 134 loss: 0.6278040253404361
  batch 135 loss: 0.6268770911075451
  batch 136 loss: 0.6258544551537317
  batch 137 loss: 0.6250804181951676
  batch 138 loss: 0.6241877729046172
  batch 139 loss: 0.6234224650928443
  batch 140 loss: 0.6225013202854566
  batch 141 loss: 0.621617594931988
  batch 142 loss: 0.6205333924629319
  batch 143 loss: 0.6195046234797764
  batch 144 loss: 0.6186244040727615
  batch 145 loss: 0.617495687254544
  batch 146 loss: 0.6164736798773073
  batch 147 loss: 0.6159107111343721
  batch 148 loss: 0.6150688929735003
  batch 149 loss: 0.6140838105406538
  batch 150 loss: 0.6132257457574208
  batch 151 loss: 0.6124069526890256
  batch 152 loss: 0.6115142334448663
  batch 153 loss: 0.6105798613401799
  batch 154 loss: 0.6097284822494953
  batch 155 loss: 0.6086893914207335
  batch 156 loss: 0.6080076589416235
  batch 157 loss: 0.6075089260651048
  batch 158 loss: 0.6065375757745549
  batch 159 loss: 0.6056766146383945
  batch 160 loss: 0.604882855899632
  batch 161 loss: 0.6039452687923953
  batch 162 loss: 0.6027855203475481
  batch 163 loss: 0.601858775308527
  batch 164 loss: 0.6010119638791899
  batch 165 loss: 0.5997657140096029
  batch 166 loss: 0.5989771569708744
  batch 167 loss: 0.598027765929342
  batch 168 loss: 0.5973139462016878
  batch 169 loss: 0.5963117326857775
  batch 170 loss: 0.5953545586151235
  batch 171 loss: 0.5943339620417322
  batch 172 loss: 0.5935014829386113
  batch 173 loss: 0.5927700431360675
  batch 174 loss: 0.5919397754230719
  batch 175 loss: 0.5910630828993662
  batch 176 loss: 0.5898744154044173
  batch 177 loss: 0.5889155842826865
  batch 178 loss: 0.5881730157337831
  batch 179 loss: 0.5875429043223738
  batch 180 loss: 0.5865586438112789
  batch 181 loss: 0.5857470786373918
  batch 182 loss: 0.58483166714291
  batch 183 loss: 0.5837886730829874
  batch 184 loss: 0.5828668321280376
  batch 185 loss: 0.582061266415828
  batch 186 loss: 0.5815091120299473
  batch 187 loss: 0.5805967896379889
  batch 188 loss: 0.5796343966367397
  batch 189 loss: 0.5787662505788147
  batch 190 loss: 0.5778820290377266
  batch 191 loss: 0.5769829193022863
  batch 192 loss: 0.5760678433192273
  batch 193 loss: 0.5752988358853395
  batch 194 loss: 0.5746569808610936
  batch 195 loss: 0.5739182151280917
  batch 196 loss: 0.5729799173316177
  batch 197 loss: 0.5719632231039444
  batch 198 loss: 0.5713000280989541
  batch 199 loss: 0.5706622324996258
  batch 200 loss: 0.5700142286717892
  batch 201 loss: 0.569228327541209
  batch 202 loss: 0.5684623766948681
  batch 203 loss: 0.567634027873354
  batch 204 loss: 0.5668300118516473
  batch 205 loss: 0.5661801601328501
  batch 206 loss: 0.5654715395089492
  batch 207 loss: 0.564670239093799
  batch 208 loss: 0.563845547202688
  batch 209 loss: 0.5629833758447729
  batch 210 loss: 0.5623218201455615
  batch 211 loss: 0.5616696580043902
  batch 212 loss: 0.5609870264833828
  batch 213 loss: 0.5603415176902019
  batch 214 loss: 0.5596193192161132
  batch 215 loss: 0.5587874391744303
  batch 216 loss: 0.5580851672976105
  batch 217 loss: 0.5575406729900343
  batch 218 loss: 0.5568901769338398
  batch 219 loss: 0.5562328200906379
  batch 220 loss: 0.5554703601381995
  batch 221 loss: 0.55488389258471
  batch 222 loss: 0.5543457356927631
  batch 223 loss: 0.5536777022975443
  batch 224 loss: 0.5532433860270041
  batch 225 loss: 0.552413578695721
  batch 226 loss: 0.5518543831015055
  batch 227 loss: 0.5510725654694477
  batch 228 loss: 0.5503894314169884
  batch 229 loss: 0.5497059150554208
  batch 230 loss: 0.5492960669424223
  batch 231 loss: 0.5487626996133235
  batch 232 loss: 0.5479809497964794
  batch 233 loss: 0.5473908111772824
  batch 234 loss: 0.5469260869117883
  batch 235 loss: 0.5463957913378452
  batch 236 loss: 0.5456410691899768
  batch 237 loss: 0.5452507133222331
  batch 238 loss: 0.5445800585155728
  batch 239 loss: 0.5438197226454523
  batch 240 loss: 0.5432065726568301
  batch 241 loss: 0.542854133112302
  batch 242 loss: 0.5420260944149711
  batch 243 loss: 0.541607847306954
  batch 244 loss: 0.5411422543349813
  batch 245 loss: 0.5405450768616735
  batch 246 loss: 0.5399532528912149
  batch 247 loss: 0.5394376458426718
  batch 248 loss: 0.5390269579666276
  batch 249 loss: 0.5383686493917641
  batch 250 loss: 0.5378054023981095
  batch 251 loss: 0.5373526166872199
  batch 252 loss: 0.5366873320132967
  batch 253 loss: 0.5359915940186722
  batch 254 loss: 0.5353114355032839
  batch 255 loss: 0.5347543622933182
  batch 256 loss: 0.5342002173420042
  batch 257 loss: 0.5335334229793994
  batch 258 loss: 0.5329991884009783
  batch 259 loss: 0.5325405788697791
  batch 260 loss: 0.5320054646867972
  batch 261 loss: 0.5315190174570485
  batch 262 loss: 0.5309247360884688
  batch 263 loss: 0.5304373628512988
  batch 264 loss: 0.5297924186469931
  batch 265 loss: 0.5292443285573204
  batch 266 loss: 0.5286875678632492
  batch 267 loss: 0.5283448112368138
  batch 268 loss: 0.5276714474407594
  batch 269 loss: 0.5271462818060666
  batch 270 loss: 0.5267633571668907
  batch 271 loss: 0.5263561607506883
  batch 272 loss: 0.5260186016778735
  batch 273 loss: 0.5255727914226798
  batch 274 loss: 0.525140008787169
  batch 275 loss: 0.5246354771744122
  batch 276 loss: 0.5241330025204713
  batch 277 loss: 0.5236526333898414
  batch 278 loss: 0.5231352339116789
  batch 279 loss: 0.522780199845632
  batch 280 loss: 0.5221667563276632
  batch 281 loss: 0.5215949635700823
  batch 282 loss: 0.5210789246550689
  batch 283 loss: 0.520555681874811
  batch 284 loss: 0.5203006254955077
  batch 285 loss: 0.5198469432822445
  batch 286 loss: 0.5194197867091719
  batch 287 loss: 0.5190109663516387
  batch 288 loss: 0.5184317234282693
  batch 289 loss: 0.5181328311923465
  batch 290 loss: 0.5174804690582999
  batch 291 loss: 0.5171080177387422
  batch 292 loss: 0.5167507395556529
  batch 293 loss: 0.5161760525695293
  batch 294 loss: 0.5156513928961591
  batch 295 loss: 0.5151859948190592
  batch 296 loss: 0.5148358029892316
  batch 297 loss: 0.5143769517892137
  batch 298 loss: 0.5139935557674241
  batch 299 loss: 0.513567105384176
  batch 300 loss: 0.5132157519459725
  batch 301 loss: 0.5127464518990628
  batch 302 loss: 0.5123943537473679
  batch 303 loss: 0.5120659163682768
  batch 304 loss: 0.5115600787103176
  batch 305 loss: 0.5109802317423898
  batch 306 loss: 0.510714594250411
  batch 307 loss: 0.5103348446979585
  batch 308 loss: 0.5101151181892916
  batch 309 loss: 0.5097368585253225
  batch 310 loss: 0.509378719329834
  batch 311 loss: 0.5091297743404793
  batch 312 loss: 0.508807051067169
  batch 313 loss: 0.5085052143270596
  batch 314 loss: 0.508069221476081
  batch 315 loss: 0.5076738434178488
  batch 316 loss: 0.5071851344236845
  batch 317 loss: 0.5068599485824536
  batch 318 loss: 0.5065599007051695
  batch 319 loss: 0.5060651309811584
  batch 320 loss: 0.5056597804650664
  batch 321 loss: 0.5052971119449888
  batch 322 loss: 0.5048888941173968
  batch 323 loss: 0.5045487531013902
  batch 324 loss: 0.5040593414946839
  batch 325 loss: 0.503636927146178
  batch 326 loss: 0.5033607428973438
  batch 327 loss: 0.5029803754175111
  batch 328 loss: 0.5025073501576738
  batch 329 loss: 0.502311245920448
  batch 330 loss: 0.5019858860608303
  batch 331 loss: 0.501583781937458
  batch 332 loss: 0.5011452652783279
  batch 333 loss: 0.5007720428544122
  batch 334 loss: 0.5003531976731238
  batch 335 loss: 0.4998444053664136
  batch 336 loss: 0.49935485892707393
  batch 337 loss: 0.49891834893990694
  batch 338 loss: 0.49863667962466474
  batch 339 loss: 0.4982918034964255
  batch 340 loss: 0.49793815893285415
  batch 341 loss: 0.497448678502589
  batch 342 loss: 0.49706220443834337
  batch 343 loss: 0.4967401825304282
  batch 344 loss: 0.49631618483122003
  batch 345 loss: 0.49607231530590334
  batch 346 loss: 0.49554755620529195
  batch 347 loss: 0.495260044288223
  batch 348 loss: 0.4949511041243871
  batch 349 loss: 0.49459823547598286
  batch 350 loss: 0.49431363182408466
  batch 351 loss: 0.49397067288387875
  batch 352 loss: 0.4937160915102471
  batch 353 loss: 0.4934117513743068
  batch 354 loss: 0.4932423131277332
  batch 355 loss: 0.49300384345188947
  batch 356 loss: 0.4926641910765948
  batch 357 loss: 0.49229667325313686
  batch 358 loss: 0.49208911171172587
  batch 359 loss: 0.4917593230610106
  batch 360 loss: 0.4914861355390814
  batch 361 loss: 0.4911143946515556
  batch 362 loss: 0.4907426834929714
  batch 363 loss: 0.4904692915188708
  batch 364 loss: 0.4901306891998092
  batch 365 loss: 0.4899128305585417
  batch 366 loss: 0.48955576960506336
  batch 367 loss: 0.48924871994948843
  batch 368 loss: 0.4888911902418603
  batch 369 loss: 0.48855787119891264
  batch 370 loss: 0.4883184342770963
  batch 371 loss: 0.48801752177852825
  batch 372 loss: 0.48767621110203446
  batch 373 loss: 0.4872960640821636
  batch 374 loss: 0.48687696783619133
  batch 375 loss: 0.48652338520685834
  batch 376 loss: 0.48640786190616325
  batch 377 loss: 0.4861189058984306
  batch 378 loss: 0.48573477290294786
  batch 379 loss: 0.48546213894532037
  batch 380 loss: 0.4852252901384705
  batch 381 loss: 0.4848929836055425
  batch 382 loss: 0.48453471259608943
  batch 383 loss: 0.4842403394123904
  batch 384 loss: 0.48391101649031043
  batch 385 loss: 0.4837286949157715
  batch 386 loss: 0.4833503507579546
  batch 387 loss: 0.48310119021462533
  batch 388 loss: 0.48290759578500825
  batch 389 loss: 0.4826372222146522
  batch 390 loss: 0.48230696534499146
  batch 391 loss: 0.48202260475024544
  batch 392 loss: 0.48178938540573024
  batch 393 loss: 0.4815880182286862
  batch 394 loss: 0.4813804696810427
  batch 395 loss: 0.48109920311577714
  batch 396 loss: 0.48078686736448845
  batch 397 loss: 0.4805586719392829
  batch 398 loss: 0.48027779603723303
  batch 399 loss: 0.4800482679877365
  batch 400 loss: 0.4798150395601988
  batch 401 loss: 0.4794949805201438
  batch 402 loss: 0.47929902545255215
  batch 403 loss: 0.479084178472571
  batch 404 loss: 0.4789855067830275
  batch 405 loss: 0.47877890945952617
  batch 406 loss: 0.4786410320715364
  batch 407 loss: 0.478359746039646
  batch 408 loss: 0.4782254483331652
  batch 409 loss: 0.4779553469498175
  batch 410 loss: 0.4777519400526838
  batch 411 loss: 0.47751752970572514
  batch 412 loss: 0.4772563824086513
  batch 413 loss: 0.47707205880929426
  batch 414 loss: 0.47676903088599587
  batch 415 loss: 0.4765216921467379
  batch 416 loss: 0.4763021244165989
  batch 417 loss: 0.47613245377437674
  batch 418 loss: 0.47589204499596044
  batch 419 loss: 0.4757060717284537
  batch 420 loss: 0.47544946464754284
  batch 421 loss: 0.47515394802331357
  batch 422 loss: 0.4750195793207223
  batch 423 loss: 0.47475166347573555
  batch 424 loss: 0.47446633123283116
  batch 425 loss: 0.47425929195740646
  batch 426 loss: 0.47396126998142457
  batch 427 loss: 0.4737116120738224
  batch 428 loss: 0.47340969372297004
  batch 429 loss: 0.47325023862865423
  batch 430 loss: 0.4729650541100391
  batch 431 loss: 0.4728877697909238
  batch 432 loss: 0.4727225698944595
  batch 433 loss: 0.47247274498741293
  batch 434 loss: 0.47233528475607595
  batch 435 loss: 0.47201172908147176
  batch 436 loss: 0.47180485739073624
  batch 437 loss: 0.4716632679207101
  batch 438 loss: 0.4715549296031804
  batch 439 loss: 0.47127259911328623
  batch 440 loss: 0.4710793966596777
  batch 441 loss: 0.47079430591492427
  batch 442 loss: 0.47053566467168645
  batch 443 loss: 0.47029108858539076
  batch 444 loss: 0.46997688052890535
  batch 445 loss: 0.4697994611236487
  batch 446 loss: 0.4695612534279246
  batch 447 loss: 0.46921956852511804
  batch 448 loss: 0.4690390235877463
  batch 449 loss: 0.4688459432337491
  batch 450 loss: 0.4685980909400516
  batch 451 loss: 0.46839708103044597
  batch 452 loss: 0.4682109769994179
  batch 453 loss: 0.4680975437427462
  batch 454 loss: 0.46796932081293957
  batch 455 loss: 0.4677354505428901
  batch 456 loss: 0.4675502055569699
  batch 457 loss: 0.4673627459116003
  batch 458 loss: 0.4671011066723078
  batch 459 loss: 0.4669183402409481
  batch 460 loss: 0.46678792154011517
  batch 461 loss: 0.4666014954997247
  batch 462 loss: 0.4664459260769221
  batch 463 loss: 0.4661597116462098
  batch 464 loss: 0.465955320938394
  batch 465 loss: 0.4657138881503895
  batch 466 loss: 0.46537328463511407
  batch 467 loss: 0.46534227086849334
  batch 468 loss: 0.46511545070470905
  batch 469 loss: 0.4650382026553408
  batch 470 loss: 0.46490379172436735
  batch 471 loss: 0.4647603128619508
  batch 472 loss: 0.4644617022220361
LOSS train 0.4644617022220361 valid 0.3136928081512451
LOSS train 0.4644617022220361 valid 0.30727535486221313
LOSS train 0.4644617022220361 valid 0.3108646869659424
LOSS train 0.4644617022220361 valid 0.30817441642284393
LOSS train 0.4644617022220361 valid 0.30468984246253966
LOSS train 0.4644617022220361 valid 0.30586892863114673
LOSS train 0.4644617022220361 valid 0.31616693309375216
LOSS train 0.4644617022220361 valid 0.3160070441663265
LOSS train 0.4644617022220361 valid 0.31405534678035313
LOSS train 0.4644617022220361 valid 0.31743870973587035
LOSS train 0.4644617022220361 valid 0.31643407995050604
LOSS train 0.4644617022220361 valid 0.317628192404906
LOSS train 0.4644617022220361 valid 0.3180485505324144
LOSS train 0.4644617022220361 valid 0.31872022577694487
LOSS train 0.4644617022220361 valid 0.3149493396282196
LOSS train 0.4644617022220361 valid 0.31577372550964355
LOSS train 0.4644617022220361 valid 0.3168143437189214
LOSS train 0.4644617022220361 valid 0.3179599146048228
LOSS train 0.4644617022220361 valid 0.31864376130856964
LOSS train 0.4644617022220361 valid 0.31881911009550096
LOSS train 0.4644617022220361 valid 0.31801780632564
LOSS train 0.4644617022220361 valid 0.31677431951869617
LOSS train 0.4644617022220361 valid 0.3162544369697571
LOSS train 0.4644617022220361 valid 0.31533079346021015
LOSS train 0.4644617022220361 valid 0.3142620623111725
LOSS train 0.4644617022220361 valid 0.31383795692370486
LOSS train 0.4644617022220361 valid 0.3137004187813512
LOSS train 0.4644617022220361 valid 0.31395801795380457
LOSS train 0.4644617022220361 valid 0.3142715158133671
LOSS train 0.4644617022220361 valid 0.3155805915594101
LOSS train 0.4644617022220361 valid 0.3161879172248225
LOSS train 0.4644617022220361 valid 0.3165288781747222
LOSS train 0.4644617022220361 valid 0.31739340948336053
LOSS train 0.4644617022220361 valid 0.31731126150664163
LOSS train 0.4644617022220361 valid 0.31869295409747533
LOSS train 0.4644617022220361 valid 0.3181709489888615
LOSS train 0.4644617022220361 valid 0.3178898506873363
LOSS train 0.4644617022220361 valid 0.3187953743495439
LOSS train 0.4644617022220361 valid 0.3181930566445375
LOSS train 0.4644617022220361 valid 0.3186179921030998
LOSS train 0.4644617022220361 valid 0.319131637491831
LOSS train 0.4644617022220361 valid 0.3195301265943618
LOSS train 0.4644617022220361 valid 0.3195601057174594
LOSS train 0.4644617022220361 valid 0.32005426626313815
LOSS train 0.4644617022220361 valid 0.3191551261478
LOSS train 0.4644617022220361 valid 0.32011586557263916
LOSS train 0.4644617022220361 valid 0.3201155738627657
LOSS train 0.4644617022220361 valid 0.32007372689743835
LOSS train 0.4644617022220361 valid 0.32058942135499446
LOSS train 0.4644617022220361 valid 0.31983571529388427
LOSS train 0.4644617022220361 valid 0.3203439753429562
LOSS train 0.4644617022220361 valid 0.32038678286167294
LOSS train 0.4644617022220361 valid 0.3205249967440119
LOSS train 0.4644617022220361 valid 0.3205307523409526
LOSS train 0.4644617022220361 valid 0.3203043520450592
LOSS train 0.4644617022220361 valid 0.3202009674693857
LOSS train 0.4644617022220361 valid 0.3196615404204318
LOSS train 0.4644617022220361 valid 0.31952195640268
LOSS train 0.4644617022220361 valid 0.31985992243734457
LOSS train 0.4644617022220361 valid 0.3195334608356158
LOSS train 0.4644617022220361 valid 0.31934648507931196
LOSS train 0.4644617022220361 valid 0.32006141495320106
LOSS train 0.4644617022220361 valid 0.32011142325779746
LOSS train 0.4644617022220361 valid 0.32095150370150805
LOSS train 0.4644617022220361 valid 0.32104876499909624
LOSS train 0.4644617022220361 valid 0.32123887493754877
LOSS train 0.4644617022220361 valid 0.3205547902121473
LOSS train 0.4644617022220361 valid 0.32048586054759864
LOSS train 0.4644617022220361 valid 0.31975325726080633
LOSS train 0.4644617022220361 valid 0.3195866904088429
LOSS train 0.4644617022220361 valid 0.319042208329053
LOSS train 0.4644617022220361 valid 0.31922487252288395
LOSS train 0.4644617022220361 valid 0.31940403335714995
LOSS train 0.4644617022220361 valid 0.3191841200396821
LOSS train 0.4644617022220361 valid 0.31911564548810323
LOSS train 0.4644617022220361 valid 0.3194519489219314
LOSS train 0.4644617022220361 valid 0.3195591178033259
LOSS train 0.4644617022220361 valid 0.3196042680587524
LOSS train 0.4644617022220361 valid 0.31957338429704496
LOSS train 0.4644617022220361 valid 0.3189988501369953
LOSS train 0.4644617022220361 valid 0.31801830176953916
LOSS train 0.4644617022220361 valid 0.31804047852027706
LOSS train 0.4644617022220361 valid 0.3179396053394639
LOSS train 0.4644617022220361 valid 0.31791584335622336
LOSS train 0.4644617022220361 valid 0.31752594358780806
LOSS train 0.4644617022220361 valid 0.31664802497902583
LOSS train 0.4644617022220361 valid 0.31662317099927484
LOSS train 0.4644617022220361 valid 0.3164500653405081
LOSS train 0.4644617022220361 valid 0.31676063929380993
LOSS train 0.4644617022220361 valid 0.3170437291264534
LOSS train 0.4644617022220361 valid 0.31707946006413346
LOSS train 0.4644617022220361 valid 0.31711516804669215
LOSS train 0.4644617022220361 valid 0.3170423249724091
LOSS train 0.4644617022220361 valid 0.31708146964615963
LOSS train 0.4644617022220361 valid 0.3168115476244374
LOSS train 0.4644617022220361 valid 0.31680126255378127
LOSS train 0.4644617022220361 valid 0.31703325537676663
LOSS train 0.4644617022220361 valid 0.3169250562787056
LOSS train 0.4644617022220361 valid 0.31688781323457005
LOSS train 0.4644617022220361 valid 0.31728264018893243
LOSS train 0.4644617022220361 valid 0.31740338038099875
LOSS train 0.4644617022220361 valid 0.3174694824744673
LOSS train 0.4644617022220361 valid 0.317401708474437
LOSS train 0.4644617022220361 valid 0.3172043844197805
LOSS train 0.4644617022220361 valid 0.3175368534667151
LOSS train 0.4644617022220361 valid 0.31777121084478666
LOSS train 0.4644617022220361 valid 0.3174493180535664
LOSS train 0.4644617022220361 valid 0.31776989596309485
LOSS train 0.4644617022220361 valid 0.3181713022496722
LOSS train 0.4644617022220361 valid 0.31852248351682316
LOSS train 0.4644617022220361 valid 0.3182479694888398
LOSS train 0.4644617022220361 valid 0.3182168133290751
LOSS train 0.4644617022220361 valid 0.31833209703981347
LOSS train 0.4644617022220361 valid 0.31834612459990014
LOSS train 0.4644617022220361 valid 0.3185255388850751
LOSS train 0.4644617022220361 valid 0.31883111763103256
LOSS train 0.4644617022220361 valid 0.3191179805077039
LOSS train 0.4644617022220361 valid 0.3190211715587115
LOSS train 0.4644617022220361 valid 0.3188771974890172
LOSS train 0.4644617022220361 valid 0.31861654358605546
LOSS train 0.4644617022220361 valid 0.31844796831450184
LOSS train 0.4644617022220361 valid 0.3182831979433044
LOSS train 0.4644617022220361 valid 0.31834058245507685
LOSS train 0.4644617022220361 valid 0.31854171341946047
LOSS train 0.4644617022220361 valid 0.31858842074871063
LOSS train 0.4644617022220361 valid 0.3186829900221219
LOSS train 0.4644617022220361 valid 0.31857724487781525
LOSS train 0.4644617022220361 valid 0.31878557975869626
LOSS train 0.4644617022220361 valid 0.31896861313387403
LOSS train 0.4644617022220361 valid 0.3190375469051875
LOSS train 0.4644617022220361 valid 0.3190067388856684
LOSS train 0.4644617022220361 valid 0.31873169101097365
LOSS train 0.4644617022220361 valid 0.3185506907844902
LOSS train 0.4644617022220361 valid 0.31861795496139955
LOSS train 0.4644617022220361 valid 0.3187487214803696
LOSS train 0.4644617022220361 valid 0.3186241591020542
LOSS train 0.4644617022220361 valid 0.3185274312748526
LOSS train 0.4644617022220361 valid 0.31851752933816635
LOSS train 0.4644617022220361 valid 0.31837543727253836
LOSS train 0.4644617022220361 valid 0.31858949714473317
LOSS train 0.4644617022220361 valid 0.3188023957166266
LOSS train 0.4644617022220361 valid 0.31908233873021435
LOSS train 0.4644617022220361 valid 0.31908356320191095
LOSS train 0.4644617022220361 valid 0.3190415171492431
LOSS train 0.4644617022220361 valid 0.3187903059967633
LOSS train 0.4644617022220361 valid 0.31891887512517303
LOSS train 0.4644617022220361 valid 0.31864532818194147
LOSS train 0.4644617022220361 valid 0.3192588760442025
LOSS train 0.4644617022220361 valid 0.3193696522272673
LOSS train 0.4644617022220361 valid 0.3193316059311231
LOSS train 0.4644617022220361 valid 0.31934296088108166
LOSS train 0.4644617022220361 valid 0.31919600098933043
LOSS train 0.4644617022220361 valid 0.31939147665999296
LOSS train 0.4644617022220361 valid 0.3194200592962178
LOSS train 0.4644617022220361 valid 0.3194142209906732
LOSS train 0.4644617022220361 valid 0.3196599589517483
LOSS train 0.4644617022220361 valid 0.3195919347986294
LOSS train 0.4644617022220361 valid 0.3197197701923455
LOSS train 0.4644617022220361 valid 0.3196287701527278
LOSS train 0.4644617022220361 valid 0.31952618798241017
LOSS train 0.4644617022220361 valid 0.3195577492439969
LOSS train 0.4644617022220361 valid 0.31953950805796516
LOSS train 0.4644617022220361 valid 0.31942163096980813
LOSS train 0.4644617022220361 valid 0.3193370829813364
LOSS train 0.4644617022220361 valid 0.31926411110343356
LOSS train 0.4644617022220361 valid 0.3194477681115449
LOSS train 0.4644617022220361 valid 0.3195074992087073
LOSS train 0.4644617022220361 valid 0.3193482736214286
LOSS train 0.4644617022220361 valid 0.3193339406562275
LOSS train 0.4644617022220361 valid 0.31956812783199195
LOSS train 0.4644617022220361 valid 0.3195241735866892
LOSS train 0.4644617022220361 valid 0.3193583905177061
LOSS train 0.4644617022220361 valid 0.31952676788575385
LOSS train 0.4644617022220361 valid 0.31955310316949054
LOSS train 0.4644617022220361 valid 0.3194730659042086
LOSS train 0.4644617022220361 valid 0.3193798416548155
LOSS train 0.4644617022220361 valid 0.31927171189569487
LOSS train 0.4644617022220361 valid 0.3194129906511039
LOSS train 0.4644617022220361 valid 0.31945843328643775
LOSS train 0.4644617022220361 valid 0.3194690302842193
LOSS train 0.4644617022220361 valid 0.3195762234992085
LOSS train 0.4644617022220361 valid 0.31970471643157056
LOSS train 0.4644617022220361 valid 0.31957748068160696
LOSS train 0.4644617022220361 valid 0.31968675270352676
LOSS train 0.4644617022220361 valid 0.31954657524018676
LOSS train 0.4644617022220361 valid 0.3195100113429049
LOSS train 0.4644617022220361 valid 0.31951001406034685
LOSS train 0.4644617022220361 valid 0.31959367408714395
LOSS train 0.4644617022220361 valid 0.31957443058490753
LOSS train 0.4644617022220361 valid 0.3196573951526692
LOSS train 0.4644617022220361 valid 0.31962878877267786
LOSS train 0.4644617022220361 valid 0.31957632621439797
LOSS train 0.4644617022220361 valid 0.31955902371073014
LOSS train 0.4644617022220361 valid 0.31950436570902463
LOSS train 0.4644617022220361 valid 0.3193538281397942
LOSS train 0.4644617022220361 valid 0.31951297599138045
LOSS train 0.4644617022220361 valid 0.31959317276623045
LOSS train 0.4644617022220361 valid 0.31942929319962104
LOSS train 0.4644617022220361 valid 0.31936865483396615
LOSS train 0.4644617022220361 valid 0.3192145905643702
LOSS train 0.4644617022220361 valid 0.319056774327411
LOSS train 0.4644617022220361 valid 0.31907118790515576
LOSS train 0.4644617022220361 valid 0.3189364779612114
LOSS train 0.4644617022220361 valid 0.31881901453815253
LOSS train 0.4644617022220361 valid 0.3186374406262142
LOSS train 0.4644617022220361 valid 0.31860297043057323
LOSS train 0.4644617022220361 valid 0.31856099656526593
LOSS train 0.4644617022220361 valid 0.3184748569216866
LOSS train 0.4644617022220361 valid 0.31827778486829056
LOSS train 0.4644617022220361 valid 0.31830378550858723
LOSS train 0.4644617022220361 valid 0.31837819813269574
LOSS train 0.4644617022220361 valid 0.3182473415333145
LOSS train 0.4644617022220361 valid 0.31840007986541086
LOSS train 0.4644617022220361 valid 0.31837692654857014
LOSS train 0.4644617022220361 valid 0.31816823503305747
LOSS train 0.4644617022220361 valid 0.31815303593046135
LOSS train 0.4644617022220361 valid 0.3182062455174011
LOSS train 0.4644617022220361 valid 0.31818360999363277
LOSS train 0.4644617022220361 valid 0.3182394321514591
LOSS train 0.4644617022220361 valid 0.31831034293228927
LOSS train 0.4644617022220361 valid 0.31832061330387496
LOSS train 0.4644617022220361 valid 0.318322126459968
LOSS train 0.4644617022220361 valid 0.31856736146548403
LOSS train 0.4644617022220361 valid 0.3186654172438596
LOSS train 0.4644617022220361 valid 0.3188324017657174
LOSS train 0.4644617022220361 valid 0.3189318170194077
LOSS train 0.4644617022220361 valid 0.3192015439271927
LOSS train 0.4644617022220361 valid 0.31920851433747693
LOSS train 0.4644617022220361 valid 0.3192584633697068
LOSS train 0.4644617022220361 valid 0.31929102663112724
LOSS train 0.4644617022220361 valid 0.3192649084381211
LOSS train 0.4644617022220361 valid 0.31934230729680635
LOSS train 0.4644617022220361 valid 0.31925500878192836
LOSS train 0.4644617022220361 valid 0.3192192804482248
LOSS train 0.4644617022220361 valid 0.3193259064821487
LOSS train 0.4644617022220361 valid 0.3193013782344632
LOSS train 0.4644617022220361 valid 0.3193773751143162
LOSS train 0.4644617022220361 valid 0.3192933745870069
LOSS train 0.4644617022220361 valid 0.31914258371087795
LOSS train 0.4644617022220361 valid 0.31912809194376074
LOSS train 0.4644617022220361 valid 0.3191672114044799
LOSS train 0.4644617022220361 valid 0.31892424606340974
LOSS train 0.4644617022220361 valid 0.3189932359957401
LOSS train 0.4644617022220361 valid 0.31905219404668106
LOSS train 0.4644617022220361 valid 0.3192266332251685
LOSS train 0.4644617022220361 valid 0.31917488169137054
LOSS train 0.4644617022220361 valid 0.319180369558122
LOSS train 0.4644617022220361 valid 0.3192033398175432
LOSS train 0.4644617022220361 valid 0.3191784818368743
LOSS train 0.4644617022220361 valid 0.31926885145902634
LOSS train 0.4644617022220361 valid 0.3192179964714316
LOSS train 0.4644617022220361 valid 0.3192728945305423
LOSS train 0.4644617022220361 valid 0.3191162774803139
LOSS train 0.4644617022220361 valid 0.3191118408727834
LOSS train 0.4644617022220361 valid 0.3191595429883284
LOSS train 0.4644617022220361 valid 0.31919893034501
LOSS train 0.4644617022220361 valid 0.31907308513320376
LOSS train 0.4644617022220361 valid 0.31920090750899427
LOSS train 0.4644617022220361 valid 0.31915776120879935
LOSS train 0.4644617022220361 valid 0.31920725239010955
LOSS train 0.4644617022220361 valid 0.319239540289645
LOSS train 0.4644617022220361 valid 0.3191614933027566
LOSS train 0.4644617022220361 valid 0.31916784679255106
LOSS train 0.4644617022220361 valid 0.31913886620014004
LOSS train 0.4644617022220361 valid 0.31910867561709205
LOSS train 0.4644617022220361 valid 0.31903748970506784
LOSS train 0.4644617022220361 valid 0.3191388782378886
LOSS train 0.4644617022220361 valid 0.3192458162223225
LOSS train 0.4644617022220361 valid 0.3193594501474976
LOSS train 0.4644617022220361 valid 0.31942179694219874
LOSS train 0.4644617022220361 valid 0.3195217452475945
LOSS train 0.4644617022220361 valid 0.3196983812376857
LOSS train 0.4644617022220361 valid 0.31977004822575567
LOSS train 0.4644617022220361 valid 0.31975048807633183
LOSS train 0.4644617022220361 valid 0.3197400953011079
LOSS train 0.4644617022220361 valid 0.3196813518694345
LOSS train 0.4644617022220361 valid 0.31952534271706745
LOSS train 0.4644617022220361 valid 0.3193674538311341
LOSS train 0.4644617022220361 valid 0.3193794997896345
LOSS train 0.4644617022220361 valid 0.3192758323358638
LOSS train 0.4644617022220361 valid 0.3191928301947821
LOSS train 0.4644617022220361 valid 0.3190788169596212
LOSS train 0.4644617022220361 valid 0.3190542910735093
LOSS train 0.4644617022220361 valid 0.31914317875470916
LOSS train 0.4644617022220361 valid 0.31921793687761874
LOSS train 0.4644617022220361 valid 0.3190857795672817
LOSS train 0.4644617022220361 valid 0.31900083099715804
LOSS train 0.4644617022220361 valid 0.3190312021825876
LOSS train 0.4644617022220361 valid 0.3190795811711711
LOSS train 0.4644617022220361 valid 0.3191352491234911
LOSS train 0.4644617022220361 valid 0.31908201746309744
LOSS train 0.4644617022220361 valid 0.3190709779625886
LOSS train 0.4644617022220361 valid 0.31908031174143836
LOSS train 0.4644617022220361 valid 0.3191036387872534
LOSS train 0.4644617022220361 valid 0.3192025828664586
LOSS train 0.4644617022220361 valid 0.31913709001162566
LOSS train 0.4644617022220361 valid 0.3191815342264946
LOSS train 0.4644617022220361 valid 0.31915513446067
LOSS train 0.4644617022220361 valid 0.3191634946343891
LOSS train 0.4644617022220361 valid 0.3191531701385975
LOSS train 0.4644617022220361 valid 0.3192601296493777
LOSS train 0.4644617022220361 valid 0.31927519064668
LOSS train 0.4644617022220361 valid 0.319415342443847
LOSS train 0.4644617022220361 valid 0.31945002809362977
LOSS train 0.4644617022220361 valid 0.3193437187886629
LOSS train 0.4644617022220361 valid 0.31935292438549157
LOSS train 0.4644617022220361 valid 0.31935544758549733
LOSS train 0.4644617022220361 valid 0.3192775159009865
LOSS train 0.4644617022220361 valid 0.3193590390740089
LOSS train 0.4644617022220361 valid 0.31931022641158874
LOSS train 0.4644617022220361 valid 0.31925091823005985
LOSS train 0.4644617022220361 valid 0.31929134353040123
LOSS train 0.4644617022220361 valid 0.3192855005923171
LOSS train 0.4644617022220361 valid 0.31942754267317475
LOSS train 0.4644617022220361 valid 0.31942577385713183
LOSS train 0.4644617022220361 valid 0.31936111277605916
LOSS train 0.4644617022220361 valid 0.3194293696139513
LOSS train 0.4644617022220361 valid 0.3194879985549165
LOSS train 0.4644617022220361 valid 0.3195524456265578
LOSS train 0.4644617022220361 valid 0.319495563255623
LOSS train 0.4644617022220361 valid 0.3195705007634059
LOSS train 0.4644617022220361 valid 0.3195225776445051
LOSS train 0.4644617022220361 valid 0.31951288685156465
LOSS train 0.4644617022220361 valid 0.3194792963748361
LOSS train 0.4644617022220361 valid 0.3195128697156906
LOSS train 0.4644617022220361 valid 0.3196358525771305
LOSS train 0.4644617022220361 valid 0.31968755300074175
LOSS train 0.4644617022220361 valid 0.31970318992872065
LOSS train 0.4644617022220361 valid 0.31981628458369465
LOSS train 0.4644617022220361 valid 0.31978135682416686
LOSS train 0.4644617022220361 valid 0.3196673863694027
LOSS train 0.4644617022220361 valid 0.31955916591617
LOSS train 0.4644617022220361 valid 0.319553970440372
LOSS train 0.4644617022220361 valid 0.31975255473823605
LOSS train 0.4644617022220361 valid 0.31976848854057827
LOSS train 0.4644617022220361 valid 0.31976671015755054
LOSS train 0.4644617022220361 valid 0.3197586129168021
LOSS train 0.4644617022220361 valid 0.3196883827183374
LOSS train 0.4644617022220361 valid 0.31972509670925703
LOSS train 0.4644617022220361 valid 0.31968120516223064
LOSS train 0.4644617022220361 valid 0.31963110332789657
LOSS train 0.4644617022220361 valid 0.3196451166829868
LOSS train 0.4644617022220361 valid 0.3196718151906489
LOSS train 0.4644617022220361 valid 0.3198533256993044
LOSS train 0.4644617022220361 valid 0.31990107976008153
LOSS train 0.4644617022220361 valid 0.31977736704439097
LOSS train 0.4644617022220361 valid 0.31966288130118453
LOSS train 0.4644617022220361 valid 0.319618028513659
LOSS train 0.4644617022220361 valid 0.3196668016449428
LOSS train 0.4644617022220361 valid 0.31960241730724065
LOSS train 0.4644617022220361 valid 0.319523226289328
LOSS train 0.4644617022220361 valid 0.3194917573762888
LOSS train 0.4644617022220361 valid 0.31948638211397545
LOSS train 0.4644617022220361 valid 0.3194722696970412
LOSS train 0.4644617022220361 valid 0.3195941791148253
LOSS train 0.4644617022220361 valid 0.31966528610399597
LOSS train 0.4644617022220361 valid 0.31957071823399275
LOSS train 0.4644617022220361 valid 0.3195206911287494
LOSS train 0.4644617022220361 valid 0.3195904201534797
LOSS train 0.4644617022220361 valid 0.31957512717280123
LOSS train 0.4644617022220361 valid 0.31949074624152723
LOSS train 0.4644617022220361 valid 0.31956237245824454
LOSS train 0.4644617022220361 valid 0.3194362019129693
LOSS train 0.4644617022220361 valid 0.319431777797885
LOSS train 0.4644617022220361 valid 0.3194233526514001
LOSS train 0.4644617022220361 valid 0.3193919332545312
LOSS train 0.4644617022220361 valid 0.31928402506200754
LOSS train 0.4644617022220361 valid 0.3192010104413266
LOSS train 0.4644617022220361 valid 0.31934262101404715
EPOCH 2:
  batch 1 loss: 0.36598217487335205
  batch 2 loss: 0.3648741692304611
  batch 3 loss: 0.3707380493481954
  batch 4 loss: 0.371872641146183
  batch 5 loss: 0.37781845331192015
  batch 6 loss: 0.38054599861303967
  batch 7 loss: 0.38004968421799795
  batch 8 loss: 0.37966328486800194
  batch 9 loss: 0.3826699952284495
  batch 10 loss: 0.38411568105220795
  batch 11 loss: 0.3828873255036094
  batch 12 loss: 0.37965819984674454
  batch 13 loss: 0.37919710920407224
  batch 14 loss: 0.38022897073200773
  batch 15 loss: 0.3838807284832001
  batch 16 loss: 0.3829274047166109
  batch 17 loss: 0.380029196248335
  batch 18 loss: 0.3812543600797653
  batch 19 loss: 0.380495171797903
  batch 20 loss: 0.379141603410244
  batch 21 loss: 0.37878405054410297
  batch 22 loss: 0.37859334593469446
  batch 23 loss: 0.3777129157729771
  batch 24 loss: 0.3755867009361585
  batch 25 loss: 0.37687556028366087
  batch 26 loss: 0.37613966831794154
  batch 27 loss: 0.37628371406484534
  batch 28 loss: 0.37442112288304735
  batch 29 loss: 0.374301847712747
  batch 30 loss: 0.37340327501297
  batch 31 loss: 0.37374011355061687
  batch 32 loss: 0.37322927359491587
  batch 33 loss: 0.37315342552734143
  batch 34 loss: 0.3731949285549276
  batch 35 loss: 0.3735210869993482
  batch 36 loss: 0.3732662151257197
  batch 37 loss: 0.3749315754787342
  batch 38 loss: 0.37522231669802414
  batch 39 loss: 0.3749073889010992
  batch 40 loss: 0.37456331178545954
  batch 41 loss: 0.37453584045898625
  batch 42 loss: 0.3748435981216885
  batch 43 loss: 0.37485613102136656
  batch 44 loss: 0.37467285652052273
  batch 45 loss: 0.3744105378786723
  batch 46 loss: 0.3742340556953264
  batch 47 loss: 0.3744117427379527
  batch 48 loss: 0.37321590321759385
  batch 49 loss: 0.3724864173908623
  batch 50 loss: 0.37175642848014834
  batch 51 loss: 0.3714764895392399
  batch 52 loss: 0.3721683925160995
  batch 53 loss: 0.37189309495799944
  batch 54 loss: 0.3722967229507588
  batch 55 loss: 0.37214181314815176
  batch 56 loss: 0.37220502751214163
  batch 57 loss: 0.37154661772543923
  batch 58 loss: 0.3720603009750103
  batch 59 loss: 0.37180418685331185
  batch 60 loss: 0.3710599442323049
  batch 61 loss: 0.3715965567065067
  batch 62 loss: 0.3716854828019296
  batch 63 loss: 0.371911874366185
  batch 64 loss: 0.37217086693271995
  batch 65 loss: 0.37148970732322106
  batch 66 loss: 0.3712690535819892
  batch 67 loss: 0.3720632724797548
  batch 68 loss: 0.3723790930474506
  batch 69 loss: 0.372277507747429
  batch 70 loss: 0.37247704352651323
  batch 71 loss: 0.3721835327820039
  batch 72 loss: 0.3722377055221134
  batch 73 loss: 0.3722627420131474
  batch 74 loss: 0.3725881181858681
  batch 75 loss: 0.37232627312342326
  batch 76 loss: 0.3724271286475031
  batch 77 loss: 0.37227291913775656
  batch 78 loss: 0.37211735661213213
  batch 79 loss: 0.3723313446286358
  batch 80 loss: 0.3720291115343571
  batch 81 loss: 0.37199248907006816
  batch 82 loss: 0.3723918592057577
  batch 83 loss: 0.3721165581639991
  batch 84 loss: 0.3716874452573912
  batch 85 loss: 0.3710456742959864
  batch 86 loss: 0.3713948023180629
  batch 87 loss: 0.37124769543779307
  batch 88 loss: 0.37069436873901973
  batch 89 loss: 0.37039644329735405
  batch 90 loss: 0.37039544814162784
  batch 91 loss: 0.37071047313920746
  batch 92 loss: 0.37058380051799444
  batch 93 loss: 0.37060502267652945
  batch 94 loss: 0.3711088280728523
  batch 95 loss: 0.37075587636546087
  batch 96 loss: 0.37080602316806716
  batch 97 loss: 0.3710118541397999
  batch 98 loss: 0.3710855455422888
  batch 99 loss: 0.3710173431671027
  batch 100 loss: 0.37079417556524275
  batch 101 loss: 0.3705427386973164
  batch 102 loss: 0.37061065873678994
  batch 103 loss: 0.3710328479415005
  batch 104 loss: 0.3708464741133727
  batch 105 loss: 0.37029572242782227
  batch 106 loss: 0.37063666194115047
  batch 107 loss: 0.3701812198785978
  batch 108 loss: 0.3700433125098546
  batch 109 loss: 0.36988811728057513
  batch 110 loss: 0.37015961706638334
  batch 111 loss: 0.36995667031219415
  batch 112 loss: 0.3696853079433952
  batch 113 loss: 0.370060578647968
  batch 114 loss: 0.3703084036446454
  batch 115 loss: 0.3705809606158215
  batch 116 loss: 0.37062603569236297
  batch 117 loss: 0.3707903840093531
  batch 118 loss: 0.3707343069678646
  batch 119 loss: 0.3709884734213853
  batch 120 loss: 0.370871010919412
  batch 121 loss: 0.37078647278557136
  batch 122 loss: 0.370632925483047
  batch 123 loss: 0.3706808187128083
  batch 124 loss: 0.3709371702805642
  batch 125 loss: 0.3708724422454834
  batch 126 loss: 0.3707241825168095
  batch 127 loss: 0.371153197419925
  batch 128 loss: 0.37095639668405056
  batch 129 loss: 0.3709736491820609
  batch 130 loss: 0.370696921302722
  batch 131 loss: 0.37065162503992327
  batch 132 loss: 0.3705572219509067
  batch 133 loss: 0.37079663877200364
  batch 134 loss: 0.37072438891254256
  batch 135 loss: 0.37085076018616003
  batch 136 loss: 0.37073377541759434
  batch 137 loss: 0.37073303182629774
  batch 138 loss: 0.3706873765458231
  batch 139 loss: 0.3711898352602403
  batch 140 loss: 0.37104910973991667
  batch 141 loss: 0.37118452005352537
  batch 142 loss: 0.3712290943088666
  batch 143 loss: 0.3709633319111137
  batch 144 loss: 0.37087404872808194
  batch 145 loss: 0.37082634527107766
  batch 146 loss: 0.370760807231681
  batch 147 loss: 0.3709261060571995
  batch 148 loss: 0.37086961478800384
  batch 149 loss: 0.37085147332025054
  batch 150 loss: 0.3706922592719396
  batch 151 loss: 0.370782297968075
  batch 152 loss: 0.3708857805713227
  batch 153 loss: 0.37085166612481757
  batch 154 loss: 0.3709127041813615
  batch 155 loss: 0.3709208075077303
  batch 156 loss: 0.3709464959609203
  batch 157 loss: 0.3711678605930061
  batch 158 loss: 0.3712699986711333
  batch 159 loss: 0.3714447893061728
  batch 160 loss: 0.3713848736137152
  batch 161 loss: 0.3713243040979279
  batch 162 loss: 0.37122181867375786
  batch 163 loss: 0.37092372738510554
  batch 164 loss: 0.3709443209738266
  batch 165 loss: 0.3707192599773407
  batch 166 loss: 0.37064229293041917
  batch 167 loss: 0.3704967295338294
  batch 168 loss: 0.37050694156260716
  batch 169 loss: 0.37020611392675773
  batch 170 loss: 0.37017082922598893
  batch 171 loss: 0.3699681825107998
  batch 172 loss: 0.3698472136328387
  batch 173 loss: 0.3697542270828534
  batch 174 loss: 0.3697670389866007
  batch 175 loss: 0.36990061214991976
  batch 176 loss: 0.3695011321793903
  batch 177 loss: 0.3694191524537943
  batch 178 loss: 0.3694706905758783
  batch 179 loss: 0.36942867443548233
  batch 180 loss: 0.36917293469111123
  batch 181 loss: 0.369013961836778
  batch 182 loss: 0.36881521671682926
  batch 183 loss: 0.36862264190866645
  batch 184 loss: 0.3683437884825727
  batch 185 loss: 0.36822365957337455
  batch 186 loss: 0.36840984930274306
  batch 187 loss: 0.36824911020018836
  batch 188 loss: 0.36798007596046367
  batch 189 loss: 0.36779777445490397
  batch 190 loss: 0.3677308909202877
  batch 191 loss: 0.36759964006109386
  batch 192 loss: 0.3674291848825912
  batch 193 loss: 0.3673928502001293
  batch 194 loss: 0.36754374482582525
  batch 195 loss: 0.3676100912766579
  batch 196 loss: 0.3673375736998052
  batch 197 loss: 0.36715075600570835
  batch 198 loss: 0.36726549613957454
  batch 199 loss: 0.3672154301674522
  batch 200 loss: 0.3672293083369732
  batch 201 loss: 0.3671331593942879
  batch 202 loss: 0.36703660405508365
  batch 203 loss: 0.3669957282214329
  batch 204 loss: 0.3667682197456266
  batch 205 loss: 0.36679972584654646
  batch 206 loss: 0.3667580962759777
  batch 207 loss: 0.3666955772517384
  batch 208 loss: 0.366507600276516
  batch 209 loss: 0.3662941283301303
  batch 210 loss: 0.36623490211509524
  batch 211 loss: 0.36622038271754837
  batch 212 loss: 0.36612304018915826
  batch 213 loss: 0.3661101845228616
  batch 214 loss: 0.3659764972524108
  batch 215 loss: 0.3657006105711294
  batch 216 loss: 0.36560431743661564
  batch 217 loss: 0.3656413303816923
  batch 218 loss: 0.36560099346375247
  batch 219 loss: 0.3655872817181017
  batch 220 loss: 0.36550948484377427
  batch 221 loss: 0.3656391703704903
  batch 222 loss: 0.3656918819960173
  batch 223 loss: 0.3656961487547699
  batch 224 loss: 0.3657486913725734
  batch 225 loss: 0.36551718327734206
  batch 226 loss: 0.3656251325016528
  batch 227 loss: 0.36542779233487166
  batch 228 loss: 0.36537021594612223
  batch 229 loss: 0.36520196412848593
  batch 230 loss: 0.3653010079394216
  batch 231 loss: 0.3653106831368946
  batch 232 loss: 0.36512348736668454
  batch 233 loss: 0.3650668121458635
  batch 234 loss: 0.3650779983936212
  batch 235 loss: 0.3651171022273125
  batch 236 loss: 0.3649932726965112
  batch 237 loss: 0.3651329804573381
  batch 238 loss: 0.36497835564513165
  batch 239 loss: 0.3649172176875829
  batch 240 loss: 0.364965217312177
  batch 241 loss: 0.3650222815171317
  batch 242 loss: 0.36471023047265927
  batch 243 loss: 0.36479900035347956
  batch 244 loss: 0.36485950160221975
  batch 245 loss: 0.36480041158442594
  batch 246 loss: 0.3647333067364809
  batch 247 loss: 0.3648109183861659
  batch 248 loss: 0.36481594738941037
  batch 249 loss: 0.36472505462696275
  batch 250 loss: 0.3646695786714554
  batch 251 loss: 0.36470417043127384
  batch 252 loss: 0.36456119647574803
  batch 253 loss: 0.36434721993834607
  batch 254 loss: 0.3642249351411354
  batch 255 loss: 0.3642212512446385
  batch 256 loss: 0.36421645910013467
  batch 257 loss: 0.3640604072507717
  batch 258 loss: 0.3639869040759035
  batch 259 loss: 0.3639279583246091
  batch 260 loss: 0.3638586954428599
  batch 261 loss: 0.36388712550488467
  batch 262 loss: 0.3637544293212527
  batch 263 loss: 0.3637190285517689
  batch 264 loss: 0.3635089594983693
  batch 265 loss: 0.3634274532210152
  batch 266 loss: 0.36328643369943575
  batch 267 loss: 0.36339628897356185
  batch 268 loss: 0.3632488258516611
  batch 269 loss: 0.3631623314215791
  batch 270 loss: 0.3633010330023589
  batch 271 loss: 0.3634229449768348
  batch 272 loss: 0.36350478352430987
  batch 273 loss: 0.3634746219867315
  batch 274 loss: 0.3634984321402807
  batch 275 loss: 0.3635001988844438
  batch 276 loss: 0.36345499386821967
  batch 277 loss: 0.3634071457687268
  batch 278 loss: 0.36332064063000163
  batch 279 loss: 0.3633638518899145
  batch 280 loss: 0.3632356295628207
  batch 281 loss: 0.3631278856581216
  batch 282 loss: 0.36305674730885956
  batch 283 loss: 0.3629292702506372
  batch 284 loss: 0.36303510810707657
  batch 285 loss: 0.36294629584278976
  batch 286 loss: 0.362905753242386
  batch 287 loss: 0.36295999384092537
  batch 288 loss: 0.3628896727330155
  batch 289 loss: 0.3629861377102281
  batch 290 loss: 0.36276423961951815
  batch 291 loss: 0.3627275009335521
  batch 292 loss: 0.3628145910493315
  batch 293 loss: 0.3626420513151449
  batch 294 loss: 0.3625113363979625
  batch 295 loss: 0.36244455656762853
  batch 296 loss: 0.36244465904058637
  batch 297 loss: 0.36231716774930856
  batch 298 loss: 0.3623036225170097
  batch 299 loss: 0.36227068364819554
  batch 300 loss: 0.36226639439662295
  batch 301 loss: 0.3621633139955641
  batch 302 loss: 0.3621755228532071
  batch 303 loss: 0.3621282706362973
  batch 304 loss: 0.3620492501282378
  batch 305 loss: 0.36186360443224674
  batch 306 loss: 0.3619333617632685
  batch 307 loss: 0.36189035619897253
  batch 308 loss: 0.3619730211310572
  batch 309 loss: 0.3618431968982166
  batch 310 loss: 0.36177767447886927
  batch 311 loss: 0.3619122523395195
  batch 312 loss: 0.36198868898627085
  batch 313 loss: 0.36204123278014577
  batch 314 loss: 0.36198134644395985
  batch 315 loss: 0.36189755201339724
  batch 316 loss: 0.36182990575893015
  batch 317 loss: 0.3618778174607912
  batch 318 loss: 0.36187980066305436
  batch 319 loss: 0.3617661768179328
  batch 320 loss: 0.3616920472122729
  batch 321 loss: 0.3616759209803703
  batch 322 loss: 0.36159205325641985
  batch 323 loss: 0.36157734364547967
  batch 324 loss: 0.361391350534963
  batch 325 loss: 0.36136094542650077
  batch 326 loss: 0.36136282715329365
  batch 327 loss: 0.3613870829252657
  batch 328 loss: 0.36120189481028697
  batch 329 loss: 0.3613347255350246
  batch 330 loss: 0.36130675276120505
  batch 331 loss: 0.36128314582242704
  batch 332 loss: 0.3611721197165639
  batch 333 loss: 0.36106684419127916
  batch 334 loss: 0.3609749929098312
  batch 335 loss: 0.36073166861462946
  batch 336 loss: 0.36056667211509885
  batch 337 loss: 0.3604421422814049
  batch 338 loss: 0.3604041470225746
  batch 339 loss: 0.3603403397717659
  batch 340 loss: 0.36024180922438115
  batch 341 loss: 0.36007090668874053
  batch 342 loss: 0.359967422851345
  batch 343 loss: 0.3598378555246414
  batch 344 loss: 0.3597044292404208
  batch 345 loss: 0.35970134864682735
  batch 346 loss: 0.3594489984774176
  batch 347 loss: 0.3594516239180002
  batch 348 loss: 0.3593855419720726
  batch 349 loss: 0.3592786032354251
  batch 350 loss: 0.35930734378950935
  batch 351 loss: 0.3592719663242329
  batch 352 loss: 0.35931489388035104
  batch 353 loss: 0.3593202441846345
  batch 354 loss: 0.35945420859560456
  batch 355 loss: 0.3594270445931126
  batch 356 loss: 0.3593215997634309
  batch 357 loss: 0.3591722537990378
  batch 358 loss: 0.3592123463333652
  batch 359 loss: 0.35920737571703026
  batch 360 loss: 0.3592369344499376
  batch 361 loss: 0.3591419976008566
  batch 362 loss: 0.3590829909177116
  batch 363 loss: 0.35900248591236505
  batch 364 loss: 0.3589014640221229
  batch 365 loss: 0.3589648806068995
  batch 366 loss: 0.35885653290592257
  batch 367 loss: 0.3587315139393715
  batch 368 loss: 0.35860654727920244
  batch 369 loss: 0.3585619698695051
  batch 370 loss: 0.3585126024645728
  batch 371 loss: 0.3584738584220249
  batch 372 loss: 0.3584461035106772
  batch 373 loss: 0.358316551264745
  batch 374 loss: 0.3581281702786206
  batch 375 loss: 0.3580158483982086
  batch 376 loss: 0.35810200743218684
  batch 377 loss: 0.3580647792360827
  batch 378 loss: 0.35793324295805873
  batch 379 loss: 0.3579466066920348
  batch 380 loss: 0.3579753274980344
  batch 381 loss: 0.35784697892471873
  batch 382 loss: 0.35774575587342544
  batch 383 loss: 0.3577410533596268
  batch 384 loss: 0.3576780129224062
  batch 385 loss: 0.3577748768515401
  batch 386 loss: 0.35760014021643705
  batch 387 loss: 0.3576683788644559
  batch 388 loss: 0.35768673430705805
  batch 389 loss: 0.35765239519754227
  batch 390 loss: 0.35758507396930306
  batch 391 loss: 0.35754057490612234
  batch 392 loss: 0.3575172612861711
  batch 393 loss: 0.35757706486845137
  batch 394 loss: 0.35753686476479934
  batch 395 loss: 0.35743043483058107
  batch 396 loss: 0.3574116129917328
  batch 397 loss: 0.357435900333246
  batch 398 loss: 0.3573181773699708
  batch 399 loss: 0.35727265327795404
  batch 400 loss: 0.3572515370696783
  batch 401 loss: 0.35710124028591145
  batch 402 loss: 0.3570718058009646
  batch 403 loss: 0.35703314822305815
  batch 404 loss: 0.3570186379964989
  batch 405 loss: 0.3570578143184568
  batch 406 loss: 0.35712302648668803
  batch 407 loss: 0.35705283745500904
  batch 408 loss: 0.3570781514486846
  batch 409 loss: 0.3570219158106153
  batch 410 loss: 0.35703754432317686
  batch 411 loss: 0.35700873425117086
  batch 412 loss: 0.3569215057254995
  batch 413 loss: 0.356942924907652
  batch 414 loss: 0.3568755617176277
  batch 415 loss: 0.3568133798708399
  batch 416 loss: 0.3568334785791544
  batch 417 loss: 0.3568237331702555
  batch 418 loss: 0.3568153903957759
  batch 419 loss: 0.35681287784280524
  batch 420 loss: 0.35680079545293536
  batch 421 loss: 0.3567064661453002
  batch 422 loss: 0.35674600465602785
  batch 423 loss: 0.3567128066616419
  batch 424 loss: 0.3566297618145088
  batch 425 loss: 0.3566435371426975
  batch 426 loss: 0.35655821580002567
  batch 427 loss: 0.35652091010392967
  batch 428 loss: 0.3564294024466354
  batch 429 loss: 0.3564194409580497
  batch 430 loss: 0.3562888089307519
  batch 431 loss: 0.35631858472757716
  batch 432 loss: 0.3563892161128698
  batch 433 loss: 0.3563425818299036
  batch 434 loss: 0.3564421530418132
  batch 435 loss: 0.3563066868261359
  batch 436 loss: 0.35627832373074436
  batch 437 loss: 0.35635419629804055
  batch 438 loss: 0.35646649405836517
  batch 439 loss: 0.3563881166839382
  batch 440 loss: 0.35641775036400014
  batch 441 loss: 0.35632328341066705
  batch 442 loss: 0.3562238937859082
  batch 443 loss: 0.3561290256998609
  batch 444 loss: 0.35599323234579583
  batch 445 loss: 0.3559863853990362
  batch 446 loss: 0.3559105088892539
  batch 447 loss: 0.3557666593483364
  batch 448 loss: 0.35580455139279366
  batch 449 loss: 0.35586208423686716
  batch 450 loss: 0.35580657223860424
  batch 451 loss: 0.35581469621732337
  batch 452 loss: 0.3557730674479915
  batch 453 loss: 0.35589135633925467
  batch 454 loss: 0.3559556833841727
  batch 455 loss: 0.3559096343569703
  batch 456 loss: 0.35588668600509044
  batch 457 loss: 0.3559206508311975
  batch 458 loss: 0.35586794254300896
  batch 459 loss: 0.35579616337819814
  batch 460 loss: 0.3558549948360609
  batch 461 loss: 0.35582012325977813
  batch 462 loss: 0.3558264555998179
  batch 463 loss: 0.35571142142843737
  batch 464 loss: 0.35571517005305864
  batch 465 loss: 0.3556256444864376
  batch 466 loss: 0.3554717709706065
  batch 467 loss: 0.3556015344069397
  batch 468 loss: 0.3555326731159137
  batch 469 loss: 0.35562220984684634
  batch 470 loss: 0.3556415280763139
  batch 471 loss: 0.35563478316724173
  batch 472 loss: 0.3555001901873088
LOSS train 0.3555001901873088 valid 0.28748369216918945
LOSS train 0.3555001901873088 valid 0.28655537962913513
LOSS train 0.3555001901873088 valid 0.2931940754254659
LOSS train 0.3555001901873088 valid 0.28452104330062866
LOSS train 0.3555001901873088 valid 0.27913864850997927
LOSS train 0.3555001901873088 valid 0.28040483593940735
LOSS train 0.3555001901873088 valid 0.2930431919438498
LOSS train 0.3555001901873088 valid 0.29209233075380325
LOSS train 0.3555001901873088 valid 0.2914886044131385
LOSS train 0.3555001901873088 valid 0.2951802134513855
LOSS train 0.3555001901873088 valid 0.2930851009759036
LOSS train 0.3555001901873088 valid 0.29407358169555664
LOSS train 0.3555001901873088 valid 0.29375028151732224
LOSS train 0.3555001901873088 valid 0.2937197578804834
LOSS train 0.3555001901873088 valid 0.2889354517062505
LOSS train 0.3555001901873088 valid 0.28955446649342775
LOSS train 0.3555001901873088 valid 0.2903707106323803
LOSS train 0.3555001901873088 valid 0.29132045225964653
LOSS train 0.3555001901873088 valid 0.29235507861564036
LOSS train 0.3555001901873088 valid 0.29195757284760476
LOSS train 0.3555001901873088 valid 0.29105551540851593
LOSS train 0.3555001901873088 valid 0.2902135408737443
LOSS train 0.3555001901873088 valid 0.29005468863507977
LOSS train 0.3555001901873088 valid 0.28883982884387177
LOSS train 0.3555001901873088 valid 0.2872553157806397
LOSS train 0.3555001901873088 valid 0.2871183122579868
LOSS train 0.3555001901873088 valid 0.2872285478644901
LOSS train 0.3555001901873088 valid 0.28763200768402647
LOSS train 0.3555001901873088 valid 0.2881402650783802
LOSS train 0.3555001901873088 valid 0.28901450932025907
LOSS train 0.3555001901873088 valid 0.28986149834048364
LOSS train 0.3555001901873088 valid 0.290164683945477
LOSS train 0.3555001901873088 valid 0.2909868197007613
LOSS train 0.3555001901873088 valid 0.29073719329693737
LOSS train 0.3555001901873088 valid 0.29254489030156816
LOSS train 0.3555001901873088 valid 0.29178203062878716
LOSS train 0.3555001901873088 valid 0.29173180058195786
LOSS train 0.3555001901873088 valid 0.2929536166944002
LOSS train 0.3555001901873088 valid 0.2923420132734837
LOSS train 0.3555001901873088 valid 0.29273711144924164
LOSS train 0.3555001901873088 valid 0.2932130876110821
LOSS train 0.3555001901873088 valid 0.2933478483131954
LOSS train 0.3555001901873088 valid 0.2928395451501358
LOSS train 0.3555001901873088 valid 0.2933746257966215
LOSS train 0.3555001901873088 valid 0.2923594918515947
LOSS train 0.3555001901873088 valid 0.29283196511475934
LOSS train 0.3555001901873088 valid 0.29275855548838353
LOSS train 0.3555001901873088 valid 0.2927817938228448
LOSS train 0.3555001901873088 valid 0.29320538773828625
LOSS train 0.3555001901873088 valid 0.2926538097858429
LOSS train 0.3555001901873088 valid 0.29298127165027693
LOSS train 0.3555001901873088 valid 0.29319711144153887
LOSS train 0.3555001901873088 valid 0.29320517294811754
LOSS train 0.3555001901873088 valid 0.2931507858965132
LOSS train 0.3555001901873088 valid 0.2931524758989161
LOSS train 0.3555001901873088 valid 0.29318574390241076
LOSS train 0.3555001901873088 valid 0.2926730623370723
LOSS train 0.3555001901873088 valid 0.2925066655052119
LOSS train 0.3555001901873088 valid 0.2931071535005408
LOSS train 0.3555001901873088 valid 0.2927985196312269
LOSS train 0.3555001901873088 valid 0.29272722953655683
LOSS train 0.3555001901873088 valid 0.2935267206161253
LOSS train 0.3555001901873088 valid 0.29362778341959395
LOSS train 0.3555001901873088 valid 0.29463072633370757
LOSS train 0.3555001901873088 valid 0.29480630984673134
LOSS train 0.3555001901873088 valid 0.29491499924298487
LOSS train 0.3555001901873088 valid 0.29408720825145496
LOSS train 0.3555001901873088 valid 0.2941592495669337
LOSS train 0.3555001901873088 valid 0.29319369641767035
LOSS train 0.3555001901873088 valid 0.29303758165666033
LOSS train 0.3555001901873088 valid 0.2926290230432027
LOSS train 0.3555001901873088 valid 0.2927622838566701
LOSS train 0.3555001901873088 valid 0.2927472417893475
LOSS train 0.3555001901873088 valid 0.29240222375940633
LOSS train 0.3555001901873088 valid 0.29244221111138663
LOSS train 0.3555001901873088 valid 0.2929694168269634
LOSS train 0.3555001901873088 valid 0.29296922006390314
LOSS train 0.3555001901873088 valid 0.293057371599552
LOSS train 0.3555001901873088 valid 0.293072075217585
LOSS train 0.3555001901873088 valid 0.2923686416819692
LOSS train 0.3555001901873088 valid 0.29140106045905456
LOSS train 0.3555001901873088 valid 0.2914540878519779
LOSS train 0.3555001901873088 valid 0.291274492101497
LOSS train 0.3555001901873088 valid 0.29116936152180034
LOSS train 0.3555001901873088 valid 0.2907989373978446
LOSS train 0.3555001901873088 valid 0.2899524330399757
LOSS train 0.3555001901873088 valid 0.29004654562336274
LOSS train 0.3555001901873088 valid 0.28987445851618593
LOSS train 0.3555001901873088 valid 0.29019188479091346
LOSS train 0.3555001901873088 valid 0.2905262092749278
LOSS train 0.3555001901873088 valid 0.29060977631872825
LOSS train 0.3555001901873088 valid 0.2906575018297071
LOSS train 0.3555001901873088 valid 0.2904902164654065
LOSS train 0.3555001901873088 valid 0.29066682147218825
LOSS train 0.3555001901873088 valid 0.29039682369483144
LOSS train 0.3555001901873088 valid 0.2903031421204408
LOSS train 0.3555001901873088 valid 0.2905308250913915
LOSS train 0.3555001901873088 valid 0.29067580949287025
LOSS train 0.3555001901873088 valid 0.29076376798177006
LOSS train 0.3555001901873088 valid 0.29115407168865204
LOSS train 0.3555001901873088 valid 0.29142370052856975
LOSS train 0.3555001901873088 valid 0.2915329795842077
LOSS train 0.3555001901873088 valid 0.29136808549316184
LOSS train 0.3555001901873088 valid 0.29112148686097217
LOSS train 0.3555001901873088 valid 0.2914784678391048
LOSS train 0.3555001901873088 valid 0.29169349259925337
LOSS train 0.3555001901873088 valid 0.29132845496462884
LOSS train 0.3555001901873088 valid 0.2915210191298414
LOSS train 0.3555001901873088 valid 0.2920471435293145
LOSS train 0.3555001901873088 valid 0.29240178086540913
LOSS train 0.3555001901873088 valid 0.2920457476431185
LOSS train 0.3555001901873088 valid 0.29195104858704973
LOSS train 0.3555001901873088 valid 0.292161391899649
LOSS train 0.3555001901873088 valid 0.29211408318134774
LOSS train 0.3555001901873088 valid 0.29217540854993074
LOSS train 0.3555001901873088 valid 0.2924565449357033
LOSS train 0.3555001901873088 valid 0.2927441446699648
LOSS train 0.3555001901873088 valid 0.29270151914176296
LOSS train 0.3555001901873088 valid 0.29243211410626646
LOSS train 0.3555001901873088 valid 0.2922621354460716
LOSS train 0.3555001901873088 valid 0.2920762679793618
LOSS train 0.3555001901873088 valid 0.2920318194588677
LOSS train 0.3555001901873088 valid 0.2921612088273211
LOSS train 0.3555001901873088 valid 0.2922763865320913
LOSS train 0.3555001901873088 valid 0.2921695263385773
LOSS train 0.3555001901873088 valid 0.2922877473017526
LOSS train 0.3555001901873088 valid 0.2922320846966871
LOSS train 0.3555001901873088 valid 0.2924254247918725
LOSS train 0.3555001901873088 valid 0.29262880877006886
LOSS train 0.3555001901873088 valid 0.29262065979150625
LOSS train 0.3555001901873088 valid 0.2925822607433523
LOSS train 0.3555001901873088 valid 0.29219376465136354
LOSS train 0.3555001901873088 valid 0.2920014057168387
LOSS train 0.3555001901873088 valid 0.2920861787982841
LOSS train 0.3555001901873088 valid 0.2922387914525138
LOSS train 0.3555001901873088 valid 0.2921046804198447
LOSS train 0.3555001901873088 valid 0.29187343557820705
LOSS train 0.3555001901873088 valid 0.2918234658629998
LOSS train 0.3555001901873088 valid 0.2916948961482631
LOSS train 0.3555001901873088 valid 0.2918617775397641
LOSS train 0.3555001901873088 valid 0.29204545619217215
LOSS train 0.3555001901873088 valid 0.2923861256157848
LOSS train 0.3555001901873088 valid 0.2923681003438843
LOSS train 0.3555001901873088 valid 0.29236326170050436
LOSS train 0.3555001901873088 valid 0.29202106112036214
LOSS train 0.3555001901873088 valid 0.2921840667928735
LOSS train 0.3555001901873088 valid 0.29189193106832956
LOSS train 0.3555001901873088 valid 0.29259525863705454
LOSS train 0.3555001901873088 valid 0.29274063762402375
LOSS train 0.3555001901873088 valid 0.2927430826425552
LOSS train 0.3555001901873088 valid 0.29285878416718236
LOSS train 0.3555001901873088 valid 0.29265497075883967
LOSS train 0.3555001901873088 valid 0.2928536419385399
LOSS train 0.3555001901873088 valid 0.29288266618530473
LOSS train 0.3555001901873088 valid 0.29281304920873336
LOSS train 0.3555001901873088 valid 0.2930339937790846
LOSS train 0.3555001901873088 valid 0.2929529984285877
LOSS train 0.3555001901873088 valid 0.29310667194143125
LOSS train 0.3555001901873088 valid 0.2930242840973836
LOSS train 0.3555001901873088 valid 0.29283600859344006
LOSS train 0.3555001901873088 valid 0.2928604959330944
LOSS train 0.3555001901873088 valid 0.2928180587880405
LOSS train 0.3555001901873088 valid 0.2927939253716381
LOSS train 0.3555001901873088 valid 0.29262541307181844
LOSS train 0.3555001901873088 valid 0.2925271308783329
LOSS train 0.3555001901873088 valid 0.2926690147583743
LOSS train 0.3555001901873088 valid 0.2927372705436752
LOSS train 0.3555001901873088 valid 0.29260143921488807
LOSS train 0.3555001901873088 valid 0.2926420274923539
LOSS train 0.3555001901873088 valid 0.2927877242074293
LOSS train 0.3555001901873088 valid 0.29266539338039377
LOSS train 0.3555001901873088 valid 0.2925512710976046
LOSS train 0.3555001901873088 valid 0.2926634598329577
LOSS train 0.3555001901873088 valid 0.29263667830790596
LOSS train 0.3555001901873088 valid 0.2925245669909886
LOSS train 0.3555001901873088 valid 0.29239054007286375
LOSS train 0.3555001901873088 valid 0.2922462474154887
LOSS train 0.3555001901873088 valid 0.29246170500691016
LOSS train 0.3555001901873088 valid 0.2923572929211835
LOSS train 0.3555001901873088 valid 0.292375050816271
LOSS train 0.3555001901873088 valid 0.29253089065709825
LOSS train 0.3555001901873088 valid 0.29261567402671984
LOSS train 0.3555001901873088 valid 0.29253848524041515
LOSS train 0.3555001901873088 valid 0.29266748642143997
LOSS train 0.3555001901873088 valid 0.2925447312561241
LOSS train 0.3555001901873088 valid 0.29253265293695596
LOSS train 0.3555001901873088 valid 0.29240342863103286
LOSS train 0.3555001901873088 valid 0.2924742414913279
LOSS train 0.3555001901873088 valid 0.29252508960703694
LOSS train 0.3555001901873088 valid 0.29268844551161716
LOSS train 0.3555001901873088 valid 0.2926311786262153
LOSS train 0.3555001901873088 valid 0.29259088666488725
LOSS train 0.3555001901873088 valid 0.29262054028288687
LOSS train 0.3555001901873088 valid 0.2925419987169738
LOSS train 0.3555001901873088 valid 0.2923140120811952
LOSS train 0.3555001901873088 valid 0.2924412876671674
LOSS train 0.3555001901873088 valid 0.2925399851980548
LOSS train 0.3555001901873088 valid 0.2923840178985788
LOSS train 0.3555001901873088 valid 0.29239347008005456
LOSS train 0.3555001901873088 valid 0.29218476310372354
LOSS train 0.3555001901873088 valid 0.2920477436253087
LOSS train 0.3555001901873088 valid 0.2920719848410918
LOSS train 0.3555001901873088 valid 0.29190268055558793
LOSS train 0.3555001901873088 valid 0.2917526272289893
LOSS train 0.3555001901873088 valid 0.29157098517185304
LOSS train 0.3555001901873088 valid 0.29158883925201823
LOSS train 0.3555001901873088 valid 0.2915678945716452
LOSS train 0.3555001901873088 valid 0.29142457175140196
LOSS train 0.3555001901873088 valid 0.2912323590385857
LOSS train 0.3555001901873088 valid 0.2912351971580869
LOSS train 0.3555001901873088 valid 0.29138208622051076
LOSS train 0.3555001901873088 valid 0.2913141160641076
LOSS train 0.3555001901873088 valid 0.2914665495285965
LOSS train 0.3555001901873088 valid 0.2914261659172094
LOSS train 0.3555001901873088 valid 0.2912767906521642
LOSS train 0.3555001901873088 valid 0.29123308261235553
LOSS train 0.3555001901873088 valid 0.29117009387038273
LOSS train 0.3555001901873088 valid 0.2911751577886966
LOSS train 0.3555001901873088 valid 0.2912990390982258
LOSS train 0.3555001901873088 valid 0.2913833982565186
LOSS train 0.3555001901873088 valid 0.2914377432063694
LOSS train 0.3555001901873088 valid 0.29140336744420164
LOSS train 0.3555001901873088 valid 0.2916374666273861
LOSS train 0.3555001901873088 valid 0.29173442042831865
LOSS train 0.3555001901873088 valid 0.29184723999765183
LOSS train 0.3555001901873088 valid 0.29192983787671656
LOSS train 0.3555001901873088 valid 0.29224227913675854
LOSS train 0.3555001901873088 valid 0.2922640840212504
LOSS train 0.3555001901873088 valid 0.2923234209223085
LOSS train 0.3555001901873088 valid 0.29246518300927205
LOSS train 0.3555001901873088 valid 0.2924195515128957
LOSS train 0.3555001901873088 valid 0.29245889700692274
LOSS train 0.3555001901873088 valid 0.29241821975667076
LOSS train 0.3555001901873088 valid 0.29239610678110367
LOSS train 0.3555001901873088 valid 0.29240795399280306
LOSS train 0.3555001901873088 valid 0.29230538654630467
LOSS train 0.3555001901873088 valid 0.29238174773972747
LOSS train 0.3555001901873088 valid 0.29228900335416075
LOSS train 0.3555001901873088 valid 0.2921675408734437
LOSS train 0.3555001901873088 valid 0.29215526158610977
LOSS train 0.3555001901873088 valid 0.2922304595407114
LOSS train 0.3555001901873088 valid 0.2919470856258692
LOSS train 0.3555001901873088 valid 0.29202587994528406
LOSS train 0.3555001901873088 valid 0.29212109927759794
LOSS train 0.3555001901873088 valid 0.29232801357094124
LOSS train 0.3555001901873088 valid 0.2922517910478561
LOSS train 0.3555001901873088 valid 0.29228272298086994
LOSS train 0.3555001901873088 valid 0.29222988553585544
LOSS train 0.3555001901873088 valid 0.2921125849327409
LOSS train 0.3555001901873088 valid 0.2922299689054489
LOSS train 0.3555001901873088 valid 0.2921599812241664
LOSS train 0.3555001901873088 valid 0.29213169157978086
LOSS train 0.3555001901873088 valid 0.29201196846754657
LOSS train 0.3555001901873088 valid 0.29204473603428815
LOSS train 0.3555001901873088 valid 0.2921429207511977
LOSS train 0.3555001901873088 valid 0.2922045429004356
LOSS train 0.3555001901873088 valid 0.2920398210041254
LOSS train 0.3555001901873088 valid 0.29213360136793565
LOSS train 0.3555001901873088 valid 0.2920742810462893
LOSS train 0.3555001901873088 valid 0.29211177906164754
LOSS train 0.3555001901873088 valid 0.29219100415935007
LOSS train 0.3555001901873088 valid 0.29212550614171356
LOSS train 0.3555001901873088 valid 0.29210648597873207
LOSS train 0.3555001901873088 valid 0.2920557999926986
LOSS train 0.3555001901873088 valid 0.29201326988778026
LOSS train 0.3555001901873088 valid 0.29199664081845966
LOSS train 0.3555001901873088 valid 0.29214337881138264
LOSS train 0.3555001901873088 valid 0.2922896221280098
LOSS train 0.3555001901873088 valid 0.2923963477620405
LOSS train 0.3555001901873088 valid 0.2924617814796942
LOSS train 0.3555001901873088 valid 0.29257770306069913
LOSS train 0.3555001901873088 valid 0.29278509537963304
LOSS train 0.3555001901873088 valid 0.2927978799674974
LOSS train 0.3555001901873088 valid 0.2927508105109208
LOSS train 0.3555001901873088 valid 0.2927694926478646
LOSS train 0.3555001901873088 valid 0.2926787685440934
LOSS train 0.3555001901873088 valid 0.29254434846798866
LOSS train 0.3555001901873088 valid 0.29238118274177577
LOSS train 0.3555001901873088 valid 0.29241443748542484
LOSS train 0.3555001901873088 valid 0.2922803812793323
LOSS train 0.3555001901873088 valid 0.29213558068479084
LOSS train 0.3555001901873088 valid 0.2920455715969099
LOSS train 0.3555001901873088 valid 0.2920040466128305
LOSS train 0.3555001901873088 valid 0.29204580953843157
LOSS train 0.3555001901873088 valid 0.29211449811333107
LOSS train 0.3555001901873088 valid 0.2920046220500986
LOSS train 0.3555001901873088 valid 0.29186696094502973
LOSS train 0.3555001901873088 valid 0.2919248840254214
LOSS train 0.3555001901873088 valid 0.2919359761950879
LOSS train 0.3555001901873088 valid 0.2920124212215687
LOSS train 0.3555001901873088 valid 0.29196509892997874
LOSS train 0.3555001901873088 valid 0.2919523783334314
LOSS train 0.3555001901873088 valid 0.2918853439365231
LOSS train 0.3555001901873088 valid 0.29193143911507663
LOSS train 0.3555001901873088 valid 0.2919959221856069
LOSS train 0.3555001901873088 valid 0.29197096431980263
LOSS train 0.3555001901873088 valid 0.29197467406992156
LOSS train 0.3555001901873088 valid 0.2919384739142936
LOSS train 0.3555001901873088 valid 0.29197774094881424
LOSS train 0.3555001901873088 valid 0.2919770680864652
LOSS train 0.3555001901873088 valid 0.2920998742810119
LOSS train 0.3555001901873088 valid 0.29209569076828606
LOSS train 0.3555001901873088 valid 0.29219976529823277
LOSS train 0.3555001901873088 valid 0.29219155660585355
LOSS train 0.3555001901873088 valid 0.2920399740582607
LOSS train 0.3555001901873088 valid 0.29203877968141456
LOSS train 0.3555001901873088 valid 0.29204822571929967
LOSS train 0.3555001901873088 valid 0.2919682519292677
LOSS train 0.3555001901873088 valid 0.29204113096673895
LOSS train 0.3555001901873088 valid 0.2919979456451631
LOSS train 0.3555001901873088 valid 0.291886171966887
LOSS train 0.3555001901873088 valid 0.2919605103536294
LOSS train 0.3555001901873088 valid 0.29199199702221745
LOSS train 0.3555001901873088 valid 0.29211677539690284
LOSS train 0.3555001901873088 valid 0.29211320285759274
LOSS train 0.3555001901873088 valid 0.29206365191295175
LOSS train 0.3555001901873088 valid 0.2921253660405848
LOSS train 0.3555001901873088 valid 0.2921720657427356
LOSS train 0.3555001901873088 valid 0.2922742367257892
LOSS train 0.3555001901873088 valid 0.29219265817664564
LOSS train 0.3555001901873088 valid 0.2922853438932205
LOSS train 0.3555001901873088 valid 0.2922217188322026
LOSS train 0.3555001901873088 valid 0.2922059593278188
LOSS train 0.3555001901873088 valid 0.2921823844037674
LOSS train 0.3555001901873088 valid 0.2922314397646831
LOSS train 0.3555001901873088 valid 0.292335961631105
LOSS train 0.3555001901873088 valid 0.29240054297702395
LOSS train 0.3555001901873088 valid 0.29237206866283244
LOSS train 0.3555001901873088 valid 0.2925227540332858
LOSS train 0.3555001901873088 valid 0.29249476885253733
LOSS train 0.3555001901873088 valid 0.2924192128674862
LOSS train 0.3555001901873088 valid 0.29231203549418106
LOSS train 0.3555001901873088 valid 0.2922849410378539
LOSS train 0.3555001901873088 valid 0.2924732651985334
LOSS train 0.3555001901873088 valid 0.29250937339974875
LOSS train 0.3555001901873088 valid 0.29252771984431003
LOSS train 0.3555001901873088 valid 0.2925401670023666
LOSS train 0.3555001901873088 valid 0.2924856580132563
LOSS train 0.3555001901873088 valid 0.2924811115219178
LOSS train 0.3555001901873088 valid 0.2924467686782865
LOSS train 0.3555001901873088 valid 0.29241449536640973
LOSS train 0.3555001901873088 valid 0.2924359856903204
LOSS train 0.3555001901873088 valid 0.29245470325210005
LOSS train 0.3555001901873088 valid 0.29261390513978724
LOSS train 0.3555001901873088 valid 0.29266230554684347
LOSS train 0.3555001901873088 valid 0.2925717491721142
LOSS train 0.3555001901873088 valid 0.29248181058453554
LOSS train 0.3555001901873088 valid 0.292399514552163
LOSS train 0.3555001901873088 valid 0.2924357352506124
LOSS train 0.3555001901873088 valid 0.29235772409609384
LOSS train 0.3555001901873088 valid 0.2922610992464924
LOSS train 0.3555001901873088 valid 0.2922762318683619
LOSS train 0.3555001901873088 valid 0.2922671109269091
LOSS train 0.3555001901873088 valid 0.29225396533301995
LOSS train 0.3555001901873088 valid 0.29238698411155756
LOSS train 0.3555001901873088 valid 0.29242984778927955
LOSS train 0.3555001901873088 valid 0.29231238694918926
LOSS train 0.3555001901873088 valid 0.2922783426149597
LOSS train 0.3555001901873088 valid 0.29231748968611854
LOSS train 0.3555001901873088 valid 0.29230367371605503
LOSS train 0.3555001901873088 valid 0.29224706385439453
LOSS train 0.3555001901873088 valid 0.2923211869860881
LOSS train 0.3555001901873088 valid 0.29219242298241815
LOSS train 0.3555001901873088 valid 0.29220440091340094
LOSS train 0.3555001901873088 valid 0.292196134671773
LOSS train 0.3555001901873088 valid 0.2921467755661636
LOSS train 0.3555001901873088 valid 0.29201132034735716
LOSS train 0.3555001901873088 valid 0.2919325268948856
LOSS train 0.3555001901873088 valid 0.2920760509929037
EPOCH 3:
  batch 1 loss: 0.3491832911968231
  batch 2 loss: 0.3423905521631241
  batch 3 loss: 0.34829888741175336
  batch 4 loss: 0.3495800048112869
  batch 5 loss: 0.35435808897018434
  batch 6 loss: 0.3534538944562276
  batch 7 loss: 0.35590910485812594
  batch 8 loss: 0.3574866093695164
  batch 9 loss: 0.3568548129664527
  batch 10 loss: 0.3584471732378006
  batch 11 loss: 0.35642625526948407
  batch 12 loss: 0.35243042061726254
  batch 13 loss: 0.35097575875429005
  batch 14 loss: 0.3514887775693621
  batch 15 loss: 0.35094391703605654
  batch 16 loss: 0.35097062960267067
  batch 17 loss: 0.3477650530198041
  batch 18 loss: 0.3492120040787591
  batch 19 loss: 0.3479844143516139
  batch 20 loss: 0.34588226974010466
  batch 21 loss: 0.345668036313284
  batch 22 loss: 0.3450388258153742
  batch 23 loss: 0.3445612827072973
  batch 24 loss: 0.34259913737575215
  batch 25 loss: 0.3439210283756256
  batch 26 loss: 0.3425472229719162
  batch 27 loss: 0.3421051115901382
  batch 28 loss: 0.33997370834861484
  batch 29 loss: 0.34072913486382056
  batch 30 loss: 0.3397544056177139
  batch 31 loss: 0.3408520144800986
  batch 32 loss: 0.33923990186303854
  batch 33 loss: 0.33948017972888367
  batch 34 loss: 0.33953949107843284
  batch 35 loss: 0.3396217482430594
  batch 36 loss: 0.3393174409866333
  batch 37 loss: 0.3406653863352698
  batch 38 loss: 0.3408132611136687
  batch 39 loss: 0.3408443881915166
  batch 40 loss: 0.34028579890727995
  batch 41 loss: 0.33988804497369907
  batch 42 loss: 0.34016593794027966
  batch 43 loss: 0.340582164914109
  batch 44 loss: 0.3404085277156396
  batch 45 loss: 0.34013383984565737
  batch 46 loss: 0.3396649399529333
  batch 47 loss: 0.3398223379825024
  batch 48 loss: 0.3386105541139841
  batch 49 loss: 0.33802129541124615
  batch 50 loss: 0.3375638401508331
  batch 51 loss: 0.3370720188991696
  batch 52 loss: 0.33753563635624373
  batch 53 loss: 0.3373098260951492
  batch 54 loss: 0.33782617527025716
  batch 55 loss: 0.33796286420388655
  batch 56 loss: 0.33796975389122963
  batch 57 loss: 0.3374279245995639
  batch 58 loss: 0.337910788840261
  batch 59 loss: 0.3379451536526114
  batch 60 loss: 0.33749808768431344
  batch 61 loss: 0.33787626418911043
  batch 62 loss: 0.33788121900250834
  batch 63 loss: 0.3379603170213245
  batch 64 loss: 0.33853961946442723
  batch 65 loss: 0.33798322540063125
  batch 66 loss: 0.33792950077490375
  batch 67 loss: 0.33850867223383774
  batch 68 loss: 0.33932485168471055
  batch 69 loss: 0.33914821001066675
  batch 70 loss: 0.33961651750973293
  batch 71 loss: 0.33944747775373324
  batch 72 loss: 0.3394382285575072
  batch 73 loss: 0.3394446401563409
  batch 74 loss: 0.3397174078870464
  batch 75 loss: 0.33916754484176637
  batch 76 loss: 0.33966981658810064
  batch 77 loss: 0.33955308446636445
  batch 78 loss: 0.33950962622960407
  batch 79 loss: 0.33981178757510605
  batch 80 loss: 0.33945438228547575
  batch 81 loss: 0.3396076372376195
  batch 82 loss: 0.3400334581369307
  batch 83 loss: 0.33985298692461957
  batch 84 loss: 0.33961661018076394
  batch 85 loss: 0.3391990156734691
  batch 86 loss: 0.33963322431542153
  batch 87 loss: 0.3396008877918638
  batch 88 loss: 0.3391184498640624
  batch 89 loss: 0.3390632462635469
  batch 90 loss: 0.3392190519306395
  batch 91 loss: 0.3394261670636607
  batch 92 loss: 0.33933963561835495
  batch 93 loss: 0.3393630478330838
  batch 94 loss: 0.33981416136660475
  batch 95 loss: 0.3394209014741998
  batch 96 loss: 0.33945911563932896
  batch 97 loss: 0.3400198678994916
  batch 98 loss: 0.34011622837611605
  batch 99 loss: 0.34010773626240814
  batch 100 loss: 0.34003786206245423
  batch 101 loss: 0.3398609022692879
  batch 102 loss: 0.3400638594346888
  batch 103 loss: 0.34060960834466136
  batch 104 loss: 0.34050540855297673
  batch 105 loss: 0.3399517530486697
  batch 106 loss: 0.340380164249888
  batch 107 loss: 0.3397954017202431
  batch 108 loss: 0.3397233557921869
  batch 109 loss: 0.3395772699369203
  batch 110 loss: 0.33971718305891213
  batch 111 loss: 0.3394209584137341
  batch 112 loss: 0.33917404817683355
  batch 113 loss: 0.3397402789740436
  batch 114 loss: 0.3399727070017865
  batch 115 loss: 0.3402473934318708
  batch 116 loss: 0.3404573777626301
  batch 117 loss: 0.34059170270577455
  batch 118 loss: 0.3402678774575056
  batch 119 loss: 0.34059426764480205
  batch 120 loss: 0.3405733441313108
  batch 121 loss: 0.3405227914822003
  batch 122 loss: 0.34025287994595826
  batch 123 loss: 0.3402387364123895
  batch 124 loss: 0.3404775569515844
  batch 125 loss: 0.3405307292938232
  batch 126 loss: 0.3404175102237671
  batch 127 loss: 0.34087157507581034
  batch 128 loss: 0.3407070853281766
  batch 129 loss: 0.34068952674089475
  batch 130 loss: 0.34046751971428213
  batch 131 loss: 0.340589776293922
  batch 132 loss: 0.3405202503005664
  batch 133 loss: 0.34068579669285537
  batch 134 loss: 0.3405013778316441
  batch 135 loss: 0.34041719127584386
  batch 136 loss: 0.3403007510830374
  batch 137 loss: 0.34017526015748073
  batch 138 loss: 0.3400780053242393
  batch 139 loss: 0.3405084136149866
  batch 140 loss: 0.34032779378550393
  batch 141 loss: 0.34050281250730474
  batch 142 loss: 0.3404534895655135
  batch 143 loss: 0.3400977699906676
  batch 144 loss: 0.3400540954122941
  batch 145 loss: 0.3399527163341128
  batch 146 loss: 0.33988522095222995
  batch 147 loss: 0.34012698965007754
  batch 148 loss: 0.3401055865593859
  batch 149 loss: 0.3399660413297231
  batch 150 loss: 0.3399171487490336
  batch 151 loss: 0.33979513372806525
  batch 152 loss: 0.3400390156005558
  batch 153 loss: 0.3399161022473005
  batch 154 loss: 0.33996717709225494
  batch 155 loss: 0.34008882353382724
  batch 156 loss: 0.34006729320837903
  batch 157 loss: 0.3403555272491115
  batch 158 loss: 0.34040558338165283
  batch 159 loss: 0.34048356631266996
  batch 160 loss: 0.3403074983507395
  batch 161 loss: 0.34037705700590004
  batch 162 loss: 0.3402294507365168
  batch 163 loss: 0.33999465415082825
  batch 164 loss: 0.340056758646558
  batch 165 loss: 0.33990028789549165
  batch 166 loss: 0.33987268625971784
  batch 167 loss: 0.339687097929195
  batch 168 loss: 0.33972001395055224
  batch 169 loss: 0.339482617625118
  batch 170 loss: 0.3393419120241614
  batch 171 loss: 0.3391935211873194
  batch 172 loss: 0.33911101468080695
  batch 173 loss: 0.33913443767266466
  batch 174 loss: 0.33921945557512084
  batch 175 loss: 0.3394756874016353
  batch 176 loss: 0.33914648419754073
  batch 177 loss: 0.33909193960960304
  batch 178 loss: 0.33917829025997204
  batch 179 loss: 0.33914244341450694
  batch 180 loss: 0.33897554179032646
  batch 181 loss: 0.3388422121659168
  batch 182 loss: 0.33880283347852935
  batch 183 loss: 0.3386863069456132
  batch 184 loss: 0.3385258475723474
  batch 185 loss: 0.33859741397806115
  batch 186 loss: 0.33873245168116783
  batch 187 loss: 0.3386897392132703
  batch 188 loss: 0.33852877943439685
  batch 189 loss: 0.3383273496514275
  batch 190 loss: 0.3382946019109927
  batch 191 loss: 0.3382268184454653
  batch 192 loss: 0.33818218143035966
  batch 193 loss: 0.3381975512430458
  batch 194 loss: 0.33838727855190787
  batch 195 loss: 0.3384277493525774
  batch 196 loss: 0.3382634021797959
  batch 197 loss: 0.338125691196035
  batch 198 loss: 0.33832892143365106
  batch 199 loss: 0.3382058413184468
  batch 200 loss: 0.33821891382336616
  batch 201 loss: 0.338228535296312
  batch 202 loss: 0.3380773535164276
  batch 203 loss: 0.3379766669473037
  batch 204 loss: 0.33784110727263433
  batch 205 loss: 0.337907315027423
  batch 206 loss: 0.33779900658477857
  batch 207 loss: 0.3377818567453375
  batch 208 loss: 0.33759665661133254
  batch 209 loss: 0.33728984440342663
  batch 210 loss: 0.3372382111492611
  batch 211 loss: 0.3373043791377714
  batch 212 loss: 0.3372821182293712
  batch 213 loss: 0.3372687074779904
  batch 214 loss: 0.3372595289321703
  batch 215 loss: 0.33692249239877214
  batch 216 loss: 0.3367874827925806
  batch 217 loss: 0.3368808017348364
  batch 218 loss: 0.3367812798930964
  batch 219 loss: 0.3368066351979835
  batch 220 loss: 0.3367793561382727
  batch 221 loss: 0.33692291594738333
  batch 222 loss: 0.3369493044174469
  batch 223 loss: 0.3369448005885821
  batch 224 loss: 0.3369848966332419
  batch 225 loss: 0.3367706893550025
  batch 226 loss: 0.33689736489173583
  batch 227 loss: 0.3366531538018046
  batch 228 loss: 0.33666966803241194
  batch 229 loss: 0.33653067397238384
  batch 230 loss: 0.3365861820138019
  batch 231 loss: 0.33657298936988367
  batch 232 loss: 0.33637008880232944
  batch 233 loss: 0.336299672530956
  batch 234 loss: 0.33642871767027765
  batch 235 loss: 0.33650310217066015
  batch 236 loss: 0.33641695976257324
  batch 237 loss: 0.33644876009804286
  batch 238 loss: 0.3362810979871189
  batch 239 loss: 0.33632556190051793
  batch 240 loss: 0.336390949289004
  batch 241 loss: 0.33641143260655065
  batch 242 loss: 0.3361486399715597
  batch 243 loss: 0.33630898545798943
  batch 244 loss: 0.3362724855542183
  batch 245 loss: 0.33631336591681654
  batch 246 loss: 0.33626172552263833
  batch 247 loss: 0.33630516654566717
  batch 248 loss: 0.33627519095616953
  batch 249 loss: 0.3361589071980442
  batch 250 loss: 0.3360711859464645
  batch 251 loss: 0.336166002004745
  batch 252 loss: 0.3360058261998116
  batch 253 loss: 0.33579392760638666
  batch 254 loss: 0.33570797952610676
  batch 255 loss: 0.33570679648249757
  batch 256 loss: 0.335677124443464
  batch 257 loss: 0.3355207674002369
  batch 258 loss: 0.3355245465456053
  batch 259 loss: 0.33552783060257957
  batch 260 loss: 0.3354968693393927
  batch 261 loss: 0.3354938214081001
  batch 262 loss: 0.33533702642863034
  batch 263 loss: 0.33533310062985006
  batch 264 loss: 0.3351977741853757
  batch 265 loss: 0.3351426438340601
  batch 266 loss: 0.33504546909852134
  batch 267 loss: 0.3352511140067926
  batch 268 loss: 0.3352041111063601
  batch 269 loss: 0.3351737072476667
  batch 270 loss: 0.3352903700537152
  batch 271 loss: 0.33534975916257204
  batch 272 loss: 0.33541262818171697
  batch 273 loss: 0.33533040442309536
  batch 274 loss: 0.33541418514112487
  batch 275 loss: 0.3354599385911768
  batch 276 loss: 0.3353814308842023
  batch 277 loss: 0.3353376910359421
  batch 278 loss: 0.335292525214257
  batch 279 loss: 0.3353930346427425
  batch 280 loss: 0.3352233809019838
  batch 281 loss: 0.3352168176946267
  batch 282 loss: 0.3352113186467624
  batch 283 loss: 0.33505629397955045
  batch 284 loss: 0.3351107792745174
  batch 285 loss: 0.3350288315823204
  batch 286 loss: 0.335050947182662
  batch 287 loss: 0.3351203745250503
  batch 288 loss: 0.3350107960609926
  batch 289 loss: 0.3352219077748823
  batch 290 loss: 0.33506979726511854
  batch 291 loss: 0.33503529490883815
  batch 292 loss: 0.3350756704194905
  batch 293 loss: 0.33498467367663726
  batch 294 loss: 0.33490489828748765
  batch 295 loss: 0.3348738673379866
  batch 296 loss: 0.3348908331748602
  batch 297 loss: 0.3347582034390382
  batch 298 loss: 0.33471866102026615
  batch 299 loss: 0.3346936208348609
  batch 300 loss: 0.3347368606925011
  batch 301 loss: 0.33464555932437856
  batch 302 loss: 0.33470488620909633
  batch 303 loss: 0.3346658684436244
  batch 304 loss: 0.3345633636375791
  batch 305 loss: 0.33445154760704665
  batch 306 loss: 0.3345291373971241
  batch 307 loss: 0.3345248074018994
  batch 308 loss: 0.3345828331136084
  batch 309 loss: 0.33445740700925436
  batch 310 loss: 0.33437849119786295
  batch 311 loss: 0.33454029985562783
  batch 312 loss: 0.3345969623098007
  batch 313 loss: 0.3346348373463359
  batch 314 loss: 0.3345939685015162
  batch 315 loss: 0.3345395102387383
  batch 316 loss: 0.3344621347098411
  batch 317 loss: 0.33449635060051636
  batch 318 loss: 0.3344775268679145
  batch 319 loss: 0.3343652440089043
  batch 320 loss: 0.33432661965489385
  batch 321 loss: 0.3343171096665094
  batch 322 loss: 0.33426371300072405
  batch 323 loss: 0.3342832777153228
  batch 324 loss: 0.33407264524771846
  batch 325 loss: 0.33404860001343945
  batch 326 loss: 0.33407859010564767
  batch 327 loss: 0.3340686870459752
  batch 328 loss: 0.3338847089467979
  batch 329 loss: 0.3339876433817449
  batch 330 loss: 0.33394357002142705
  batch 331 loss: 0.33394590065198365
  batch 332 loss: 0.333914463150214
  batch 333 loss: 0.333836027720311
  batch 334 loss: 0.33373413328639046
  batch 335 loss: 0.33352866662082387
  batch 336 loss: 0.33340667596175555
  batch 337 loss: 0.33326366534926416
  batch 338 loss: 0.33320246892567923
  batch 339 loss: 0.33320674071621403
  batch 340 loss: 0.3331812278312795
  batch 341 loss: 0.3331187126223992
  batch 342 loss: 0.3330293418022624
  batch 343 loss: 0.3330123836375534
  batch 344 loss: 0.33292703243882155
  batch 345 loss: 0.3329685746759608
  batch 346 loss: 0.3328025925882979
  batch 347 loss: 0.3327735193524649
  batch 348 loss: 0.3326881638374822
  batch 349 loss: 0.3325462857939111
  batch 350 loss: 0.33257072380610875
  batch 351 loss: 0.33252513739797807
  batch 352 loss: 0.33261513769287954
  batch 353 loss: 0.3326340337988337
  batch 354 loss: 0.3328173315457705
  batch 355 loss: 0.3327907117319779
  batch 356 loss: 0.33270514848526944
  batch 357 loss: 0.33261428009562133
  batch 358 loss: 0.33264968423537034
  batch 359 loss: 0.3326145933531121
  batch 360 loss: 0.33261982128024103
  batch 361 loss: 0.3325418701957798
  batch 362 loss: 0.33248025639939705
  batch 363 loss: 0.3324139242986674
  batch 364 loss: 0.33237991997828853
  batch 365 loss: 0.3324563589814591
  batch 366 loss: 0.3323786039658583
  batch 367 loss: 0.3322657327236207
  batch 368 loss: 0.33219975200684176
  batch 369 loss: 0.33214935143465596
  batch 370 loss: 0.3321167278128701
  batch 371 loss: 0.33208523831277525
  batch 372 loss: 0.3320580489372694
  batch 373 loss: 0.33192746984415333
  batch 374 loss: 0.33176861392622964
  batch 375 loss: 0.33169261598587035
  batch 376 loss: 0.3318031247943006
  batch 377 loss: 0.331779446937045
  batch 378 loss: 0.33166638426679784
  batch 379 loss: 0.3316497551105268
  batch 380 loss: 0.3316813474423007
  batch 381 loss: 0.33156472507111356
  batch 382 loss: 0.33147694193879973
  batch 383 loss: 0.33142744008617053
  batch 384 loss: 0.3313570353978624
  batch 385 loss: 0.33145357006556025
  batch 386 loss: 0.33131026441877987
  batch 387 loss: 0.3313154196246342
  batch 388 loss: 0.3313133350967132
  batch 389 loss: 0.33127955483287036
  batch 390 loss: 0.33124355054818666
  batch 391 loss: 0.33129122792302496
  batch 392 loss: 0.3313287146085379
  batch 393 loss: 0.33138438134096354
  batch 394 loss: 0.3313655800323196
  batch 395 loss: 0.3313235168970084
  batch 396 loss: 0.3313169638917904
  batch 397 loss: 0.33129455408161174
  batch 398 loss: 0.3312008631738586
  batch 399 loss: 0.33119550287574157
  batch 400 loss: 0.3312173181772232
  batch 401 loss: 0.3310986963888059
  batch 402 loss: 0.33107351819377634
  batch 403 loss: 0.3310758624567879
  batch 404 loss: 0.33103825520760943
  batch 405 loss: 0.33108988602956135
  batch 406 loss: 0.33111999466501435
  batch 407 loss: 0.33108539802436454
  batch 408 loss: 0.3311212795476119
  batch 409 loss: 0.33103661956880376
  batch 410 loss: 0.3310693029223419
  batch 411 loss: 0.3310089637763309
  batch 412 loss: 0.33093071352798964
  batch 413 loss: 0.33101427345818524
  batch 414 loss: 0.33099132013205745
  batch 415 loss: 0.3309545625405139
  batch 416 loss: 0.33097962075128007
  batch 417 loss: 0.3309586251096474
  batch 418 loss: 0.3309336604113784
  batch 419 loss: 0.3309421978873114
  batch 420 loss: 0.3309046555133093
  batch 421 loss: 0.3308128667132588
  batch 422 loss: 0.3308686678302232
  batch 423 loss: 0.33088840265927866
  batch 424 loss: 0.33082655550173995
  batch 425 loss: 0.3308376615187701
  batch 426 loss: 0.3307732647993195
  batch 427 loss: 0.3307451415257376
  batch 428 loss: 0.33064925698476416
  batch 429 loss: 0.33060913326301217
  batch 430 loss: 0.33050191381642985
  batch 431 loss: 0.3305284367774866
  batch 432 loss: 0.33056076271114526
  batch 433 loss: 0.3305171781819632
  batch 434 loss: 0.3306069820432619
  batch 435 loss: 0.3304833008640114
  batch 436 loss: 0.3304566102563788
  batch 437 loss: 0.33052381914049317
  batch 438 loss: 0.33066101739667864
  batch 439 loss: 0.3306244016372533
  batch 440 loss: 0.33067988042127
  batch 441 loss: 0.3306173069271641
  batch 442 loss: 0.33060133012171783
  batch 443 loss: 0.33051357904352396
  batch 444 loss: 0.33039641413871235
  batch 445 loss: 0.3303991401463412
  batch 446 loss: 0.3303343034913187
  batch 447 loss: 0.3302149317275224
  batch 448 loss: 0.3302270970972521
  batch 449 loss: 0.33030970955479116
  batch 450 loss: 0.3302780384487576
  batch 451 loss: 0.33025989154490026
  batch 452 loss: 0.3302040338779973
  batch 453 loss: 0.33032439040558753
  batch 454 loss: 0.33040675507768136
  batch 455 loss: 0.3303611892265278
  batch 456 loss: 0.3303620254522876
  batch 457 loss: 0.33037004103024104
  batch 458 loss: 0.33032467089365664
  batch 459 loss: 0.33026745928398665
  batch 460 loss: 0.3303277394045954
  batch 461 loss: 0.33027475399981354
  batch 462 loss: 0.33027492747420356
  batch 463 loss: 0.3301588160430381
  batch 464 loss: 0.3301903231647508
  batch 465 loss: 0.33009532683639115
  batch 466 loss: 0.32993137542270284
  batch 467 loss: 0.33007071596396814
  batch 468 loss: 0.33000750839710236
  batch 469 loss: 0.3301011196204594
  batch 470 loss: 0.3301307108808071
  batch 471 loss: 0.3301487878882455
  batch 472 loss: 0.33000900009173456
LOSS train 0.33000900009173456 valid 0.28371158242225647
LOSS train 0.33000900009173456 valid 0.2776946872472763
LOSS train 0.33000900009173456 valid 0.28219279646873474
LOSS train 0.33000900009173456 valid 0.27323876321315765
LOSS train 0.33000900009173456 valid 0.2662560135126114
LOSS train 0.33000900009173456 valid 0.26752707610527676
LOSS train 0.33000900009173456 valid 0.2792932710477284
LOSS train 0.33000900009173456 valid 0.27565305121243
LOSS train 0.33000900009173456 valid 0.2756493439277013
LOSS train 0.33000900009173456 valid 0.2785798653960228
LOSS train 0.33000900009173456 valid 0.27675345946442
LOSS train 0.33000900009173456 valid 0.2775906485815843
LOSS train 0.33000900009173456 valid 0.276297648365681
LOSS train 0.33000900009173456 valid 0.2761854880622455
LOSS train 0.33000900009173456 valid 0.27102396488189695
LOSS train 0.33000900009173456 valid 0.2716558687388897
LOSS train 0.33000900009173456 valid 0.27266738344641295
LOSS train 0.33000900009173456 valid 0.27361633049117196
LOSS train 0.33000900009173456 valid 0.2745374268607089
LOSS train 0.33000900009173456 valid 0.2740357890725136
LOSS train 0.33000900009173456 valid 0.27328527967135113
LOSS train 0.33000900009173456 valid 0.27255527539686725
LOSS train 0.33000900009173456 valid 0.27295957052189374
LOSS train 0.33000900009173456 valid 0.27202852194507915
LOSS train 0.33000900009173456 valid 0.2700943470001221
LOSS train 0.33000900009173456 valid 0.27006909938958973
LOSS train 0.33000900009173456 valid 0.26999578100663646
LOSS train 0.33000900009173456 valid 0.2706835812755993
LOSS train 0.33000900009173456 valid 0.2710321853900778
LOSS train 0.33000900009173456 valid 0.2718874841928482
LOSS train 0.33000900009173456 valid 0.2730973958969116
LOSS train 0.33000900009173456 valid 0.2732767118141055
LOSS train 0.33000900009173456 valid 0.27432508089325647
LOSS train 0.33000900009173456 valid 0.27422846503117504
LOSS train 0.33000900009173456 valid 0.2759026110172272
LOSS train 0.33000900009173456 valid 0.2754858144455486
LOSS train 0.33000900009173456 valid 0.27577758882496806
LOSS train 0.33000900009173456 valid 0.2770226919337323
LOSS train 0.33000900009173456 valid 0.2764454033130255
LOSS train 0.33000900009173456 valid 0.27683028355240824
LOSS train 0.33000900009173456 valid 0.27725929676032646
LOSS train 0.33000900009173456 valid 0.2773591989562625
LOSS train 0.33000900009173456 valid 0.2767618610415348
LOSS train 0.33000900009173456 valid 0.2771965583617037
LOSS train 0.33000900009173456 valid 0.27633478343486784
LOSS train 0.33000900009173456 valid 0.27669160618730215
LOSS train 0.33000900009173456 valid 0.2768193397750246
LOSS train 0.33000900009173456 valid 0.27675483593096334
LOSS train 0.33000900009173456 valid 0.2773371676401216
LOSS train 0.33000900009173456 valid 0.2767863517999649
LOSS train 0.33000900009173456 valid 0.27704420276716646
LOSS train 0.33000900009173456 valid 0.2772493436932564
LOSS train 0.33000900009173456 valid 0.27717116081489707
LOSS train 0.33000900009173456 valid 0.27716194755501217
LOSS train 0.33000900009173456 valid 0.27714881084182047
LOSS train 0.33000900009173456 valid 0.2772875688970089
LOSS train 0.33000900009173456 valid 0.27694045242510346
LOSS train 0.33000900009173456 valid 0.2766788103457155
LOSS train 0.33000900009173456 valid 0.2773491639201924
LOSS train 0.33000900009173456 valid 0.2769187395771345
LOSS train 0.33000900009173456 valid 0.27672137295613525
LOSS train 0.33000900009173456 valid 0.277361846739246
LOSS train 0.33000900009173456 valid 0.2775787403659215
LOSS train 0.33000900009173456 valid 0.2784287082031369
LOSS train 0.33000900009173456 valid 0.27860246163148145
LOSS train 0.33000900009173456 valid 0.27862968905405566
LOSS train 0.33000900009173456 valid 0.27782237707678953
LOSS train 0.33000900009173456 valid 0.2780220916166025
LOSS train 0.33000900009173456 valid 0.27703209955623187
LOSS train 0.33000900009173456 valid 0.27697161329644066
LOSS train 0.33000900009173456 valid 0.2765578012651121
LOSS train 0.33000900009173456 valid 0.27662369877927834
LOSS train 0.33000900009173456 valid 0.27656041575621254
LOSS train 0.33000900009173456 valid 0.276176682396515
LOSS train 0.33000900009173456 valid 0.2761693849166234
LOSS train 0.33000900009173456 valid 0.2766272568781125
LOSS train 0.33000900009173456 valid 0.2765239498444966
LOSS train 0.33000900009173456 valid 0.2765585579551183
LOSS train 0.33000900009173456 valid 0.2766709382398219
LOSS train 0.33000900009173456 valid 0.27593100946396587
LOSS train 0.33000900009173456 valid 0.27499716682934466
LOSS train 0.33000900009173456 valid 0.2750135091383283
LOSS train 0.33000900009173456 valid 0.2747770492211882
LOSS train 0.33000900009173456 valid 0.2747329929400058
LOSS train 0.33000900009173456 valid 0.2742604224120869
LOSS train 0.33000900009173456 valid 0.27348325574813886
LOSS train 0.33000900009173456 valid 0.2735377524433465
LOSS train 0.33000900009173456 valid 0.2733034130863168
LOSS train 0.33000900009173456 valid 0.2736259000689796
LOSS train 0.33000900009173456 valid 0.2739437167843183
LOSS train 0.33000900009173456 valid 0.27401632381664526
LOSS train 0.33000900009173456 valid 0.27400475886204967
LOSS train 0.33000900009173456 valid 0.27387293932899354
LOSS train 0.33000900009173456 valid 0.27415454086471114
LOSS train 0.33000900009173456 valid 0.27385799131895366
LOSS train 0.33000900009173456 valid 0.2739362803598245
LOSS train 0.33000900009173456 valid 0.2740718415103008
LOSS train 0.33000900009173456 valid 0.27428495214909926
LOSS train 0.33000900009173456 valid 0.2743678725127018
LOSS train 0.33000900009173456 valid 0.27469083935022354
LOSS train 0.33000900009173456 valid 0.2749616727970614
LOSS train 0.33000900009173456 valid 0.27520666987288234
LOSS train 0.33000900009173456 valid 0.2749859627010753
LOSS train 0.33000900009173456 valid 0.2747789400701339
LOSS train 0.33000900009173456 valid 0.27517066030275256
LOSS train 0.33000900009173456 valid 0.2754014218753239
LOSS train 0.33000900009173456 valid 0.2750291161448042
LOSS train 0.33000900009173456 valid 0.2752439365894706
LOSS train 0.33000900009173456 valid 0.27580178194089766
LOSS train 0.33000900009173456 valid 0.2761572309515693
LOSS train 0.33000900009173456 valid 0.27574709272599435
LOSS train 0.33000900009173456 valid 0.27562866306730677
LOSS train 0.33000900009173456 valid 0.275732933683733
LOSS train 0.33000900009173456 valid 0.2756280410185195
LOSS train 0.33000900009173456 valid 0.2756925766882689
LOSS train 0.33000900009173456 valid 0.27600218946563787
LOSS train 0.33000900009173456 valid 0.27634509990357947
LOSS train 0.33000900009173456 valid 0.27628225517474997
LOSS train 0.33000900009173456 valid 0.2759941742450249
LOSS train 0.33000900009173456 valid 0.27577360247572263
LOSS train 0.33000900009173456 valid 0.2755050138254796
LOSS train 0.33000900009173456 valid 0.275525766928665
LOSS train 0.33000900009173456 valid 0.27569777512453436
LOSS train 0.33000900009173456 valid 0.27588606325368725
LOSS train 0.33000900009173456 valid 0.2758569589853287
LOSS train 0.33000900009173456 valid 0.2759725201934103
LOSS train 0.33000900009173456 valid 0.27588935008668525
LOSS train 0.33000900009173456 valid 0.27606099855620414
LOSS train 0.33000900009173456 valid 0.27636031598545785
LOSS train 0.33000900009173456 valid 0.27627634280003033
LOSS train 0.33000900009173456 valid 0.2762449614646781
LOSS train 0.33000900009173456 valid 0.2759167670526288
LOSS train 0.33000900009173456 valid 0.2756916601630978
LOSS train 0.33000900009173456 valid 0.2757653784173638
LOSS train 0.33000900009173456 valid 0.2758962466760918
LOSS train 0.33000900009173456 valid 0.2757732406915987
LOSS train 0.33000900009173456 valid 0.2755233066795516
LOSS train 0.33000900009173456 valid 0.27544427285159845
LOSS train 0.33000900009173456 valid 0.2752835424683934
LOSS train 0.33000900009173456 valid 0.27540953819240843
LOSS train 0.33000900009173456 valid 0.27558954049509465
LOSS train 0.33000900009173456 valid 0.2759170324449808
LOSS train 0.33000900009173456 valid 0.2759823632407022
LOSS train 0.33000900009173456 valid 0.2760353936917252
LOSS train 0.33000900009173456 valid 0.27570546598270024
LOSS train 0.33000900009173456 valid 0.27586476504802704
LOSS train 0.33000900009173456 valid 0.27559781348218726
LOSS train 0.33000900009173456 valid 0.2762687390519155
LOSS train 0.33000900009173456 valid 0.2764276795139249
LOSS train 0.33000900009173456 valid 0.2764655147989591
LOSS train 0.33000900009173456 valid 0.276600173076257
LOSS train 0.33000900009173456 valid 0.2763746742551264
LOSS train 0.33000900009173456 valid 0.2765245908031277
LOSS train 0.33000900009173456 valid 0.27653430315194194
LOSS train 0.33000900009173456 valid 0.2765220100841215
LOSS train 0.33000900009173456 valid 0.2767798110651664
LOSS train 0.33000900009173456 valid 0.2767262013664671
LOSS train 0.33000900009173456 valid 0.27687157435884957
LOSS train 0.33000900009173456 valid 0.2767892666395355
LOSS train 0.33000900009173456 valid 0.27664889479056
LOSS train 0.33000900009173456 valid 0.2766330837277892
LOSS train 0.33000900009173456 valid 0.2765728309750557
LOSS train 0.33000900009173456 valid 0.2765429849090752
LOSS train 0.33000900009173456 valid 0.2762933485209942
LOSS train 0.33000900009173456 valid 0.2761933519081636
LOSS train 0.33000900009173456 valid 0.27630196161657933
LOSS train 0.33000900009173456 valid 0.2763597195198436
LOSS train 0.33000900009173456 valid 0.27617029065177556
LOSS train 0.33000900009173456 valid 0.27625627947982245
LOSS train 0.33000900009173456 valid 0.27642345042789684
LOSS train 0.33000900009173456 valid 0.27632426344163236
LOSS train 0.33000900009173456 valid 0.2762060984969139
LOSS train 0.33000900009173456 valid 0.2763173423406017
LOSS train 0.33000900009173456 valid 0.2762816806976823
LOSS train 0.33000900009173456 valid 0.2761400447572981
LOSS train 0.33000900009173456 valid 0.2760317119007761
LOSS train 0.33000900009173456 valid 0.27591649068277435
LOSS train 0.33000900009173456 valid 0.2761046325557687
LOSS train 0.33000900009173456 valid 0.27602385841934374
LOSS train 0.33000900009173456 valid 0.27597429669565626
LOSS train 0.33000900009173456 valid 0.2762182700699864
LOSS train 0.33000900009173456 valid 0.27629891889435904
LOSS train 0.33000900009173456 valid 0.27619516865803245
LOSS train 0.33000900009173456 valid 0.276366401463747
LOSS train 0.33000900009173456 valid 0.27621833730388334
LOSS train 0.33000900009173456 valid 0.2762705390491793
LOSS train 0.33000900009173456 valid 0.2761076376399892
LOSS train 0.33000900009173456 valid 0.2761293164593108
LOSS train 0.33000900009173456 valid 0.2761869179824042
LOSS train 0.33000900009173456 valid 0.27634852936393334
LOSS train 0.33000900009173456 valid 0.2763105646477944
LOSS train 0.33000900009173456 valid 0.2762883515097201
LOSS train 0.33000900009173456 valid 0.2763215936527351
LOSS train 0.33000900009173456 valid 0.2762637321174759
LOSS train 0.33000900009173456 valid 0.27596096449937574
LOSS train 0.33000900009173456 valid 0.2760895315025534
LOSS train 0.33000900009173456 valid 0.2761790305376053
LOSS train 0.33000900009173456 valid 0.27607140313796324
LOSS train 0.33000900009173456 valid 0.2760550699491597
LOSS train 0.33000900009173456 valid 0.27588891617953776
LOSS train 0.33000900009173456 valid 0.2757557198331131
LOSS train 0.33000900009173456 valid 0.27574776149917357
LOSS train 0.33000900009173456 valid 0.2755902507915873
LOSS train 0.33000900009173456 valid 0.275503477045134
LOSS train 0.33000900009173456 valid 0.27533559951840375
LOSS train 0.33000900009173456 valid 0.27532128765455727
LOSS train 0.33000900009173456 valid 0.27526261545897684
LOSS train 0.33000900009173456 valid 0.2751351800532295
LOSS train 0.33000900009173456 valid 0.2749870882222527
LOSS train 0.33000900009173456 valid 0.275015290081501
LOSS train 0.33000900009173456 valid 0.2751294558788363
LOSS train 0.33000900009173456 valid 0.2750386602051978
LOSS train 0.33000900009173456 valid 0.27515294567240234
LOSS train 0.33000900009173456 valid 0.275154109522004
LOSS train 0.33000900009173456 valid 0.27498216927051544
LOSS train 0.33000900009173456 valid 0.27494281385507846
LOSS train 0.33000900009173456 valid 0.2748848140102378
LOSS train 0.33000900009173456 valid 0.2749329313909242
LOSS train 0.33000900009173456 valid 0.27501951607122815
LOSS train 0.33000900009173456 valid 0.2751032823188738
LOSS train 0.33000900009173456 valid 0.27512440119124104
LOSS train 0.33000900009173456 valid 0.27506422318585283
LOSS train 0.33000900009173456 valid 0.27528158665505226
LOSS train 0.33000900009173456 valid 0.27537312431793126
LOSS train 0.33000900009173456 valid 0.2754279962513182
LOSS train 0.33000900009173456 valid 0.2754838558293022
LOSS train 0.33000900009173456 valid 0.2757566167525783
LOSS train 0.33000900009173456 valid 0.27578931007730334
LOSS train 0.33000900009173456 valid 0.27585473116568604
LOSS train 0.33000900009173456 valid 0.276031480340854
LOSS train 0.33000900009173456 valid 0.2759779626543904
LOSS train 0.33000900009173456 valid 0.27602660187102596
LOSS train 0.33000900009173456 valid 0.27597219423418906
LOSS train 0.33000900009173456 valid 0.2759366723844129
LOSS train 0.33000900009173456 valid 0.2759697586297989
LOSS train 0.33000900009173456 valid 0.27586578685095753
LOSS train 0.33000900009173456 valid 0.2759457912751894
LOSS train 0.33000900009173456 valid 0.2758326276886363
LOSS train 0.33000900009173456 valid 0.2756793483405931
LOSS train 0.33000900009173456 valid 0.2756266993160049
LOSS train 0.33000900009173456 valid 0.27570463407831075
LOSS train 0.33000900009173456 valid 0.2754634357065209
LOSS train 0.33000900009173456 valid 0.2755192390072002
LOSS train 0.33000900009173456 valid 0.27559880295493566
LOSS train 0.33000900009173456 valid 0.2758001380428976
LOSS train 0.33000900009173456 valid 0.2757334209433416
LOSS train 0.33000900009173456 valid 0.2757630648038648
LOSS train 0.33000900009173456 valid 0.27573402676611175
LOSS train 0.33000900009173456 valid 0.27559335775164717
LOSS train 0.33000900009173456 valid 0.2757270474433899
LOSS train 0.33000900009173456 valid 0.27568069694051706
LOSS train 0.33000900009173456 valid 0.27561760504567434
LOSS train 0.33000900009173456 valid 0.27549839485068567
LOSS train 0.33000900009173456 valid 0.2755179528997639
LOSS train 0.33000900009173456 valid 0.2756062339918286
LOSS train 0.33000900009173456 valid 0.2756468549487181
LOSS train 0.33000900009173456 valid 0.27549047936261395
LOSS train 0.33000900009173456 valid 0.2755870025518329
LOSS train 0.33000900009173456 valid 0.27552596655131306
LOSS train 0.33000900009173456 valid 0.2755229915563877
LOSS train 0.33000900009173456 valid 0.27562482649339115
LOSS train 0.33000900009173456 valid 0.27557107533207376
LOSS train 0.33000900009173456 valid 0.27553649739620806
LOSS train 0.33000900009173456 valid 0.27548065375198016
LOSS train 0.33000900009173456 valid 0.2754287672492693
LOSS train 0.33000900009173456 valid 0.2754033974238804
LOSS train 0.33000900009173456 valid 0.2755395822533954
LOSS train 0.33000900009173456 valid 0.27567289847491394
LOSS train 0.33000900009173456 valid 0.27579369343346377
LOSS train 0.33000900009173456 valid 0.2758617049014127
LOSS train 0.33000900009173456 valid 0.27595555496391777
LOSS train 0.33000900009173456 valid 0.2761482613928178
LOSS train 0.33000900009173456 valid 0.27612756416474504
LOSS train 0.33000900009173456 valid 0.2760307426652769
LOSS train 0.33000900009173456 valid 0.27601316571235657
LOSS train 0.33000900009173456 valid 0.27593734136958054
LOSS train 0.33000900009173456 valid 0.27581136918455257
LOSS train 0.33000900009173456 valid 0.27563261353283475
LOSS train 0.33000900009173456 valid 0.2756907285114343
LOSS train 0.33000900009173456 valid 0.2755258100905589
LOSS train 0.33000900009173456 valid 0.27540348311123897
LOSS train 0.33000900009173456 valid 0.27530324696860414
LOSS train 0.33000900009173456 valid 0.27528773437751053
LOSS train 0.33000900009173456 valid 0.27527914631744504
LOSS train 0.33000900009173456 valid 0.2753121997703586
LOSS train 0.33000900009173456 valid 0.27522482806360804
LOSS train 0.33000900009173456 valid 0.2750750529329951
LOSS train 0.33000900009173456 valid 0.2751600484156774
LOSS train 0.33000900009173456 valid 0.2751507340933625
LOSS train 0.33000900009173456 valid 0.275233638851807
LOSS train 0.33000900009173456 valid 0.27518730715582873
LOSS train 0.33000900009173456 valid 0.27516110103628405
LOSS train 0.33000900009173456 valid 0.2751024307956468
LOSS train 0.33000900009173456 valid 0.2751779394591747
LOSS train 0.33000900009173456 valid 0.2752056570881504
LOSS train 0.33000900009173456 valid 0.2751789407653583
LOSS train 0.33000900009173456 valid 0.2751921136872937
LOSS train 0.33000900009173456 valid 0.27511027330320154
LOSS train 0.33000900009173456 valid 0.27515142423452743
LOSS train 0.33000900009173456 valid 0.27511408254504205
LOSS train 0.33000900009173456 valid 0.2752598581024975
LOSS train 0.33000900009173456 valid 0.2752653354939246
LOSS train 0.33000900009173456 valid 0.27538035623114493
LOSS train 0.33000900009173456 valid 0.2753465141806948
LOSS train 0.33000900009173456 valid 0.2752055204794055
LOSS train 0.33000900009173456 valid 0.27520013298668894
LOSS train 0.33000900009173456 valid 0.2752269067185709
LOSS train 0.33000900009173456 valid 0.2751719844708969
LOSS train 0.33000900009173456 valid 0.27523333601002553
LOSS train 0.33000900009173456 valid 0.2751815331078345
LOSS train 0.33000900009173456 valid 0.2750887831499339
LOSS train 0.33000900009173456 valid 0.2751832256714503
LOSS train 0.33000900009173456 valid 0.27519998182884803
LOSS train 0.33000900009173456 valid 0.2753199573345245
LOSS train 0.33000900009173456 valid 0.2753263920072525
LOSS train 0.33000900009173456 valid 0.2752938441272023
LOSS train 0.33000900009173456 valid 0.27535370932392517
LOSS train 0.33000900009173456 valid 0.27543205425799266
LOSS train 0.33000900009173456 valid 0.2755165073938878
LOSS train 0.33000900009173456 valid 0.27541605243459344
LOSS train 0.33000900009173456 valid 0.2755375938066441
LOSS train 0.33000900009173456 valid 0.2754919474354442
LOSS train 0.33000900009173456 valid 0.275469173680149
LOSS train 0.33000900009173456 valid 0.27546967455634364
LOSS train 0.33000900009173456 valid 0.2755309121425335
LOSS train 0.33000900009173456 valid 0.2756496616667765
LOSS train 0.33000900009173456 valid 0.27569625583628266
LOSS train 0.33000900009173456 valid 0.2756736908198857
LOSS train 0.33000900009173456 valid 0.27585296183371616
LOSS train 0.33000900009173456 valid 0.27580894204703243
LOSS train 0.33000900009173456 valid 0.2757293962369873
LOSS train 0.33000900009173456 valid 0.27561390440327577
LOSS train 0.33000900009173456 valid 0.27561308882078966
LOSS train 0.33000900009173456 valid 0.2757597573235363
LOSS train 0.33000900009173456 valid 0.27581620736798246
LOSS train 0.33000900009173456 valid 0.2758261678917777
LOSS train 0.33000900009173456 valid 0.2758481318147671
LOSS train 0.33000900009173456 valid 0.2757997746033781
LOSS train 0.33000900009173456 valid 0.2757948233930059
LOSS train 0.33000900009173456 valid 0.2757820051820839
LOSS train 0.33000900009173456 valid 0.2757598822854481
LOSS train 0.33000900009173456 valid 0.27579288999413887
LOSS train 0.33000900009173456 valid 0.27579186257224736
LOSS train 0.33000900009173456 valid 0.2759447462943404
LOSS train 0.33000900009173456 valid 0.2759949688894161
LOSS train 0.33000900009173456 valid 0.2759190153955035
LOSS train 0.33000900009173456 valid 0.2758452269203038
LOSS train 0.33000900009173456 valid 0.27575174817579917
LOSS train 0.33000900009173456 valid 0.2758103328567521
LOSS train 0.33000900009173456 valid 0.27576396401439396
LOSS train 0.33000900009173456 valid 0.2756851475547861
LOSS train 0.33000900009173456 valid 0.2757004290734502
LOSS train 0.33000900009173456 valid 0.2756740814312322
LOSS train 0.33000900009173456 valid 0.27565627620886946
LOSS train 0.33000900009173456 valid 0.2757666129881228
LOSS train 0.33000900009173456 valid 0.27581441155477854
LOSS train 0.33000900009173456 valid 0.27572479794005383
LOSS train 0.33000900009173456 valid 0.27569140210830967
LOSS train 0.33000900009173456 valid 0.2757109440136753
LOSS train 0.33000900009173456 valid 0.2756857523487674
LOSS train 0.33000900009173456 valid 0.2756339598065268
LOSS train 0.33000900009173456 valid 0.27571134985481177
LOSS train 0.33000900009173456 valid 0.2755476131255305
LOSS train 0.33000900009173456 valid 0.27554554836108136
LOSS train 0.33000900009173456 valid 0.27554014463947246
LOSS train 0.33000900009173456 valid 0.27550293106198964
LOSS train 0.33000900009173456 valid 0.27534108225266357
LOSS train 0.33000900009173456 valid 0.2752850820188937
LOSS train 0.33000900009173456 valid 0.2753980555508518
EPOCH 4:
  batch 1 loss: 0.31989678740501404
  batch 2 loss: 0.30765992403030396
  batch 3 loss: 0.31144752105077106
  batch 4 loss: 0.31309352815151215
  batch 5 loss: 0.3216838240623474
  batch 6 loss: 0.32416675488154095
  batch 7 loss: 0.32556713478905813
  batch 8 loss: 0.32589486241340637
  batch 9 loss: 0.3270784980720944
  batch 10 loss: 0.3288172483444214
  batch 11 loss: 0.3270231214436618
  batch 12 loss: 0.32404878238836926
  batch 13 loss: 0.32217299708953273
  batch 14 loss: 0.3227275013923645
  batch 15 loss: 0.32230295340220133
  batch 16 loss: 0.3234109617769718
  batch 17 loss: 0.32075683158986706
  batch 18 loss: 0.32288086745474076
  batch 19 loss: 0.32232219137643514
  batch 20 loss: 0.320423124730587
  batch 21 loss: 0.3201929231484731
  batch 22 loss: 0.31956657631830737
  batch 23 loss: 0.31947370845338574
  batch 24 loss: 0.3182424393792947
  batch 25 loss: 0.31979257822036744
  batch 26 loss: 0.3183973076251837
  batch 27 loss: 0.3187542899891182
  batch 28 loss: 0.3162602883364473
  batch 29 loss: 0.31756720512077724
  batch 30 loss: 0.31659204413493475
  batch 31 loss: 0.3181635415361774
  batch 32 loss: 0.3173150303773582
  batch 33 loss: 0.31758134879849176
  batch 34 loss: 0.31768857337096157
  batch 35 loss: 0.3176346331834793
  batch 36 loss: 0.3178275273078018
  batch 37 loss: 0.3193862265026247
  batch 38 loss: 0.31942791334892573
  batch 39 loss: 0.31970245372026396
  batch 40 loss: 0.31924945004284383
  batch 41 loss: 0.31862354169531565
  batch 42 loss: 0.3191277026420548
  batch 43 loss: 0.31941396725732224
  batch 44 loss: 0.3195965699851513
  batch 45 loss: 0.31924495332770875
  batch 46 loss: 0.31906718048064603
  batch 47 loss: 0.3193543531159137
  batch 48 loss: 0.31825926930954057
  batch 49 loss: 0.3176527920426155
  batch 50 loss: 0.3171705946326256
  batch 51 loss: 0.3164915188270457
  batch 52 loss: 0.3164310624393133
  batch 53 loss: 0.31606567890014287
  batch 54 loss: 0.31663301835457486
  batch 55 loss: 0.3164723301475698
  batch 56 loss: 0.3164823004709823
  batch 57 loss: 0.31590600991458223
  batch 58 loss: 0.3165962950422846
  batch 59 loss: 0.31659076703807054
  batch 60 loss: 0.3162704067925612
  batch 61 loss: 0.3167002306121295
  batch 62 loss: 0.3168211860522147
  batch 63 loss: 0.316787578047268
  batch 64 loss: 0.317405246431008
  batch 65 loss: 0.31693771000091847
  batch 66 loss: 0.3168822384693406
  batch 67 loss: 0.3172807468852
  batch 68 loss: 0.3180561295765288
  batch 69 loss: 0.3177499004464219
  batch 70 loss: 0.3178854450583458
  batch 71 loss: 0.31769313421887413
  batch 72 loss: 0.3177262352158626
  batch 73 loss: 0.3177918234508332
  batch 74 loss: 0.3179828611177367
  batch 75 loss: 0.31749288260936737
  batch 76 loss: 0.3181508130540973
  batch 77 loss: 0.3179875192317096
  batch 78 loss: 0.31780418543479383
  batch 79 loss: 0.3183719163076787
  batch 80 loss: 0.318170084990561
  batch 81 loss: 0.3184296876927953
  batch 82 loss: 0.31894754727439184
  batch 83 loss: 0.31898803721709423
  batch 84 loss: 0.3187282747101216
  batch 85 loss: 0.31856550872325895
  batch 86 loss: 0.318930251480535
  batch 87 loss: 0.3189997500044176
  batch 88 loss: 0.3186439602551135
  batch 89 loss: 0.318448151597816
  batch 90 loss: 0.31843779534101485
  batch 91 loss: 0.31861595252713004
  batch 92 loss: 0.3186037551773631
  batch 93 loss: 0.3185814723212232
  batch 94 loss: 0.318906326084695
  batch 95 loss: 0.31862346482904336
  batch 96 loss: 0.31870176969096065
  batch 97 loss: 0.31941673605097937
  batch 98 loss: 0.3195790692555661
  batch 99 loss: 0.3195366565928315
  batch 100 loss: 0.3193678642809391
  batch 101 loss: 0.31924893493109413
  batch 102 loss: 0.31938742261891273
  batch 103 loss: 0.31984594131557686
  batch 104 loss: 0.31977746965220344
  batch 105 loss: 0.31941127479076387
  batch 106 loss: 0.3196383072239048
  batch 107 loss: 0.3191156853860784
  batch 108 loss: 0.31898133608478085
  batch 109 loss: 0.3187621879741686
  batch 110 loss: 0.31881166086955504
  batch 111 loss: 0.31871901855275436
  batch 112 loss: 0.3183022249223931
  batch 113 loss: 0.3187075743896771
  batch 114 loss: 0.31897177573358804
  batch 115 loss: 0.31897075111451356
  batch 116 loss: 0.3190752913982704
  batch 117 loss: 0.31928667795454335
  batch 118 loss: 0.318919039757575
  batch 119 loss: 0.3191495473895754
  batch 120 loss: 0.31905035364131135
  batch 121 loss: 0.31895758802733143
  batch 122 loss: 0.3186767727869456
  batch 123 loss: 0.31862622390433054
  batch 124 loss: 0.31884666568329256
  batch 125 loss: 0.3189847589731216
  batch 126 loss: 0.31899568615924745
  batch 127 loss: 0.3195864099451876
  batch 128 loss: 0.31937392347026616
  batch 129 loss: 0.31947494673636534
  batch 130 loss: 0.31936183652052513
  batch 131 loss: 0.3195712785911924
  batch 132 loss: 0.31949969573002873
  batch 133 loss: 0.3196242062893129
  batch 134 loss: 0.3196571261357905
  batch 135 loss: 0.31960055927435554
  batch 136 loss: 0.3195690476499936
  batch 137 loss: 0.31953007080694185
  batch 138 loss: 0.3194308526058128
  batch 139 loss: 0.319862643484589
  batch 140 loss: 0.3197381416601794
  batch 141 loss: 0.3198742816845576
  batch 142 loss: 0.3198300993568461
  batch 143 loss: 0.319586284823351
  batch 144 loss: 0.3194468432209558
  batch 145 loss: 0.3193450809552752
  batch 146 loss: 0.319256981043783
  batch 147 loss: 0.3195364131611221
  batch 148 loss: 0.31955665198935046
  batch 149 loss: 0.3194943469442777
  batch 150 loss: 0.31948799043893816
  batch 151 loss: 0.31936333264341416
  batch 152 loss: 0.319446080119202
  batch 153 loss: 0.3192569571577646
  batch 154 loss: 0.31931119179957873
  batch 155 loss: 0.3194478916545068
  batch 156 loss: 0.3193483726145365
  batch 157 loss: 0.3195369702046085
  batch 158 loss: 0.31957155192577386
  batch 159 loss: 0.31964153491850916
  batch 160 loss: 0.3194675541482866
  batch 161 loss: 0.3196068027560015
  batch 162 loss: 0.3194990177397375
  batch 163 loss: 0.31931559778064306
  batch 164 loss: 0.3194145633861786
  batch 165 loss: 0.3192796932025389
  batch 166 loss: 0.31920499947056713
  batch 167 loss: 0.31891856819926623
  batch 168 loss: 0.3189083413176593
  batch 169 loss: 0.31859412198588694
  batch 170 loss: 0.31848326158874174
  batch 171 loss: 0.31837887783148133
  batch 172 loss: 0.31838503921794337
  batch 173 loss: 0.3184399907127281
  batch 174 loss: 0.3184743034599841
  batch 175 loss: 0.31859707040446145
  batch 176 loss: 0.3183097005398436
  batch 177 loss: 0.31822579013089003
  batch 178 loss: 0.3182325292001949
  batch 179 loss: 0.31823539875406126
  batch 180 loss: 0.3180926661524508
  batch 181 loss: 0.31798800340344235
  batch 182 loss: 0.3179873004555702
  batch 183 loss: 0.3178572780936142
  batch 184 loss: 0.3176618873587121
  batch 185 loss: 0.3176603040985159
  batch 186 loss: 0.3178584573249663
  batch 187 loss: 0.3178211503965969
  batch 188 loss: 0.31760404996098357
  batch 189 loss: 0.31738889524860986
  batch 190 loss: 0.31737274990270015
  batch 191 loss: 0.31728657338943783
  batch 192 loss: 0.31731605242627364
  batch 193 loss: 0.3173027568993791
  batch 194 loss: 0.317507046937328
  batch 195 loss: 0.31764940932775154
  batch 196 loss: 0.3175411466889235
  batch 197 loss: 0.31743606569500743
  batch 198 loss: 0.3176560664568285
  batch 199 loss: 0.3177744457919394
  batch 200 loss: 0.31783050902187826
  batch 201 loss: 0.31791809179
  batch 202 loss: 0.3178786687774233
  batch 203 loss: 0.3179427967429748
  batch 204 loss: 0.3177625745096627
  batch 205 loss: 0.31787163651571043
  batch 206 loss: 0.31796193752184654
  batch 207 loss: 0.31800226867198944
  batch 208 loss: 0.31786224167220867
  batch 209 loss: 0.31764496549179677
  batch 210 loss: 0.31770574755611874
  batch 211 loss: 0.3176561583416157
  batch 212 loss: 0.31764000070826065
  batch 213 loss: 0.31773638284542194
  batch 214 loss: 0.3177517655993176
  batch 215 loss: 0.31745160270568934
  batch 216 loss: 0.3173017194839539
  batch 217 loss: 0.31735894063376063
  batch 218 loss: 0.3173335996379546
  batch 219 loss: 0.31728261811276004
  batch 220 loss: 0.31724915470589293
  batch 221 loss: 0.31740338682319247
  batch 222 loss: 0.31750407902238603
  batch 223 loss: 0.3175141074331352
  batch 224 loss: 0.317516763095877
  batch 225 loss: 0.3173363972372479
  batch 226 loss: 0.31747143623311963
  batch 227 loss: 0.31726274846146285
  batch 228 loss: 0.31723548883670255
  batch 229 loss: 0.3170980794851436
  batch 230 loss: 0.31716700682173604
  batch 231 loss: 0.31716367092741515
  batch 232 loss: 0.31702028581037606
  batch 233 loss: 0.3170376489730352
  batch 234 loss: 0.31711340670147514
  batch 235 loss: 0.3172054462610407
  batch 236 loss: 0.3171067251113512
  batch 237 loss: 0.31719171818550124
  batch 238 loss: 0.3170962267938782
  batch 239 loss: 0.31715379879813815
  batch 240 loss: 0.3172625202064713
  batch 241 loss: 0.3173253629588487
  batch 242 loss: 0.3171315095271946
  batch 243 loss: 0.31724262501231926
  batch 244 loss: 0.3171471579030889
  batch 245 loss: 0.31717967810679454
  batch 246 loss: 0.31712607560845896
  batch 247 loss: 0.3171129653207686
  batch 248 loss: 0.3170496916939174
  batch 249 loss: 0.31701777025638334
  batch 250 loss: 0.31693177312612536
  batch 251 loss: 0.3169386228360978
  batch 252 loss: 0.31678553688384237
  batch 253 loss: 0.3166185582697156
  batch 254 loss: 0.3165478526255277
  batch 255 loss: 0.31657106391355105
  batch 256 loss: 0.3165285204886459
  batch 257 loss: 0.3164687087679651
  batch 258 loss: 0.31646265072184937
  batch 259 loss: 0.31641447124103783
  batch 260 loss: 0.3163483946942366
  batch 261 loss: 0.3164175741403039
  batch 262 loss: 0.31627358706625364
  batch 263 loss: 0.31627036370478656
  batch 264 loss: 0.31614930572157557
  batch 265 loss: 0.3160636803451574
  batch 266 loss: 0.3159890858862633
  batch 267 loss: 0.31607947317178775
  batch 268 loss: 0.31602970449559725
  batch 269 loss: 0.31598355429987923
  batch 270 loss: 0.3161128772077737
  batch 271 loss: 0.31616970275380957
  batch 272 loss: 0.316232235828305
  batch 273 loss: 0.3161616181825107
  batch 274 loss: 0.3162996741215678
  batch 275 loss: 0.3162822869149121
  batch 276 loss: 0.3162066899240017
  batch 277 loss: 0.3161416412583327
  batch 278 loss: 0.31604057695153803
  batch 279 loss: 0.3160978484645112
  batch 280 loss: 0.3159276553030525
  batch 281 loss: 0.3158125751701538
  batch 282 loss: 0.31584365364718947
  batch 283 loss: 0.31572698362092666
  batch 284 loss: 0.3157348061426425
  batch 285 loss: 0.315666845336295
  batch 286 loss: 0.31568836316570537
  batch 287 loss: 0.31577814222420547
  batch 288 loss: 0.3156422274704609
  batch 289 loss: 0.3157932765034243
  batch 290 loss: 0.315650647416197
  batch 291 loss: 0.31562940704658676
  batch 292 loss: 0.31564980883100263
  batch 293 loss: 0.31556088093401224
  batch 294 loss: 0.3154218958652749
  batch 295 loss: 0.3154106214389963
  batch 296 loss: 0.31551616774821606
  batch 297 loss: 0.3154931449227863
  batch 298 loss: 0.31547972814948766
  batch 299 loss: 0.31545791902071657
  batch 300 loss: 0.3154298497736454
  batch 301 loss: 0.3153397149322833
  batch 302 loss: 0.3154692483560139
  batch 303 loss: 0.31544487365204904
  batch 304 loss: 0.31533922328564684
  batch 305 loss: 0.31522685821916235
  batch 306 loss: 0.31530383682134105
  batch 307 loss: 0.31528699111666664
  batch 308 loss: 0.3153309679360359
  batch 309 loss: 0.3152500101181296
  batch 310 loss: 0.31519979041430257
  batch 311 loss: 0.31536781457267776
  batch 312 loss: 0.3153860890425933
  batch 313 loss: 0.3154077073350882
  batch 314 loss: 0.31537552903981725
  batch 315 loss: 0.3153383194927185
  batch 316 loss: 0.31521436010923565
  batch 317 loss: 0.31529023749783214
  batch 318 loss: 0.3152194883932108
  batch 319 loss: 0.3151272907619566
  batch 320 loss: 0.31512291585095226
  batch 321 loss: 0.3151559305618114
  batch 322 loss: 0.31517327771238657
  batch 323 loss: 0.31516228450156586
  batch 324 loss: 0.3149312924547696
  batch 325 loss: 0.3149054083915857
  batch 326 loss: 0.31493976972768645
  batch 327 loss: 0.31494985776027773
  batch 328 loss: 0.31477257632082556
  batch 329 loss: 0.31493343530636064
  batch 330 loss: 0.31492604075959235
  batch 331 loss: 0.3149085012897624
  batch 332 loss: 0.31488437120275325
  batch 333 loss: 0.3148479056698424
  batch 334 loss: 0.314773126767424
  batch 335 loss: 0.3145559314916383
  batch 336 loss: 0.31445908710537923
  batch 337 loss: 0.3143522968012784
  batch 338 loss: 0.3143298002125243
  batch 339 loss: 0.3142993995661581
  batch 340 loss: 0.3142929869101328
  batch 341 loss: 0.3141922056237973
  batch 342 loss: 0.3141331138056621
  batch 343 loss: 0.31414604521527584
  batch 344 loss: 0.3140884289128143
  batch 345 loss: 0.3141329525605492
  batch 346 loss: 0.31398396327488687
  batch 347 loss: 0.31403612416312743
  batch 348 loss: 0.3139778729742286
  batch 349 loss: 0.31386444660005053
  batch 350 loss: 0.3139287125638553
  batch 351 loss: 0.3138891338368087
  batch 352 loss: 0.3139911361292682
  batch 353 loss: 0.31399942022713995
  batch 354 loss: 0.31417272105223715
  batch 355 loss: 0.3141881156555364
  batch 356 loss: 0.31413908089312276
  batch 357 loss: 0.314028938987008
  batch 358 loss: 0.3141028805734725
  batch 359 loss: 0.3140802427005635
  batch 360 loss: 0.31407624917725724
  batch 361 loss: 0.31400266120473436
  batch 362 loss: 0.31398193624632137
  batch 363 loss: 0.3138965529597495
  batch 364 loss: 0.31382745009038476
  batch 365 loss: 0.313880811203016
  batch 366 loss: 0.31376521790125333
  batch 367 loss: 0.31363821366664824
  batch 368 loss: 0.3135289182562543
  batch 369 loss: 0.3135014620980596
  batch 370 loss: 0.31343232268417204
  batch 371 loss: 0.31344577433124704
  batch 372 loss: 0.31341159491929954
  batch 373 loss: 0.3132974767093684
  batch 374 loss: 0.3131507062577309
  batch 375 loss: 0.3130624684095383
  batch 376 loss: 0.3131531068381477
  batch 377 loss: 0.31313456120636485
  batch 378 loss: 0.31300974889564764
  batch 379 loss: 0.3130105467695045
  batch 380 loss: 0.3130411331982989
  batch 381 loss: 0.31295016435344075
  batch 382 loss: 0.3128541771576043
  batch 383 loss: 0.31283570833834906
  batch 384 loss: 0.31276477962577093
  batch 385 loss: 0.3128286782410238
  batch 386 loss: 0.31273108006604594
  batch 387 loss: 0.31278273320783323
  batch 388 loss: 0.31278001835819375
  batch 389 loss: 0.3127998928407777
  batch 390 loss: 0.31274373879035317
  batch 391 loss: 0.31283577906963467
  batch 392 loss: 0.31287451244282477
  batch 393 loss: 0.3129886119192793
  batch 394 loss: 0.3130008572627445
  batch 395 loss: 0.31295716079730024
  batch 396 loss: 0.31299746476791124
  batch 397 loss: 0.31300279614456955
  batch 398 loss: 0.31294031834332786
  batch 399 loss: 0.3129431365202543
  batch 400 loss: 0.3129511771723628
  batch 401 loss: 0.31282991397559196
  batch 402 loss: 0.3128570330454342
  batch 403 loss: 0.31287486340952275
  batch 404 loss: 0.3128551202319046
  batch 405 loss: 0.3128902713825673
  batch 406 loss: 0.31293826190562085
  batch 407 loss: 0.31289713370272804
  batch 408 loss: 0.3129528024982588
  batch 409 loss: 0.31291353633322166
  batch 410 loss: 0.3129744239333199
  batch 411 loss: 0.3129533519492532
  batch 412 loss: 0.3128919675584557
  batch 413 loss: 0.3129717013809929
  batch 414 loss: 0.31292603805589214
  batch 415 loss: 0.31289127156677016
  batch 416 loss: 0.31290527114358085
  batch 417 loss: 0.31294191744830685
  batch 418 loss: 0.3129494495725518
  batch 419 loss: 0.3129318181896551
  batch 420 loss: 0.3129057477982271
  batch 421 loss: 0.3128419560590436
  batch 422 loss: 0.3129292126586087
  batch 423 loss: 0.3129753334091065
  batch 424 loss: 0.3129133453135783
  batch 425 loss: 0.31291445420068853
  batch 426 loss: 0.31286564025100966
  batch 427 loss: 0.31285784196379035
  batch 428 loss: 0.3127932007137303
  batch 429 loss: 0.3127474976933642
  batch 430 loss: 0.31265620474898537
  batch 431 loss: 0.3126457270986796
  batch 432 loss: 0.3127166577442377
  batch 433 loss: 0.31272425631590306
  batch 434 loss: 0.31283719175010233
  batch 435 loss: 0.31277518823914147
  batch 436 loss: 0.31278954818844795
  batch 437 loss: 0.31288745484444863
  batch 438 loss: 0.31299451527666283
  batch 439 loss: 0.31298405349526154
  batch 440 loss: 0.31307726465165614
  batch 441 loss: 0.3130786797019089
  batch 442 loss: 0.3131148557250316
  batch 443 loss: 0.31306792069100364
  batch 444 loss: 0.3129655941985212
  batch 445 loss: 0.3129587492581164
  batch 446 loss: 0.3128997026337102
  batch 447 loss: 0.3128002906339014
  batch 448 loss: 0.3128504563854741
  batch 449 loss: 0.31293205956706494
  batch 450 loss: 0.31290309412611855
  batch 451 loss: 0.31290569805649593
  batch 452 loss: 0.3128623555917128
  batch 453 loss: 0.31295492472106523
  batch 454 loss: 0.3130291048847631
  batch 455 loss: 0.31300516734411427
  batch 456 loss: 0.31300642815206137
  batch 457 loss: 0.3130254068898946
  batch 458 loss: 0.31300135526204215
  batch 459 loss: 0.31295235649822584
  batch 460 loss: 0.3130258581236653
  batch 461 loss: 0.31296720037362064
  batch 462 loss: 0.31295300371848145
  batch 463 loss: 0.31283820966021286
  batch 464 loss: 0.31289068535612574
  batch 465 loss: 0.312828591401859
  batch 466 loss: 0.31266592139210314
  batch 467 loss: 0.3128441876913291
  batch 468 loss: 0.31281118461082125
  batch 469 loss: 0.3129121544582249
  batch 470 loss: 0.3129524033437384
  batch 471 loss: 0.31294560302721214
  batch 472 loss: 0.31281933892455144
LOSS train 0.31281933892455144 valid 0.2527109682559967
LOSS train 0.31281933892455144 valid 0.24518994241952896
LOSS train 0.31281933892455144 valid 0.24842941264311472
LOSS train 0.31281933892455144 valid 0.2386978678405285
LOSS train 0.31281933892455144 valid 0.2329331547021866
LOSS train 0.31281933892455144 valid 0.23559518406788507
LOSS train 0.31281933892455144 valid 0.24604048260620662
LOSS train 0.31281933892455144 valid 0.24119537882506847
LOSS train 0.31281933892455144 valid 0.24140661623742846
LOSS train 0.31281933892455144 valid 0.2436504542827606
LOSS train 0.31281933892455144 valid 0.2411699972369454
LOSS train 0.31281933892455144 valid 0.24200976639986038
LOSS train 0.31281933892455144 valid 0.24078638163896707
LOSS train 0.31281933892455144 valid 0.2397878776703562
LOSS train 0.31281933892455144 valid 0.23518235683441163
LOSS train 0.31281933892455144 valid 0.236064238473773
LOSS train 0.31281933892455144 valid 0.23704920095555923
LOSS train 0.31281933892455144 valid 0.23794635136922201
LOSS train 0.31281933892455144 valid 0.23884894973353335
LOSS train 0.31281933892455144 valid 0.23838648274540902
LOSS train 0.31281933892455144 valid 0.2377568618172691
LOSS train 0.31281933892455144 valid 0.23662396317178552
LOSS train 0.31281933892455144 valid 0.23686673135861105
LOSS train 0.31281933892455144 valid 0.23612911502520242
LOSS train 0.31281933892455144 valid 0.23404442846775056
LOSS train 0.31281933892455144 valid 0.23394257575273514
LOSS train 0.31281933892455144 valid 0.23400111165311602
LOSS train 0.31281933892455144 valid 0.23470606388790266
LOSS train 0.31281933892455144 valid 0.23503332816321273
LOSS train 0.31281933892455144 valid 0.23550375004609425
LOSS train 0.31281933892455144 valid 0.23656199151469814
LOSS train 0.31281933892455144 valid 0.2365715098567307
LOSS train 0.31281933892455144 valid 0.23766417620760022
LOSS train 0.31281933892455144 valid 0.23768827056183534
LOSS train 0.31281933892455144 valid 0.23913747923714773
LOSS train 0.31281933892455144 valid 0.2386717903945181
LOSS train 0.31281933892455144 valid 0.23917326170044975
LOSS train 0.31281933892455144 valid 0.24019598411886314
LOSS train 0.31281933892455144 valid 0.2396714721734707
LOSS train 0.31281933892455144 valid 0.2400784607976675
LOSS train 0.31281933892455144 valid 0.24048358747145024
LOSS train 0.31281933892455144 valid 0.2404689554657255
LOSS train 0.31281933892455144 valid 0.2400222774161849
LOSS train 0.31281933892455144 valid 0.24022849547592076
LOSS train 0.31281933892455144 valid 0.23953579697344038
LOSS train 0.31281933892455144 valid 0.23985859071430954
LOSS train 0.31281933892455144 valid 0.23997634046889366
LOSS train 0.31281933892455144 valid 0.2397426605845491
LOSS train 0.31281933892455144 valid 0.240239822438785
LOSS train 0.31281933892455144 valid 0.23974651366472244
LOSS train 0.31281933892455144 valid 0.2399761790738386
LOSS train 0.31281933892455144 valid 0.2399443703202101
LOSS train 0.31281933892455144 valid 0.24000493647917262
LOSS train 0.31281933892455144 valid 0.2400932455504382
LOSS train 0.31281933892455144 valid 0.24008646471933884
LOSS train 0.31281933892455144 valid 0.24013625990067208
LOSS train 0.31281933892455144 valid 0.23986359019028514
LOSS train 0.31281933892455144 valid 0.23953112625870213
LOSS train 0.31281933892455144 valid 0.24003790874602432
LOSS train 0.31281933892455144 valid 0.2396413005888462
LOSS train 0.31281933892455144 valid 0.23942209389366087
LOSS train 0.31281933892455144 valid 0.23999612009333027
LOSS train 0.31281933892455144 valid 0.24020476024302226
LOSS train 0.31281933892455144 valid 0.24103017593733966
LOSS train 0.31281933892455144 valid 0.24112550616264344
LOSS train 0.31281933892455144 valid 0.24119245193221353
LOSS train 0.31281933892455144 valid 0.2404714480264863
LOSS train 0.31281933892455144 valid 0.24052489285959916
LOSS train 0.31281933892455144 valid 0.23963807026545206
LOSS train 0.31281933892455144 valid 0.23959237975733622
LOSS train 0.31281933892455144 valid 0.23923409551801816
LOSS train 0.31281933892455144 valid 0.2393356559591161
LOSS train 0.31281933892455144 valid 0.2391225366967998
LOSS train 0.31281933892455144 valid 0.23877841294617266
LOSS train 0.31281933892455144 valid 0.23874338587125143
LOSS train 0.31281933892455144 valid 0.23910398624445262
LOSS train 0.31281933892455144 valid 0.23902350741547423
LOSS train 0.31281933892455144 valid 0.23907994669981492
LOSS train 0.31281933892455144 valid 0.2393472751107397
LOSS train 0.31281933892455144 valid 0.23867980260401964
LOSS train 0.31281933892455144 valid 0.2377329844015616
LOSS train 0.31281933892455144 valid 0.23775638894336978
LOSS train 0.31281933892455144 valid 0.2374278683978391
LOSS train 0.31281933892455144 valid 0.23728138545439356
LOSS train 0.31281933892455144 valid 0.2367312820518718
LOSS train 0.31281933892455144 valid 0.2359699717787809
LOSS train 0.31281933892455144 valid 0.23606211803425317
LOSS train 0.31281933892455144 valid 0.23576938140798698
LOSS train 0.31281933892455144 valid 0.23601697854111703
LOSS train 0.31281933892455144 valid 0.2362932504879104
LOSS train 0.31281933892455144 valid 0.23642000212119177
LOSS train 0.31281933892455144 valid 0.23645528162951054
LOSS train 0.31281933892455144 valid 0.236367229011751
LOSS train 0.31281933892455144 valid 0.2367073042278594
LOSS train 0.31281933892455144 valid 0.23636198216363002
LOSS train 0.31281933892455144 valid 0.23635545295352736
LOSS train 0.31281933892455144 valid 0.23647956427225134
LOSS train 0.31281933892455144 valid 0.23669501789370362
LOSS train 0.31281933892455144 valid 0.23681042606782432
LOSS train 0.31281933892455144 valid 0.23707474753260613
LOSS train 0.31281933892455144 valid 0.237327710089117
LOSS train 0.31281933892455144 valid 0.23754866348177778
LOSS train 0.31281933892455144 valid 0.2372912861479139
LOSS train 0.31281933892455144 valid 0.23712710506067827
LOSS train 0.31281933892455144 valid 0.23749263357548486
LOSS train 0.31281933892455144 valid 0.23772148942610002
LOSS train 0.31281933892455144 valid 0.2373451898309672
LOSS train 0.31281933892455144 valid 0.2374315304612672
LOSS train 0.31281933892455144 valid 0.23796825220278645
LOSS train 0.31281933892455144 valid 0.23814498443495144
LOSS train 0.31281933892455144 valid 0.23777873236853797
LOSS train 0.31281933892455144 valid 0.23767949427877152
LOSS train 0.31281933892455144 valid 0.23781794916212032
LOSS train 0.31281933892455144 valid 0.23770330950879215
LOSS train 0.31281933892455144 valid 0.237741630880729
LOSS train 0.31281933892455144 valid 0.2380796155795969
LOSS train 0.31281933892455144 valid 0.23835123769747904
LOSS train 0.31281933892455144 valid 0.23829940605466648
LOSS train 0.31281933892455144 valid 0.23804102024110427
LOSS train 0.31281933892455144 valid 0.23781235938270887
LOSS train 0.31281933892455144 valid 0.23758676795920064
LOSS train 0.31281933892455144 valid 0.23755484233137036
LOSS train 0.31281933892455144 valid 0.23772491526797535
LOSS train 0.31281933892455144 valid 0.23782763582083485
LOSS train 0.31281933892455144 valid 0.23781399381160737
LOSS train 0.31281933892455144 valid 0.23793209273190724
LOSS train 0.31281933892455144 valid 0.23790505516716814
LOSS train 0.31281933892455144 valid 0.23805964493658394
LOSS train 0.31281933892455144 valid 0.23838361162085867
LOSS train 0.31281933892455144 valid 0.23828288110402915
LOSS train 0.31281933892455144 valid 0.2382642878148392
LOSS train 0.31281933892455144 valid 0.23799233025673663
LOSS train 0.31281933892455144 valid 0.23781248781465947
LOSS train 0.31281933892455144 valid 0.23783982567377945
LOSS train 0.31281933892455144 valid 0.23790742810125703
LOSS train 0.31281933892455144 valid 0.23778076776686838
LOSS train 0.31281933892455144 valid 0.2375488436787668
LOSS train 0.31281933892455144 valid 0.23748332175655643
LOSS train 0.31281933892455144 valid 0.23729105283030502
LOSS train 0.31281933892455144 valid 0.23739960704530988
LOSS train 0.31281933892455144 valid 0.2375866136652358
LOSS train 0.31281933892455144 valid 0.2378553611711717
LOSS train 0.31281933892455144 valid 0.2379746889317786
LOSS train 0.31281933892455144 valid 0.23798210815423065
LOSS train 0.31281933892455144 valid 0.23767066227978673
LOSS train 0.31281933892455144 valid 0.23787433745926373
LOSS train 0.31281933892455144 valid 0.23765595875629764
LOSS train 0.31281933892455144 valid 0.23824532048122302
LOSS train 0.31281933892455144 valid 0.23839822551548082
LOSS train 0.31281933892455144 valid 0.238381416896979
LOSS train 0.31281933892455144 valid 0.23846652442651078
LOSS train 0.31281933892455144 valid 0.23821253907915793
LOSS train 0.31281933892455144 valid 0.2383435239004933
LOSS train 0.31281933892455144 valid 0.23838183274129768
LOSS train 0.31281933892455144 valid 0.23834978620852193
LOSS train 0.31281933892455144 valid 0.23856367505131623
LOSS train 0.31281933892455144 valid 0.23850499283356272
LOSS train 0.31281933892455144 valid 0.23865502176782752
LOSS train 0.31281933892455144 valid 0.23863192596150645
LOSS train 0.31281933892455144 valid 0.2384702941402793
LOSS train 0.31281933892455144 valid 0.23844248843119012
LOSS train 0.31281933892455144 valid 0.23837820495720263
LOSS train 0.31281933892455144 valid 0.23835106258012034
LOSS train 0.31281933892455144 valid 0.23811145199508202
LOSS train 0.31281933892455144 valid 0.23800764354792509
LOSS train 0.31281933892455144 valid 0.23808448914303837
LOSS train 0.31281933892455144 valid 0.23815035177562052
LOSS train 0.31281933892455144 valid 0.2379833652327458
LOSS train 0.31281933892455144 valid 0.23806780514985146
LOSS train 0.31281933892455144 valid 0.23826398963437362
LOSS train 0.31281933892455144 valid 0.23815538477130802
LOSS train 0.31281933892455144 valid 0.23805342536679533
LOSS train 0.31281933892455144 valid 0.23811860735705823
LOSS train 0.31281933892455144 valid 0.23809550753955183
LOSS train 0.31281933892455144 valid 0.2379720781530653
LOSS train 0.31281933892455144 valid 0.23790923116559332
LOSS train 0.31281933892455144 valid 0.23779759720220403
LOSS train 0.31281933892455144 valid 0.23795375512557082
LOSS train 0.31281933892455144 valid 0.23788206787082736
LOSS train 0.31281933892455144 valid 0.23782367731134096
LOSS train 0.31281933892455144 valid 0.23798002302646637
LOSS train 0.31281933892455144 valid 0.2380526438355446
LOSS train 0.31281933892455144 valid 0.23797912895679474
LOSS train 0.31281933892455144 valid 0.23812508801727192
LOSS train 0.31281933892455144 valid 0.23799583581653802
LOSS train 0.31281933892455144 valid 0.2380428097100668
LOSS train 0.31281933892455144 valid 0.2378508237434581
LOSS train 0.31281933892455144 valid 0.23789920800543846
LOSS train 0.31281933892455144 valid 0.23792404291175662
LOSS train 0.31281933892455144 valid 0.2380787040842207
LOSS train 0.31281933892455144 valid 0.238065466440785
LOSS train 0.31281933892455144 valid 0.23808099407081804
LOSS train 0.31281933892455144 valid 0.23813177259166007
LOSS train 0.31281933892455144 valid 0.23805717670733167
LOSS train 0.31281933892455144 valid 0.23778989169842157
LOSS train 0.31281933892455144 valid 0.23789039140149038
LOSS train 0.31281933892455144 valid 0.2379726810655013
LOSS train 0.31281933892455144 valid 0.2378622852642127
LOSS train 0.31281933892455144 valid 0.23787258595377955
LOSS train 0.31281933892455144 valid 0.23774070762097835
LOSS train 0.31281933892455144 valid 0.23758049264772615
LOSS train 0.31281933892455144 valid 0.2375651068321549
LOSS train 0.31281933892455144 valid 0.23742278492803057
LOSS train 0.31281933892455144 valid 0.23736702950269567
LOSS train 0.31281933892455144 valid 0.2371766579587285
LOSS train 0.31281933892455144 valid 0.23715144251156778
LOSS train 0.31281933892455144 valid 0.2370881859375083
LOSS train 0.31281933892455144 valid 0.23696476342872932
LOSS train 0.31281933892455144 valid 0.23682276861804524
LOSS train 0.31281933892455144 valid 0.23683464803865978
LOSS train 0.31281933892455144 valid 0.23695504319328833
LOSS train 0.31281933892455144 valid 0.23687691023608423
LOSS train 0.31281933892455144 valid 0.2369878650970862
LOSS train 0.31281933892455144 valid 0.23697469327772888
LOSS train 0.31281933892455144 valid 0.2367999127437902
LOSS train 0.31281933892455144 valid 0.2367854155600071
LOSS train 0.31281933892455144 valid 0.23670069344582095
LOSS train 0.31281933892455144 valid 0.23674046678827443
LOSS train 0.31281933892455144 valid 0.2368353607992059
LOSS train 0.31281933892455144 valid 0.23691473278132352
LOSS train 0.31281933892455144 valid 0.23691617485085223
LOSS train 0.31281933892455144 valid 0.23682023900317717
LOSS train 0.31281933892455144 valid 0.23696710917714464
LOSS train 0.31281933892455144 valid 0.23705431732482143
LOSS train 0.31281933892455144 valid 0.2370820328262117
LOSS train 0.31281933892455144 valid 0.2371314738308434
LOSS train 0.31281933892455144 valid 0.23738623984847299
LOSS train 0.31281933892455144 valid 0.23742731307682238
LOSS train 0.31281933892455144 valid 0.2374866909335274
LOSS train 0.31281933892455144 valid 0.23768221632294032
LOSS train 0.31281933892455144 valid 0.237662145650232
LOSS train 0.31281933892455144 valid 0.23767805337135134
LOSS train 0.31281933892455144 valid 0.2376484686085083
LOSS train 0.31281933892455144 valid 0.23762750109800926
LOSS train 0.31281933892455144 valid 0.23765432447829146
LOSS train 0.31281933892455144 valid 0.23753062225246835
LOSS train 0.31281933892455144 valid 0.2376216718541922
LOSS train 0.31281933892455144 valid 0.23750797990991288
LOSS train 0.31281933892455144 valid 0.23735686238839537
LOSS train 0.31281933892455144 valid 0.2372851261248191
LOSS train 0.31281933892455144 valid 0.23737851292265896
LOSS train 0.31281933892455144 valid 0.2371824823381487
LOSS train 0.31281933892455144 valid 0.2372300399796953
LOSS train 0.31281933892455144 valid 0.2373349008867975
LOSS train 0.31281933892455144 valid 0.23754377127910148
LOSS train 0.31281933892455144 valid 0.23751425319086245
LOSS train 0.31281933892455144 valid 0.23752182655730228
LOSS train 0.31281933892455144 valid 0.23748548124586383
LOSS train 0.31281933892455144 valid 0.23734815556361494
LOSS train 0.31281933892455144 valid 0.23749136412143707
LOSS train 0.31281933892455144 valid 0.23745389068981565
LOSS train 0.31281933892455144 valid 0.23739905708602496
LOSS train 0.31281933892455144 valid 0.23730937783190384
LOSS train 0.31281933892455144 valid 0.23732713920863593
LOSS train 0.31281933892455144 valid 0.23742774058790767
LOSS train 0.31281933892455144 valid 0.2374565708450973
LOSS train 0.31281933892455144 valid 0.23729610222786782
LOSS train 0.31281933892455144 valid 0.23741343039874882
LOSS train 0.31281933892455144 valid 0.2373899180571545
LOSS train 0.31281933892455144 valid 0.23740203237304322
LOSS train 0.31281933892455144 valid 0.2375286409338772
LOSS train 0.31281933892455144 valid 0.23746395776517518
LOSS train 0.31281933892455144 valid 0.2374418312152529
LOSS train 0.31281933892455144 valid 0.23738978842668462
LOSS train 0.31281933892455144 valid 0.23735481230717784
LOSS train 0.31281933892455144 valid 0.23736272018430823
LOSS train 0.31281933892455144 valid 0.2374862575910511
LOSS train 0.31281933892455144 valid 0.23757575202121664
LOSS train 0.31281933892455144 valid 0.237699377326274
LOSS train 0.31281933892455144 valid 0.23775685247447756
LOSS train 0.31281933892455144 valid 0.23784569032297803
LOSS train 0.31281933892455144 valid 0.23803710888194687
LOSS train 0.31281933892455144 valid 0.2380409287118213
LOSS train 0.31281933892455144 valid 0.23796375976861828
LOSS train 0.31281933892455144 valid 0.2379534325274554
LOSS train 0.31281933892455144 valid 0.23787768131148987
LOSS train 0.31281933892455144 valid 0.23773213726088457
LOSS train 0.31281933892455144 valid 0.2375749069795334
LOSS train 0.31281933892455144 valid 0.2376201024619482
LOSS train 0.31281933892455144 valid 0.2374877505004406
LOSS train 0.31281933892455144 valid 0.23739285141335686
LOSS train 0.31281933892455144 valid 0.23726587483646178
LOSS train 0.31281933892455144 valid 0.2372580243294315
LOSS train 0.31281933892455144 valid 0.23724132854963692
LOSS train 0.31281933892455144 valid 0.23728788134298825
LOSS train 0.31281933892455144 valid 0.23720267863123567
LOSS train 0.31281933892455144 valid 0.23706096717081834
LOSS train 0.31281933892455144 valid 0.23710704724200898
LOSS train 0.31281933892455144 valid 0.2370878389446793
LOSS train 0.31281933892455144 valid 0.23716329814031206
LOSS train 0.31281933892455144 valid 0.2371055330709903
LOSS train 0.31281933892455144 valid 0.23710084767782524
LOSS train 0.31281933892455144 valid 0.23707281810838615
LOSS train 0.31281933892455144 valid 0.23714415587130047
LOSS train 0.31281933892455144 valid 0.2371739909810535
LOSS train 0.31281933892455144 valid 0.23712842156355446
LOSS train 0.31281933892455144 valid 0.23715819022069476
LOSS train 0.31281933892455144 valid 0.2370732362138345
LOSS train 0.31281933892455144 valid 0.23712869915475812
LOSS train 0.31281933892455144 valid 0.23708307499686876
LOSS train 0.31281933892455144 valid 0.23722316473227403
LOSS train 0.31281933892455144 valid 0.23723288329429185
LOSS train 0.31281933892455144 valid 0.2373319598216035
LOSS train 0.31281933892455144 valid 0.2373372483998537
LOSS train 0.31281933892455144 valid 0.23721140369040067
LOSS train 0.31281933892455144 valid 0.23720896132046881
LOSS train 0.31281933892455144 valid 0.23723441168229045
LOSS train 0.31281933892455144 valid 0.23719332609083746
LOSS train 0.31281933892455144 valid 0.23723334845601549
LOSS train 0.31281933892455144 valid 0.23717171338296705
LOSS train 0.31281933892455144 valid 0.23707999495064713
LOSS train 0.31281933892455144 valid 0.2371580319908949
LOSS train 0.31281933892455144 valid 0.23717014467754302
LOSS train 0.31281933892455144 valid 0.23727948811783153
LOSS train 0.31281933892455144 valid 0.23729603158103096
LOSS train 0.31281933892455144 valid 0.23728419180157817
LOSS train 0.31281933892455144 valid 0.23730413911846535
LOSS train 0.31281933892455144 valid 0.23740684995486303
LOSS train 0.31281933892455144 valid 0.23747238367328824
LOSS train 0.31281933892455144 valid 0.23736087731085717
LOSS train 0.31281933892455144 valid 0.23745888689782388
LOSS train 0.31281933892455144 valid 0.23739275271477905
LOSS train 0.31281933892455144 valid 0.2373976215674043
LOSS train 0.31281933892455144 valid 0.23739529782791197
LOSS train 0.31281933892455144 valid 0.23744558549844302
LOSS train 0.31281933892455144 valid 0.23757685743775103
LOSS train 0.31281933892455144 valid 0.23761084361360707
LOSS train 0.31281933892455144 valid 0.23761010233585428
LOSS train 0.31281933892455144 valid 0.23777195023185937
LOSS train 0.31281933892455144 valid 0.2377279792771195
LOSS train 0.31281933892455144 valid 0.237653957753023
LOSS train 0.31281933892455144 valid 0.23756041754800153
LOSS train 0.31281933892455144 valid 0.2375412336788378
LOSS train 0.31281933892455144 valid 0.23765521657145666
LOSS train 0.31281933892455144 valid 0.23772421136720856
LOSS train 0.31281933892455144 valid 0.2377233761140988
LOSS train 0.31281933892455144 valid 0.23774923300177125
LOSS train 0.31281933892455144 valid 0.23771458978836352
LOSS train 0.31281933892455144 valid 0.23771491317094956
LOSS train 0.31281933892455144 valid 0.23769975050407297
LOSS train 0.31281933892455144 valid 0.23768299219370587
LOSS train 0.31281933892455144 valid 0.23770117119216083
LOSS train 0.31281933892455144 valid 0.23767448727139232
LOSS train 0.31281933892455144 valid 0.23781667654077673
LOSS train 0.31281933892455144 valid 0.2378347437882769
LOSS train 0.31281933892455144 valid 0.2377634448123116
LOSS train 0.31281933892455144 valid 0.2377261071967804
LOSS train 0.31281933892455144 valid 0.23763686748242926
LOSS train 0.31281933892455144 valid 0.2377184946677063
LOSS train 0.31281933892455144 valid 0.23765491911343165
LOSS train 0.31281933892455144 valid 0.23756553572297437
LOSS train 0.31281933892455144 valid 0.23758747851984066
LOSS train 0.31281933892455144 valid 0.23757377591923343
LOSS train 0.31281933892455144 valid 0.23755642405505908
LOSS train 0.31281933892455144 valid 0.2376607422677564
LOSS train 0.31281933892455144 valid 0.23769303850745888
LOSS train 0.31281933892455144 valid 0.23761855032430643
LOSS train 0.31281933892455144 valid 0.23757586636523295
LOSS train 0.31281933892455144 valid 0.23757707616080811
LOSS train 0.31281933892455144 valid 0.23755486413008636
LOSS train 0.31281933892455144 valid 0.23749804575192301
LOSS train 0.31281933892455144 valid 0.23756882964082845
LOSS train 0.31281933892455144 valid 0.23740977598944316
LOSS train 0.31281933892455144 valid 0.2374053363289152
LOSS train 0.31281933892455144 valid 0.23739860882497812
LOSS train 0.31281933892455144 valid 0.23736766363427939
LOSS train 0.31281933892455144 valid 0.23721270916897205
LOSS train 0.31281933892455144 valid 0.23716282208814568
LOSS train 0.31281933892455144 valid 0.23724671402759345
EPOCH 5:
  batch 1 loss: 0.29829567670822144
  batch 2 loss: 0.2885405421257019
  batch 3 loss: 0.29690099755922955
  batch 4 loss: 0.29710131883621216
  batch 5 loss: 0.30561151504516604
  batch 6 loss: 0.30560603737831116
  batch 7 loss: 0.3069171905517578
  batch 8 loss: 0.3082197792828083
  batch 9 loss: 0.3105178972085317
  batch 10 loss: 0.31189546585083006
  batch 11 loss: 0.31068611957810144
  batch 12 loss: 0.3089835171898206
  batch 13 loss: 0.30823227304678696
  batch 14 loss: 0.30834418535232544
  batch 15 loss: 0.30808080236117047
  batch 16 loss: 0.3081080224364996
  batch 17 loss: 0.3054671550498289
  batch 18 loss: 0.30744313697020215
  batch 19 loss: 0.3059357232169101
  batch 20 loss: 0.30396285504102705
  batch 21 loss: 0.30409641351018635
  batch 22 loss: 0.30406865206631745
  batch 23 loss: 0.30432003347770026
  batch 24 loss: 0.3031645392378171
  batch 25 loss: 0.3046943759918213
  batch 26 loss: 0.3031153598657021
  batch 27 loss: 0.30344755782021415
  batch 28 loss: 0.30109008454850744
  batch 29 loss: 0.30182989866569127
  batch 30 loss: 0.3006685420870781
  batch 31 loss: 0.3021851186790774
  batch 32 loss: 0.30098561523482203
  batch 33 loss: 0.3012838882930351
  batch 34 loss: 0.3015272078268668
  batch 35 loss: 0.3016485950776509
  batch 36 loss: 0.3021846161120468
  batch 37 loss: 0.3032517284154892
  batch 38 loss: 0.3036541895646798
  batch 39 loss: 0.3040154029925664
  batch 40 loss: 0.3039584767073393
  batch 41 loss: 0.30367211488688867
  batch 42 loss: 0.304007830009574
  batch 43 loss: 0.3046453231295874
  batch 44 loss: 0.3044100007550283
  batch 45 loss: 0.30411348972055646
  batch 46 loss: 0.30378338467815647
  batch 47 loss: 0.3037309408821958
  batch 48 loss: 0.3025771037985881
  batch 49 loss: 0.30212191537934907
  batch 50 loss: 0.3021066904067993
  batch 51 loss: 0.30124899920295267
  batch 52 loss: 0.3009845018386841
  batch 53 loss: 0.3006968155222119
  batch 54 loss: 0.30109868778122795
  batch 55 loss: 0.30099306919357993
  batch 56 loss: 0.3006263311420168
  batch 57 loss: 0.30040988796635676
  batch 58 loss: 0.30108359301912374
  batch 59 loss: 0.3011836626772153
  batch 60 loss: 0.30107981016238533
  batch 61 loss: 0.30156806990748547
  batch 62 loss: 0.30186315265394026
  batch 63 loss: 0.3019883055535574
  batch 64 loss: 0.3025540141388774
  batch 65 loss: 0.3022312191816477
  batch 66 loss: 0.30238968630631763
  batch 67 loss: 0.30267127369766805
  batch 68 loss: 0.3034577001543606
  batch 69 loss: 0.30316360748332477
  batch 70 loss: 0.3034107152904783
  batch 71 loss: 0.3031863700336134
  batch 72 loss: 0.30326763167977333
  batch 73 loss: 0.3031885734159652
  batch 74 loss: 0.3034186500149804
  batch 75 loss: 0.3028024713198344
  batch 76 loss: 0.30350126482938466
  batch 77 loss: 0.30334398653600125
  batch 78 loss: 0.3033226625277446
  batch 79 loss: 0.303819741629347
  batch 80 loss: 0.30358413569629195
  batch 81 loss: 0.30401188760627934
  batch 82 loss: 0.3043275453695437
  batch 83 loss: 0.3041960614991475
  batch 84 loss: 0.3038619976668131
  batch 85 loss: 0.3036449243040646
  batch 86 loss: 0.3041694143483805
  batch 87 loss: 0.30432229617546347
  batch 88 loss: 0.3039291931146925
  batch 89 loss: 0.30387659186727545
  batch 90 loss: 0.30392158296373156
  batch 91 loss: 0.30415114039903157
  batch 92 loss: 0.30435694462579227
  batch 93 loss: 0.3044936272405809
  batch 94 loss: 0.3047454617124923
  batch 95 loss: 0.30455817360627024
  batch 96 loss: 0.30455250944942236
  batch 97 loss: 0.3052269945439604
  batch 98 loss: 0.3052521390574319
  batch 99 loss: 0.30528815798085146
  batch 100 loss: 0.3050194516777992
  batch 101 loss: 0.3047743587210627
  batch 102 loss: 0.30491048681969735
  batch 103 loss: 0.30541500246640546
  batch 104 loss: 0.30546056708464253
  batch 105 loss: 0.3051799240566435
  batch 106 loss: 0.30534004152945754
  batch 107 loss: 0.3049191088876992
  batch 108 loss: 0.30481256461805767
  batch 109 loss: 0.3046618724634888
  batch 110 loss: 0.304701643911275
  batch 111 loss: 0.304645682508881
  batch 112 loss: 0.3043517049934183
  batch 113 loss: 0.30461506505983066
  batch 114 loss: 0.3048524046153353
  batch 115 loss: 0.30510675129683124
  batch 116 loss: 0.3050806111302869
  batch 117 loss: 0.30525068671275407
  batch 118 loss: 0.30480177801544384
  batch 119 loss: 0.3051081500133547
  batch 120 loss: 0.30508589297533034
  batch 121 loss: 0.3049480405228197
  batch 122 loss: 0.3047323502966615
  batch 123 loss: 0.30470887868385005
  batch 124 loss: 0.30500140906341616
  batch 125 loss: 0.30503013730049133
  batch 126 loss: 0.30497242675887215
  batch 127 loss: 0.3055558246890391
  batch 128 loss: 0.30539115983992815
  batch 129 loss: 0.3055579835592314
  batch 130 loss: 0.30555305160008944
  batch 131 loss: 0.3056938693723606
  batch 132 loss: 0.30566855768362683
  batch 133 loss: 0.3058564441096514
  batch 134 loss: 0.3059241280182084
  batch 135 loss: 0.30593781736161973
  batch 136 loss: 0.30589860195622726
  batch 137 loss: 0.30583003315612345
  batch 138 loss: 0.3057410945926887
  batch 139 loss: 0.30609364668242367
  batch 140 loss: 0.30590703253235135
  batch 141 loss: 0.3060553071769417
  batch 142 loss: 0.30607360979201087
  batch 143 loss: 0.30573349169917874
  batch 144 loss: 0.30567814223468304
  batch 145 loss: 0.30550770574602587
  batch 146 loss: 0.3054636927091912
  batch 147 loss: 0.30565278323329226
  batch 148 loss: 0.3056160946552818
  batch 149 loss: 0.3056312519431914
  batch 150 loss: 0.3057027101516724
  batch 151 loss: 0.30567130308277557
  batch 152 loss: 0.3057873547077179
  batch 153 loss: 0.30560478061632396
  batch 154 loss: 0.305721740056942
  batch 155 loss: 0.30579914969782673
  batch 156 loss: 0.30568393205220884
  batch 157 loss: 0.305759950428252
  batch 158 loss: 0.3058574686322031
  batch 159 loss: 0.30596970949532853
  batch 160 loss: 0.3057875961065292
  batch 161 loss: 0.30598114569735085
  batch 162 loss: 0.30596292865129165
  batch 163 loss: 0.3058549399756215
  batch 164 loss: 0.3058863242224949
  batch 165 loss: 0.30581693811850114
  batch 166 loss: 0.3057159373918212
  batch 167 loss: 0.3054435040779456
  batch 168 loss: 0.30538205713743255
  batch 169 loss: 0.30515133999508515
  batch 170 loss: 0.30505049456568323
  batch 171 loss: 0.30500977255447564
  batch 172 loss: 0.305026778648066
  batch 173 loss: 0.30499939525747577
  batch 174 loss: 0.3050985843285747
  batch 175 loss: 0.3052413776942662
  batch 176 loss: 0.3050039405511184
  batch 177 loss: 0.30500932618723076
  batch 178 loss: 0.30506519303562935
  batch 179 loss: 0.30516646810750053
  batch 180 loss: 0.30497445927725897
  batch 181 loss: 0.30485624766481517
  batch 182 loss: 0.30488258305486743
  batch 183 loss: 0.3046854636708244
  batch 184 loss: 0.30442401297066524
  batch 185 loss: 0.30447043244903155
  batch 186 loss: 0.30473354026194543
  batch 187 loss: 0.3047105572759149
  batch 188 loss: 0.30445722680776677
  batch 189 loss: 0.30432304787257364
  batch 190 loss: 0.30429675516329313
  batch 191 loss: 0.30427017246241345
  batch 192 loss: 0.3043454249079029
  batch 193 loss: 0.3043605970903999
  batch 194 loss: 0.3045452905377162
  batch 195 loss: 0.3046829407031719
  batch 196 loss: 0.30456631585043303
  batch 197 loss: 0.30446959071353
  batch 198 loss: 0.3047472021796487
  batch 199 loss: 0.30507716341833374
  batch 200 loss: 0.3052277779579163
  batch 201 loss: 0.30520739217302695
  batch 202 loss: 0.305137101523947
  batch 203 loss: 0.30535633487654434
  batch 204 loss: 0.30522505880570877
  batch 205 loss: 0.30532417021146635
  batch 206 loss: 0.3053496251407179
  batch 207 loss: 0.30540628839230194
  batch 208 loss: 0.3053096246260863
  batch 209 loss: 0.30508523081478317
  batch 210 loss: 0.30511411527792615
  batch 211 loss: 0.30517376712148225
  batch 212 loss: 0.3052394350744643
  batch 213 loss: 0.3053688348179132
  batch 214 loss: 0.3053749225964056
  batch 215 loss: 0.30508630989595903
  batch 216 loss: 0.30504287827622006
  batch 217 loss: 0.30500570427162854
  batch 218 loss: 0.30499332206785135
  batch 219 loss: 0.304951005262327
  batch 220 loss: 0.30495881377296014
  batch 221 loss: 0.30507916677321784
  batch 222 loss: 0.30523030694809045
  batch 223 loss: 0.30532274191422315
  batch 224 loss: 0.3053569479047188
  batch 225 loss: 0.30519014338652295
  batch 226 loss: 0.3053600652951055
  batch 227 loss: 0.30510882560639657
  batch 228 loss: 0.3050435033153024
  batch 229 loss: 0.30492089870975525
  batch 230 loss: 0.30495911519164626
  batch 231 loss: 0.30490909697431506
  batch 232 loss: 0.30471007776414527
  batch 233 loss: 0.30469307002847285
  batch 234 loss: 0.304760397817844
  batch 235 loss: 0.3048432463661153
  batch 236 loss: 0.30469690345354
  batch 237 loss: 0.304724021362856
  batch 238 loss: 0.304575597976937
  batch 239 loss: 0.304587084690896
  batch 240 loss: 0.3046735822533568
  batch 241 loss: 0.3046879829583821
  batch 242 loss: 0.3045390998898459
  batch 243 loss: 0.3046414860114149
  batch 244 loss: 0.30455196630515036
  batch 245 loss: 0.30455925665339645
  batch 246 loss: 0.3045621284745573
  batch 247 loss: 0.30458286712285476
  batch 248 loss: 0.30451868985208774
  batch 249 loss: 0.3045136759199771
  batch 250 loss: 0.3044923021197319
  batch 251 loss: 0.3044851324354035
  batch 252 loss: 0.30430000754339354
  batch 253 loss: 0.30418941042875586
  batch 254 loss: 0.3041380521232688
  batch 255 loss: 0.3041773620189405
  batch 256 loss: 0.30411452398402616
  batch 257 loss: 0.3040812882351968
  batch 258 loss: 0.3041102551790171
  batch 259 loss: 0.3041636736121417
  batch 260 loss: 0.30413822560356213
  batch 261 loss: 0.3041877457122693
  batch 262 loss: 0.30412576142840714
  batch 263 loss: 0.3041873784459589
  batch 264 loss: 0.3040526995266026
  batch 265 loss: 0.30407746934665825
  batch 266 loss: 0.30401046209989635
  batch 267 loss: 0.30415862290823503
  batch 268 loss: 0.3041730661556792
  batch 269 loss: 0.3041695431151798
  batch 270 loss: 0.3043522465560171
  batch 271 loss: 0.30457248332535647
  batch 272 loss: 0.3046341063454747
  batch 273 loss: 0.30457083597069695
  batch 274 loss: 0.3047100256067993
  batch 275 loss: 0.3047781252319163
  batch 276 loss: 0.3048229427549286
  batch 277 loss: 0.3047695381008761
  batch 278 loss: 0.30476809625359746
  batch 279 loss: 0.304893956549706
  batch 280 loss: 0.3048093764377492
  batch 281 loss: 0.3046742680340051
  batch 282 loss: 0.30465997771379794
  batch 283 loss: 0.30464587958246575
  batch 284 loss: 0.3046952830758733
  batch 285 loss: 0.3046532459949192
  batch 286 loss: 0.30475122502425334
  batch 287 loss: 0.3049046279349809
  batch 288 loss: 0.3047722359187901
  batch 289 loss: 0.30490126643832577
  batch 290 loss: 0.3048093473602986
  batch 291 loss: 0.3048287150274028
  batch 292 loss: 0.30490250941621116
  batch 293 loss: 0.3048526809992644
  batch 294 loss: 0.30476149191864493
  batch 295 loss: 0.3047774146674043
  batch 296 loss: 0.3048094730018764
  batch 297 loss: 0.30476426832403
  batch 298 loss: 0.30476576004852385
  batch 299 loss: 0.3046467598764394
  batch 300 loss: 0.30462611868977546
  batch 301 loss: 0.3044746047337586
  batch 302 loss: 0.3046158734240279
  batch 303 loss: 0.3046207262246129
  batch 304 loss: 0.30454330121804224
  batch 305 loss: 0.30447571556099123
  batch 306 loss: 0.3045527849809017
  batch 307 loss: 0.3045370533625544
  batch 308 loss: 0.3045941868482472
  batch 309 loss: 0.30454673943589033
  batch 310 loss: 0.30448808405668504
  batch 311 loss: 0.3046930244018794
  batch 312 loss: 0.30477484692938817
  batch 313 loss: 0.30485548374180593
  batch 314 loss: 0.30484611512559234
  batch 315 loss: 0.3048296587334739
  batch 316 loss: 0.3046967309392706
  batch 317 loss: 0.30476813714022893
  batch 318 loss: 0.3047341836979554
  batch 319 loss: 0.30467876058574006
  batch 320 loss: 0.30466365716420113
  batch 321 loss: 0.3047280063231786
  batch 322 loss: 0.3047105282637643
  batch 323 loss: 0.3046535969979992
  batch 324 loss: 0.3044087504623113
  batch 325 loss: 0.304423486498686
  batch 326 loss: 0.30441293216373294
  batch 327 loss: 0.3044442050045054
  batch 328 loss: 0.3042651800575053
  batch 329 loss: 0.3044248174842005
  batch 330 loss: 0.3043874349106442
  batch 331 loss: 0.3043760748698272
  batch 332 loss: 0.304339294916535
  batch 333 loss: 0.3042831586019413
  batch 334 loss: 0.30421802950298
  batch 335 loss: 0.30403430172756535
  batch 336 loss: 0.30391637568495106
  batch 337 loss: 0.30377642315113224
  batch 338 loss: 0.3037511501351052
  batch 339 loss: 0.30364859011848416
  batch 340 loss: 0.3036229533745962
  batch 341 loss: 0.30355575221899317
  batch 342 loss: 0.3035552641540243
  batch 343 loss: 0.30355751423203214
  batch 344 loss: 0.3035144420817148
  batch 345 loss: 0.30364858408768974
  batch 346 loss: 0.3034987045247431
  batch 347 loss: 0.30356732178318396
  batch 348 loss: 0.30359148255538665
  batch 349 loss: 0.30355357163102714
  batch 350 loss: 0.3036328569906098
  batch 351 loss: 0.30363876905366566
  batch 352 loss: 0.30366263644431124
  batch 353 loss: 0.3037262100082619
  batch 354 loss: 0.3038729996246807
  batch 355 loss: 0.30389693685820407
  batch 356 loss: 0.303831655615836
  batch 357 loss: 0.3037388279241006
  batch 358 loss: 0.303815969999609
  batch 359 loss: 0.3038395082469106
  batch 360 loss: 0.30384492530590956
  batch 361 loss: 0.30379515518937417
  batch 362 loss: 0.3038024793260664
  batch 363 loss: 0.3038049038530381
  batch 364 loss: 0.3037549366580916
  batch 365 loss: 0.30381801753011467
  batch 366 loss: 0.3037191183596361
  batch 367 loss: 0.3036171878797157
  batch 368 loss: 0.30351724682132836
  batch 369 loss: 0.3034846204810026
  batch 370 loss: 0.3034791761556187
  batch 371 loss: 0.3034506914509596
  batch 372 loss: 0.30344032732549536
  batch 373 loss: 0.3033541196872338
  batch 374 loss: 0.3031645163415588
  batch 375 loss: 0.30308859344323474
  batch 376 loss: 0.30318000512395765
  batch 377 loss: 0.30320219497939954
  batch 378 loss: 0.30310565331783246
  batch 379 loss: 0.30307330655076575
  batch 380 loss: 0.30312267396795123
  batch 381 loss: 0.3030157011403186
  batch 382 loss: 0.30292060171590424
  batch 383 loss: 0.30286321693705515
  batch 384 loss: 0.30279275157954544
  batch 385 loss: 0.30287194441665305
  batch 386 loss: 0.3027961365685562
  batch 387 loss: 0.30283742014905896
  batch 388 loss: 0.30286380260721923
  batch 389 loss: 0.3027946118140895
  batch 390 loss: 0.30272847899259664
  batch 391 loss: 0.30277437531887114
  batch 392 loss: 0.30280166929017527
  batch 393 loss: 0.30285096308808895
  batch 394 loss: 0.3028506892662363
  batch 395 loss: 0.3027623490064959
  batch 396 loss: 0.30272990611918044
  batch 397 loss: 0.3027007904476123
  batch 398 loss: 0.30261609096772707
  batch 399 loss: 0.30257174585546764
  batch 400 loss: 0.30259877439588306
  batch 401 loss: 0.302479262995601
  batch 402 loss: 0.3024610589022067
  batch 403 loss: 0.3024741882647829
  batch 404 loss: 0.30248887309491046
  batch 405 loss: 0.30254184864921335
  batch 406 loss: 0.30258239170747436
  batch 407 loss: 0.3025307530486906
  batch 408 loss: 0.3025905578671133
  batch 409 loss: 0.302511195718222
  batch 410 loss: 0.30253392214455255
  batch 411 loss: 0.3025071815635166
  batch 412 loss: 0.3024476706909323
  batch 413 loss: 0.3025180377408898
  batch 414 loss: 0.3025066712556254
  batch 415 loss: 0.30244440676936185
  batch 416 loss: 0.30245868736304915
  batch 417 loss: 0.3024541484437686
  batch 418 loss: 0.3024270518473461
  batch 419 loss: 0.30243430341769517
  batch 420 loss: 0.3024206371889228
  batch 421 loss: 0.302338967590887
  batch 422 loss: 0.30240820245861444
  batch 423 loss: 0.3024571806756035
  batch 424 loss: 0.3023958558673566
  batch 425 loss: 0.30240468021701367
  batch 426 loss: 0.302356400365281
  batch 427 loss: 0.3023597490243108
  batch 428 loss: 0.30227801325583015
  batch 429 loss: 0.30223097053024317
  batch 430 loss: 0.3021619304321533
  batch 431 loss: 0.30219611058793877
  batch 432 loss: 0.3022458268053554
  batch 433 loss: 0.3022363915110295
  batch 434 loss: 0.30234807018997484
  batch 435 loss: 0.30223104737955947
  batch 436 loss: 0.3022266988975739
  batch 437 loss: 0.30229238545185366
  batch 438 loss: 0.30243707924536917
  batch 439 loss: 0.302410678365236
  batch 440 loss: 0.30246146053753115
  batch 441 loss: 0.302419180671374
  batch 442 loss: 0.30242605719765925
  batch 443 loss: 0.3023700242185162
  batch 444 loss: 0.302277694447889
  batch 445 loss: 0.3022819737035237
  batch 446 loss: 0.3022194080756384
  batch 447 loss: 0.30212286751408973
  batch 448 loss: 0.30218144316625384
  batch 449 loss: 0.3022147222921949
  batch 450 loss: 0.3022061241666476
  batch 451 loss: 0.30218308559410323
  batch 452 loss: 0.3021332075308382
  batch 453 loss: 0.3022234488862503
  batch 454 loss: 0.3023149393137856
  batch 455 loss: 0.30227426909483396
  batch 456 loss: 0.3022646663154949
  batch 457 loss: 0.3022806564553822
  batch 458 loss: 0.3022614305261441
  batch 459 loss: 0.3021917600582368
  batch 460 loss: 0.30229386515591455
  batch 461 loss: 0.3022622437319373
  batch 462 loss: 0.30227956120973026
  batch 463 loss: 0.3021808666042324
  batch 464 loss: 0.30219332472389115
  batch 465 loss: 0.30214238932696724
  batch 466 loss: 0.30200755592963213
  batch 467 loss: 0.3021267384163062
  batch 468 loss: 0.302108471815148
  batch 469 loss: 0.3022107216023179
  batch 470 loss: 0.3022475851660079
  batch 471 loss: 0.30224100062168835
  batch 472 loss: 0.30213501686388156
LOSS train 0.30213501686388156 valid 0.23472043871879578
LOSS train 0.30213501686388156 valid 0.22594764083623886
LOSS train 0.30213501686388156 valid 0.22858156760533652
LOSS train 0.30213501686388156 valid 0.22057082876563072
LOSS train 0.30213501686388156 valid 0.21538539230823517
LOSS train 0.30213501686388156 valid 0.2180753101905187
LOSS train 0.30213501686388156 valid 0.22895604797771998
LOSS train 0.30213501686388156 valid 0.2233114242553711
LOSS train 0.30213501686388156 valid 0.22376898262235853
LOSS train 0.30213501686388156 valid 0.22625203430652618
LOSS train 0.30213501686388156 valid 0.22338866103779187
LOSS train 0.30213501686388156 valid 0.22369289646546045
LOSS train 0.30213501686388156 valid 0.22220712441664475
LOSS train 0.30213501686388156 valid 0.2215095545564379
LOSS train 0.30213501686388156 valid 0.217074716091156
LOSS train 0.30213501686388156 valid 0.21778882294893265
LOSS train 0.30213501686388156 valid 0.21830305106499615
LOSS train 0.30213501686388156 valid 0.21917350755797493
LOSS train 0.30213501686388156 valid 0.21992414719180056
LOSS train 0.30213501686388156 valid 0.21945024877786637
LOSS train 0.30213501686388156 valid 0.21908971667289734
LOSS train 0.30213501686388156 valid 0.21768949722701852
LOSS train 0.30213501686388156 valid 0.21826726846072986
LOSS train 0.30213501686388156 valid 0.2175805481771628
LOSS train 0.30213501686388156 valid 0.21564645767211915
LOSS train 0.30213501686388156 valid 0.21541073115972373
LOSS train 0.30213501686388156 valid 0.21557842084655054
LOSS train 0.30213501686388156 valid 0.21658303854720934
LOSS train 0.30213501686388156 valid 0.21680676885719957
LOSS train 0.30213501686388156 valid 0.21727283944686254
LOSS train 0.30213501686388156 valid 0.21844442573285872
LOSS train 0.30213501686388156 valid 0.2184374569915235
LOSS train 0.30213501686388156 valid 0.2193466325600942
LOSS train 0.30213501686388156 valid 0.21919150781982086
LOSS train 0.30213501686388156 valid 0.220562145113945
LOSS train 0.30213501686388156 valid 0.22022365406155586
LOSS train 0.30213501686388156 valid 0.2208020497818251
LOSS train 0.30213501686388156 valid 0.22165021653238096
LOSS train 0.30213501686388156 valid 0.22102611454633567
LOSS train 0.30213501686388156 valid 0.22138869613409043
LOSS train 0.30213501686388156 valid 0.22170627444255642
LOSS train 0.30213501686388156 valid 0.22157433167809532
LOSS train 0.30213501686388156 valid 0.22118463419204534
LOSS train 0.30213501686388156 valid 0.22131641547788272
LOSS train 0.30213501686388156 valid 0.22081381811036005
LOSS train 0.30213501686388156 valid 0.22116594534853232
LOSS train 0.30213501686388156 valid 0.22127400782514126
LOSS train 0.30213501686388156 valid 0.2209803725903233
LOSS train 0.30213501686388156 valid 0.22140505757867074
LOSS train 0.30213501686388156 valid 0.2210160732269287
LOSS train 0.30213501686388156 valid 0.22138994172507642
LOSS train 0.30213501686388156 valid 0.22147254244639322
LOSS train 0.30213501686388156 valid 0.22156801369954954
LOSS train 0.30213501686388156 valid 0.2217662066773132
LOSS train 0.30213501686388156 valid 0.22168656397949565
LOSS train 0.30213501686388156 valid 0.22181965942893708
LOSS train 0.30213501686388156 valid 0.22158682973761307
LOSS train 0.30213501686388156 valid 0.22115608407505627
LOSS train 0.30213501686388156 valid 0.22170051553491818
LOSS train 0.30213501686388156 valid 0.2213092712064584
LOSS train 0.30213501686388156 valid 0.2211821311321415
LOSS train 0.30213501686388156 valid 0.22167642630877032
LOSS train 0.30213501686388156 valid 0.2218382301784697
LOSS train 0.30213501686388156 valid 0.2226236960850656
LOSS train 0.30213501686388156 valid 0.2225762624007005
LOSS train 0.30213501686388156 valid 0.22263834738370145
LOSS train 0.30213501686388156 valid 0.22201583923688575
LOSS train 0.30213501686388156 valid 0.22205827450927565
LOSS train 0.30213501686388156 valid 0.22104680516581604
LOSS train 0.30213501686388156 valid 0.22101145940167563
LOSS train 0.30213501686388156 valid 0.22072126475018514
LOSS train 0.30213501686388156 valid 0.22083424280087152
LOSS train 0.30213501686388156 valid 0.22068703092940867
LOSS train 0.30213501686388156 valid 0.22037165471025416
LOSS train 0.30213501686388156 valid 0.22037065108617146
LOSS train 0.30213501686388156 valid 0.22082769282554326
LOSS train 0.30213501686388156 valid 0.2206513210163488
LOSS train 0.30213501686388156 valid 0.22066687028377485
LOSS train 0.30213501686388156 valid 0.22083595206465903
LOSS train 0.30213501686388156 valid 0.22026217952370644
LOSS train 0.30213501686388156 valid 0.21932133627526554
LOSS train 0.30213501686388156 valid 0.21928687266460278
LOSS train 0.30213501686388156 valid 0.21898365774786616
LOSS train 0.30213501686388156 valid 0.21889275525297439
LOSS train 0.30213501686388156 valid 0.21835067377370945
LOSS train 0.30213501686388156 valid 0.2176152349904526
LOSS train 0.30213501686388156 valid 0.21764512754034723
LOSS train 0.30213501686388156 valid 0.21734647943892263
LOSS train 0.30213501686388156 valid 0.21770373957880426
LOSS train 0.30213501686388156 valid 0.21797016825940874
LOSS train 0.30213501686388156 valid 0.21805984747933818
LOSS train 0.30213501686388156 valid 0.2181551791727543
LOSS train 0.30213501686388156 valid 0.218049917009569
LOSS train 0.30213501686388156 valid 0.21843488435161876
LOSS train 0.30213501686388156 valid 0.2180734013256274
LOSS train 0.30213501686388156 valid 0.21813833030561605
LOSS train 0.30213501686388156 valid 0.21828551298564242
LOSS train 0.30213501686388156 valid 0.21850480929929383
LOSS train 0.30213501686388156 valid 0.21858291267746627
LOSS train 0.30213501686388156 valid 0.21883188501000406
LOSS train 0.30213501686388156 valid 0.21915822645815292
LOSS train 0.30213501686388156 valid 0.2193948734624713
LOSS train 0.30213501686388156 valid 0.2191291418179725
LOSS train 0.30213501686388156 valid 0.21888726820739415
LOSS train 0.30213501686388156 valid 0.21929541812056585
LOSS train 0.30213501686388156 valid 0.21953481955910628
LOSS train 0.30213501686388156 valid 0.21918015014902453
LOSS train 0.30213501686388156 valid 0.21924500167369843
LOSS train 0.30213501686388156 valid 0.21975275327306276
LOSS train 0.30213501686388156 valid 0.21996811438690533
LOSS train 0.30213501686388156 valid 0.21964856780864098
LOSS train 0.30213501686388156 valid 0.21962839032390288
LOSS train 0.30213501686388156 valid 0.21971986897751294
LOSS train 0.30213501686388156 valid 0.21956221484824232
LOSS train 0.30213501686388156 valid 0.21958214044570923
LOSS train 0.30213501686388156 valid 0.21998580545186996
LOSS train 0.30213501686388156 valid 0.22025408296503574
LOSS train 0.30213501686388156 valid 0.2202145371396663
LOSS train 0.30213501686388156 valid 0.2199523258609932
LOSS train 0.30213501686388156 valid 0.2197480679800113
LOSS train 0.30213501686388156 valid 0.21954532903580626
LOSS train 0.30213501686388156 valid 0.21950694515568311
LOSS train 0.30213501686388156 valid 0.21961150748458336
LOSS train 0.30213501686388156 valid 0.21966227035849326
LOSS train 0.30213501686388156 valid 0.21967087459564208
LOSS train 0.30213501686388156 valid 0.21977893558759537
LOSS train 0.30213501686388156 valid 0.219797989632201
LOSS train 0.30213501686388156 valid 0.21999646211043
LOSS train 0.30213501686388156 valid 0.22021535154460936
LOSS train 0.30213501686388156 valid 0.22009677439928055
LOSS train 0.30213501686388156 valid 0.22011002841341587
LOSS train 0.30213501686388156 valid 0.2198233570564877
LOSS train 0.30213501686388156 valid 0.21967611086547822
LOSS train 0.30213501686388156 valid 0.21975485291054
LOSS train 0.30213501686388156 valid 0.21979225123370136
LOSS train 0.30213501686388156 valid 0.21961235309786656
LOSS train 0.30213501686388156 valid 0.21938522442849012
LOSS train 0.30213501686388156 valid 0.21934289647185284
LOSS train 0.30213501686388156 valid 0.21917741054253612
LOSS train 0.30213501686388156 valid 0.21926302505390985
LOSS train 0.30213501686388156 valid 0.21942076710521752
LOSS train 0.30213501686388156 valid 0.21962255743187917
LOSS train 0.30213501686388156 valid 0.21972315034249446
LOSS train 0.30213501686388156 valid 0.2197503750729892
LOSS train 0.30213501686388156 valid 0.21947998209246275
LOSS train 0.30213501686388156 valid 0.21966858422511246
LOSS train 0.30213501686388156 valid 0.2194596242134263
LOSS train 0.30213501686388156 valid 0.22006130913222158
LOSS train 0.30213501686388156 valid 0.22017693429585272
LOSS train 0.30213501686388156 valid 0.22011005441347758
LOSS train 0.30213501686388156 valid 0.22017357986889138
LOSS train 0.30213501686388156 valid 0.21999506917046874
LOSS train 0.30213501686388156 valid 0.22012010547849867
LOSS train 0.30213501686388156 valid 0.22016425604944106
LOSS train 0.30213501686388156 valid 0.22014421061162026
LOSS train 0.30213501686388156 valid 0.22034437944873786
LOSS train 0.30213501686388156 valid 0.2203225841757598
LOSS train 0.30213501686388156 valid 0.2204291829014126
LOSS train 0.30213501686388156 valid 0.22039514161505788
LOSS train 0.30213501686388156 valid 0.22019204180687665
LOSS train 0.30213501686388156 valid 0.22017983370315955
LOSS train 0.30213501686388156 valid 0.22005575941300687
LOSS train 0.30213501686388156 valid 0.22005209701558565
LOSS train 0.30213501686388156 valid 0.219802901875682
LOSS train 0.30213501686388156 valid 0.21970481755155505
LOSS train 0.30213501686388156 valid 0.219770888607186
LOSS train 0.30213501686388156 valid 0.2198422961606237
LOSS train 0.30213501686388156 valid 0.21970189540159135
LOSS train 0.30213501686388156 valid 0.2198085785793835
LOSS train 0.30213501686388156 valid 0.22000693617498174
LOSS train 0.30213501686388156 valid 0.2198856449440906
LOSS train 0.30213501686388156 valid 0.21978406114287155
LOSS train 0.30213501686388156 valid 0.21981197578369538
LOSS train 0.30213501686388156 valid 0.21980868562542158
LOSS train 0.30213501686388156 valid 0.2197055516072682
LOSS train 0.30213501686388156 valid 0.21970638624307784
LOSS train 0.30213501686388156 valid 0.21959794274831224
LOSS train 0.30213501686388156 valid 0.2197077733914504
LOSS train 0.30213501686388156 valid 0.21964098388256306
LOSS train 0.30213501686388156 valid 0.21959730610251427
LOSS train 0.30213501686388156 valid 0.21974324858978966
LOSS train 0.30213501686388156 valid 0.2198250310925337
LOSS train 0.30213501686388156 valid 0.21975586551134704
LOSS train 0.30213501686388156 valid 0.21990126282300637
LOSS train 0.30213501686388156 valid 0.21976177096366883
LOSS train 0.30213501686388156 valid 0.21980176465485685
LOSS train 0.30213501686388156 valid 0.21964719636555025
LOSS train 0.30213501686388156 valid 0.219678835982972
LOSS train 0.30213501686388156 valid 0.219701133865528
LOSS train 0.30213501686388156 valid 0.21985465594028172
LOSS train 0.30213501686388156 valid 0.2197986086001571
LOSS train 0.30213501686388156 valid 0.21981259466459355
LOSS train 0.30213501686388156 valid 0.21987290271205606
LOSS train 0.30213501686388156 valid 0.21979822932752138
LOSS train 0.30213501686388156 valid 0.21954707686717695
LOSS train 0.30213501686388156 valid 0.21965111754074387
LOSS train 0.30213501686388156 valid 0.21974542381497203
LOSS train 0.30213501686388156 valid 0.21962193128737537
LOSS train 0.30213501686388156 valid 0.21963809745096083
LOSS train 0.30213501686388156 valid 0.219528304412961
LOSS train 0.30213501686388156 valid 0.2193610628890754
LOSS train 0.30213501686388156 valid 0.2193366999525835
LOSS train 0.30213501686388156 valid 0.21922689588199107
LOSS train 0.30213501686388156 valid 0.21915087315673923
LOSS train 0.30213501686388156 valid 0.2189977821053528
LOSS train 0.30213501686388156 valid 0.2189820095607378
LOSS train 0.30213501686388156 valid 0.21891020281591278
LOSS train 0.30213501686388156 valid 0.21881573396519974
LOSS train 0.30213501686388156 valid 0.2186888788590591
LOSS train 0.30213501686388156 valid 0.2187481770912806
LOSS train 0.30213501686388156 valid 0.21888080832517542
LOSS train 0.30213501686388156 valid 0.218785804538232
LOSS train 0.30213501686388156 valid 0.2188936793048617
LOSS train 0.30213501686388156 valid 0.21885627932916177
LOSS train 0.30213501686388156 valid 0.21867277539053628
LOSS train 0.30213501686388156 valid 0.21861710068252352
LOSS train 0.30213501686388156 valid 0.21851767510313042
LOSS train 0.30213501686388156 valid 0.21854746669804284
LOSS train 0.30213501686388156 valid 0.21861168755788238
LOSS train 0.30213501686388156 valid 0.2187151905487884
LOSS train 0.30213501686388156 valid 0.21869622211380782
LOSS train 0.30213501686388156 valid 0.21861366189277923
LOSS train 0.30213501686388156 valid 0.21875839476628153
LOSS train 0.30213501686388156 valid 0.21882771860275949
LOSS train 0.30213501686388156 valid 0.21884776982996199
LOSS train 0.30213501686388156 valid 0.21887102676967604
LOSS train 0.30213501686388156 valid 0.2190933854438135
LOSS train 0.30213501686388156 valid 0.2191155087529567
LOSS train 0.30213501686388156 valid 0.21917409706844512
LOSS train 0.30213501686388156 valid 0.21935562335926553
LOSS train 0.30213501686388156 valid 0.21934577500149285
LOSS train 0.30213501686388156 valid 0.21936446030078263
LOSS train 0.30213501686388156 valid 0.21933357372816029
LOSS train 0.30213501686388156 valid 0.21932929856145483
LOSS train 0.30213501686388156 valid 0.21936954595941177
LOSS train 0.30213501686388156 valid 0.219247854993505
LOSS train 0.30213501686388156 valid 0.2193230308812379
LOSS train 0.30213501686388156 valid 0.21919086888557723
LOSS train 0.30213501686388156 valid 0.21908725274407215
LOSS train 0.30213501686388156 valid 0.21903907706340153
LOSS train 0.30213501686388156 valid 0.21913394301016795
LOSS train 0.30213501686388156 valid 0.21893159078418717
LOSS train 0.30213501686388156 valid 0.21897422727733973
LOSS train 0.30213501686388156 valid 0.2191108040145186
LOSS train 0.30213501686388156 valid 0.21928537646118476
LOSS train 0.30213501686388156 valid 0.21927262079424975
LOSS train 0.30213501686388156 valid 0.21924440659250807
LOSS train 0.30213501686388156 valid 0.2192204558560925
LOSS train 0.30213501686388156 valid 0.21905559223579116
LOSS train 0.30213501686388156 valid 0.21920238906145095
LOSS train 0.30213501686388156 valid 0.2191665448634273
LOSS train 0.30213501686388156 valid 0.21907540463975497
LOSS train 0.30213501686388156 valid 0.21897860201215555
LOSS train 0.30213501686388156 valid 0.21898852364988777
LOSS train 0.30213501686388156 valid 0.21908187059795156
LOSS train 0.30213501686388156 valid 0.21909950411645696
LOSS train 0.30213501686388156 valid 0.21895908793354776
LOSS train 0.30213501686388156 valid 0.21908110235781633
LOSS train 0.30213501686388156 valid 0.2190635790005614
LOSS train 0.30213501686388156 valid 0.21905634437616056
LOSS train 0.30213501686388156 valid 0.2191812513660197
LOSS train 0.30213501686388156 valid 0.21912843057217488
LOSS train 0.30213501686388156 valid 0.2191346228802612
LOSS train 0.30213501686388156 valid 0.2190994159741835
LOSS train 0.30213501686388156 valid 0.21905841636207868
LOSS train 0.30213501686388156 valid 0.21903523119320548
LOSS train 0.30213501686388156 valid 0.21916131055756902
LOSS train 0.30213501686388156 valid 0.2192554710182681
LOSS train 0.30213501686388156 valid 0.2193777616028449
LOSS train 0.30213501686388156 valid 0.21947096481367392
LOSS train 0.30213501686388156 valid 0.2195416712013118
LOSS train 0.30213501686388156 valid 0.2197303082793951
LOSS train 0.30213501686388156 valid 0.21974704886749113
LOSS train 0.30213501686388156 valid 0.21967032220024263
LOSS train 0.30213501686388156 valid 0.21967643174258145
LOSS train 0.30213501686388156 valid 0.2195994627756485
LOSS train 0.30213501686388156 valid 0.21946911863471627
LOSS train 0.30213501686388156 valid 0.2193210440681135
LOSS train 0.30213501686388156 valid 0.21935706470815938
LOSS train 0.30213501686388156 valid 0.2192376687590565
LOSS train 0.30213501686388156 valid 0.2191404477026963
LOSS train 0.30213501686388156 valid 0.21901743943598254
LOSS train 0.30213501686388156 valid 0.21900396412337206
LOSS train 0.30213501686388156 valid 0.21900409864078105
LOSS train 0.30213501686388156 valid 0.21902569140258588
LOSS train 0.30213501686388156 valid 0.21894931803633283
LOSS train 0.30213501686388156 valid 0.21883088407408485
LOSS train 0.30213501686388156 valid 0.21886707712999648
LOSS train 0.30213501686388156 valid 0.21883483710585994
LOSS train 0.30213501686388156 valid 0.21891053031230795
LOSS train 0.30213501686388156 valid 0.21883242456978538
LOSS train 0.30213501686388156 valid 0.2188418779062898
LOSS train 0.30213501686388156 valid 0.21881453616627247
LOSS train 0.30213501686388156 valid 0.21891745337012675
LOSS train 0.30213501686388156 valid 0.2189305296388723
LOSS train 0.30213501686388156 valid 0.21888375050715497
LOSS train 0.30213501686388156 valid 0.21889591834159813
LOSS train 0.30213501686388156 valid 0.21880150541363147
LOSS train 0.30213501686388156 valid 0.21884668875099425
LOSS train 0.30213501686388156 valid 0.21878709996740023
LOSS train 0.30213501686388156 valid 0.21891027347392022
LOSS train 0.30213501686388156 valid 0.2189150495047601
LOSS train 0.30213501686388156 valid 0.2190188682905518
LOSS train 0.30213501686388156 valid 0.2190160745460736
LOSS train 0.30213501686388156 valid 0.21888311534631447
LOSS train 0.30213501686388156 valid 0.2188713924557555
LOSS train 0.30213501686388156 valid 0.2189146448230122
LOSS train 0.30213501686388156 valid 0.21885537931864912
LOSS train 0.30213501686388156 valid 0.2189079172017119
LOSS train 0.30213501686388156 valid 0.21881101804394876
LOSS train 0.30213501686388156 valid 0.2187034121663624
LOSS train 0.30213501686388156 valid 0.21876491587131452
LOSS train 0.30213501686388156 valid 0.21877605003861192
LOSS train 0.30213501686388156 valid 0.21888373331848981
LOSS train 0.30213501686388156 valid 0.21888258636943878
LOSS train 0.30213501686388156 valid 0.21886983103578603
LOSS train 0.30213501686388156 valid 0.21890237415062516
LOSS train 0.30213501686388156 valid 0.21901460568297584
LOSS train 0.30213501686388156 valid 0.21907711729734294
LOSS train 0.30213501686388156 valid 0.21898342142812907
LOSS train 0.30213501686388156 valid 0.2190594655980945
LOSS train 0.30213501686388156 valid 0.21898579379971722
LOSS train 0.30213501686388156 valid 0.21898993374577985
LOSS train 0.30213501686388156 valid 0.2190065552698977
LOSS train 0.30213501686388156 valid 0.21904193662680113
LOSS train 0.30213501686388156 valid 0.2191528456251314
LOSS train 0.30213501686388156 valid 0.21916960015756273
LOSS train 0.30213501686388156 valid 0.21915768073281136
LOSS train 0.30213501686388156 valid 0.21933221513558304
LOSS train 0.30213501686388156 valid 0.21928915611722252
LOSS train 0.30213501686388156 valid 0.2192131728834256
LOSS train 0.30213501686388156 valid 0.2191295079348317
LOSS train 0.30213501686388156 valid 0.21911342267517572
LOSS train 0.30213501686388156 valid 0.2192203850446347
LOSS train 0.30213501686388156 valid 0.21927831422037153
LOSS train 0.30213501686388156 valid 0.21928106935783512
LOSS train 0.30213501686388156 valid 0.21929388203677508
LOSS train 0.30213501686388156 valid 0.2192751133053966
LOSS train 0.30213501686388156 valid 0.21926185233754747
LOSS train 0.30213501686388156 valid 0.21927918638376628
LOSS train 0.30213501686388156 valid 0.2192671305535476
LOSS train 0.30213501686388156 valid 0.21926707077270363
LOSS train 0.30213501686388156 valid 0.21923515239714186
LOSS train 0.30213501686388156 valid 0.21938028173564478
LOSS train 0.30213501686388156 valid 0.21938828286053477
LOSS train 0.30213501686388156 valid 0.21934696196476158
LOSS train 0.30213501686388156 valid 0.219306252703543
LOSS train 0.30213501686388156 valid 0.21919413159290949
LOSS train 0.30213501686388156 valid 0.21926700458314835
LOSS train 0.30213501686388156 valid 0.2192150505525725
LOSS train 0.30213501686388156 valid 0.21912846622983276
LOSS train 0.30213501686388156 valid 0.21915352877906777
LOSS train 0.30213501686388156 valid 0.21914431448529867
LOSS train 0.30213501686388156 valid 0.2191363877411616
LOSS train 0.30213501686388156 valid 0.21925426423549652
LOSS train 0.30213501686388156 valid 0.21929398534840414
LOSS train 0.30213501686388156 valid 0.2192306661722707
LOSS train 0.30213501686388156 valid 0.2191892624234354
LOSS train 0.30213501686388156 valid 0.21917688290388804
LOSS train 0.30213501686388156 valid 0.2191629042641984
LOSS train 0.30213501686388156 valid 0.21909416303905424
LOSS train 0.30213501686388156 valid 0.2191404836530185
LOSS train 0.30213501686388156 valid 0.218971732644667
LOSS train 0.30213501686388156 valid 0.21898522629187658
LOSS train 0.30213501686388156 valid 0.21898916339221067
LOSS train 0.30213501686388156 valid 0.21895703677434086
LOSS train 0.30213501686388156 valid 0.21879301705379903
LOSS train 0.30213501686388156 valid 0.21873489973823662
LOSS train 0.30213501686388156 valid 0.21881219122791032
EPOCH 6:
  batch 1 loss: 0.28309208154678345
  batch 2 loss: 0.2798928916454315
  batch 3 loss: 0.2928388516108195
  batch 4 loss: 0.29169921576976776
  batch 5 loss: 0.2993959426879883
  batch 6 loss: 0.30037493507067364
  batch 7 loss: 0.3013503508908408
  batch 8 loss: 0.30465104430913925
  batch 9 loss: 0.3041747013727824
  batch 10 loss: 0.30702359676361085
  batch 11 loss: 0.30467442219907587
  batch 12 loss: 0.30181622753540677
  batch 13 loss: 0.30068225127000076
  batch 14 loss: 0.30096811056137085
  batch 15 loss: 0.3004728277524312
  batch 16 loss: 0.3002729956060648
  batch 17 loss: 0.297229066491127
  batch 18 loss: 0.2995865336722798
  batch 19 loss: 0.297816915731681
  batch 20 loss: 0.2962908439338207
  batch 21 loss: 0.29611902932326
  batch 22 loss: 0.29567609795115213
  batch 23 loss: 0.296010025817415
  batch 24 loss: 0.2946117203682661
  batch 25 loss: 0.2959670859575272
  batch 26 loss: 0.2946457490324974
  batch 27 loss: 0.294854000210762
  batch 28 loss: 0.2924223095178604
  batch 29 loss: 0.2932084696046237
  batch 30 loss: 0.2922746330499649
  batch 31 loss: 0.2938318665950529
  batch 32 loss: 0.29294895473867655
  batch 33 loss: 0.2930999127301303
  batch 34 loss: 0.29310431462876935
  batch 35 loss: 0.29282446673938206
  batch 36 loss: 0.2933178510930803
  batch 37 loss: 0.2948668510527224
  batch 38 loss: 0.2953492803008933
  batch 39 loss: 0.2953390395029997
  batch 40 loss: 0.2952467449009418
  batch 41 loss: 0.29531099810832884
  batch 42 loss: 0.2955405194134939
  batch 43 loss: 0.29586828933205717
  batch 44 loss: 0.29567063735290006
  batch 45 loss: 0.2948728442192078
  batch 46 loss: 0.2946945713913959
  batch 47 loss: 0.29454037420293117
  batch 48 loss: 0.29373447969555855
  batch 49 loss: 0.29319267856831455
  batch 50 loss: 0.2934813040494919
  batch 51 loss: 0.2928810733206132
  batch 52 loss: 0.2929835090270409
  batch 53 loss: 0.2925039085577119
  batch 54 loss: 0.292651472820176
  batch 55 loss: 0.2922608592293479
  batch 56 loss: 0.29175193661025595
  batch 57 loss: 0.29168742372278583
  batch 58 loss: 0.29250776305280884
  batch 59 loss: 0.2925799514277507
  batch 60 loss: 0.29248694479465487
  batch 61 loss: 0.29328826362969446
  batch 62 loss: 0.2936731336578246
  batch 63 loss: 0.2937473493909079
  batch 64 loss: 0.2942998865619302
  batch 65 loss: 0.2939423065919142
  batch 66 loss: 0.29410725922295544
  batch 67 loss: 0.2943882795412149
  batch 68 loss: 0.2949558075736551
  batch 69 loss: 0.2946998027787692
  batch 70 loss: 0.29472443759441375
  batch 71 loss: 0.29437898036459803
  batch 72 loss: 0.2944011319842603
  batch 73 loss: 0.2942816439556749
  batch 74 loss: 0.2945727335440146
  batch 75 loss: 0.2941433902581533
  batch 76 loss: 0.2948940529635078
  batch 77 loss: 0.29468888624922024
  batch 78 loss: 0.29454607497423124
  batch 79 loss: 0.29502403321145454
  batch 80 loss: 0.29468011036515235
  batch 81 loss: 0.29487913479039696
  batch 82 loss: 0.2951321947138484
  batch 83 loss: 0.29501748228647623
  batch 84 loss: 0.2947407041986783
  batch 85 loss: 0.29433601708973156
  batch 86 loss: 0.29481112853039143
  batch 87 loss: 0.29488529179288053
  batch 88 loss: 0.29453719000924716
  batch 89 loss: 0.2945020399736554
  batch 90 loss: 0.29451383385393354
  batch 91 loss: 0.29457931433405193
  batch 92 loss: 0.2947288064853005
  batch 93 loss: 0.29486777705530964
  batch 94 loss: 0.29517229218432245
  batch 95 loss: 0.2948822485773187
  batch 96 loss: 0.29483613930642605
  batch 97 loss: 0.2954814667554246
  batch 98 loss: 0.2956331256701022
  batch 99 loss: 0.2956669740002565
  batch 100 loss: 0.295551535487175
  batch 101 loss: 0.2953820600368009
  batch 102 loss: 0.2955147521752937
  batch 103 loss: 0.29591853931112194
  batch 104 loss: 0.29587037202257377
  batch 105 loss: 0.29565008793558395
  batch 106 loss: 0.29591065645217896
  batch 107 loss: 0.2954341323576241
  batch 108 loss: 0.2953445969356431
  batch 109 loss: 0.2950477859842668
  batch 110 loss: 0.2950726094571027
  batch 111 loss: 0.29508523951779614
  batch 112 loss: 0.29470233379730154
  batch 113 loss: 0.29477965066918227
  batch 114 loss: 0.29526034807949736
  batch 115 loss: 0.2952090545840885
  batch 116 loss: 0.2952462488721157
  batch 117 loss: 0.2954082104385409
  batch 118 loss: 0.29497593161413227
  batch 119 loss: 0.2951174718993051
  batch 120 loss: 0.29512709751725197
  batch 121 loss: 0.2950156200523219
  batch 122 loss: 0.29466343023737923
  batch 123 loss: 0.2945889874202449
  batch 124 loss: 0.2949556537693547
  batch 125 loss: 0.29483913016319274
  batch 126 loss: 0.29479170886297074
  batch 127 loss: 0.29546982378471554
  batch 128 loss: 0.2953363861888647
  batch 129 loss: 0.2955734482569288
  batch 130 loss: 0.29551723301410676
  batch 131 loss: 0.29575022986827004
  batch 132 loss: 0.29584167288108304
  batch 133 loss: 0.29606193102392037
  batch 134 loss: 0.29617237177357747
  batch 135 loss: 0.29627436951354696
  batch 136 loss: 0.2962285394177717
  batch 137 loss: 0.29614712330546694
  batch 138 loss: 0.29600535527519556
  batch 139 loss: 0.2964583125045831
  batch 140 loss: 0.2962378753083093
  batch 141 loss: 0.29626090666081045
  batch 142 loss: 0.2962104254205462
  batch 143 loss: 0.295883438595525
  batch 144 loss: 0.2958179648137755
  batch 145 loss: 0.29563238641311385
  batch 146 loss: 0.29573580928861276
  batch 147 loss: 0.29583308445353085
  batch 148 loss: 0.2959218325244414
  batch 149 loss: 0.29598946059310194
  batch 150 loss: 0.296074467698733
  batch 151 loss: 0.296024050538903
  batch 152 loss: 0.2962318690199601
  batch 153 loss: 0.2960204914504406
  batch 154 loss: 0.2961675962070366
  batch 155 loss: 0.29627570279182924
  batch 156 loss: 0.29622588153832996
  batch 157 loss: 0.2963397333956069
  batch 158 loss: 0.29634062247940257
  batch 159 loss: 0.29648714939003473
  batch 160 loss: 0.2963251981884241
  batch 161 loss: 0.296435088474558
  batch 162 loss: 0.29639124704731834
  batch 163 loss: 0.2962369163709184
  batch 164 loss: 0.2963679185727747
  batch 165 loss: 0.2962410811221961
  batch 166 loss: 0.2961243450282568
  batch 167 loss: 0.2958497563878933
  batch 168 loss: 0.29573838288585347
  batch 169 loss: 0.29543073810416565
  batch 170 loss: 0.2952916710692294
  batch 171 loss: 0.29511927974502944
  batch 172 loss: 0.2952442322532798
  batch 173 loss: 0.29521106562517974
  batch 174 loss: 0.2954048064762148
  batch 175 loss: 0.29552753371851787
  batch 176 loss: 0.2953162243251096
  batch 177 loss: 0.29532304835521567
  batch 178 loss: 0.2953623061936893
  batch 179 loss: 0.2954818136032733
  batch 180 loss: 0.2953423750897249
  batch 181 loss: 0.29528412181698815
  batch 182 loss: 0.29532857886055014
  batch 183 loss: 0.29507710764316913
  batch 184 loss: 0.2948645270712998
  batch 185 loss: 0.2948436970646317
  batch 186 loss: 0.2950561275725724
  batch 187 loss: 0.2949960115440389
  batch 188 loss: 0.29475474151525094
  batch 189 loss: 0.29454997260734517
  batch 190 loss: 0.29457374732745323
  batch 191 loss: 0.2945819378523302
  batch 192 loss: 0.2946659674247106
  batch 193 loss: 0.29473549930542864
  batch 194 loss: 0.29494227453605415
  batch 195 loss: 0.2950994033079881
  batch 196 loss: 0.29504419619939765
  batch 197 loss: 0.2949526173814299
  batch 198 loss: 0.295229476660189
  batch 199 loss: 0.2954633297333166
  batch 200 loss: 0.29570193514227866
  batch 201 loss: 0.29572018580650217
  batch 202 loss: 0.29555121447780347
  batch 203 loss: 0.29569756338749026
  batch 204 loss: 0.29559873979465634
  batch 205 loss: 0.29577926251946424
  batch 206 loss: 0.2957026211960802
  batch 207 loss: 0.2957533511562624
  batch 208 loss: 0.2956757986774811
  batch 209 loss: 0.2955835678360679
  batch 210 loss: 0.29563422870068323
  batch 211 loss: 0.2955666702788023
  batch 212 loss: 0.2955575183315097
  batch 213 loss: 0.29564098657017024
  batch 214 loss: 0.29568244043354674
  batch 215 loss: 0.2953553705021392
  batch 216 loss: 0.295370712807333
  batch 217 loss: 0.2953881167459048
  batch 218 loss: 0.2953744915510536
  batch 219 loss: 0.29533566303176967
  batch 220 loss: 0.2953071668066762
  batch 221 loss: 0.2954664471057745
  batch 222 loss: 0.29561596578574395
  batch 223 loss: 0.2957037252294643
  batch 224 loss: 0.2957344404421747
  batch 225 loss: 0.29560893608464134
  batch 226 loss: 0.29574401617313906
  batch 227 loss: 0.29550976033777915
  batch 228 loss: 0.2954217234724446
  batch 229 loss: 0.29529311433109134
  batch 230 loss: 0.2953205256358437
  batch 231 loss: 0.2953334721651944
  batch 232 loss: 0.29507422370129616
  batch 233 loss: 0.2951072330382761
  batch 234 loss: 0.295112171361589
  batch 235 loss: 0.29522942393384083
  batch 236 loss: 0.2951336895762864
  batch 237 loss: 0.2952640099364494
  batch 238 loss: 0.29519603244897696
  batch 239 loss: 0.2952309148580958
  batch 240 loss: 0.29542520580192405
  batch 241 loss: 0.295563956390278
  batch 242 loss: 0.29540766258377676
  batch 243 loss: 0.29549280483536267
  batch 244 loss: 0.2954679293954959
  batch 245 loss: 0.2954637843735364
  batch 246 loss: 0.295499278156738
  batch 247 loss: 0.2955803939929375
  batch 248 loss: 0.2955979136449675
  batch 249 loss: 0.2955456487385623
  batch 250 loss: 0.2954636282920837
  batch 251 loss: 0.29557437951346316
  batch 252 loss: 0.29546618473435204
  batch 253 loss: 0.295263144338555
  batch 254 loss: 0.2952312335489303
  batch 255 loss: 0.2953952362724379
  batch 256 loss: 0.2954739367123693
  batch 257 loss: 0.2954912113772292
  batch 258 loss: 0.2954757919376211
  batch 259 loss: 0.295547461532718
  batch 260 loss: 0.29557362026893175
  batch 261 loss: 0.29565921996745115
  batch 262 loss: 0.2956543303173007
  batch 263 loss: 0.2956820532622899
  batch 264 loss: 0.2955879999155348
  batch 265 loss: 0.2956860948283717
  batch 266 loss: 0.2956419292473255
  batch 267 loss: 0.29575292916780105
  batch 268 loss: 0.29576103745111776
  batch 269 loss: 0.295819148033525
  batch 270 loss: 0.2960383945041233
  batch 271 loss: 0.29628102057974276
  batch 272 loss: 0.29637007393381176
  batch 273 loss: 0.2963092749590402
  batch 274 loss: 0.2964758126840104
  batch 275 loss: 0.2964881975000555
  batch 276 loss: 0.2965468888481458
  batch 277 loss: 0.2965503199436174
  batch 278 loss: 0.29656732693421756
  batch 279 loss: 0.29668242262683037
  batch 280 loss: 0.29657915628382137
  batch 281 loss: 0.29646430946753966
  batch 282 loss: 0.2964526464541753
  batch 283 loss: 0.2964226154893531
  batch 284 loss: 0.29651086126834575
  batch 285 loss: 0.29644241259809123
  batch 286 loss: 0.2964644179894374
  batch 287 loss: 0.29656719766842776
  batch 288 loss: 0.29645458463993335
  batch 289 loss: 0.2966507877445551
  batch 290 loss: 0.29645058769604254
  batch 291 loss: 0.2964431886205968
  batch 292 loss: 0.29648231347538023
  batch 293 loss: 0.29645625021270516
  batch 294 loss: 0.2963232435539466
  batch 295 loss: 0.29631234074043017
  batch 296 loss: 0.2963715518849927
  batch 297 loss: 0.2963380925984495
  batch 298 loss: 0.2962901850274745
  batch 299 loss: 0.2962200825030987
  batch 300 loss: 0.29616629749536516
  batch 301 loss: 0.29599310160673337
  batch 302 loss: 0.29606660465333634
  batch 303 loss: 0.2960643246524011
  batch 304 loss: 0.2960744649661999
  batch 305 loss: 0.2959915663863792
  batch 306 loss: 0.29610011171476514
  batch 307 loss: 0.29612779816315304
  batch 308 loss: 0.2962251648597129
  batch 309 loss: 0.29618462066627244
  batch 310 loss: 0.29613044843558345
  batch 311 loss: 0.29628571623582933
  batch 312 loss: 0.2963990631680458
  batch 313 loss: 0.2964082776833647
  batch 314 loss: 0.29641318506306147
  batch 315 loss: 0.29640803360749807
  batch 316 loss: 0.296304051587476
  batch 317 loss: 0.29634441378928883
  batch 318 loss: 0.29633359550117694
  batch 319 loss: 0.2962352120110234
  batch 320 loss: 0.2961893463972956
  batch 321 loss: 0.29622518371013096
  batch 322 loss: 0.296174645932935
  batch 323 loss: 0.29609185102000696
  batch 324 loss: 0.2958511183107341
  batch 325 loss: 0.2958333430840419
  batch 326 loss: 0.2957994368361549
  batch 327 loss: 0.29578075715161245
  batch 328 loss: 0.2955979654429162
  batch 329 loss: 0.29569541769368307
  batch 330 loss: 0.2956216338457483
  batch 331 loss: 0.295658659043629
  batch 332 loss: 0.29560986713293086
  batch 333 loss: 0.2955567526656228
  batch 334 loss: 0.29548369495276205
  batch 335 loss: 0.2952720565582389
  batch 336 loss: 0.2951499502218905
  batch 337 loss: 0.29498212132326573
  batch 338 loss: 0.2949143528232913
  batch 339 loss: 0.2947694201648763
  batch 340 loss: 0.2947124501799836
  batch 341 loss: 0.2946770926637034
  batch 342 loss: 0.29466976709010306
  batch 343 loss: 0.2947020339983198
  batch 344 loss: 0.29469953187156556
  batch 345 loss: 0.2948319854511731
  batch 346 loss: 0.294691945733018
  batch 347 loss: 0.29476147662493957
  batch 348 loss: 0.29478382494771616
  batch 349 loss: 0.2947463959797064
  batch 350 loss: 0.2948617599265916
  batch 351 loss: 0.29487024264627715
  batch 352 loss: 0.2948853375644169
  batch 353 loss: 0.29495650002199936
  batch 354 loss: 0.29510680240931486
  batch 355 loss: 0.29515399886688715
  batch 356 loss: 0.2950804979864801
  batch 357 loss: 0.2950314988358682
  batch 358 loss: 0.295096080105065
  batch 359 loss: 0.2951271866440441
  batch 360 loss: 0.29518428564899496
  batch 361 loss: 0.29510921986974836
  batch 362 loss: 0.29508826856949055
  batch 363 loss: 0.29510372749865876
  batch 364 loss: 0.29514074526154077
  batch 365 loss: 0.2952167378304756
  batch 366 loss: 0.29512830717804656
  batch 367 loss: 0.2950663390780015
  batch 368 loss: 0.29498505304850964
  batch 369 loss: 0.2948940737541452
  batch 370 loss: 0.29486922392168563
  batch 371 loss: 0.2948799045339749
  batch 372 loss: 0.294883760673705
  batch 373 loss: 0.29482203566197096
  batch 374 loss: 0.2946283011672331
  batch 375 loss: 0.2946276857058207
  batch 376 loss: 0.294705303029177
  batch 377 loss: 0.29473634273683363
  batch 378 loss: 0.29472213660283064
  batch 379 loss: 0.294766418065748
  batch 380 loss: 0.2947877148264333
  batch 381 loss: 0.2947322159927348
  batch 382 loss: 0.29471018208259064
  batch 383 loss: 0.29470096742515467
  batch 384 loss: 0.2946736622446527
  batch 385 loss: 0.2947603265960495
  batch 386 loss: 0.2946843236221551
  batch 387 loss: 0.29469093287637993
  batch 388 loss: 0.29471542524922756
  batch 389 loss: 0.2946649897067896
  batch 390 loss: 0.2945636665973908
  batch 391 loss: 0.2946472471327428
  batch 392 loss: 0.2946936669854485
  batch 393 loss: 0.2947281934984464
  batch 394 loss: 0.2947315258271803
  batch 395 loss: 0.29467572086974037
  batch 396 loss: 0.29468741478643035
  batch 397 loss: 0.294660037575801
  batch 398 loss: 0.29456997726430845
  batch 399 loss: 0.2946116349900277
  batch 400 loss: 0.29464908204972745
  batch 401 loss: 0.29452692098599714
  batch 402 loss: 0.29453445663351324
  batch 403 loss: 0.294528537626302
  batch 404 loss: 0.294518321051751
  batch 405 loss: 0.29454614494317843
  batch 406 loss: 0.2945976231999585
  batch 407 loss: 0.2945288017355546
  batch 408 loss: 0.29461112549053686
  batch 409 loss: 0.2945400986155554
  batch 410 loss: 0.2945622359470623
  batch 411 loss: 0.29454643379017675
  batch 412 loss: 0.2944896762593857
  batch 413 loss: 0.2945740984585903
  batch 414 loss: 0.2945506085736164
  batch 415 loss: 0.2945267292031323
  batch 416 loss: 0.2945270541553887
  batch 417 loss: 0.2945035637782918
  batch 418 loss: 0.29450869856126
  batch 419 loss: 0.29453603546346285
  batch 420 loss: 0.2945141171060857
  batch 421 loss: 0.29443504935630427
  batch 422 loss: 0.2945492858358469
  batch 423 loss: 0.2945785728540826
  batch 424 loss: 0.29453207837101425
  batch 425 loss: 0.29456318080425264
  batch 426 loss: 0.29451988137244056
  batch 427 loss: 0.2945124269997487
  batch 428 loss: 0.29441673938776847
  batch 429 loss: 0.2943782488574515
  batch 430 loss: 0.29432421330102654
  batch 431 loss: 0.2943845373641034
  batch 432 loss: 0.29451828570691524
  batch 433 loss: 0.29450770094130385
  batch 434 loss: 0.2945855429164276
  batch 435 loss: 0.2945058015913799
  batch 436 loss: 0.2944925351222174
  batch 437 loss: 0.29454610348429777
  batch 438 loss: 0.29469699793618565
  batch 439 loss: 0.29467258933329094
  batch 440 loss: 0.294703179767186
  batch 441 loss: 0.29469408297619853
  batch 442 loss: 0.2946724159752621
  batch 443 loss: 0.29462362005129356
  batch 444 loss: 0.2945846251740649
  batch 445 loss: 0.2946142055010528
  batch 446 loss: 0.29454727708090583
  batch 447 loss: 0.29449087138010616
  batch 448 loss: 0.29453277877265854
  batch 449 loss: 0.29458999291958415
  batch 450 loss: 0.2945452684495184
  batch 451 loss: 0.29451001513162894
  batch 452 loss: 0.29445668768170663
  batch 453 loss: 0.2945449549301834
  batch 454 loss: 0.2946121243325099
  batch 455 loss: 0.2945720645425084
  batch 456 loss: 0.2945694203504868
  batch 457 loss: 0.29459443137343133
  batch 458 loss: 0.294590752490483
  batch 459 loss: 0.2945237247707537
  batch 460 loss: 0.29461436054628826
  batch 461 loss: 0.29458943213014954
  batch 462 loss: 0.29459965135498994
  batch 463 loss: 0.294508113684716
  batch 464 loss: 0.29451281130956164
  batch 465 loss: 0.2944475249898049
  batch 466 loss: 0.29430784703963814
  batch 467 loss: 0.29442672463377234
  batch 468 loss: 0.29441412535106015
  batch 469 loss: 0.2945413201538993
  batch 470 loss: 0.2946078601986804
  batch 471 loss: 0.2945766907398868
  batch 472 loss: 0.29446674687630037
LOSS train 0.29446674687630037 valid 0.27321791648864746
LOSS train 0.29446674687630037 valid 0.2609199583530426
LOSS train 0.29446674687630037 valid 0.2679013212521871
LOSS train 0.29446674687630037 valid 0.2590576969087124
LOSS train 0.29446674687630037 valid 0.25291288197040557
LOSS train 0.29446674687630037 valid 0.25623076409101486
LOSS train 0.29446674687630037 valid 0.2675835256065641
LOSS train 0.29446674687630037 valid 0.2624019980430603
LOSS train 0.29446674687630037 valid 0.2628931403160095
LOSS train 0.29446674687630037 valid 0.2659439444541931
LOSS train 0.29446674687630037 valid 0.26297654482451355
LOSS train 0.29446674687630037 valid 0.26271914318203926
LOSS train 0.29446674687630037 valid 0.26120897554434264
LOSS train 0.29446674687630037 valid 0.2607507184147835
LOSS train 0.29446674687630037 valid 0.2561345428228378
LOSS train 0.29446674687630037 valid 0.2569218808785081
LOSS train 0.29446674687630037 valid 0.2576496662462459
LOSS train 0.29446674687630037 valid 0.25802986241049236
LOSS train 0.29446674687630037 valid 0.2588364081947427
LOSS train 0.29446674687630037 valid 0.25826496705412866
LOSS train 0.29446674687630037 valid 0.2576365336066201
LOSS train 0.29446674687630037 valid 0.25602326406673953
LOSS train 0.29446674687630037 valid 0.25683447135531384
LOSS train 0.29446674687630037 valid 0.2561138408879439
LOSS train 0.29446674687630037 valid 0.2540558671951294
LOSS train 0.29446674687630037 valid 0.25339243159844327
LOSS train 0.29446674687630037 valid 0.2538320676044182
LOSS train 0.29446674687630037 valid 0.25507292790072306
LOSS train 0.29446674687630037 valid 0.2553037754420576
LOSS train 0.29446674687630037 valid 0.2557805995146433
LOSS train 0.29446674687630037 valid 0.2569824774419108
LOSS train 0.29446674687630037 valid 0.25685299932956696
LOSS train 0.29446674687630037 valid 0.2580098067269181
LOSS train 0.29446674687630037 valid 0.2578877429751789
LOSS train 0.29446674687630037 valid 0.2593751421996525
LOSS train 0.29446674687630037 valid 0.25913064512941575
LOSS train 0.29446674687630037 valid 0.2594776661009402
LOSS train 0.29446674687630037 valid 0.26028980863721746
LOSS train 0.29446674687630037 valid 0.25943670861231977
LOSS train 0.29446674687630037 valid 0.25980506353080274
LOSS train 0.29446674687630037 valid 0.26013364697374947
LOSS train 0.29446674687630037 valid 0.2601177007669494
LOSS train 0.29446674687630037 valid 0.2596474036920902
LOSS train 0.29446674687630037 valid 0.25975621661001985
LOSS train 0.29446674687630037 valid 0.2593357079558902
LOSS train 0.29446674687630037 valid 0.25985127881817194
LOSS train 0.29446674687630037 valid 0.25993292509241306
LOSS train 0.29446674687630037 valid 0.25958470720797777
LOSS train 0.29446674687630037 valid 0.2600245156458446
LOSS train 0.29446674687630037 valid 0.25959559857845305
LOSS train 0.29446674687630037 valid 0.260042703034831
LOSS train 0.29446674687630037 valid 0.2601175227990517
LOSS train 0.29446674687630037 valid 0.26006287448811083
LOSS train 0.29446674687630037 valid 0.26031648615996045
LOSS train 0.29446674687630037 valid 0.26024399453943425
LOSS train 0.29446674687630037 valid 0.26027313832725796
LOSS train 0.29446674687630037 valid 0.2600059062242508
LOSS train 0.29446674687630037 valid 0.25948919253102665
LOSS train 0.29446674687630037 valid 0.26006262383218537
LOSS train 0.29446674687630037 valid 0.2595797931154569
LOSS train 0.29446674687630037 valid 0.25941414593673146
LOSS train 0.29446674687630037 valid 0.25998537170310176
LOSS train 0.29446674687630037 valid 0.2602132305739418
LOSS train 0.29446674687630037 valid 0.261109926039353
LOSS train 0.29446674687630037 valid 0.2611397640063213
LOSS train 0.29446674687630037 valid 0.2611278663530494
LOSS train 0.29446674687630037 valid 0.26033406760265576
LOSS train 0.29446674687630037 valid 0.2605350677143125
LOSS train 0.29446674687630037 valid 0.25940089877964795
LOSS train 0.29446674687630037 valid 0.2593310588172504
LOSS train 0.29446674687630037 valid 0.258937516472709
LOSS train 0.29446674687630037 valid 0.25895352268384564
LOSS train 0.29446674687630037 valid 0.2587620152594292
LOSS train 0.29446674687630037 valid 0.2584310925087413
LOSS train 0.29446674687630037 valid 0.2583776770035426
LOSS train 0.29446674687630037 valid 0.2589285857975483
LOSS train 0.29446674687630037 valid 0.2587885738580258
LOSS train 0.29446674687630037 valid 0.2587504747968454
LOSS train 0.29446674687630037 valid 0.2588952125250539
LOSS train 0.29446674687630037 valid 0.25815652031451464
LOSS train 0.29446674687630037 valid 0.2571200892145251
LOSS train 0.29446674687630037 valid 0.2571923869048677
LOSS train 0.29446674687630037 valid 0.25689523532448044
LOSS train 0.29446674687630037 valid 0.25682077361714273
LOSS train 0.29446674687630037 valid 0.25616467735346626
LOSS train 0.29446674687630037 valid 0.2554002248270567
LOSS train 0.29446674687630037 valid 0.2554802315673609
LOSS train 0.29446674687630037 valid 0.25514707003127446
LOSS train 0.29446674687630037 valid 0.2555260383680965
LOSS train 0.29446674687630037 valid 0.2557855314678616
LOSS train 0.29446674687630037 valid 0.2559243830350729
LOSS train 0.29446674687630037 valid 0.25601652167413547
LOSS train 0.29446674687630037 valid 0.2558491037737939
LOSS train 0.29446674687630037 valid 0.2562926744527005
LOSS train 0.29446674687630037 valid 0.2559313913709239
LOSS train 0.29446674687630037 valid 0.25601157592609525
LOSS train 0.29446674687630037 valid 0.25610489743886533
LOSS train 0.29446674687630037 valid 0.25639670038101625
LOSS train 0.29446674687630037 valid 0.25648132523502964
LOSS train 0.29446674687630037 valid 0.2567432101070881
LOSS train 0.29446674687630037 valid 0.25711494020306236
LOSS train 0.29446674687630037 valid 0.25744211717563514
LOSS train 0.29446674687630037 valid 0.25721662137114887
LOSS train 0.29446674687630037 valid 0.2568374377890275
LOSS train 0.29446674687630037 valid 0.25727945069471997
LOSS train 0.29446674687630037 valid 0.2575597711048036
LOSS train 0.29446674687630037 valid 0.2571521583840112
LOSS train 0.29446674687630037 valid 0.2571784802884967
LOSS train 0.29446674687630037 valid 0.2576977699448209
LOSS train 0.29446674687630037 valid 0.25796344402161514
LOSS train 0.29446674687630037 valid 0.25754177436098324
LOSS train 0.29446674687630037 valid 0.25749541792486397
LOSS train 0.29446674687630037 valid 0.25758477887221143
LOSS train 0.29446674687630037 valid 0.25740355031009304
LOSS train 0.29446674687630037 valid 0.2574493199586868
LOSS train 0.29446674687630037 valid 0.25781776434902487
LOSS train 0.29446674687630037 valid 0.2581317089816444
LOSS train 0.29446674687630037 valid 0.2580830125232874
LOSS train 0.29446674687630037 valid 0.25787201235775187
LOSS train 0.29446674687630037 valid 0.25770167633891106
LOSS train 0.29446674687630037 valid 0.25748780677633837
LOSS train 0.29446674687630037 valid 0.2574404664948338
LOSS train 0.29446674687630037 valid 0.2575503622855597
LOSS train 0.29446674687630037 valid 0.25771403925553443
LOSS train 0.29446674687630037 valid 0.25769156658649445
LOSS train 0.29446674687630037 valid 0.25780374698695685
LOSS train 0.29446674687630037 valid 0.25775931939834684
LOSS train 0.29446674687630037 valid 0.25798295356798917
LOSS train 0.29446674687630037 valid 0.2582613050706627
LOSS train 0.29446674687630037 valid 0.25815864285597434
LOSS train 0.29446674687630037 valid 0.2581167549804877
LOSS train 0.29446674687630037 valid 0.25783542916178703
LOSS train 0.29446674687630037 valid 0.25766147852392124
LOSS train 0.29446674687630037 valid 0.25773235277008655
LOSS train 0.29446674687630037 valid 0.25774069947225076
LOSS train 0.29446674687630037 valid 0.2575312095749028
LOSS train 0.29446674687630037 valid 0.2572668344870101
LOSS train 0.29446674687630037 valid 0.257239420560823
LOSS train 0.29446674687630037 valid 0.2570394875763132
LOSS train 0.29446674687630037 valid 0.257123247214726
LOSS train 0.29446674687630037 valid 0.2572940125110302
LOSS train 0.29446674687630037 valid 0.2575465199393286
LOSS train 0.29446674687630037 valid 0.2576704619230924
LOSS train 0.29446674687630037 valid 0.25773506632281673
LOSS train 0.29446674687630037 valid 0.2574383052258656
LOSS train 0.29446674687630037 valid 0.2576066005515726
LOSS train 0.29446674687630037 valid 0.2573389341028369
LOSS train 0.29446674687630037 valid 0.25807017098004753
LOSS train 0.29446674687630037 valid 0.2582761624315441
LOSS train 0.29446674687630037 valid 0.2581934329867363
LOSS train 0.29446674687630037 valid 0.2582943106921303
LOSS train 0.29446674687630037 valid 0.25808647245560823
LOSS train 0.29446674687630037 valid 0.25823406115466474
LOSS train 0.29446674687630037 valid 0.2582822559135301
LOSS train 0.29446674687630037 valid 0.25824226419771873
LOSS train 0.29446674687630037 valid 0.258534888139902
LOSS train 0.29446674687630037 valid 0.2585155188467852
LOSS train 0.29446674687630037 valid 0.2585734923994994
LOSS train 0.29446674687630037 valid 0.25849916368910353
LOSS train 0.29446674687630037 valid 0.25828412771224973
LOSS train 0.29446674687630037 valid 0.25826498699484407
LOSS train 0.29446674687630037 valid 0.25816111266613007
LOSS train 0.29446674687630037 valid 0.2581462187269714
LOSS train 0.29446674687630037 valid 0.2578548760857524
LOSS train 0.29446674687630037 valid 0.25774917376763895
LOSS train 0.29446674687630037 valid 0.2578075543405062
LOSS train 0.29446674687630037 valid 0.2579293214453908
LOSS train 0.29446674687630037 valid 0.2577545220653216
LOSS train 0.29446674687630037 valid 0.2578265144980165
LOSS train 0.29446674687630037 valid 0.25799695043002857
LOSS train 0.29446674687630037 valid 0.2578982331773691
LOSS train 0.29446674687630037 valid 0.25776442114350406
LOSS train 0.29446674687630037 valid 0.2578002489198839
LOSS train 0.29446674687630037 valid 0.2577850961650925
LOSS train 0.29446674687630037 valid 0.2576731663090842
LOSS train 0.29446674687630037 valid 0.2577290013432503
LOSS train 0.29446674687630037 valid 0.2576824057910402
LOSS train 0.29446674687630037 valid 0.2578300540701727
LOSS train 0.29446674687630037 valid 0.25774622096695715
LOSS train 0.29446674687630037 valid 0.25769904007514316
LOSS train 0.29446674687630037 valid 0.25783354504990974
LOSS train 0.29446674687630037 valid 0.2578927131144555
LOSS train 0.29446674687630037 valid 0.257824751441596
LOSS train 0.29446674687630037 valid 0.2580056787347016
LOSS train 0.29446674687630037 valid 0.2578274952398764
LOSS train 0.29446674687630037 valid 0.25784179824654774
LOSS train 0.29446674687630037 valid 0.2576671085733781
LOSS train 0.29446674687630037 valid 0.2577215036020634
LOSS train 0.29446674687630037 valid 0.2577817724653022
LOSS train 0.29446674687630037 valid 0.25800524142227677
LOSS train 0.29446674687630037 valid 0.2579460572353832
LOSS train 0.29446674687630037 valid 0.2579873001668602
LOSS train 0.29446674687630037 valid 0.25805546451417893
LOSS train 0.29446674687630037 valid 0.2579652548758025
LOSS train 0.29446674687630037 valid 0.2576700917421243
LOSS train 0.29446674687630037 valid 0.2577466664417666
LOSS train 0.29446674687630037 valid 0.2578930341198965
LOSS train 0.29446674687630037 valid 0.25771308637628654
LOSS train 0.29446674687630037 valid 0.25776486091278306
LOSS train 0.29446674687630037 valid 0.25767380490899083
LOSS train 0.29446674687630037 valid 0.2575288287442715
LOSS train 0.29446674687630037 valid 0.25751962313557614
LOSS train 0.29446674687630037 valid 0.25739205588260894
LOSS train 0.29446674687630037 valid 0.25734169810426
LOSS train 0.29446674687630037 valid 0.2572101436010221
LOSS train 0.29446674687630037 valid 0.2571825992713854
LOSS train 0.29446674687630037 valid 0.25710349814327443
LOSS train 0.29446674687630037 valid 0.25702943867788863
LOSS train 0.29446674687630037 valid 0.2569010010318893
LOSS train 0.29446674687630037 valid 0.2569822827265376
LOSS train 0.29446674687630037 valid 0.2570828800376558
LOSS train 0.29446674687630037 valid 0.2569980054812611
LOSS train 0.29446674687630037 valid 0.2571106562871888
LOSS train 0.29446674687630037 valid 0.257041263524617
LOSS train 0.29446674687630037 valid 0.25685755762943
LOSS train 0.29446674687630037 valid 0.25680990986250063
LOSS train 0.29446674687630037 valid 0.25665613397749887
LOSS train 0.29446674687630037 valid 0.25670751402957726
LOSS train 0.29446674687630037 valid 0.2567450135401939
LOSS train 0.29446674687630037 valid 0.2568730537864295
LOSS train 0.29446674687630037 valid 0.2569014138091204
LOSS train 0.29446674687630037 valid 0.2568511476253604
LOSS train 0.29446674687630037 valid 0.25702079134938965
LOSS train 0.29446674687630037 valid 0.2571251180156001
LOSS train 0.29446674687630037 valid 0.25714695500002965
LOSS train 0.29446674687630037 valid 0.25719115553440247
LOSS train 0.29446674687630037 valid 0.2574164166980903
LOSS train 0.29446674687630037 valid 0.25747528861750635
LOSS train 0.29446674687630037 valid 0.2574995283086227
LOSS train 0.29446674687630037 valid 0.25768616918636406
LOSS train 0.29446674687630037 valid 0.25768493064296194
LOSS train 0.29446674687630037 valid 0.25771446750852567
LOSS train 0.29446674687630037 valid 0.2576507215464064
LOSS train 0.29446674687630037 valid 0.257652460064134
LOSS train 0.29446674687630037 valid 0.25775196343026263
LOSS train 0.29446674687630037 valid 0.25758302723199633
LOSS train 0.29446674687630037 valid 0.2576999662420418
LOSS train 0.29446674687630037 valid 0.2575697669712435
LOSS train 0.29446674687630037 valid 0.2574536836546335
LOSS train 0.29446674687630037 valid 0.2573861414566636
LOSS train 0.29446674687630037 valid 0.25743231270075834
LOSS train 0.29446674687630037 valid 0.2571955914955494
LOSS train 0.29446674687630037 valid 0.2572554877878707
LOSS train 0.29446674687630037 valid 0.257416947882195
LOSS train 0.29446674687630037 valid 0.25760255255261244
LOSS train 0.29446674687630037 valid 0.2575540495354955
LOSS train 0.29446674687630037 valid 0.2575574397799457
LOSS train 0.29446674687630037 valid 0.2575128612018401
LOSS train 0.29446674687630037 valid 0.2573678929762668
LOSS train 0.29446674687630037 valid 0.25749471300840376
LOSS train 0.29446674687630037 valid 0.25748329351385274
LOSS train 0.29446674687630037 valid 0.25737201753589845
LOSS train 0.29446674687630037 valid 0.25726863677087036
LOSS train 0.29446674687630037 valid 0.2572645625378203
LOSS train 0.29446674687630037 valid 0.25731770308578716
LOSS train 0.29446674687630037 valid 0.25732293288456276
LOSS train 0.29446674687630037 valid 0.2571535373128342
LOSS train 0.29446674687630037 valid 0.25729232396959334
LOSS train 0.29446674687630037 valid 0.2572880951709269
LOSS train 0.29446674687630037 valid 0.25726655521071873
LOSS train 0.29446674687630037 valid 0.25742530440233674
LOSS train 0.29446674687630037 valid 0.2573641652938064
LOSS train 0.29446674687630037 valid 0.25739126281366603
LOSS train 0.29446674687630037 valid 0.25735938430509786
LOSS train 0.29446674687630037 valid 0.25734331737149435
LOSS train 0.29446674687630037 valid 0.2573080099839017
LOSS train 0.29446674687630037 valid 0.2574192144897547
LOSS train 0.29446674687630037 valid 0.2575032974579441
LOSS train 0.29446674687630037 valid 0.2576477442310645
LOSS train 0.29446674687630037 valid 0.25778662584446094
LOSS train 0.29446674687630037 valid 0.2578572594591612
LOSS train 0.29446674687630037 valid 0.2580906389390721
LOSS train 0.29446674687630037 valid 0.25811554202229986
LOSS train 0.29446674687630037 valid 0.2579987697357679
LOSS train 0.29446674687630037 valid 0.25801735119386154
LOSS train 0.29446674687630037 valid 0.25793193561443384
LOSS train 0.29446674687630037 valid 0.2577680680725979
LOSS train 0.29446674687630037 valid 0.25759800748644973
LOSS train 0.29446674687630037 valid 0.25761465685555585
LOSS train 0.29446674687630037 valid 0.257488852845771
LOSS train 0.29446674687630037 valid 0.25738827942529185
LOSS train 0.29446674687630037 valid 0.25726963526813695
LOSS train 0.29446674687630037 valid 0.25727125949657426
LOSS train 0.29446674687630037 valid 0.25731720303145933
LOSS train 0.29446674687630037 valid 0.2573423488098278
LOSS train 0.29446674687630037 valid 0.2572371933635298
LOSS train 0.29446674687630037 valid 0.25715224642163786
LOSS train 0.29446674687630037 valid 0.25721000305687386
LOSS train 0.29446674687630037 valid 0.25719278189756467
LOSS train 0.29446674687630037 valid 0.25728499062102417
LOSS train 0.29446674687630037 valid 0.2571880961816335
LOSS train 0.29446674687630037 valid 0.25721596038504824
LOSS train 0.29446674687630037 valid 0.2571744533325625
LOSS train 0.29446674687630037 valid 0.2573218987304337
LOSS train 0.29446674687630037 valid 0.2573894813909369
LOSS train 0.29446674687630037 valid 0.25733374935146924
LOSS train 0.29446674687630037 valid 0.25733461072950653
LOSS train 0.29446674687630037 valid 0.2572191460620637
LOSS train 0.29446674687630037 valid 0.2572590311036062
LOSS train 0.29446674687630037 valid 0.2571969299018383
LOSS train 0.29446674687630037 valid 0.25735736848706026
LOSS train 0.29446674687630037 valid 0.25736958385500686
LOSS train 0.29446674687630037 valid 0.25748341552691884
LOSS train 0.29446674687630037 valid 0.25748616628544896
LOSS train 0.29446674687630037 valid 0.2573364933494662
LOSS train 0.29446674687630037 valid 0.2573271515030487
LOSS train 0.29446674687630037 valid 0.2573687242471434
LOSS train 0.29446674687630037 valid 0.2573176816879929
LOSS train 0.29446674687630037 valid 0.2573693175724795
LOSS train 0.29446674687630037 valid 0.25726644050690434
LOSS train 0.29446674687630037 valid 0.2571487891999855
LOSS train 0.29446674687630037 valid 0.2572249431545154
LOSS train 0.29446674687630037 valid 0.25726069719456257
LOSS train 0.29446674687630037 valid 0.25737887739565724
LOSS train 0.29446674687630037 valid 0.25739380286799535
LOSS train 0.29446674687630037 valid 0.25737357097147384
LOSS train 0.29446674687630037 valid 0.2574039919049206
LOSS train 0.29446674687630037 valid 0.25752949494423355
LOSS train 0.29446674687630037 valid 0.2575675896996615
LOSS train 0.29446674687630037 valid 0.2574774693697691
LOSS train 0.29446674687630037 valid 0.25755310402109616
LOSS train 0.29446674687630037 valid 0.25751595888633905
LOSS train 0.29446674687630037 valid 0.2575106551126799
LOSS train 0.29446674687630037 valid 0.2575329277710414
LOSS train 0.29446674687630037 valid 0.25755149350716516
LOSS train 0.29446674687630037 valid 0.25767939181232746
LOSS train 0.29446674687630037 valid 0.2577221751304212
LOSS train 0.29446674687630037 valid 0.2577229127197004
LOSS train 0.29446674687630037 valid 0.2579255648025263
LOSS train 0.29446674687630037 valid 0.2578760851964806
LOSS train 0.29446674687630037 valid 0.257788989479088
LOSS train 0.29446674687630037 valid 0.25768598034439316
LOSS train 0.29446674687630037 valid 0.2576541666154031
LOSS train 0.29446674687630037 valid 0.25776327102484103
LOSS train 0.29446674687630037 valid 0.2578344265916454
LOSS train 0.29446674687630037 valid 0.25785031843753087
LOSS train 0.29446674687630037 valid 0.25783923480560234
LOSS train 0.29446674687630037 valid 0.2578009840475737
LOSS train 0.29446674687630037 valid 0.2577727103831142
LOSS train 0.29446674687630037 valid 0.25777757553493275
LOSS train 0.29446674687630037 valid 0.2577782737369761
LOSS train 0.29446674687630037 valid 0.25775047892715497
LOSS train 0.29446674687630037 valid 0.2577055048403865
LOSS train 0.29446674687630037 valid 0.257893874504885
LOSS train 0.29446674687630037 valid 0.25790442275828207
LOSS train 0.29446674687630037 valid 0.257863799271556
LOSS train 0.29446674687630037 valid 0.257809255712314
LOSS train 0.29446674687630037 valid 0.257697482860979
LOSS train 0.29446674687630037 valid 0.25778487585164755
LOSS train 0.29446674687630037 valid 0.25771299204656056
LOSS train 0.29446674687630037 valid 0.25760762087809735
LOSS train 0.29446674687630037 valid 0.25763988676904276
LOSS train 0.29446674687630037 valid 0.2576521674313559
LOSS train 0.29446674687630037 valid 0.25763542407144935
LOSS train 0.29446674687630037 valid 0.2577790505869288
LOSS train 0.29446674687630037 valid 0.2578400880349486
LOSS train 0.29446674687630037 valid 0.2577712698226549
LOSS train 0.29446674687630037 valid 0.257731323635112
LOSS train 0.29446674687630037 valid 0.25773748273969027
LOSS train 0.29446674687630037 valid 0.2577173647781213
LOSS train 0.29446674687630037 valid 0.2576418310156159
LOSS train 0.29446674687630037 valid 0.2577085844060993
LOSS train 0.29446674687630037 valid 0.25750123469297553
LOSS train 0.29446674687630037 valid 0.2575144500850321
LOSS train 0.29446674687630037 valid 0.2575187155645188
LOSS train 0.29446674687630037 valid 0.2574729945490269
LOSS train 0.29446674687630037 valid 0.25729030784858997
LOSS train 0.29446674687630037 valid 0.25723826464103616
LOSS train 0.29446674687630037 valid 0.2572760455169006
EPOCH 7:
  batch 1 loss: 0.3023808002471924
  batch 2 loss: 0.2886073589324951
  batch 3 loss: 0.2984529435634613
  batch 4 loss: 0.29526325315237045
  batch 5 loss: 0.30454534888267515
  batch 6 loss: 0.3006210923194885
  batch 7 loss: 0.29922529629298616
  batch 8 loss: 0.3035789988934994
  batch 9 loss: 0.30099337299664813
  batch 10 loss: 0.3012495070695877
  batch 11 loss: 0.2975426505912434
  batch 12 loss: 0.2944681942462921
  batch 13 loss: 0.29224149309671843
  batch 14 loss: 0.2916510914053236
  batch 15 loss: 0.29275317589441935
  batch 16 loss: 0.29123159870505333
  batch 17 loss: 0.28870971237911897
  batch 18 loss: 0.29153380460209316
  batch 19 loss: 0.2901556868302195
  batch 20 loss: 0.2882692217826843
  batch 21 loss: 0.2884026439416976
  batch 22 loss: 0.28806034001437103
  batch 23 loss: 0.287739889777225
  batch 24 loss: 0.2862214334309101
  batch 25 loss: 0.28711727499961853
  batch 26 loss: 0.28524863949188817
  batch 27 loss: 0.2852782562926964
  batch 28 loss: 0.2834196963480541
  batch 29 loss: 0.2842187049060032
  batch 30 loss: 0.28405769864718117
  batch 31 loss: 0.2848947673074661
  batch 32 loss: 0.2837181128561497
  batch 33 loss: 0.2835565922838269
  batch 34 loss: 0.28401966831263376
  batch 35 loss: 0.28360244631767273
  batch 36 loss: 0.2841007361809413
  batch 37 loss: 0.2851247924405175
  batch 38 loss: 0.2854013890028
  batch 39 loss: 0.28554723583734953
  batch 40 loss: 0.2852348618209362
  batch 41 loss: 0.28492697346501233
  batch 42 loss: 0.2853281036728904
  batch 43 loss: 0.2854157506033432
  batch 44 loss: 0.28495004231279547
  batch 45 loss: 0.28387266099452974
  batch 46 loss: 0.2835322297785593
  batch 47 loss: 0.2837960488618688
  batch 48 loss: 0.28284920907268923
  batch 49 loss: 0.2823616822763365
  batch 50 loss: 0.2825996509194374
  batch 51 loss: 0.28207659750592473
  batch 52 loss: 0.2821018217274776
  batch 53 loss: 0.2818705835994684
  batch 54 loss: 0.2819065123244568
  batch 55 loss: 0.281686087900942
  batch 56 loss: 0.28163579132940086
  batch 57 loss: 0.281420702735583
  batch 58 loss: 0.2819108169140487
  batch 59 loss: 0.2821458363431995
  batch 60 loss: 0.28219603275259336
  batch 61 loss: 0.2828719681892239
  batch 62 loss: 0.2833615375622626
  batch 63 loss: 0.28348143493372296
  batch 64 loss: 0.2842577660921961
  batch 65 loss: 0.2839344586317356
  batch 66 loss: 0.28418909216468985
  batch 67 loss: 0.2844912073950269
  batch 68 loss: 0.28514311133938675
  batch 69 loss: 0.28484452569830243
  batch 70 loss: 0.2851739221385547
  batch 71 loss: 0.2849658490486548
  batch 72 loss: 0.2849063788437181
  batch 73 loss: 0.2848462958450187
  batch 74 loss: 0.285325694527175
  batch 75 loss: 0.2848449891805649
  batch 76 loss: 0.2856921779089852
  batch 77 loss: 0.2855283465091284
  batch 78 loss: 0.28533930560717213
  batch 79 loss: 0.2860032929272591
  batch 80 loss: 0.2859213860705495
  batch 81 loss: 0.28607555543199
  batch 82 loss: 0.2864302907411645
  batch 83 loss: 0.2865313300526286
  batch 84 loss: 0.2864261179098061
  batch 85 loss: 0.2859257130061879
  batch 86 loss: 0.2863733525886092
  batch 87 loss: 0.2865649660428365
  batch 88 loss: 0.2863831218670715
  batch 89 loss: 0.28623258565249066
  batch 90 loss: 0.28636479808224574
  batch 91 loss: 0.28669055313854425
  batch 92 loss: 0.28688465641892474
  batch 93 loss: 0.28707577528492095
  batch 94 loss: 0.2875178484840596
  batch 95 loss: 0.28745063543319704
  batch 96 loss: 0.28742027655243874
  batch 97 loss: 0.28814869750406324
  batch 98 loss: 0.2887536682644669
  batch 99 loss: 0.28896110557546517
  batch 100 loss: 0.28879720240831375
  batch 101 loss: 0.2887107325662481
  batch 102 loss: 0.28889408473875006
  batch 103 loss: 0.289389264815062
  batch 104 loss: 0.2893000548848739
  batch 105 loss: 0.2890848389693669
  batch 106 loss: 0.28940334241345245
  batch 107 loss: 0.28887641262785296
  batch 108 loss: 0.28869031093738695
  batch 109 loss: 0.2884216366011068
  batch 110 loss: 0.2885008755055341
  batch 111 loss: 0.28848669201404126
  batch 112 loss: 0.2882114313542843
  batch 113 loss: 0.2881744836283996
  batch 114 loss: 0.2884896337463145
  batch 115 loss: 0.2885842204093933
  batch 116 loss: 0.28852832445810583
  batch 117 loss: 0.28856549583948576
  batch 118 loss: 0.2881904721765195
  batch 119 loss: 0.2884176178138797
  batch 120 loss: 0.28818020249406495
  batch 121 loss: 0.2879371278542132
  batch 122 loss: 0.28787441234119604
  batch 123 loss: 0.2878645292627133
  batch 124 loss: 0.2880675701364394
  batch 125 loss: 0.28801804661750796
  batch 126 loss: 0.28807065292956335
  batch 127 loss: 0.2886866647896804
  batch 128 loss: 0.2885475102812052
  batch 129 loss: 0.28877234135487284
  batch 130 loss: 0.2887516328921685
  batch 131 loss: 0.28902820617188024
  batch 132 loss: 0.2890477972951802
  batch 133 loss: 0.28928465323340624
  batch 134 loss: 0.2893723018133818
  batch 135 loss: 0.2895836368755058
  batch 136 loss: 0.2896253326359917
  batch 137 loss: 0.28960373693139013
  batch 138 loss: 0.28957666499891144
  batch 139 loss: 0.2900221787339492
  batch 140 loss: 0.28976806487355916
  batch 141 loss: 0.2896804543251687
  batch 142 loss: 0.2896769861100425
  batch 143 loss: 0.289455668075935
  batch 144 loss: 0.2895330310695701
  batch 145 loss: 0.2893158631078128
  batch 146 loss: 0.28941676939186983
  batch 147 loss: 0.28977074975870093
  batch 148 loss: 0.2896607457785993
  batch 149 loss: 0.2896742694729927
  batch 150 loss: 0.28979298889636995
  batch 151 loss: 0.2898319637933314
  batch 152 loss: 0.2899491565400048
  batch 153 loss: 0.28988705585205476
  batch 154 loss: 0.29032051679375886
  batch 155 loss: 0.2903264976316883
  batch 156 loss: 0.2902595198307282
  batch 157 loss: 0.29048209528254854
  batch 158 loss: 0.2906050855600381
  batch 159 loss: 0.29083566433228786
  batch 160 loss: 0.29062776621431113
  batch 161 loss: 0.2907855771713375
  batch 162 loss: 0.29092709352204826
  batch 163 loss: 0.29090248203716396
  batch 164 loss: 0.2908788734092945
  batch 165 loss: 0.2908568682092609
  batch 166 loss: 0.29079123379954375
  batch 167 loss: 0.290710625177372
  batch 168 loss: 0.29060166967766626
  batch 169 loss: 0.29034045867665986
  batch 170 loss: 0.2903969102046069
  batch 171 loss: 0.29044374091583386
  batch 172 loss: 0.29033156224461487
  batch 173 loss: 0.2904235972145389
  batch 174 loss: 0.29037737161263655
  batch 175 loss: 0.2904539978504181
  batch 176 loss: 0.29021899444474414
  batch 177 loss: 0.2901661228493782
  batch 178 loss: 0.29024867554394046
  batch 179 loss: 0.29031020078246156
  batch 180 loss: 0.29017655063006614
  batch 181 loss: 0.2900490158993895
  batch 182 loss: 0.2901346199623831
  batch 183 loss: 0.28992391992462135
  batch 184 loss: 0.2897464549573867
  batch 185 loss: 0.28974755414434383
  batch 186 loss: 0.2898550273109508
  batch 187 loss: 0.28977039671517946
  batch 188 loss: 0.28951114978879056
  batch 189 loss: 0.2891836764005126
  batch 190 loss: 0.28917565486933056
  batch 191 loss: 0.2890222656165118
  batch 192 loss: 0.28901910067846376
  batch 193 loss: 0.28910779644170576
  batch 194 loss: 0.289306859994672
  batch 195 loss: 0.2892731903455196
  batch 196 loss: 0.28910404185251315
  batch 197 loss: 0.28893276368300924
  batch 198 loss: 0.28913787549192255
  batch 199 loss: 0.28918409736911255
  batch 200 loss: 0.28927504301071166
  batch 201 loss: 0.28926590411224173
  batch 202 loss: 0.2890885004607758
  batch 203 loss: 0.28911780386135494
  batch 204 loss: 0.28896892625911563
  batch 205 loss: 0.28916291710807057
  batch 206 loss: 0.28911735030632574
  batch 207 loss: 0.2891657486342002
  batch 208 loss: 0.28905055285073245
  batch 209 loss: 0.28891439571905364
  batch 210 loss: 0.28901461987268356
  batch 211 loss: 0.28895302801900563
  batch 212 loss: 0.28889315519130454
  batch 213 loss: 0.28893147094148985
  batch 214 loss: 0.28898649168348756
  batch 215 loss: 0.2886736776246581
  batch 216 loss: 0.2885175491510718
  batch 217 loss: 0.2884979045473485
  batch 218 loss: 0.28853660117868984
  batch 219 loss: 0.28854271667460873
  batch 220 loss: 0.28850420395081694
  batch 221 loss: 0.2887531257997271
  batch 222 loss: 0.28889117376492907
  batch 223 loss: 0.28898623338461993
  batch 224 loss: 0.28896553182442275
  batch 225 loss: 0.2888321989774704
  batch 226 loss: 0.2889566058210567
  batch 227 loss: 0.2886997257166497
  batch 228 loss: 0.2886051746706168
  batch 229 loss: 0.28849389622044874
  batch 230 loss: 0.28849619115176406
  batch 231 loss: 0.2884425711193126
  batch 232 loss: 0.28826041140689934
  batch 233 loss: 0.28834549865241726
  batch 234 loss: 0.28837923877514327
  batch 235 loss: 0.2884511377583159
  batch 236 loss: 0.2883719946494547
  batch 237 loss: 0.2884691058960645
  batch 238 loss: 0.28842201259206324
  batch 239 loss: 0.28842706928442713
  batch 240 loss: 0.2885639427229762
  batch 241 loss: 0.28868803289174044
  batch 242 loss: 0.2885144117696226
  batch 243 loss: 0.2886176727436207
  batch 244 loss: 0.2885829168753546
  batch 245 loss: 0.28857381015407796
  batch 246 loss: 0.28857521422025634
  batch 247 loss: 0.288617942376658
  batch 248 loss: 0.2886120890898089
  batch 249 loss: 0.2885378720051792
  batch 250 loss: 0.28848933351039885
  batch 251 loss: 0.2885815089204872
  batch 252 loss: 0.2884927328143801
  batch 253 loss: 0.288272591329846
  batch 254 loss: 0.2882702208644762
  batch 255 loss: 0.2883930615350312
  batch 256 loss: 0.28847999463323504
  batch 257 loss: 0.28850483940733085
  batch 258 loss: 0.2885067261928736
  batch 259 loss: 0.2885549517211767
  batch 260 loss: 0.2885594991537241
  batch 261 loss: 0.28864189605603274
  batch 262 loss: 0.28864917329704487
  batch 263 loss: 0.2886843793519096
  batch 264 loss: 0.2885921450726914
  batch 265 loss: 0.2886731261352323
  batch 266 loss: 0.28868804020541056
  batch 267 loss: 0.2888136746061875
  batch 268 loss: 0.28875967515493506
  batch 269 loss: 0.2888110562549648
  batch 270 loss: 0.2890661028800187
  batch 271 loss: 0.2892147492878551
  batch 272 loss: 0.28930815144935074
  batch 273 loss: 0.2892276343408522
  batch 274 loss: 0.28938063206898906
  batch 275 loss: 0.28937612251801925
  batch 276 loss: 0.28940374944089114
  batch 277 loss: 0.2893539499289723
  batch 278 loss: 0.28937706833691906
  batch 279 loss: 0.28952562979899854
  batch 280 loss: 0.2893988393247128
  batch 281 loss: 0.2892842546264472
  batch 282 loss: 0.2892904004729386
  batch 283 loss: 0.2892568643227904
  batch 284 loss: 0.28928032590889596
  batch 285 loss: 0.2891814905300475
  batch 286 loss: 0.28918126444299735
  batch 287 loss: 0.28923961125599795
  batch 288 loss: 0.2890812749457028
  batch 289 loss: 0.2892678520258735
  batch 290 loss: 0.2890615775667388
  batch 291 loss: 0.28904076785975713
  batch 292 loss: 0.2890596364254821
  batch 293 loss: 0.2890756122081353
  batch 294 loss: 0.2889268888401336
  batch 295 loss: 0.28896289735527364
  batch 296 loss: 0.28899939806276076
  batch 297 loss: 0.28896895587845683
  batch 298 loss: 0.2889370769762353
  batch 299 loss: 0.2888660364625446
  batch 300 loss: 0.2887718158463637
  batch 301 loss: 0.2886242115022336
  batch 302 loss: 0.2886839188289958
  batch 303 loss: 0.28864292893866106
  batch 304 loss: 0.2886490211086838
  batch 305 loss: 0.28856691131826306
  batch 306 loss: 0.2886723861390469
  batch 307 loss: 0.288607179439029
  batch 308 loss: 0.28877930478616193
  batch 309 loss: 0.2887400350138593
  batch 310 loss: 0.28866475714791207
  batch 311 loss: 0.2888314461401421
  batch 312 loss: 0.2889366433597528
  batch 313 loss: 0.2889562292030444
  batch 314 loss: 0.28899715821834127
  batch 315 loss: 0.28898946330660863
  batch 316 loss: 0.2888912534977816
  batch 317 loss: 0.28893751564086045
  batch 318 loss: 0.28892132339987364
  batch 319 loss: 0.28878618627319513
  batch 320 loss: 0.28873522612266245
  batch 321 loss: 0.2887367239900839
  batch 322 loss: 0.28873006763480463
  batch 323 loss: 0.2886089846814749
  batch 324 loss: 0.2883626312753301
  batch 325 loss: 0.2883447327063634
  batch 326 loss: 0.28831627645009866
  batch 327 loss: 0.2883204922581302
  batch 328 loss: 0.2881479323273752
  batch 329 loss: 0.2882860328288788
  batch 330 loss: 0.28819254076842105
  batch 331 loss: 0.2882266773017872
  batch 332 loss: 0.28817728287484273
  batch 333 loss: 0.2881508362185848
  batch 334 loss: 0.2880754196893669
  batch 335 loss: 0.2878565656160241
  batch 336 loss: 0.2877552576274389
  batch 337 loss: 0.2876372361660711
  batch 338 loss: 0.2875718060417993
  batch 339 loss: 0.2874015160893972
  batch 340 loss: 0.28729436003109987
  batch 341 loss: 0.2872062704779885
  batch 342 loss: 0.28722266295151405
  batch 343 loss: 0.28722407484193585
  batch 344 loss: 0.28719509375649827
  batch 345 loss: 0.287290527768757
  batch 346 loss: 0.28711997777912657
  batch 347 loss: 0.28716338793035884
  batch 348 loss: 0.28717624237653855
  batch 349 loss: 0.28715554618391403
  batch 350 loss: 0.2872501764552934
  batch 351 loss: 0.28726217528523884
  batch 352 loss: 0.2873034698634662
  batch 353 loss: 0.28736083555660574
  batch 354 loss: 0.28750156964791024
  batch 355 loss: 0.28749240867688625
  batch 356 loss: 0.2874306295946073
  batch 357 loss: 0.2873935514554924
  batch 358 loss: 0.28745378058549415
  batch 359 loss: 0.2874431107452653
  batch 360 loss: 0.28754175682034755
  batch 361 loss: 0.2874691701414182
  batch 362 loss: 0.2874567733861465
  batch 363 loss: 0.28744757097450496
  batch 364 loss: 0.2874716327494972
  batch 365 loss: 0.2875221810520512
  batch 366 loss: 0.2874423637986183
  batch 367 loss: 0.2873578123159565
  batch 368 loss: 0.2872814393966742
  batch 369 loss: 0.28715960568367305
  batch 370 loss: 0.28712912423385156
  batch 371 loss: 0.28713821726185934
  batch 372 loss: 0.2871067942550746
  batch 373 loss: 0.2869457471866071
  batch 374 loss: 0.2867495341495397
  batch 375 loss: 0.2867085340420405
  batch 376 loss: 0.28678537893326994
  batch 377 loss: 0.2867470622774144
  batch 378 loss: 0.2867073664668376
  batch 379 loss: 0.28672222769512035
  batch 380 loss: 0.2867237299288574
  batch 381 loss: 0.2866336229670392
  batch 382 loss: 0.28661274515989565
  batch 383 loss: 0.2866065499443298
  batch 384 loss: 0.2865354993458216
  batch 385 loss: 0.28657579216864204
  batch 386 loss: 0.28649439847994346
  batch 387 loss: 0.2864734249752621
  batch 388 loss: 0.2865166666043788
  batch 389 loss: 0.2864525586673104
  batch 390 loss: 0.2863706770997781
  batch 391 loss: 0.28648137455553657
  batch 392 loss: 0.28654489046608916
  batch 393 loss: 0.286576402361763
  batch 394 loss: 0.28658699493874146
  batch 395 loss: 0.2865201816905903
  batch 396 loss: 0.28648599797878604
  batch 397 loss: 0.2864807665873535
  batch 398 loss: 0.28640553336496927
  batch 399 loss: 0.2864684244818556
  batch 400 loss: 0.28652611825615165
  batch 401 loss: 0.28640406287073195
  batch 402 loss: 0.28640079487170744
  batch 403 loss: 0.28641051908136894
  batch 404 loss: 0.28640283153641344
  batch 405 loss: 0.28642393663341614
  batch 406 loss: 0.2864622038793681
  batch 407 loss: 0.2864084786733187
  batch 408 loss: 0.2864802768636568
  batch 409 loss: 0.286423282073879
  batch 410 loss: 0.28645985355464426
  batch 411 loss: 0.2863985050986283
  batch 412 loss: 0.2863561473786831
  batch 413 loss: 0.2864460840135741
  batch 414 loss: 0.28640024734723973
  batch 415 loss: 0.28638193704277637
  batch 416 loss: 0.28639999773496616
  batch 417 loss: 0.28634815146740106
  batch 418 loss: 0.28629984271725967
  batch 419 loss: 0.2863396756475467
  batch 420 loss: 0.28630885442807563
  batch 421 loss: 0.2862252642239074
  batch 422 loss: 0.28637330738995315
  batch 423 loss: 0.2864146444321242
  batch 424 loss: 0.2863631981934579
  batch 425 loss: 0.2864074499116224
  batch 426 loss: 0.28635893919518296
  batch 427 loss: 0.28635669016307636
  batch 428 loss: 0.28627625278362606
  batch 429 loss: 0.28628025289043124
  batch 430 loss: 0.2862197572755259
  batch 431 loss: 0.28627402229956295
  batch 432 loss: 0.286369233392179
  batch 433 loss: 0.28638013698349923
  batch 434 loss: 0.2864777729739242
  batch 435 loss: 0.2863895787932407
  batch 436 loss: 0.28642018881002695
  batch 437 loss: 0.2864745681086721
  batch 438 loss: 0.28663451836941994
  batch 439 loss: 0.2866266575856198
  batch 440 loss: 0.2866472124714743
  batch 441 loss: 0.2866390043037549
  batch 442 loss: 0.2866164464419244
  batch 443 loss: 0.2865801903988922
  batch 444 loss: 0.2865600402357879
  batch 445 loss: 0.2866070492548889
  batch 446 loss: 0.2865388385049431
  batch 447 loss: 0.2864837827498481
  batch 448 loss: 0.28652450791560113
  batch 449 loss: 0.2865159030016387
  batch 450 loss: 0.28647047575977114
  batch 451 loss: 0.2864437505371555
  batch 452 loss: 0.28641529008746147
  batch 453 loss: 0.2864477703837106
  batch 454 loss: 0.28649975901001873
  batch 455 loss: 0.2864733833860565
  batch 456 loss: 0.2864774503140596
  batch 457 loss: 0.2865148153098981
  batch 458 loss: 0.28651513526923794
  batch 459 loss: 0.2864574992254149
  batch 460 loss: 0.2865882049112216
  batch 461 loss: 0.28657020124204763
  batch 462 loss: 0.2865928977534368
  batch 463 loss: 0.28651633984198066
  batch 464 loss: 0.286533631288029
  batch 465 loss: 0.28648447538575816
  batch 466 loss: 0.2863539356914201
  batch 467 loss: 0.2864316500228784
  batch 468 loss: 0.2864118129906491
  batch 469 loss: 0.2864982836536253
  batch 470 loss: 0.28654890535993777
  batch 471 loss: 0.2865196061488676
  batch 472 loss: 0.28638191937895147
LOSS train 0.28638191937895147 valid 0.2658582329750061
LOSS train 0.28638191937895147 valid 0.25344281643629074
LOSS train 0.28638191937895147 valid 0.2620427558819453
LOSS train 0.28638191937895147 valid 0.25183239951729774
LOSS train 0.28638191937895147 valid 0.24629526436328888
LOSS train 0.28638191937895147 valid 0.24953442563613257
LOSS train 0.28638191937895147 valid 0.2606356165238789
LOSS train 0.28638191937895147 valid 0.2553857434540987
LOSS train 0.28638191937895147 valid 0.25511812998188865
LOSS train 0.28638191937895147 valid 0.2582730606198311
LOSS train 0.28638191937895147 valid 0.25542275472120807
LOSS train 0.28638191937895147 valid 0.2547634653747082
LOSS train 0.28638191937895147 valid 0.2529862098968946
LOSS train 0.28638191937895147 valid 0.25326771289110184
LOSS train 0.28638191937895147 valid 0.2486696590979894
LOSS train 0.28638191937895147 valid 0.24948677700012922
LOSS train 0.28638191937895147 valid 0.25030584107427034
LOSS train 0.28638191937895147 valid 0.2503914278414514
LOSS train 0.28638191937895147 valid 0.25218722616371353
LOSS train 0.28638191937895147 valid 0.2514928996562958
LOSS train 0.28638191937895147 valid 0.2506312813077654
LOSS train 0.28638191937895147 valid 0.24912372028285806
LOSS train 0.28638191937895147 valid 0.24975303528101547
LOSS train 0.28638191937895147 valid 0.2491015437990427
LOSS train 0.28638191937895147 valid 0.24705771267414092
LOSS train 0.28638191937895147 valid 0.2464461991420159
LOSS train 0.28638191937895147 valid 0.24698622579927798
LOSS train 0.28638191937895147 valid 0.2483306388769831
LOSS train 0.28638191937895147 valid 0.24911505189435235
LOSS train 0.28638191937895147 valid 0.24969693124294282
LOSS train 0.28638191937895147 valid 0.25069951530425777
LOSS train 0.28638191937895147 valid 0.2506937915459275
LOSS train 0.28638191937895147 valid 0.2518466160152898
LOSS train 0.28638191937895147 valid 0.251472577890929
LOSS train 0.28638191937895147 valid 0.25303141857896533
LOSS train 0.28638191937895147 valid 0.25277555319998
LOSS train 0.28638191937895147 valid 0.2531370327279374
LOSS train 0.28638191937895147 valid 0.2539280221650475
LOSS train 0.28638191937895147 valid 0.2530558269757491
LOSS train 0.28638191937895147 valid 0.25352073311805723
LOSS train 0.28638191937895147 valid 0.25365175052386957
LOSS train 0.28638191937895147 valid 0.2537944508450372
LOSS train 0.28638191937895147 valid 0.2531931802283886
LOSS train 0.28638191937895147 valid 0.25321432744914835
LOSS train 0.28638191937895147 valid 0.2526991433567471
LOSS train 0.28638191937895147 valid 0.2533072829246521
LOSS train 0.28638191937895147 valid 0.2534071314842143
LOSS train 0.28638191937895147 valid 0.25318793797244626
LOSS train 0.28638191937895147 valid 0.2536447245855721
LOSS train 0.28638191937895147 valid 0.25306228786706925
LOSS train 0.28638191937895147 valid 0.25340507223325615
LOSS train 0.28638191937895147 valid 0.2535201096190856
LOSS train 0.28638191937895147 valid 0.25347232790488117
LOSS train 0.28638191937895147 valid 0.253927220624906
LOSS train 0.28638191937895147 valid 0.253754430196502
LOSS train 0.28638191937895147 valid 0.2536700141749212
LOSS train 0.28638191937895147 valid 0.25339544682126297
LOSS train 0.28638191937895147 valid 0.2529001384973526
LOSS train 0.28638191937895147 valid 0.25350474654617955
LOSS train 0.28638191937895147 valid 0.2529570293923219
LOSS train 0.28638191937895147 valid 0.25273904717359386
LOSS train 0.28638191937895147 valid 0.253341268387533
LOSS train 0.28638191937895147 valid 0.2535315182000872
LOSS train 0.28638191937895147 valid 0.25436604605056345
LOSS train 0.28638191937895147 valid 0.25447640258532306
LOSS train 0.28638191937895147 valid 0.25447585930426914
LOSS train 0.28638191937895147 valid 0.2535721052493622
LOSS train 0.28638191937895147 valid 0.2539416330263895
LOSS train 0.28638191937895147 valid 0.2528163097474886
LOSS train 0.28638191937895147 valid 0.2528573485357421
LOSS train 0.28638191937895147 valid 0.2524269334447216
LOSS train 0.28638191937895147 valid 0.25248908603356945
LOSS train 0.28638191937895147 valid 0.2523671113056679
LOSS train 0.28638191937895147 valid 0.2520221910766653
LOSS train 0.28638191937895147 valid 0.25199405749638876
LOSS train 0.28638191937895147 valid 0.25252315833380345
LOSS train 0.28638191937895147 valid 0.25222941536407967
LOSS train 0.28638191937895147 valid 0.2521619074619733
LOSS train 0.28638191937895147 valid 0.25228218639953226
LOSS train 0.28638191937895147 valid 0.2514862950891256
LOSS train 0.28638191937895147 valid 0.25040532115065023
LOSS train 0.28638191937895147 valid 0.2504443911517539
LOSS train 0.28638191937895147 valid 0.250126692185919
LOSS train 0.28638191937895147 valid 0.2500995752357301
LOSS train 0.28638191937895147 valid 0.24936859975842868
LOSS train 0.28638191937895147 valid 0.24859455626371296
LOSS train 0.28638191937895147 valid 0.24867696580530582
LOSS train 0.28638191937895147 valid 0.24826603365892713
LOSS train 0.28638191937895147 valid 0.24862912113077185
LOSS train 0.28638191937895147 valid 0.2488940386308564
LOSS train 0.28638191937895147 valid 0.24897954899531144
LOSS train 0.28638191937895147 valid 0.24906919718436574
LOSS train 0.28638191937895147 valid 0.24890666735428635
LOSS train 0.28638191937895147 valid 0.24938310001124728
LOSS train 0.28638191937895147 valid 0.2489040897080773
LOSS train 0.28638191937895147 valid 0.24890960551177463
LOSS train 0.28638191937895147 valid 0.24898108701730512
LOSS train 0.28638191937895147 valid 0.2492835495849045
LOSS train 0.28638191937895147 valid 0.24938434407566534
LOSS train 0.28638191937895147 valid 0.2496301443874836
LOSS train 0.28638191937895147 valid 0.24997610694701128
LOSS train 0.28638191937895147 valid 0.2503315632541974
LOSS train 0.28638191937895147 valid 0.25003297916315137
LOSS train 0.28638191937895147 valid 0.24962797144857737
LOSS train 0.28638191937895147 valid 0.25011060138543445
LOSS train 0.28638191937895147 valid 0.2503657887847918
LOSS train 0.28638191937895147 valid 0.24993997519818423
LOSS train 0.28638191937895147 valid 0.2499684220938771
LOSS train 0.28638191937895147 valid 0.2503793932702563
LOSS train 0.28638191937895147 valid 0.25063373053615745
LOSS train 0.28638191937895147 valid 0.25020419410220135
LOSS train 0.28638191937895147 valid 0.2501839951479009
LOSS train 0.28638191937895147 valid 0.2502671824356096
LOSS train 0.28638191937895147 valid 0.25011589561115233
LOSS train 0.28638191937895147 valid 0.2501990697954012
LOSS train 0.28638191937895147 valid 0.25057380839154636
LOSS train 0.28638191937895147 valid 0.25087458468400514
LOSS train 0.28638191937895147 valid 0.2508654619677592
LOSS train 0.28638191937895147 valid 0.2506888005412927
LOSS train 0.28638191937895147 valid 0.25049367919564247
LOSS train 0.28638191937895147 valid 0.25024993779245486
LOSS train 0.28638191937895147 valid 0.2501892349759086
LOSS train 0.28638191937895147 valid 0.25033820693085834
LOSS train 0.28638191937895147 valid 0.25051035107143466
LOSS train 0.28638191937895147 valid 0.2504632480144501
LOSS train 0.28638191937895147 valid 0.250607478476706
LOSS train 0.28638191937895147 valid 0.2505605830920963
LOSS train 0.28638191937895147 valid 0.250756453955546
LOSS train 0.28638191937895147 valid 0.2510556349920672
LOSS train 0.28638191937895147 valid 0.25101192662349114
LOSS train 0.28638191937895147 valid 0.2509754837014293
LOSS train 0.28638191937895147 valid 0.25063646652481775
LOSS train 0.28638191937895147 valid 0.25039045747957733
LOSS train 0.28638191937895147 valid 0.2506033722589265
LOSS train 0.28638191937895147 valid 0.2506007578637865
LOSS train 0.28638191937895147 valid 0.25036311281078
LOSS train 0.28638191937895147 valid 0.25014421235035805
LOSS train 0.28638191937895147 valid 0.25009129963059357
LOSS train 0.28638191937895147 valid 0.24987542639962204
LOSS train 0.28638191937895147 valid 0.24997664636799266
LOSS train 0.28638191937895147 valid 0.250147707179083
LOSS train 0.28638191937895147 valid 0.250318206426963
LOSS train 0.28638191937895147 valid 0.25047503078317307
LOSS train 0.28638191937895147 valid 0.25059539503935313
LOSS train 0.28638191937895147 valid 0.2503221400852861
LOSS train 0.28638191937895147 valid 0.2504821515654864
LOSS train 0.28638191937895147 valid 0.250231156645178
LOSS train 0.28638191937895147 valid 0.25101582595222705
LOSS train 0.28638191937895147 valid 0.2511835575303775
LOSS train 0.28638191937895147 valid 0.2510355280836423
LOSS train 0.28638191937895147 valid 0.2511871008091415
LOSS train 0.28638191937895147 valid 0.25094484508429704
LOSS train 0.28638191937895147 valid 0.251098206522418
LOSS train 0.28638191937895147 valid 0.2511454844823131
LOSS train 0.28638191937895147 valid 0.2511017658056751
LOSS train 0.28638191937895147 valid 0.25142806376784277
LOSS train 0.28638191937895147 valid 0.2513804643586942
LOSS train 0.28638191937895147 valid 0.25141278503439096
LOSS train 0.28638191937895147 valid 0.2513853850417167
LOSS train 0.28638191937895147 valid 0.2511415190063417
LOSS train 0.28638191937895147 valid 0.2510728877708779
LOSS train 0.28638191937895147 valid 0.2509399178771325
LOSS train 0.28638191937895147 valid 0.25086829559934654
LOSS train 0.28638191937895147 valid 0.25063458521191667
LOSS train 0.28638191937895147 valid 0.2504953212810285
LOSS train 0.28638191937895147 valid 0.2504984386355044
LOSS train 0.28638191937895147 valid 0.2506149040010875
LOSS train 0.28638191937895147 valid 0.2504436173254535
LOSS train 0.28638191937895147 valid 0.25049392623308847
LOSS train 0.28638191937895147 valid 0.2506940263159135
LOSS train 0.28638191937895147 valid 0.25058907706137984
LOSS train 0.28638191937895147 valid 0.25045850179916207
LOSS train 0.28638191937895147 valid 0.2504247293479181
LOSS train 0.28638191937895147 valid 0.25044907535287153
LOSS train 0.28638191937895147 valid 0.25036447542054313
LOSS train 0.28638191937895147 valid 0.2504539510065859
LOSS train 0.28638191937895147 valid 0.2503600935477995
LOSS train 0.28638191937895147 valid 0.2504692568203037
LOSS train 0.28638191937895147 valid 0.25042107093600585
LOSS train 0.28638191937895147 valid 0.2503444561527835
LOSS train 0.28638191937895147 valid 0.2504710244704347
LOSS train 0.28638191937895147 valid 0.2505380349991086
LOSS train 0.28638191937895147 valid 0.25046696509819866
LOSS train 0.28638191937895147 valid 0.25059743274165236
LOSS train 0.28638191937895147 valid 0.25042807806182554
LOSS train 0.28638191937895147 valid 0.25039716938170054
LOSS train 0.28638191937895147 valid 0.25022519200562154
LOSS train 0.28638191937895147 valid 0.2502239530390881
LOSS train 0.28638191937895147 valid 0.2502635502941394
LOSS train 0.28638191937895147 valid 0.2504771273387106
LOSS train 0.28638191937895147 valid 0.25042912972535136
LOSS train 0.28638191937895147 valid 0.2504548200716575
LOSS train 0.28638191937895147 valid 0.2505329575563342
LOSS train 0.28638191937895147 valid 0.250457347437893
LOSS train 0.28638191937895147 valid 0.2501581174440873
LOSS train 0.28638191937895147 valid 0.25026595858590944
LOSS train 0.28638191937895147 valid 0.2504290430830215
LOSS train 0.28638191937895147 valid 0.2502473291724619
LOSS train 0.28638191937895147 valid 0.2502879925708675
LOSS train 0.28638191937895147 valid 0.25024016000330446
LOSS train 0.28638191937895147 valid 0.2500951192272243
LOSS train 0.28638191937895147 valid 0.2500866518014728
LOSS train 0.28638191937895147 valid 0.24998345515998127
LOSS train 0.28638191937895147 valid 0.2499211751947216
LOSS train 0.28638191937895147 valid 0.2497722790008638
LOSS train 0.28638191937895147 valid 0.24972495378799808
LOSS train 0.28638191937895147 valid 0.24970148676547452
LOSS train 0.28638191937895147 valid 0.24961914845670646
LOSS train 0.28638191937895147 valid 0.24949900691874288
LOSS train 0.28638191937895147 valid 0.24958152721325558
LOSS train 0.28638191937895147 valid 0.24970889494034917
LOSS train 0.28638191937895147 valid 0.24964728058790261
LOSS train 0.28638191937895147 valid 0.24972211216257212
LOSS train 0.28638191937895147 valid 0.24963823399532623
LOSS train 0.28638191937895147 valid 0.24941875740539196
LOSS train 0.28638191937895147 valid 0.24934277766280705
LOSS train 0.28638191937895147 valid 0.24914581084855691
LOSS train 0.28638191937895147 valid 0.2491900076696632
LOSS train 0.28638191937895147 valid 0.24927264118575615
LOSS train 0.28638191937895147 valid 0.24940414110367948
LOSS train 0.28638191937895147 valid 0.2494793095335162
LOSS train 0.28638191937895147 valid 0.24940895537535349
LOSS train 0.28638191937895147 valid 0.2495269056392892
LOSS train 0.28638191937895147 valid 0.2496494779895459
LOSS train 0.28638191937895147 valid 0.24968023459116617
LOSS train 0.28638191937895147 valid 0.2497528378678634
LOSS train 0.28638191937895147 valid 0.24998119814805522
LOSS train 0.28638191937895147 valid 0.2500388989584488
LOSS train 0.28638191937895147 valid 0.25004713985597204
LOSS train 0.28638191937895147 valid 0.2502204168102016
LOSS train 0.28638191937895147 valid 0.25022386549871206
LOSS train 0.28638191937895147 valid 0.25027711162793226
LOSS train 0.28638191937895147 valid 0.25018661453232743
LOSS train 0.28638191937895147 valid 0.25019155856635833
LOSS train 0.28638191937895147 valid 0.25033305623429886
LOSS train 0.28638191937895147 valid 0.2501473913632207
LOSS train 0.28638191937895147 valid 0.25029552353836815
LOSS train 0.28638191937895147 valid 0.25017103183419764
LOSS train 0.28638191937895147 valid 0.2500922627528841
LOSS train 0.28638191937895147 valid 0.2500362075865269
LOSS train 0.28638191937895147 valid 0.2500920483680187
LOSS train 0.28638191937895147 valid 0.2498426554986268
LOSS train 0.28638191937895147 valid 0.24991286064616938
LOSS train 0.28638191937895147 valid 0.25010511346283504
LOSS train 0.28638191937895147 valid 0.250285537449681
LOSS train 0.28638191937895147 valid 0.25024578514380186
LOSS train 0.28638191937895147 valid 0.25022642435091225
LOSS train 0.28638191937895147 valid 0.2501814284271771
LOSS train 0.28638191937895147 valid 0.25003061440575075
LOSS train 0.28638191937895147 valid 0.25015995526313783
LOSS train 0.28638191937895147 valid 0.2501191039128133
LOSS train 0.28638191937895147 valid 0.25002118785466465
LOSS train 0.28638191937895147 valid 0.24991154323217898
LOSS train 0.28638191937895147 valid 0.24987687076639942
LOSS train 0.28638191937895147 valid 0.2499598930863773
LOSS train 0.28638191937895147 valid 0.24995981197571382
LOSS train 0.28638191937895147 valid 0.24979958900681729
LOSS train 0.28638191937895147 valid 0.24991861967615378
LOSS train 0.28638191937895147 valid 0.24991414123520428
LOSS train 0.28638191937895147 valid 0.24990333502109235
LOSS train 0.28638191937895147 valid 0.25006199807956303
LOSS train 0.28638191937895147 valid 0.25001538050083716
LOSS train 0.28638191937895147 valid 0.25006556080321396
LOSS train 0.28638191937895147 valid 0.25003800891114003
LOSS train 0.28638191937895147 valid 0.2500731490693002
LOSS train 0.28638191937895147 valid 0.2500678937798156
LOSS train 0.28638191937895147 valid 0.2501644849888841
LOSS train 0.28638191937895147 valid 0.25021287593156544
LOSS train 0.28638191937895147 valid 0.25036953035118853
LOSS train 0.28638191937895147 valid 0.25051276545833656
LOSS train 0.28638191937895147 valid 0.25060824994891334
LOSS train 0.28638191937895147 valid 0.2508560933501405
LOSS train 0.28638191937895147 valid 0.2508653970537605
LOSS train 0.28638191937895147 valid 0.25074980734256064
LOSS train 0.28638191937895147 valid 0.25076828582720323
LOSS train 0.28638191937895147 valid 0.25065595861794293
LOSS train 0.28638191937895147 valid 0.25049434774403967
LOSS train 0.28638191937895147 valid 0.2503060557001786
LOSS train 0.28638191937895147 valid 0.25031239285691237
LOSS train 0.28638191937895147 valid 0.2501954175531864
LOSS train 0.28638191937895147 valid 0.25006739869448635
LOSS train 0.28638191937895147 valid 0.24995527405899468
LOSS train 0.28638191937895147 valid 0.24995095290691188
LOSS train 0.28638191937895147 valid 0.2500073211398763
LOSS train 0.28638191937895147 valid 0.2500558306773504
LOSS train 0.28638191937895147 valid 0.24994039775191487
LOSS train 0.28638191937895147 valid 0.24986579053701008
LOSS train 0.28638191937895147 valid 0.2499160597411295
LOSS train 0.28638191937895147 valid 0.24991784649530496
LOSS train 0.28638191937895147 valid 0.25001915996444635
LOSS train 0.28638191937895147 valid 0.24988855252560882
LOSS train 0.28638191937895147 valid 0.24993739368980877
LOSS train 0.28638191937895147 valid 0.24992811654411484
LOSS train 0.28638191937895147 valid 0.2500880893294503
LOSS train 0.28638191937895147 valid 0.25016004700781935
LOSS train 0.28638191937895147 valid 0.250085985690758
LOSS train 0.28638191937895147 valid 0.2500914531424391
LOSS train 0.28638191937895147 valid 0.24996549581121277
LOSS train 0.28638191937895147 valid 0.2500041169067689
LOSS train 0.28638191937895147 valid 0.2499268916249275
LOSS train 0.28638191937895147 valid 0.25007301539281673
LOSS train 0.28638191937895147 valid 0.25008188740701864
LOSS train 0.28638191937895147 valid 0.25016583900640504
LOSS train 0.28638191937895147 valid 0.25015191170141887
LOSS train 0.28638191937895147 valid 0.25001223575873455
LOSS train 0.28638191937895147 valid 0.25000277176207186
LOSS train 0.28638191937895147 valid 0.25007123701541356
LOSS train 0.28638191937895147 valid 0.2500323199413039
LOSS train 0.28638191937895147 valid 0.2500830106557766
LOSS train 0.28638191937895147 valid 0.24997060827670559
LOSS train 0.28638191937895147 valid 0.24984396102919074
LOSS train 0.28638191937895147 valid 0.24992588778527883
LOSS train 0.28638191937895147 valid 0.24993032402695178
LOSS train 0.28638191937895147 valid 0.2500336269379421
LOSS train 0.28638191937895147 valid 0.25005188049778104
LOSS train 0.28638191937895147 valid 0.2500316990138609
LOSS train 0.28638191937895147 valid 0.250060812437948
LOSS train 0.28638191937895147 valid 0.2501869787397625
LOSS train 0.28638191937895147 valid 0.2502256776658719
LOSS train 0.28638191937895147 valid 0.2501313329208642
LOSS train 0.28638191937895147 valid 0.25020836553655307
LOSS train 0.28638191937895147 valid 0.2501859456208182
LOSS train 0.28638191937895147 valid 0.2501906898829959
LOSS train 0.28638191937895147 valid 0.2502271137027829
LOSS train 0.28638191937895147 valid 0.2502724327032383
LOSS train 0.28638191937895147 valid 0.25038830666271455
LOSS train 0.28638191937895147 valid 0.25044430473346596
LOSS train 0.28638191937895147 valid 0.25043276897290856
LOSS train 0.28638191937895147 valid 0.2506460422440503
LOSS train 0.28638191937895147 valid 0.2506022848866203
LOSS train 0.28638191937895147 valid 0.25050902366638184
LOSS train 0.28638191937895147 valid 0.2504339787705117
LOSS train 0.28638191937895147 valid 0.25040660212347815
LOSS train 0.28638191937895147 valid 0.25051795062190757
LOSS train 0.28638191937895147 valid 0.25057003355737945
LOSS train 0.28638191937895147 valid 0.25058565022689955
LOSS train 0.28638191937895147 valid 0.2505879277468435
LOSS train 0.28638191937895147 valid 0.25053495221589445
LOSS train 0.28638191937895147 valid 0.250509656284411
LOSS train 0.28638191937895147 valid 0.25052785049466525
LOSS train 0.28638191937895147 valid 0.25052090360621904
LOSS train 0.28638191937895147 valid 0.2504722333093833
LOSS train 0.28638191937895147 valid 0.2504157229345672
LOSS train 0.28638191937895147 valid 0.25060857035392936
LOSS train 0.28638191937895147 valid 0.2506165846534397
LOSS train 0.28638191937895147 valid 0.2505662369762542
LOSS train 0.28638191937895147 valid 0.25050612974922665
LOSS train 0.28638191937895147 valid 0.25040586806576826
LOSS train 0.28638191937895147 valid 0.25049735780773325
LOSS train 0.28638191937895147 valid 0.25042215125901357
LOSS train 0.28638191937895147 valid 0.2503023429049386
LOSS train 0.28638191937895147 valid 0.25034984535622323
LOSS train 0.28638191937895147 valid 0.250376003870883
LOSS train 0.28638191937895147 valid 0.2503336766934664
LOSS train 0.28638191937895147 valid 0.2504944882342513
LOSS train 0.28638191937895147 valid 0.25055202050657754
LOSS train 0.28638191937895147 valid 0.25049019465092515
LOSS train 0.28638191937895147 valid 0.2504582494580546
LOSS train 0.28638191937895147 valid 0.25047762492739056
LOSS train 0.28638191937895147 valid 0.25044338897698454
LOSS train 0.28638191937895147 valid 0.2503780599198513
LOSS train 0.28638191937895147 valid 0.2504363579891663
LOSS train 0.28638191937895147 valid 0.25020613277089826
LOSS train 0.28638191937895147 valid 0.2502344764441579
LOSS train 0.28638191937895147 valid 0.2502397854442466
LOSS train 0.28638191937895147 valid 0.25018503012064375
LOSS train 0.28638191937895147 valid 0.24998446860495316
LOSS train 0.28638191937895147 valid 0.24993868467762417
LOSS train 0.28638191937895147 valid 0.2499695034611839
EPOCH 8:
  batch 1 loss: 0.2880096435546875
  batch 2 loss: 0.27295663952827454
  batch 3 loss: 0.2794719735781352
  batch 4 loss: 0.27884913980960846
  batch 5 loss: 0.2947055518627167
  batch 6 loss: 0.2901802311340968
  batch 7 loss: 0.28884020021983553
  batch 8 loss: 0.2924787290394306
  batch 9 loss: 0.28975236746999955
  batch 10 loss: 0.2889114826917648
  batch 11 loss: 0.28548129580237647
  batch 12 loss: 0.2836449195941289
  batch 13 loss: 0.28182347462727475
  batch 14 loss: 0.28124332427978516
  batch 15 loss: 0.2822269896666209
  batch 16 loss: 0.281190013512969
  batch 17 loss: 0.2790851926102358
  batch 18 loss: 0.2820871439245012
  batch 19 loss: 0.2807610411393015
  batch 20 loss: 0.2786069966852665
  batch 21 loss: 0.2783211505129224
  batch 22 loss: 0.278378774496642
  batch 23 loss: 0.27822506362977234
  batch 24 loss: 0.2767140219608943
  batch 25 loss: 0.27777822613716124
  batch 26 loss: 0.27706024509209853
  batch 27 loss: 0.27696595037425004
  batch 28 loss: 0.27497213972466333
  batch 29 loss: 0.2755431173176601
  batch 30 loss: 0.2751454174518585
  batch 31 loss: 0.27603915910567006
  batch 32 loss: 0.27481849770992994
  batch 33 loss: 0.27531341260129755
  batch 34 loss: 0.2754261686521418
  batch 35 loss: 0.2751589153494154
  batch 36 loss: 0.2757056835624907
  batch 37 loss: 0.27656371609584707
  batch 38 loss: 0.27725776010438014
  batch 39 loss: 0.2770600563440567
  batch 40 loss: 0.27669973075389864
  batch 41 loss: 0.27680814484270605
  batch 42 loss: 0.2771970062028794
  batch 43 loss: 0.27731996913288914
  batch 44 loss: 0.2770974548025565
  batch 45 loss: 0.27585364315244887
  batch 46 loss: 0.27514655726111453
  batch 47 loss: 0.2750806475573398
  batch 48 loss: 0.27445503945151967
  batch 49 loss: 0.2739888143782713
  batch 50 loss: 0.2742475652694702
  batch 51 loss: 0.2738398594014785
  batch 52 loss: 0.2737748267558905
  batch 53 loss: 0.2735246245591146
  batch 54 loss: 0.27358764630776866
  batch 55 loss: 0.2734097594564611
  batch 56 loss: 0.273375860282353
  batch 57 loss: 0.2732911475917749
  batch 58 loss: 0.2735889178925547
  batch 59 loss: 0.27388657149622
  batch 60 loss: 0.2737829814354579
  batch 61 loss: 0.2744503309492205
  batch 62 loss: 0.2748594731092453
  batch 63 loss: 0.2749355624592493
  batch 64 loss: 0.2758178343065083
  batch 65 loss: 0.27540754836339215
  batch 66 loss: 0.27559080561905197
  batch 67 loss: 0.2757423851472228
  batch 68 loss: 0.27660884730079593
  batch 69 loss: 0.276239028227502
  batch 70 loss: 0.2767024421266147
  batch 71 loss: 0.2764565019120633
  batch 72 loss: 0.27645012218919063
  batch 73 loss: 0.27634321398114503
  batch 74 loss: 0.2767362930887454
  batch 75 loss: 0.2762353336811066
  batch 76 loss: 0.2768988950472129
  batch 77 loss: 0.2765730298184729
  batch 78 loss: 0.27650486696989107
  batch 79 loss: 0.2771738639360742
  batch 80 loss: 0.2768783140927553
  batch 81 loss: 0.2770407074763451
  batch 82 loss: 0.2772432347623313
  batch 83 loss: 0.27728221179491064
  batch 84 loss: 0.2772830586348261
  batch 85 loss: 0.2766487863133935
  batch 86 loss: 0.2771735567339631
  batch 87 loss: 0.27740279234003745
  batch 88 loss: 0.27722099982202053
  batch 89 loss: 0.2772662483909157
  batch 90 loss: 0.27730983710951274
  batch 91 loss: 0.2776595774915192
  batch 92 loss: 0.2779727216971957
  batch 93 loss: 0.2781444936990738
  batch 94 loss: 0.2785745163230186
  batch 95 loss: 0.2786221471272017
  batch 96 loss: 0.27875913322592777
  batch 97 loss: 0.2792023111557223
  batch 98 loss: 0.2795231249563548
  batch 99 loss: 0.27970351850745656
  batch 100 loss: 0.2795381708443165
  batch 101 loss: 0.2792675856611516
  batch 102 loss: 0.27939877775954264
  batch 103 loss: 0.28004431739015484
  batch 104 loss: 0.28000557752182853
  batch 105 loss: 0.2798796626783553
  batch 106 loss: 0.28019565132991325
  batch 107 loss: 0.2796890678528313
  batch 108 loss: 0.27953071519732475
  batch 109 loss: 0.2794151668428281
  batch 110 loss: 0.2794247230345553
  batch 111 loss: 0.2794004594152038
  batch 112 loss: 0.27917049306311775
  batch 113 loss: 0.27915869046629005
  batch 114 loss: 0.27953790611865226
  batch 115 loss: 0.27954093256722323
  batch 116 loss: 0.2795117077385557
  batch 117 loss: 0.279557914688037
  batch 118 loss: 0.27907268740868163
  batch 119 loss: 0.2792202871637184
  batch 120 loss: 0.2790326112260421
  batch 121 loss: 0.2788488055802574
  batch 122 loss: 0.27877273899121363
  batch 123 loss: 0.2788504041065045
  batch 124 loss: 0.2791822087620535
  batch 125 loss: 0.2791599065065384
  batch 126 loss: 0.2792534713470747
  batch 127 loss: 0.2798203669899092
  batch 128 loss: 0.27972709957975894
  batch 129 loss: 0.28000232681285503
  batch 130 loss: 0.2799019862826054
  batch 131 loss: 0.28014660279714426
  batch 132 loss: 0.2803150344301354
  batch 133 loss: 0.2805285687957491
  batch 134 loss: 0.2806957496413544
  batch 135 loss: 0.28091054459412895
  batch 136 loss: 0.28092614900978174
  batch 137 loss: 0.28089769576152746
  batch 138 loss: 0.28086222978173825
  batch 139 loss: 0.28131496166582587
  batch 140 loss: 0.28121009479675974
  batch 141 loss: 0.2812091988240573
  batch 142 loss: 0.28118985419122267
  batch 143 loss: 0.2811141060990887
  batch 144 loss: 0.28114760553257334
  batch 145 loss: 0.28089145966644946
  batch 146 loss: 0.28099177767560907
  batch 147 loss: 0.281501891560295
  batch 148 loss: 0.2814220630035207
  batch 149 loss: 0.2813737897464893
  batch 150 loss: 0.28153440485397974
  batch 151 loss: 0.28188824150341235
  batch 152 loss: 0.28205086339853314
  batch 153 loss: 0.28199779724373536
  batch 154 loss: 0.28262134834930497
  batch 155 loss: 0.28271739223311027
  batch 156 loss: 0.28262085477129006
  batch 157 loss: 0.2827713539835754
  batch 158 loss: 0.28302367248489885
  batch 159 loss: 0.2834717480454055
  batch 160 loss: 0.28326770747080443
  batch 161 loss: 0.2833552139331095
  batch 162 loss: 0.28349118569382914
  batch 163 loss: 0.28363205017122023
  batch 164 loss: 0.2836228179313788
  batch 165 loss: 0.2835299548777667
  batch 166 loss: 0.2834402926894556
  batch 167 loss: 0.2834338814377071
  batch 168 loss: 0.28345196330476374
  batch 169 loss: 0.2832172467158391
  batch 170 loss: 0.2831531668410582
  batch 171 loss: 0.28319708889687967
  batch 172 loss: 0.28311128013355785
  batch 173 loss: 0.28315317372366183
  batch 174 loss: 0.283154834961069
  batch 175 loss: 0.2833214489051274
  batch 176 loss: 0.283215048130263
  batch 177 loss: 0.2831910712234044
  batch 178 loss: 0.2833083739106575
  batch 179 loss: 0.2834341034875902
  batch 180 loss: 0.2833098381757736
  batch 181 loss: 0.28317835482444553
  batch 182 loss: 0.28322702187758225
  batch 183 loss: 0.2830158786369803
  batch 184 loss: 0.28279342899179977
  batch 185 loss: 0.2826984954041404
  batch 186 loss: 0.28285939182325076
  batch 187 loss: 0.2828222670179
  batch 188 loss: 0.2825206311300714
  batch 189 loss: 0.2823168755208374
  batch 190 loss: 0.28227650334960536
  batch 191 loss: 0.28221720457077026
  batch 192 loss: 0.28225804508353275
  batch 193 loss: 0.28229182974044525
  batch 194 loss: 0.2824905995548386
  batch 195 loss: 0.2825510138120407
  batch 196 loss: 0.28239958961399236
  batch 197 loss: 0.28227258832926677
  batch 198 loss: 0.2824039954428721
  batch 199 loss: 0.28250577222162754
  batch 200 loss: 0.28263535887002944
  batch 201 loss: 0.2825952488095013
  batch 202 loss: 0.282417724849564
  batch 203 loss: 0.2825409411062748
  batch 204 loss: 0.282439025520694
  batch 205 loss: 0.2826415442112016
  batch 206 loss: 0.2825720209664511
  batch 207 loss: 0.28255212688503634
  batch 208 loss: 0.28246513589356953
  batch 209 loss: 0.28239043969571875
  batch 210 loss: 0.28254973043998083
  batch 211 loss: 0.28251155441123726
  batch 212 loss: 0.2824400158001567
  batch 213 loss: 0.28244313100973767
  batch 214 loss: 0.28254071790202756
  batch 215 loss: 0.28235825210116633
  batch 216 loss: 0.28217679648487654
  batch 217 loss: 0.2821585835399716
  batch 218 loss: 0.28215660715321883
  batch 219 loss: 0.28212242025762935
  batch 220 loss: 0.28209461772983724
  batch 221 loss: 0.2823024471690752
  batch 222 loss: 0.2824210031880989
  batch 223 loss: 0.2825457950878571
  batch 224 loss: 0.28246571256646086
  batch 225 loss: 0.2822614052560594
  batch 226 loss: 0.2823929910638691
  batch 227 loss: 0.2821644643067263
  batch 228 loss: 0.2820730473388705
  batch 229 loss: 0.2819451067645477
  batch 230 loss: 0.28192679726559183
  batch 231 loss: 0.28180961291511336
  batch 232 loss: 0.2815456119976167
  batch 233 loss: 0.2815243025578143
  batch 234 loss: 0.28160443717343175
  batch 235 loss: 0.28181170015893087
  batch 236 loss: 0.28172838959400937
  batch 237 loss: 0.28182573817701784
  batch 238 loss: 0.28178357257812964
  batch 239 loss: 0.2817220175989502
  batch 240 loss: 0.2819187995667259
  batch 241 loss: 0.28208922542229725
  batch 242 loss: 0.2819626366427122
  batch 243 loss: 0.2820259470630575
  batch 244 loss: 0.28204296743039226
  batch 245 loss: 0.2820813550632827
  batch 246 loss: 0.28208299798936376
  batch 247 loss: 0.2821083221478983
  batch 248 loss: 0.282152502707416
  batch 249 loss: 0.28213871357192
  batch 250 loss: 0.2819976228475571
  batch 251 loss: 0.28200581836035526
  batch 252 loss: 0.2819409139809154
  batch 253 loss: 0.28179860880723584
  batch 254 loss: 0.2817362016580236
  batch 255 loss: 0.28182723755929984
  batch 256 loss: 0.28192951425444335
  batch 257 loss: 0.2820375515329235
  batch 258 loss: 0.2820886395467344
  batch 259 loss: 0.28217153279937834
  batch 260 loss: 0.2822548515521563
  batch 261 loss: 0.2823709073651339
  batch 262 loss: 0.28244251388174885
  batch 263 loss: 0.2825225549732324
  batch 264 loss: 0.2824315880284165
  batch 265 loss: 0.2824625396503592
  batch 266 loss: 0.282489215408949
  batch 267 loss: 0.2826399170280842
  batch 268 loss: 0.2826135985886873
  batch 269 loss: 0.28261419720809255
  batch 270 loss: 0.28282011570753873
  batch 271 loss: 0.2829467674023111
  batch 272 loss: 0.2829831031096332
  batch 273 loss: 0.28289654021298055
  batch 274 loss: 0.2830475704730862
  batch 275 loss: 0.28301704981110315
  batch 276 loss: 0.28297932230044104
  batch 277 loss: 0.2829438209103333
  batch 278 loss: 0.2829107214435399
  batch 279 loss: 0.2830210654966293
  batch 280 loss: 0.2828907338104078
  batch 281 loss: 0.28275194186120695
  batch 282 loss: 0.2827870286208518
  batch 283 loss: 0.2827431614849677
  batch 284 loss: 0.2827474324321243
  batch 285 loss: 0.28262662997371274
  batch 286 loss: 0.28260809782293295
  batch 287 loss: 0.2826399882599867
  batch 288 loss: 0.28251102721939486
  batch 289 loss: 0.28267980096249434
  batch 290 loss: 0.28245624211327786
  batch 291 loss: 0.282435378053344
  batch 292 loss: 0.28244255513769306
  batch 293 loss: 0.2823909628920181
  batch 294 loss: 0.28224008190794053
  batch 295 loss: 0.2822612959449574
  batch 296 loss: 0.2823498210794217
  batch 297 loss: 0.2822958404568309
  batch 298 loss: 0.2822920273014363
  batch 299 loss: 0.28225601457034466
  batch 300 loss: 0.28220209141572317
  batch 301 loss: 0.28207497036338247
  batch 302 loss: 0.282079453302535
  batch 303 loss: 0.2820470275068441
  batch 304 loss: 0.28204040503815603
  batch 305 loss: 0.28197265726621035
  batch 306 loss: 0.28203368790788585
  batch 307 loss: 0.2819675785710835
  batch 308 loss: 0.28211625588017625
  batch 309 loss: 0.2820998748529305
  batch 310 loss: 0.28201675895721684
  batch 311 loss: 0.28216115914740364
  batch 312 loss: 0.2822686223647533
  batch 313 loss: 0.28227074249103046
  batch 314 loss: 0.28229305167107066
  batch 315 loss: 0.2822534214882624
  batch 316 loss: 0.2821426844766623
  batch 317 loss: 0.28220656537671196
  batch 318 loss: 0.28215043429496156
  batch 319 loss: 0.28206977467738725
  batch 320 loss: 0.2820228660013527
  batch 321 loss: 0.2820599350695298
  batch 322 loss: 0.2820424197345787
  batch 323 loss: 0.2819192555297639
  batch 324 loss: 0.28167041598094833
  batch 325 loss: 0.28159286063451033
  batch 326 loss: 0.2815885563478148
  batch 327 loss: 0.2815594015591735
  batch 328 loss: 0.28137988092877514
  batch 329 loss: 0.2815213584338278
  batch 330 loss: 0.2814292642654795
  batch 331 loss: 0.28143390414397884
  batch 332 loss: 0.28140847811318304
  batch 333 loss: 0.28139021818165305
  batch 334 loss: 0.28131764435661055
  batch 335 loss: 0.28114465670799144
  batch 336 loss: 0.28100624547473024
  batch 337 loss: 0.2808377584470132
  batch 338 loss: 0.28079936675418765
  batch 339 loss: 0.2805756117302408
  batch 340 loss: 0.28042957120958495
  batch 341 loss: 0.280377034261779
  batch 342 loss: 0.2803533506410861
  batch 343 loss: 0.2804015630275098
  batch 344 loss: 0.2803592481325532
  batch 345 loss: 0.2805374641781268
  batch 346 loss: 0.2803728425003201
  batch 347 loss: 0.28043022163491427
  batch 348 loss: 0.2804382022513055
  batch 349 loss: 0.2804183923668711
  batch 350 loss: 0.2805506981696401
  batch 351 loss: 0.2805802112715876
  batch 352 loss: 0.28063449377871375
  batch 353 loss: 0.28069504356755753
  batch 354 loss: 0.28086739925846543
  batch 355 loss: 0.2808787375688553
  batch 356 loss: 0.2808314810057035
  batch 357 loss: 0.2807559327167623
  batch 358 loss: 0.2808005030178491
  batch 359 loss: 0.280773455759609
  batch 360 loss: 0.280888342567616
  batch 361 loss: 0.28081861802910835
  batch 362 loss: 0.2808001928484242
  batch 363 loss: 0.28072265205140284
  batch 364 loss: 0.28073606938942447
  batch 365 loss: 0.2808159214584795
  batch 366 loss: 0.28071817800484067
  batch 367 loss: 0.2806537624357182
  batch 368 loss: 0.28059771239919507
  batch 369 loss: 0.28048297504422465
  batch 370 loss: 0.280486132244806
  batch 371 loss: 0.2804534665015187
  batch 372 loss: 0.28040874637262797
  batch 373 loss: 0.28026758561824666
  batch 374 loss: 0.2800690685124958
  batch 375 loss: 0.28006787653764087
  batch 376 loss: 0.2801626774462614
  batch 377 loss: 0.2801422529337577
  batch 378 loss: 0.28007679866262214
  batch 379 loss: 0.28008618797507323
  batch 380 loss: 0.2801066686840434
  batch 381 loss: 0.2800281795225744
  batch 382 loss: 0.2800065451813618
  batch 383 loss: 0.2800023298584139
  batch 384 loss: 0.27994897569684934
  batch 385 loss: 0.2800350617665749
  batch 386 loss: 0.28000240626267203
  batch 387 loss: 0.2800231202859287
  batch 388 loss: 0.28006764958353386
  batch 389 loss: 0.2800194403889247
  batch 390 loss: 0.27993333354974403
  batch 391 loss: 0.2799922598292456
  batch 392 loss: 0.28001278692058157
  batch 393 loss: 0.28003281263904717
  batch 394 loss: 0.28005443339420455
  batch 395 loss: 0.27995992902713485
  batch 396 loss: 0.27992831565665477
  batch 397 loss: 0.27992592294060015
  batch 398 loss: 0.2798215600488773
  batch 399 loss: 0.2798734921635243
  batch 400 loss: 0.2799254121258855
  batch 401 loss: 0.2798352801517358
  batch 402 loss: 0.27985851396227357
  batch 403 loss: 0.2798843642987921
  batch 404 loss: 0.27990789139772404
  batch 405 loss: 0.27992278620784666
  batch 406 loss: 0.27998302579660134
  batch 407 loss: 0.27994188809131226
  batch 408 loss: 0.27998925340087977
  batch 409 loss: 0.2799157719230302
  batch 410 loss: 0.2799573030777094
  batch 411 loss: 0.27988644002272844
  batch 412 loss: 0.2798456022678648
  batch 413 loss: 0.2799747564241326
  batch 414 loss: 0.279941882164294
  batch 415 loss: 0.2799403228314526
  batch 416 loss: 0.2799165078008977
  batch 417 loss: 0.27983209362132944
  batch 418 loss: 0.27979422977381346
  batch 419 loss: 0.27982661697062217
  batch 420 loss: 0.27977545247191477
  batch 421 loss: 0.27969902520225054
  batch 422 loss: 0.2798695338987061
  batch 423 loss: 0.27986945863592994
  batch 424 loss: 0.27980477440188517
  batch 425 loss: 0.27980823832399704
  batch 426 loss: 0.27975907278172846
  batch 427 loss: 0.2797382379024872
  batch 428 loss: 0.27967528253793716
  batch 429 loss: 0.27966572799326933
  batch 430 loss: 0.2796047127524088
  batch 431 loss: 0.27964418780776024
  batch 432 loss: 0.27971797057048037
  batch 433 loss: 0.27969216095933186
  batch 434 loss: 0.2797841513486502
  batch 435 loss: 0.2796834296878727
  batch 436 loss: 0.27972358075577186
  batch 437 loss: 0.27980003279188403
  batch 438 loss: 0.2800187828470038
  batch 439 loss: 0.28000504436959983
  batch 440 loss: 0.28002918687733735
  batch 441 loss: 0.279993731013231
  batch 442 loss: 0.2800031135794264
  batch 443 loss: 0.2799821680340218
  batch 444 loss: 0.2799661592454524
  batch 445 loss: 0.28002133215411323
  batch 446 loss: 0.27996763005652237
  batch 447 loss: 0.2799029136264084
  batch 448 loss: 0.279937783056604
  batch 449 loss: 0.27992946088712306
  batch 450 loss: 0.2798612736662229
  batch 451 loss: 0.27981991101130677
  batch 452 loss: 0.2798299832615705
  batch 453 loss: 0.27985580995777587
  batch 454 loss: 0.27992741386139447
  batch 455 loss: 0.2798910723610239
  batch 456 loss: 0.2799029523800862
  batch 457 loss: 0.2799323544525735
  batch 458 loss: 0.27992676227764274
  batch 459 loss: 0.27988077343029655
  batch 460 loss: 0.28001624187049656
  batch 461 loss: 0.28002794145115545
  batch 462 loss: 0.28006519251442574
  batch 463 loss: 0.27999691394827536
  batch 464 loss: 0.280024827714881
  batch 465 loss: 0.2799841272895054
  batch 466 loss: 0.27985955435830634
  batch 467 loss: 0.27991363744408976
  batch 468 loss: 0.2798875442936889
  batch 469 loss: 0.27996614854981394
  batch 470 loss: 0.27999820303409656
  batch 471 loss: 0.279940987341976
  batch 472 loss: 0.27981270363522787
LOSS train 0.27981270363522787 valid 0.23728424310684204
LOSS train 0.27981270363522787 valid 0.2222341001033783
LOSS train 0.27981270363522787 valid 0.22829083104928335
LOSS train 0.27981270363522787 valid 0.2184329740703106
LOSS train 0.27981270363522787 valid 0.21334539949893952
LOSS train 0.27981270363522787 valid 0.21414079517126083
LOSS train 0.27981270363522787 valid 0.22553689777851105
LOSS train 0.27981270363522787 valid 0.22061113826930523
LOSS train 0.27981270363522787 valid 0.21974928345945147
LOSS train 0.27981270363522787 valid 0.22242760211229323
LOSS train 0.27981270363522787 valid 0.21991931037469345
LOSS train 0.27981270363522787 valid 0.21938453242182732
LOSS train 0.27981270363522787 valid 0.21774733983553374
LOSS train 0.27981270363522787 valid 0.21885863372257777
LOSS train 0.27981270363522787 valid 0.21420036256313324
LOSS train 0.27981270363522787 valid 0.21547095105051994
LOSS train 0.27981270363522787 valid 0.2159204684636172
LOSS train 0.27981270363522787 valid 0.21615062322881487
LOSS train 0.27981270363522787 valid 0.21812518019425242
LOSS train 0.27981270363522787 valid 0.2177836664021015
LOSS train 0.27981270363522787 valid 0.21754093042441777
LOSS train 0.27981270363522787 valid 0.21631028164516797
LOSS train 0.27981270363522787 valid 0.21728606975596884
LOSS train 0.27981270363522787 valid 0.21676605691512427
LOSS train 0.27981270363522787 valid 0.2145978969335556
LOSS train 0.27981270363522787 valid 0.2145248938065309
LOSS train 0.27981270363522787 valid 0.215204068907985
LOSS train 0.27981270363522787 valid 0.21660830080509186
LOSS train 0.27981270363522787 valid 0.2173474076493033
LOSS train 0.27981270363522787 valid 0.21784981489181518
LOSS train 0.27981270363522787 valid 0.21882802151864575
LOSS train 0.27981270363522787 valid 0.2188304797746241
LOSS train 0.27981270363522787 valid 0.22001742910255084
LOSS train 0.27981270363522787 valid 0.21975826910313437
LOSS train 0.27981270363522787 valid 0.22157494298049382
LOSS train 0.27981270363522787 valid 0.22138005702031982
LOSS train 0.27981270363522787 valid 0.2216795167407474
LOSS train 0.27981270363522787 valid 0.22235522380000666
LOSS train 0.27981270363522787 valid 0.22171785090214166
LOSS train 0.27981270363522787 valid 0.22220589369535446
LOSS train 0.27981270363522787 valid 0.2221610833231996
LOSS train 0.27981270363522787 valid 0.222382634523369
LOSS train 0.27981270363522787 valid 0.22177536508371665
LOSS train 0.27981270363522787 valid 0.22193732451308856
LOSS train 0.27981270363522787 valid 0.22158479392528535
LOSS train 0.27981270363522787 valid 0.2222276223094567
LOSS train 0.27981270363522787 valid 0.22250130201907867
LOSS train 0.27981270363522787 valid 0.22225235713024935
LOSS train 0.27981270363522787 valid 0.2227495793177157
LOSS train 0.27981270363522787 valid 0.22203880935907364
LOSS train 0.27981270363522787 valid 0.22244595283386753
LOSS train 0.27981270363522787 valid 0.22251972556114197
LOSS train 0.27981270363522787 valid 0.22262391785405716
LOSS train 0.27981270363522787 valid 0.223029809417548
LOSS train 0.27981270363522787 valid 0.22291175723075868
LOSS train 0.27981270363522787 valid 0.222853777131864
LOSS train 0.27981270363522787 valid 0.22288062122830174
LOSS train 0.27981270363522787 valid 0.22253979588377065
LOSS train 0.27981270363522787 valid 0.22323185710583704
LOSS train 0.27981270363522787 valid 0.22262212360898653
LOSS train 0.27981270363522787 valid 0.22232941330456343
LOSS train 0.27981270363522787 valid 0.2229740648500381
LOSS train 0.27981270363522787 valid 0.22313742055779412
LOSS train 0.27981270363522787 valid 0.22393822972662747
LOSS train 0.27981270363522787 valid 0.22386234540205735
LOSS train 0.27981270363522787 valid 0.22396780753677542
LOSS train 0.27981270363522787 valid 0.22320410030991283
LOSS train 0.27981270363522787 valid 0.22357609736568787
LOSS train 0.27981270363522787 valid 0.22261255914750305
LOSS train 0.27981270363522787 valid 0.2227522543498448
LOSS train 0.27981270363522787 valid 0.22233966433666122
LOSS train 0.27981270363522787 valid 0.22255248286657864
LOSS train 0.27981270363522787 valid 0.22262750280230012
LOSS train 0.27981270363522787 valid 0.22232376784086227
LOSS train 0.27981270363522787 valid 0.22223935147126517
LOSS train 0.27981270363522787 valid 0.2228060429425616
LOSS train 0.27981270363522787 valid 0.22245189063734822
LOSS train 0.27981270363522787 valid 0.22238568044625795
LOSS train 0.27981270363522787 valid 0.2224219067941738
LOSS train 0.27981270363522787 valid 0.22173834405839443
LOSS train 0.27981270363522787 valid 0.22075139390833584
LOSS train 0.27981270363522787 valid 0.22084701769962545
LOSS train 0.27981270363522787 valid 0.22062152677989869
LOSS train 0.27981270363522787 valid 0.22061690351083166
LOSS train 0.27981270363522787 valid 0.219914728753707
LOSS train 0.27981270363522787 valid 0.21915925068910733
LOSS train 0.27981270363522787 valid 0.21920466251756954
LOSS train 0.27981270363522787 valid 0.21885558251630177
LOSS train 0.27981270363522787 valid 0.2191405524028821
LOSS train 0.27981270363522787 valid 0.2193225392036968
LOSS train 0.27981270363522787 valid 0.21931937173172666
LOSS train 0.27981270363522787 valid 0.21926801246793373
LOSS train 0.27981270363522787 valid 0.21913573242002918
LOSS train 0.27981270363522787 valid 0.21955368176419685
LOSS train 0.27981270363522787 valid 0.2191062728041097
LOSS train 0.27981270363522787 valid 0.21909072327738008
LOSS train 0.27981270363522787 valid 0.21918722679934552
LOSS train 0.27981270363522787 valid 0.2195619347752357
LOSS train 0.27981270363522787 valid 0.21969923557657184
LOSS train 0.27981270363522787 valid 0.21990602776408197
LOSS train 0.27981270363522787 valid 0.22039826243820757
LOSS train 0.27981270363522787 valid 0.2206727418245054
LOSS train 0.27981270363522787 valid 0.22048902207786597
LOSS train 0.27981270363522787 valid 0.2201641067289389
LOSS train 0.27981270363522787 valid 0.22052070044335864
LOSS train 0.27981270363522787 valid 0.2207425800415705
LOSS train 0.27981270363522787 valid 0.22028976943448325
LOSS train 0.27981270363522787 valid 0.22033112772085048
LOSS train 0.27981270363522787 valid 0.22071869356916585
LOSS train 0.27981270363522787 valid 0.22104942771521482
LOSS train 0.27981270363522787 valid 0.22066048284371695
LOSS train 0.27981270363522787 valid 0.22063814835356815
LOSS train 0.27981270363522787 valid 0.22071352812041223
LOSS train 0.27981270363522787 valid 0.22058103468857312
LOSS train 0.27981270363522787 valid 0.22067782036636185
LOSS train 0.27981270363522787 valid 0.22112443927547026
LOSS train 0.27981270363522787 valid 0.22141410945317683
LOSS train 0.27981270363522787 valid 0.22137799290782315
LOSS train 0.27981270363522787 valid 0.22119752424103872
LOSS train 0.27981270363522787 valid 0.22102785632014274
LOSS train 0.27981270363522787 valid 0.22086424977818797
LOSS train 0.27981270363522787 valid 0.2208610063449281
LOSS train 0.27981270363522787 valid 0.22100390990575156
LOSS train 0.27981270363522787 valid 0.2212023141403352
LOSS train 0.27981270363522787 valid 0.22113426196575164
LOSS train 0.27981270363522787 valid 0.2212381191433422
LOSS train 0.27981270363522787 valid 0.2212347722663654
LOSS train 0.27981270363522787 valid 0.22131196700502187
LOSS train 0.27981270363522787 valid 0.22160669849362485
LOSS train 0.27981270363522787 valid 0.22154315698605317
LOSS train 0.27981270363522787 valid 0.2214890575135937
LOSS train 0.27981270363522787 valid 0.22114598355961568
LOSS train 0.27981270363522787 valid 0.22087082838205466
LOSS train 0.27981270363522787 valid 0.22111206946533118
LOSS train 0.27981270363522787 valid 0.2210678470355493
LOSS train 0.27981270363522787 valid 0.2207943294197321
LOSS train 0.27981270363522787 valid 0.2205810681746824
LOSS train 0.27981270363522787 valid 0.22044039539236954
LOSS train 0.27981270363522787 valid 0.22022572384892608
LOSS train 0.27981270363522787 valid 0.2203787972884519
LOSS train 0.27981270363522787 valid 0.2205979932918616
LOSS train 0.27981270363522787 valid 0.2207343350623695
LOSS train 0.27981270363522787 valid 0.22097201082673107
LOSS train 0.27981270363522787 valid 0.22109906934201717
LOSS train 0.27981270363522787 valid 0.22090023273024065
LOSS train 0.27981270363522787 valid 0.22095820471032024
LOSS train 0.27981270363522787 valid 0.22076787297822992
LOSS train 0.27981270363522787 valid 0.22155622322414373
LOSS train 0.27981270363522787 valid 0.22165079554855424
LOSS train 0.27981270363522787 valid 0.22154310365517935
LOSS train 0.27981270363522787 valid 0.22172911119776845
LOSS train 0.27981270363522787 valid 0.22145460036240125
LOSS train 0.27981270363522787 valid 0.2215850103914348
LOSS train 0.27981270363522787 valid 0.22166519718510763
LOSS train 0.27981270363522787 valid 0.2216042673395526
LOSS train 0.27981270363522787 valid 0.22189100401905867
LOSS train 0.27981270363522787 valid 0.22185653903681762
LOSS train 0.27981270363522787 valid 0.22190411917016475
LOSS train 0.27981270363522787 valid 0.22195028977574044
LOSS train 0.27981270363522787 valid 0.2217087961733341
LOSS train 0.27981270363522787 valid 0.2216528081190512
LOSS train 0.27981270363522787 valid 0.22148203822197737
LOSS train 0.27981270363522787 valid 0.22137881705731702
LOSS train 0.27981270363522787 valid 0.22118661470892953
LOSS train 0.27981270363522787 valid 0.22107942095308591
LOSS train 0.27981270363522787 valid 0.22103901315166288
LOSS train 0.27981270363522787 valid 0.22119941914866784
LOSS train 0.27981270363522787 valid 0.22101552074863798
LOSS train 0.27981270363522787 valid 0.22110995826636545
LOSS train 0.27981270363522787 valid 0.22131383909898647
LOSS train 0.27981270363522787 valid 0.22120820665568636
LOSS train 0.27981270363522787 valid 0.2211244703205519
LOSS train 0.27981270363522787 valid 0.2210427007578701
LOSS train 0.27981270363522787 valid 0.2210543242165412
LOSS train 0.27981270363522787 valid 0.2209841230085918
LOSS train 0.27981270363522787 valid 0.22110326376489617
LOSS train 0.27981270363522787 valid 0.22097921430391107
LOSS train 0.27981270363522787 valid 0.2210902733246932
LOSS train 0.27981270363522787 valid 0.22104148400229448
LOSS train 0.27981270363522787 valid 0.22101831419600382
LOSS train 0.27981270363522787 valid 0.22110804437932388
LOSS train 0.27981270363522787 valid 0.22118800567401634
LOSS train 0.27981270363522787 valid 0.22115654290699568
LOSS train 0.27981270363522787 valid 0.2212856594959031
LOSS train 0.27981270363522787 valid 0.22112475351707356
LOSS train 0.27981270363522787 valid 0.22109476973613104
LOSS train 0.27981270363522787 valid 0.22094967674444066
LOSS train 0.27981270363522787 valid 0.22091900644467233
LOSS train 0.27981270363522787 valid 0.2209115830994157
LOSS train 0.27981270363522787 valid 0.22117754283704255
LOSS train 0.27981270363522787 valid 0.22108662690167652
LOSS train 0.27981270363522787 valid 0.22112975285078088
LOSS train 0.27981270363522787 valid 0.22121499455654559
LOSS train 0.27981270363522787 valid 0.2211294413841877
LOSS train 0.27981270363522787 valid 0.22087062826523415
LOSS train 0.27981270363522787 valid 0.22094330548936006
LOSS train 0.27981270363522787 valid 0.22109852292513485
LOSS train 0.27981270363522787 valid 0.22093754735859958
LOSS train 0.27981270363522787 valid 0.22099857504044346
LOSS train 0.27981270363522787 valid 0.22095242619514466
LOSS train 0.27981270363522787 valid 0.22079399072412234
LOSS train 0.27981270363522787 valid 0.22075769718330684
LOSS train 0.27981270363522787 valid 0.22064999046877687
LOSS train 0.27981270363522787 valid 0.220608905176906
LOSS train 0.27981270363522787 valid 0.2204417643750586
LOSS train 0.27981270363522787 valid 0.22035760852029024
LOSS train 0.27981270363522787 valid 0.2203338668830153
LOSS train 0.27981270363522787 valid 0.22026021864551765
LOSS train 0.27981270363522787 valid 0.22017655838904768
LOSS train 0.27981270363522787 valid 0.22022980997959773
LOSS train 0.27981270363522787 valid 0.22040104901338642
LOSS train 0.27981270363522787 valid 0.22031892955584345
LOSS train 0.27981270363522787 valid 0.22040793859343014
LOSS train 0.27981270363522787 valid 0.22029836649928136
LOSS train 0.27981270363522787 valid 0.2201027260963307
LOSS train 0.27981270363522787 valid 0.22002382445390578
LOSS train 0.27981270363522787 valid 0.2198424778088996
LOSS train 0.27981270363522787 valid 0.21986849189078042
LOSS train 0.27981270363522787 valid 0.21995867162806804
LOSS train 0.27981270363522787 valid 0.22007276510650461
LOSS train 0.27981270363522787 valid 0.2201358540430328
LOSS train 0.27981270363522787 valid 0.22003998708080602
LOSS train 0.27981270363522787 valid 0.2201405997500826
LOSS train 0.27981270363522787 valid 0.22024871156151807
LOSS train 0.27981270363522787 valid 0.2202788030438953
LOSS train 0.27981270363522787 valid 0.2203498056768316
LOSS train 0.27981270363522787 valid 0.220544983243102
LOSS train 0.27981270363522787 valid 0.22062152584916667
LOSS train 0.27981270363522787 valid 0.2206440361045854
LOSS train 0.27981270363522787 valid 0.22079362700814786
LOSS train 0.27981270363522787 valid 0.22082707608417
LOSS train 0.27981270363522787 valid 0.22090240895491223
LOSS train 0.27981270363522787 valid 0.22085297721649955
LOSS train 0.27981270363522787 valid 0.22085554025366774
LOSS train 0.27981270363522787 valid 0.22097044574453475
LOSS train 0.27981270363522787 valid 0.22080188396118455
LOSS train 0.27981270363522787 valid 0.22095208064916264
LOSS train 0.27981270363522787 valid 0.22083343853469656
LOSS train 0.27981270363522787 valid 0.22077586754345993
LOSS train 0.27981270363522787 valid 0.2207506535574794
LOSS train 0.27981270363522787 valid 0.22085258265748559
LOSS train 0.27981270363522787 valid 0.22062416368525875
LOSS train 0.27981270363522787 valid 0.22071019704204528
LOSS train 0.27981270363522787 valid 0.22092894849474312
LOSS train 0.27981270363522787 valid 0.22106033399397013
LOSS train 0.27981270363522787 valid 0.22102575955109868
LOSS train 0.27981270363522787 valid 0.22103011722748095
LOSS train 0.27981270363522787 valid 0.2210028396258431
LOSS train 0.27981270363522787 valid 0.2208704159681098
LOSS train 0.27981270363522787 valid 0.22103592526912688
LOSS train 0.27981270363522787 valid 0.22099023392475933
LOSS train 0.27981270363522787 valid 0.22092116263414185
LOSS train 0.27981270363522787 valid 0.22080552990257504
LOSS train 0.27981270363522787 valid 0.22075617483516377
LOSS train 0.27981270363522787 valid 0.22080873677543567
LOSS train 0.27981270363522787 valid 0.22078092105221003
LOSS train 0.27981270363522787 valid 0.22065220436929264
LOSS train 0.27981270363522787 valid 0.22077640330837678
LOSS train 0.27981270363522787 valid 0.2208209006137369
LOSS train 0.27981270363522787 valid 0.22081660851836205
LOSS train 0.27981270363522787 valid 0.22096593461045816
LOSS train 0.27981270363522787 valid 0.22093754807728846
LOSS train 0.27981270363522787 valid 0.2209810677476709
LOSS train 0.27981270363522787 valid 0.22094903773430621
LOSS train 0.27981270363522787 valid 0.22098781995053562
LOSS train 0.27981270363522787 valid 0.22099136230640842
LOSS train 0.27981270363522787 valid 0.22105082110519267
LOSS train 0.27981270363522787 valid 0.2211127896139871
LOSS train 0.27981270363522787 valid 0.2213503038129842
LOSS train 0.27981270363522787 valid 0.22149914348566974
LOSS train 0.27981270363522787 valid 0.22158189079418394
LOSS train 0.27981270363522787 valid 0.2218376497792847
LOSS train 0.27981270363522787 valid 0.22186120121906966
LOSS train 0.27981270363522787 valid 0.22176223764889433
LOSS train 0.27981270363522787 valid 0.2217755814032121
LOSS train 0.27981270363522787 valid 0.2216477719122085
LOSS train 0.27981270363522787 valid 0.2214851975978927
LOSS train 0.27981270363522787 valid 0.22129837230598326
LOSS train 0.27981270363522787 valid 0.22133547986280105
LOSS train 0.27981270363522787 valid 0.22121436659778868
LOSS train 0.27981270363522787 valid 0.22107973514502582
LOSS train 0.27981270363522787 valid 0.2209804173796735
LOSS train 0.27981270363522787 valid 0.22098367521703874
LOSS train 0.27981270363522787 valid 0.2210274348166627
LOSS train 0.27981270363522787 valid 0.221049541025831
LOSS train 0.27981270363522787 valid 0.2209535424525921
LOSS train 0.27981270363522787 valid 0.22086942133587828
LOSS train 0.27981270363522787 valid 0.22091259114030334
LOSS train 0.27981270363522787 valid 0.22089368347271915
LOSS train 0.27981270363522787 valid 0.22101300086440712
LOSS train 0.27981270363522787 valid 0.22089278974483922
LOSS train 0.27981270363522787 valid 0.2209398570942552
LOSS train 0.27981270363522787 valid 0.22090107370358686
LOSS train 0.27981270363522787 valid 0.22109417642663126
LOSS train 0.27981270363522787 valid 0.22115888216737972
LOSS train 0.27981270363522787 valid 0.22105747607310075
LOSS train 0.27981270363522787 valid 0.22104437403405958
LOSS train 0.27981270363522787 valid 0.22092902585364027
LOSS train 0.27981270363522787 valid 0.22098515608836974
LOSS train 0.27981270363522787 valid 0.22089503690600396
LOSS train 0.27981270363522787 valid 0.22104403975596063
LOSS train 0.27981270363522787 valid 0.22106314973523286
LOSS train 0.27981270363522787 valid 0.22114443370808076
LOSS train 0.27981270363522787 valid 0.22115977593746625
LOSS train 0.27981270363522787 valid 0.22104010596627094
LOSS train 0.27981270363522787 valid 0.22104336837537927
LOSS train 0.27981270363522787 valid 0.22105040908442258
LOSS train 0.27981270363522787 valid 0.2210365632040934
LOSS train 0.27981270363522787 valid 0.22108601752222548
LOSS train 0.27981270363522787 valid 0.22097185705938646
LOSS train 0.27981270363522787 valid 0.22086537464063652
LOSS train 0.27981270363522787 valid 0.22093964783618084
LOSS train 0.27981270363522787 valid 0.22092364128595723
LOSS train 0.27981270363522787 valid 0.22101760883430008
LOSS train 0.27981270363522787 valid 0.22103590468565623
LOSS train 0.27981270363522787 valid 0.22101654168925708
LOSS train 0.27981270363522787 valid 0.2210395506315803
LOSS train 0.27981270363522787 valid 0.221140223104249
LOSS train 0.27981270363522787 valid 0.22115633872609153
LOSS train 0.27981270363522787 valid 0.22105752984061838
LOSS train 0.27981270363522787 valid 0.22111782487307755
LOSS train 0.27981270363522787 valid 0.22108816017645486
LOSS train 0.27981270363522787 valid 0.22110994427750355
LOSS train 0.27981270363522787 valid 0.22115574216033207
LOSS train 0.27981270363522787 valid 0.22121003513152782
LOSS train 0.27981270363522787 valid 0.22131859602555176
LOSS train 0.27981270363522787 valid 0.22139463133950482
LOSS train 0.27981270363522787 valid 0.22139533927164426
LOSS train 0.27981270363522787 valid 0.22158927386654909
LOSS train 0.27981270363522787 valid 0.22153075712196754
LOSS train 0.27981270363522787 valid 0.22143144086228397
LOSS train 0.27981270363522787 valid 0.2213974281786436
LOSS train 0.27981270363522787 valid 0.22139559905450265
LOSS train 0.27981270363522787 valid 0.22151476010947885
LOSS train 0.27981270363522787 valid 0.2215714315869915
LOSS train 0.27981270363522787 valid 0.2215861106912295
LOSS train 0.27981270363522787 valid 0.22160432596058097
LOSS train 0.27981270363522787 valid 0.22157606323795206
LOSS train 0.27981270363522787 valid 0.2215595131075136
LOSS train 0.27981270363522787 valid 0.2215901255607605
LOSS train 0.27981270363522787 valid 0.221582286431293
LOSS train 0.27981270363522787 valid 0.22152732678672724
LOSS train 0.27981270363522787 valid 0.22146374425978424
LOSS train 0.27981270363522787 valid 0.22168529280569665
LOSS train 0.27981270363522787 valid 0.2216794410045596
LOSS train 0.27981270363522787 valid 0.22162498968701833
LOSS train 0.27981270363522787 valid 0.2215856791659116
LOSS train 0.27981270363522787 valid 0.22148340062677174
LOSS train 0.27981270363522787 valid 0.22153407745170048
LOSS train 0.27981270363522787 valid 0.22147252372332982
LOSS train 0.27981270363522787 valid 0.22137149217461588
LOSS train 0.27981270363522787 valid 0.22139830531721766
LOSS train 0.27981270363522787 valid 0.22143057558590562
LOSS train 0.27981270363522787 valid 0.22139246286140324
LOSS train 0.27981270363522787 valid 0.22157786400385307
LOSS train 0.27981270363522787 valid 0.22160488630781014
LOSS train 0.27981270363522787 valid 0.2215592917524466
LOSS train 0.27981270363522787 valid 0.22150951561481594
LOSS train 0.27981270363522787 valid 0.22152673683458715
LOSS train 0.27981270363522787 valid 0.22147556493679682
LOSS train 0.27981270363522787 valid 0.22141552953838972
LOSS train 0.27981270363522787 valid 0.22147665201629724
LOSS train 0.27981270363522787 valid 0.22125200972412573
LOSS train 0.27981270363522787 valid 0.22130098419038804
LOSS train 0.27981270363522787 valid 0.22133740982780717
LOSS train 0.27981270363522787 valid 0.22128101177717166
LOSS train 0.27981270363522787 valid 0.2210862995899341
LOSS train 0.27981270363522787 valid 0.22104723055077635
LOSS train 0.27981270363522787 valid 0.22107263442462052
EPOCH 9:
  batch 1 loss: 0.2515305280685425
  batch 2 loss: 0.24516741186380386
  batch 3 loss: 0.2525814523299535
  batch 4 loss: 0.25630388781428337
  batch 5 loss: 0.27102459967136383
  batch 6 loss: 0.2697806879878044
  batch 7 loss: 0.27103756793907713
  batch 8 loss: 0.27452355064451694
  batch 9 loss: 0.2729146265321308
  batch 10 loss: 0.27398181408643724
  batch 11 loss: 0.2722987627441233
  batch 12 loss: 0.270524771263202
  batch 13 loss: 0.26867379477390874
  batch 14 loss: 0.2685611407671656
  batch 15 loss: 0.2696174830198288
  batch 16 loss: 0.26925724651664495
  batch 17 loss: 0.2667376846075058
  batch 18 loss: 0.2697523824042744
  batch 19 loss: 0.26844301898228495
  batch 20 loss: 0.2666154406964779
  batch 21 loss: 0.2669785086597715
  batch 22 loss: 0.26758723841472104
  batch 23 loss: 0.2675945169251898
  batch 24 loss: 0.2660351029286782
  batch 25 loss: 0.2674938064813614
  batch 26 loss: 0.2661873245468506
  batch 27 loss: 0.2664313807531639
  batch 28 loss: 0.2652119185243334
  batch 29 loss: 0.26545919940389434
  batch 30 loss: 0.26468431651592256
  batch 31 loss: 0.265743471922413
  batch 32 loss: 0.2646432304754853
  batch 33 loss: 0.2649427643328002
  batch 34 loss: 0.264995524111916
  batch 35 loss: 0.26478907976831706
  batch 36 loss: 0.26536983996629715
  batch 37 loss: 0.2667593496876794
  batch 38 loss: 0.26758630024759394
  batch 39 loss: 0.26719144521615446
  batch 40 loss: 0.2674550339579582
  batch 41 loss: 0.267964986039371
  batch 42 loss: 0.26825291982718874
  batch 43 loss: 0.26872143565222273
  batch 44 loss: 0.2688266560435295
  batch 45 loss: 0.2680214719639884
  batch 46 loss: 0.26752137263184006
  batch 47 loss: 0.2676686260928499
  batch 48 loss: 0.26707464052985114
  batch 49 loss: 0.26650611812971076
  batch 50 loss: 0.26652784913778305
  batch 51 loss: 0.2661183701426375
  batch 52 loss: 0.26612935530451626
  batch 53 loss: 0.2658776840511358
  batch 54 loss: 0.2659470651436735
  batch 55 loss: 0.2659464101899754
  batch 56 loss: 0.26602654930736336
  batch 57 loss: 0.2658556446171643
  batch 58 loss: 0.26626773152885763
  batch 59 loss: 0.26647610103679914
  batch 60 loss: 0.2663228683173656
  batch 61 loss: 0.2669638456379781
  batch 62 loss: 0.26741140095456956
  batch 63 loss: 0.26729680313950493
  batch 64 loss: 0.26822858420200646
  batch 65 loss: 0.2678143164286247
  batch 66 loss: 0.2681194758324912
  batch 67 loss: 0.2683886451952493
  batch 68 loss: 0.2691802704597221
  batch 69 loss: 0.26882969102133875
  batch 70 loss: 0.26906465015241077
  batch 71 loss: 0.2686574622778825
  batch 72 loss: 0.2688225727114413
  batch 73 loss: 0.268843656125134
  batch 74 loss: 0.2691193613651636
  batch 75 loss: 0.2687183739741643
  batch 76 loss: 0.26926761493086815
  batch 77 loss: 0.26884263250734897
  batch 78 loss: 0.2688487076606506
  batch 79 loss: 0.26964962784248064
  batch 80 loss: 0.26923110000789163
  batch 81 loss: 0.26925717496577606
  batch 82 loss: 0.26954720296510837
  batch 83 loss: 0.26956580094544286
  batch 84 loss: 0.26955222125564304
  batch 85 loss: 0.2690001301905688
  batch 86 loss: 0.2695733041957367
  batch 87 loss: 0.2696474537082102
  batch 88 loss: 0.26947786151008174
  batch 89 loss: 0.26945611667097286
  batch 90 loss: 0.26955530908372666
  batch 91 loss: 0.26999697449443105
  batch 92 loss: 0.27039914383836416
  batch 93 loss: 0.27059772162027257
  batch 94 loss: 0.27119355728017525
  batch 95 loss: 0.27114276415423344
  batch 96 loss: 0.27144869851569337
  batch 97 loss: 0.2719582199435873
  batch 98 loss: 0.2724065111607921
  batch 99 loss: 0.27276063597563543
  batch 100 loss: 0.2725813341140747
  batch 101 loss: 0.2722856417150781
  batch 102 loss: 0.27243713976121414
  batch 103 loss: 0.2729786897747262
  batch 104 loss: 0.27293258561537814
  batch 105 loss: 0.2728977367991493
  batch 106 loss: 0.2731865886247383
  batch 107 loss: 0.2726938916701023
  batch 108 loss: 0.27257765526020966
  batch 109 loss: 0.2722805128731859
  batch 110 loss: 0.27240655747326936
  batch 111 loss: 0.27231159833100466
  batch 112 loss: 0.27202791773847174
  batch 113 loss: 0.27203323645929317
  batch 114 loss: 0.27235240528458043
  batch 115 loss: 0.27227604130040045
  batch 116 loss: 0.27220928360675944
  batch 117 loss: 0.2722792396178612
  batch 118 loss: 0.2718526363625365
  batch 119 loss: 0.2719446316737087
  batch 120 loss: 0.27171142511069774
  batch 121 loss: 0.27151320960896075
  batch 122 loss: 0.2713453518074067
  batch 123 loss: 0.27135942428092646
  batch 124 loss: 0.2716774272341882
  batch 125 loss: 0.27164921355247496
  batch 126 loss: 0.2717103222532878
  batch 127 loss: 0.2721964817816817
  batch 128 loss: 0.2719541434198618
  batch 129 loss: 0.27225581227346907
  batch 130 loss: 0.27221493331285623
  batch 131 loss: 0.2723368460895451
  batch 132 loss: 0.27231323764179693
  batch 133 loss: 0.2725426919926378
  batch 134 loss: 0.2726846664254345
  batch 135 loss: 0.27264938641477515
  batch 136 loss: 0.2726199968772776
  batch 137 loss: 0.27254076943780386
  batch 138 loss: 0.27246524631113245
  batch 139 loss: 0.2729074633378777
  batch 140 loss: 0.2727729143840926
  batch 141 loss: 0.2726938754108781
  batch 142 loss: 0.2727206615914761
  batch 143 loss: 0.2724903433264552
  batch 144 loss: 0.2722892475624879
  batch 145 loss: 0.27200609137272014
  batch 146 loss: 0.27218708538845793
  batch 147 loss: 0.27270149596694376
  batch 148 loss: 0.27261349357463216
  batch 149 loss: 0.27254922397984754
  batch 150 loss: 0.27276291688283283
  batch 151 loss: 0.2728864961112572
  batch 152 loss: 0.2730609449116807
  batch 153 loss: 0.2730712341327293
  batch 154 loss: 0.2735699639691935
  batch 155 loss: 0.2736381000088107
  batch 156 loss: 0.2736409457448201
  batch 157 loss: 0.27381631390304323
  batch 158 loss: 0.273983044526245
  batch 159 loss: 0.2744183011774747
  batch 160 loss: 0.2742247238755226
  batch 161 loss: 0.2743222150743378
  batch 162 loss: 0.2743816844843052
  batch 163 loss: 0.2744541985491302
  batch 164 loss: 0.2744243993628316
  batch 165 loss: 0.2744004901611444
  batch 166 loss: 0.2743939838136535
  batch 167 loss: 0.27422722308578606
  batch 168 loss: 0.27413222131629783
  batch 169 loss: 0.27389957169456597
  batch 170 loss: 0.2738863565465983
  batch 171 loss: 0.2739453253864545
  batch 172 loss: 0.27394736358939215
  batch 173 loss: 0.27395382480469743
  batch 174 loss: 0.2740423918284219
  batch 175 loss: 0.274122936470168
  batch 176 loss: 0.27401645422320475
  batch 177 loss: 0.2739045489474205
  batch 178 loss: 0.27407595188765044
  batch 179 loss: 0.27421398477514364
  batch 180 loss: 0.274132344375054
  batch 181 loss: 0.2740065288313186
  batch 182 loss: 0.27407361755331794
  batch 183 loss: 0.2738275286929855
  batch 184 loss: 0.2736335601657629
  batch 185 loss: 0.2735600826708046
  batch 186 loss: 0.2736732188251711
  batch 187 loss: 0.273571026596156
  batch 188 loss: 0.27325964703204786
  batch 189 loss: 0.27304711866946446
  batch 190 loss: 0.2729678859051905
  batch 191 loss: 0.27291477966995137
  batch 192 loss: 0.27309892210178077
  batch 193 loss: 0.2732510732708817
  batch 194 loss: 0.2734582537074679
  batch 195 loss: 0.2735778257632867
  batch 196 loss: 0.2734521491339012
  batch 197 loss: 0.2733914063212835
  batch 198 loss: 0.27356653193933794
  batch 199 loss: 0.2736756144905809
  batch 200 loss: 0.27369095526635645
  batch 201 loss: 0.2736826920835533
  batch 202 loss: 0.2735712317989604
  batch 203 loss: 0.2735794248986127
  batch 204 loss: 0.273441331059325
  batch 205 loss: 0.2735242057137373
  batch 206 loss: 0.27340886344030063
  batch 207 loss: 0.27341407393487754
  batch 208 loss: 0.27330673657930815
  batch 209 loss: 0.2731481960943441
  batch 210 loss: 0.27322372574181786
  batch 211 loss: 0.27320579718357013
  batch 212 loss: 0.2731784944123817
  batch 213 loss: 0.2731716605559201
  batch 214 loss: 0.2732380872992712
  batch 215 loss: 0.27301629677761435
  batch 216 loss: 0.2728153670689574
  batch 217 loss: 0.27278738671458813
  batch 218 loss: 0.27280711409969066
  batch 219 loss: 0.272780967425538
  batch 220 loss: 0.2727136771109971
  batch 221 loss: 0.2728657978017945
  batch 222 loss: 0.27291457375159134
  batch 223 loss: 0.2731056581564548
  batch 224 loss: 0.2731495685210185
  batch 225 loss: 0.27301595979266696
  batch 226 loss: 0.2731360580805129
  batch 227 loss: 0.27292133710195315
  batch 228 loss: 0.27284866708673927
  batch 229 loss: 0.27274418189536015
  batch 230 loss: 0.272679862898329
  batch 231 loss: 0.27256481549440525
  batch 232 loss: 0.2722825427250615
  batch 233 loss: 0.2722579569519845
  batch 234 loss: 0.27240580651495194
  batch 235 loss: 0.2724836731210668
  batch 236 loss: 0.27242236496028255
  batch 237 loss: 0.2725115394541986
  batch 238 loss: 0.2724270154448116
  batch 239 loss: 0.2723964304355398
  batch 240 loss: 0.2725837405771017
  batch 241 loss: 0.2727298079931884
  batch 242 loss: 0.27261474402236546
  batch 243 loss: 0.27278815479680835
  batch 244 loss: 0.27283760435024246
  batch 245 loss: 0.27291665058963155
  batch 246 loss: 0.2728990661056061
  batch 247 loss: 0.2730251779319786
  batch 248 loss: 0.2731302180477688
  batch 249 loss: 0.2730812772091134
  batch 250 loss: 0.27299527126550677
  batch 251 loss: 0.27302825599315156
  batch 252 loss: 0.2728861327327433
  batch 253 loss: 0.27266889486623846
  batch 254 loss: 0.27259419348061553
  batch 255 loss: 0.27264584624299815
  batch 256 loss: 0.2726878516259603
  batch 257 loss: 0.27270668843608886
  batch 258 loss: 0.2727754986447881
  batch 259 loss: 0.2728037957864378
  batch 260 loss: 0.27282282222922033
  batch 261 loss: 0.272911817691792
  batch 262 loss: 0.272912155182762
  batch 263 loss: 0.2729728236057912
  batch 264 loss: 0.2728852357602481
  batch 265 loss: 0.27285118440412126
  batch 266 loss: 0.27285096385425195
  batch 267 loss: 0.27296410339155447
  batch 268 loss: 0.272922625292593
  batch 269 loss: 0.2729068227860121
  batch 270 loss: 0.27319028708669874
  batch 271 loss: 0.27329549655263274
  batch 272 loss: 0.27334960385718765
  batch 273 loss: 0.2732987329636738
  batch 274 loss: 0.273479137542474
  batch 275 loss: 0.273485667813908
  batch 276 loss: 0.27348133003798086
  batch 277 loss: 0.2734956152817833
  batch 278 loss: 0.27357415490442044
  batch 279 loss: 0.27371840141580095
  batch 280 loss: 0.2735987185367516
  batch 281 loss: 0.2734451384094686
  batch 282 loss: 0.27346680212950875
  batch 283 loss: 0.27341766344784846
  batch 284 loss: 0.2734296800385059
  batch 285 loss: 0.2733158179542475
  batch 286 loss: 0.2733701015269006
  batch 287 loss: 0.2734118345930186
  batch 288 loss: 0.2732552631965114
  batch 289 loss: 0.27346834215532123
  batch 290 loss: 0.27329569767261375
  batch 291 loss: 0.2732609541145797
  batch 292 loss: 0.2733283880842875
  batch 293 loss: 0.27329469988370514
  batch 294 loss: 0.2731224530104066
  batch 295 loss: 0.2731925391039606
  batch 296 loss: 0.27329601946513393
  batch 297 loss: 0.2732544598053602
  batch 298 loss: 0.2732747847701879
  batch 299 loss: 0.273253971966214
  batch 300 loss: 0.27328232740362485
  batch 301 loss: 0.27313066489276694
  batch 302 loss: 0.2731364798664257
  batch 303 loss: 0.27314443861690685
  batch 304 loss: 0.2731343451887369
  batch 305 loss: 0.27306654169911243
  batch 306 loss: 0.2731490249142927
  batch 307 loss: 0.2730860924681934
  batch 308 loss: 0.2731928132571183
  batch 309 loss: 0.2731861502993068
  batch 310 loss: 0.2731147546922007
  batch 311 loss: 0.2732426451333466
  batch 312 loss: 0.2733604475282706
  batch 313 loss: 0.27334535369476953
  batch 314 loss: 0.27338824634719044
  batch 315 loss: 0.2733744784953102
  batch 316 loss: 0.2732563674732854
  batch 317 loss: 0.2733650966679636
  batch 318 loss: 0.27335245986966966
  batch 319 loss: 0.27329558250859237
  batch 320 loss: 0.27323778192512693
  batch 321 loss: 0.27327519854838234
  batch 322 loss: 0.27320675065983896
  batch 323 loss: 0.2730796147057146
  batch 324 loss: 0.27285500868787
  batch 325 loss: 0.27281461876172286
  batch 326 loss: 0.27280933556198345
  batch 327 loss: 0.2727754922179272
  batch 328 loss: 0.2726292572915554
  batch 329 loss: 0.27275821331061856
  batch 330 loss: 0.2726371494658066
  batch 331 loss: 0.2726541848071032
  batch 332 loss: 0.2726253452879119
  batch 333 loss: 0.2725958101473771
  batch 334 loss: 0.27249867834909236
  batch 335 loss: 0.2723243473181084
  batch 336 loss: 0.2722039469739511
  batch 337 loss: 0.2720259949786019
  batch 338 loss: 0.2719768918830262
  batch 339 loss: 0.271782753609978
  batch 340 loss: 0.27162042952635707
  batch 341 loss: 0.2716262943583849
  batch 342 loss: 0.2715998201342354
  batch 343 loss: 0.2715976053180917
  batch 344 loss: 0.27152633398424747
  batch 345 loss: 0.2717238163602525
  batch 346 loss: 0.27154582557995194
  batch 347 loss: 0.27162788494863155
  batch 348 loss: 0.271645142086621
  batch 349 loss: 0.2716360560119323
  batch 350 loss: 0.27175491222313475
  batch 351 loss: 0.2717763892775587
  batch 352 loss: 0.2718595357781107
  batch 353 loss: 0.27192648198381697
  batch 354 loss: 0.27212850800005056
  batch 355 loss: 0.27215997484368337
  batch 356 loss: 0.27209772010532657
  batch 357 loss: 0.27199035682598083
  batch 358 loss: 0.27205379340568736
  batch 359 loss: 0.2720283616054025
  batch 360 loss: 0.2721407160990768
  batch 361 loss: 0.27205032300090526
  batch 362 loss: 0.27206913649377246
  batch 363 loss: 0.27200419613809296
  batch 364 loss: 0.27200348331378055
  batch 365 loss: 0.2720365880286857
  batch 366 loss: 0.27196670160756087
  batch 367 loss: 0.27189087165311504
  batch 368 loss: 0.27181660865797946
  batch 369 loss: 0.2717011828167329
  batch 370 loss: 0.27171282144011677
  batch 371 loss: 0.2717220394839495
  batch 372 loss: 0.27168115544863924
  batch 373 loss: 0.2715166591047921
  batch 374 loss: 0.2713455072539375
  batch 375 loss: 0.2713280906677246
  batch 376 loss: 0.2714135759371392
  batch 377 loss: 0.27141397970108516
  batch 378 loss: 0.27135017490576185
  batch 379 loss: 0.2713997849216562
  batch 380 loss: 0.27144901925011683
  batch 381 loss: 0.271388048614104
  batch 382 loss: 0.2713685235818019
  batch 383 loss: 0.2713930100669438
  batch 384 loss: 0.2713789937940116
  batch 385 loss: 0.27151271766656404
  batch 386 loss: 0.27151151893206826
  batch 387 loss: 0.2715406837158425
  batch 388 loss: 0.2716037442466033
  batch 389 loss: 0.2715456767299794
  batch 390 loss: 0.27149917555925174
  batch 391 loss: 0.2715685074515355
  batch 392 loss: 0.2715868261471695
  batch 393 loss: 0.27163628627174075
  batch 394 loss: 0.27168533610662227
  batch 395 loss: 0.271615373975114
  batch 396 loss: 0.2716211239722642
  batch 397 loss: 0.2716151463520016
  batch 398 loss: 0.27154736617701736
  batch 399 loss: 0.27158178691577195
  batch 400 loss: 0.2716520633548498
  batch 401 loss: 0.2715880184102237
  batch 402 loss: 0.27160991172292337
  batch 403 loss: 0.27160347454601125
  batch 404 loss: 0.27165454021184754
  batch 405 loss: 0.27162705770245305
  batch 406 loss: 0.27172286874555013
  batch 407 loss: 0.27168030235046836
  batch 408 loss: 0.27178996260844024
  batch 409 loss: 0.27170780366673736
  batch 410 loss: 0.2717649528166143
  batch 411 loss: 0.27174177577315745
  batch 412 loss: 0.2717114874315493
  batch 413 loss: 0.2718318591827919
  batch 414 loss: 0.2717982413256226
  batch 415 loss: 0.2718141138553619
  batch 416 loss: 0.2718508639014684
  batch 417 loss: 0.2718075087888064
  batch 418 loss: 0.2717684860292234
  batch 419 loss: 0.2718144943293068
  batch 420 loss: 0.2717727957736878
  batch 421 loss: 0.2716984060059817
  batch 422 loss: 0.2718640414741932
  batch 423 loss: 0.27192134362586
  batch 424 loss: 0.2718769535422325
  batch 425 loss: 0.2718942995632396
  batch 426 loss: 0.27184888454670075
  batch 427 loss: 0.27183943763549767
  batch 428 loss: 0.271807894737364
  batch 429 loss: 0.2718426827645246
  batch 430 loss: 0.27178800002086995
  batch 431 loss: 0.27186918071970306
  batch 432 loss: 0.2719128798969366
  batch 433 loss: 0.2718794814571222
  batch 434 loss: 0.2719468554730789
  batch 435 loss: 0.27181292429737663
  batch 436 loss: 0.2718105989448521
  batch 437 loss: 0.2719000348100946
  batch 438 loss: 0.2720912088678308
  batch 439 loss: 0.2720891333658223
  batch 440 loss: 0.27211733216589146
  batch 441 loss: 0.2720841230854156
  batch 442 loss: 0.2721383926960138
  batch 443 loss: 0.272128225247424
  batch 444 loss: 0.27213760391548947
  batch 445 loss: 0.2721963248895795
  batch 446 loss: 0.2721822402135139
  batch 447 loss: 0.2721246871972244
  batch 448 loss: 0.27217125410347115
  batch 449 loss: 0.2721846077450134
  batch 450 loss: 0.27212153775824444
  batch 451 loss: 0.27207505825221406
  batch 452 loss: 0.27209048932500646
  batch 453 loss: 0.2721531421735587
  batch 454 loss: 0.27222279846536956
  batch 455 loss: 0.2722150958828874
  batch 456 loss: 0.2722676190825408
  batch 457 loss: 0.2723049373738875
  batch 458 loss: 0.27231443536724065
  batch 459 loss: 0.27227940678206924
  batch 460 loss: 0.27242001097487367
  batch 461 loss: 0.27246350906546873
  batch 462 loss: 0.27252246558795246
  batch 463 loss: 0.2724846395830871
  batch 464 loss: 0.27252392319898155
  batch 465 loss: 0.27248787505011407
  batch 466 loss: 0.2724027311021678
  batch 467 loss: 0.27244785801373916
  batch 468 loss: 0.27241602581408286
  batch 469 loss: 0.2725204215692813
  batch 470 loss: 0.2725676543218024
  batch 471 loss: 0.2725182150010091
  batch 472 loss: 0.27237797856835994
LOSS train 0.27237797856835994 valid 0.24721097946166992
LOSS train 0.27237797856835994 valid 0.228014275431633
LOSS train 0.27237797856835994 valid 0.2310065229733785
LOSS train 0.27237797856835994 valid 0.21996845677495003
LOSS train 0.27237797856835994 valid 0.2123970717191696
LOSS train 0.27237797856835994 valid 0.2120505819718043
LOSS train 0.27237797856835994 valid 0.2242478004523686
LOSS train 0.27237797856835994 valid 0.2199477832764387
LOSS train 0.27237797856835994 valid 0.21955705682436624
LOSS train 0.27237797856835994 valid 0.22192714214324952
LOSS train 0.27237797856835994 valid 0.21942953900857406
LOSS train 0.27237797856835994 valid 0.21842102209726968
LOSS train 0.27237797856835994 valid 0.2165473298384593
LOSS train 0.27237797856835994 valid 0.21791661530733109
LOSS train 0.27237797856835994 valid 0.21356919507185618
LOSS train 0.27237797856835994 valid 0.21488224063068628
LOSS train 0.27237797856835994 valid 0.21515466097523184
LOSS train 0.27237797856835994 valid 0.2150163675347964
LOSS train 0.27237797856835994 valid 0.2174883147603587
LOSS train 0.27237797856835994 valid 0.21711793318390846
LOSS train 0.27237797856835994 valid 0.21723193781716482
LOSS train 0.27237797856835994 valid 0.21596140617674048
LOSS train 0.27237797856835994 valid 0.2170693006204522
LOSS train 0.27237797856835994 valid 0.2164489198476076
LOSS train 0.27237797856835994 valid 0.2144182151556015
LOSS train 0.27237797856835994 valid 0.21430088980839804
LOSS train 0.27237797856835994 valid 0.21477285910535743
LOSS train 0.27237797856835994 valid 0.2159094363451004
LOSS train 0.27237797856835994 valid 0.21655398915553914
LOSS train 0.27237797856835994 valid 0.21720590094725292
LOSS train 0.27237797856835994 valid 0.21823007637454617
LOSS train 0.27237797856835994 valid 0.21825931826606393
LOSS train 0.27237797856835994 valid 0.21929356410647882
LOSS train 0.27237797856835994 valid 0.21914759585086038
LOSS train 0.27237797856835994 valid 0.2211441206080573
LOSS train 0.27237797856835994 valid 0.22107409892810714
LOSS train 0.27237797856835994 valid 0.2212667138995351
LOSS train 0.27237797856835994 valid 0.22189898749715403
LOSS train 0.27237797856835994 valid 0.2213009905356627
LOSS train 0.27237797856835994 valid 0.22146486379206182
LOSS train 0.27237797856835994 valid 0.22127084579409623
LOSS train 0.27237797856835994 valid 0.22133339906022662
LOSS train 0.27237797856835994 valid 0.2207828153704488
LOSS train 0.27237797856835994 valid 0.22091163085265594
LOSS train 0.27237797856835994 valid 0.2205075528886583
LOSS train 0.27237797856835994 valid 0.2212028049904367
LOSS train 0.27237797856835994 valid 0.22145551506509173
LOSS train 0.27237797856835994 valid 0.22118891092638174
LOSS train 0.27237797856835994 valid 0.22153973123248744
LOSS train 0.27237797856835994 valid 0.22085816115140916
LOSS train 0.27237797856835994 valid 0.22132938311380498
LOSS train 0.27237797856835994 valid 0.221262237486931
LOSS train 0.27237797856835994 valid 0.22136395804162295
LOSS train 0.27237797856835994 valid 0.22184596569449813
LOSS train 0.27237797856835994 valid 0.22169347378340634
LOSS train 0.27237797856835994 valid 0.22168605960905552
LOSS train 0.27237797856835994 valid 0.2216248640365768
LOSS train 0.27237797856835994 valid 0.22136262573044876
LOSS train 0.27237797856835994 valid 0.22214378391282033
LOSS train 0.27237797856835994 valid 0.22155199125409125
LOSS train 0.27237797856835994 valid 0.2213285504305949
LOSS train 0.27237797856835994 valid 0.2219701916460068
LOSS train 0.27237797856835994 valid 0.2222237688681436
LOSS train 0.27237797856835994 valid 0.22327028890140355
LOSS train 0.27237797856835994 valid 0.22323969304561614
LOSS train 0.27237797856835994 valid 0.2234294360334223
LOSS train 0.27237797856835994 valid 0.22250398304035415
LOSS train 0.27237797856835994 valid 0.22283533215522766
LOSS train 0.27237797856835994 valid 0.22184844811757407
LOSS train 0.27237797856835994 valid 0.22207412826163428
LOSS train 0.27237797856835994 valid 0.22168649019489825
LOSS train 0.27237797856835994 valid 0.22199100835455787
LOSS train 0.27237797856835994 valid 0.22190042921941575
LOSS train 0.27237797856835994 valid 0.22162441869039792
LOSS train 0.27237797856835994 valid 0.22140152434508006
LOSS train 0.27237797856835994 valid 0.22206003238496028
LOSS train 0.27237797856835994 valid 0.22169866364497642
LOSS train 0.27237797856835994 valid 0.2215828477190091
LOSS train 0.27237797856835994 valid 0.22162959715233574
LOSS train 0.27237797856835994 valid 0.2209320707246661
LOSS train 0.27237797856835994 valid 0.21986100132818576
LOSS train 0.27237797856835994 valid 0.21996035790298044
LOSS train 0.27237797856835994 valid 0.2196584922003459
LOSS train 0.27237797856835994 valid 0.2196367551528272
LOSS train 0.27237797856835994 valid 0.21889263444087084
LOSS train 0.27237797856835994 valid 0.2181062873366267
LOSS train 0.27237797856835994 valid 0.21819040744469084
LOSS train 0.27237797856835994 valid 0.21780429018492048
LOSS train 0.27237797856835994 valid 0.21804856534084577
LOSS train 0.27237797856835994 valid 0.21812770201100243
LOSS train 0.27237797856835994 valid 0.21811317447777634
LOSS train 0.27237797856835994 valid 0.21810667832260547
LOSS train 0.27237797856835994 valid 0.21788337730592297
LOSS train 0.27237797856835994 valid 0.21825240964585163
LOSS train 0.27237797856835994 valid 0.21777644314263997
LOSS train 0.27237797856835994 valid 0.21777781952793399
LOSS train 0.27237797856835994 valid 0.21789885442895987
LOSS train 0.27237797856835994 valid 0.21832608918146212
LOSS train 0.27237797856835994 valid 0.2184113554581247
LOSS train 0.27237797856835994 valid 0.21852524489164352
LOSS train 0.27237797856835994 valid 0.218980348346257
LOSS train 0.27237797856835994 valid 0.21920678165613436
LOSS train 0.27237797856835994 valid 0.2190137085984054
LOSS train 0.27237797856835994 valid 0.21872009308292314
LOSS train 0.27237797856835994 valid 0.21910895052410306
LOSS train 0.27237797856835994 valid 0.2193073219848129
LOSS train 0.27237797856835994 valid 0.21880939895304563
LOSS train 0.27237797856835994 valid 0.21880059496120172
LOSS train 0.27237797856835994 valid 0.21919670755710077
LOSS train 0.27237797856835994 valid 0.21957709193229674
LOSS train 0.27237797856835994 valid 0.21924828865506626
LOSS train 0.27237797856835994 valid 0.21926618660134928
LOSS train 0.27237797856835994 valid 0.21943348923088174
LOSS train 0.27237797856835994 valid 0.2193305757746362
LOSS train 0.27237797856835994 valid 0.21951119731301846
LOSS train 0.27237797856835994 valid 0.21998255661335483
LOSS train 0.27237797856835994 valid 0.2203019504618441
LOSS train 0.27237797856835994 valid 0.22028495926978225
LOSS train 0.27237797856835994 valid 0.22014439807218664
LOSS train 0.27237797856835994 valid 0.21993313108881316
LOSS train 0.27237797856835994 valid 0.219809598297127
LOSS train 0.27237797856835994 valid 0.21984562485433015
LOSS train 0.27237797856835994 valid 0.22007951540190998
LOSS train 0.27237797856835994 valid 0.2203349508345127
LOSS train 0.27237797856835994 valid 0.220372953414917
LOSS train 0.27237797856835994 valid 0.2204318888603695
LOSS train 0.27237797856835994 valid 0.22043488086677912
LOSS train 0.27237797856835994 valid 0.2205226463265717
LOSS train 0.27237797856835994 valid 0.22084843973780788
LOSS train 0.27237797856835994 valid 0.2206892097225556
LOSS train 0.27237797856835994 valid 0.22066475155244347
LOSS train 0.27237797856835994 valid 0.22035086143648985
LOSS train 0.27237797856835994 valid 0.22003285990173654
LOSS train 0.27237797856835994 valid 0.2202431023565691
LOSS train 0.27237797856835994 valid 0.2202359101286641
LOSS train 0.27237797856835994 valid 0.219899965559735
LOSS train 0.27237797856835994 valid 0.21968675018662084
LOSS train 0.27237797856835994 valid 0.2195661117633184
LOSS train 0.27237797856835994 valid 0.21937082911566866
LOSS train 0.27237797856835994 valid 0.2195789427629539
LOSS train 0.27237797856835994 valid 0.21973243535410428
LOSS train 0.27237797856835994 valid 0.21985782945240048
LOSS train 0.27237797856835994 valid 0.22009585610219648
LOSS train 0.27237797856835994 valid 0.22029310640775496
LOSS train 0.27237797856835994 valid 0.2201002385081916
LOSS train 0.27237797856835994 valid 0.22011257452915792
LOSS train 0.27237797856835994 valid 0.21992199954126967
LOSS train 0.27237797856835994 valid 0.22071761527174227
LOSS train 0.27237797856835994 valid 0.2207861295682472
LOSS train 0.27237797856835994 valid 0.2206538909673691
LOSS train 0.27237797856835994 valid 0.22088613356186065
LOSS train 0.27237797856835994 valid 0.22055328049157796
LOSS train 0.27237797856835994 valid 0.22070029105236327
LOSS train 0.27237797856835994 valid 0.22079722573617835
LOSS train 0.27237797856835994 valid 0.220665513603918
LOSS train 0.27237797856835994 valid 0.22099939208382216
LOSS train 0.27237797856835994 valid 0.22089690359155084
LOSS train 0.27237797856835994 valid 0.22093601015549671
LOSS train 0.27237797856835994 valid 0.22095463879453311
LOSS train 0.27237797856835994 valid 0.22072073528543115
LOSS train 0.27237797856835994 valid 0.22064815655998563
LOSS train 0.27237797856835994 valid 0.22046439368047832
LOSS train 0.27237797856835994 valid 0.22034170464869657
LOSS train 0.27237797856835994 valid 0.22016617410430095
LOSS train 0.27237797856835994 valid 0.2201321359836694
LOSS train 0.27237797856835994 valid 0.21999389435871539
LOSS train 0.27237797856835994 valid 0.22017172844467048
LOSS train 0.27237797856835994 valid 0.2200119256795872
LOSS train 0.27237797856835994 valid 0.22011821157128147
LOSS train 0.27237797856835994 valid 0.2203268247492173
LOSS train 0.27237797856835994 valid 0.22024298428792005
LOSS train 0.27237797856835994 valid 0.22014673002237498
LOSS train 0.27237797856835994 valid 0.22008948157288435
LOSS train 0.27237797856835994 valid 0.22015944791251216
LOSS train 0.27237797856835994 valid 0.2200614467688969
LOSS train 0.27237797856835994 valid 0.22011638449674303
LOSS train 0.27237797856835994 valid 0.2199685218307258
LOSS train 0.27237797856835994 valid 0.220096563020449
LOSS train 0.27237797856835994 valid 0.22005414263496187
LOSS train 0.27237797856835994 valid 0.22006007507443429
LOSS train 0.27237797856835994 valid 0.22015086656117308
LOSS train 0.27237797856835994 valid 0.22026281230724776
LOSS train 0.27237797856835994 valid 0.2201438292942412
LOSS train 0.27237797856835994 valid 0.22027358526121016
LOSS train 0.27237797856835994 valid 0.22011092281019365
LOSS train 0.27237797856835994 valid 0.2200553096750731
LOSS train 0.27237797856835994 valid 0.21990193036150804
LOSS train 0.27237797856835994 valid 0.21986177024689127
LOSS train 0.27237797856835994 valid 0.21983519701099902
LOSS train 0.27237797856835994 valid 0.22010589665488192
LOSS train 0.27237797856835994 valid 0.22003364118293942
LOSS train 0.27237797856835994 valid 0.22002552651489773
LOSS train 0.27237797856835994 valid 0.2200734705659392
LOSS train 0.27237797856835994 valid 0.21990607571356074
LOSS train 0.27237797856835994 valid 0.2196477770805359
LOSS train 0.27237797856835994 valid 0.21973278867651005
LOSS train 0.27237797856835994 valid 0.21991272013502072
LOSS train 0.27237797856835994 valid 0.21974979852787171
LOSS train 0.27237797856835994 valid 0.21981483727843318
LOSS train 0.27237797856835994 valid 0.2197348180413246
LOSS train 0.27237797856835994 valid 0.21957703016290617
LOSS train 0.27237797856835994 valid 0.21951156943151268
LOSS train 0.27237797856835994 valid 0.21941957553031996
LOSS train 0.27237797856835994 valid 0.2193976315651454
LOSS train 0.27237797856835994 valid 0.21924944825288725
LOSS train 0.27237797856835994 valid 0.2191456382714429
LOSS train 0.27237797856835994 valid 0.21909190260845682
LOSS train 0.27237797856835994 valid 0.21899536681862977
LOSS train 0.27237797856835994 valid 0.21890394717597506
LOSS train 0.27237797856835994 valid 0.21896077472539174
LOSS train 0.27237797856835994 valid 0.2190928212698037
LOSS train 0.27237797856835994 valid 0.21903977992961993
LOSS train 0.27237797856835994 valid 0.21911399157394265
LOSS train 0.27237797856835994 valid 0.2190405248063747
LOSS train 0.27237797856835994 valid 0.21885214614313703
LOSS train 0.27237797856835994 valid 0.21877166787507357
LOSS train 0.27237797856835994 valid 0.21858506405957834
LOSS train 0.27237797856835994 valid 0.21859909122416732
LOSS train 0.27237797856835994 valid 0.21871850268753695
LOSS train 0.27237797856835994 valid 0.2188201818953861
LOSS train 0.27237797856835994 valid 0.21888233188590314
LOSS train 0.27237797856835994 valid 0.21875408124010842
LOSS train 0.27237797856835994 valid 0.21883510627821423
LOSS train 0.27237797856835994 valid 0.2189389642194978
LOSS train 0.27237797856835994 valid 0.21894807875156402
LOSS train 0.27237797856835994 valid 0.21906369753116
LOSS train 0.27237797856835994 valid 0.21926735151181662
LOSS train 0.27237797856835994 valid 0.21934306967938155
LOSS train 0.27237797856835994 valid 0.21939648486120733
LOSS train 0.27237797856835994 valid 0.21956578117349873
LOSS train 0.27237797856835994 valid 0.21958522588917703
LOSS train 0.27237797856835994 valid 0.219657181601586
LOSS train 0.27237797856835994 valid 0.21960528578625216
LOSS train 0.27237797856835994 valid 0.21963953411477244
LOSS train 0.27237797856835994 valid 0.21973093171068964
LOSS train 0.27237797856835994 valid 0.21954436878026543
LOSS train 0.27237797856835994 valid 0.21974690377963746
LOSS train 0.27237797856835994 valid 0.21962604796936533
LOSS train 0.27237797856835994 valid 0.21959902514473664
LOSS train 0.27237797856835994 valid 0.2195799404134353
LOSS train 0.27237797856835994 valid 0.21968662751166157
LOSS train 0.27237797856835994 valid 0.2194692944445886
LOSS train 0.27237797856835994 valid 0.21958669382358284
LOSS train 0.27237797856835994 valid 0.21981809166122657
LOSS train 0.27237797856835994 valid 0.2199521814073835
LOSS train 0.27237797856835994 valid 0.21993391058309292
LOSS train 0.27237797856835994 valid 0.21990371558830324
LOSS train 0.27237797856835994 valid 0.21984344522558874
LOSS train 0.27237797856835994 valid 0.21973076396917243
LOSS train 0.27237797856835994 valid 0.21987532442808153
LOSS train 0.27237797856835994 valid 0.21982982161035575
LOSS train 0.27237797856835994 valid 0.21978705660218284
LOSS train 0.27237797856835994 valid 0.2196987195330646
LOSS train 0.27237797856835994 valid 0.2196446891258082
LOSS train 0.27237797856835994 valid 0.2196985948903888
LOSS train 0.27237797856835994 valid 0.21966721973149106
LOSS train 0.27237797856835994 valid 0.2195447759289686
LOSS train 0.27237797856835994 valid 0.21964920254409775
LOSS train 0.27237797856835994 valid 0.21967667260685483
LOSS train 0.27237797856835994 valid 0.21966767001610535
LOSS train 0.27237797856835994 valid 0.2197912649404957
LOSS train 0.27237797856835994 valid 0.21975289802742368
LOSS train 0.27237797856835994 valid 0.21980643952300793
LOSS train 0.27237797856835994 valid 0.21978162296793677
LOSS train 0.27237797856835994 valid 0.21980995468373568
LOSS train 0.27237797856835994 valid 0.21981871744295708
LOSS train 0.27237797856835994 valid 0.21985562541958098
LOSS train 0.27237797856835994 valid 0.21989857260860615
LOSS train 0.27237797856835994 valid 0.2201107760344296
LOSS train 0.27237797856835994 valid 0.22025635794357018
LOSS train 0.27237797856835994 valid 0.2203468101380936
LOSS train 0.27237797856835994 valid 0.22059413423652158
LOSS train 0.27237797856835994 valid 0.22060522442553943
LOSS train 0.27237797856835994 valid 0.22050280433936711
LOSS train 0.27237797856835994 valid 0.2205113836851987
LOSS train 0.27237797856835994 valid 0.22039025600837625
LOSS train 0.27237797856835994 valid 0.22023206077758156
LOSS train 0.27237797856835994 valid 0.22004029438864414
LOSS train 0.27237797856835994 valid 0.2200908187149246
LOSS train 0.27237797856835994 valid 0.21998807835791792
LOSS train 0.27237797856835994 valid 0.21986551169186724
LOSS train 0.27237797856835994 valid 0.21978232539291923
LOSS train 0.27237797856835994 valid 0.21979150139190282
LOSS train 0.27237797856835994 valid 0.21984445251209636
LOSS train 0.27237797856835994 valid 0.2198678267629523
LOSS train 0.27237797856835994 valid 0.21978163031431344
LOSS train 0.27237797856835994 valid 0.219680790130685
LOSS train 0.27237797856835994 valid 0.21971782271025908
LOSS train 0.27237797856835994 valid 0.21971363019984486
LOSS train 0.27237797856835994 valid 0.21981489437407462
LOSS train 0.27237797856835994 valid 0.21969396705479965
LOSS train 0.27237797856835994 valid 0.21972323964311652
LOSS train 0.27237797856835994 valid 0.21969299740555343
LOSS train 0.27237797856835994 valid 0.21992399867902807
LOSS train 0.27237797856835994 valid 0.22002502513133873
LOSS train 0.27237797856835994 valid 0.2199618744890432
LOSS train 0.27237797856835994 valid 0.2199237062674179
LOSS train 0.27237797856835994 valid 0.21982262418574136
LOSS train 0.27237797856835994 valid 0.219861685033626
LOSS train 0.27237797856835994 valid 0.21978636130690574
LOSS train 0.27237797856835994 valid 0.21993399522827314
LOSS train 0.27237797856835994 valid 0.21995459965720082
LOSS train 0.27237797856835994 valid 0.22003173331419626
LOSS train 0.27237797856835994 valid 0.22008035523130706
LOSS train 0.27237797856835994 valid 0.21995956497114214
LOSS train 0.27237797856835994 valid 0.21992401686174418
LOSS train 0.27237797856835994 valid 0.21995352960953107
LOSS train 0.27237797856835994 valid 0.21994278057442082
LOSS train 0.27237797856835994 valid 0.21998429857797222
LOSS train 0.27237797856835994 valid 0.21986832714849902
LOSS train 0.27237797856835994 valid 0.21971535850376178
LOSS train 0.27237797856835994 valid 0.219804677586907
LOSS train 0.27237797856835994 valid 0.2197533737832365
LOSS train 0.27237797856835994 valid 0.21984603488521212
LOSS train 0.27237797856835994 valid 0.21985768737300995
LOSS train 0.27237797856835994 valid 0.21983205578938314
LOSS train 0.27237797856835994 valid 0.2198494535515361
LOSS train 0.27237797856835994 valid 0.2199616436875841
LOSS train 0.27237797856835994 valid 0.21995149562164534
LOSS train 0.27237797856835994 valid 0.21985162431374192
LOSS train 0.27237797856835994 valid 0.21991866641326857
LOSS train 0.27237797856835994 valid 0.21989384335205422
LOSS train 0.27237797856835994 valid 0.2198963857287593
LOSS train 0.27237797856835994 valid 0.21992469827334085
LOSS train 0.27237797856835994 valid 0.21997001757988563
LOSS train 0.27237797856835994 valid 0.22008104924043995
LOSS train 0.27237797856835994 valid 0.22017510440371452
LOSS train 0.27237797856835994 valid 0.22016283647134538
LOSS train 0.27237797856835994 valid 0.22037842656884873
LOSS train 0.27237797856835994 valid 0.22029247478102193
LOSS train 0.27237797856835994 valid 0.2201874994889487
LOSS train 0.27237797856835994 valid 0.22015536423907223
LOSS train 0.27237797856835994 valid 0.22013522783020237
LOSS train 0.27237797856835994 valid 0.22027541085810004
LOSS train 0.27237797856835994 valid 0.22032650904868967
LOSS train 0.27237797856835994 valid 0.22030968666963635
LOSS train 0.27237797856835994 valid 0.22035233946157845
LOSS train 0.27237797856835994 valid 0.22033831922436606
LOSS train 0.27237797856835994 valid 0.22031241664140977
LOSS train 0.27237797856835994 valid 0.22032883929855684
LOSS train 0.27237797856835994 valid 0.2203295400852332
LOSS train 0.27237797856835994 valid 0.2202399224478599
LOSS train 0.27237797856835994 valid 0.2202186810379473
LOSS train 0.27237797856835994 valid 0.22044613619529924
LOSS train 0.27237797856835994 valid 0.2204320066648981
LOSS train 0.27237797856835994 valid 0.22036112590849055
LOSS train 0.27237797856835994 valid 0.2202877568494346
LOSS train 0.27237797856835994 valid 0.2201805353678506
LOSS train 0.27237797856835994 valid 0.22023659035093804
LOSS train 0.27237797856835994 valid 0.22018051956381116
LOSS train 0.27237797856835994 valid 0.22007885987092968
LOSS train 0.27237797856835994 valid 0.22008829220960086
LOSS train 0.27237797856835994 valid 0.2201311448140455
LOSS train 0.27237797856835994 valid 0.22009648719413133
LOSS train 0.27237797856835994 valid 0.22027546477989413
LOSS train 0.27237797856835994 valid 0.22029432135351587
LOSS train 0.27237797856835994 valid 0.2202464488207125
LOSS train 0.27237797856835994 valid 0.22018570452928543
LOSS train 0.27237797856835994 valid 0.22020165422998764
LOSS train 0.27237797856835994 valid 0.22015593511362871
LOSS train 0.27237797856835994 valid 0.22007386089196826
LOSS train 0.27237797856835994 valid 0.22013100154327425
LOSS train 0.27237797856835994 valid 0.21989381777189324
LOSS train 0.27237797856835994 valid 0.21994521186410726
LOSS train 0.27237797856835994 valid 0.21995797924799462
LOSS train 0.27237797856835994 valid 0.21987616808199492
LOSS train 0.27237797856835994 valid 0.21968517507291946
LOSS train 0.27237797856835994 valid 0.21963241299533326
LOSS train 0.27237797856835994 valid 0.21966485453007345
EPOCH 10:
  batch 1 loss: 0.24481478333473206
  batch 2 loss: 0.2348056212067604
  batch 3 loss: 0.2448902279138565
  batch 4 loss: 0.251220528036356
  batch 5 loss: 0.2656012088060379
  batch 6 loss: 0.2646895969907443
  batch 7 loss: 0.2653984193290983
  batch 8 loss: 0.26872905157506466
  batch 9 loss: 0.2683033463027742
  batch 10 loss: 0.270015050470829
  batch 11 loss: 0.2682728591290387
  batch 12 loss: 0.2657719614605109
  batch 13 loss: 0.2637968361377716
  batch 14 loss: 0.26407652880464283
  batch 15 loss: 0.2647295395533244
  batch 16 loss: 0.2647359985858202
  batch 17 loss: 0.2628415270763285
  batch 18 loss: 0.26474854432874256
  batch 19 loss: 0.2634513025221072
  batch 20 loss: 0.2618601396679878
  batch 21 loss: 0.26260541876157123
  batch 22 loss: 0.263330875472589
  batch 23 loss: 0.2634911550127942
  batch 24 loss: 0.2612533966700236
  batch 25 loss: 0.2625395393371582
  batch 26 loss: 0.26078348091015446
  batch 27 loss: 0.260957364682798
  batch 28 loss: 0.25953740254044533
  batch 29 loss: 0.2596654321612983
  batch 30 loss: 0.2593471641341845
  batch 31 loss: 0.26067732659078413
  batch 32 loss: 0.2600613725371659
  batch 33 loss: 0.2604468966072256
  batch 34 loss: 0.2600248521741699
  batch 35 loss: 0.2600415676832199
  batch 36 loss: 0.26046793204214835
  batch 37 loss: 0.2612533678074141
  batch 38 loss: 0.262133089727477
  batch 39 loss: 0.2619239668815564
  batch 40 loss: 0.2624015148729086
  batch 41 loss: 0.2628586179599529
  batch 42 loss: 0.26306811365343274
  batch 43 loss: 0.26360450615716535
  batch 44 loss: 0.26362618973309343
  batch 45 loss: 0.2628033638000488
  batch 46 loss: 0.2623095126903575
  batch 47 loss: 0.2621661212216032
  batch 48 loss: 0.26142972024778527
  batch 49 loss: 0.26107263382600276
  batch 50 loss: 0.2610790354013443
  batch 51 loss: 0.2607729960890377
  batch 52 loss: 0.2608978060575632
  batch 53 loss: 0.2605510193784282
  batch 54 loss: 0.2608231281240781
  batch 55 loss: 0.2606234834952788
  batch 56 loss: 0.2607877869158983
  batch 57 loss: 0.2607921437736143
  batch 58 loss: 0.2613667827741853
  batch 59 loss: 0.2618748618889663
  batch 60 loss: 0.2618547730147839
  batch 61 loss: 0.26223860192494314
  batch 62 loss: 0.26289217246155583
  batch 63 loss: 0.26287915616754504
  batch 64 loss: 0.26389348902739584
  batch 65 loss: 0.26325098551236664
  batch 66 loss: 0.2635220176342762
  batch 67 loss: 0.2636408036324515
  batch 68 loss: 0.26453131831744137
  batch 69 loss: 0.2641008055728415
  batch 70 loss: 0.26442408391407557
  batch 71 loss: 0.2642915735782032
  batch 72 loss: 0.26443178537819123
  batch 73 loss: 0.26451187917630964
  batch 74 loss: 0.2648433273708498
  batch 75 loss: 0.26456633110841116
  batch 76 loss: 0.2651582099497318
  batch 77 loss: 0.26477642202532137
  batch 78 loss: 0.26484973862385136
  batch 79 loss: 0.26528918497924564
  batch 80 loss: 0.2647809151560068
  batch 81 loss: 0.26477974947587946
  batch 82 loss: 0.26481195339342445
  batch 83 loss: 0.2646331790700016
  batch 84 loss: 0.2644702426734425
  batch 85 loss: 0.2640557937762317
  batch 86 loss: 0.2648486397987188
  batch 87 loss: 0.2648809339808322
  batch 88 loss: 0.2646544389426708
  batch 89 loss: 0.26480277970935523
  batch 90 loss: 0.2648522930012809
  batch 91 loss: 0.2652685141170418
  batch 92 loss: 0.2655599314881408
  batch 93 loss: 0.2657881186854455
  batch 94 loss: 0.2663164382919352
  batch 95 loss: 0.2662292248324344
  batch 96 loss: 0.2664361161490281
  batch 97 loss: 0.26685040881953287
  batch 98 loss: 0.26721201745831236
  batch 99 loss: 0.26737985707292655
  batch 100 loss: 0.2673109769821167
  batch 101 loss: 0.26700171252878585
  batch 102 loss: 0.26700325295621274
  batch 103 loss: 0.26744654847001564
  batch 104 loss: 0.26740369014441967
  batch 105 loss: 0.2673020724739347
  batch 106 loss: 0.2675023800078428
  batch 107 loss: 0.2669236377299389
  batch 108 loss: 0.26677972616420853
  batch 109 loss: 0.26662443277485876
  batch 110 loss: 0.2666937363418666
  batch 111 loss: 0.26659762657977437
  batch 112 loss: 0.2663363276847771
  batch 113 loss: 0.26629813556122567
  batch 114 loss: 0.26663723901698466
  batch 115 loss: 0.26652987625287927
  batch 116 loss: 0.2665405037074253
  batch 117 loss: 0.2667086977734525
  batch 118 loss: 0.2662668974470284
  batch 119 loss: 0.26626111116228984
  batch 120 loss: 0.26610878966748713
  batch 121 loss: 0.26600293026975363
  batch 122 loss: 0.26587623321130627
  batch 123 loss: 0.26587992361406
  batch 124 loss: 0.2660461487548967
  batch 125 loss: 0.2659943314790726
  batch 126 loss: 0.2660077799643789
  batch 127 loss: 0.2664288041863854
  batch 128 loss: 0.2662277682684362
  batch 129 loss: 0.26628860600234927
  batch 130 loss: 0.2661609422702056
  batch 131 loss: 0.26635936039094704
  batch 132 loss: 0.26632615359443607
  batch 133 loss: 0.26660351130299104
  batch 134 loss: 0.26672471036661916
  batch 135 loss: 0.266753015032521
  batch 136 loss: 0.26678748713696704
  batch 137 loss: 0.2668422399646174
  batch 138 loss: 0.26681850159513776
  batch 139 loss: 0.26726080273552766
  batch 140 loss: 0.26722932904958724
  batch 141 loss: 0.2672369676701566
  batch 142 loss: 0.26727280587377683
  batch 143 loss: 0.2670795565301722
  batch 144 loss: 0.26687435453964603
  batch 145 loss: 0.26660528265196703
  batch 146 loss: 0.26668353929911576
  batch 147 loss: 0.2672101234903141
  batch 148 loss: 0.2670838849971423
  batch 149 loss: 0.2669814492032032
  batch 150 loss: 0.2672436981399854
  batch 151 loss: 0.26751398615884464
  batch 152 loss: 0.26761969129898044
  batch 153 loss: 0.26759782793864706
  batch 154 loss: 0.26831349237011626
  batch 155 loss: 0.2684791289029583
  batch 156 loss: 0.2685134148177428
  batch 157 loss: 0.26881879653520646
  batch 158 loss: 0.2691175393477271
  batch 159 loss: 0.2696532787954282
  batch 160 loss: 0.2695419666357338
  batch 161 loss: 0.26958486410031407
  batch 162 loss: 0.26971945654094953
  batch 163 loss: 0.26979055808731384
  batch 164 loss: 0.26977522462242987
  batch 165 loss: 0.26975588283755564
  batch 166 loss: 0.2696909589401211
  batch 167 loss: 0.26952436735887014
  batch 168 loss: 0.2694892909023024
  batch 169 loss: 0.26930783918270695
  batch 170 loss: 0.26926283687353136
  batch 171 loss: 0.2692662366823843
  batch 172 loss: 0.26929435454482253
  batch 173 loss: 0.26937027533963925
  batch 174 loss: 0.2694930387125618
  batch 175 loss: 0.2696349873713085
  batch 176 loss: 0.26951445630666887
  batch 177 loss: 0.26940566747538786
  batch 178 loss: 0.26949881912952056
  batch 179 loss: 0.2695772578762896
  batch 180 loss: 0.26954160605867705
  batch 181 loss: 0.2693801951177871
  batch 182 loss: 0.26945528035962973
  batch 183 loss: 0.2692358567903602
  batch 184 loss: 0.26904476693143015
  batch 185 loss: 0.26893688440322877
  batch 186 loss: 0.26916636766925933
  batch 187 loss: 0.26909819309086724
  batch 188 loss: 0.2688040405987425
  batch 189 loss: 0.2685387840977422
  batch 190 loss: 0.2684645790802805
  batch 191 loss: 0.2683138611741091
  batch 192 loss: 0.26833142852410674
  batch 193 loss: 0.2683990979132875
  batch 194 loss: 0.2685599649689861
  batch 195 loss: 0.2686422384702242
  batch 196 loss: 0.26852752929743456
  batch 197 loss: 0.26835480657628347
  batch 198 loss: 0.2685142118069861
  batch 199 loss: 0.26878130862760785
  batch 200 loss: 0.2689088486880064
  batch 201 loss: 0.26889769703298066
  batch 202 loss: 0.2688105273807403
  batch 203 loss: 0.2688823580888692
  batch 204 loss: 0.26874714907185704
  batch 205 loss: 0.2688435219409989
  batch 206 loss: 0.26877026527541353
  batch 207 loss: 0.2688480288078244
  batch 208 loss: 0.2687757597663082
  batch 209 loss: 0.26864591383477715
  batch 210 loss: 0.2687343783321835
  batch 211 loss: 0.2686942911826039
  batch 212 loss: 0.2686928600635169
  batch 213 loss: 0.26874983772425587
  batch 214 loss: 0.26886885160597684
  batch 215 loss: 0.26858752106511313
  batch 216 loss: 0.26849577562124644
  batch 217 loss: 0.26847949802600846
  batch 218 loss: 0.2684756829104292
  batch 219 loss: 0.26845967021162653
  batch 220 loss: 0.26834957613186405
  batch 221 loss: 0.2685028120403376
  batch 222 loss: 0.2686039042902422
  batch 223 loss: 0.26876305598314565
  batch 224 loss: 0.2687408949381539
  batch 225 loss: 0.2686162795623144
  batch 226 loss: 0.2686949028773645
  batch 227 loss: 0.2684357610042925
  batch 228 loss: 0.268431139358303
  batch 229 loss: 0.2683156417707168
  batch 230 loss: 0.26827299879944844
  batch 231 loss: 0.2681997535806714
  batch 232 loss: 0.26788120607620686
  batch 233 loss: 0.26783217745533316
  batch 234 loss: 0.2679552720525326
  batch 235 loss: 0.2680835174119219
  batch 236 loss: 0.2680429397624428
  batch 237 loss: 0.2680617929888174
  batch 238 loss: 0.2679509026538424
  batch 239 loss: 0.26785687310426304
  batch 240 loss: 0.26793316726883254
  batch 241 loss: 0.2680687577892636
  batch 242 loss: 0.26797592781545704
  batch 243 loss: 0.2681142146327368
  batch 244 loss: 0.2681490643957599
  batch 245 loss: 0.2681851065280486
  batch 246 loss: 0.2681558761533683
  batch 247 loss: 0.268239292598929
  batch 248 loss: 0.26840264067774816
  batch 249 loss: 0.2683573347856721
  batch 250 loss: 0.26822811460494994
  batch 251 loss: 0.26819381592758146
  batch 252 loss: 0.2680221066351921
  batch 253 loss: 0.26784300544987555
  batch 254 loss: 0.26781401941626093
  batch 255 loss: 0.2678833496336843
  batch 256 loss: 0.2679702090099454
  batch 257 loss: 0.26804758138693724
  batch 258 loss: 0.2681208650271098
  batch 259 loss: 0.2682117449270712
  batch 260 loss: 0.2683144612954213
  batch 261 loss: 0.2684164079213051
  batch 262 loss: 0.2684187852699338
  batch 263 loss: 0.268488736207041
  batch 264 loss: 0.2683662152990247
  batch 265 loss: 0.2683305945598854
  batch 266 loss: 0.26832126478727597
  batch 267 loss: 0.26838748134700546
  batch 268 loss: 0.26835399248929165
  batch 269 loss: 0.268350580866452
  batch 270 loss: 0.2686455603550982
  batch 271 loss: 0.268750874593689
  batch 272 loss: 0.268773888128207
  batch 273 loss: 0.2687377982633018
  batch 274 loss: 0.268948933667075
  batch 275 loss: 0.2689719169248234
  batch 276 loss: 0.2689961369378843
  batch 277 loss: 0.2690011348427418
  batch 278 loss: 0.269047633403068
  batch 279 loss: 0.26925179634683877
  batch 280 loss: 0.26915283857711725
  batch 281 loss: 0.2690379517252335
  batch 282 loss: 0.26907411571724194
  batch 283 loss: 0.26900410804858055
  batch 284 loss: 0.269025779788343
  batch 285 loss: 0.26899460967172656
  batch 286 loss: 0.2690311872979978
  batch 287 loss: 0.2690421291448513
  batch 288 loss: 0.26891667473440367
  batch 289 loss: 0.2691338316482656
  batch 290 loss: 0.2689289820091478
  batch 291 loss: 0.2689252337536861
  batch 292 loss: 0.26894342302255436
  batch 293 loss: 0.26893061658831585
  batch 294 loss: 0.26878853841703765
  batch 295 loss: 0.26882895261554396
  batch 296 loss: 0.2688727127136411
  batch 297 loss: 0.26885451020215095
  batch 298 loss: 0.2688905720742757
  batch 299 loss: 0.26883985426114954
  batch 300 loss: 0.2688123632470767
  batch 301 loss: 0.26874876245113705
  batch 302 loss: 0.2687702718258694
  batch 303 loss: 0.2687581261687546
  batch 304 loss: 0.2687501244148926
  batch 305 loss: 0.2686301926120383
  batch 306 loss: 0.2687122071880141
  batch 307 loss: 0.2686085564111654
  batch 308 loss: 0.26872761228254866
  batch 309 loss: 0.2687150107811184
  batch 310 loss: 0.26864493359481134
  batch 311 loss: 0.2688126113157947
  batch 312 loss: 0.2688882381010514
  batch 313 loss: 0.26893443188156946
  batch 314 loss: 0.26896511516563454
  batch 315 loss: 0.26896802163313305
  batch 316 loss: 0.2688429369296454
  batch 317 loss: 0.26892027207165486
  batch 318 loss: 0.2689060513047302
  batch 319 loss: 0.26883828915585545
  batch 320 loss: 0.26878043957985936
  batch 321 loss: 0.26879065267950575
  batch 322 loss: 0.2686848285960855
  batch 323 loss: 0.26860164027465017
  batch 324 loss: 0.26836831909086967
  batch 325 loss: 0.2682857118661587
  batch 326 loss: 0.2682470142201412
  batch 327 loss: 0.26820361974224766
  batch 328 loss: 0.26806554929694026
  batch 329 loss: 0.2681883317630704
  batch 330 loss: 0.2680814226016854
  batch 331 loss: 0.2680561045055303
  batch 332 loss: 0.26803372768932077
  batch 333 loss: 0.2680276077878368
  batch 334 loss: 0.2678943120701584
  batch 335 loss: 0.267746026569338
  batch 336 loss: 0.26765182206318494
  batch 337 loss: 0.2674798395937911
  batch 338 loss: 0.2674418640207257
  batch 339 loss: 0.2672539639965271
  batch 340 loss: 0.26716006227275907
  batch 341 loss: 0.26712554496579155
  batch 342 loss: 0.2671492650059232
  batch 343 loss: 0.26716783473338745
  batch 344 loss: 0.26711898275412793
  batch 345 loss: 0.26726970685564955
  batch 346 loss: 0.2671168907787759
  batch 347 loss: 0.2671639729903136
  batch 348 loss: 0.26715753017657107
  batch 349 loss: 0.2671372146268287
  batch 350 loss: 0.2672542093907084
  batch 351 loss: 0.2673088117287709
  batch 352 loss: 0.26739732518961484
  batch 353 loss: 0.26742791323249804
  batch 354 loss: 0.26760406434367606
  batch 355 loss: 0.2676155128529374
  batch 356 loss: 0.2675675396085455
  batch 357 loss: 0.26746583957465087
  batch 358 loss: 0.26753588041613224
  batch 359 loss: 0.2675424527408021
  batch 360 loss: 0.2676418622748719
  batch 361 loss: 0.2675455173379497
  batch 362 loss: 0.26756338285313125
  batch 363 loss: 0.26745904686529776
  batch 364 loss: 0.26741111421814334
  batch 365 loss: 0.2674308854014906
  batch 366 loss: 0.2673441888611825
  batch 367 loss: 0.2672680101131548
  batch 368 loss: 0.26719542353859416
  batch 369 loss: 0.26711675431831744
  batch 370 loss: 0.2670981722506317
  batch 371 loss: 0.2670911382231108
  batch 372 loss: 0.2670387980998844
  batch 373 loss: 0.2669018597689135
  batch 374 loss: 0.2667593668329524
  batch 375 loss: 0.26673846753438313
  batch 376 loss: 0.2667637860521357
  batch 377 loss: 0.26673973808237983
  batch 378 loss: 0.2666341597756381
  batch 379 loss: 0.2666558675998748
  batch 380 loss: 0.26666298249834464
  batch 381 loss: 0.26660596174517953
  batch 382 loss: 0.26658009023878587
  batch 383 loss: 0.26660534295961064
  batch 384 loss: 0.26657806569710374
  batch 385 loss: 0.2666840212685721
  batch 386 loss: 0.2666545652354937
  batch 387 loss: 0.2667028243227523
  batch 388 loss: 0.26676721832494144
  batch 389 loss: 0.26672710122056975
  batch 390 loss: 0.266615739885049
  batch 391 loss: 0.2666518488884582
  batch 392 loss: 0.26665097775355895
  batch 393 loss: 0.2666637219166331
  batch 394 loss: 0.2667042975725256
  batch 395 loss: 0.2666046427775033
  batch 396 loss: 0.26652259488749985
  batch 397 loss: 0.26652710830384596
  batch 398 loss: 0.2664618600702765
  batch 399 loss: 0.26647048419280756
  batch 400 loss: 0.26652142822742464
  batch 401 loss: 0.2664312471326747
  batch 402 loss: 0.26646305880143273
  batch 403 loss: 0.2664658909400107
  batch 404 loss: 0.26649792005520057
  batch 405 loss: 0.2664514736628827
  batch 406 loss: 0.2664939590128772
  batch 407 loss: 0.2664688796317548
  batch 408 loss: 0.2665746799286674
  batch 409 loss: 0.26647995288476967
  batch 410 loss: 0.26652513041001996
  batch 411 loss: 0.26648943097196937
  batch 412 loss: 0.26645117596659845
  batch 413 loss: 0.2665553726165693
  batch 414 loss: 0.2665234542483293
  batch 415 loss: 0.2665146891252104
  batch 416 loss: 0.2665310022421181
  batch 417 loss: 0.26648356449260985
  batch 418 loss: 0.26639663927150115
  batch 419 loss: 0.2664292532603894
  batch 420 loss: 0.26639167088128274
  batch 421 loss: 0.26630039233493125
  batch 422 loss: 0.2665133305353011
  batch 423 loss: 0.2665754653178772
  batch 424 loss: 0.2665361426419245
  batch 425 loss: 0.2665425666991402
  batch 426 loss: 0.2664983617797704
  batch 427 loss: 0.26649666003516465
  batch 428 loss: 0.26646251457734643
  batch 429 loss: 0.2664897435959125
  batch 430 loss: 0.2664451011045035
  batch 431 loss: 0.2665284126305525
  batch 432 loss: 0.26657713008009726
  batch 433 loss: 0.26653531038871264
  batch 434 loss: 0.2666032239794731
  batch 435 loss: 0.26647092580110177
  batch 436 loss: 0.2665087751907493
  batch 437 loss: 0.26658490562739035
  batch 438 loss: 0.26679162276253854
  batch 439 loss: 0.26679333477601375
  batch 440 loss: 0.2668393608859994
  batch 441 loss: 0.26678956227372835
  batch 442 loss: 0.26681695416637136
  batch 443 loss: 0.26682194653671426
  batch 444 loss: 0.26680245294093013
  batch 445 loss: 0.2668357469727484
  batch 446 loss: 0.2668187611907587
  batch 447 loss: 0.26676953962318584
  batch 448 loss: 0.26684765540994704
  batch 449 loss: 0.266838837473058
  batch 450 loss: 0.26677549418475893
  batch 451 loss: 0.26672875121268885
  batch 452 loss: 0.2667372022318629
  batch 453 loss: 0.2667899440324333
  batch 454 loss: 0.26684700464624667
  batch 455 loss: 0.2668480415265639
  batch 456 loss: 0.2669153189972827
  batch 457 loss: 0.26697688107939205
  batch 458 loss: 0.26695286602953117
  batch 459 loss: 0.26692478056826624
  batch 460 loss: 0.2670397693696229
  batch 461 loss: 0.2670506017844227
  batch 462 loss: 0.2671080208443976
  batch 463 loss: 0.26707961917180995
  batch 464 loss: 0.26713332161307335
  batch 465 loss: 0.2670904902360773
  batch 466 loss: 0.2670110767875107
  batch 467 loss: 0.26704998962118487
  batch 468 loss: 0.2670302184728476
  batch 469 loss: 0.26713471854927695
  batch 470 loss: 0.26719797190199507
  batch 471 loss: 0.26717008881255067
  batch 472 loss: 0.26701717280735404
LOSS train 0.26701717280735404 valid 0.2672646641731262
LOSS train 0.26701717280735404 valid 0.24077720940113068
LOSS train 0.26701717280735404 valid 0.2440271476904551
LOSS train 0.26701717280735404 valid 0.2315121591091156
LOSS train 0.26701717280735404 valid 0.2240348607301712
LOSS train 0.26701717280735404 valid 0.22340696305036545
LOSS train 0.26701717280735404 valid 0.23528970777988434
LOSS train 0.26701717280735404 valid 0.2302695345133543
LOSS train 0.26701717280735404 valid 0.22914905183845097
LOSS train 0.26701717280735404 valid 0.23184788674116136
LOSS train 0.26701717280735404 valid 0.22982396591793408
LOSS train 0.26701717280735404 valid 0.22897432868679365
LOSS train 0.26701717280735404 valid 0.22709536781677833
LOSS train 0.26701717280735404 valid 0.22822562711579458
LOSS train 0.26701717280735404 valid 0.2235519766807556
LOSS train 0.26701717280735404 valid 0.22518689930438995
LOSS train 0.26701717280735404 valid 0.22578338577466853
LOSS train 0.26701717280735404 valid 0.22593663384517035
LOSS train 0.26701717280735404 valid 0.22854732447548917
LOSS train 0.26701717280735404 valid 0.2282764807343483
LOSS train 0.26701717280735404 valid 0.2284580689100992
LOSS train 0.26701717280735404 valid 0.22693741592493924
LOSS train 0.26701717280735404 valid 0.2282559949418773
LOSS train 0.26701717280735404 valid 0.22745378191272417
LOSS train 0.26701717280735404 valid 0.22548551619052887
LOSS train 0.26701717280735404 valid 0.2251992838887068
LOSS train 0.26701717280735404 valid 0.22525081038475037
LOSS train 0.26701717280735404 valid 0.2263071664742061
LOSS train 0.26701717280735404 valid 0.2269737329976312
LOSS train 0.26701717280735404 valid 0.22737854967514673
LOSS train 0.26701717280735404 valid 0.22855995980001265
LOSS train 0.26701717280735404 valid 0.2285443227738142
LOSS train 0.26701717280735404 valid 0.2296214374628934
LOSS train 0.26701717280735404 valid 0.2292337154640871
LOSS train 0.26701717280735404 valid 0.2311707420008523
LOSS train 0.26701717280735404 valid 0.2305682963795132
LOSS train 0.26701717280735404 valid 0.23093006578651634
LOSS train 0.26701717280735404 valid 0.2316159637350785
LOSS train 0.26701717280735404 valid 0.23099442246632698
LOSS train 0.26701717280735404 valid 0.23119806833565235
LOSS train 0.26701717280735404 valid 0.23109472779239096
LOSS train 0.26701717280735404 valid 0.23126212613923208
LOSS train 0.26701717280735404 valid 0.23054398076478824
LOSS train 0.26701717280735404 valid 0.23057344284924594
LOSS train 0.26701717280735404 valid 0.2302655279636383
LOSS train 0.26701717280735404 valid 0.23109829490599426
LOSS train 0.26701717280735404 valid 0.23145202024185912
LOSS train 0.26701717280735404 valid 0.2313773004959027
LOSS train 0.26701717280735404 valid 0.23166988850856313
LOSS train 0.26701717280735404 valid 0.23100441455841064
LOSS train 0.26701717280735404 valid 0.2316112921518438
LOSS train 0.26701717280735404 valid 0.2313728710779777
LOSS train 0.26701717280735404 valid 0.23147757548206258
LOSS train 0.26701717280735404 valid 0.23205596208572388
LOSS train 0.26701717280735404 valid 0.2320128332484852
LOSS train 0.26701717280735404 valid 0.23178906446056707
LOSS train 0.26701717280735404 valid 0.2317795808378019
LOSS train 0.26701717280735404 valid 0.23131701812661928
LOSS train 0.26701717280735404 valid 0.23211186972715087
LOSS train 0.26701717280735404 valid 0.23141502117117246
LOSS train 0.26701717280735404 valid 0.23122393400942692
LOSS train 0.26701717280735404 valid 0.23178588334591158
LOSS train 0.26701717280735404 valid 0.2320665359970123
LOSS train 0.26701717280735404 valid 0.23304594424553216
LOSS train 0.26701717280735404 valid 0.23297090232372283
LOSS train 0.26701717280735404 valid 0.23317365687001834
LOSS train 0.26701717280735404 valid 0.23229881587313184
LOSS train 0.26701717280735404 valid 0.23268819030593424
LOSS train 0.26701717280735404 valid 0.2317004022390946
LOSS train 0.26701717280735404 valid 0.23187039090054376
LOSS train 0.26701717280735404 valid 0.2314653025126793
LOSS train 0.26701717280735404 valid 0.23176144663658407
LOSS train 0.26701717280735404 valid 0.23173291899570048
LOSS train 0.26701717280735404 valid 0.23137375329797333
LOSS train 0.26701717280735404 valid 0.2311056790749232
LOSS train 0.26701717280735404 valid 0.23185688433678528
LOSS train 0.26701717280735404 valid 0.23149648618388485
LOSS train 0.26701717280735404 valid 0.2313786240724417
LOSS train 0.26701717280735404 valid 0.2315410922599744
LOSS train 0.26701717280735404 valid 0.23086595367640256
LOSS train 0.26701717280735404 valid 0.2297675602230025
LOSS train 0.26701717280735404 valid 0.22985005960231875
LOSS train 0.26701717280735404 valid 0.22946675750146428
LOSS train 0.26701717280735404 valid 0.22938972072941916
LOSS train 0.26701717280735404 valid 0.22858758042840396
LOSS train 0.26701717280735404 valid 0.22782285130301186
LOSS train 0.26701717280735404 valid 0.22792043391315417
LOSS train 0.26701717280735404 valid 0.22752092507752505
LOSS train 0.26701717280735404 valid 0.22773230343722226
LOSS train 0.26701717280735404 valid 0.22781053053008185
LOSS train 0.26701717280735404 valid 0.22779653999176655
LOSS train 0.26701717280735404 valid 0.2277616187930107
LOSS train 0.26701717280735404 valid 0.22754370445205319
LOSS train 0.26701717280735404 valid 0.2278840882664031
LOSS train 0.26701717280735404 valid 0.22735837682297355
LOSS train 0.26701717280735404 valid 0.22730172937735915
LOSS train 0.26701717280735404 valid 0.22742856009719298
LOSS train 0.26701717280735404 valid 0.2278438523715856
LOSS train 0.26701717280735404 valid 0.22791787741160152
LOSS train 0.26701717280735404 valid 0.2280317071080208
LOSS train 0.26701717280735404 valid 0.22850697583491258
LOSS train 0.26701717280735404 valid 0.22874280956445955
LOSS train 0.26701717280735404 valid 0.2284570692233669
LOSS train 0.26701717280735404 valid 0.22812328731211332
LOSS train 0.26701717280735404 valid 0.22857678149427688
LOSS train 0.26701717280735404 valid 0.22885722273363257
LOSS train 0.26701717280735404 valid 0.22838685348212162
LOSS train 0.26701717280735404 valid 0.2283025534340629
LOSS train 0.26701717280735404 valid 0.22876834062808152
LOSS train 0.26701717280735404 valid 0.2291105794635686
LOSS train 0.26701717280735404 valid 0.22867911651327805
LOSS train 0.26701717280735404 valid 0.22864767456693308
LOSS train 0.26701717280735404 valid 0.2287979060042221
LOSS train 0.26701717280735404 valid 0.228722432595596
LOSS train 0.26701717280735404 valid 0.22889779378538547
LOSS train 0.26701717280735404 valid 0.2293203700462292
LOSS train 0.26701717280735404 valid 0.22969951525203183
LOSS train 0.26701717280735404 valid 0.2296755061311237
LOSS train 0.26701717280735404 valid 0.22961716992514475
LOSS train 0.26701717280735404 valid 0.22937839478254318
LOSS train 0.26701717280735404 valid 0.22927960310100523
LOSS train 0.26701717280735404 valid 0.22927205618776259
LOSS train 0.26701717280735404 valid 0.2295264539922156
LOSS train 0.26701717280735404 valid 0.22972996448797564
LOSS train 0.26701717280735404 valid 0.22976679718494417
LOSS train 0.26701717280735404 valid 0.2299091653927924
LOSS train 0.26701717280735404 valid 0.22989100628481138
LOSS train 0.26701717280735404 valid 0.230031797895208
LOSS train 0.26701717280735404 valid 0.23031599276749662
LOSS train 0.26701717280735404 valid 0.23012500623097787
LOSS train 0.26701717280735404 valid 0.2301144580577166
LOSS train 0.26701717280735404 valid 0.22978960756551137
LOSS train 0.26701717280735404 valid 0.2294936629390358
LOSS train 0.26701717280735404 valid 0.22973233161132728
LOSS train 0.26701717280735404 valid 0.229744671009205
LOSS train 0.26701717280735404 valid 0.22944501042366028
LOSS train 0.26701717280735404 valid 0.2292102707998596
LOSS train 0.26701717280735404 valid 0.22910148218490076
LOSS train 0.26701717280735404 valid 0.2288793629879574
LOSS train 0.26701717280735404 valid 0.2289321929216385
LOSS train 0.26701717280735404 valid 0.229068194403716
LOSS train 0.26701717280735404 valid 0.2292118748308907
LOSS train 0.26701717280735404 valid 0.22944632089221395
LOSS train 0.26701717280735404 valid 0.2296627118355698
LOSS train 0.26701717280735404 valid 0.22949265621859452
LOSS train 0.26701717280735404 valid 0.22955625585905493
LOSS train 0.26701717280735404 valid 0.22933169469541434
LOSS train 0.26701717280735404 valid 0.2301802951339129
LOSS train 0.26701717280735404 valid 0.23022871289477254
LOSS train 0.26701717280735404 valid 0.23011014680067698
LOSS train 0.26701717280735404 valid 0.2302344710818979
LOSS train 0.26701717280735404 valid 0.2298934443020507
LOSS train 0.26701717280735404 valid 0.2300595607048546
LOSS train 0.26701717280735404 valid 0.23014975252089562
LOSS train 0.26701717280735404 valid 0.23006303339235246
LOSS train 0.26701717280735404 valid 0.23039478703569144
LOSS train 0.26701717280735404 valid 0.23026796102903452
LOSS train 0.26701717280735404 valid 0.23032410603158082
LOSS train 0.26701717280735404 valid 0.23029468035173117
LOSS train 0.26701717280735404 valid 0.23005444956943394
LOSS train 0.26701717280735404 valid 0.22996697820121456
LOSS train 0.26701717280735404 valid 0.22981009466780555
LOSS train 0.26701717280735404 valid 0.2297057055074013
LOSS train 0.26701717280735404 valid 0.22954774620692905
LOSS train 0.26701717280735404 valid 0.22953435516718662
LOSS train 0.26701717280735404 valid 0.22942883718803705
LOSS train 0.26701717280735404 valid 0.22956010190669648
LOSS train 0.26701717280735404 valid 0.2293960094629299
LOSS train 0.26701717280735404 valid 0.22947275779656404
LOSS train 0.26701717280735404 valid 0.22962804611991433
LOSS train 0.26701717280735404 valid 0.2295473681555854
LOSS train 0.26701717280735404 valid 0.22942606827547385
LOSS train 0.26701717280735404 valid 0.22935637487152408
LOSS train 0.26701717280735404 valid 0.2294055488945424
LOSS train 0.26701717280735404 valid 0.22932205719607218
LOSS train 0.26701717280735404 valid 0.22932567295025696
LOSS train 0.26701717280735404 valid 0.22920011160737377
LOSS train 0.26701717280735404 valid 0.22932488027583348
LOSS train 0.26701717280735404 valid 0.22923432998151086
LOSS train 0.26701717280735404 valid 0.22925922067628968
LOSS train 0.26701717280735404 valid 0.22935214737502252
LOSS train 0.26701717280735404 valid 0.22948296580995833
LOSS train 0.26701717280735404 valid 0.22939936450270357
LOSS train 0.26701717280735404 valid 0.229595177685437
LOSS train 0.26701717280735404 valid 0.22940741237756368
LOSS train 0.26701717280735404 valid 0.22935983883116834
LOSS train 0.26701717280735404 valid 0.22920650243759155
LOSS train 0.26701717280735404 valid 0.22920422699857265
LOSS train 0.26701717280735404 valid 0.22916232719623222
LOSS train 0.26701717280735404 valid 0.229440029828172
LOSS train 0.26701717280735404 valid 0.229357263336631
LOSS train 0.26701717280735404 valid 0.229326972970739
LOSS train 0.26701717280735404 valid 0.22938120619929517
LOSS train 0.26701717280735404 valid 0.22921129638693996
LOSS train 0.26701717280735404 valid 0.22892363629279994
LOSS train 0.26701717280735404 valid 0.22901754719870432
LOSS train 0.26701717280735404 valid 0.22921387026757759
LOSS train 0.26701717280735404 valid 0.22901016654390277
LOSS train 0.26701717280735404 valid 0.22907796382305012
LOSS train 0.26701717280735404 valid 0.2289871157705784
LOSS train 0.26701717280735404 valid 0.2288685871742258
LOSS train 0.26701717280735404 valid 0.2288224466956488
LOSS train 0.26701717280735404 valid 0.22871213666911194
LOSS train 0.26701717280735404 valid 0.22869969188582664
LOSS train 0.26701717280735404 valid 0.2285795559243458
LOSS train 0.26701717280735404 valid 0.2284764119432968
LOSS train 0.26701717280735404 valid 0.22839512017326077
LOSS train 0.26701717280735404 valid 0.22830570209771395
LOSS train 0.26701717280735404 valid 0.22820810125204935
LOSS train 0.26701717280735404 valid 0.2283191204071045
LOSS train 0.26701717280735404 valid 0.2284430796501196
LOSS train 0.26701717280735404 valid 0.228364977634178
LOSS train 0.26701717280735404 valid 0.22846378903713585
LOSS train 0.26701717280735404 valid 0.22836764574607957
LOSS train 0.26701717280735404 valid 0.2281692433495854
LOSS train 0.26701717280735404 valid 0.22809108581255982
LOSS train 0.26701717280735404 valid 0.2279083018753386
LOSS train 0.26701717280735404 valid 0.2279982793085072
LOSS train 0.26701717280735404 valid 0.22814905105932662
LOSS train 0.26701717280735404 valid 0.22830014208501034
LOSS train 0.26701717280735404 valid 0.22838258217362797
LOSS train 0.26701717280735404 valid 0.22828937315189088
LOSS train 0.26701717280735404 valid 0.22836969264954196
LOSS train 0.26701717280735404 valid 0.22846845923257725
LOSS train 0.26701717280735404 valid 0.22846502761046092
LOSS train 0.26701717280735404 valid 0.22862262806269976
LOSS train 0.26701717280735404 valid 0.22880485529153882
LOSS train 0.26701717280735404 valid 0.22890227229187363
LOSS train 0.26701717280735404 valid 0.22892578909230546
LOSS train 0.26701717280735404 valid 0.22911216571279194
LOSS train 0.26701717280735404 valid 0.22914089488260675
LOSS train 0.26701717280735404 valid 0.2292059000966878
LOSS train 0.26701717280735404 valid 0.22916637565700793
LOSS train 0.26701717280735404 valid 0.22918940781273395
LOSS train 0.26701717280735404 valid 0.2293097160598065
LOSS train 0.26701717280735404 valid 0.2291411336953357
LOSS train 0.26701717280735404 valid 0.22933869887504899
LOSS train 0.26701717280735404 valid 0.22923783488383814
LOSS train 0.26701717280735404 valid 0.22923693341440735
LOSS train 0.26701717280735404 valid 0.229221972450614
LOSS train 0.26701717280735404 valid 0.22930636593909678
LOSS train 0.26701717280735404 valid 0.229091436101878
LOSS train 0.26701717280735404 valid 0.22921924182662257
LOSS train 0.26701717280735404 valid 0.22940175253592554
LOSS train 0.26701717280735404 valid 0.22957805243073678
LOSS train 0.26701717280735404 valid 0.22954816995112876
LOSS train 0.26701717280735404 valid 0.2295120759652211
LOSS train 0.26701717280735404 valid 0.22946655630104004
LOSS train 0.26701717280735404 valid 0.2293657181133707
LOSS train 0.26701717280735404 valid 0.2294728437066078
LOSS train 0.26701717280735404 valid 0.2294498772853874
LOSS train 0.26701717280735404 valid 0.22941823664402206
LOSS train 0.26701717280735404 valid 0.22932679373991818
LOSS train 0.26701717280735404 valid 0.22929805481997062
LOSS train 0.26701717280735404 valid 0.22934127280525132
LOSS train 0.26701717280735404 valid 0.22932508209487423
LOSS train 0.26701717280735404 valid 0.2292372608115237
LOSS train 0.26701717280735404 valid 0.22935913612907247
LOSS train 0.26701717280735404 valid 0.2293378512256394
LOSS train 0.26701717280735404 valid 0.22932180320987336
LOSS train 0.26701717280735404 valid 0.22947718899597153
LOSS train 0.26701717280735404 valid 0.22944944954782953
LOSS train 0.26701717280735404 valid 0.22951385531815285
LOSS train 0.26701717280735404 valid 0.22946944906178748
LOSS train 0.26701717280735404 valid 0.229476871243063
LOSS train 0.26701717280735404 valid 0.22944325600799761
LOSS train 0.26701717280735404 valid 0.22945678524310223
LOSS train 0.26701717280735404 valid 0.2295057944842239
LOSS train 0.26701717280735404 valid 0.22974795402204237
LOSS train 0.26701717280735404 valid 0.2299026600740574
LOSS train 0.26701717280735404 valid 0.23001758528811464
LOSS train 0.26701717280735404 valid 0.23033763128606713
LOSS train 0.26701717280735404 valid 0.23035645517674122
LOSS train 0.26701717280735404 valid 0.23024263657140034
LOSS train 0.26701717280735404 valid 0.23025128191167657
LOSS train 0.26701717280735404 valid 0.23011218734841415
LOSS train 0.26701717280735404 valid 0.2299589572830751
LOSS train 0.26701717280735404 valid 0.2297712766009269
LOSS train 0.26701717280735404 valid 0.22981434109817697
LOSS train 0.26701717280735404 valid 0.22970344296523504
LOSS train 0.26701717280735404 valid 0.22961009710072622
LOSS train 0.26701717280735404 valid 0.22951983322277136
LOSS train 0.26701717280735404 valid 0.22951594058279434
LOSS train 0.26701717280735404 valid 0.2295361471847749
LOSS train 0.26701717280735404 valid 0.2295871118704478
LOSS train 0.26701717280735404 valid 0.22951033456133796
LOSS train 0.26701717280735404 valid 0.22941034279426215
LOSS train 0.26701717280735404 valid 0.22947039605221814
LOSS train 0.26701717280735404 valid 0.2294508341996315
LOSS train 0.26701717280735404 valid 0.22953904492074045
LOSS train 0.26701717280735404 valid 0.2294543853023208
LOSS train 0.26701717280735404 valid 0.22950688438260392
LOSS train 0.26701717280735404 valid 0.22946632162702776
LOSS train 0.26701717280735404 valid 0.22968845718166456
LOSS train 0.26701717280735404 valid 0.22978831388182558
LOSS train 0.26701717280735404 valid 0.22969352081418037
LOSS train 0.26701717280735404 valid 0.22969288787857853
LOSS train 0.26701717280735404 valid 0.22962020057559812
LOSS train 0.26701717280735404 valid 0.22965367766327682
LOSS train 0.26701717280735404 valid 0.2295745520790418
LOSS train 0.26701717280735404 valid 0.22972823664595518
LOSS train 0.26701717280735404 valid 0.2297241853562412
LOSS train 0.26701717280735404 valid 0.22980745623607446
LOSS train 0.26701717280735404 valid 0.22987006222339054
LOSS train 0.26701717280735404 valid 0.22976108320423813
LOSS train 0.26701717280735404 valid 0.2297076346243129
LOSS train 0.26701717280735404 valid 0.2297134922168154
LOSS train 0.26701717280735404 valid 0.22971317767129315
LOSS train 0.26701717280735404 valid 0.22973142062768967
LOSS train 0.26701717280735404 valid 0.22961109679552816
LOSS train 0.26701717280735404 valid 0.2294496156970021
LOSS train 0.26701717280735404 valid 0.22957067468609565
LOSS train 0.26701717280735404 valid 0.22952229912859945
LOSS train 0.26701717280735404 valid 0.22962489785851947
LOSS train 0.26701717280735404 valid 0.22963506210417975
LOSS train 0.26701717280735404 valid 0.2296127315752114
LOSS train 0.26701717280735404 valid 0.22964003992381532
LOSS train 0.26701717280735404 valid 0.22977483853604058
LOSS train 0.26701717280735404 valid 0.22980259924099364
LOSS train 0.26701717280735404 valid 0.22969612441956996
LOSS train 0.26701717280735404 valid 0.22977444668796576
LOSS train 0.26701717280735404 valid 0.22974636692623174
LOSS train 0.26701717280735404 valid 0.2297475137762241
LOSS train 0.26701717280735404 valid 0.2297470114297337
LOSS train 0.26701717280735404 valid 0.2298098245033851
LOSS train 0.26701717280735404 valid 0.22991929664933608
LOSS train 0.26701717280735404 valid 0.22999570302277894
LOSS train 0.26701717280735404 valid 0.22995673824192547
LOSS train 0.26701717280735404 valid 0.23017671491418565
LOSS train 0.26701717280735404 valid 0.23010532783739496
LOSS train 0.26701717280735404 valid 0.230000016777537
LOSS train 0.26701717280735404 valid 0.22997764834617995
LOSS train 0.26701717280735404 valid 0.22995479646566752
LOSS train 0.26701717280735404 valid 0.23010026253090649
LOSS train 0.26701717280735404 valid 0.23016447286997269
LOSS train 0.26701717280735404 valid 0.23014477290035712
LOSS train 0.26701717280735404 valid 0.2301782087950749
LOSS train 0.26701717280735404 valid 0.2301837679373442
LOSS train 0.26701717280735404 valid 0.23015830580112154
LOSS train 0.26701717280735404 valid 0.23015654236078262
LOSS train 0.26701717280735404 valid 0.23012902432522817
LOSS train 0.26701717280735404 valid 0.23004824180177777
LOSS train 0.26701717280735404 valid 0.2299986705401201
LOSS train 0.26701717280735404 valid 0.23026795706949954
LOSS train 0.26701717280735404 valid 0.23024262077566507
LOSS train 0.26701717280735404 valid 0.23017755930306594
LOSS train 0.26701717280735404 valid 0.23009265443948917
LOSS train 0.26701717280735404 valid 0.2299591875367466
LOSS train 0.26701717280735404 valid 0.229995628610723
LOSS train 0.26701717280735404 valid 0.22992332586220332
LOSS train 0.26701717280735404 valid 0.22983134697135696
LOSS train 0.26701717280735404 valid 0.2298290693116459
LOSS train 0.26701717280735404 valid 0.22984994584879187
LOSS train 0.26701717280735404 valid 0.22981194688775444
LOSS train 0.26701717280735404 valid 0.23000686974592613
LOSS train 0.26701717280735404 valid 0.2300149333778392
LOSS train 0.26701717280735404 valid 0.22996801734805442
LOSS train 0.26701717280735404 valid 0.22992195105752466
LOSS train 0.26701717280735404 valid 0.22991942632995277
LOSS train 0.26701717280735404 valid 0.22986153463522593
LOSS train 0.26701717280735404 valid 0.22979901561776686
LOSS train 0.26701717280735404 valid 0.2298537559848464
LOSS train 0.26701717280735404 valid 0.22958657441060404
LOSS train 0.26701717280735404 valid 0.22963123973254318
LOSS train 0.26701717280735404 valid 0.22963418736033245
LOSS train 0.26701717280735404 valid 0.229549458973069
LOSS train 0.26701717280735404 valid 0.22935216132886405
LOSS train 0.26701717280735404 valid 0.22930421785491964
LOSS train 0.26701717280735404 valid 0.2293710413017893
EPOCH 11:
  batch 1 loss: 0.22796764969825745
  batch 2 loss: 0.221245676279068
  batch 3 loss: 0.2345470388730367
  batch 4 loss: 0.24256113171577454
  batch 5 loss: 0.25393842458724974
  batch 6 loss: 0.2566743493080139
  batch 7 loss: 0.25669161336762564
  batch 8 loss: 0.25968921557068825
  batch 9 loss: 0.26078131794929504
  batch 10 loss: 0.263161900639534
  batch 11 loss: 0.2617924741723321
  batch 12 loss: 0.2597193817297618
  batch 13 loss: 0.25868579057546764
  batch 14 loss: 0.25927778439862387
  batch 15 loss: 0.25983585715293883
  batch 16 loss: 0.2596910782158375
  batch 17 loss: 0.2569974222603966
  batch 18 loss: 0.2588183300362693
  batch 19 loss: 0.2571081126991071
  batch 20 loss: 0.2558443285524845
  batch 21 loss: 0.25627860781692324
  batch 22 loss: 0.2567476894367825
  batch 23 loss: 0.2561476593432219
  batch 24 loss: 0.25421282028158504
  batch 25 loss: 0.2556790816783905
  batch 26 loss: 0.2540452927350998
  batch 27 loss: 0.25433413960315565
  batch 28 loss: 0.252872892256294
  batch 29 loss: 0.2526245605328987
  batch 30 loss: 0.2520199159781138
  batch 31 loss: 0.2532856493226944
  batch 32 loss: 0.2530982284806669
  batch 33 loss: 0.25306445495648816
  batch 34 loss: 0.2526870444417
  batch 35 loss: 0.25278991843972887
  batch 36 loss: 0.2529740586049027
  batch 37 loss: 0.25368922989110687
  batch 38 loss: 0.2545699222307456
  batch 39 loss: 0.254378331777377
  batch 40 loss: 0.254808396846056
  batch 41 loss: 0.2553555965423584
  batch 42 loss: 0.25554885892640977
  batch 43 loss: 0.25599916106046633
  batch 44 loss: 0.25611970167268405
  batch 45 loss: 0.2551231990257899
  batch 46 loss: 0.2548034913513971
  batch 47 loss: 0.2547616942765865
  batch 48 loss: 0.25380542998512584
  batch 49 loss: 0.253422027643846
  batch 50 loss: 0.2531123819947243
  batch 51 loss: 0.2527692145576664
  batch 52 loss: 0.25320534436748576
  batch 53 loss: 0.252798411924884
  batch 54 loss: 0.252901559902562
  batch 55 loss: 0.25275729650800877
  batch 56 loss: 0.2529395039060286
  batch 57 loss: 0.253224833754071
  batch 58 loss: 0.2537921042791728
  batch 59 loss: 0.25405611350374707
  batch 60 loss: 0.25425368125240005
  batch 61 loss: 0.2548946257008881
  batch 62 loss: 0.25571891112673667
  batch 63 loss: 0.2556812827076231
  batch 64 loss: 0.25664218817837536
  batch 65 loss: 0.2564252692919511
  batch 66 loss: 0.25661641946344665
  batch 67 loss: 0.25672250407845226
  batch 68 loss: 0.25742266853066054
  batch 69 loss: 0.2570807203866433
  batch 70 loss: 0.2574762595551355
  batch 71 loss: 0.25728055268106326
  batch 72 loss: 0.25759431740476024
  batch 73 loss: 0.2576734842094657
  batch 74 loss: 0.2578177293007438
  batch 75 loss: 0.2576172387599945
  batch 76 loss: 0.2582184570400338
  batch 77 loss: 0.2579292111195527
  batch 78 loss: 0.2580319667855899
  batch 79 loss: 0.2586925903075858
  batch 80 loss: 0.2583523992449045
  batch 81 loss: 0.258401835774198
  batch 82 loss: 0.25842615525896956
  batch 83 loss: 0.2582782293299595
  batch 84 loss: 0.25830447549621266
  batch 85 loss: 0.2578204919310177
  batch 86 loss: 0.25852521283682
  batch 87 loss: 0.25871304499691933
  batch 88 loss: 0.25850880501622503
  batch 89 loss: 0.2585291602973188
  batch 90 loss: 0.2587189626362589
  batch 91 loss: 0.25913259337891587
  batch 92 loss: 0.25947168841958046
  batch 93 loss: 0.25982941366652007
  batch 94 loss: 0.26049424945674043
  batch 95 loss: 0.26064111260991346
  batch 96 loss: 0.2609719908796251
  batch 97 loss: 0.2613599086852418
  batch 98 loss: 0.2618671641970167
  batch 99 loss: 0.26218099290072316
  batch 100 loss: 0.26219740822911264
  batch 101 loss: 0.26192478540510233
  batch 102 loss: 0.26209091132178025
  batch 103 loss: 0.2627168704294464
  batch 104 loss: 0.26266501600352615
  batch 105 loss: 0.26254621900263286
  batch 106 loss: 0.26278291825415956
  batch 107 loss: 0.2622190907458279
  batch 108 loss: 0.26216661364391997
  batch 109 loss: 0.2619867946850051
  batch 110 loss: 0.26205711405385623
  batch 111 loss: 0.26203774050012363
  batch 112 loss: 0.2618550965562463
  batch 113 loss: 0.26183537347654323
  batch 114 loss: 0.2620906616773522
  batch 115 loss: 0.2620762865180555
  batch 116 loss: 0.26212522862800236
  batch 117 loss: 0.26233144180896956
  batch 118 loss: 0.2619381197175737
  batch 119 loss: 0.2620604730704251
  batch 120 loss: 0.26184810847043993
  batch 121 loss: 0.2616661168819617
  batch 122 loss: 0.2614427626621528
  batch 123 loss: 0.26147940342988424
  batch 124 loss: 0.2617487604579618
  batch 125 loss: 0.26171623611450195
  batch 126 loss: 0.26179151473537327
  batch 127 loss: 0.2622086356474659
  batch 128 loss: 0.2620340106077492
  batch 129 loss: 0.2622206814067308
  batch 130 loss: 0.26200008896680976
  batch 131 loss: 0.26213213596635193
  batch 132 loss: 0.262019678611647
  batch 133 loss: 0.2621894818275495
  batch 134 loss: 0.26227066443482444
  batch 135 loss: 0.2621925931285929
  batch 136 loss: 0.2621025885729229
  batch 137 loss: 0.26207978642769975
  batch 138 loss: 0.26216580081677093
  batch 139 loss: 0.2624535200407179
  batch 140 loss: 0.26245165488549643
  batch 141 loss: 0.26252714708341773
  batch 142 loss: 0.26257535017712014
  batch 143 loss: 0.2624132405002634
  batch 144 loss: 0.2621726201226314
  batch 145 loss: 0.2619277267620481
  batch 146 loss: 0.26197080824473135
  batch 147 loss: 0.2623191406937683
  batch 148 loss: 0.2621475964583255
  batch 149 loss: 0.2618980274704479
  batch 150 loss: 0.2621351137757301
  batch 151 loss: 0.26230098780812017
  batch 152 loss: 0.26238792105332803
  batch 153 loss: 0.2624235011978087
  batch 154 loss: 0.26311917322409617
  batch 155 loss: 0.2632389598315762
  batch 156 loss: 0.26323658275680667
  batch 157 loss: 0.2634950453878208
  batch 158 loss: 0.26384717688153064
  batch 159 loss: 0.2643213259911387
  batch 160 loss: 0.2641420771367848
  batch 161 loss: 0.264268612732058
  batch 162 loss: 0.2644915927523448
  batch 163 loss: 0.26450841951589643
  batch 164 loss: 0.2645019691710065
  batch 165 loss: 0.26454524352695
  batch 166 loss: 0.26444113981651973
  batch 167 loss: 0.26429337843092615
  batch 168 loss: 0.2642578678649096
  batch 169 loss: 0.26410769622706803
  batch 170 loss: 0.26408712986637567
  batch 171 loss: 0.26412214382350097
  batch 172 loss: 0.2641086859065433
  batch 173 loss: 0.2641197684183286
  batch 174 loss: 0.2641648095229576
  batch 175 loss: 0.2642831471988133
  batch 176 loss: 0.26414400203661487
  batch 177 loss: 0.26405661809915876
  batch 178 loss: 0.26416418542352954
  batch 179 loss: 0.2642658977202197
  batch 180 loss: 0.26420941435628464
  batch 181 loss: 0.2640661570084029
  batch 182 loss: 0.2641515822692232
  batch 183 loss: 0.2639112941554335
  batch 184 loss: 0.2637382489831551
  batch 185 loss: 0.26364350480002324
  batch 186 loss: 0.2637792263300188
  batch 187 loss: 0.2637441051516303
  batch 188 loss: 0.263376126264004
  batch 189 loss: 0.26310132506978573
  batch 190 loss: 0.263032554090023
  batch 191 loss: 0.26292201603582394
  batch 192 loss: 0.26295413069116574
  batch 193 loss: 0.263021585314385
  batch 194 loss: 0.2631499820302442
  batch 195 loss: 0.2632144733117177
  batch 196 loss: 0.2630491099339359
  batch 197 loss: 0.2629355914550384
  batch 198 loss: 0.26308570117360414
  batch 199 loss: 0.26327588622594
  batch 200 loss: 0.2633773302286863
  batch 201 loss: 0.26343551611722404
  batch 202 loss: 0.26337428206559454
  batch 203 loss: 0.26342697426896966
  batch 204 loss: 0.2632629794671255
  batch 205 loss: 0.2634225533502858
  batch 206 loss: 0.26335453618209337
  batch 207 loss: 0.2634158993256841
  batch 208 loss: 0.2632986379500765
  batch 209 loss: 0.2631458923149337
  batch 210 loss: 0.26314798629000075
  batch 211 loss: 0.26313741137913615
  batch 212 loss: 0.26306923429639834
  batch 213 loss: 0.2631394817655635
  batch 214 loss: 0.26321242338864603
  batch 215 loss: 0.26295957489069116
  batch 216 loss: 0.2627713912063175
  batch 217 loss: 0.2627449675639104
  batch 218 loss: 0.2626910077049098
  batch 219 loss: 0.26258902374195725
  batch 220 loss: 0.26248377507383175
  batch 221 loss: 0.26255933812301085
  batch 222 loss: 0.2626615797614192
  batch 223 loss: 0.26277248140407783
  batch 224 loss: 0.2628345065084951
  batch 225 loss: 0.2626685439215766
  batch 226 loss: 0.2627851798207359
  batch 227 loss: 0.2625316762188983
  batch 228 loss: 0.26251457893011865
  batch 229 loss: 0.2624352816410981
  batch 230 loss: 0.26236537008181865
  batch 231 loss: 0.2622835316828319
  batch 232 loss: 0.2619723096361448
  batch 233 loss: 0.2619748574086013
  batch 234 loss: 0.26209488313676965
  batch 235 loss: 0.26215745356488734
  batch 236 loss: 0.2620925582566504
  batch 237 loss: 0.26210262843325166
  batch 238 loss: 0.26199074847107173
  batch 239 loss: 0.26192247693009957
  batch 240 loss: 0.26205989594260853
  batch 241 loss: 0.2622045764537273
  batch 242 loss: 0.26210250957938264
  batch 243 loss: 0.26233054016843255
  batch 244 loss: 0.2623591656323339
  batch 245 loss: 0.26239183946531647
  batch 246 loss: 0.2624064635455124
  batch 247 loss: 0.26250906765219656
  batch 248 loss: 0.2626301922625111
  batch 249 loss: 0.26266323574575556
  batch 250 loss: 0.262598317861557
  batch 251 loss: 0.2625679991159781
  batch 252 loss: 0.26245574919240816
  batch 253 loss: 0.2622975571589036
  batch 254 loss: 0.26217740275493756
  batch 255 loss: 0.26218131482601165
  batch 256 loss: 0.2621971676708199
  batch 257 loss: 0.2622347810968815
  batch 258 loss: 0.2622781471573105
  batch 259 loss: 0.26238066274456995
  batch 260 loss: 0.26245151007404693
  batch 261 loss: 0.2625540634795624
  batch 262 loss: 0.2625553266123961
  batch 263 loss: 0.26260575578466566
  batch 264 loss: 0.2624674303965135
  batch 265 loss: 0.2624493458360996
  batch 266 loss: 0.2623896409470336
  batch 267 loss: 0.2624984345632546
  batch 268 loss: 0.262438050790954
  batch 269 loss: 0.26240655375457606
  batch 270 loss: 0.2626666932746216
  batch 271 loss: 0.262781155340346
  batch 272 loss: 0.2628678978037308
  batch 273 loss: 0.2628553738415023
  batch 274 loss: 0.26301031092005056
  batch 275 loss: 0.2630325788259506
  batch 276 loss: 0.2630784357695476
  batch 277 loss: 0.2630979734530087
  batch 278 loss: 0.26317050597436137
  batch 279 loss: 0.26338096785502624
  batch 280 loss: 0.2633038287184068
  batch 281 loss: 0.26319669452214156
  batch 282 loss: 0.263259859953789
  batch 283 loss: 0.26321822968262243
  batch 284 loss: 0.2632311737348496
  batch 285 loss: 0.26320774957799076
  batch 286 loss: 0.263208904414327
  batch 287 loss: 0.2632211148219657
  batch 288 loss: 0.2630445279387964
  batch 289 loss: 0.263263382507443
  batch 290 loss: 0.26308924417043555
  batch 291 loss: 0.26308871949046747
  batch 292 loss: 0.26310154198579594
  batch 293 loss: 0.2631290263588518
  batch 294 loss: 0.2630297494988863
  batch 295 loss: 0.2630932620016195
  batch 296 loss: 0.2631312250285535
  batch 297 loss: 0.2631423633909386
  batch 298 loss: 0.2631560455032643
  batch 299 loss: 0.26312650735162973
  batch 300 loss: 0.26308640370766323
  batch 301 loss: 0.2630363551782215
  batch 302 loss: 0.26306430171459716
  batch 303 loss: 0.26305640820819554
  batch 304 loss: 0.26308884751051664
  batch 305 loss: 0.26299222184986365
  batch 306 loss: 0.2630579893787702
  batch 307 loss: 0.26297170151910876
  batch 308 loss: 0.2631120573874418
  batch 309 loss: 0.2631120870317842
  batch 310 loss: 0.2630757432310812
  batch 311 loss: 0.26319491388905086
  batch 312 loss: 0.2632651732613643
  batch 313 loss: 0.26333104892851067
  batch 314 loss: 0.26341726441102425
  batch 315 loss: 0.26342334269531187
  batch 316 loss: 0.26334693730820585
  batch 317 loss: 0.263422579842411
  batch 318 loss: 0.26342339209228194
  batch 319 loss: 0.2633540652762386
  batch 320 loss: 0.26329007362946866
  batch 321 loss: 0.2632852453866109
  batch 322 loss: 0.2632095841054591
  batch 323 loss: 0.26308239249068516
  batch 324 loss: 0.26284739237140725
  batch 325 loss: 0.26277666885119216
  batch 326 loss: 0.2627406599788578
  batch 327 loss: 0.26274011427656224
  batch 328 loss: 0.26260675326353167
  batch 329 loss: 0.2627088386780585
  batch 330 loss: 0.262610600753264
  batch 331 loss: 0.26258868015064574
  batch 332 loss: 0.26256878135434114
  batch 333 loss: 0.2625399640909544
  batch 334 loss: 0.2624201398052855
  batch 335 loss: 0.2622249984029514
  batch 336 loss: 0.26208719198725056
  batch 337 loss: 0.2619144630007645
  batch 338 loss: 0.2618562941103292
  batch 339 loss: 0.2616485013107283
  batch 340 loss: 0.26154766529798507
  batch 341 loss: 0.26149507920063836
  batch 342 loss: 0.2614327399005667
  batch 343 loss: 0.26142187160236147
  batch 344 loss: 0.2613555075557426
  batch 345 loss: 0.2614839531805204
  batch 346 loss: 0.2613371591692026
  batch 347 loss: 0.2614137360273246
  batch 348 loss: 0.2614259256542414
  batch 349 loss: 0.2614051014782706
  batch 350 loss: 0.2615400052922113
  batch 351 loss: 0.26160572796126036
  batch 352 loss: 0.26172455840490083
  batch 353 loss: 0.2617562716304412
  batch 354 loss: 0.2618821294462613
  batch 355 loss: 0.26188980802683764
  batch 356 loss: 0.261824492616265
  batch 357 loss: 0.26170611677884387
  batch 358 loss: 0.26172863941785346
  batch 359 loss: 0.26167628858082803
  batch 360 loss: 0.2617508675489161
  batch 361 loss: 0.26165807383377465
  batch 362 loss: 0.2616766578008457
  batch 363 loss: 0.2615852494295635
  batch 364 loss: 0.2615253125818876
  batch 365 loss: 0.26159041928101895
  batch 366 loss: 0.2615137507762414
  batch 367 loss: 0.26142376742505896
  batch 368 loss: 0.2613704580732662
  batch 369 loss: 0.26130302024243
  batch 370 loss: 0.26133055578212483
  batch 371 loss: 0.2613627027388853
  batch 372 loss: 0.2613183759072775
  batch 373 loss: 0.2611832916736603
  batch 374 loss: 0.26105624126559274
  batch 375 loss: 0.26101803437868754
  batch 376 loss: 0.26103278312911377
  batch 377 loss: 0.2609955415485392
  batch 378 loss: 0.2608739187320073
  batch 379 loss: 0.26095389289717563
  batch 380 loss: 0.26095993150221675
  batch 381 loss: 0.26092437863975687
  batch 382 loss: 0.260874780177758
  batch 383 loss: 0.2608870521148876
  batch 384 loss: 0.2609042428666726
  batch 385 loss: 0.2610341401456238
  batch 386 loss: 0.26100687007057854
  batch 387 loss: 0.26105249555690035
  batch 388 loss: 0.261113125984509
  batch 389 loss: 0.26104884918367344
  batch 390 loss: 0.2610271236835382
  batch 391 loss: 0.2611244141750628
  batch 392 loss: 0.2611187735990602
  batch 393 loss: 0.26114050979529324
  batch 394 loss: 0.26117755843298085
  batch 395 loss: 0.26111687587786325
  batch 396 loss: 0.2610961463716295
  batch 397 loss: 0.2610916944985426
  batch 398 loss: 0.2610260928096484
  batch 399 loss: 0.26107671431132723
  batch 400 loss: 0.2611399456858635
  batch 401 loss: 0.2610490475955449
  batch 402 loss: 0.2610990643056471
  batch 403 loss: 0.2611251906041176
  batch 404 loss: 0.2611680783995307
  batch 405 loss: 0.26114484123241755
  batch 406 loss: 0.2612158542342961
  batch 407 loss: 0.26116692763581617
  batch 408 loss: 0.2612450280610253
  batch 409 loss: 0.26115668657939417
  batch 410 loss: 0.2611981798235963
  batch 411 loss: 0.26114574433678256
  batch 412 loss: 0.2611066318903733
  batch 413 loss: 0.2612023584129735
  batch 414 loss: 0.26117686750952174
  batch 415 loss: 0.2611625082162489
  batch 416 loss: 0.2611756836637281
  batch 417 loss: 0.26113954683144885
  batch 418 loss: 0.26104651423590036
  batch 419 loss: 0.2610674649477005
  batch 420 loss: 0.2610702842119194
  batch 421 loss: 0.26094887959560703
  batch 422 loss: 0.26108629108181497
  batch 423 loss: 0.26114629117450533
  batch 424 loss: 0.26109956559848113
  batch 425 loss: 0.26110314246486216
  batch 426 loss: 0.2610488464364983
  batch 427 loss: 0.26105503912413147
  batch 428 loss: 0.26103974165064153
  batch 429 loss: 0.26104471042300714
  batch 430 loss: 0.2609990402709606
  batch 431 loss: 0.2611037371053497
  batch 432 loss: 0.26115840790724315
  batch 433 loss: 0.2611526473840445
  batch 434 loss: 0.26125688583070783
  batch 435 loss: 0.261128746812371
  batch 436 loss: 0.26114104049058134
  batch 437 loss: 0.2612055973480168
  batch 438 loss: 0.2614123081984041
  batch 439 loss: 0.26142334941443657
  batch 440 loss: 0.26146346835250206
  batch 441 loss: 0.26139837662243787
  batch 442 loss: 0.26144941727635007
  batch 443 loss: 0.2614418719275003
  batch 444 loss: 0.2614503049434305
  batch 445 loss: 0.26150223066967526
  batch 446 loss: 0.2614668370495997
  batch 447 loss: 0.26142332471190416
  batch 448 loss: 0.26150469688166467
  batch 449 loss: 0.26147963163050886
  batch 450 loss: 0.26140525115860835
  batch 451 loss: 0.26135665637558686
  batch 452 loss: 0.2613462306295348
  batch 453 loss: 0.26136750262436226
  batch 454 loss: 0.26135447755915475
  batch 455 loss: 0.26136522984111704
  batch 456 loss: 0.26145130608296185
  batch 457 loss: 0.26151727792079393
  batch 458 loss: 0.26149112331034313
  batch 459 loss: 0.2614642653719792
  batch 460 loss: 0.26158673601306004
  batch 461 loss: 0.2616024209982402
  batch 462 loss: 0.26165069665485646
  batch 463 loss: 0.26163788226716717
  batch 464 loss: 0.2616811614997428
  batch 465 loss: 0.26162296286834186
  batch 466 loss: 0.26158065585491486
  batch 467 loss: 0.26163166190240317
  batch 468 loss: 0.26161739105979603
  batch 469 loss: 0.26169774987931443
  batch 470 loss: 0.2617350007942382
  batch 471 loss: 0.26171718423913237
  batch 472 loss: 0.26160147003198075
LOSS train 0.26160147003198075 valid 0.27969658374786377
LOSS train 0.26160147003198075 valid 0.2625909373164177
LOSS train 0.26160147003198075 valid 0.26783710221449536
LOSS train 0.26160147003198075 valid 0.25440791994333267
LOSS train 0.26160147003198075 valid 0.246847465634346
LOSS train 0.26160147003198075 valid 0.24476589759190878
LOSS train 0.26160147003198075 valid 0.25822711842400686
LOSS train 0.26160147003198075 valid 0.25364936143159866
LOSS train 0.26160147003198075 valid 0.25246381097369724
LOSS train 0.26160147003198075 valid 0.2554290235042572
LOSS train 0.26160147003198075 valid 0.2535796355117451
LOSS train 0.26160147003198075 valid 0.2523885319630305
LOSS train 0.26160147003198075 valid 0.2503890120066129
LOSS train 0.26160147003198075 valid 0.2510981134005955
LOSS train 0.26160147003198075 valid 0.24630226492881774
LOSS train 0.26160147003198075 valid 0.2481484655290842
LOSS train 0.26160147003198075 valid 0.24842804495026083
LOSS train 0.26160147003198075 valid 0.24833079096343783
LOSS train 0.26160147003198075 valid 0.25107164367249135
LOSS train 0.26160147003198075 valid 0.25124913081526756
LOSS train 0.26160147003198075 valid 0.2517341694661549
LOSS train 0.26160147003198075 valid 0.25020949881185184
LOSS train 0.26160147003198075 valid 0.2515604502480963
LOSS train 0.26160147003198075 valid 0.25088909020026523
LOSS train 0.26160147003198075 valid 0.2491033774614334
LOSS train 0.26160147003198075 valid 0.24901146040512964
LOSS train 0.26160147003198075 valid 0.2492146525118086
LOSS train 0.26160147003198075 valid 0.2505782055003302
LOSS train 0.26160147003198075 valid 0.25140929941473333
LOSS train 0.26160147003198075 valid 0.25191758771737416
LOSS train 0.26160147003198075 valid 0.2532428301149799
LOSS train 0.26160147003198075 valid 0.25319656264036894
LOSS train 0.26160147003198075 valid 0.254546812989495
LOSS train 0.26160147003198075 valid 0.25406795082723393
LOSS train 0.26160147003198075 valid 0.25639359993594035
LOSS train 0.26160147003198075 valid 0.25622988699211013
LOSS train 0.26160147003198075 valid 0.2565468449044872
LOSS train 0.26160147003198075 valid 0.25716075261956767
LOSS train 0.26160147003198075 valid 0.25662414232889813
LOSS train 0.26160147003198075 valid 0.2571397162973881
LOSS train 0.26160147003198075 valid 0.2567671770002784
LOSS train 0.26160147003198075 valid 0.25683100663480307
LOSS train 0.26160147003198075 valid 0.25604621962059376
LOSS train 0.26160147003198075 valid 0.25606862862001767
LOSS train 0.26160147003198075 valid 0.2556477189064026
LOSS train 0.26160147003198075 valid 0.2566765125678933
LOSS train 0.26160147003198075 valid 0.2569142155190732
LOSS train 0.26160147003198075 valid 0.25682314050694305
LOSS train 0.26160147003198075 valid 0.2570138968983475
LOSS train 0.26160147003198075 valid 0.25628454983234406
LOSS train 0.26160147003198075 valid 0.2568880378031263
LOSS train 0.26160147003198075 valid 0.25665245892909855
LOSS train 0.26160147003198075 valid 0.2566876715084292
LOSS train 0.26160147003198075 valid 0.257207782180221
LOSS train 0.26160147003198075 valid 0.25722190737724304
LOSS train 0.26160147003198075 valid 0.25714543408581186
LOSS train 0.26160147003198075 valid 0.25694828650407625
LOSS train 0.26160147003198075 valid 0.25651403850522536
LOSS train 0.26160147003198075 valid 0.25724570629960397
LOSS train 0.26160147003198075 valid 0.256569754332304
LOSS train 0.26160147003198075 valid 0.2563214710012811
LOSS train 0.26160147003198075 valid 0.25702924473631766
LOSS train 0.26160147003198075 valid 0.257375830932269
LOSS train 0.26160147003198075 valid 0.25830271444283426
LOSS train 0.26160147003198075 valid 0.2583450402204807
LOSS train 0.26160147003198075 valid 0.2584263011813164
LOSS train 0.26160147003198075 valid 0.25753431578180686
LOSS train 0.26160147003198075 valid 0.2580253692234264
LOSS train 0.26160147003198075 valid 0.2570374564848084
LOSS train 0.26160147003198075 valid 0.2571862429380417
LOSS train 0.26160147003198075 valid 0.25679457796291566
LOSS train 0.26160147003198075 valid 0.25705482334726387
LOSS train 0.26160147003198075 valid 0.25703690374550753
LOSS train 0.26160147003198075 valid 0.25661952048540115
LOSS train 0.26160147003198075 valid 0.25626235981782275
LOSS train 0.26160147003198075 valid 0.25697383970806476
LOSS train 0.26160147003198075 valid 0.25666394655580643
LOSS train 0.26160147003198075 valid 0.25652084633325917
LOSS train 0.26160147003198075 valid 0.2566069560714915
LOSS train 0.26160147003198075 valid 0.2557719752192497
LOSS train 0.26160147003198075 valid 0.25466054769945734
LOSS train 0.26160147003198075 valid 0.2548300712937262
LOSS train 0.26160147003198075 valid 0.25441942085702735
LOSS train 0.26160147003198075 valid 0.25443173270849956
LOSS train 0.26160147003198075 valid 0.25359285824439104
LOSS train 0.26160147003198075 valid 0.2528221226015756
LOSS train 0.26160147003198075 valid 0.2529354427737751
LOSS train 0.26160147003198075 valid 0.2524686216969382
LOSS train 0.26160147003198075 valid 0.2526629709460762
LOSS train 0.26160147003198075 valid 0.25281068864795897
LOSS train 0.26160147003198075 valid 0.2527632156571189
LOSS train 0.26160147003198075 valid 0.25270517154232314
LOSS train 0.26160147003198075 valid 0.2526194083113824
LOSS train 0.26160147003198075 valid 0.25286596775689024
LOSS train 0.26160147003198075 valid 0.2523036762287742
LOSS train 0.26160147003198075 valid 0.2523773228749633
LOSS train 0.26160147003198075 valid 0.25261966100673083
LOSS train 0.26160147003198075 valid 0.2530666845185416
LOSS train 0.26160147003198075 valid 0.253095608166974
LOSS train 0.26160147003198075 valid 0.25324447244405746
LOSS train 0.26160147003198075 valid 0.253761762734687
LOSS train 0.26160147003198075 valid 0.2539288451274236
LOSS train 0.26160147003198075 valid 0.2536218163457889
LOSS train 0.26160147003198075 valid 0.253314733505249
LOSS train 0.26160147003198075 valid 0.25378540527252924
LOSS train 0.26160147003198075 valid 0.2540905694916563
LOSS train 0.26160147003198075 valid 0.2536153943739205
LOSS train 0.26160147003198075 valid 0.25353984962458964
LOSS train 0.26160147003198075 valid 0.2540145985303669
LOSS train 0.26160147003198075 valid 0.2543291372331706
LOSS train 0.26160147003198075 valid 0.2539184536482837
LOSS train 0.26160147003198075 valid 0.25378160644322634
LOSS train 0.26160147003198075 valid 0.25393999453666993
LOSS train 0.26160147003198075 valid 0.2539121078556044
LOSS train 0.26160147003198075 valid 0.25421211343744526
LOSS train 0.26160147003198075 valid 0.25469240584763986
LOSS train 0.26160147003198075 valid 0.2550340161109582
LOSS train 0.26160147003198075 valid 0.25503109610181746
LOSS train 0.26160147003198075 valid 0.2549426611982474
LOSS train 0.26160147003198075 valid 0.25475035769244037
LOSS train 0.26160147003198075 valid 0.2546470968191289
LOSS train 0.26160147003198075 valid 0.25459368759002843
LOSS train 0.26160147003198075 valid 0.25475177835158214
LOSS train 0.26160147003198075 valid 0.2550173107414476
LOSS train 0.26160147003198075 valid 0.2550665296316147
LOSS train 0.26160147003198075 valid 0.25525806927018696
LOSS train 0.26160147003198075 valid 0.25521238042613653
LOSS train 0.26160147003198075 valid 0.2554612692911178
LOSS train 0.26160147003198075 valid 0.2557896077632904
LOSS train 0.26160147003198075 valid 0.25557338148355485
LOSS train 0.26160147003198075 valid 0.25555387241694766
LOSS train 0.26160147003198075 valid 0.25523117105617665
LOSS train 0.26160147003198075 valid 0.25494664369669173
LOSS train 0.26160147003198075 valid 0.25524309588902033
LOSS train 0.26160147003198075 valid 0.2552274611261156
LOSS train 0.26160147003198075 valid 0.2548545351361527
LOSS train 0.26160147003198075 valid 0.2545638758770741
LOSS train 0.26160147003198075 valid 0.2544339290563611
LOSS train 0.26160147003198075 valid 0.25417117270634326
LOSS train 0.26160147003198075 valid 0.2542651504278183
LOSS train 0.26160147003198075 valid 0.2543470320549417
LOSS train 0.26160147003198075 valid 0.2545278490009442
LOSS train 0.26160147003198075 valid 0.25471758383970994
LOSS train 0.26160147003198075 valid 0.2548865098506212
LOSS train 0.26160147003198075 valid 0.2546848493403402
LOSS train 0.26160147003198075 valid 0.254764380214149
LOSS train 0.26160147003198075 valid 0.2545357763159032
LOSS train 0.26160147003198075 valid 0.2554126930599277
LOSS train 0.26160147003198075 valid 0.2555214339654718
LOSS train 0.26160147003198075 valid 0.25539075295130415
LOSS train 0.26160147003198075 valid 0.2554983713768965
LOSS train 0.26160147003198075 valid 0.25515308152688176
LOSS train 0.26160147003198075 valid 0.255301092574799
LOSS train 0.26160147003198075 valid 0.25537712620450304
LOSS train 0.26160147003198075 valid 0.25527791832723923
LOSS train 0.26160147003198075 valid 0.255599532276392
LOSS train 0.26160147003198075 valid 0.2554676204350344
LOSS train 0.26160147003198075 valid 0.25551360921014715
LOSS train 0.26160147003198075 valid 0.25547221715345325
LOSS train 0.26160147003198075 valid 0.25524265468120577
LOSS train 0.26160147003198075 valid 0.2551646175221627
LOSS train 0.26160147003198075 valid 0.25497454597994135
LOSS train 0.26160147003198075 valid 0.254824895800257
LOSS train 0.26160147003198075 valid 0.2546143162904716
LOSS train 0.26160147003198075 valid 0.25459146337075667
LOSS train 0.26160147003198075 valid 0.2544772346694785
LOSS train 0.26160147003198075 valid 0.2545418448433905
LOSS train 0.26160147003198075 valid 0.2543841964077382
LOSS train 0.26160147003198075 valid 0.2544310311593953
LOSS train 0.26160147003198075 valid 0.254639557705206
LOSS train 0.26160147003198075 valid 0.25455849770216915
LOSS train 0.26160147003198075 valid 0.25440920464867767
LOSS train 0.26160147003198075 valid 0.2543587897382031
LOSS train 0.26160147003198075 valid 0.2544689783933519
LOSS train 0.26160147003198075 valid 0.25432208240032195
LOSS train 0.26160147003198075 valid 0.25426995618776843
LOSS train 0.26160147003198075 valid 0.2541198851698536
LOSS train 0.26160147003198075 valid 0.25422577442747823
LOSS train 0.26160147003198075 valid 0.25412159192495504
LOSS train 0.26160147003198075 valid 0.2541925768057505
LOSS train 0.26160147003198075 valid 0.254255466698283
LOSS train 0.26160147003198075 valid 0.2544483365593376
LOSS train 0.26160147003198075 valid 0.2543738793805649
LOSS train 0.26160147003198075 valid 0.2546005177757014
LOSS train 0.26160147003198075 valid 0.254431880970259
LOSS train 0.26160147003198075 valid 0.2543554658530861
LOSS train 0.26160147003198075 valid 0.25418733721748393
LOSS train 0.26160147003198075 valid 0.25419292250212205
LOSS train 0.26160147003198075 valid 0.25419206515191095
LOSS train 0.26160147003198075 valid 0.2544895503081773
LOSS train 0.26160147003198075 valid 0.2544586628831494
LOSS train 0.26160147003198075 valid 0.25440222801019746
LOSS train 0.26160147003198075 valid 0.25444195137740416
LOSS train 0.26160147003198075 valid 0.25426374414225217
LOSS train 0.26160147003198075 valid 0.2538901761556283
LOSS train 0.26160147003198075 valid 0.25398764789712674
LOSS train 0.26160147003198075 valid 0.25420439939208445
LOSS train 0.26160147003198075 valid 0.25400653805094536
LOSS train 0.26160147003198075 valid 0.2540791790239775
LOSS train 0.26160147003198075 valid 0.25399495400488376
LOSS train 0.26160147003198075 valid 0.253860758904794
LOSS train 0.26160147003198075 valid 0.2537932687319151
LOSS train 0.26160147003198075 valid 0.25363822340084413
LOSS train 0.26160147003198075 valid 0.2536164826768286
LOSS train 0.26160147003198075 valid 0.25346709046421984
LOSS train 0.26160147003198075 valid 0.25336191513873996
LOSS train 0.26160147003198075 valid 0.2533327669337176
LOSS train 0.26160147003198075 valid 0.2532842561889153
LOSS train 0.26160147003198075 valid 0.2531740755270543
LOSS train 0.26160147003198075 valid 0.25335488248439064
LOSS train 0.26160147003198075 valid 0.25347908413240694
LOSS train 0.26160147003198075 valid 0.2534059187713659
LOSS train 0.26160147003198075 valid 0.253483363020588
LOSS train 0.26160147003198075 valid 0.25340138194716977
LOSS train 0.26160147003198075 valid 0.2532062674677649
LOSS train 0.26160147003198075 valid 0.2531197319428126
LOSS train 0.26160147003198075 valid 0.2528960842141358
LOSS train 0.26160147003198075 valid 0.25299095287235507
LOSS train 0.26160147003198075 valid 0.25310401285075707
LOSS train 0.26160147003198075 valid 0.2532717668197372
LOSS train 0.26160147003198075 valid 0.25337340645660644
LOSS train 0.26160147003198075 valid 0.25331329218707643
LOSS train 0.26160147003198075 valid 0.25337939980051444
LOSS train 0.26160147003198075 valid 0.25347975940842715
LOSS train 0.26160147003198075 valid 0.25345163775814905
LOSS train 0.26160147003198075 valid 0.25361640494217913
LOSS train 0.26160147003198075 valid 0.25382278833357774
LOSS train 0.26160147003198075 valid 0.25390415109301867
LOSS train 0.26160147003198075 valid 0.2539365594173623
LOSS train 0.26160147003198075 valid 0.25412223241899323
LOSS train 0.26160147003198075 valid 0.25414720738863017
LOSS train 0.26160147003198075 valid 0.25423351733078214
LOSS train 0.26160147003198075 valid 0.2541962380445055
LOSS train 0.26160147003198075 valid 0.25422364747167653
LOSS train 0.26160147003198075 valid 0.2543443069179007
LOSS train 0.26160147003198075 valid 0.254126582105281
LOSS train 0.26160147003198075 valid 0.25434539677724555
LOSS train 0.26160147003198075 valid 0.2542342389581584
LOSS train 0.26160147003198075 valid 0.25428908464798866
LOSS train 0.26160147003198075 valid 0.25429390569527943
LOSS train 0.26160147003198075 valid 0.25439234331435684
LOSS train 0.26160147003198075 valid 0.2541269760363358
LOSS train 0.26160147003198075 valid 0.2542921646264355
LOSS train 0.26160147003198075 valid 0.2545221134653834
LOSS train 0.26160147003198075 valid 0.2546897023916245
LOSS train 0.26160147003198075 valid 0.25465767333904904
LOSS train 0.26160147003198075 valid 0.25463128198496243
LOSS train 0.26160147003198075 valid 0.25458346914139485
LOSS train 0.26160147003198075 valid 0.2544681799100585
LOSS train 0.26160147003198075 valid 0.2546232361197472
LOSS train 0.26160147003198075 valid 0.2546164636236738
LOSS train 0.26160147003198075 valid 0.2546179477302801
LOSS train 0.26160147003198075 valid 0.2545224644332064
LOSS train 0.26160147003198075 valid 0.25449757565428893
LOSS train 0.26160147003198075 valid 0.2545453508110607
LOSS train 0.26160147003198075 valid 0.25453871622448787
LOSS train 0.26160147003198075 valid 0.25445755927711144
LOSS train 0.26160147003198075 valid 0.2545711293585541
LOSS train 0.26160147003198075 valid 0.25455955033366745
LOSS train 0.26160147003198075 valid 0.2545429351811226
LOSS train 0.26160147003198075 valid 0.2547493256714152
LOSS train 0.26160147003198075 valid 0.25472824662241317
LOSS train 0.26160147003198075 valid 0.2548067368028735
LOSS train 0.26160147003198075 valid 0.25476249928275746
LOSS train 0.26160147003198075 valid 0.2548059940338135
LOSS train 0.26160147003198075 valid 0.2547908799540728
LOSS train 0.26160147003198075 valid 0.2548307740732972
LOSS train 0.26160147003198075 valid 0.2548994973524293
LOSS train 0.26160147003198075 valid 0.25513294516442875
LOSS train 0.26160147003198075 valid 0.25532231408136863
LOSS train 0.26160147003198075 valid 0.2554600948117316
LOSS train 0.26160147003198075 valid 0.2558167403673424
LOSS train 0.26160147003198075 valid 0.25585538694710086
LOSS train 0.26160147003198075 valid 0.25571374214478654
LOSS train 0.26160147003198075 valid 0.2557164226878773
LOSS train 0.26160147003198075 valid 0.25559839671072754
LOSS train 0.26160147003198075 valid 0.2554310041644513
LOSS train 0.26160147003198075 valid 0.25522355354732745
LOSS train 0.26160147003198075 valid 0.2552809042849421
LOSS train 0.26160147003198075 valid 0.25515151476221426
LOSS train 0.26160147003198075 valid 0.25505795125647374
LOSS train 0.26160147003198075 valid 0.2549753890286946
LOSS train 0.26160147003198075 valid 0.25497592901383187
LOSS train 0.26160147003198075 valid 0.2549670343352875
LOSS train 0.26160147003198075 valid 0.25501886004941504
LOSS train 0.26160147003198075 valid 0.2549475642455208
LOSS train 0.26160147003198075 valid 0.2548252277154125
LOSS train 0.26160147003198075 valid 0.25487276409856147
LOSS train 0.26160147003198075 valid 0.25485727433308597
LOSS train 0.26160147003198075 valid 0.25498466435177575
LOSS train 0.26160147003198075 valid 0.25490760854429395
LOSS train 0.26160147003198075 valid 0.2549826952896706
LOSS train 0.26160147003198075 valid 0.25497293970690654
LOSS train 0.26160147003198075 valid 0.2552256100640005
LOSS train 0.26160147003198075 valid 0.2553483786219257
LOSS train 0.26160147003198075 valid 0.25522487897526575
LOSS train 0.26160147003198075 valid 0.25520707747149546
LOSS train 0.26160147003198075 valid 0.255121672013462
LOSS train 0.26160147003198075 valid 0.2551655194233094
LOSS train 0.26160147003198075 valid 0.25507810652256013
LOSS train 0.26160147003198075 valid 0.25523211156014985
LOSS train 0.26160147003198075 valid 0.25526255951417204
LOSS train 0.26160147003198075 valid 0.2553526442436495
LOSS train 0.26160147003198075 valid 0.2554236879866374
LOSS train 0.26160147003198075 valid 0.25530956123695997
LOSS train 0.26160147003198075 valid 0.2552555677255774
LOSS train 0.26160147003198075 valid 0.25527468503298123
LOSS train 0.26160147003198075 valid 0.25525651528657256
LOSS train 0.26160147003198075 valid 0.25528895464336987
LOSS train 0.26160147003198075 valid 0.25516718544306294
LOSS train 0.26160147003198075 valid 0.25500180325515787
LOSS train 0.26160147003198075 valid 0.255154953027765
LOSS train 0.26160147003198075 valid 0.2551254264938946
LOSS train 0.26160147003198075 valid 0.25522657759060524
LOSS train 0.26160147003198075 valid 0.2552199764384164
LOSS train 0.26160147003198075 valid 0.25519388390681413
LOSS train 0.26160147003198075 valid 0.2552397191430492
LOSS train 0.26160147003198075 valid 0.25535692082448574
LOSS train 0.26160147003198075 valid 0.2554040817911722
LOSS train 0.26160147003198075 valid 0.25530373081564905
LOSS train 0.26160147003198075 valid 0.25539912065241566
LOSS train 0.26160147003198075 valid 0.2553612109481918
LOSS train 0.26160147003198075 valid 0.2553390997113089
LOSS train 0.26160147003198075 valid 0.2553409249694259
LOSS train 0.26160147003198075 valid 0.25539931168923014
LOSS train 0.26160147003198075 valid 0.2555356760697862
LOSS train 0.26160147003198075 valid 0.2556231634332499
LOSS train 0.26160147003198075 valid 0.25557587818219896
LOSS train 0.26160147003198075 valid 0.25582492962977804
LOSS train 0.26160147003198075 valid 0.2557411057931004
LOSS train 0.26160147003198075 valid 0.25560686870103877
LOSS train 0.26160147003198075 valid 0.2555761408644268
LOSS train 0.26160147003198075 valid 0.2555584655151711
LOSS train 0.26160147003198075 valid 0.2556928820060399
LOSS train 0.26160147003198075 valid 0.2557711367286853
LOSS train 0.26160147003198075 valid 0.25574899557977915
LOSS train 0.26160147003198075 valid 0.25580398729540826
LOSS train 0.26160147003198075 valid 0.2557786525882913
LOSS train 0.26160147003198075 valid 0.2557284842550227
LOSS train 0.26160147003198075 valid 0.25575257688760755
LOSS train 0.26160147003198075 valid 0.2557503150180638
LOSS train 0.26160147003198075 valid 0.2556545169294229
LOSS train 0.26160147003198075 valid 0.25560854470938354
LOSS train 0.26160147003198075 valid 0.25589525859889595
LOSS train 0.26160147003198075 valid 0.25589201852031374
LOSS train 0.26160147003198075 valid 0.2558024830556329
LOSS train 0.26160147003198075 valid 0.25574298641008325
LOSS train 0.26160147003198075 valid 0.2555810359103241
LOSS train 0.26160147003198075 valid 0.25563607008375205
LOSS train 0.26160147003198075 valid 0.255545374410493
LOSS train 0.26160147003198075 valid 0.25543665860453224
LOSS train 0.26160147003198075 valid 0.25541079145940865
LOSS train 0.26160147003198075 valid 0.2554346062981711
LOSS train 0.26160147003198075 valid 0.2554240761335287
LOSS train 0.26160147003198075 valid 0.25563171598273265
LOSS train 0.26160147003198075 valid 0.25565823979592056
LOSS train 0.26160147003198075 valid 0.2555916964256463
LOSS train 0.26160147003198075 valid 0.2555456715505882
LOSS train 0.26160147003198075 valid 0.25555492912660405
LOSS train 0.26160147003198075 valid 0.25549216754734516
LOSS train 0.26160147003198075 valid 0.25541231758541677
LOSS train 0.26160147003198075 valid 0.25548373718452716
LOSS train 0.26160147003198075 valid 0.25519278329430206
LOSS train 0.26160147003198075 valid 0.2552144773468211
LOSS train 0.26160147003198075 valid 0.25521524315827515
LOSS train 0.26160147003198075 valid 0.25512354943107385
LOSS train 0.26160147003198075 valid 0.2549262595355348
LOSS train 0.26160147003198075 valid 0.2548793459554081
LOSS train 0.26160147003198075 valid 0.254916808711804
EPOCH 12:
  batch 1 loss: 0.2432655394077301
  batch 2 loss: 0.2349090650677681
  batch 3 loss: 0.24217176934083304
  batch 4 loss: 0.24396106228232384
  batch 5 loss: 0.25709969103336333
  batch 6 loss: 0.259092353284359
  batch 7 loss: 0.2587571846587317
  batch 8 loss: 0.2608520220965147
  batch 9 loss: 0.2609527094496621
  batch 10 loss: 0.2630751892924309
  batch 11 loss: 0.260633570226756
  batch 12 loss: 0.2584564636150996
  batch 13 loss: 0.25591826324279493
  batch 14 loss: 0.2553762676460402
  batch 15 loss: 0.2549969603617986
  batch 16 loss: 0.2537724692374468
  batch 17 loss: 0.25104864993516135
  batch 18 loss: 0.253107020424472
  batch 19 loss: 0.2521795599084151
  batch 20 loss: 0.2507772848010063
  batch 21 loss: 0.2516566685267857
  batch 22 loss: 0.25173306465148926
  batch 23 loss: 0.2516566858343456
  batch 24 loss: 0.24974853607515493
  batch 25 loss: 0.2517086380720139
  batch 26 loss: 0.25024382598125017
  batch 27 loss: 0.2502990204978872
  batch 28 loss: 0.24886529466935567
  batch 29 loss: 0.24952197588723282
  batch 30 loss: 0.24913542866706848
  batch 31 loss: 0.25044333165691746
  batch 32 loss: 0.24980187602341175
  batch 33 loss: 0.2510406483303417
  batch 34 loss: 0.2505044814418344
  batch 35 loss: 0.2504542878695897
  batch 36 loss: 0.25053255011638004
  batch 37 loss: 0.25163964886923096
  batch 38 loss: 0.2529184810425106
  batch 39 loss: 0.25287139874238235
  batch 40 loss: 0.2532578565180302
  batch 41 loss: 0.2538823674364788
  batch 42 loss: 0.2542040078412919
  batch 43 loss: 0.25466290185617846
  batch 44 loss: 0.2550016323273832
  batch 45 loss: 0.2542389495505227
  batch 46 loss: 0.25387106997811276
  batch 47 loss: 0.25376251498435404
  batch 48 loss: 0.2532664366687338
  batch 49 loss: 0.2528609599994153
  batch 50 loss: 0.252326974272728
  batch 51 loss: 0.2517951867159675
  batch 52 loss: 0.25306500035982865
  batch 53 loss: 0.25238883720253996
  batch 54 loss: 0.2525602452181004
  batch 55 loss: 0.25256806937131016
  batch 56 loss: 0.2529048557792391
  batch 57 loss: 0.2531262385217767
  batch 58 loss: 0.2539339471479942
  batch 59 loss: 0.25413639717182873
  batch 60 loss: 0.25439926236867905
  batch 61 loss: 0.25531253023225753
  batch 62 loss: 0.25607247458350274
  batch 63 loss: 0.25586933299662573
  batch 64 loss: 0.25654774089343846
  batch 65 loss: 0.25624537261632774
  batch 66 loss: 0.25656315074725583
  batch 67 loss: 0.25659710146597964
  batch 68 loss: 0.2573264046188663
  batch 69 loss: 0.2568365370017895
  batch 70 loss: 0.2570973072733198
  batch 71 loss: 0.25668589581905954
  batch 72 loss: 0.2570573969019784
  batch 73 loss: 0.2570964412329948
  batch 74 loss: 0.25713602795794205
  batch 75 loss: 0.2568854355812073
  batch 76 loss: 0.25725268141219493
  batch 77 loss: 0.2568678016012365
  batch 78 loss: 0.2569482827033752
  batch 79 loss: 0.25776795571363426
  batch 80 loss: 0.2574502667412162
  batch 81 loss: 0.257400275932418
  batch 82 loss: 0.2573874136660157
  batch 83 loss: 0.25724410651678065
  batch 84 loss: 0.2572361763034548
  batch 85 loss: 0.2567259127602858
  batch 86 loss: 0.25717866472726647
  batch 87 loss: 0.2571958377100955
  batch 88 loss: 0.25682480684058234
  batch 89 loss: 0.25685229947727717
  batch 90 loss: 0.2568358227610588
  batch 91 loss: 0.257239981175779
  batch 92 loss: 0.2575411675092967
  batch 93 loss: 0.2578505750304909
  batch 94 loss: 0.2584539203250662
  batch 95 loss: 0.2584421372727344
  batch 96 loss: 0.2586223071751495
  batch 97 loss: 0.259091141144025
  batch 98 loss: 0.2594387751756882
  batch 99 loss: 0.25976209372583065
  batch 100 loss: 0.2597216020524502
  batch 101 loss: 0.25942386273700413
  batch 102 loss: 0.2595225547750791
  batch 103 loss: 0.25988516341714024
  batch 104 loss: 0.25982582640762514
  batch 105 loss: 0.25987446237178075
  batch 106 loss: 0.2602659260045807
  batch 107 loss: 0.2596350412502467
  batch 108 loss: 0.2595296449683331
  batch 109 loss: 0.25940216968365765
  batch 110 loss: 0.25952693020755596
  batch 111 loss: 0.259607499112954
  batch 112 loss: 0.25956960009144886
  batch 113 loss: 0.2595713749132325
  batch 114 loss: 0.25979527269016234
  batch 115 loss: 0.2597965451686279
  batch 116 loss: 0.2599673197957976
  batch 117 loss: 0.2600833914982967
  batch 118 loss: 0.25974396237377395
  batch 119 loss: 0.25993845305022073
  batch 120 loss: 0.25983383630712825
  batch 121 loss: 0.259710596243212
  batch 122 loss: 0.2595182079516473
  batch 123 loss: 0.259535738001994
  batch 124 loss: 0.25997393349966696
  batch 125 loss: 0.25996216213703155
  batch 126 loss: 0.2599569345040927
  batch 127 loss: 0.2604917990645086
  batch 128 loss: 0.2603601024020463
  batch 129 loss: 0.2605950354143631
  batch 130 loss: 0.26041393199792273
  batch 131 loss: 0.2605406702247285
  batch 132 loss: 0.26051939583637496
  batch 133 loss: 0.26067963276142464
  batch 134 loss: 0.26067327974892374
  batch 135 loss: 0.2605377557101073
  batch 136 loss: 0.26034448197221055
  batch 137 loss: 0.2603189970237495
  batch 138 loss: 0.26037689147220144
  batch 139 loss: 0.26076619670116646
  batch 140 loss: 0.26070774272084235
  batch 141 loss: 0.2608020427167838
  batch 142 loss: 0.26074944761857183
  batch 143 loss: 0.26056512617147887
  batch 144 loss: 0.2603382088450922
  batch 145 loss: 0.2600731575283511
  batch 146 loss: 0.2600924045051614
  batch 147 loss: 0.26041159368291195
  batch 148 loss: 0.26027346190971296
  batch 149 loss: 0.2601099688334753
  batch 150 loss: 0.2602284425497055
  batch 151 loss: 0.2603035492218093
  batch 152 loss: 0.2604147099742764
  batch 153 loss: 0.26031529903411865
  batch 154 loss: 0.26095856958395475
  batch 155 loss: 0.26107010956733456
  batch 156 loss: 0.2611387500014061
  batch 157 loss: 0.2613001378478518
  batch 158 loss: 0.26166148163095304
  batch 159 loss: 0.2622621499892301
  batch 160 loss: 0.2621455854736269
  batch 161 loss: 0.2622095463994127
  batch 162 loss: 0.26234283905338357
  batch 163 loss: 0.26265147319234955
  batch 164 loss: 0.26253895943121214
  batch 165 loss: 0.26255019460663653
  batch 166 loss: 0.2624035674225853
  batch 167 loss: 0.26236476432420536
  batch 168 loss: 0.2623373042082503
  batch 169 loss: 0.2623075621544257
  batch 170 loss: 0.2622723012286074
  batch 171 loss: 0.2623548086797982
  batch 172 loss: 0.2623912349863108
  batch 173 loss: 0.2624595407288888
  batch 174 loss: 0.2625051861008008
  batch 175 loss: 0.2626541232211249
  batch 176 loss: 0.2625937686217102
  batch 177 loss: 0.26254823441896064
  batch 178 loss: 0.2626452162192109
  batch 179 loss: 0.26273279225027096
  batch 180 loss: 0.2627176423039701
  batch 181 loss: 0.2625283365914835
  batch 182 loss: 0.2625645542374024
  batch 183 loss: 0.2624346770224024
  batch 184 loss: 0.2622125390917063
  batch 185 loss: 0.26209645142426363
  batch 186 loss: 0.2622372972708876
  batch 187 loss: 0.26227189592499145
  batch 188 loss: 0.2620057777521458
  batch 189 loss: 0.26178244686631297
  batch 190 loss: 0.2616977782625901
  batch 191 loss: 0.26164812567346385
  batch 192 loss: 0.26171219317863387
  batch 193 loss: 0.26172735221645377
  batch 194 loss: 0.26188567495837656
  batch 195 loss: 0.2620175014703702
  batch 196 loss: 0.26194520233845225
  batch 197 loss: 0.2618010034264647
  batch 198 loss: 0.2619723953833484
  batch 199 loss: 0.2622115540893833
  batch 200 loss: 0.2623749401420355
  batch 201 loss: 0.26242836857613044
  batch 202 loss: 0.2623657343706282
  batch 203 loss: 0.2624267041683197
  batch 204 loss: 0.2623366801177754
  batch 205 loss: 0.2625896100590869
  batch 206 loss: 0.26260784919400815
  batch 207 loss: 0.26257980661691677
  batch 208 loss: 0.2624737951331414
  batch 209 loss: 0.26242091139537865
  batch 210 loss: 0.2625838740950539
  batch 211 loss: 0.2625017771364953
  batch 212 loss: 0.26240568003564513
  batch 213 loss: 0.2624469132210727
  batch 214 loss: 0.26253139011770765
  batch 215 loss: 0.26233466426993524
  batch 216 loss: 0.26221824195925836
  batch 217 loss: 0.2622415382878572
  batch 218 loss: 0.2621855804816299
  batch 219 loss: 0.26217133779776153
  batch 220 loss: 0.26210631619800223
  batch 221 loss: 0.2622573572586025
  batch 222 loss: 0.26231261134684625
  batch 223 loss: 0.262482362344126
  batch 224 loss: 0.26255726614700897
  batch 225 loss: 0.2623990437057283
  batch 226 loss: 0.2625505222024116
  batch 227 loss: 0.262238990236484
  batch 228 loss: 0.2622017787213911
  batch 229 loss: 0.2621084693737946
  batch 230 loss: 0.26207289242226145
  batch 231 loss: 0.26199585676709297
  batch 232 loss: 0.26165695867404853
  batch 233 loss: 0.26157701872449063
  batch 234 loss: 0.26161557181268674
  batch 235 loss: 0.2616725479034667
  batch 236 loss: 0.26159080825114656
  batch 237 loss: 0.26166340130290905
  batch 238 loss: 0.261582448202021
  batch 239 loss: 0.26148729450283686
  batch 240 loss: 0.2615374847625693
  batch 241 loss: 0.2616611864433249
  batch 242 loss: 0.26161115807442625
  batch 243 loss: 0.26170629790290395
  batch 244 loss: 0.2617242837049922
  batch 245 loss: 0.2617465860989629
  batch 246 loss: 0.2617130636926589
  batch 247 loss: 0.261752718494006
  batch 248 loss: 0.26184197479198057
  batch 249 loss: 0.2618315775231664
  batch 250 loss: 0.26176395392417906
  batch 251 loss: 0.2616917606131489
  batch 252 loss: 0.2615153905418184
  batch 253 loss: 0.2613597013969195
  batch 254 loss: 0.26132508889427336
  batch 255 loss: 0.2613379820889118
  batch 256 loss: 0.26136850204784423
  batch 257 loss: 0.26143194230614003
  batch 258 loss: 0.26144027224806854
  batch 259 loss: 0.2615148464224974
  batch 260 loss: 0.2616007135464595
  batch 261 loss: 0.2616784357933249
  batch 262 loss: 0.26175864814346983
  batch 263 loss: 0.2618144886575271
  batch 264 loss: 0.26176055752192484
  batch 265 loss: 0.2617404470466218
  batch 266 loss: 0.2616699285861245
  batch 267 loss: 0.26179704499825107
  batch 268 loss: 0.261710982658525
  batch 269 loss: 0.26167874625402754
  batch 270 loss: 0.26202616377009286
  batch 271 loss: 0.26212448206115035
  batch 272 loss: 0.262210162849549
  batch 273 loss: 0.2621752184716773
  batch 274 loss: 0.26238394615641475
  batch 275 loss: 0.2624026303399693
  batch 276 loss: 0.262428711451914
  batch 277 loss: 0.2624548369689108
  batch 278 loss: 0.2625368113676421
  batch 279 loss: 0.26273863968242456
  batch 280 loss: 0.26266010898564546
  batch 281 loss: 0.2625587144038007
  batch 282 loss: 0.2625805311274867
  batch 283 loss: 0.26254015460663044
  batch 284 loss: 0.2625259870899395
  batch 285 loss: 0.2624906142029846
  batch 286 loss: 0.2624983781805405
  batch 287 loss: 0.26246292587979747
  batch 288 loss: 0.2622679189985825
  batch 289 loss: 0.2624745867553467
  batch 290 loss: 0.26227121163031153
  batch 291 loss: 0.26226424520572844
  batch 292 loss: 0.2623157316997443
  batch 293 loss: 0.2623626726580968
  batch 294 loss: 0.26227138850356446
  batch 295 loss: 0.2623121567196765
  batch 296 loss: 0.26236731654687506
  batch 297 loss: 0.2623138336420862
  batch 298 loss: 0.262329761813951
  batch 299 loss: 0.2623187498902796
  batch 300 loss: 0.26231193472941716
  batch 301 loss: 0.26226271520619376
  batch 302 loss: 0.2622692222330744
  batch 303 loss: 0.262263887540342
  batch 304 loss: 0.2621993956303126
  batch 305 loss: 0.2620956589452556
  batch 306 loss: 0.26223253351605796
  batch 307 loss: 0.26211947702623734
  batch 308 loss: 0.26221552234771967
  batch 309 loss: 0.2622508600016628
  batch 310 loss: 0.26218613136199215
  batch 311 loss: 0.26232500877411036
  batch 312 loss: 0.2624189992172596
  batch 313 loss: 0.26249375149083976
  batch 314 loss: 0.2625727037525481
  batch 315 loss: 0.26258374140376134
  batch 316 loss: 0.26253082919158516
  batch 317 loss: 0.2626474063949254
  batch 318 loss: 0.2627195179368715
  batch 319 loss: 0.2626565473860708
  batch 320 loss: 0.26258829864673316
  batch 321 loss: 0.2625538445428896
  batch 322 loss: 0.2624908771296466
  batch 323 loss: 0.2623653563183528
  batch 324 loss: 0.2622042388919695
  batch 325 loss: 0.262168352833161
  batch 326 loss: 0.26212194091154756
  batch 327 loss: 0.262066557912287
  batch 328 loss: 0.2619482132538063
  batch 329 loss: 0.2620534197537732
  batch 330 loss: 0.26192113662307914
  batch 331 loss: 0.2618845852030008
  batch 332 loss: 0.2617950761264347
  batch 333 loss: 0.26174731096168896
  batch 334 loss: 0.26160072843114773
  batch 335 loss: 0.2614734106989049
  batch 336 loss: 0.26137608147802804
  batch 337 loss: 0.26122204159063117
  batch 338 loss: 0.26114542398756074
  batch 339 loss: 0.2609578905555703
  batch 340 loss: 0.26080584324458067
  batch 341 loss: 0.26073949995977785
  batch 342 loss: 0.26071346145972873
  batch 343 loss: 0.2607137177671705
  batch 344 loss: 0.26058454997837543
  batch 345 loss: 0.2607250511214353
  batch 346 loss: 0.26058521061446627
  batch 347 loss: 0.2606671130021642
  batch 348 loss: 0.2606674354394962
  batch 349 loss: 0.26065816463406244
  batch 350 loss: 0.2608311504551343
  batch 351 loss: 0.26090693928067843
  batch 352 loss: 0.2610339497385377
  batch 353 loss: 0.2610705861710962
  batch 354 loss: 0.26124623354713794
  batch 355 loss: 0.2613340053340079
  batch 356 loss: 0.2613026714475637
  batch 357 loss: 0.2611666073401769
  batch 358 loss: 0.26122264801290446
  batch 359 loss: 0.26115342439217154
  batch 360 loss: 0.2611895326524973
  batch 361 loss: 0.26108692396545674
  batch 362 loss: 0.2610986681027307
  batch 363 loss: 0.26097204792762263
  batch 364 loss: 0.26092056303725136
  batch 365 loss: 0.2609600826077265
  batch 366 loss: 0.26086684434466023
  batch 367 loss: 0.2607570888236043
  batch 368 loss: 0.2606846423738677
  batch 369 loss: 0.2606229380298113
  batch 370 loss: 0.2606133451735651
  batch 371 loss: 0.26060830458315876
  batch 372 loss: 0.26054961911292485
  batch 373 loss: 0.26045051592604407
  batch 374 loss: 0.26032543931415375
  batch 375 loss: 0.2602655849456787
  batch 376 loss: 0.26028327849951194
  batch 377 loss: 0.2602420135818679
  batch 378 loss: 0.26010285617497864
  batch 379 loss: 0.2601447855576991
  batch 380 loss: 0.2601892739534378
  batch 381 loss: 0.2601537901976603
  batch 382 loss: 0.2600809794836019
  batch 383 loss: 0.2600688706723579
  batch 384 loss: 0.26005019220368314
  batch 385 loss: 0.26014937383013886
  batch 386 loss: 0.26011750893426067
  batch 387 loss: 0.26013988984185593
  batch 388 loss: 0.2601763967463036
  batch 389 loss: 0.26011379381071936
  batch 390 loss: 0.260051773641354
  batch 391 loss: 0.2601266881770185
  batch 392 loss: 0.260132919350753
  batch 393 loss: 0.26013921762392417
  batch 394 loss: 0.26019145204179783
  batch 395 loss: 0.2600931389422356
  batch 396 loss: 0.26002225505583215
  batch 397 loss: 0.25999142796326646
  batch 398 loss: 0.2599270431764761
  batch 399 loss: 0.2599652778908125
  batch 400 loss: 0.26004590090364216
  batch 401 loss: 0.25993961106957936
  batch 402 loss: 0.2599360706720186
  batch 403 loss: 0.2599255282515036
  batch 404 loss: 0.25994138194635363
  batch 405 loss: 0.25992208781801623
  batch 406 loss: 0.25997961929132196
  batch 407 loss: 0.259951768658085
  batch 408 loss: 0.26000807737456816
  batch 409 loss: 0.2599054007702177
  batch 410 loss: 0.2599417655569751
  batch 411 loss: 0.2598721872643543
  batch 412 loss: 0.2598238211595318
  batch 413 loss: 0.25990413042736976
  batch 414 loss: 0.2599034469556693
  batch 415 loss: 0.2599032625735524
  batch 416 loss: 0.25992724718526006
  batch 417 loss: 0.25989788599151503
  batch 418 loss: 0.2598074057527136
  batch 419 loss: 0.2598643728614138
  batch 420 loss: 0.25981537380388803
  batch 421 loss: 0.2596830198643043
  batch 422 loss: 0.25981515148098433
  batch 423 loss: 0.2598584858308729
  batch 424 loss: 0.2598079475662056
  batch 425 loss: 0.2597619735142764
  batch 426 loss: 0.259707951279873
  batch 427 loss: 0.259740451948425
  batch 428 loss: 0.25974821187903946
  batch 429 loss: 0.25974838848992104
  batch 430 loss: 0.2597258207409881
  batch 431 loss: 0.2598220401739576
  batch 432 loss: 0.25985162512019827
  batch 433 loss: 0.2598353636044553
  batch 434 loss: 0.2599017922230031
  batch 435 loss: 0.2597807560158872
  batch 436 loss: 0.25979037999833393
  batch 437 loss: 0.25986469542134544
  batch 438 loss: 0.2600361259832774
  batch 439 loss: 0.26003247608084884
  batch 440 loss: 0.2600750990889289
  batch 441 loss: 0.259980263420784
  batch 442 loss: 0.2600181187709532
  batch 443 loss: 0.2600204690988123
  batch 444 loss: 0.2600149877436526
  batch 445 loss: 0.26006925300265965
  batch 446 loss: 0.2600631140539999
  batch 447 loss: 0.260023544032035
  batch 448 loss: 0.26010253466665745
  batch 449 loss: 0.26010291288318504
  batch 450 loss: 0.26001349323325684
  batch 451 loss: 0.259953073545993
  batch 452 loss: 0.2599582697833534
  batch 453 loss: 0.2599484030379365
  batch 454 loss: 0.2599578071795896
  batch 455 loss: 0.2599603507544968
  batch 456 loss: 0.2600547612497681
  batch 457 loss: 0.26010738785992044
  batch 458 loss: 0.26007208156533634
  batch 459 loss: 0.26005521721829516
  batch 460 loss: 0.260183846950531
  batch 461 loss: 0.26020188949630474
  batch 462 loss: 0.26023890110321374
  batch 463 loss: 0.26021750158676576
  batch 464 loss: 0.2602850844890907
  batch 465 loss: 0.26022872985691153
  batch 466 loss: 0.2601779227668635
  batch 467 loss: 0.2602098675113674
  batch 468 loss: 0.26020375152046865
  batch 469 loss: 0.2602811873531036
  batch 470 loss: 0.26033888830149426
  batch 471 loss: 0.2602962750285041
  batch 472 loss: 0.2601926683811313
LOSS train 0.2601926683811313 valid 0.2446276992559433
LOSS train 0.2601926683811313 valid 0.22333771735429764
LOSS train 0.2601926683811313 valid 0.22699636220932007
LOSS train 0.2601926683811313 valid 0.21465133130550385
LOSS train 0.2601926683811313 valid 0.20923798084259032
LOSS train 0.2601926683811313 valid 0.20799086491266885
LOSS train 0.2601926683811313 valid 0.2203869138445173
LOSS train 0.2601926683811313 valid 0.21634251810610294
LOSS train 0.2601926683811313 valid 0.2152045336034563
LOSS train 0.2601926683811313 valid 0.21712915152311324
LOSS train 0.2601926683811313 valid 0.21555837446993048
LOSS train 0.2601926683811313 valid 0.21382369846105576
LOSS train 0.2601926683811313 valid 0.2111205401328894
LOSS train 0.2601926683811313 valid 0.21163489456687654
LOSS train 0.2601926683811313 valid 0.20721239149570464
LOSS train 0.2601926683811313 valid 0.20973154995590448
LOSS train 0.2601926683811313 valid 0.2104885376551572
LOSS train 0.2601926683811313 valid 0.21009404129452175
LOSS train 0.2601926683811313 valid 0.21260309219360352
LOSS train 0.2601926683811313 valid 0.21275172457098962
LOSS train 0.2601926683811313 valid 0.2133099344514665
LOSS train 0.2601926683811313 valid 0.21219368820840662
LOSS train 0.2601926683811313 valid 0.21401283663252127
LOSS train 0.2601926683811313 valid 0.2133236713707447
LOSS train 0.2601926683811313 valid 0.21160373628139495
LOSS train 0.2601926683811313 valid 0.21144594767918953
LOSS train 0.2601926683811313 valid 0.21152971464174766
LOSS train 0.2601926683811313 valid 0.2128743055675711
LOSS train 0.2601926683811313 valid 0.2137713067490479
LOSS train 0.2601926683811313 valid 0.21409440835316976
LOSS train 0.2601926683811313 valid 0.21519703730460135
LOSS train 0.2601926683811313 valid 0.21502693463116884
LOSS train 0.2601926683811313 valid 0.21608916828126618
LOSS train 0.2601926683811313 valid 0.21557323108701146
LOSS train 0.2601926683811313 valid 0.21791275569370816
LOSS train 0.2601926683811313 valid 0.21781509866317114
LOSS train 0.2601926683811313 valid 0.2181921077741159
LOSS train 0.2601926683811313 valid 0.21874048482430608
LOSS train 0.2601926683811313 valid 0.21828502378402612
LOSS train 0.2601926683811313 valid 0.21869512535631658
LOSS train 0.2601926683811313 valid 0.21813033085043837
LOSS train 0.2601926683811313 valid 0.21823816640036448
LOSS train 0.2601926683811313 valid 0.21739795006984888
LOSS train 0.2601926683811313 valid 0.21715654669837517
LOSS train 0.2601926683811313 valid 0.21708576248751746
LOSS train 0.2601926683811313 valid 0.21789702028036118
LOSS train 0.2601926683811313 valid 0.21810128333720755
LOSS train 0.2601926683811313 valid 0.21791835315525532
LOSS train 0.2601926683811313 valid 0.21802431679501824
LOSS train 0.2601926683811313 valid 0.2175135451555252
LOSS train 0.2601926683811313 valid 0.21822346133344314
LOSS train 0.2601926683811313 valid 0.21801183831233245
LOSS train 0.2601926683811313 valid 0.2180789051190862
LOSS train 0.2601926683811313 valid 0.21870260878845496
LOSS train 0.2601926683811313 valid 0.21884843788363717
LOSS train 0.2601926683811313 valid 0.21875530986913613
LOSS train 0.2601926683811313 valid 0.21878916861718162
LOSS train 0.2601926683811313 valid 0.2183190612443562
LOSS train 0.2601926683811313 valid 0.21907182846028925
LOSS train 0.2601926683811313 valid 0.21848935708403588
LOSS train 0.2601926683811313 valid 0.21824052856593837
LOSS train 0.2601926683811313 valid 0.21894629948562191
LOSS train 0.2601926683811313 valid 0.21917570181309232
LOSS train 0.2601926683811313 valid 0.22005560458637774
LOSS train 0.2601926683811313 valid 0.21999191916905916
LOSS train 0.2601926683811313 valid 0.22005556139982108
LOSS train 0.2601926683811313 valid 0.21934827739623056
LOSS train 0.2601926683811313 valid 0.21990634873509407
LOSS train 0.2601926683811313 valid 0.21902796345344488
LOSS train 0.2601926683811313 valid 0.21924465426376888
LOSS train 0.2601926683811313 valid 0.2188445619294341
LOSS train 0.2601926683811313 valid 0.21919451736741596
LOSS train 0.2601926683811313 valid 0.2192212845364662
LOSS train 0.2601926683811313 valid 0.2189434845302556
LOSS train 0.2601926683811313 valid 0.21854443609714508
LOSS train 0.2601926683811313 valid 0.21910429059674866
LOSS train 0.2601926683811313 valid 0.21873214550606615
LOSS train 0.2601926683811313 valid 0.2186330035328865
LOSS train 0.2601926683811313 valid 0.21868747585936438
LOSS train 0.2601926683811313 valid 0.21797617115080356
LOSS train 0.2601926683811313 valid 0.21693672111004958
LOSS train 0.2601926683811313 valid 0.21700097384249292
LOSS train 0.2601926683811313 valid 0.21672645193266582
LOSS train 0.2601926683811313 valid 0.21660161887605986
LOSS train 0.2601926683811313 valid 0.2159067532595466
LOSS train 0.2601926683811313 valid 0.2151537473465121
LOSS train 0.2601926683811313 valid 0.21519490511252962
LOSS train 0.2601926683811313 valid 0.21475147845392878
LOSS train 0.2601926683811313 valid 0.2149490477998605
LOSS train 0.2601926683811313 valid 0.21504798465304906
LOSS train 0.2601926683811313 valid 0.2150482633611658
LOSS train 0.2601926683811313 valid 0.21499006855099098
LOSS train 0.2601926683811313 valid 0.21484928842513792
LOSS train 0.2601926683811313 valid 0.21518513909045686
LOSS train 0.2601926683811313 valid 0.2146531696382322
LOSS train 0.2601926683811313 valid 0.21472010854631662
LOSS train 0.2601926683811313 valid 0.21496072595881432
LOSS train 0.2601926683811313 valid 0.2153949056352888
LOSS train 0.2601926683811313 valid 0.21534771163656255
LOSS train 0.2601926683811313 valid 0.2154831199347973
LOSS train 0.2601926683811313 valid 0.2160398672417839
LOSS train 0.2601926683811313 valid 0.21624557586277232
LOSS train 0.2601926683811313 valid 0.21586860181058495
LOSS train 0.2601926683811313 valid 0.2156376547824878
LOSS train 0.2601926683811313 valid 0.21610255539417267
LOSS train 0.2601926683811313 valid 0.2163287434937819
LOSS train 0.2601926683811313 valid 0.2159262962430437
LOSS train 0.2601926683811313 valid 0.21589377563860682
LOSS train 0.2601926683811313 valid 0.21630645960295966
LOSS train 0.2601926683811313 valid 0.21659766205332495
LOSS train 0.2601926683811313 valid 0.21612807987509547
LOSS train 0.2601926683811313 valid 0.21602825088692562
LOSS train 0.2601926683811313 valid 0.21621525828289775
LOSS train 0.2601926683811313 valid 0.21617059305048825
LOSS train 0.2601926683811313 valid 0.21638696413973105
LOSS train 0.2601926683811313 valid 0.21685846779367018
LOSS train 0.2601926683811313 valid 0.21716533524867815
LOSS train 0.2601926683811313 valid 0.21719657061463696
LOSS train 0.2601926683811313 valid 0.21708199882707677
LOSS train 0.2601926683811313 valid 0.21683852051695188
LOSS train 0.2601926683811313 valid 0.21684513567400374
LOSS train 0.2601926683811313 valid 0.2168451723505239
LOSS train 0.2601926683811313 valid 0.2170583940133816
LOSS train 0.2601926683811313 valid 0.21724814476986085
LOSS train 0.2601926683811313 valid 0.21730811762809754
LOSS train 0.2601926683811313 valid 0.21747128594489323
LOSS train 0.2601926683811313 valid 0.21745988433285962
LOSS train 0.2601926683811313 valid 0.21765129326377064
LOSS train 0.2601926683811313 valid 0.21794909712418106
LOSS train 0.2601926683811313 valid 0.21775201020332483
LOSS train 0.2601926683811313 valid 0.2176145268533066
LOSS train 0.2601926683811313 valid 0.21739757264202292
LOSS train 0.2601926683811313 valid 0.21710438701443205
LOSS train 0.2601926683811313 valid 0.217302135900775
LOSS train 0.2601926683811313 valid 0.2173028376367357
LOSS train 0.2601926683811313 valid 0.21700327591422727
LOSS train 0.2601926683811313 valid 0.2167637226355337
LOSS train 0.2601926683811313 valid 0.2166808343667915
LOSS train 0.2601926683811313 valid 0.21654100105059232
LOSS train 0.2601926683811313 valid 0.21659833256687436
LOSS train 0.2601926683811313 valid 0.21673945607023037
LOSS train 0.2601926683811313 valid 0.21684248021370928
LOSS train 0.2601926683811313 valid 0.21694250352732786
LOSS train 0.2601926683811313 valid 0.21714383187807268
LOSS train 0.2601926683811313 valid 0.21703500172187543
LOSS train 0.2601926683811313 valid 0.21714462411322005
LOSS train 0.2601926683811313 valid 0.21698742881924116
LOSS train 0.2601926683811313 valid 0.21783919269974167
LOSS train 0.2601926683811313 valid 0.21785660198070858
LOSS train 0.2601926683811313 valid 0.2177109733223915
LOSS train 0.2601926683811313 valid 0.21784114442913738
LOSS train 0.2601926683811313 valid 0.2175752092152834
LOSS train 0.2601926683811313 valid 0.21766519517290825
LOSS train 0.2601926683811313 valid 0.21771980802734175
LOSS train 0.2601926683811313 valid 0.21764580511277723
LOSS train 0.2601926683811313 valid 0.21796100643964914
LOSS train 0.2601926683811313 valid 0.21786225107824728
LOSS train 0.2601926683811313 valid 0.21794211892764778
LOSS train 0.2601926683811313 valid 0.2178920678934961
LOSS train 0.2601926683811313 valid 0.21765811191871762
LOSS train 0.2601926683811313 valid 0.21753975628695874
LOSS train 0.2601926683811313 valid 0.2173486136727863
LOSS train 0.2601926683811313 valid 0.2172550590126061
LOSS train 0.2601926683811313 valid 0.21706773804091825
LOSS train 0.2601926683811313 valid 0.21698916030652596
LOSS train 0.2601926683811313 valid 0.21690320124827236
LOSS train 0.2601926683811313 valid 0.2170426836627686
LOSS train 0.2601926683811313 valid 0.21688126204978853
LOSS train 0.2601926683811313 valid 0.21698953943139704
LOSS train 0.2601926683811313 valid 0.21717690965708564
LOSS train 0.2601926683811313 valid 0.21711846175249558
LOSS train 0.2601926683811313 valid 0.21704569441634555
LOSS train 0.2601926683811313 valid 0.2169635121016144
LOSS train 0.2601926683811313 valid 0.21711908965960316
LOSS train 0.2601926683811313 valid 0.21706719083445414
LOSS train 0.2601926683811313 valid 0.21703497879207134
LOSS train 0.2601926683811313 valid 0.2168712493053264
LOSS train 0.2601926683811313 valid 0.21700551804531826
LOSS train 0.2601926683811313 valid 0.21688950994161255
LOSS train 0.2601926683811313 valid 0.21696410990423626
LOSS train 0.2601926683811313 valid 0.21700807426186555
LOSS train 0.2601926683811313 valid 0.21717209852003788
LOSS train 0.2601926683811313 valid 0.21709590954858748
LOSS train 0.2601926683811313 valid 0.2172985562811727
LOSS train 0.2601926683811313 valid 0.21711298960286218
LOSS train 0.2601926683811313 valid 0.21705341803771194
LOSS train 0.2601926683811313 valid 0.21686488692773218
LOSS train 0.2601926683811313 valid 0.2168433946814943
LOSS train 0.2601926683811313 valid 0.21686662623175867
LOSS train 0.2601926683811313 valid 0.2170975721980396
LOSS train 0.2601926683811313 valid 0.21702372996594893
LOSS train 0.2601926683811313 valid 0.21703083883039653
LOSS train 0.2601926683811313 valid 0.21701955857054558
LOSS train 0.2601926683811313 valid 0.21687865940873155
LOSS train 0.2601926683811313 valid 0.21654499211372474
LOSS train 0.2601926683811313 valid 0.2165969723493469
LOSS train 0.2601926683811313 valid 0.21677075447467378
LOSS train 0.2601926683811313 valid 0.21654839121332073
LOSS train 0.2601926683811313 valid 0.216595501306668
LOSS train 0.2601926683811313 valid 0.2165168233215809
LOSS train 0.2601926683811313 valid 0.21636283575598872
LOSS train 0.2601926683811313 valid 0.21636137174497735
LOSS train 0.2601926683811313 valid 0.21622155459937203
LOSS train 0.2601926683811313 valid 0.21621177265164898
LOSS train 0.2601926683811313 valid 0.21601356187971626
LOSS train 0.2601926683811313 valid 0.21590462826120044
LOSS train 0.2601926683811313 valid 0.21583253562738355
LOSS train 0.2601926683811313 valid 0.21574886408276284
LOSS train 0.2601926683811313 valid 0.2156249908596705
LOSS train 0.2601926683811313 valid 0.21573049475749334
LOSS train 0.2601926683811313 valid 0.21588623269474336
LOSS train 0.2601926683811313 valid 0.21580439960619188
LOSS train 0.2601926683811313 valid 0.21591292062835515
LOSS train 0.2601926683811313 valid 0.21580076099277656
LOSS train 0.2601926683811313 valid 0.21563238543133403
LOSS train 0.2601926683811313 valid 0.2155506647433396
LOSS train 0.2601926683811313 valid 0.21534450111850614
LOSS train 0.2601926683811313 valid 0.2154674128791608
LOSS train 0.2601926683811313 valid 0.21561202563379453
LOSS train 0.2601926683811313 valid 0.21573996035890144
LOSS train 0.2601926683811313 valid 0.21583832112642434
LOSS train 0.2601926683811313 valid 0.2157630291473758
LOSS train 0.2601926683811313 valid 0.21579125303056743
LOSS train 0.2601926683811313 valid 0.21586375131404825
LOSS train 0.2601926683811313 valid 0.2158459170659383
LOSS train 0.2601926683811313 valid 0.21597306858913035
LOSS train 0.2601926683811313 valid 0.21613100860349932
LOSS train 0.2601926683811313 valid 0.21619214431235664
LOSS train 0.2601926683811313 valid 0.2162544072715476
LOSS train 0.2601926683811313 valid 0.21640890564607537
LOSS train 0.2601926683811313 valid 0.2164214618536301
LOSS train 0.2601926683811313 valid 0.21649952354873048
LOSS train 0.2601926683811313 valid 0.2164550344959349
LOSS train 0.2601926683811313 valid 0.21652331445206943
LOSS train 0.2601926683811313 valid 0.2166030077224082
LOSS train 0.2601926683811313 valid 0.21642149353431442
LOSS train 0.2601926683811313 valid 0.21662124500998967
LOSS train 0.2601926683811313 valid 0.21648747705611862
LOSS train 0.2601926683811313 valid 0.21646742890569456
LOSS train 0.2601926683811313 valid 0.2164410905291637
LOSS train 0.2601926683811313 valid 0.21656316710458257
LOSS train 0.2601926683811313 valid 0.21634147082232247
LOSS train 0.2601926683811313 valid 0.21647202827557616
LOSS train 0.2601926683811313 valid 0.21666678450390942
LOSS train 0.2601926683811313 valid 0.2167962742095091
LOSS train 0.2601926683811313 valid 0.2167684257757373
LOSS train 0.2601926683811313 valid 0.21672345124758208
LOSS train 0.2601926683811313 valid 0.2166629800152394
LOSS train 0.2601926683811313 valid 0.2165701431801999
LOSS train 0.2601926683811313 valid 0.21674949389696122
LOSS train 0.2601926683811313 valid 0.2167552592626131
LOSS train 0.2601926683811313 valid 0.21677771801986392
LOSS train 0.2601926683811313 valid 0.2166806790432911
LOSS train 0.2601926683811313 valid 0.2166728616699459
LOSS train 0.2601926683811313 valid 0.21669270770222532
LOSS train 0.2601926683811313 valid 0.216701619385276
LOSS train 0.2601926683811313 valid 0.21664248928717603
LOSS train 0.2601926683811313 valid 0.21670978013859238
LOSS train 0.2601926683811313 valid 0.21671040010958564
LOSS train 0.2601926683811313 valid 0.21671033954391114
LOSS train 0.2601926683811313 valid 0.21693332892953207
LOSS train 0.2601926683811313 valid 0.2168988423379323
LOSS train 0.2601926683811313 valid 0.21696003755915755
LOSS train 0.2601926683811313 valid 0.2169288375386686
LOSS train 0.2601926683811313 valid 0.21691418492569114
LOSS train 0.2601926683811313 valid 0.216915300017909
LOSS train 0.2601926683811313 valid 0.21695766544967107
LOSS train 0.2601926683811313 valid 0.2170386943243333
LOSS train 0.2601926683811313 valid 0.21725107762671758
LOSS train 0.2601926683811313 valid 0.21738503613957652
LOSS train 0.2601926683811313 valid 0.2174537283028184
LOSS train 0.2601926683811313 valid 0.21772757391719258
LOSS train 0.2601926683811313 valid 0.21779684072885758
LOSS train 0.2601926683811313 valid 0.21763817960545964
LOSS train 0.2601926683811313 valid 0.21767117343165657
LOSS train 0.2601926683811313 valid 0.2175329491712045
LOSS train 0.2601926683811313 valid 0.21740663530378995
LOSS train 0.2601926683811313 valid 0.21722700307480722
LOSS train 0.2601926683811313 valid 0.21727959181459147
LOSS train 0.2601926683811313 valid 0.21713948148701873
LOSS train 0.2601926683811313 valid 0.21704313540798065
LOSS train 0.2601926683811313 valid 0.21698026274535673
LOSS train 0.2601926683811313 valid 0.21697766136364885
LOSS train 0.2601926683811313 valid 0.21692964670733667
LOSS train 0.2601926683811313 valid 0.21696214116456217
LOSS train 0.2601926683811313 valid 0.21691869704665004
LOSS train 0.2601926683811313 valid 0.21675151389234987
LOSS train 0.2601926683811313 valid 0.2167855438051952
LOSS train 0.2601926683811313 valid 0.21678982468831084
LOSS train 0.2601926683811313 valid 0.21690700932823379
LOSS train 0.2601926683811313 valid 0.2168362718267539
LOSS train 0.2601926683811313 valid 0.21688197073462892
LOSS train 0.2601926683811313 valid 0.21687732169042268
LOSS train 0.2601926683811313 valid 0.21712067424237322
LOSS train 0.2601926683811313 valid 0.21723815460326307
LOSS train 0.2601926683811313 valid 0.21712577926951485
LOSS train 0.2601926683811313 valid 0.2170981598221493
LOSS train 0.2601926683811313 valid 0.21702281225647702
LOSS train 0.2601926683811313 valid 0.2170572294339687
LOSS train 0.2601926683811313 valid 0.2169661505520344
LOSS train 0.2601926683811313 valid 0.21711434988286407
LOSS train 0.2601926683811313 valid 0.2171321657319732
LOSS train 0.2601926683811313 valid 0.21720438984164311
LOSS train 0.2601926683811313 valid 0.21730560701536505
LOSS train 0.2601926683811313 valid 0.21722082490803765
LOSS train 0.2601926683811313 valid 0.21716931140890308
LOSS train 0.2601926683811313 valid 0.21718132466757337
LOSS train 0.2601926683811313 valid 0.21715983724245777
LOSS train 0.2601926683811313 valid 0.21721990040012162
LOSS train 0.2601926683811313 valid 0.21708747095638706
LOSS train 0.2601926683811313 valid 0.21694743168507358
LOSS train 0.2601926683811313 valid 0.21709221033140635
LOSS train 0.2601926683811313 valid 0.21702973639812714
LOSS train 0.2601926683811313 valid 0.21710067465426816
LOSS train 0.2601926683811313 valid 0.21711744202507866
LOSS train 0.2601926683811313 valid 0.21711815049565292
LOSS train 0.2601926683811313 valid 0.21715736943840605
LOSS train 0.2601926683811313 valid 0.2172387276992858
LOSS train 0.2601926683811313 valid 0.21727174618587972
LOSS train 0.2601926683811313 valid 0.21718117306008936
LOSS train 0.2601926683811313 valid 0.2172355322265922
LOSS train 0.2601926683811313 valid 0.21719631413865534
LOSS train 0.2601926683811313 valid 0.217174179828942
LOSS train 0.2601926683811313 valid 0.21714508892209441
LOSS train 0.2601926683811313 valid 0.2171779743524698
LOSS train 0.2601926683811313 valid 0.21727775512663133
LOSS train 0.2601926683811313 valid 0.21735858292936913
LOSS train 0.2601926683811313 valid 0.21731632302810505
LOSS train 0.2601926683811313 valid 0.21754087149915724
LOSS train 0.2601926683811313 valid 0.21745892763137817
LOSS train 0.2601926683811313 valid 0.21735144368291262
LOSS train 0.2601926683811313 valid 0.21734333060771585
LOSS train 0.2601926683811313 valid 0.21731391687829932
LOSS train 0.2601926683811313 valid 0.21743264004677357
LOSS train 0.2601926683811313 valid 0.21751744644855386
LOSS train 0.2601926683811313 valid 0.21752209278444448
LOSS train 0.2601926683811313 valid 0.21757979903270652
LOSS train 0.2601926683811313 valid 0.21755070058551765
LOSS train 0.2601926683811313 valid 0.21748185834701786
LOSS train 0.2601926683811313 valid 0.21748720898347743
LOSS train 0.2601926683811313 valid 0.21749744474713054
LOSS train 0.2601926683811313 valid 0.21743122884753155
LOSS train 0.2601926683811313 valid 0.21738173089291543
LOSS train 0.2601926683811313 valid 0.217643640935421
LOSS train 0.2601926683811313 valid 0.21762614051500956
LOSS train 0.2601926683811313 valid 0.2175637673952676
LOSS train 0.2601926683811313 valid 0.21747931966864067
LOSS train 0.2601926683811313 valid 0.21734827774009485
LOSS train 0.2601926683811313 valid 0.21737712634497863
LOSS train 0.2601926683811313 valid 0.21731213561126164
LOSS train 0.2601926683811313 valid 0.21721055496622016
LOSS train 0.2601926683811313 valid 0.21718560459769584
LOSS train 0.2601926683811313 valid 0.21720958591858322
LOSS train 0.2601926683811313 valid 0.2171771808785234
LOSS train 0.2601926683811313 valid 0.21738596990914413
LOSS train 0.2601926683811313 valid 0.21739474977000375
LOSS train 0.2601926683811313 valid 0.21734508492198645
LOSS train 0.2601926683811313 valid 0.21730953267666214
LOSS train 0.2601926683811313 valid 0.21732093468515987
LOSS train 0.2601926683811313 valid 0.21725996794799965
LOSS train 0.2601926683811313 valid 0.2172018763018447
LOSS train 0.2601926683811313 valid 0.21725199034036194
LOSS train 0.2601926683811313 valid 0.2169862072658276
LOSS train 0.2601926683811313 valid 0.21700165469895352
LOSS train 0.2601926683811313 valid 0.2170129803353793
LOSS train 0.2601926683811313 valid 0.21693388673939992
LOSS train 0.2601926683811313 valid 0.21676652100982718
LOSS train 0.2601926683811313 valid 0.21674204127782065
LOSS train 0.2601926683811313 valid 0.216765029037871
EPOCH 13:
  batch 1 loss: 0.2524053454399109
  batch 2 loss: 0.23794960975646973
  batch 3 loss: 0.23864693939685822
  batch 4 loss: 0.2411181516945362
  batch 5 loss: 0.25288763344287873
  batch 6 loss: 0.2549300615986188
  batch 7 loss: 0.25590299495628904
  batch 8 loss: 0.2611467856913805
  batch 9 loss: 0.2607957505517536
  batch 10 loss: 0.26292317658662795
  batch 11 loss: 0.26164473593235016
  batch 12 loss: 0.2597450949251652
  batch 13 loss: 0.2572388442663046
  batch 14 loss: 0.2563149460724422
  batch 15 loss: 0.2570048133532206
  batch 16 loss: 0.25455966033041477
  batch 17 loss: 0.25194154942736907
  batch 18 loss: 0.25405260920524597
  batch 19 loss: 0.25372862502148275
  batch 20 loss: 0.25179616510868075
  batch 21 loss: 0.25221763480277287
  batch 22 loss: 0.2528006596998735
  batch 23 loss: 0.2532942554225092
  batch 24 loss: 0.2516537147263686
  batch 25 loss: 0.25317501425743105
  batch 26 loss: 0.2517808372011551
  batch 27 loss: 0.2519528407741476
  batch 28 loss: 0.25101834748472485
  batch 29 loss: 0.25135115302842237
  batch 30 loss: 0.25054097920656204
  batch 31 loss: 0.2513252213116615
  batch 32 loss: 0.2508903364650905
  batch 33 loss: 0.2511337539463332
  batch 34 loss: 0.251123271882534
  batch 35 loss: 0.25139027280466897
  batch 36 loss: 0.25143924810820156
  batch 37 loss: 0.2522913729016845
  batch 38 loss: 0.25333753110546814
  batch 39 loss: 0.25342904107692915
  batch 40 loss: 0.2535850014537573
  batch 41 loss: 0.2542388719029543
  batch 42 loss: 0.2546932636981919
  batch 43 loss: 0.254980199905329
  batch 44 loss: 0.25543204796585167
  batch 45 loss: 0.2548887872033649
  batch 46 loss: 0.25453651826018875
  batch 47 loss: 0.2547457126226831
  batch 48 loss: 0.2541042172039549
  batch 49 loss: 0.2539046598332269
  batch 50 loss: 0.25359726399183274
  batch 51 loss: 0.2531759993118398
  batch 52 loss: 0.25422797495355975
  batch 53 loss: 0.2538884557643027
  batch 54 loss: 0.254012535015742
  batch 55 loss: 0.2538704972375523
  batch 56 loss: 0.2540476495133979
  batch 57 loss: 0.254398299152391
  batch 58 loss: 0.25498485590877207
  batch 59 loss: 0.25577684201426426
  batch 60 loss: 0.25611959025263786
  batch 61 loss: 0.25680976983953696
  batch 62 loss: 0.2576928439159547
  batch 63 loss: 0.25753707332270487
  batch 64 loss: 0.2586894251871854
  batch 65 loss: 0.25845107298630937
  batch 66 loss: 0.2586967552250082
  batch 67 loss: 0.25893228578923355
  batch 68 loss: 0.2594805031138308
  batch 69 loss: 0.2592431010975354
  batch 70 loss: 0.2596114237393652
  batch 71 loss: 0.25908175845381237
  batch 72 loss: 0.25956537057128215
  batch 73 loss: 0.2596218020540394
  batch 74 loss: 0.2598659877438803
  batch 75 loss: 0.2596848076581955
  batch 76 loss: 0.26025389977975893
  batch 77 loss: 0.2598355594393495
  batch 78 loss: 0.2598209365820273
  batch 79 loss: 0.26067123760150956
  batch 80 loss: 0.2605546537786722
  batch 81 loss: 0.2605918016698625
  batch 82 loss: 0.2605852313884875
  batch 83 loss: 0.2605117268591042
  batch 84 loss: 0.2605586236431485
  batch 85 loss: 0.26002991076777965
  batch 86 loss: 0.2606243163693783
  batch 87 loss: 0.26050868784559184
  batch 88 loss: 0.2600634108212861
  batch 89 loss: 0.2598882081468454
  batch 90 loss: 0.25995667692687774
  batch 91 loss: 0.2604279686788936
  batch 92 loss: 0.26058484986424446
  batch 93 loss: 0.26083472355078624
  batch 94 loss: 0.261322114537371
  batch 95 loss: 0.2611096159407967
  batch 96 loss: 0.26138741212586564
  batch 97 loss: 0.2617195841577864
  batch 98 loss: 0.26200341053155
  batch 99 loss: 0.2622609764638573
  batch 100 loss: 0.26229501217603685
  batch 101 loss: 0.26203970360283807
  batch 102 loss: 0.2621618047648785
  batch 103 loss: 0.2623477826998072
  batch 104 loss: 0.26240815365543735
  batch 105 loss: 0.26262059580712094
  batch 106 loss: 0.2629542092107377
  batch 107 loss: 0.2622850386895866
  batch 108 loss: 0.26231354419831876
  batch 109 loss: 0.26226381752469125
  batch 110 loss: 0.2626011379740455
  batch 111 loss: 0.26262237923639314
  batch 112 loss: 0.2624938629035439
  batch 113 loss: 0.26269049744690415
  batch 114 loss: 0.263206877729349
  batch 115 loss: 0.26314148021780925
  batch 116 loss: 0.26315295799025173
  batch 117 loss: 0.26329896439853895
  batch 118 loss: 0.26302606372509973
  batch 119 loss: 0.2629337273225063
  batch 120 loss: 0.26272504044075806
  batch 121 loss: 0.2624852168412248
  batch 122 loss: 0.26223802310032923
  batch 123 loss: 0.2622302265428915
  batch 124 loss: 0.2624835226564638
  batch 125 loss: 0.2623624448776245
  batch 126 loss: 0.2623319398789179
  batch 127 loss: 0.26278867449347426
  batch 128 loss: 0.2627279758453369
  batch 129 loss: 0.26294599577438
  batch 130 loss: 0.2627248368584193
  batch 131 loss: 0.2628321398533028
  batch 132 loss: 0.26295418371305324
  batch 133 loss: 0.26317102422839717
  batch 134 loss: 0.263221311369049
  batch 135 loss: 0.26313092984535075
  batch 136 loss: 0.26306681345929117
  batch 137 loss: 0.26310793794419646
  batch 138 loss: 0.2631727376061937
  batch 139 loss: 0.26353507601528714
  batch 140 loss: 0.2634471045008728
  batch 141 loss: 0.26346027481217754
  batch 142 loss: 0.26339433069380236
  batch 143 loss: 0.2632090402441425
  batch 144 loss: 0.263070502732363
  batch 145 loss: 0.2628103003419679
  batch 146 loss: 0.26276344122135475
  batch 147 loss: 0.2630362393093758
  batch 148 loss: 0.2628130834247615
  batch 149 loss: 0.2625771882950059
  batch 150 loss: 0.26252098580201466
  batch 151 loss: 0.26248631512881904
  batch 152 loss: 0.26249881186767626
  batch 153 loss: 0.262350630916022
  batch 154 loss: 0.2627883011644537
  batch 155 loss: 0.262862781939968
  batch 156 loss: 0.26293730143553173
  batch 157 loss: 0.2629333709835247
  batch 158 loss: 0.26328852301156974
  batch 159 loss: 0.2637845650783875
  batch 160 loss: 0.2636042904108763
  batch 161 loss: 0.26364839650829386
  batch 162 loss: 0.26367246736714867
  batch 163 loss: 0.26394623426571945
  batch 164 loss: 0.2638680827690334
  batch 165 loss: 0.2639023827783989
  batch 166 loss: 0.26385338353105336
  batch 167 loss: 0.26368495456115926
  batch 168 loss: 0.2636416236027366
  batch 169 loss: 0.263592821518345
  batch 170 loss: 0.26345916293999727
  batch 171 loss: 0.2634357253187581
  batch 172 loss: 0.2632968104336151
  batch 173 loss: 0.26332124955736835
  batch 174 loss: 0.2634121882675708
  batch 175 loss: 0.2634631600550243
  batch 176 loss: 0.2633014628663659
  batch 177 loss: 0.2633381158450229
  batch 178 loss: 0.26349504501297233
  batch 179 loss: 0.26356847784039694
  batch 180 loss: 0.26362025116880733
  batch 181 loss: 0.26361777695173716
  batch 182 loss: 0.26364830701233266
  batch 183 loss: 0.26336802175787627
  batch 184 loss: 0.26318175863960513
  batch 185 loss: 0.2631116411170444
  batch 186 loss: 0.26327333991886465
  batch 187 loss: 0.26328231386322387
  batch 188 loss: 0.26291019699357926
  batch 189 loss: 0.2627449924983675
  batch 190 loss: 0.2625708564331657
  batch 191 loss: 0.26242024736254627
  batch 192 loss: 0.2624664388907452
  batch 193 loss: 0.2625431640778181
  batch 194 loss: 0.2626500801020062
  batch 195 loss: 0.26275105705628027
  batch 196 loss: 0.2627383979911707
  batch 197 loss: 0.26256986476745703
  batch 198 loss: 0.26270875628247403
  batch 199 loss: 0.26294226614973654
  batch 200 loss: 0.26315377913415433
  batch 201 loss: 0.2631814502809771
  batch 202 loss: 0.26314540716386076
  batch 203 loss: 0.26316673804092877
  batch 204 loss: 0.26302215603052403
  batch 205 loss: 0.2631966510923897
  batch 206 loss: 0.2631928693611645
  batch 207 loss: 0.2632233673247738
  batch 208 loss: 0.263072948043163
  batch 209 loss: 0.2629310250567477
  batch 210 loss: 0.2630175117935453
  batch 211 loss: 0.26301066753988583
  batch 212 loss: 0.26298493681088936
  batch 213 loss: 0.2629886416482254
  batch 214 loss: 0.2630567674603418
  batch 215 loss: 0.26285669921442517
  batch 216 loss: 0.2627567686830406
  batch 217 loss: 0.26276857642808815
  batch 218 loss: 0.26272369982725985
  batch 219 loss: 0.26265290964683985
  batch 220 loss: 0.2625701523639939
  batch 221 loss: 0.26264762217642496
  batch 222 loss: 0.26259847978750867
  batch 223 loss: 0.2626638612939638
  batch 224 loss: 0.26269846755479065
  batch 225 loss: 0.2624532835351096
  batch 226 loss: 0.26253114383568804
  batch 227 loss: 0.2622233590496794
  batch 228 loss: 0.26218042542275627
  batch 229 loss: 0.2620317346404213
  batch 230 loss: 0.26199277432068535
  batch 231 loss: 0.2619182055904752
  batch 232 loss: 0.26163129842486876
  batch 233 loss: 0.2615881200256266
  batch 234 loss: 0.2615749518840741
  batch 235 loss: 0.2616124243178266
  batch 236 loss: 0.2615257070983871
  batch 237 loss: 0.2614671164172611
  batch 238 loss: 0.2613665082124101
  batch 239 loss: 0.2612793555943038
  batch 240 loss: 0.2612935225789746
  batch 241 loss: 0.26138422567072744
  batch 242 loss: 0.2612730510348131
  batch 243 loss: 0.26126769019497764
  batch 244 loss: 0.2612203833509664
  batch 245 loss: 0.2612170394586057
  batch 246 loss: 0.26120445253403207
  batch 247 loss: 0.2612027766009574
  batch 248 loss: 0.26124496048977297
  batch 249 loss: 0.26118418616224004
  batch 250 loss: 0.26114817017316816
  batch 251 loss: 0.2610265322296743
  batch 252 loss: 0.26083444549687324
  batch 253 loss: 0.2606580520689252
  batch 254 loss: 0.26053950642272244
  batch 255 loss: 0.2605748658671099
  batch 256 loss: 0.26050455385120586
  batch 257 loss: 0.26050798583355395
  batch 258 loss: 0.26053958028092866
  batch 259 loss: 0.2606086216032735
  batch 260 loss: 0.2606765032387697
  batch 261 loss: 0.2607536530015112
  batch 262 loss: 0.2607682088517961
  batch 263 loss: 0.2608735169861253
  batch 264 loss: 0.26077656131802185
  batch 265 loss: 0.26076380644204483
  batch 266 loss: 0.2607163905649257
  batch 267 loss: 0.2607615338952354
  batch 268 loss: 0.2607143039245214
  batch 269 loss: 0.2606367525890414
  batch 270 loss: 0.26096612253674756
  batch 271 loss: 0.2610567950564557
  batch 272 loss: 0.2611065114136128
  batch 273 loss: 0.26107615587257205
  batch 274 loss: 0.26129428319034786
  batch 275 loss: 0.26132659776644274
  batch 276 loss: 0.2613382773014946
  batch 277 loss: 0.2613579570063615
  batch 278 loss: 0.2614497032632931
  batch 279 loss: 0.26165345252414757
  batch 280 loss: 0.2615635068288871
  batch 281 loss: 0.26145491518371894
  batch 282 loss: 0.26151600157749566
  batch 283 loss: 0.26146517809835845
  batch 284 loss: 0.26140915896271316
  batch 285 loss: 0.2614075720310211
  batch 286 loss: 0.26140742056019656
  batch 287 loss: 0.2613270995093555
  batch 288 loss: 0.2611140558599598
  batch 289 loss: 0.2613462463897817
  batch 290 loss: 0.2611282301360163
  batch 291 loss: 0.26109059631210013
  batch 292 loss: 0.2611311205650029
  batch 293 loss: 0.2611789022899732
  batch 294 loss: 0.2610952391916392
  batch 295 loss: 0.26110053607973
  batch 296 loss: 0.2611398967738087
  batch 297 loss: 0.2610433983682382
  batch 298 loss: 0.2610397844906621
  batch 299 loss: 0.2610212104974383
  batch 300 loss: 0.26100618263085684
  batch 301 loss: 0.26090059451684605
  batch 302 loss: 0.2608641598694372
  batch 303 loss: 0.26083217344858467
  batch 304 loss: 0.26078668422997
  batch 305 loss: 0.2606355690076703
  batch 306 loss: 0.26070619919915605
  batch 307 loss: 0.2605831341071704
  batch 308 loss: 0.26069787565570374
  batch 309 loss: 0.2607221000113534
  batch 310 loss: 0.26064744586906125
  batch 311 loss: 0.26077724657833
  batch 312 loss: 0.2608328412454098
  batch 313 loss: 0.26088535847564853
  batch 314 loss: 0.26094370411269985
  batch 315 loss: 0.26094932873097676
  batch 316 loss: 0.26089543872807597
  batch 317 loss: 0.260982428204374
  batch 318 loss: 0.2610266749870102
  batch 319 loss: 0.26099299230732514
  batch 320 loss: 0.26088918312452736
  batch 321 loss: 0.26082286303659835
  batch 322 loss: 0.2607911548444203
  batch 323 loss: 0.2606816336097363
  batch 324 loss: 0.26045875518042366
  batch 325 loss: 0.26043610976292536
  batch 326 loss: 0.2604629695415497
  batch 327 loss: 0.26037417001315943
  batch 328 loss: 0.26023996030775515
  batch 329 loss: 0.26029512485472267
  batch 330 loss: 0.2601044478290009
  batch 331 loss: 0.2600422907631923
  batch 332 loss: 0.25998953608683795
  batch 333 loss: 0.2599474184953415
  batch 334 loss: 0.25984992912251076
  batch 335 loss: 0.25968061249647567
  batch 336 loss: 0.25960598655399825
  batch 337 loss: 0.25945964455604553
  batch 338 loss: 0.25936557083616596
  batch 339 loss: 0.2591396023829778
  batch 340 loss: 0.2590247931287569
  batch 341 loss: 0.25898629619229224
  batch 342 loss: 0.2588858477204864
  batch 343 loss: 0.2589060691980857
  batch 344 loss: 0.2587855827323226
  batch 345 loss: 0.25886172773181526
  batch 346 loss: 0.2586957888968418
  batch 347 loss: 0.2587876119771677
  batch 348 loss: 0.2588228078066618
  batch 349 loss: 0.2587751902874698
  batch 350 loss: 0.2589153158238956
  batch 351 loss: 0.2590112578749996
  batch 352 loss: 0.2591985108923506
  batch 353 loss: 0.2592390982945985
  batch 354 loss: 0.2594145318798426
  batch 355 loss: 0.25946394847312443
  batch 356 loss: 0.2594316114116921
  batch 357 loss: 0.25933381230557334
  batch 358 loss: 0.25936218888067003
  batch 359 loss: 0.259352929661865
  batch 360 loss: 0.25940758776333595
  batch 361 loss: 0.25925243602565123
  batch 362 loss: 0.25927422784310017
  batch 363 loss: 0.2591587879358901
  batch 364 loss: 0.25909814435047107
  batch 365 loss: 0.25914454811239895
  batch 366 loss: 0.25906429594494607
  batch 367 loss: 0.2589663353619199
  batch 368 loss: 0.25890078452294285
  batch 369 loss: 0.2588201591639015
  batch 370 loss: 0.25884041206256764
  batch 371 loss: 0.2588465761624257
  batch 372 loss: 0.25880060789565884
  batch 373 loss: 0.2587309427779098
  batch 374 loss: 0.2585799547440228
  batch 375 loss: 0.2584865277608236
  batch 376 loss: 0.25847914862505933
  batch 377 loss: 0.2584507677219907
  batch 378 loss: 0.2583072498125374
  batch 379 loss: 0.2583741734005845
  batch 380 loss: 0.2583996402982034
  batch 381 loss: 0.25832089832605026
  batch 382 loss: 0.25820043199349446
  batch 383 loss: 0.2581850967270276
  batch 384 loss: 0.25815349879364174
  batch 385 loss: 0.258253796069653
  batch 386 loss: 0.2581799510152229
  batch 387 loss: 0.2582050510694198
  batch 388 loss: 0.25823166255944785
  batch 389 loss: 0.25821228884639347
  batch 390 loss: 0.25814937700827917
  batch 391 loss: 0.2582360014814855
  batch 392 loss: 0.2582266353146762
  batch 393 loss: 0.258250698750559
  batch 394 loss: 0.25832378966402886
  batch 395 loss: 0.25817838099938406
  batch 396 loss: 0.25811057101295454
  batch 397 loss: 0.2580985257247233
  batch 398 loss: 0.2580457245135427
  batch 399 loss: 0.25806580958211034
  batch 400 loss: 0.25815314412117
  batch 401 loss: 0.2580306139447148
  batch 402 loss: 0.2580471985894649
  batch 403 loss: 0.25807343386390963
  batch 404 loss: 0.25809951086003
  batch 405 loss: 0.2581011619464851
  batch 406 loss: 0.258148301227633
  batch 407 loss: 0.25810955329374835
  batch 408 loss: 0.25817810554130405
  batch 409 loss: 0.2580868386114722
  batch 410 loss: 0.25809829082430863
  batch 411 loss: 0.25803522852650523
  batch 412 loss: 0.25797040872637506
  batch 413 loss: 0.2580594791699264
  batch 414 loss: 0.2580430486423958
  batch 415 loss: 0.25802032775189504
  batch 416 loss: 0.25804275338752913
  batch 417 loss: 0.25800577281333276
  batch 418 loss: 0.2579317087191714
  batch 419 loss: 0.25798092722039234
  batch 420 loss: 0.2579357040070352
  batch 421 loss: 0.2578206427236068
  batch 422 loss: 0.2579344884973567
  batch 423 loss: 0.2580066788633383
  batch 424 loss: 0.2579492832762453
  batch 425 loss: 0.25789732242331787
  batch 426 loss: 0.25782827924534746
  batch 427 loss: 0.2578505413384292
  batch 428 loss: 0.25787795234088584
  batch 429 loss: 0.2579176816003862
  batch 430 loss: 0.257867138711519
  batch 431 loss: 0.2579669745989023
  batch 432 loss: 0.2579996806948825
  batch 433 loss: 0.25797596852581167
  batch 434 loss: 0.258072165746568
  batch 435 loss: 0.2579647874695131
  batch 436 loss: 0.2579830835035088
  batch 437 loss: 0.2580434241474902
  batch 438 loss: 0.2582304851921726
  batch 439 loss: 0.25822090668124326
  batch 440 loss: 0.25824187804352156
  batch 441 loss: 0.25814148999936454
  batch 442 loss: 0.258207353329227
  batch 443 loss: 0.25821056921648927
  batch 444 loss: 0.2581803957419889
  batch 445 loss: 0.25821461051367645
  batch 446 loss: 0.2582106626688632
  batch 447 loss: 0.2581620841084977
  batch 448 loss: 0.25825541612825226
  batch 449 loss: 0.258235049373854
  batch 450 loss: 0.2582122258676423
  batch 451 loss: 0.2581786269821772
  batch 452 loss: 0.2581751315269323
  batch 453 loss: 0.2581504344874397
  batch 454 loss: 0.25814982326414093
  batch 455 loss: 0.2581683182126873
  batch 456 loss: 0.25821333107326117
  batch 457 loss: 0.25823749275776176
  batch 458 loss: 0.25819860837995745
  batch 459 loss: 0.2582015183821223
  batch 460 loss: 0.2582630810206351
  batch 461 loss: 0.25822656787527876
  batch 462 loss: 0.2582473738433479
  batch 463 loss: 0.2582066300813889
  batch 464 loss: 0.25823558908726635
  batch 465 loss: 0.25815243727417403
  batch 466 loss: 0.25810584547437826
  batch 467 loss: 0.2581839819919374
  batch 468 loss: 0.2581598901341104
  batch 469 loss: 0.2582264169574038
  batch 470 loss: 0.2582693655440148
  batch 471 loss: 0.25821098103138546
  batch 472 loss: 0.2581214441649489
LOSS train 0.2581214441649489 valid 0.23716095089912415
LOSS train 0.2581214441649489 valid 0.21697382628917694
LOSS train 0.2581214441649489 valid 0.21797871589660645
LOSS train 0.2581214441649489 valid 0.20751671120524406
LOSS train 0.2581214441649489 valid 0.20118174850940704
LOSS train 0.2581214441649489 valid 0.201346717774868
LOSS train 0.2581214441649489 valid 0.21437572794301168
LOSS train 0.2581214441649489 valid 0.20956666581332684
LOSS train 0.2581214441649489 valid 0.20869604415363735
LOSS train 0.2581214441649489 valid 0.21095354557037355
LOSS train 0.2581214441649489 valid 0.2097343003207987
LOSS train 0.2581214441649489 valid 0.20820695906877518
LOSS train 0.2581214441649489 valid 0.2054857863829686
LOSS train 0.2581214441649489 valid 0.20689100878579275
LOSS train 0.2581214441649489 valid 0.2028739164272944
LOSS train 0.2581214441649489 valid 0.20605392288416624
LOSS train 0.2581214441649489 valid 0.20729241476339452
LOSS train 0.2581214441649489 valid 0.20699586222569147
LOSS train 0.2581214441649489 valid 0.2091834639248095
LOSS train 0.2581214441649489 valid 0.2094038836658001
LOSS train 0.2581214441649489 valid 0.21004978319009146
LOSS train 0.2581214441649489 valid 0.20894379846074365
LOSS train 0.2581214441649489 valid 0.21057133247023044
LOSS train 0.2581214441649489 valid 0.21001251166065535
LOSS train 0.2581214441649489 valid 0.20787680447101592
LOSS train 0.2581214441649489 valid 0.20788037318449754
LOSS train 0.2581214441649489 valid 0.2079496963156594
LOSS train 0.2581214441649489 valid 0.20914114426289285
LOSS train 0.2581214441649489 valid 0.20954520332402196
LOSS train 0.2581214441649489 valid 0.20994084030389787
LOSS train 0.2581214441649489 valid 0.21124581511943571
LOSS train 0.2581214441649489 valid 0.21123970858752728
LOSS train 0.2581214441649489 valid 0.21206916642911505
LOSS train 0.2581214441649489 valid 0.21180150394930558
LOSS train 0.2581214441649489 valid 0.21404158345290591
LOSS train 0.2581214441649489 valid 0.21381580581267676
LOSS train 0.2581214441649489 valid 0.21431704389082418
LOSS train 0.2581214441649489 valid 0.21496474154685674
LOSS train 0.2581214441649489 valid 0.2143335888783137
LOSS train 0.2581214441649489 valid 0.2150036573410034
LOSS train 0.2581214441649489 valid 0.21445986737565295
LOSS train 0.2581214441649489 valid 0.21464589060772032
LOSS train 0.2581214441649489 valid 0.21366776456666547
LOSS train 0.2581214441649489 valid 0.21352502263405107
LOSS train 0.2581214441649489 valid 0.21346252262592316
LOSS train 0.2581214441649489 valid 0.21425705338301865
LOSS train 0.2581214441649489 valid 0.21462619463179974
LOSS train 0.2581214441649489 valid 0.21454654106249413
LOSS train 0.2581214441649489 valid 0.21457268115209074
LOSS train 0.2581214441649489 valid 0.21393434911966325
LOSS train 0.2581214441649489 valid 0.2146799403662775
LOSS train 0.2581214441649489 valid 0.2144626324566511
LOSS train 0.2581214441649489 valid 0.21467797289479454
LOSS train 0.2581214441649489 valid 0.21545567197932136
LOSS train 0.2581214441649489 valid 0.21555945141748947
LOSS train 0.2581214441649489 valid 0.2154100090265274
LOSS train 0.2581214441649489 valid 0.2156871447437688
LOSS train 0.2581214441649489 valid 0.21511638215903578
LOSS train 0.2581214441649489 valid 0.21581190521434201
LOSS train 0.2581214441649489 valid 0.21520588894685108
LOSS train 0.2581214441649489 valid 0.2150462606891257
LOSS train 0.2581214441649489 valid 0.21575601110535284
LOSS train 0.2581214441649489 valid 0.21604108219108883
LOSS train 0.2581214441649489 valid 0.2168171696830541
LOSS train 0.2581214441649489 valid 0.21697746813297272
LOSS train 0.2581214441649489 valid 0.21693881736560303
LOSS train 0.2581214441649489 valid 0.21631234461691842
LOSS train 0.2581214441649489 valid 0.21683306400390231
LOSS train 0.2581214441649489 valid 0.2160293818383977
LOSS train 0.2581214441649489 valid 0.21622830331325532
LOSS train 0.2581214441649489 valid 0.21582353010144031
LOSS train 0.2581214441649489 valid 0.21616973955598143
LOSS train 0.2581214441649489 valid 0.21624301210658192
LOSS train 0.2581214441649489 valid 0.21592056449200656
LOSS train 0.2581214441649489 valid 0.21556695719559987
LOSS train 0.2581214441649489 valid 0.21624354292687617
LOSS train 0.2581214441649489 valid 0.21578083796934647
LOSS train 0.2581214441649489 valid 0.2155436397745059
LOSS train 0.2581214441649489 valid 0.21567666323124607
LOSS train 0.2581214441649489 valid 0.21495395712554455
LOSS train 0.2581214441649489 valid 0.21389989260538125
LOSS train 0.2581214441649489 valid 0.21402612910038088
LOSS train 0.2581214441649489 valid 0.2137570657643927
LOSS train 0.2581214441649489 valid 0.21367363365633146
LOSS train 0.2581214441649489 valid 0.21295585509608775
LOSS train 0.2581214441649489 valid 0.21232323781695478
LOSS train 0.2581214441649489 valid 0.212393951313249
LOSS train 0.2581214441649489 valid 0.2118876747448336
LOSS train 0.2581214441649489 valid 0.2119816712449106
LOSS train 0.2581214441649489 valid 0.21211005863216187
LOSS train 0.2581214441649489 valid 0.21206144661038787
LOSS train 0.2581214441649489 valid 0.21208388423142227
LOSS train 0.2581214441649489 valid 0.21189057314267723
LOSS train 0.2581214441649489 valid 0.21219480862008763
LOSS train 0.2581214441649489 valid 0.2117341642317019
LOSS train 0.2581214441649489 valid 0.21174899504209557
LOSS train 0.2581214441649489 valid 0.21201870813197696
LOSS train 0.2581214441649489 valid 0.21237810670721288
LOSS train 0.2581214441649489 valid 0.21225999175298094
LOSS train 0.2581214441649489 valid 0.21249837920069695
LOSS train 0.2581214441649489 valid 0.2130800891335648
LOSS train 0.2581214441649489 valid 0.21332225466475768
LOSS train 0.2581214441649489 valid 0.21290047987572197
LOSS train 0.2581214441649489 valid 0.21267112459127718
LOSS train 0.2581214441649489 valid 0.21303572484425137
LOSS train 0.2581214441649489 valid 0.21334294131341972
LOSS train 0.2581214441649489 valid 0.21290579062198925
LOSS train 0.2581214441649489 valid 0.21276071281344802
LOSS train 0.2581214441649489 valid 0.21322907985897238
LOSS train 0.2581214441649489 valid 0.2134644386443225
LOSS train 0.2581214441649489 valid 0.21296876387016192
LOSS train 0.2581214441649489 valid 0.21290464566222259
LOSS train 0.2581214441649489 valid 0.21302852337866757
LOSS train 0.2581214441649489 valid 0.21301875760157904
LOSS train 0.2581214441649489 valid 0.21321859372698743
LOSS train 0.2581214441649489 valid 0.21373767624127454
LOSS train 0.2581214441649489 valid 0.21401591165962383
LOSS train 0.2581214441649489 valid 0.21401350339085368
LOSS train 0.2581214441649489 valid 0.2138587538184238
LOSS train 0.2581214441649489 valid 0.21363299588362375
LOSS train 0.2581214441649489 valid 0.21358964216610618
LOSS train 0.2581214441649489 valid 0.21363563884477146
LOSS train 0.2581214441649489 valid 0.21392163852366006
LOSS train 0.2581214441649489 valid 0.21409077677995927
LOSS train 0.2581214441649489 valid 0.2141697771549225
LOSS train 0.2581214441649489 valid 0.21432497222272176
LOSS train 0.2581214441649489 valid 0.2143071769025382
LOSS train 0.2581214441649489 valid 0.2145168277202174
LOSS train 0.2581214441649489 valid 0.2147712022528168
LOSS train 0.2581214441649489 valid 0.21463158887166242
LOSS train 0.2581214441649489 valid 0.21451862224640736
LOSS train 0.2581214441649489 valid 0.21423865674120007
LOSS train 0.2581214441649489 valid 0.21402119884365484
LOSS train 0.2581214441649489 valid 0.21422434653808822
LOSS train 0.2581214441649489 valid 0.2141998475348508
LOSS train 0.2581214441649489 valid 0.2139110118150711
LOSS train 0.2581214441649489 valid 0.21368426171532512
LOSS train 0.2581214441649489 valid 0.21355763347684473
LOSS train 0.2581214441649489 valid 0.21343758294908263
LOSS train 0.2581214441649489 valid 0.21346604259950774
LOSS train 0.2581214441649489 valid 0.21362314568766466
LOSS train 0.2581214441649489 valid 0.21366106173102284
LOSS train 0.2581214441649489 valid 0.21377503798974978
LOSS train 0.2581214441649489 valid 0.21399353972325721
LOSS train 0.2581214441649489 valid 0.2138850621108351
LOSS train 0.2581214441649489 valid 0.21392387845744826
LOSS train 0.2581214441649489 valid 0.2137455000561111
LOSS train 0.2581214441649489 valid 0.2145645204629447
LOSS train 0.2581214441649489 valid 0.2145672821558562
LOSS train 0.2581214441649489 valid 0.21444543411334355
LOSS train 0.2581214441649489 valid 0.21456762506867086
LOSS train 0.2581214441649489 valid 0.2142859372849527
LOSS train 0.2581214441649489 valid 0.21440794762053522
LOSS train 0.2581214441649489 valid 0.21449259155756467
LOSS train 0.2581214441649489 valid 0.21446564562859075
LOSS train 0.2581214441649489 valid 0.21477637745631048
LOSS train 0.2581214441649489 valid 0.2146834883910076
LOSS train 0.2581214441649489 valid 0.21476489786483063
LOSS train 0.2581214441649489 valid 0.21473639566193586
LOSS train 0.2581214441649489 valid 0.2144770197570324
LOSS train 0.2581214441649489 valid 0.21437786824954963
LOSS train 0.2581214441649489 valid 0.21416035368118758
LOSS train 0.2581214441649489 valid 0.21408736422383712
LOSS train 0.2581214441649489 valid 0.2139126103280521
LOSS train 0.2581214441649489 valid 0.213852109060143
LOSS train 0.2581214441649489 valid 0.21379452468041915
LOSS train 0.2581214441649489 valid 0.21397764498959043
LOSS train 0.2581214441649489 valid 0.21377423041987986
LOSS train 0.2581214441649489 valid 0.2138938215180967
LOSS train 0.2581214441649489 valid 0.21407675217179692
LOSS train 0.2581214441649489 valid 0.21409939848191556
LOSS train 0.2581214441649489 valid 0.21398658998483835
LOSS train 0.2581214441649489 valid 0.2139217338465542
LOSS train 0.2581214441649489 valid 0.2140846134259783
LOSS train 0.2581214441649489 valid 0.21404976172106607
LOSS train 0.2581214441649489 valid 0.21399000803516668
LOSS train 0.2581214441649489 valid 0.21387021369853262
LOSS train 0.2581214441649489 valid 0.21398223434271438
LOSS train 0.2581214441649489 valid 0.21384589831922307
LOSS train 0.2581214441649489 valid 0.21387660221921073
LOSS train 0.2581214441649489 valid 0.2139101295181401
LOSS train 0.2581214441649489 valid 0.21406251928963504
LOSS train 0.2581214441649489 valid 0.21402191830788805
LOSS train 0.2581214441649489 valid 0.21422886856548165
LOSS train 0.2581214441649489 valid 0.21402280370931367
LOSS train 0.2581214441649489 valid 0.21396056234195668
LOSS train 0.2581214441649489 valid 0.2138272947806088
LOSS train 0.2581214441649489 valid 0.21380838942020497
LOSS train 0.2581214441649489 valid 0.21379943452184164
LOSS train 0.2581214441649489 valid 0.21400403552933744
LOSS train 0.2581214441649489 valid 0.21392275915720077
LOSS train 0.2581214441649489 valid 0.21388818284807107
LOSS train 0.2581214441649489 valid 0.21386615020932312
LOSS train 0.2581214441649489 valid 0.2137145986876537
LOSS train 0.2581214441649489 valid 0.21335282111779239
LOSS train 0.2581214441649489 valid 0.21336849467182645
LOSS train 0.2581214441649489 valid 0.21349733160231923
LOSS train 0.2581214441649489 valid 0.21328597809329178
LOSS train 0.2581214441649489 valid 0.21330333639628923
LOSS train 0.2581214441649489 valid 0.21321253962814807
LOSS train 0.2581214441649489 valid 0.2130831572695158
LOSS train 0.2581214441649489 valid 0.2130785116022176
LOSS train 0.2581214441649489 valid 0.21293940472191777
LOSS train 0.2581214441649489 valid 0.212953129965885
LOSS train 0.2581214441649489 valid 0.21278635852220582
LOSS train 0.2581214441649489 valid 0.21269616173598374
LOSS train 0.2581214441649489 valid 0.2125904550730894
LOSS train 0.2581214441649489 valid 0.2125421793988118
LOSS train 0.2581214441649489 valid 0.21244983981100565
LOSS train 0.2581214441649489 valid 0.2125482733760561
LOSS train 0.2581214441649489 valid 0.2126969986758526
LOSS train 0.2581214441649489 valid 0.212642094225816
LOSS train 0.2581214441649489 valid 0.2127449461411982
LOSS train 0.2581214441649489 valid 0.2126430389340793
LOSS train 0.2581214441649489 valid 0.21250785970410635
LOSS train 0.2581214441649489 valid 0.21245831444307609
LOSS train 0.2581214441649489 valid 0.21227624765189562
LOSS train 0.2581214441649489 valid 0.21236591731463003
LOSS train 0.2581214441649489 valid 0.21255536527121993
LOSS train 0.2581214441649489 valid 0.2127166015858
LOSS train 0.2581214441649489 valid 0.21275687386277575
LOSS train 0.2581214441649489 valid 0.21268822205764754
LOSS train 0.2581214441649489 valid 0.2127187173195484
LOSS train 0.2581214441649489 valid 0.21282692054020508
LOSS train 0.2581214441649489 valid 0.21279180162482791
LOSS train 0.2581214441649489 valid 0.21294531496488942
LOSS train 0.2581214441649489 valid 0.21306978559966655
LOSS train 0.2581214441649489 valid 0.21315917483808702
LOSS train 0.2581214441649489 valid 0.21320935576243172
LOSS train 0.2581214441649489 valid 0.21334429564683333
LOSS train 0.2581214441649489 valid 0.2133335674350912
LOSS train 0.2581214441649489 valid 0.21340985596179962
LOSS train 0.2581214441649489 valid 0.2133621874669079
LOSS train 0.2581214441649489 valid 0.21341433242345467
LOSS train 0.2581214441649489 valid 0.2134761740552618
LOSS train 0.2581214441649489 valid 0.21327792145943236
LOSS train 0.2581214441649489 valid 0.2134344335095289
LOSS train 0.2581214441649489 valid 0.21330455878452093
LOSS train 0.2581214441649489 valid 0.21329156642927785
LOSS train 0.2581214441649489 valid 0.21326594439645608
LOSS train 0.2581214441649489 valid 0.21336483837905265
LOSS train 0.2581214441649489 valid 0.21318064827071734
LOSS train 0.2581214441649489 valid 0.21329411385971822
LOSS train 0.2581214441649489 valid 0.213400521361437
LOSS train 0.2581214441649489 valid 0.21356447168758938
LOSS train 0.2581214441649489 valid 0.213520228317598
LOSS train 0.2581214441649489 valid 0.21343883202384842
LOSS train 0.2581214441649489 valid 0.21339809630186327
LOSS train 0.2581214441649489 valid 0.21331771777336855
LOSS train 0.2581214441649489 valid 0.21351475155353547
LOSS train 0.2581214441649489 valid 0.21350040556900055
LOSS train 0.2581214441649489 valid 0.21351249196699687
LOSS train 0.2581214441649489 valid 0.21342250304259802
LOSS train 0.2581214441649489 valid 0.21345465248963963
LOSS train 0.2581214441649489 valid 0.21347881380249473
LOSS train 0.2581214441649489 valid 0.21348451101221144
LOSS train 0.2581214441649489 valid 0.21340141314940694
LOSS train 0.2581214441649489 valid 0.21346369709155355
LOSS train 0.2581214441649489 valid 0.2134285543876265
LOSS train 0.2581214441649489 valid 0.2134646046620149
LOSS train 0.2581214441649489 valid 0.2136434465075818
LOSS train 0.2581214441649489 valid 0.2136278104691105
LOSS train 0.2581214441649489 valid 0.21366734627081868
LOSS train 0.2581214441649489 valid 0.2136313754952315
LOSS train 0.2581214441649489 valid 0.2135923241669277
LOSS train 0.2581214441649489 valid 0.21358865835612878
LOSS train 0.2581214441649489 valid 0.21361905124303554
LOSS train 0.2581214441649489 valid 0.2136841979044587
LOSS train 0.2581214441649489 valid 0.213917990931791
LOSS train 0.2581214441649489 valid 0.2140788816743427
LOSS train 0.2581214441649489 valid 0.21416329595215647
LOSS train 0.2581214441649489 valid 0.21441699767156558
LOSS train 0.2581214441649489 valid 0.2144850743013424
LOSS train 0.2581214441649489 valid 0.2143433894010356
LOSS train 0.2581214441649489 valid 0.21439105331897734
LOSS train 0.2581214441649489 valid 0.214268174238395
LOSS train 0.2581214441649489 valid 0.2141694406632482
LOSS train 0.2581214441649489 valid 0.2139674508743149
LOSS train 0.2581214441649489 valid 0.21402005002062807
LOSS train 0.2581214441649489 valid 0.21388626119920184
LOSS train 0.2581214441649489 valid 0.21380506678620267
LOSS train 0.2581214441649489 valid 0.213737519867454
LOSS train 0.2581214441649489 valid 0.21374201200665519
LOSS train 0.2581214441649489 valid 0.2136969682301434
LOSS train 0.2581214441649489 valid 0.21374187260343316
LOSS train 0.2581214441649489 valid 0.21372161409654816
LOSS train 0.2581214441649489 valid 0.21354004626490097
LOSS train 0.2581214441649489 valid 0.21359419108678898
LOSS train 0.2581214441649489 valid 0.213606655700809
LOSS train 0.2581214441649489 valid 0.21371860303755463
LOSS train 0.2581214441649489 valid 0.21364002677378377
LOSS train 0.2581214441649489 valid 0.21369702797638226
LOSS train 0.2581214441649489 valid 0.21369878118762384
LOSS train 0.2581214441649489 valid 0.2139692470735433
LOSS train 0.2581214441649489 valid 0.21409583202863144
LOSS train 0.2581214441649489 valid 0.21400876730881832
LOSS train 0.2581214441649489 valid 0.21397341461695404
LOSS train 0.2581214441649489 valid 0.21390634565145378
LOSS train 0.2581214441649489 valid 0.21394627125167529
LOSS train 0.2581214441649489 valid 0.21385452111562092
LOSS train 0.2581214441649489 valid 0.21400463501876374
LOSS train 0.2581214441649489 valid 0.21402704079222207
LOSS train 0.2581214441649489 valid 0.21404734278472737
LOSS train 0.2581214441649489 valid 0.21411768655831875
LOSS train 0.2581214441649489 valid 0.21405438560931409
LOSS train 0.2581214441649489 valid 0.21400844982636522
LOSS train 0.2581214441649489 valid 0.21398768474496538
LOSS train 0.2581214441649489 valid 0.21398468281735072
LOSS train 0.2581214441649489 valid 0.2140493695789942
LOSS train 0.2581214441649489 valid 0.2139096090870519
LOSS train 0.2581214441649489 valid 0.21377127064194326
LOSS train 0.2581214441649489 valid 0.2138695967121002
LOSS train 0.2581214441649489 valid 0.2138054050957433
LOSS train 0.2581214441649489 valid 0.21389271945330748
LOSS train 0.2581214441649489 valid 0.21389124715138996
LOSS train 0.2581214441649489 valid 0.2139106014672714
LOSS train 0.2581214441649489 valid 0.21395450106554603
LOSS train 0.2581214441649489 valid 0.21405776519820374
LOSS train 0.2581214441649489 valid 0.2140916782113078
LOSS train 0.2581214441649489 valid 0.21399099919945003
LOSS train 0.2581214441649489 valid 0.21406970997094366
LOSS train 0.2581214441649489 valid 0.21402622768597573
LOSS train 0.2581214441649489 valid 0.21400356744834145
LOSS train 0.2581214441649489 valid 0.21398058013967525
LOSS train 0.2581214441649489 valid 0.21402005126843085
LOSS train 0.2581214441649489 valid 0.2141316588396675
LOSS train 0.2581214441649489 valid 0.2142496752903002
LOSS train 0.2581214441649489 valid 0.21420021305178724
LOSS train 0.2581214441649489 valid 0.2144299640934518
LOSS train 0.2581214441649489 valid 0.2143413269610116
LOSS train 0.2581214441649489 valid 0.21425711095693248
LOSS train 0.2581214441649489 valid 0.2142874925700297
LOSS train 0.2581214441649489 valid 0.21425086170345456
LOSS train 0.2581214441649489 valid 0.214377021896625
LOSS train 0.2581214441649489 valid 0.21447524081415204
LOSS train 0.2581214441649489 valid 0.2144655647377173
LOSS train 0.2581214441649489 valid 0.2144724943340709
LOSS train 0.2581214441649489 valid 0.21446486337650458
LOSS train 0.2581214441649489 valid 0.21439470926568924
LOSS train 0.2581214441649489 valid 0.21442520798129194
LOSS train 0.2581214441649489 valid 0.21441206777375466
LOSS train 0.2581214441649489 valid 0.2143343269650699
LOSS train 0.2581214441649489 valid 0.21429366420726387
LOSS train 0.2581214441649489 valid 0.21453444096584653
LOSS train 0.2581214441649489 valid 0.2145235349734624
LOSS train 0.2581214441649489 valid 0.21446571645536863
LOSS train 0.2581214441649489 valid 0.21439066267322737
LOSS train 0.2581214441649489 valid 0.21428245524394102
LOSS train 0.2581214441649489 valid 0.21431419454023967
LOSS train 0.2581214441649489 valid 0.21423694227422987
LOSS train 0.2581214441649489 valid 0.21411742253011448
LOSS train 0.2581214441649489 valid 0.21410318781537088
LOSS train 0.2581214441649489 valid 0.21416325246646115
LOSS train 0.2581214441649489 valid 0.21411961035034752
LOSS train 0.2581214441649489 valid 0.21435870893404518
LOSS train 0.2581214441649489 valid 0.21437576095039926
LOSS train 0.2581214441649489 valid 0.21429542583577774
LOSS train 0.2581214441649489 valid 0.21426897267056577
LOSS train 0.2581214441649489 valid 0.2143021451729586
LOSS train 0.2581214441649489 valid 0.21423285694585906
LOSS train 0.2581214441649489 valid 0.2141619119947967
LOSS train 0.2581214441649489 valid 0.21424440405645423
LOSS train 0.2581214441649489 valid 0.21397534530024884
LOSS train 0.2581214441649489 valid 0.2139978672858778
LOSS train 0.2581214441649489 valid 0.21399916410446168
LOSS train 0.2581214441649489 valid 0.21390005905445808
LOSS train 0.2581214441649489 valid 0.2137283712463093
LOSS train 0.2581214441649489 valid 0.213714346613573
LOSS train 0.2581214441649489 valid 0.2137601347715874
EPOCH 14:
  batch 1 loss: 0.24048911035060883
  batch 2 loss: 0.22017639130353928
  batch 3 loss: 0.23517517745494843
  batch 4 loss: 0.23345617577433586
  batch 5 loss: 0.24847882091999055
  batch 6 loss: 0.2513564849893252
  batch 7 loss: 0.2506414438996996
  batch 8 loss: 0.254291869699955
  batch 9 loss: 0.25441176692644757
  batch 10 loss: 0.2567081779241562
  batch 11 loss: 0.2557745779102499
  batch 12 loss: 0.2537287672360738
  batch 13 loss: 0.25112203565927654
  batch 14 loss: 0.2514257611972945
  batch 15 loss: 0.25093093812465667
  batch 16 loss: 0.24874991551041603
  batch 17 loss: 0.24604140923303716
  batch 18 loss: 0.24998180485434002
  batch 19 loss: 0.2491877439774965
  batch 20 loss: 0.24736631959676741
  batch 21 loss: 0.24783661393892198
  batch 22 loss: 0.24880308590152048
  batch 23 loss: 0.24897145058797754
  batch 24 loss: 0.2470179939021667
  batch 25 loss: 0.24907511413097383
  batch 26 loss: 0.2478680478838774
  batch 27 loss: 0.24820214013258615
  batch 28 loss: 0.24784388606037414
  batch 29 loss: 0.24799805160226493
  batch 30 loss: 0.24754882454872132
  batch 31 loss: 0.2484107152108223
  batch 32 loss: 0.2477700379677117
  batch 33 loss: 0.2478267058278575
  batch 34 loss: 0.2472698938320665
  batch 35 loss: 0.24708853236266545
  batch 36 loss: 0.24683370689551035
  batch 37 loss: 0.24734060587109746
  batch 38 loss: 0.24763427831624685
  batch 39 loss: 0.24759537096206957
  batch 40 loss: 0.2476396668702364
  batch 41 loss: 0.2481619210504904
  batch 42 loss: 0.24873400763386772
  batch 43 loss: 0.2488093837056049
  batch 44 loss: 0.249276974323121
  batch 45 loss: 0.2484216829140981
  batch 46 loss: 0.2480366605779399
  batch 47 loss: 0.24857018729473682
  batch 48 loss: 0.24785341105113426
  batch 49 loss: 0.2475421082000343
  batch 50 loss: 0.2477432382106781
  batch 51 loss: 0.24728140439472945
  batch 52 loss: 0.24754024898776641
  batch 53 loss: 0.24765107907214254
  batch 54 loss: 0.24870343975446843
  batch 55 loss: 0.24868564253503625
  batch 56 loss: 0.2492302547075919
  batch 57 loss: 0.24982441334347977
  batch 58 loss: 0.2507005136074691
  batch 59 loss: 0.25127068591319907
  batch 60 loss: 0.2516530138750871
  batch 61 loss: 0.25254187403155154
  batch 62 loss: 0.2534589529518158
  batch 63 loss: 0.2534489160965359
  batch 64 loss: 0.25500279222615063
  batch 65 loss: 0.2550934674648138
  batch 66 loss: 0.25510481970779825
  batch 67 loss: 0.2553968983354853
  batch 68 loss: 0.256541497347986
  batch 69 loss: 0.2564435605553613
  batch 70 loss: 0.25663587493555884
  batch 71 loss: 0.25639306659429845
  batch 72 loss: 0.25654832315113807
  batch 73 loss: 0.25660572966484174
  batch 74 loss: 0.2570078759580045
  batch 75 loss: 0.2568243642648061
  batch 76 loss: 0.25734790610639674
  batch 77 loss: 0.2569083970088463
  batch 78 loss: 0.256890064630753
  batch 79 loss: 0.25768965030018287
  batch 80 loss: 0.2573277659714222
  batch 81 loss: 0.25741125772028794
  batch 82 loss: 0.25748041235819097
  batch 83 loss: 0.2573090604988925
  batch 84 loss: 0.2573764643498829
  batch 85 loss: 0.25688513780341427
  batch 86 loss: 0.2573680494760358
  batch 87 loss: 0.2573619859999624
  batch 88 loss: 0.25689002363519237
  batch 89 loss: 0.25683164027299776
  batch 90 loss: 0.2568551219171948
  batch 91 loss: 0.25729115847702866
  batch 92 loss: 0.25740925125453784
  batch 93 loss: 0.2574826991686257
  batch 94 loss: 0.2580899244293253
  batch 95 loss: 0.25781464717890085
  batch 96 loss: 0.25795271325235564
  batch 97 loss: 0.25828579452234446
  batch 98 loss: 0.2584552039601365
  batch 99 loss: 0.25859659022153025
  batch 100 loss: 0.2586397136747837
  batch 101 loss: 0.25836262992112946
  batch 102 loss: 0.2585242519191667
  batch 103 loss: 0.25869654596430586
  batch 104 loss: 0.2585958338414247
  batch 105 loss: 0.25882969853423893
  batch 106 loss: 0.25916500206825865
  batch 107 loss: 0.25845007876926496
  batch 108 loss: 0.2584437853484242
  batch 109 loss: 0.2583972372866552
  batch 110 loss: 0.2587850363417105
  batch 111 loss: 0.25872898947548223
  batch 112 loss: 0.2586687706144793
  batch 113 loss: 0.2588918489956223
  batch 114 loss: 0.2592944867516819
  batch 115 loss: 0.259218484033709
  batch 116 loss: 0.2592476797771865
  batch 117 loss: 0.25935597284736794
  batch 118 loss: 0.2591522656759973
  batch 119 loss: 0.25902666226655496
  batch 120 loss: 0.2588788838436206
  batch 121 loss: 0.25862624810254276
  batch 122 loss: 0.25833625639559793
  batch 123 loss: 0.258229389544425
  batch 124 loss: 0.25829893170345214
  batch 125 loss: 0.2582458990812302
  batch 126 loss: 0.2583927298112521
  batch 127 loss: 0.25886175634823444
  batch 128 loss: 0.2586634261533618
  batch 129 loss: 0.25900032931520034
  batch 130 loss: 0.2589447585436014
  batch 131 loss: 0.25901557493755833
  batch 132 loss: 0.25892636225079046
  batch 133 loss: 0.25920108789788154
  batch 134 loss: 0.2594118903377163
  batch 135 loss: 0.25932690688857324
  batch 136 loss: 0.2593007790034308
  batch 137 loss: 0.25939275737661516
  batch 138 loss: 0.2595931057264839
  batch 139 loss: 0.2598925379754828
  batch 140 loss: 0.25983778121215956
  batch 141 loss: 0.25984095332893076
  batch 142 loss: 0.2598495196918367
  batch 143 loss: 0.2595640424039814
  batch 144 loss: 0.2594055604810516
  batch 145 loss: 0.2591938163699775
  batch 146 loss: 0.2590910576998371
  batch 147 loss: 0.2593620519857017
  batch 148 loss: 0.2591248076107051
  batch 149 loss: 0.2588828028048445
  batch 150 loss: 0.25884622037410737
  batch 151 loss: 0.2587671358853776
  batch 152 loss: 0.25876924532808754
  batch 153 loss: 0.2586250547684875
  batch 154 loss: 0.258833508696649
  batch 155 loss: 0.25878540575504305
  batch 156 loss: 0.25892960901061696
  batch 157 loss: 0.25902451536837656
  batch 158 loss: 0.2592257662098619
  batch 159 loss: 0.2596493658592116
  batch 160 loss: 0.25946026453748344
  batch 161 loss: 0.2595635829319865
  batch 162 loss: 0.2595401775137878
  batch 163 loss: 0.25974699699439885
  batch 164 loss: 0.25965848665048435
  batch 165 loss: 0.25968259925192055
  batch 166 loss: 0.2596634293356574
  batch 167 loss: 0.25952908825017734
  batch 168 loss: 0.2594049298869712
  batch 169 loss: 0.25926821171884706
  batch 170 loss: 0.2591471942032085
  batch 171 loss: 0.2590855032379864
  batch 172 loss: 0.258887046644854
  batch 173 loss: 0.2588950814194762
  batch 174 loss: 0.2589634446234539
  batch 175 loss: 0.2591353275094713
  batch 176 loss: 0.25904517874798993
  batch 177 loss: 0.2590515605137173
  batch 178 loss: 0.25919981150144944
  batch 179 loss: 0.25934609254645236
  batch 180 loss: 0.25934025529358123
  batch 181 loss: 0.25922628158693156
  batch 182 loss: 0.25936403502147276
  batch 183 loss: 0.2591404931141379
  batch 184 loss: 0.2589781770239706
  batch 185 loss: 0.25884293011716897
  batch 186 loss: 0.2589368502939901
  batch 187 loss: 0.25897454450474705
  batch 188 loss: 0.2585538368117302
  batch 189 loss: 0.25836690534990303
  batch 190 loss: 0.2581919176013846
  batch 191 loss: 0.25802262650110336
  batch 192 loss: 0.25814166587466997
  batch 193 loss: 0.25822653779711746
  batch 194 loss: 0.2582691612931871
  batch 195 loss: 0.2583704661100339
  batch 196 loss: 0.2582540411730202
  batch 197 loss: 0.25808598554981543
  batch 198 loss: 0.25819747067160076
  batch 199 loss: 0.2583794850650145
  batch 200 loss: 0.2585873497277498
  batch 201 loss: 0.25862769915986417
  batch 202 loss: 0.2585560205106688
  batch 203 loss: 0.2586256496865174
  batch 204 loss: 0.2584587552529924
  batch 205 loss: 0.25865098376099654
  batch 206 loss: 0.2586264732537918
  batch 207 loss: 0.25861126162867615
  batch 208 loss: 0.258460839278996
  batch 209 loss: 0.2583604641080473
  batch 210 loss: 0.2584383746697789
  batch 211 loss: 0.25836068165810755
  batch 212 loss: 0.25829154927775544
  batch 213 loss: 0.25836601839378964
  batch 214 loss: 0.25842505874477817
  batch 215 loss: 0.2582155985194583
  batch 216 loss: 0.25806261543874387
  batch 217 loss: 0.25804744956130804
  batch 218 loss: 0.2579880928227661
  batch 219 loss: 0.2579569145696893
  batch 220 loss: 0.2578381925143979
  batch 221 loss: 0.2579262157505993
  batch 222 loss: 0.25794971318126797
  batch 223 loss: 0.25796090467361055
  batch 224 loss: 0.257927273599697
  batch 225 loss: 0.2577316333850225
  batch 226 loss: 0.2578859775599125
  batch 227 loss: 0.2575543800508398
  batch 228 loss: 0.2574522741661783
  batch 229 loss: 0.2572929356816554
  batch 230 loss: 0.2571919212522714
  batch 231 loss: 0.25715074375336305
  batch 232 loss: 0.25687419417603263
  batch 233 loss: 0.2568184183390867
  batch 234 loss: 0.25682286905427265
  batch 235 loss: 0.25683214905414176
  batch 236 loss: 0.2567302314406734
  batch 237 loss: 0.2566730236076604
  batch 238 loss: 0.256555697935469
  batch 239 loss: 0.25643233287783346
  batch 240 loss: 0.2564404225597779
  batch 241 loss: 0.2564780902565762
  batch 242 loss: 0.25627548746333634
  batch 243 loss: 0.25631409748591516
  batch 244 loss: 0.2563034802919529
  batch 245 loss: 0.25628232724812566
  batch 246 loss: 0.25629865081329656
  batch 247 loss: 0.25634032680920743
  batch 248 loss: 0.25637994430238203
  batch 249 loss: 0.2563467662497218
  batch 250 loss: 0.25627486950159073
  batch 251 loss: 0.2561848072179285
  batch 252 loss: 0.25601604900189806
  batch 253 loss: 0.25588422813434375
  batch 254 loss: 0.25576888611466864
  batch 255 loss: 0.2557528135823269
  batch 256 loss: 0.2557206302881241
  batch 257 loss: 0.25575810636063956
  batch 258 loss: 0.25581035306749417
  batch 259 loss: 0.2558286270579776
  batch 260 loss: 0.2558948474434706
  batch 261 loss: 0.2560013437864881
  batch 262 loss: 0.2560833301252991
  batch 263 loss: 0.25620524137645617
  batch 264 loss: 0.2561461834632086
  batch 265 loss: 0.2561633996806055
  batch 266 loss: 0.25614005960243985
  batch 267 loss: 0.2562345798207579
  batch 268 loss: 0.25616223226065066
  batch 269 loss: 0.25614809219943546
  batch 270 loss: 0.2564852927018095
  batch 271 loss: 0.256501505240743
  batch 272 loss: 0.25661127187092514
  batch 273 loss: 0.25659897197515535
  batch 274 loss: 0.25675298790209483
  batch 275 loss: 0.2568053509430452
  batch 276 loss: 0.25684062234949373
  batch 277 loss: 0.25689794327593024
  batch 278 loss: 0.2569806897918955
  batch 279 loss: 0.25719522698164843
  batch 280 loss: 0.2571214135204043
  batch 281 loss: 0.25701236783185466
  batch 282 loss: 0.25703298279368286
  batch 283 loss: 0.2569857189175097
  batch 284 loss: 0.25700721742821414
  batch 285 loss: 0.2570119978043071
  batch 286 loss: 0.2570211271514426
  batch 287 loss: 0.25697879524388795
  batch 288 loss: 0.256834427981327
  batch 289 loss: 0.2569927386037206
  batch 290 loss: 0.2568252962211083
  batch 291 loss: 0.2567905777508451
  batch 292 loss: 0.25685188643736384
  batch 293 loss: 0.25692462432913404
  batch 294 loss: 0.25684300313393277
  batch 295 loss: 0.25686034282385295
  batch 296 loss: 0.25693665016945955
  batch 297 loss: 0.2569141379410169
  batch 298 loss: 0.25692809363139557
  batch 299 loss: 0.25693162174328515
  batch 300 loss: 0.2569570174316565
  batch 301 loss: 0.2568587264250283
  batch 302 loss: 0.25688121457960433
  batch 303 loss: 0.2568276534379512
  batch 304 loss: 0.25677474107789366
  batch 305 loss: 0.2566345388283495
  batch 306 loss: 0.25664109103625116
  batch 307 loss: 0.2565802406119213
  batch 308 loss: 0.2566716372482962
  batch 309 loss: 0.25667969631724374
  batch 310 loss: 0.256594729615796
  batch 311 loss: 0.25670254422154076
  batch 312 loss: 0.25681467316089535
  batch 313 loss: 0.25683244758139784
  batch 314 loss: 0.2568860692772896
  batch 315 loss: 0.2568922426019396
  batch 316 loss: 0.2568189164128485
  batch 317 loss: 0.2569036522310239
  batch 318 loss: 0.25692741957100684
  batch 319 loss: 0.25690498994809335
  batch 320 loss: 0.25684286234900355
  batch 321 loss: 0.2567885067325515
  batch 322 loss: 0.25673165843353507
  batch 323 loss: 0.25661274841510845
  batch 324 loss: 0.25641409942397364
  batch 325 loss: 0.2564055306177873
  batch 326 loss: 0.2563491981545109
  batch 327 loss: 0.256310578560975
  batch 328 loss: 0.25617556746413067
  batch 329 loss: 0.25626786374755905
  batch 330 loss: 0.25613862280592775
  batch 331 loss: 0.2561135913310094
  batch 332 loss: 0.2560605740241976
  batch 333 loss: 0.25602057911791243
  batch 334 loss: 0.25593196332990054
  batch 335 loss: 0.2557604556208226
  batch 336 loss: 0.2556635957832138
  batch 337 loss: 0.2555340351177606
  batch 338 loss: 0.2553800240864415
  batch 339 loss: 0.25514928974584844
  batch 340 loss: 0.2550329850877033
  batch 341 loss: 0.25499421030370373
  batch 342 loss: 0.2548601858709988
  batch 343 loss: 0.2547971810005149
  batch 344 loss: 0.2547398850668308
  batch 345 loss: 0.2548064840876538
  batch 346 loss: 0.25459566746833007
  batch 347 loss: 0.25465345082090635
  batch 348 loss: 0.2547578315796523
  batch 349 loss: 0.2547349279538267
  batch 350 loss: 0.2548736463700022
  batch 351 loss: 0.2549999411034788
  batch 352 loss: 0.2552136652256278
  batch 353 loss: 0.25526297966245215
  batch 354 loss: 0.25552564905693304
  batch 355 loss: 0.25564126175054364
  batch 356 loss: 0.25566333369090316
  batch 357 loss: 0.25557212957314085
  batch 358 loss: 0.25558219712706254
  batch 359 loss: 0.2555714597027946
  batch 360 loss: 0.25555868616534605
  batch 361 loss: 0.25542980564598233
  batch 362 loss: 0.25546477883230917
  batch 363 loss: 0.2553660461561082
  batch 364 loss: 0.25531375056112204
  batch 365 loss: 0.2553932251995557
  batch 366 loss: 0.25530961302460214
  batch 367 loss: 0.2552178690478977
  batch 368 loss: 0.25513765511467407
  batch 369 loss: 0.25507213377209537
  batch 370 loss: 0.2551184409776249
  batch 371 loss: 0.25509813255216235
  batch 372 loss: 0.2550719814595356
  batch 373 loss: 0.25504182470867526
  batch 374 loss: 0.25487044425731037
  batch 375 loss: 0.254798058907191
  batch 376 loss: 0.25480807382375636
  batch 377 loss: 0.2547861585092165
  batch 378 loss: 0.25468999123762526
  batch 379 loss: 0.2547273021731968
  batch 380 loss: 0.25477241991381894
  batch 381 loss: 0.2547507528557865
  batch 382 loss: 0.25466722386513707
  batch 383 loss: 0.2546519982985977
  batch 384 loss: 0.2546280953877916
  batch 385 loss: 0.25472973902504165
  batch 386 loss: 0.25466504233643183
  batch 387 loss: 0.2546972161500645
  batch 388 loss: 0.2547318040172464
  batch 389 loss: 0.2547293990697223
  batch 390 loss: 0.254645897486271
  batch 391 loss: 0.25471249954474856
  batch 392 loss: 0.25471052831533003
  batch 393 loss: 0.2547247459870258
  batch 394 loss: 0.254786500395252
  batch 395 loss: 0.2546838914669013
  batch 396 loss: 0.2545923836378738
  batch 397 loss: 0.2545698572991777
  batch 398 loss: 0.2545056587188088
  batch 399 loss: 0.25447236341342594
  batch 400 loss: 0.2545614847540855
  batch 401 loss: 0.2544288881253126
  batch 402 loss: 0.25442859997500233
  batch 403 loss: 0.2544462151326258
  batch 404 loss: 0.2545011261164552
  batch 405 loss: 0.25451271857744384
  batch 406 loss: 0.25456411461231154
  batch 407 loss: 0.2545684784724027
  batch 408 loss: 0.25461406319164764
  batch 409 loss: 0.25454874387725934
  batch 410 loss: 0.25461140954639855
  batch 411 loss: 0.25455183283836014
  batch 412 loss: 0.25448525928467225
  batch 413 loss: 0.25455891631417354
  batch 414 loss: 0.25455296025184043
  batch 415 loss: 0.2545202233705176
  batch 416 loss: 0.25451604706736713
  batch 417 loss: 0.2544722927988862
  batch 418 loss: 0.25444783776570734
  batch 419 loss: 0.25449347126170957
  batch 420 loss: 0.25445185796845526
  batch 421 loss: 0.2543270823295779
  batch 422 loss: 0.25443313786345073
  batch 423 loss: 0.25447027138643513
  batch 424 loss: 0.2544262785605102
  batch 425 loss: 0.25439557285869824
  batch 426 loss: 0.25433734807889785
  batch 427 loss: 0.254360950314189
  batch 428 loss: 0.25437244745058435
  batch 429 loss: 0.2544214825530152
  batch 430 loss: 0.2543862613827683
  batch 431 loss: 0.25445480191790587
  batch 432 loss: 0.25449057916800183
  batch 433 loss: 0.2544769121303845
  batch 434 loss: 0.25458136582017493
  batch 435 loss: 0.2544801811719763
  batch 436 loss: 0.25449303575201865
  batch 437 loss: 0.2546071914499903
  batch 438 loss: 0.25481212618824556
  batch 439 loss: 0.2548102370902455
  batch 440 loss: 0.25481798340651124
  batch 441 loss: 0.25476523737112683
  batch 442 loss: 0.2547436296872424
  batch 443 loss: 0.2547024308491507
  batch 444 loss: 0.25464807125227945
  batch 445 loss: 0.2546663142657012
  batch 446 loss: 0.2546375955166838
  batch 447 loss: 0.25458026915721976
  batch 448 loss: 0.25465071597136557
  batch 449 loss: 0.2546458538032587
  batch 450 loss: 0.2545935386750433
  batch 451 loss: 0.2545187793457323
  batch 452 loss: 0.25452603963492193
  batch 453 loss: 0.25452799226669287
  batch 454 loss: 0.2545030129835469
  batch 455 loss: 0.2544795574722709
  batch 456 loss: 0.2545014266905032
  batch 457 loss: 0.25448196767717274
  batch 458 loss: 0.25446091794030634
  batch 459 loss: 0.2544573924640165
  batch 460 loss: 0.2545437663793564
  batch 461 loss: 0.25450052708839904
  batch 462 loss: 0.2545294120133697
  batch 463 loss: 0.2544551959691758
  batch 464 loss: 0.2544727478947105
  batch 465 loss: 0.254358894401981
  batch 466 loss: 0.25430852280282157
  batch 467 loss: 0.25439300434364753
  batch 468 loss: 0.2543748668434783
  batch 469 loss: 0.25445398132302866
  batch 470 loss: 0.2544723585247993
  batch 471 loss: 0.25443861859745787
  batch 472 loss: 0.2542986311018467
LOSS train 0.2542986311018467 valid 0.22918957471847534
LOSS train 0.2542986311018467 valid 0.21004615724086761
LOSS train 0.2542986311018467 valid 0.21282334625720978
LOSS train 0.2542986311018467 valid 0.20129093155264854
LOSS train 0.2542986311018467 valid 0.1951959639787674
LOSS train 0.2542986311018467 valid 0.1947515830397606
LOSS train 0.2542986311018467 valid 0.20909308748585836
LOSS train 0.2542986311018467 valid 0.20498425140976906
LOSS train 0.2542986311018467 valid 0.20444885889689127
LOSS train 0.2542986311018467 valid 0.2078225165605545
LOSS train 0.2542986311018467 valid 0.20643187111074274
LOSS train 0.2542986311018467 valid 0.20504396905501684
LOSS train 0.2542986311018467 valid 0.20202171458647802
LOSS train 0.2542986311018467 valid 0.2036788005914007
LOSS train 0.2542986311018467 valid 0.19954436620076496
LOSS train 0.2542986311018467 valid 0.20216661226004362
LOSS train 0.2542986311018467 valid 0.20318893124075496
LOSS train 0.2542986311018467 valid 0.20249902456998825
LOSS train 0.2542986311018467 valid 0.20479306657063334
LOSS train 0.2542986311018467 valid 0.205062285810709
LOSS train 0.2542986311018467 valid 0.2059879835162844
LOSS train 0.2542986311018467 valid 0.2048378742553971
LOSS train 0.2542986311018467 valid 0.20628850032453952
LOSS train 0.2542986311018467 valid 0.2056887118766705
LOSS train 0.2542986311018467 valid 0.20382721483707428
LOSS train 0.2542986311018467 valid 0.2038233715754289
LOSS train 0.2542986311018467 valid 0.20422546179206283
LOSS train 0.2542986311018467 valid 0.2051741714988436
LOSS train 0.2542986311018467 valid 0.20556287210563134
LOSS train 0.2542986311018467 valid 0.2059381032983462
LOSS train 0.2542986311018467 valid 0.20713059796440986
LOSS train 0.2542986311018467 valid 0.2072741501033306
LOSS train 0.2542986311018467 valid 0.2085324056220777
LOSS train 0.2542986311018467 valid 0.2080017626285553
LOSS train 0.2542986311018467 valid 0.21037659304482595
LOSS train 0.2542986311018467 valid 0.21007807966735628
LOSS train 0.2542986311018467 valid 0.21059316114799395
LOSS train 0.2542986311018467 valid 0.21118673956707903
LOSS train 0.2542986311018467 valid 0.2107621519229351
LOSS train 0.2542986311018467 valid 0.2112589679658413
LOSS train 0.2542986311018467 valid 0.2108376015250276
LOSS train 0.2542986311018467 valid 0.2112022741209893
LOSS train 0.2542986311018467 valid 0.21037522824697716
LOSS train 0.2542986311018467 valid 0.2102729661220854
LOSS train 0.2542986311018467 valid 0.2100050022204717
LOSS train 0.2542986311018467 valid 0.2107458027160686
LOSS train 0.2542986311018467 valid 0.2110078861104681
LOSS train 0.2542986311018467 valid 0.21078068700929484
LOSS train 0.2542986311018467 valid 0.21086609606840173
LOSS train 0.2542986311018467 valid 0.21027003824710847
LOSS train 0.2542986311018467 valid 0.21094602755471772
LOSS train 0.2542986311018467 valid 0.21068940225702065
LOSS train 0.2542986311018467 valid 0.21085226985643496
LOSS train 0.2542986311018467 valid 0.21166837767318444
LOSS train 0.2542986311018467 valid 0.21171346740289168
LOSS train 0.2542986311018467 valid 0.21168822502451284
LOSS train 0.2542986311018467 valid 0.2118812966765019
LOSS train 0.2542986311018467 valid 0.21159967446121677
LOSS train 0.2542986311018467 valid 0.2123612142720465
LOSS train 0.2542986311018467 valid 0.21191385512550673
LOSS train 0.2542986311018467 valid 0.21170114640329704
LOSS train 0.2542986311018467 valid 0.21244866521127762
LOSS train 0.2542986311018467 valid 0.21284224873497373
LOSS train 0.2542986311018467 valid 0.21377164078876376
LOSS train 0.2542986311018467 valid 0.21394436634503877
LOSS train 0.2542986311018467 valid 0.21392204996311304
LOSS train 0.2542986311018467 valid 0.21323720688250528
LOSS train 0.2542986311018467 valid 0.21392091977245667
LOSS train 0.2542986311018467 valid 0.2131248526815055
LOSS train 0.2542986311018467 valid 0.21341231784650258
LOSS train 0.2542986311018467 valid 0.21292717230152075
LOSS train 0.2542986311018467 valid 0.21319095169504484
LOSS train 0.2542986311018467 valid 0.21334595284233354
LOSS train 0.2542986311018467 valid 0.21300806870331634
LOSS train 0.2542986311018467 valid 0.2127576074997584
LOSS train 0.2542986311018467 valid 0.21338564530014992
LOSS train 0.2542986311018467 valid 0.21292539908514393
LOSS train 0.2542986311018467 valid 0.2127311254541079
LOSS train 0.2542986311018467 valid 0.21289953155608116
LOSS train 0.2542986311018467 valid 0.21218586768954992
LOSS train 0.2542986311018467 valid 0.21118689392819817
LOSS train 0.2542986311018467 valid 0.21131395839336442
LOSS train 0.2542986311018467 valid 0.21110141582517739
LOSS train 0.2542986311018467 valid 0.2110191425752072
LOSS train 0.2542986311018467 valid 0.21032301397884592
LOSS train 0.2542986311018467 valid 0.20965107634317043
LOSS train 0.2542986311018467 valid 0.20971948439362406
LOSS train 0.2542986311018467 valid 0.2090750809081576
LOSS train 0.2542986311018467 valid 0.20911303129089012
LOSS train 0.2542986311018467 valid 0.2092694933215777
LOSS train 0.2542986311018467 valid 0.20921572829995835
LOSS train 0.2542986311018467 valid 0.20935383309488712
LOSS train 0.2542986311018467 valid 0.20912483110222765
LOSS train 0.2542986311018467 valid 0.20944353129635465
LOSS train 0.2542986311018467 valid 0.20900669474350778
LOSS train 0.2542986311018467 valid 0.20893283964445195
LOSS train 0.2542986311018467 valid 0.20927933165707538
LOSS train 0.2542986311018467 valid 0.20964364677059408
LOSS train 0.2542986311018467 valid 0.2094858285754618
LOSS train 0.2542986311018467 valid 0.2096926310658455
LOSS train 0.2542986311018467 valid 0.21020504537195261
LOSS train 0.2542986311018467 valid 0.2104703852651166
LOSS train 0.2542986311018467 valid 0.21006151426185682
LOSS train 0.2542986311018467 valid 0.20976438444967455
LOSS train 0.2542986311018467 valid 0.2101490788516544
LOSS train 0.2542986311018467 valid 0.21041518660648814
LOSS train 0.2542986311018467 valid 0.21002549712903032
LOSS train 0.2542986311018467 valid 0.20991056909163794
LOSS train 0.2542986311018467 valid 0.2104493784248282
LOSS train 0.2542986311018467 valid 0.21058145477013154
LOSS train 0.2542986311018467 valid 0.2101172226237821
LOSS train 0.2542986311018467 valid 0.2100370651377099
LOSS train 0.2542986311018467 valid 0.21015562473145205
LOSS train 0.2542986311018467 valid 0.2100514921179989
LOSS train 0.2542986311018467 valid 0.21032372570556143
LOSS train 0.2542986311018467 valid 0.21085193637630034
LOSS train 0.2542986311018467 valid 0.21110135762609988
LOSS train 0.2542986311018467 valid 0.21109263485265992
LOSS train 0.2542986311018467 valid 0.2109107794631429
LOSS train 0.2542986311018467 valid 0.2107584840307633
LOSS train 0.2542986311018467 valid 0.21071697439043974
LOSS train 0.2542986311018467 valid 0.21072164202322724
LOSS train 0.2542986311018467 valid 0.21100053324447415
LOSS train 0.2542986311018467 valid 0.21110395318077457
LOSS train 0.2542986311018467 valid 0.21117837822437285
LOSS train 0.2542986311018467 valid 0.21128890947217033
LOSS train 0.2542986311018467 valid 0.21121415976933608
LOSS train 0.2542986311018467 valid 0.21145813970360905
LOSS train 0.2542986311018467 valid 0.2117174964311511
LOSS train 0.2542986311018467 valid 0.2115708427933546
LOSS train 0.2542986311018467 valid 0.21140329164403085
LOSS train 0.2542986311018467 valid 0.21113212045395013
LOSS train 0.2542986311018467 valid 0.21089747671346018
LOSS train 0.2542986311018467 valid 0.211066362461937
LOSS train 0.2542986311018467 valid 0.2110960859943319
LOSS train 0.2542986311018467 valid 0.210716400295496
LOSS train 0.2542986311018467 valid 0.21057596846218526
LOSS train 0.2542986311018467 valid 0.21044686792985254
LOSS train 0.2542986311018467 valid 0.2103622489267116
LOSS train 0.2542986311018467 valid 0.21037063747644424
LOSS train 0.2542986311018467 valid 0.21054300718696406
LOSS train 0.2542986311018467 valid 0.21057776036396833
LOSS train 0.2542986311018467 valid 0.2105742830496568
LOSS train 0.2542986311018467 valid 0.2107747747666306
LOSS train 0.2542986311018467 valid 0.210643067339371
LOSS train 0.2542986311018467 valid 0.21069325527099714
LOSS train 0.2542986311018467 valid 0.21053736104446202
LOSS train 0.2542986311018467 valid 0.2112825180630426
LOSS train 0.2542986311018467 valid 0.21137318755156242
LOSS train 0.2542986311018467 valid 0.21127462367216746
LOSS train 0.2542986311018467 valid 0.21136688406499016
LOSS train 0.2542986311018467 valid 0.21108697148922242
LOSS train 0.2542986311018467 valid 0.2111969899897482
LOSS train 0.2542986311018467 valid 0.21127993981172513
LOSS train 0.2542986311018467 valid 0.21122487927636793
LOSS train 0.2542986311018467 valid 0.21149360207028878
LOSS train 0.2542986311018467 valid 0.21136582002138635
LOSS train 0.2542986311018467 valid 0.21143947362522536
LOSS train 0.2542986311018467 valid 0.21141175419654487
LOSS train 0.2542986311018467 valid 0.211145977396518
LOSS train 0.2542986311018467 valid 0.21106530679679064
LOSS train 0.2542986311018467 valid 0.2108476229103995
LOSS train 0.2542986311018467 valid 0.21077970794739168
LOSS train 0.2542986311018467 valid 0.21059152202271833
LOSS train 0.2542986311018467 valid 0.2105243441733447
LOSS train 0.2542986311018467 valid 0.21051653600241765
LOSS train 0.2542986311018467 valid 0.2107879657766776
LOSS train 0.2542986311018467 valid 0.21057484059461526
LOSS train 0.2542986311018467 valid 0.21070263442203138
LOSS train 0.2542986311018467 valid 0.21087976694107055
LOSS train 0.2542986311018467 valid 0.2108814169963201
LOSS train 0.2542986311018467 valid 0.21070142861369046
LOSS train 0.2542986311018467 valid 0.21060791103481558
LOSS train 0.2542986311018467 valid 0.2107608838156722
LOSS train 0.2542986311018467 valid 0.2107142505475453
LOSS train 0.2542986311018467 valid 0.2106837887655605
LOSS train 0.2542986311018467 valid 0.2105501398528363
LOSS train 0.2542986311018467 valid 0.21067141281085067
LOSS train 0.2542986311018467 valid 0.21053420165397602
LOSS train 0.2542986311018467 valid 0.2106071802477042
LOSS train 0.2542986311018467 valid 0.21062135852832162
LOSS train 0.2542986311018467 valid 0.21079402269570383
LOSS train 0.2542986311018467 valid 0.21075164099208643
LOSS train 0.2542986311018467 valid 0.21097565620489742
LOSS train 0.2542986311018467 valid 0.2107770869860778
LOSS train 0.2542986311018467 valid 0.21074603225595206
LOSS train 0.2542986311018467 valid 0.21058785246017783
LOSS train 0.2542986311018467 valid 0.21061018600742867
LOSS train 0.2542986311018467 valid 0.21056384636611536
LOSS train 0.2542986311018467 valid 0.2107975981737438
LOSS train 0.2542986311018467 valid 0.21069814108741222
LOSS train 0.2542986311018467 valid 0.21068717849751314
LOSS train 0.2542986311018467 valid 0.21063498550437276
LOSS train 0.2542986311018467 valid 0.2104634692527584
LOSS train 0.2542986311018467 valid 0.21012811462084452
LOSS train 0.2542986311018467 valid 0.2101470197798038
LOSS train 0.2542986311018467 valid 0.2102174965409458
LOSS train 0.2542986311018467 valid 0.21002199816884418
LOSS train 0.2542986311018467 valid 0.21005029684335144
LOSS train 0.2542986311018467 valid 0.2099268715083599
LOSS train 0.2542986311018467 valid 0.20979736604500765
LOSS train 0.2542986311018467 valid 0.20974388762865917
LOSS train 0.2542986311018467 valid 0.20959123066199825
LOSS train 0.2542986311018467 valid 0.2096138382513149
LOSS train 0.2542986311018467 valid 0.2094265365745963
LOSS train 0.2542986311018467 valid 0.2093640933337721
LOSS train 0.2542986311018467 valid 0.20929232599654635
LOSS train 0.2542986311018467 valid 0.20923393169561258
LOSS train 0.2542986311018467 valid 0.20912353623141514
LOSS train 0.2542986311018467 valid 0.20920113034191587
LOSS train 0.2542986311018467 valid 0.2093472254078535
LOSS train 0.2542986311018467 valid 0.20926802485900106
LOSS train 0.2542986311018467 valid 0.20940240734899548
LOSS train 0.2542986311018467 valid 0.20930635838586592
LOSS train 0.2542986311018467 valid 0.2092029542423958
LOSS train 0.2542986311018467 valid 0.2091090506011689
LOSS train 0.2542986311018467 valid 0.20898884953716385
LOSS train 0.2542986311018467 valid 0.20904347984068986
LOSS train 0.2542986311018467 valid 0.2092107767792053
LOSS train 0.2542986311018467 valid 0.20936273302544248
LOSS train 0.2542986311018467 valid 0.20938847224097445
LOSS train 0.2542986311018467 valid 0.20933070456659472
LOSS train 0.2542986311018467 valid 0.20936825958335345
LOSS train 0.2542986311018467 valid 0.2094600309085633
LOSS train 0.2542986311018467 valid 0.20944983541965484
LOSS train 0.2542986311018467 valid 0.2096150576145248
LOSS train 0.2542986311018467 valid 0.20978749553275003
LOSS train 0.2542986311018467 valid 0.20986197914993554
LOSS train 0.2542986311018467 valid 0.20990968817706712
LOSS train 0.2542986311018467 valid 0.2100607410721157
LOSS train 0.2542986311018467 valid 0.2100672356107018
LOSS train 0.2542986311018467 valid 0.21015386925689106
LOSS train 0.2542986311018467 valid 0.21013032462719683
LOSS train 0.2542986311018467 valid 0.21014346927404404
LOSS train 0.2542986311018467 valid 0.2102266860135058
LOSS train 0.2542986311018467 valid 0.20999044085205612
LOSS train 0.2542986311018467 valid 0.21015540890804324
LOSS train 0.2542986311018467 valid 0.21001214671786092
LOSS train 0.2542986311018467 valid 0.20998092413447392
LOSS train 0.2542986311018467 valid 0.20993113021055856
LOSS train 0.2542986311018467 valid 0.21001807207635825
LOSS train 0.2542986311018467 valid 0.20986571144466556
LOSS train 0.2542986311018467 valid 0.2099722607881444
LOSS train 0.2542986311018467 valid 0.21008058332028937
LOSS train 0.2542986311018467 valid 0.21026774839479095
LOSS train 0.2542986311018467 valid 0.21021817236896453
LOSS train 0.2542986311018467 valid 0.21017163261952188
LOSS train 0.2542986311018467 valid 0.21013643838945897
LOSS train 0.2542986311018467 valid 0.21004680978007106
LOSS train 0.2542986311018467 valid 0.21022716349363327
LOSS train 0.2542986311018467 valid 0.2102436693422348
LOSS train 0.2542986311018467 valid 0.21031465824870837
LOSS train 0.2542986311018467 valid 0.21022877851022562
LOSS train 0.2542986311018467 valid 0.2102194554575785
LOSS train 0.2542986311018467 valid 0.2102685366191116
LOSS train 0.2542986311018467 valid 0.21024817938450724
LOSS train 0.2542986311018467 valid 0.21018045647830816
LOSS train 0.2542986311018467 valid 0.2102333366177803
LOSS train 0.2542986311018467 valid 0.21020582479399605
LOSS train 0.2542986311018467 valid 0.2102002230974344
LOSS train 0.2542986311018467 valid 0.2103839552037104
LOSS train 0.2542986311018467 valid 0.21039335259044442
LOSS train 0.2542986311018467 valid 0.21047384832975075
LOSS train 0.2542986311018467 valid 0.21047226909660932
LOSS train 0.2542986311018467 valid 0.21044397365372136
LOSS train 0.2542986311018467 valid 0.21043431607628227
LOSS train 0.2542986311018467 valid 0.21044997614197963
LOSS train 0.2542986311018467 valid 0.21053683090565808
LOSS train 0.2542986311018467 valid 0.21076251350370925
LOSS train 0.2542986311018467 valid 0.21095820632245804
LOSS train 0.2542986311018467 valid 0.21100999662357062
LOSS train 0.2542986311018467 valid 0.2112951249103336
LOSS train 0.2542986311018467 valid 0.21138032031801593
LOSS train 0.2542986311018467 valid 0.21124706656610878
LOSS train 0.2542986311018467 valid 0.21127710190686314
LOSS train 0.2542986311018467 valid 0.21115665564286537
LOSS train 0.2542986311018467 valid 0.21105586320484587
LOSS train 0.2542986311018467 valid 0.21085379326300654
LOSS train 0.2542986311018467 valid 0.2109372006415466
LOSS train 0.2542986311018467 valid 0.21080537321312087
LOSS train 0.2542986311018467 valid 0.21075934251220202
LOSS train 0.2542986311018467 valid 0.21066327704816845
LOSS train 0.2542986311018467 valid 0.21065235369618285
LOSS train 0.2542986311018467 valid 0.21062006507541092
LOSS train 0.2542986311018467 valid 0.21064305315937912
LOSS train 0.2542986311018467 valid 0.21059801528503844
LOSS train 0.2542986311018467 valid 0.21042727229902553
LOSS train 0.2542986311018467 valid 0.2104421031868292
LOSS train 0.2542986311018467 valid 0.21048574701312503
LOSS train 0.2542986311018467 valid 0.21055385593710274
LOSS train 0.2542986311018467 valid 0.21046999649903209
LOSS train 0.2542986311018467 valid 0.21053883810974147
LOSS train 0.2542986311018467 valid 0.21051803678782727
LOSS train 0.2542986311018467 valid 0.21075756097731946
LOSS train 0.2542986311018467 valid 0.21087134773448363
LOSS train 0.2542986311018467 valid 0.21078190569942062
LOSS train 0.2542986311018467 valid 0.21076807143912973
LOSS train 0.2542986311018467 valid 0.21070599605973134
LOSS train 0.2542986311018467 valid 0.21072794736229059
LOSS train 0.2542986311018467 valid 0.21062105725208918
LOSS train 0.2542986311018467 valid 0.21075108097835238
LOSS train 0.2542986311018467 valid 0.21079448679622437
LOSS train 0.2542986311018467 valid 0.21084650991970164
LOSS train 0.2542986311018467 valid 0.21091375191156803
LOSS train 0.2542986311018467 valid 0.21084622814029944
LOSS train 0.2542986311018467 valid 0.2108129960259581
LOSS train 0.2542986311018467 valid 0.21080685462741977
LOSS train 0.2542986311018467 valid 0.21079665707883896
LOSS train 0.2542986311018467 valid 0.210866716832019
LOSS train 0.2542986311018467 valid 0.21075405079510903
LOSS train 0.2542986311018467 valid 0.21061581210308136
LOSS train 0.2542986311018467 valid 0.21072704727068925
LOSS train 0.2542986311018467 valid 0.21068459349318433
LOSS train 0.2542986311018467 valid 0.21080431346870532
LOSS train 0.2542986311018467 valid 0.21078421052486176
LOSS train 0.2542986311018467 valid 0.21077763935244537
LOSS train 0.2542986311018467 valid 0.21083569155318505
LOSS train 0.2542986311018467 valid 0.2109634484978592
LOSS train 0.2542986311018467 valid 0.21101490602037376
LOSS train 0.2542986311018467 valid 0.21091596242040395
LOSS train 0.2542986311018467 valid 0.21099380763520334
LOSS train 0.2542986311018467 valid 0.2109452437910234
LOSS train 0.2542986311018467 valid 0.2109107837403891
LOSS train 0.2542986311018467 valid 0.2108934196976968
LOSS train 0.2542986311018467 valid 0.2109231367478004
LOSS train 0.2542986311018467 valid 0.21100949975976183
LOSS train 0.2542986311018467 valid 0.21112604826597628
LOSS train 0.2542986311018467 valid 0.2110688006732522
LOSS train 0.2542986311018467 valid 0.21128229330375928
LOSS train 0.2542986311018467 valid 0.21120576578559297
LOSS train 0.2542986311018467 valid 0.21111900460684047
LOSS train 0.2542986311018467 valid 0.21113018229244704
LOSS train 0.2542986311018467 valid 0.2110953430334727
LOSS train 0.2542986311018467 valid 0.21121965000729362
LOSS train 0.2542986311018467 valid 0.2113184647328818
LOSS train 0.2542986311018467 valid 0.21130567008540743
LOSS train 0.2542986311018467 valid 0.2113159848957458
LOSS train 0.2542986311018467 valid 0.21131717263770528
LOSS train 0.2542986311018467 valid 0.21122843224917892
LOSS train 0.2542986311018467 valid 0.21122412948923952
LOSS train 0.2542986311018467 valid 0.211227293937437
LOSS train 0.2542986311018467 valid 0.2111440762790323
LOSS train 0.2542986311018467 valid 0.2110751353790739
LOSS train 0.2542986311018467 valid 0.21130899088673813
LOSS train 0.2542986311018467 valid 0.21131207165510757
LOSS train 0.2542986311018467 valid 0.211293717943175
LOSS train 0.2542986311018467 valid 0.2111767740002283
LOSS train 0.2542986311018467 valid 0.21106330506589221
LOSS train 0.2542986311018467 valid 0.21110950350590627
LOSS train 0.2542986311018467 valid 0.21102845507008688
LOSS train 0.2542986311018467 valid 0.21090621915128496
LOSS train 0.2542986311018467 valid 0.21089665714482014
LOSS train 0.2542986311018467 valid 0.21097097915225257
LOSS train 0.2542986311018467 valid 0.21093457878309455
LOSS train 0.2542986311018467 valid 0.2111596450839244
LOSS train 0.2542986311018467 valid 0.21116183589348633
LOSS train 0.2542986311018467 valid 0.21110696368524673
LOSS train 0.2542986311018467 valid 0.21106946651496034
LOSS train 0.2542986311018467 valid 0.2110838459643813
LOSS train 0.2542986311018467 valid 0.21099487588637406
LOSS train 0.2542986311018467 valid 0.21091669835047047
LOSS train 0.2542986311018467 valid 0.21100962322555195
LOSS train 0.2542986311018467 valid 0.2107691491848838
LOSS train 0.2542986311018467 valid 0.21076899525392187
LOSS train 0.2542986311018467 valid 0.21076620592646403
LOSS train 0.2542986311018467 valid 0.21069082377549728
LOSS train 0.2542986311018467 valid 0.21052383564473498
LOSS train 0.2542986311018467 valid 0.21049555852685287
LOSS train 0.2542986311018467 valid 0.2105455553628565
EPOCH 15:
  batch 1 loss: 0.234257310628891
  batch 2 loss: 0.21341083943843842
  batch 3 loss: 0.2218689719835917
  batch 4 loss: 0.2264275625348091
  batch 5 loss: 0.23729599118232728
  batch 6 loss: 0.2412096063296
  batch 7 loss: 0.24038170065198625
  batch 8 loss: 0.24854376167058945
  batch 9 loss: 0.24908596277236938
  batch 10 loss: 0.25013976693153384
  batch 11 loss: 0.24945722384886307
  batch 12 loss: 0.24879040569067
  batch 13 loss: 0.24748570987811455
  batch 14 loss: 0.24756005193505967
  batch 15 loss: 0.24824661215146382
  batch 16 loss: 0.2463466413319111
  batch 17 loss: 0.24464907891610088
  batch 18 loss: 0.24678509102927315
  batch 19 loss: 0.24566702936825
  batch 20 loss: 0.24406249299645424
  batch 21 loss: 0.24538656998248326
  batch 22 loss: 0.24623164331371133
  batch 23 loss: 0.24611193265603937
  batch 24 loss: 0.244186511884133
  batch 25 loss: 0.24576329946517944
  batch 26 loss: 0.24428175790951803
  batch 27 loss: 0.24430281144601326
  batch 28 loss: 0.24407134417976653
  batch 29 loss: 0.24431746375971827
  batch 30 loss: 0.2439877450466156
  batch 31 loss: 0.2451403179476338
  batch 32 loss: 0.24453328736126423
  batch 33 loss: 0.24497734145684677
  batch 34 loss: 0.2445090272847344
  batch 35 loss: 0.24529806460653034
  batch 36 loss: 0.24528277458416092
  batch 37 loss: 0.24556484375451063
  batch 38 loss: 0.24638629704713821
  batch 39 loss: 0.24636433751155168
  batch 40 loss: 0.24644614458084108
  batch 41 loss: 0.24669775730226098
  batch 42 loss: 0.2468288555031731
  batch 43 loss: 0.2475018445835557
  batch 44 loss: 0.24787299470468002
  batch 45 loss: 0.24705383016003502
  batch 46 loss: 0.24681331152501312
  batch 47 loss: 0.24755426987688592
  batch 48 loss: 0.24709669314324856
  batch 49 loss: 0.24656492958263476
  batch 50 loss: 0.24654608249664306
  batch 51 loss: 0.2464213321606318
  batch 52 loss: 0.24612045832551444
  batch 53 loss: 0.24579600989818573
  batch 54 loss: 0.24697281944530983
  batch 55 loss: 0.24743292142044415
  batch 56 loss: 0.24773200095764228
  batch 57 loss: 0.24796736789377113
  batch 58 loss: 0.2490653228656999
  batch 59 loss: 0.2495138177932319
  batch 60 loss: 0.2495044007897377
  batch 61 loss: 0.2501100878246495
  batch 62 loss: 0.25098912033342546
  batch 63 loss: 0.25108975787011406
  batch 64 loss: 0.25206977128982544
  batch 65 loss: 0.2519859728904871
  batch 66 loss: 0.25194205998471286
  batch 67 loss: 0.2523123837673842
  batch 68 loss: 0.2529561399098705
  batch 69 loss: 0.2526928369981655
  batch 70 loss: 0.2529010817408562
  batch 71 loss: 0.25246677117448457
  batch 72 loss: 0.2525714236415095
  batch 73 loss: 0.2526241289834454
  batch 74 loss: 0.25271470139960983
  batch 75 loss: 0.2525828375418981
  batch 76 loss: 0.2530608959496021
  batch 77 loss: 0.25300819378394585
  batch 78 loss: 0.2529291078830377
  batch 79 loss: 0.2536433712591099
  batch 80 loss: 0.2534352600574493
  batch 81 loss: 0.25326782207430143
  batch 82 loss: 0.25321044900068423
  batch 83 loss: 0.2530214355652591
  batch 84 loss: 0.25297166637721513
  batch 85 loss: 0.2525242877357146
  batch 86 loss: 0.2530125651595204
  batch 87 loss: 0.25303938796465425
  batch 88 loss: 0.25259728221730754
  batch 89 loss: 0.2525604988416929
  batch 90 loss: 0.2524266943335533
  batch 91 loss: 0.2527010842011525
  batch 92 loss: 0.2527535501057687
  batch 93 loss: 0.25294479968086364
  batch 94 loss: 0.25316769407784684
  batch 95 loss: 0.25253293012317857
  batch 96 loss: 0.2525586498280366
  batch 97 loss: 0.25300600233766224
  batch 98 loss: 0.2532477564349466
  batch 99 loss: 0.25331722967552417
  batch 100 loss: 0.2533934357762337
  batch 101 loss: 0.2532175327881728
  batch 102 loss: 0.2533927662115471
  batch 103 loss: 0.2536464038404446
  batch 104 loss: 0.25347291592221993
  batch 105 loss: 0.25365231434504193
  batch 106 loss: 0.2539835350131089
  batch 107 loss: 0.25353742676360586
  batch 108 loss: 0.2534658981970063
  batch 109 loss: 0.2533547850923801
  batch 110 loss: 0.25384144972671163
  batch 111 loss: 0.2541115968614011
  batch 112 loss: 0.2539057661113994
  batch 113 loss: 0.25412144663587083
  batch 114 loss: 0.25459143898466174
  batch 115 loss: 0.25461442068867063
  batch 116 loss: 0.25475018946775074
  batch 117 loss: 0.25479671091605455
  batch 118 loss: 0.254745812360513
  batch 119 loss: 0.25479604128529043
  batch 120 loss: 0.2546687955657641
  batch 121 loss: 0.25450315073994567
  batch 122 loss: 0.2541961939852746
  batch 123 loss: 0.25419571983620404
  batch 124 loss: 0.25433638944260534
  batch 125 loss: 0.2542119460105896
  batch 126 loss: 0.25433904784066336
  batch 127 loss: 0.2549255929124637
  batch 128 loss: 0.25470056757330894
  batch 129 loss: 0.25495256610619005
  batch 130 loss: 0.2548733154168496
  batch 131 loss: 0.25518587072386995
  batch 132 loss: 0.2550690792726748
  batch 133 loss: 0.2554637744910735
  batch 134 loss: 0.2557333140675701
  batch 135 loss: 0.2556844749936351
  batch 136 loss: 0.25559880573521643
  batch 137 loss: 0.2555275242911638
  batch 138 loss: 0.25580523271059646
  batch 139 loss: 0.2561538694359416
  batch 140 loss: 0.25608452100838935
  batch 141 loss: 0.2560683278538657
  batch 142 loss: 0.2560552920044308
  batch 143 loss: 0.255831309236013
  batch 144 loss: 0.2557043445607026
  batch 145 loss: 0.2555144465175168
  batch 146 loss: 0.25531620062785604
  batch 147 loss: 0.2556202893151718
  batch 148 loss: 0.2554531805217266
  batch 149 loss: 0.255249038438669
  batch 150 loss: 0.2552322419484456
  batch 151 loss: 0.25517928837150927
  batch 152 loss: 0.2551706409768054
  batch 153 loss: 0.25505498178254543
  batch 154 loss: 0.2552727411513205
  batch 155 loss: 0.2552306232913848
  batch 156 loss: 0.2553114931170757
  batch 157 loss: 0.25537834805288134
  batch 158 loss: 0.25554445027550565
  batch 159 loss: 0.25603821929895654
  batch 160 loss: 0.25585708357393744
  batch 161 loss: 0.25591189520699636
  batch 162 loss: 0.255941418034059
  batch 163 loss: 0.2561288229161245
  batch 164 loss: 0.2559181395463827
  batch 165 loss: 0.25592308803038166
  batch 166 loss: 0.2558206501136343
  batch 167 loss: 0.2556498550369354
  batch 168 loss: 0.25555847257020925
  batch 169 loss: 0.25545947888546444
  batch 170 loss: 0.25540885215296466
  batch 171 loss: 0.25540193104953096
  batch 172 loss: 0.2552612457684306
  batch 173 loss: 0.2551517336699315
  batch 174 loss: 0.25519022927887136
  batch 175 loss: 0.2553406061444964
  batch 176 loss: 0.2552605370398272
  batch 177 loss: 0.25520880301456667
  batch 178 loss: 0.25523312766565365
  batch 179 loss: 0.2553220605217545
  batch 180 loss: 0.2553738025327524
  batch 181 loss: 0.2553480207426113
  batch 182 loss: 0.25551376622784266
  batch 183 loss: 0.25531693804459493
  batch 184 loss: 0.2551825069214987
  batch 185 loss: 0.255096744524466
  batch 186 loss: 0.2551967546504031
  batch 187 loss: 0.25521829692437686
  batch 188 loss: 0.2548699382137745
  batch 189 loss: 0.2546581665359477
  batch 190 loss: 0.25453146217684997
  batch 191 loss: 0.2543999350663879
  batch 192 loss: 0.2544655667152256
  batch 193 loss: 0.2545382073969421
  batch 194 loss: 0.2546285308667065
  batch 195 loss: 0.2546850922015997
  batch 196 loss: 0.25448531695470517
  batch 197 loss: 0.25428114120427725
  batch 198 loss: 0.2543766073355771
  batch 199 loss: 0.25453463114386227
  batch 200 loss: 0.2547321767359972
  batch 201 loss: 0.25476295475046434
  batch 202 loss: 0.2546201111509068
  batch 203 loss: 0.254675536687151
  batch 204 loss: 0.25452693179249763
  batch 205 loss: 0.2546784302810343
  batch 206 loss: 0.2546847661577382
  batch 207 loss: 0.2546196406446217
  batch 208 loss: 0.25451066796309674
  batch 209 loss: 0.2543497717266448
  batch 210 loss: 0.25442731820401693
  batch 211 loss: 0.25436478387123035
  batch 212 loss: 0.25428203106769975
  batch 213 loss: 0.2543434116622092
  batch 214 loss: 0.2544160165658621
  batch 215 loss: 0.2541771367538807
  batch 216 loss: 0.2540378039357839
  batch 217 loss: 0.2539981765681148
  batch 218 loss: 0.2540020577677893
  batch 219 loss: 0.25399145211803315
  batch 220 loss: 0.25396495644341816
  batch 221 loss: 0.2540994211694234
  batch 222 loss: 0.2541202330240258
  batch 223 loss: 0.25419194207865026
  batch 224 loss: 0.2541639846084373
  batch 225 loss: 0.2539241370889876
  batch 226 loss: 0.2540365276874694
  batch 227 loss: 0.25372812434177566
  batch 228 loss: 0.2536489086454375
  batch 229 loss: 0.2534917655470069
  batch 230 loss: 0.25346299358036206
  batch 231 loss: 0.253396398512832
  batch 232 loss: 0.25308977896026497
  batch 233 loss: 0.25302037187400295
  batch 234 loss: 0.25301617842454177
  batch 235 loss: 0.253034781775576
  batch 236 loss: 0.2529859746790538
  batch 237 loss: 0.2529486021034828
  batch 238 loss: 0.2528005583571787
  batch 239 loss: 0.2526871981605825
  batch 240 loss: 0.25271782111376523
  batch 241 loss: 0.2527972966185249
  batch 242 loss: 0.25260362564778527
  batch 243 loss: 0.25260620849368015
  batch 244 loss: 0.25258827203365625
  batch 245 loss: 0.2525733180192052
  batch 246 loss: 0.2525357586460385
  batch 247 loss: 0.25254550920082974
  batch 248 loss: 0.2525761149703495
  batch 249 loss: 0.2526079716093569
  batch 250 loss: 0.2524851825833321
  batch 251 loss: 0.2524077264792416
  batch 252 loss: 0.25226040675290046
  batch 253 loss: 0.25212851796932373
  batch 254 loss: 0.2520392619249389
  batch 255 loss: 0.2520787636439005
  batch 256 loss: 0.2520436720806174
  batch 257 loss: 0.25199393742511245
  batch 258 loss: 0.2520722452868787
  batch 259 loss: 0.2521122388512932
  batch 260 loss: 0.2521304982785995
  batch 261 loss: 0.25217374742487836
  batch 262 loss: 0.2521999219560441
  batch 263 loss: 0.25230126003575415
  batch 264 loss: 0.25220209693141055
  batch 265 loss: 0.25222769565177416
  batch 266 loss: 0.2522086995212655
  batch 267 loss: 0.2522993519734801
  batch 268 loss: 0.2521844510711841
  batch 269 loss: 0.25215929558285993
  batch 270 loss: 0.25245977849872026
  batch 271 loss: 0.25242203083645376
  batch 272 loss: 0.25250133526894974
  batch 273 loss: 0.2525283818170701
  batch 274 loss: 0.25269416012685664
  batch 275 loss: 0.25269307293675164
  batch 276 loss: 0.2526697333416213
  batch 277 loss: 0.25273819289267707
  batch 278 loss: 0.2528500548905606
  batch 279 loss: 0.2530333215510973
  batch 280 loss: 0.25297150532049795
  batch 281 loss: 0.2528589912474792
  batch 282 loss: 0.25294719240132796
  batch 283 loss: 0.2528379050350021
  batch 284 loss: 0.2528135023591384
  batch 285 loss: 0.2528430840948172
  batch 286 loss: 0.25288099441286566
  batch 287 loss: 0.2528195003703081
  batch 288 loss: 0.252651995461848
  batch 289 loss: 0.2528968018231507
  batch 290 loss: 0.25270483041631764
  batch 291 loss: 0.2526586319982391
  batch 292 loss: 0.25273321109683544
  batch 293 loss: 0.2528276844236224
  batch 294 loss: 0.25272919926919096
  batch 295 loss: 0.25274784403332207
  batch 296 loss: 0.2528882384098865
  batch 297 loss: 0.25292368749978167
  batch 298 loss: 0.25297418676766775
  batch 299 loss: 0.2529614027028897
  batch 300 loss: 0.25302807753284773
  batch 301 loss: 0.2529906420612652
  batch 302 loss: 0.25303295560625216
  batch 303 loss: 0.253000873101033
  batch 304 loss: 0.25294096375766556
  batch 305 loss: 0.2528602421283722
  batch 306 loss: 0.25293484963233176
  batch 307 loss: 0.2528531647951673
  batch 308 loss: 0.2529020209300828
  batch 309 loss: 0.2529218968932297
  batch 310 loss: 0.2528688782165127
  batch 311 loss: 0.25293223042388435
  batch 312 loss: 0.2530820735085469
  batch 313 loss: 0.2531206092228905
  batch 314 loss: 0.25317962804607524
  batch 315 loss: 0.2532002331245513
  batch 316 loss: 0.25311339146728756
  batch 317 loss: 0.2532358396128525
  batch 318 loss: 0.25324232473313435
  batch 319 loss: 0.25319010336944675
  batch 320 loss: 0.2531155132222921
  batch 321 loss: 0.25311762154845063
  batch 322 loss: 0.25310334896448977
  batch 323 loss: 0.2529920466913158
  batch 324 loss: 0.2528030917416384
  batch 325 loss: 0.25277771500440743
  batch 326 loss: 0.25275674910267437
  batch 327 loss: 0.2527327696117786
  batch 328 loss: 0.2526205960752034
  batch 329 loss: 0.2527609032700489
  batch 330 loss: 0.25260885403011785
  batch 331 loss: 0.2525734986963589
  batch 332 loss: 0.2525409363120435
  batch 333 loss: 0.2525120822338013
  batch 334 loss: 0.25240822926074447
  batch 335 loss: 0.2522042461740437
  batch 336 loss: 0.25211042746724116
  batch 337 loss: 0.2519898733859246
  batch 338 loss: 0.25189947574978044
  batch 339 loss: 0.2516591028477942
  batch 340 loss: 0.2515976977698943
  batch 341 loss: 0.2515392823477994
  batch 342 loss: 0.25146637460467414
  batch 343 loss: 0.2513809049007844
  batch 344 loss: 0.251235046272361
  batch 345 loss: 0.2512808861075968
  batch 346 loss: 0.25107831288279825
  batch 347 loss: 0.2510645172266177
  batch 348 loss: 0.25109612736208686
  batch 349 loss: 0.2510758900539923
  batch 350 loss: 0.25115140293325694
  batch 351 loss: 0.2512011794622807
  batch 352 loss: 0.25142253867604514
  batch 353 loss: 0.25146363510963937
  batch 354 loss: 0.25167790619330216
  batch 355 loss: 0.2517373226897817
  batch 356 loss: 0.25179173797369003
  batch 357 loss: 0.25170789905288976
  batch 358 loss: 0.25172020494937897
  batch 359 loss: 0.2516978657843343
  batch 360 loss: 0.2516862796826495
  batch 361 loss: 0.25159566038368153
  batch 362 loss: 0.25159713679420354
  batch 363 loss: 0.25148464924047803
  batch 364 loss: 0.2514122729036179
  batch 365 loss: 0.25154005064539714
  batch 366 loss: 0.25143423037287965
  batch 367 loss: 0.25136476125801616
  batch 368 loss: 0.2513314960119517
  batch 369 loss: 0.25122522702062033
  batch 370 loss: 0.2512632330527177
  batch 371 loss: 0.25121723428730053
  batch 372 loss: 0.25121217693692893
  batch 373 loss: 0.251217252926916
  batch 374 loss: 0.25108734697581614
  batch 375 loss: 0.25103011818726856
  batch 376 loss: 0.2510563199745214
  batch 377 loss: 0.2510535253374899
  batch 378 loss: 0.25101866927884875
  batch 379 loss: 0.25106170502218533
  batch 380 loss: 0.251095152567876
  batch 381 loss: 0.2510414299533123
  batch 382 loss: 0.2510045516397316
  batch 383 loss: 0.2510076243012132
  batch 384 loss: 0.25105736784947413
  batch 385 loss: 0.2511807030671603
  batch 386 loss: 0.25112050659761526
  batch 387 loss: 0.25116192974939516
  batch 388 loss: 0.25119830210952415
  batch 389 loss: 0.2511868438944412
  batch 390 loss: 0.2511007853807547
  batch 391 loss: 0.25119679678431556
  batch 392 loss: 0.25119741709560767
  batch 393 loss: 0.2512368654628443
  batch 394 loss: 0.25125916877071264
  batch 395 loss: 0.2511620901807954
  batch 396 loss: 0.2511013128766508
  batch 397 loss: 0.2510625615303102
  batch 398 loss: 0.25096515330237956
  batch 399 loss: 0.25094157991403326
  batch 400 loss: 0.2510813092067838
  batch 401 loss: 0.2509629897717526
  batch 402 loss: 0.2509971861148355
  batch 403 loss: 0.25100650825068616
  batch 404 loss: 0.2510140491372878
  batch 405 loss: 0.25104674016251977
  batch 406 loss: 0.25112279086130596
  batch 407 loss: 0.2511605097196026
  batch 408 loss: 0.2512079576286031
  batch 409 loss: 0.25114666646879286
  batch 410 loss: 0.25120295413383625
  batch 411 loss: 0.25113704485179733
  batch 412 loss: 0.25108248765752156
  batch 413 loss: 0.251160935866342
  batch 414 loss: 0.2511550382067616
  batch 415 loss: 0.2511358681572489
  batch 416 loss: 0.25114682456478477
  batch 417 loss: 0.2510934134753202
  batch 418 loss: 0.2510576635313947
  batch 419 loss: 0.25110439340937396
  batch 420 loss: 0.25106029212474823
  batch 421 loss: 0.25096776578885077
  batch 422 loss: 0.2511188103971888
  batch 423 loss: 0.25117622305315435
  batch 424 loss: 0.2511324608382189
  batch 425 loss: 0.25112927563050214
  batch 426 loss: 0.2510877968000134
  batch 427 loss: 0.2510870094880008
  batch 428 loss: 0.2510693240388532
  batch 429 loss: 0.2510908589218602
  batch 430 loss: 0.25106360759845997
  batch 431 loss: 0.2511192723382653
  batch 432 loss: 0.2511560978298938
  batch 433 loss: 0.2511177669702997
  batch 434 loss: 0.2512418150146436
  batch 435 loss: 0.2511384985227694
  batch 436 loss: 0.2511692554305453
  batch 437 loss: 0.2512872208174088
  batch 438 loss: 0.2514800082330834
  batch 439 loss: 0.2514735271216525
  batch 440 loss: 0.2514701964164322
  batch 441 loss: 0.25140738983949024
  batch 442 loss: 0.2514036899912951
  batch 443 loss: 0.2513439653958478
  batch 444 loss: 0.2513234140770929
  batch 445 loss: 0.25136121370819176
  batch 446 loss: 0.25134582035744674
  batch 447 loss: 0.2512831261894047
  batch 448 loss: 0.251353605051658
  batch 449 loss: 0.2513397124602163
  batch 450 loss: 0.2512999212410715
  batch 451 loss: 0.2512608605319275
  batch 452 loss: 0.25123888359660596
  batch 453 loss: 0.25124978742852117
  batch 454 loss: 0.25120129045673406
  batch 455 loss: 0.25117503355492604
  batch 456 loss: 0.2511692891845055
  batch 457 loss: 0.2511470685967731
  batch 458 loss: 0.2511309701217314
  batch 459 loss: 0.2511277423120532
  batch 460 loss: 0.25121649299626764
  batch 461 loss: 0.25115633363191064
  batch 462 loss: 0.2512026602378139
  batch 463 loss: 0.2511344860684022
  batch 464 loss: 0.2511512480740403
  batch 465 loss: 0.2510462583713634
  batch 466 loss: 0.25098368295272533
  batch 467 loss: 0.2510459727470063
  batch 468 loss: 0.25103254068611014
  batch 469 loss: 0.2511018481590092
  batch 470 loss: 0.25112562604407046
  batch 471 loss: 0.2510994269083513
  batch 472 loss: 0.25095263600223144
LOSS train 0.25095263600223144 valid 0.2555555999279022
LOSS train 0.25095263600223144 valid 0.23765501379966736
LOSS train 0.25095263600223144 valid 0.2437217434247335
LOSS train 0.25095263600223144 valid 0.23458555340766907
LOSS train 0.25095263600223144 valid 0.22913607954978943
LOSS train 0.25095263600223144 valid 0.2283371537923813
LOSS train 0.25095263600223144 valid 0.24341080444199698
LOSS train 0.25095263600223144 valid 0.23958974331617355
LOSS train 0.25095263600223144 valid 0.2388914426167806
LOSS train 0.25095263600223144 valid 0.24200097918510438
LOSS train 0.25095263600223144 valid 0.24024013904008
LOSS train 0.25095263600223144 valid 0.2389089452723662
LOSS train 0.25095263600223144 valid 0.23653283715248108
LOSS train 0.25095263600223144 valid 0.23852568226201193
LOSS train 0.25095263600223144 valid 0.23434385657310486
LOSS train 0.25095263600223144 valid 0.2370463814586401
LOSS train 0.25095263600223144 valid 0.23873693101546345
LOSS train 0.25095263600223144 valid 0.23782839957210752
LOSS train 0.25095263600223144 valid 0.2399779173888658
LOSS train 0.25095263600223144 valid 0.2403495766222477
LOSS train 0.25095263600223144 valid 0.2407680501540502
LOSS train 0.25095263600223144 valid 0.23954696208238602
LOSS train 0.25095263600223144 valid 0.24144270044306052
LOSS train 0.25095263600223144 valid 0.2405350704987844
LOSS train 0.25095263600223144 valid 0.23871995031833648
LOSS train 0.25095263600223144 valid 0.23857924342155457
LOSS train 0.25095263600223144 valid 0.23884028803419183
LOSS train 0.25095263600223144 valid 0.2398731649986335
LOSS train 0.25095263600223144 valid 0.2403296545661729
LOSS train 0.25095263600223144 valid 0.24095973918835323
LOSS train 0.25095263600223144 valid 0.24232906631885037
LOSS train 0.25095263600223144 valid 0.24249806115403771
LOSS train 0.25095263600223144 valid 0.243799858472564
LOSS train 0.25095263600223144 valid 0.24319124265628703
LOSS train 0.25095263600223144 valid 0.2455897045986993
LOSS train 0.25095263600223144 valid 0.24525805769695175
LOSS train 0.25095263600223144 valid 0.24564286301264893
LOSS train 0.25095263600223144 valid 0.2459723012227761
LOSS train 0.25095263600223144 valid 0.24556302680419043
LOSS train 0.25095263600223144 valid 0.24586162976920606
LOSS train 0.25095263600223144 valid 0.24544315439898792
LOSS train 0.25095263600223144 valid 0.24580253802594684
LOSS train 0.25095263600223144 valid 0.2448121517203575
LOSS train 0.25095263600223144 valid 0.24484757672656665
LOSS train 0.25095263600223144 valid 0.24436742001109654
LOSS train 0.25095263600223144 valid 0.24545228546080383
LOSS train 0.25095263600223144 valid 0.24575372452431538
LOSS train 0.25095263600223144 valid 0.24539469927549362
LOSS train 0.25095263600223144 valid 0.2454638414236964
LOSS train 0.25095263600223144 valid 0.24485189318656922
LOSS train 0.25095263600223144 valid 0.24552173766435362
LOSS train 0.25095263600223144 valid 0.24514223205355498
LOSS train 0.25095263600223144 valid 0.24515305290806969
LOSS train 0.25095263600223144 valid 0.246052538631139
LOSS train 0.25095263600223144 valid 0.24619884463873776
LOSS train 0.25095263600223144 valid 0.24629364455384867
LOSS train 0.25095263600223144 valid 0.2463757487242682
LOSS train 0.25095263600223144 valid 0.24616986872821017
LOSS train 0.25095263600223144 valid 0.24709820999937543
LOSS train 0.25095263600223144 valid 0.24649294167757035
LOSS train 0.25095263600223144 valid 0.24620677359768603
LOSS train 0.25095263600223144 valid 0.246872638021746
LOSS train 0.25095263600223144 valid 0.2470968739380912
LOSS train 0.25095263600223144 valid 0.24795427825301886
LOSS train 0.25095263600223144 valid 0.24818406196740958
LOSS train 0.25095263600223144 valid 0.24820994314822284
LOSS train 0.25095263600223144 valid 0.24745432721145116
LOSS train 0.25095263600223144 valid 0.24813736120567603
LOSS train 0.25095263600223144 valid 0.2473779772070871
LOSS train 0.25095263600223144 valid 0.24780948864562172
LOSS train 0.25095263600223144 valid 0.24739565605848607
LOSS train 0.25095263600223144 valid 0.24769210939606032
LOSS train 0.25095263600223144 valid 0.24776435388277654
LOSS train 0.25095263600223144 valid 0.24737823875369253
LOSS train 0.25095263600223144 valid 0.2469980130592982
LOSS train 0.25095263600223144 valid 0.24771401580227048
LOSS train 0.25095263600223144 valid 0.2473934768856346
LOSS train 0.25095263600223144 valid 0.24723223654123452
LOSS train 0.25095263600223144 valid 0.24750112883652312
LOSS train 0.25095263600223144 valid 0.24676913637667894
LOSS train 0.25095263600223144 valid 0.24577013669926442
LOSS train 0.25095263600223144 valid 0.24582503136338257
LOSS train 0.25095263600223144 valid 0.24559475373790926
LOSS train 0.25095263600223144 valid 0.24556900649553254
LOSS train 0.25095263600223144 valid 0.24485629183404586
LOSS train 0.25095263600223144 valid 0.24407881515663724
LOSS train 0.25095263600223144 valid 0.24420023152883025
LOSS train 0.25095263600223144 valid 0.24350206604735417
LOSS train 0.25095263600223144 valid 0.24360258107104998
LOSS train 0.25095263600223144 valid 0.24371101640992696
LOSS train 0.25095263600223144 valid 0.2436461959566389
LOSS train 0.25095263600223144 valid 0.2437188952513363
LOSS train 0.25095263600223144 valid 0.2435293402723087
LOSS train 0.25095263600223144 valid 0.2439110440776703
LOSS train 0.25095263600223144 valid 0.24340841738801253
LOSS train 0.25095263600223144 valid 0.24340834220250449
LOSS train 0.25095263600223144 valid 0.2437932780108501
LOSS train 0.25095263600223144 valid 0.24410581892850447
LOSS train 0.25095263600223144 valid 0.24400605788134566
LOSS train 0.25095263600223144 valid 0.24416496396064757
LOSS train 0.25095263600223144 valid 0.24472597506966923
LOSS train 0.25095263600223144 valid 0.2449754765805076
LOSS train 0.25095263600223144 valid 0.24454769419813618
LOSS train 0.25095263600223144 valid 0.24419925892009184
LOSS train 0.25095263600223144 valid 0.24459754867213113
LOSS train 0.25095263600223144 valid 0.24486722375424402
LOSS train 0.25095263600223144 valid 0.24450221727384586
LOSS train 0.25095263600223144 valid 0.24437783010028027
LOSS train 0.25095263600223144 valid 0.2449600566691215
LOSS train 0.25095263600223144 valid 0.24501238749785856
LOSS train 0.25095263600223144 valid 0.24454078172241245
LOSS train 0.25095263600223144 valid 0.24441857808934792
LOSS train 0.25095263600223144 valid 0.24460397977217108
LOSS train 0.25095263600223144 valid 0.24454043884026377
LOSS train 0.25095263600223144 valid 0.24479249456654426
LOSS train 0.25095263600223144 valid 0.2452301675903386
LOSS train 0.25095263600223144 valid 0.24558103899670464
LOSS train 0.25095263600223144 valid 0.24559155106544495
LOSS train 0.25095263600223144 valid 0.2455037829755735
LOSS train 0.25095263600223144 valid 0.24537224657833576
LOSS train 0.25095263600223144 valid 0.24522096103380533
LOSS train 0.25095263600223144 valid 0.24518486173426518
LOSS train 0.25095263600223144 valid 0.24548406693024363
LOSS train 0.25095263600223144 valid 0.24557902567809628
LOSS train 0.25095263600223144 valid 0.2457254602909088
LOSS train 0.25095263600223144 valid 0.24596078764824642
LOSS train 0.25095263600223144 valid 0.2458210750592975
LOSS train 0.25095263600223144 valid 0.24610968388151377
LOSS train 0.25095263600223144 valid 0.2465013123528902
LOSS train 0.25095263600223144 valid 0.2463002318373093
LOSS train 0.25095263600223144 valid 0.24622920292024394
LOSS train 0.25095263600223144 valid 0.245912458296075
LOSS train 0.25095263600223144 valid 0.24565166839979646
LOSS train 0.25095263600223144 valid 0.24589379220756133
LOSS train 0.25095263600223144 valid 0.24589701471505343
LOSS train 0.25095263600223144 valid 0.245451711337356
LOSS train 0.25095263600223144 valid 0.24531979673970356
LOSS train 0.25095263600223144 valid 0.2452027597937031
LOSS train 0.25095263600223144 valid 0.2450919467553818
LOSS train 0.25095263600223144 valid 0.2451968451695783
LOSS train 0.25095263600223144 valid 0.24541419438013795
LOSS train 0.25095263600223144 valid 0.24548801769253234
LOSS train 0.25095263600223144 valid 0.24543856490742078
LOSS train 0.25095263600223144 valid 0.24567782527042759
LOSS train 0.25095263600223144 valid 0.24551874954124978
LOSS train 0.25095263600223144 valid 0.2455938693187008
LOSS train 0.25095263600223144 valid 0.24545386443738224
LOSS train 0.25095263600223144 valid 0.24621237482170802
LOSS train 0.25095263600223144 valid 0.2463690213509054
LOSS train 0.25095263600223144 valid 0.24621480643749238
LOSS train 0.25095263600223144 valid 0.24633939554359738
LOSS train 0.25095263600223144 valid 0.24601025310786148
LOSS train 0.25095263600223144 valid 0.24610890613661873
LOSS train 0.25095263600223144 valid 0.24615718540433165
LOSS train 0.25095263600223144 valid 0.24609499275684357
LOSS train 0.25095263600223144 valid 0.2463443564871947
LOSS train 0.25095263600223144 valid 0.24622203400180598
LOSS train 0.25095263600223144 valid 0.24628376941892166
LOSS train 0.25095263600223144 valid 0.2462390069879076
LOSS train 0.25095263600223144 valid 0.2460156016983092
LOSS train 0.25095263600223144 valid 0.24591299206573772
LOSS train 0.25095263600223144 valid 0.245701179276278
LOSS train 0.25095263600223144 valid 0.2456049271887797
LOSS train 0.25095263600223144 valid 0.2453702983514565
LOSS train 0.25095263600223144 valid 0.24538003197222044
LOSS train 0.25095263600223144 valid 0.24536445584282818
LOSS train 0.25095263600223144 valid 0.24557218142969164
LOSS train 0.25095263600223144 valid 0.24534376568737484
LOSS train 0.25095263600223144 valid 0.24542451786571706
LOSS train 0.25095263600223144 valid 0.2456194092245663
LOSS train 0.25095263600223144 valid 0.2456503168881288
LOSS train 0.25095263600223144 valid 0.2454579406699469
LOSS train 0.25095263600223144 valid 0.24539703259922865
LOSS train 0.25095263600223144 valid 0.24558323407652735
LOSS train 0.25095263600223144 valid 0.24548978405339378
LOSS train 0.25095263600223144 valid 0.24546169227158482
LOSS train 0.25095263600223144 valid 0.2453032494601557
LOSS train 0.25095263600223144 valid 0.245448797104064
LOSS train 0.25095263600223144 valid 0.24535349194563966
LOSS train 0.25095263600223144 valid 0.24548081672853894
LOSS train 0.25095263600223144 valid 0.245485738166788
LOSS train 0.25095263600223144 valid 0.24562523325720986
LOSS train 0.25095263600223144 valid 0.24558091253205075
LOSS train 0.25095263600223144 valid 0.24577975669956725
LOSS train 0.25095263600223144 valid 0.24553784566956596
LOSS train 0.25095263600223144 valid 0.24547029591055328
LOSS train 0.25095263600223144 valid 0.24529656528789093
LOSS train 0.25095263600223144 valid 0.2452376571265941
LOSS train 0.25095263600223144 valid 0.24518717218328406
LOSS train 0.25095263600223144 valid 0.24543960314047963
LOSS train 0.25095263600223144 valid 0.24538263311872932
LOSS train 0.25095263600223144 valid 0.2454432954546064
LOSS train 0.25095263600223144 valid 0.24541806595621948
LOSS train 0.25095263600223144 valid 0.24524052925023837
LOSS train 0.25095263600223144 valid 0.2448829139654453
LOSS train 0.25095263600223144 valid 0.2449725229977345
LOSS train 0.25095263600223144 valid 0.24507591801548972
LOSS train 0.25095263600223144 valid 0.24485828441501867
LOSS train 0.25095263600223144 valid 0.24490480798872272
LOSS train 0.25095263600223144 valid 0.24479474179446697
LOSS train 0.25095263600223144 valid 0.24470220965829062
LOSS train 0.25095263600223144 valid 0.24468425786731265
LOSS train 0.25095263600223144 valid 0.24452912286290981
LOSS train 0.25095263600223144 valid 0.2446078135832852
LOSS train 0.25095263600223144 valid 0.24435784409685832
LOSS train 0.25095263600223144 valid 0.24432177364247518
LOSS train 0.25095263600223144 valid 0.2442996123994606
LOSS train 0.25095263600223144 valid 0.24428643193095922
LOSS train 0.25095263600223144 valid 0.24418875853125557
LOSS train 0.25095263600223144 valid 0.24432673972277413
LOSS train 0.25095263600223144 valid 0.24447095895548002
LOSS train 0.25095263600223144 valid 0.2444726344690008
LOSS train 0.25095263600223144 valid 0.2446518851697725
LOSS train 0.25095263600223144 valid 0.24454998900400143
LOSS train 0.25095263600223144 valid 0.24443355466044225
LOSS train 0.25095263600223144 valid 0.2443199590952308
LOSS train 0.25095263600223144 valid 0.2441411378196857
LOSS train 0.25095263600223144 valid 0.24420082582793104
LOSS train 0.25095263600223144 valid 0.24435982761317737
LOSS train 0.25095263600223144 valid 0.24448845982551576
LOSS train 0.25095263600223144 valid 0.2445300936159505
LOSS train 0.25095263600223144 valid 0.2444626627875878
LOSS train 0.25095263600223144 valid 0.2445509082934247
LOSS train 0.25095263600223144 valid 0.24464430640052473
LOSS train 0.25095263600223144 valid 0.2446126694811715
LOSS train 0.25095263600223144 valid 0.24481164162929078
LOSS train 0.25095263600223144 valid 0.24499921041175657
LOSS train 0.25095263600223144 valid 0.24507860179271615
LOSS train 0.25095263600223144 valid 0.24505626537133512
LOSS train 0.25095263600223144 valid 0.24523921356253
LOSS train 0.25095263600223144 valid 0.2452394842972487
LOSS train 0.25095263600223144 valid 0.2453341573348333
LOSS train 0.25095263600223144 valid 0.24530008646295817
LOSS train 0.25095263600223144 valid 0.2452885034119981
LOSS train 0.25095263600223144 valid 0.2453663261012828
LOSS train 0.25095263600223144 valid 0.24514995647941606
LOSS train 0.25095263600223144 valid 0.24529622268827656
LOSS train 0.25095263600223144 valid 0.2451200398201702
LOSS train 0.25095263600223144 valid 0.24508417824820994
LOSS train 0.25095263600223144 valid 0.24501178183903297
LOSS train 0.25095263600223144 valid 0.2451065530791817
LOSS train 0.25095263600223144 valid 0.24493150554658952
LOSS train 0.25095263600223144 valid 0.2449964330888089
LOSS train 0.25095263600223144 valid 0.24512035967629464
LOSS train 0.25095263600223144 valid 0.2453016435005227
LOSS train 0.25095263600223144 valid 0.2452323393729644
LOSS train 0.25095263600223144 valid 0.24516100300709728
LOSS train 0.25095263600223144 valid 0.24509731716205996
LOSS train 0.25095263600223144 valid 0.24499951656084942
LOSS train 0.25095263600223144 valid 0.24518435072898864
LOSS train 0.25095263600223144 valid 0.24519928874247576
LOSS train 0.25095263600223144 valid 0.245291550362867
LOSS train 0.25095263600223144 valid 0.2451681430514151
LOSS train 0.25095263600223144 valid 0.24514893571457524
LOSS train 0.25095263600223144 valid 0.24515598176741132
LOSS train 0.25095263600223144 valid 0.24510797759285197
LOSS train 0.25095263600223144 valid 0.24509258324765973
LOSS train 0.25095263600223144 valid 0.2451573613193608
LOSS train 0.25095263600223144 valid 0.24518274656824163
LOSS train 0.25095263600223144 valid 0.24518921747803687
LOSS train 0.25095263600223144 valid 0.24536979615231583
LOSS train 0.25095263600223144 valid 0.24537144476220807
LOSS train 0.25095263600223144 valid 0.24545408575707062
LOSS train 0.25095263600223144 valid 0.2454169350817348
LOSS train 0.25095263600223144 valid 0.24541462873512843
LOSS train 0.25095263600223144 valid 0.24537344226487598
LOSS train 0.25095263600223144 valid 0.24543295856495476
LOSS train 0.25095263600223144 valid 0.24553986066090527
LOSS train 0.25095263600223144 valid 0.2457656507487634
LOSS train 0.25095263600223144 valid 0.24596472354950727
LOSS train 0.25095263600223144 valid 0.24601674481291613
LOSS train 0.25095263600223144 valid 0.24631589579889002
LOSS train 0.25095263600223144 valid 0.24640954681586869
LOSS train 0.25095263600223144 valid 0.24628882887807205
LOSS train 0.25095263600223144 valid 0.24630371250889518
LOSS train 0.25095263600223144 valid 0.2461874671820281
LOSS train 0.25095263600223144 valid 0.24606337293390762
LOSS train 0.25095263600223144 valid 0.24585217971810333
LOSS train 0.25095263600223144 valid 0.24594524091503525
LOSS train 0.25095263600223144 valid 0.24580451202179704
LOSS train 0.25095263600223144 valid 0.2457119599780154
LOSS train 0.25095263600223144 valid 0.2456062178239755
LOSS train 0.25095263600223144 valid 0.245597848622622
LOSS train 0.25095263600223144 valid 0.24557767299489236
LOSS train 0.25095263600223144 valid 0.24563695981837155
LOSS train 0.25095263600223144 valid 0.24556947874647755
LOSS train 0.25095263600223144 valid 0.24537809068525293
LOSS train 0.25095263600223144 valid 0.2453570069434742
LOSS train 0.25095263600223144 valid 0.2453867824535469
LOSS train 0.25095263600223144 valid 0.2454664596709712
LOSS train 0.25095263600223144 valid 0.24536485140471115
LOSS train 0.25095263600223144 valid 0.24543666069025863
LOSS train 0.25095263600223144 valid 0.24544670969349533
LOSS train 0.25095263600223144 valid 0.2456788891533605
LOSS train 0.25095263600223144 valid 0.24576470483157595
LOSS train 0.25095263600223144 valid 0.24566351320292498
LOSS train 0.25095263600223144 valid 0.24567668019521116
LOSS train 0.25095263600223144 valid 0.24558039299593676
LOSS train 0.25095263600223144 valid 0.245654048429285
LOSS train 0.25095263600223144 valid 0.24550593122839928
LOSS train 0.25095263600223144 valid 0.24566153162143556
LOSS train 0.25095263600223144 valid 0.24566907869861615
LOSS train 0.25095263600223144 valid 0.2457256714994758
LOSS train 0.25095263600223144 valid 0.24577916398840516
LOSS train 0.25095263600223144 valid 0.24569342307379988
LOSS train 0.25095263600223144 valid 0.24566916522442125
LOSS train 0.25095263600223144 valid 0.24565077871569593
LOSS train 0.25095263600223144 valid 0.24560244468512474
LOSS train 0.25095263600223144 valid 0.24569199116098842
LOSS train 0.25095263600223144 valid 0.24556664209212026
LOSS train 0.25095263600223144 valid 0.24542591895704485
LOSS train 0.25095263600223144 valid 0.24555281673868498
LOSS train 0.25095263600223144 valid 0.2455184142620038
LOSS train 0.25095263600223144 valid 0.24567438766455194
LOSS train 0.25095263600223144 valid 0.24566065957621921
LOSS train 0.25095263600223144 valid 0.24564662816215166
LOSS train 0.25095263600223144 valid 0.24567833904776287
LOSS train 0.25095263600223144 valid 0.24582798596260683
LOSS train 0.25095263600223144 valid 0.24590643647135613
LOSS train 0.25095263600223144 valid 0.24581616837531328
LOSS train 0.25095263600223144 valid 0.2459073061140898
LOSS train 0.25095263600223144 valid 0.24590097552871112
LOSS train 0.25095263600223144 valid 0.24583688950391008
LOSS train 0.25095263600223144 valid 0.2458086821400089
LOSS train 0.25095263600223144 valid 0.24582817554473876
LOSS train 0.25095263600223144 valid 0.2459090455360939
LOSS train 0.25095263600223144 valid 0.2460625980060764
LOSS train 0.25095263600223144 valid 0.24599481941904963
LOSS train 0.25095263600223144 valid 0.2462297100517163
LOSS train 0.25095263600223144 valid 0.24615707943836848
LOSS train 0.25095263600223144 valid 0.2460651209077086
LOSS train 0.25095263600223144 valid 0.24604188233434435
LOSS train 0.25095263600223144 valid 0.2460188273195032
LOSS train 0.25095263600223144 valid 0.2461432731615569
LOSS train 0.25095263600223144 valid 0.24624586381129365
LOSS train 0.25095263600223144 valid 0.2462235124604333
LOSS train 0.25095263600223144 valid 0.24623557885428918
LOSS train 0.25095263600223144 valid 0.24626020359746098
LOSS train 0.25095263600223144 valid 0.24619348426308252
LOSS train 0.25095263600223144 valid 0.24618913373526405
LOSS train 0.25095263600223144 valid 0.2461713722024845
LOSS train 0.25095263600223144 valid 0.2460515986234821
LOSS train 0.25095263600223144 valid 0.24598240192012955
LOSS train 0.25095263600223144 valid 0.2462306562377963
LOSS train 0.25095263600223144 valid 0.2462685031303461
LOSS train 0.25095263600223144 valid 0.24623348598363082
LOSS train 0.25095263600223144 valid 0.24612194402760662
LOSS train 0.25095263600223144 valid 0.245996857597225
LOSS train 0.25095263600223144 valid 0.24606011419378243
LOSS train 0.25095263600223144 valid 0.24595545423882348
LOSS train 0.25095263600223144 valid 0.2458273359173723
LOSS train 0.25095263600223144 valid 0.24578540619801392
LOSS train 0.25095263600223144 valid 0.24588059636080906
LOSS train 0.25095263600223144 valid 0.24585117197642892
LOSS train 0.25095263600223144 valid 0.24607387965833638
LOSS train 0.25095263600223144 valid 0.2460750046740757
LOSS train 0.25095263600223144 valid 0.24600689384449764
LOSS train 0.25095263600223144 valid 0.2459604080412641
LOSS train 0.25095263600223144 valid 0.24598977438752698
LOSS train 0.25095263600223144 valid 0.2458963847822613
LOSS train 0.25095263600223144 valid 0.2458148075272832
LOSS train 0.25095263600223144 valid 0.24590854486707825
LOSS train 0.25095263600223144 valid 0.2456443761744775
LOSS train 0.25095263600223144 valid 0.24564353147378334
LOSS train 0.25095263600223144 valid 0.24563551087902016
LOSS train 0.25095263600223144 valid 0.24555282746670676
LOSS train 0.25095263600223144 valid 0.24536249597813192
LOSS train 0.25095263600223144 valid 0.2453202895577187
LOSS train 0.25095263600223144 valid 0.2453467880241916
Training bichrom
DEVICE = cpu
####################
Total Parameters = 606342
Total Trainable Parameters = 1157
bimodal_network(
  (base_model): bichrom_seq(
    (conv1d): Conv1d(4, 256, kernel_size=(24,), stride=(1,))
    (relu): ReLU()
    (batchNorm1d): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (maxPool1d): MaxPool1d(kernel_size=15, stride=15, padding=0, dilation=1, ceil_mode=True)
    (lstm): LSTM(256, 32, batch_first=True)
    (tanh): Tanh()
    (model_dense_repeat): Sequential(
      (0): Linear(in_features=32, out_features=512, bias=True)
      (1): ReLU()
      (2): Dropout(p=0.5, inplace=False)
      (3): Linear(in_features=512, out_features=512, bias=True)
      (4): ReLU()
      (5): Dropout(p=0.5, inplace=False)
      (6): Linear(in_features=512, out_features=512, bias=True)
      (7): ReLU()
      (8): Dropout(p=0.5, inplace=False)
    )
    (linear): Linear(in_features=512, out_features=1, bias=True)
    (sigmoid): Sigmoid()
  )
  (linear): Linear(in_features=512, out_features=1, bias=True)
  (tanh): Tanh()
  (model): bichrom_chrom(
    (_reshape): _reshape()
    (conv1d): Conv1d(12, 15, kernel_size=(1,), stride=(1,), padding=valid)
    (relu): ReLU()
    (lstm): LSTM(15, 5, batch_first=True)
    (relu2): ReLU()
    (linear): Linear(in_features=5, out_features=1, bias=True)
    (tanh): Tanh()
  )
  (linear2): Linear(in_features=2, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)
base_model.conv1d.weight False
base_model.conv1d.bias False
base_model.batchNorm1d.weight False
base_model.batchNorm1d.bias False
base_model.lstm.weight_ih_l0 False
base_model.lstm.weight_hh_l0 False
base_model.lstm.bias_ih_l0 False
base_model.lstm.bias_hh_l0 False
base_model.model_dense_repeat.0.weight False
base_model.model_dense_repeat.0.bias False
base_model.model_dense_repeat.3.weight False
base_model.model_dense_repeat.3.bias False
base_model.model_dense_repeat.6.weight False
base_model.model_dense_repeat.6.bias False
base_model.linear.weight False
base_model.linear.bias False
linear.weight True
linear.bias True
model.conv1d.weight True
model.conv1d.bias True
model.lstm.weight_ih_l0 True
model.lstm.weight_hh_l0 True
model.lstm.bias_ih_l0 True
model.lstm.bias_hh_l0 True
model.linear.weight True
model.linear.bias True
linear2.weight True
linear2.bias True
####################
EPOCH 1:
  batch 1 loss: 0.6892685890197754
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.6855992078781128
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.684924840927124
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.6843309104442596
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.6818418264389038
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.6794723272323608
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.6771214859826225
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.6746097654104233
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.672114630540212
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.6691315531730652
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.6665188886902549
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.6638365040222803
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.6604795593481797
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.6570638077599662
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.6537177483240764
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.6506680212914944
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.646959992016063
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.6433742708630033
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.6396551759619462
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.6361189156770706
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.6321493642670768
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.6281388889659535
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.6241876482963562
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.6205263907710711
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.616720688343048
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.6127965862934406
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.6089714081199081
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.6049625277519226
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.6009104724588066
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.5972282946109772
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.5933679034633021
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.5893302466720343
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.5850788725144935
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.5813518350615221
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.5780773162841797
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.5742836760150062
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.5705729803523502
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.5664888259611631
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.5628329515457153
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.5590462863445282
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.5552814530163277
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.5516688028971354
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.5487113082131674
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.5451527698473497
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.541897690958447
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.5383396867824637
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.5348442488528312
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.5311824368933836
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.527987929023042
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.5249484097957611
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.5218737470168694
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.5186859873624948
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.515794315428104
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.5127185279572452
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.5096375925974412
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.506769383592265
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.5041865115625817
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.5016500235631548
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.4991683389170695
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.4964751566449801
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.49388156513698767
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.4910720976129655
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.4887262838227408
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.48610898526385427
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.48355756860512955
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.4813712045098796
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.4788822024615843
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.47652363382718144
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.4744203565777212
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.47233623010771614
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.470138473829753
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.4681774208115207
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.46636458496524863
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.4640981514711638
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.46240082224210105
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.46053765792595713
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.4583956850813581
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.4560659993917514
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.45418211553670185
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.4524259276688099
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.45065186972971316
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.44878662105013684
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.44703082111944636
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.44548103106873377
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.4437059058862574
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.44185085733269536
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.4400117136966223
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.4381720491430976
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.43662362821986167
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.43507374326388043
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.4335153685821282
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.4319463981234509
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.4305068228834419
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.4293353861950813
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.428028825082277
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.4267892378071944
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.42505490196119877
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.4238097427450881
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.42261335313922227
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.42102321088314054
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.4195802601257173
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.4181388479237463
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.4169583983212999
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.41570716351270676
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.41416261252902803
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.41290116057081044
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.41171954558274454
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.4104280024766922
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.4094638116315964
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.40857052315365183
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.40724228174836785
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.40592638802315506
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.4045290924542773
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.4035858108024848
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.4023147244816241
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.40118577010158835
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.40009145642447674
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.3987658583259178
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.3975196434419696
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.39629962978263694
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.39517285299202626
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.3939603255664716
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.3930370716786966
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.3917964889157203
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.3907101421356201
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.3898053150328379
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.3889102105080612
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.38804340292699635
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.3870795565520146
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.3862204191776422
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.3850910012958614
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.38411121634822903
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.3831721105073628
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.382268247764502
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.38121088367921335
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.3805288536583676
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.37993594112187407
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.37910787394081336
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.37844595334512726
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.3777779074651854
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.3769358085825088
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.37596045570894027
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.3753271689573368
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.37472416553646326
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.3738929336440974
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.37294930474807136
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.3721787300036878
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.3712844261647882
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.3706748554570563
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.36993917693694434
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.36911237920751633
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.3683005978206271
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.3674703042686375
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.3666735079574895
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.3660228576390974
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.3652851208089254
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.3645660333382856
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.36389033147428607
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.3634341778057926
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.3627438970841467
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.3618748006613358
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.36118285008418705
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.36068045891867095
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.3601148708201036
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.35953378388375945
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.3588609516800168
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.35810933168419823
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.3575279997395618
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.35701268690930316
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.35646510781610713
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.35579815445936214
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.3551624897260999
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.3544985077456932
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.3538595135698373
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.3532371642759868
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.3525722984393889
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.35192886963065734
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.3511574605710051
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.35060034242755206
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.349996420658297
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.34949647038351767
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.3489617889562806
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.348193095030029
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.347708690425624
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.3471983634136819
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.3466456381864445
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.3462627925337317
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.34570651438007965
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.3453261915338102
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.3448372367181276
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.344279720521098
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.34365664748474956
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.3433143451732675
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.3426639535224315
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.3421276001594005
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.3417655619583568
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.34138237499646124
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.34106693191058707
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.34055613587849104
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.3402793560922146
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.3397094835689412
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.33927132381070957
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.3387731014948173
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.3382134492344716
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.3375128536689572
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.3369616686719135
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.33656460665850246
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.3359521787900191
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.3353383750008624
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.33511163755541756
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.3346268807259781
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.33424036050180217
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.3338837247219444
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.3333909151570819
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.3330104696889256
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.3324446752667427
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.3320379298403516
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.33169056475162506
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.3314979534987445
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.33126159540631556
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.33101737296958855
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.33057326521422414
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.3301467280751387
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.329713038900601
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.32921883443991345
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.3287783132585804
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.32830825448036194
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.327744538454633
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.32740609458440256
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.3271060315163239
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.3267514696626952
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.32641504120467035
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.3260416937232529
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.3256486993696954
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.32524304928931785
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.3248828004224826
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.3246597658984269
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.3243485760538518
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.324168583713316
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.3237573233122627
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.32342879016617027
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.32296973571550747
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.32264628337979806
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.3222418160589992
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.32196442892356797
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.3217200843420455
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.32133887756449975
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.32089966595653563
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.3205777831585053
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.32026253151893613
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.3199630091152343
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.31950790854910066
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.31919619016967743
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.31882102382699334
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.3185406673772662
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.3182287485105917
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.31782095545924594
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.31745756226916644
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.3171418294253036
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.3168293436559347
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.3165882913203075
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.3162569551522495
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.3158906371874501
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.31543347832154145
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.3150897080043577
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.31496880847708625
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.31472660785310724
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.3144511606043844
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.3141164456712269
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.31372257537311976
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.3134267270784976
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.31319647786371846
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.31292498723054546
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.31264255587419454
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.31228730792349035
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.3120900971841985
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.31185770201553936
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.3116231437424104
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.31140591967917686
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.3110859417489597
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.3108094775803997
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.3105818061752522
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.3102595490196147
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.30994288539382775
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.3097585244136944
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.30951864840267423
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.3092577225343691
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.3090818561096158
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.30876845054354224
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.3085278318359934
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.3081727574268977
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.307929870914923
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.30774246303701563
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.3075309783017554
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.30725131696563657
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.30694162120690216
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.306797231889333
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.30651132647423135
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.3061481060890051
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.3059454142550627
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.30578234875915056
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.30547143257414267
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.30520087028100545
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.3049598683259989
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.3048205005340889
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.3046325129621169
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.30439304252206695
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.3041207959222329
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.30384307829693297
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.30362722585278173
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.3033395644361183
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.3032250053798541
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.30295763947902776
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.3025990557044175
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.3024150537593024
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.30218250346900544
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.3019885572631276
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.30174621114940764
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.30143270555147933
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.30126795284450053
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.30104859187224203
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.300807350772138
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.3006047567254619
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.3004064020919211
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.300174034008613
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.2999426038718662
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.2996693529211418
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.29947473840197414
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.2991887872251696
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.2990663223645904
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.298901927435146
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.29874430843685046
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.29853634786856426
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.2983100859972531
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.29807371434880725
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.29785346009192015
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.2975238796865197
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.2972751987668184
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.2970201858816597
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.29683767855167387
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.29664132523396847
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.29655462547003875
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.2963437373280178
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.2960742795190146
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.2958450896152552
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.29561981209957533
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.2955023964561028
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.2952825448036879
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.2950739719208469
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.2947773287125996
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.2945750421098834
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.2944429919800975
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.29425003862921306
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.2941427747745298
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.29402125805196627
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.29382606574826026
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.29359246661015254
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.2934943089605044
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.2932800258766642
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.29294676267438463
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.2926867945207453
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.29244704182306047
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.29223207369980403
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.2919511908849517
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.2917681795277008
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.29155172252915595
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.29133986062509815
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.2911811225197237
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.29099148996477203
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.29086432642227894
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.2906557062083499
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.29045475254295977
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.2903258150528327
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.29016279421228774
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.2899493316809336
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.28986913726684893
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.28971290007352196
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.2895106928689139
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.2893732592345542
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.28912382843463047
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.2889198914678704
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.28873303704236813
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.2886157325291447
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.28831765707582235
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.2880992967974056
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.2878456128461991
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.2877716785176472
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.28765552563919233
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.2875168852910285
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.2874136798274823
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.2872747530031692
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.2871292881591588
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.2869689679934475
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.286788239091786
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.28667112453074395
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.28658325669139323
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.28634721273439057
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.28624861998174655
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.28598354267596005
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.28587426409125327
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.2856647589631806
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.2855358827692359
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.28541110969950484
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.2852699853258558
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.28506493469079336
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.2848879213770622
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.28473752004801495
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.284587236851746
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.28442693762207966
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.28434604609884867
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.2841937056133057
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.284028864709787
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.2838471329457535
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.28377860377808123
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.2836962439209582
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.28360566296256506
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.2835017973713452
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.28332841392766916
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.2833444206910941
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.2831974701157638
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.2831640408823439
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.28303662652229244
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.2828288411910934
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.28268627731024093
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.28255756599061627
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.2824084890980116
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.282218048981537
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.28204911243971265
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.281856276285954
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.28174030105041903
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.281598147791389
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.28146012586162045
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.2812561251687673
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.28112614357389065
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.28091601503306424
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.2808815680375887
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.2808128141838571
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.28068434640969314
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.2804118421666986
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.2802149246362123
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.2800865106774566
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.27993795595961996
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.27982038155231737
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.2798212125293306
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.27976728480183677
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.27964998377411887
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.2794803410301806
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.27944341441616416
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.27936321171726575
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.27916265500916376
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.2790151455516033
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.27901229740375966
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.2788319368959789
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.27869085609781585
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.2785658252108228
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.2784321163699292
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.27831222834289726
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.2781996818739254
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.2780152823865284
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.2779826788474684
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.2778255419397561
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.2777578496249207
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.2775367822490037
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.27747996931831387
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.27734107660349977
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.27726059275891135
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.2771402846311859
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.27703224485501265
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.2769315580188084
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.2768382923717194
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.2767684663114021
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.27669816524169205
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.27669816524169205 valid 0.2967511713504791
LOSS train 0.27669816524169205 valid 0.2763788402080536
LOSS train 0.27669816524169205 valid 0.28053922454516095
LOSS train 0.27669816524169205 valid 0.27243607491254807
LOSS train 0.27669816524169205 valid 0.2678096771240234
LOSS train 0.27669816524169205 valid 0.2669406880935033
LOSS train 0.27669816524169205 valid 0.27577715686389376
LOSS train 0.27669816524169205 valid 0.27211693301796913
LOSS train 0.27669816524169205 valid 0.27043174372779
LOSS train 0.27669816524169205 valid 0.27437048554420473
LOSS train 0.27669816524169205 valid 0.27434872497211804
LOSS train 0.27669816524169205 valid 0.27303290367126465
LOSS train 0.27669816524169205 valid 0.2711430650490981
LOSS train 0.27669816524169205 valid 0.271576464176178
LOSS train 0.27669816524169205 valid 0.267270490527153
LOSS train 0.27669816524169205 valid 0.2682040361687541
LOSS train 0.27669816524169205 valid 0.2689114099039751
LOSS train 0.27669816524169205 valid 0.26955674837032956
LOSS train 0.27669816524169205 valid 0.2716740926629619
LOSS train 0.27669816524169205 valid 0.2718531168997288
LOSS train 0.27669816524169205 valid 0.2719260645764215
LOSS train 0.27669816524169205 valid 0.27011493864384567
LOSS train 0.27669816524169205 valid 0.27124435810939124
LOSS train 0.27669816524169205 valid 0.2700247894972563
LOSS train 0.27669816524169205 valid 0.26835557878017424
LOSS train 0.27669816524169205 valid 0.2678384500054213
LOSS train 0.27669816524169205 valid 0.2675617806337498
LOSS train 0.27669816524169205 valid 0.26887076401284765
LOSS train 0.27669816524169205 valid 0.2692986793559173
LOSS train 0.27669816524169205 valid 0.2702667777736982
LOSS train 0.27669816524169205 valid 0.271405087844018
LOSS train 0.27669816524169205 valid 0.27169204177334905
LOSS train 0.27669816524169205 valid 0.27298151588801184
LOSS train 0.27669816524169205 valid 0.2724911513573983
LOSS train 0.27669816524169205 valid 0.2742438405752182
LOSS train 0.27669816524169205 valid 0.2738248436815209
LOSS train 0.27669816524169205 valid 0.2743715478761776
LOSS train 0.27669816524169205 valid 0.27483802994615153
LOSS train 0.27669816524169205 valid 0.2741198444213623
LOSS train 0.27669816524169205 valid 0.2744172427803278
LOSS train 0.27669816524169205 valid 0.27449110913567426
LOSS train 0.27669816524169205 valid 0.2746486426109359
LOSS train 0.27669816524169205 valid 0.27403229856213857
LOSS train 0.27669816524169205 valid 0.2741332172670148
LOSS train 0.27669816524169205 valid 0.27351997792720795
LOSS train 0.27669816524169205 valid 0.2744642544699752
LOSS train 0.27669816524169205 valid 0.2749479371816554
LOSS train 0.27669816524169205 valid 0.2751131209855278
LOSS train 0.27669816524169205 valid 0.27540834308887013
LOSS train 0.27669816524169205 valid 0.274739596247673
LOSS train 0.27669816524169205 valid 0.2751935282174279
LOSS train 0.27669816524169205 valid 0.2749629863179647
LOSS train 0.27669816524169205 valid 0.27490813777131856
LOSS train 0.27669816524169205 valid 0.275312798442664
LOSS train 0.27669816524169205 valid 0.2751383114944805
LOSS train 0.27669816524169205 valid 0.2750717130090509
LOSS train 0.27669816524169205 valid 0.274626312810078
LOSS train 0.27669816524169205 valid 0.2743100017823022
LOSS train 0.27669816524169205 valid 0.274771586060524
LOSS train 0.27669816524169205 valid 0.27416047702233
LOSS train 0.27669816524169205 valid 0.2739781771526962
LOSS train 0.27669816524169205 valid 0.27436489491693433
LOSS train 0.27669816524169205 valid 0.27455547357362414
LOSS train 0.27669816524169205 valid 0.27519525261595845
LOSS train 0.27669816524169205 valid 0.2751726815333733
LOSS train 0.27669816524169205 valid 0.27532691982659424
LOSS train 0.27669816524169205 valid 0.2745928851081364
LOSS train 0.27669816524169205 valid 0.27488188160692945
LOSS train 0.27669816524169205 valid 0.2739641571390456
LOSS train 0.27669816524169205 valid 0.27396699488162995
LOSS train 0.27669816524169205 valid 0.2736329524449899
LOSS train 0.27669816524169205 valid 0.27390166082315975
LOSS train 0.27669816524169205 valid 0.27383369125732004
LOSS train 0.27669816524169205 valid 0.2736083849056347
LOSS train 0.27669816524169205 valid 0.2733115832010905
LOSS train 0.27669816524169205 valid 0.2739498434882415
LOSS train 0.27669816524169205 valid 0.27386813194720777
LOSS train 0.27669816524169205 valid 0.2737881759038338
LOSS train 0.27669816524169205 valid 0.2739457672909845
LOSS train 0.27669816524169205 valid 0.27350592873990537
LOSS train 0.27669816524169205 valid 0.2725204830920255
LOSS train 0.27669816524169205 valid 0.2725459411740303
LOSS train 0.27669816524169205 valid 0.27229363857263544
LOSS train 0.27669816524169205 valid 0.2723440749659425
LOSS train 0.27669816524169205 valid 0.2716640532016754
LOSS train 0.27669816524169205 valid 0.2710472057725108
LOSS train 0.27669816524169205 valid 0.27111217105525665
LOSS train 0.27669816524169205 valid 0.27075588516891
LOSS train 0.27669816524169205 valid 0.271061377411478
LOSS train 0.27669816524169205 valid 0.2712711872326003
LOSS train 0.27669816524169205 valid 0.27127084938379437
LOSS train 0.27669816524169205 valid 0.27130017542968626
LOSS train 0.27669816524169205 valid 0.27128753979359904
LOSS train 0.27669816524169205 valid 0.2715704781894988
LOSS train 0.27669816524169205 valid 0.27106470136266003
LOSS train 0.27669816524169205 valid 0.27119544536496204
LOSS train 0.27669816524169205 valid 0.2715037540369427
LOSS train 0.27669816524169205 valid 0.27168311133068435
LOSS train 0.27669816524169205 valid 0.2718113174643179
LOSS train 0.27669816524169205 valid 0.27195513114333153
LOSS train 0.27669816524169205 valid 0.27223136534195136
LOSS train 0.27669816524169205 valid 0.2724070991663372
LOSS train 0.27669816524169205 valid 0.2722871782420908
LOSS train 0.27669816524169205 valid 0.27190761941556746
LOSS train 0.27669816524169205 valid 0.27226231594880423
LOSS train 0.27669816524169205 valid 0.2724821653287366
LOSS train 0.27669816524169205 valid 0.272147768708033
LOSS train 0.27669816524169205 valid 0.27218868348885467
LOSS train 0.27669816524169205 valid 0.27268482061154253
LOSS train 0.27669816524169205 valid 0.272945842824199
LOSS train 0.27669816524169205 valid 0.2725669632892351
LOSS train 0.27669816524169205 valid 0.27248267283929245
LOSS train 0.27669816524169205 valid 0.2727260598830417
LOSS train 0.27669816524169205 valid 0.2726897308439539
LOSS train 0.27669816524169205 valid 0.27278454316698986
LOSS train 0.27669816524169205 valid 0.27288083744973973
LOSS train 0.27669816524169205 valid 0.2731979821736996
LOSS train 0.27669816524169205 valid 0.27310019946199354
LOSS train 0.27669816524169205 valid 0.2731178674627753
LOSS train 0.27669816524169205 valid 0.2728067475060622
LOSS train 0.27669816524169205 valid 0.2727143230024448
LOSS train 0.27669816524169205 valid 0.2725506021839673
LOSS train 0.27669816524169205 valid 0.272789189001409
LOSS train 0.27669816524169205 valid 0.27305265107462484
LOSS train 0.27669816524169205 valid 0.2730744333267212
LOSS train 0.27669816524169205 valid 0.2732587390475803
LOSS train 0.27669816524169205 valid 0.27320983560066525
LOSS train 0.27669816524169205 valid 0.27343353629112244
LOSS train 0.27669816524169205 valid 0.27375387452369515
LOSS train 0.27669816524169205 valid 0.2736204025837091
LOSS train 0.27669816524169205 valid 0.27369121536043767
LOSS train 0.27669816524169205 valid 0.2733324319124222
LOSS train 0.27669816524169205 valid 0.2730553019091599
LOSS train 0.27669816524169205 valid 0.2734056505471913
LOSS train 0.27669816524169205 valid 0.2733842645530348
LOSS train 0.27669816524169205 valid 0.27315027431091843
LOSS train 0.27669816524169205 valid 0.2728443811409665
LOSS train 0.27669816524169205 valid 0.27274319162403327
LOSS train 0.27669816524169205 valid 0.2724584509357274
LOSS train 0.27669816524169205 valid 0.27251421670828546
LOSS train 0.27669816524169205 valid 0.27255861313207774
LOSS train 0.27669816524169205 valid 0.2727484200412119
LOSS train 0.27669816524169205 valid 0.272890971778156
LOSS train 0.27669816524169205 valid 0.2730531467952662
LOSS train 0.27669816524169205 valid 0.27287598910002875
LOSS train 0.27669816524169205 valid 0.27299401858081557
LOSS train 0.27669816524169205 valid 0.27278682782130986
LOSS train 0.27669816524169205 valid 0.27347538610165184
LOSS train 0.27669816524169205 valid 0.27356967159965695
LOSS train 0.27669816524169205 valid 0.27343270589907964
LOSS train 0.27669816524169205 valid 0.2734718864524601
LOSS train 0.27669816524169205 valid 0.273174123642476
LOSS train 0.27669816524169205 valid 0.27327578412551506
LOSS train 0.27669816524169205 valid 0.2733464745151532
LOSS train 0.27669816524169205 valid 0.27331894241994426
LOSS train 0.27669816524169205 valid 0.2735837135368433
LOSS train 0.27669816524169205 valid 0.27351387072900296
LOSS train 0.27669816524169205 valid 0.27355899925850613
LOSS train 0.27669816524169205 valid 0.273530290842806
LOSS train 0.27669816524169205 valid 0.2733893207274377
LOSS train 0.27669816524169205 valid 0.2733704252457767
LOSS train 0.27669816524169205 valid 0.2732542989614569
LOSS train 0.27669816524169205 valid 0.2731581069757602
LOSS train 0.27669816524169205 valid 0.2729590063596644
LOSS train 0.27669816524169205 valid 0.27297429487560737
LOSS train 0.27669816524169205 valid 0.27297079841415567
LOSS train 0.27669816524169205 valid 0.2729441940249083
LOSS train 0.27669816524169205 valid 0.27282749941306456
LOSS train 0.27669816524169205 valid 0.2728065374863924
LOSS train 0.27669816524169205 valid 0.2730223847662701
LOSS train 0.27669816524169205 valid 0.27301229728244203
LOSS train 0.27669816524169205 valid 0.2728851000063641
LOSS train 0.27669816524169205 valid 0.2729215707220783
LOSS train 0.27669816524169205 valid 0.27293834013157875
LOSS train 0.27669816524169205 valid 0.27276945982660566
LOSS train 0.27669816524169205 valid 0.2727954081175002
LOSS train 0.27669816524169205 valid 0.27273977762561735
LOSS train 0.27669816524169205 valid 0.2728019602847903
LOSS train 0.27669816524169205 valid 0.2727370222187575
LOSS train 0.27669816524169205 valid 0.2727797304590543
LOSS train 0.27669816524169205 valid 0.2728312148902956
LOSS train 0.27669816524169205 valid 0.27290055057504675
LOSS train 0.27669816524169205 valid 0.2728147467628854
LOSS train 0.27669816524169205 valid 0.2729444941100867
LOSS train 0.27669816524169205 valid 0.27269204379738987
LOSS train 0.27669816524169205 valid 0.27262106257420715
LOSS train 0.27669816524169205 valid 0.27248511108803875
LOSS train 0.27669816524169205 valid 0.2724370407138733
LOSS train 0.27669816524169205 valid 0.2724546519694505
LOSS train 0.27669816524169205 valid 0.27270481202163194
LOSS train 0.27669816524169205 valid 0.27268209171856883
LOSS train 0.27669816524169205 valid 0.2726780724866937
LOSS train 0.27669816524169205 valid 0.27275703615784025
LOSS train 0.27669816524169205 valid 0.2725530882010755
LOSS train 0.27669816524169205 valid 0.2723009623778172
LOSS train 0.27669816524169205 valid 0.2724457563338231
LOSS train 0.27669816524169205 valid 0.272670415074087
LOSS train 0.27669816524169205 valid 0.2725361136324478
LOSS train 0.27669816524169205 valid 0.27261645612704694
LOSS train 0.27669816524169205 valid 0.27256763122975824
LOSS train 0.27669816524169205 valid 0.27249688021282653
LOSS train 0.27669816524169205 valid 0.27244045201799655
LOSS train 0.27669816524169205 valid 0.2722937264319124
LOSS train 0.27669816524169205 valid 0.2722383205066709
LOSS train 0.27669816524169205 valid 0.27219399119295723
LOSS train 0.27669816524169205 valid 0.27214835982010205
LOSS train 0.27669816524169205 valid 0.2722134999750893
LOSS train 0.27669816524169205 valid 0.2721798356192616
LOSS train 0.27669816524169205 valid 0.27204394269217713
LOSS train 0.27669816524169205 valid 0.27219174319789524
LOSS train 0.27669816524169205 valid 0.2723067539845598
LOSS train 0.27669816524169205 valid 0.27235329080865067
LOSS train 0.27669816524169205 valid 0.2723868815272067
LOSS train 0.27669816524169205 valid 0.272291088494185
LOSS train 0.27669816524169205 valid 0.27207051390825315
LOSS train 0.27669816524169205 valid 0.27203083797185507
LOSS train 0.27669816524169205 valid 0.27184301518624826
LOSS train 0.27669816524169205 valid 0.27190143157035934
LOSS train 0.27669816524169205 valid 0.2720150291647541
LOSS train 0.27669816524169205 valid 0.2721216908910058
LOSS train 0.27669816524169205 valid 0.2721901434848751
LOSS train 0.27669816524169205 valid 0.2721491584906707
LOSS train 0.27669816524169205 valid 0.2722862537933572
LOSS train 0.27669816524169205 valid 0.27242195659450125
LOSS train 0.27669816524169205 valid 0.2723943877220154
LOSS train 0.27669816524169205 valid 0.2724916795186237
LOSS train 0.27669816524169205 valid 0.27265197603187896
LOSS train 0.27669816524169205 valid 0.2726763876383765
LOSS train 0.27669816524169205 valid 0.27264704584554816
LOSS train 0.27669816524169205 valid 0.2728139872136323
LOSS train 0.27669816524169205 valid 0.27282463794662837
LOSS train 0.27669816524169205 valid 0.27284264256214275
LOSS train 0.27669816524169205 valid 0.27274563665553736
LOSS train 0.27669816524169205 valid 0.2726993784945235
LOSS train 0.27669816524169205 valid 0.27281619373788224
LOSS train 0.27669816524169205 valid 0.27265422020928337
LOSS train 0.27669816524169205 valid 0.2728965029686312
LOSS train 0.27669816524169205 valid 0.2728291384312285
LOSS train 0.27669816524169205 valid 0.2728204300712841
LOSS train 0.27669816524169205 valid 0.2728015328447024
LOSS train 0.27669816524169205 valid 0.2728517654031144
LOSS train 0.27669816524169205 valid 0.2726288078860803
LOSS train 0.27669816524169205 valid 0.2727258573956941
LOSS train 0.27669816524169205 valid 0.2728740564990239
LOSS train 0.27669816524169205 valid 0.27300765788068576
LOSS train 0.27669816524169205 valid 0.2729587790563824
LOSS train 0.27669816524169205 valid 0.272962528141404
LOSS train 0.27669816524169205 valid 0.27294154939872606
LOSS train 0.27669816524169205 valid 0.27281562313736685
LOSS train 0.27669816524169205 valid 0.27287784284353256
LOSS train 0.27669816524169205 valid 0.27295977500567875
LOSS train 0.27669816524169205 valid 0.27299847258698373
LOSS train 0.27669816524169205 valid 0.2729214239615225
LOSS train 0.27669816524169205 valid 0.27287967555870224
LOSS train 0.27669816524169205 valid 0.2728680850828395
LOSS train 0.27669816524169205 valid 0.2728275332483463
LOSS train 0.27669816524169205 valid 0.2727419873735784
LOSS train 0.27669816524169205 valid 0.2728710475471593
LOSS train 0.27669816524169205 valid 0.27285678834298405
LOSS train 0.27669816524169205 valid 0.272862229266992
LOSS train 0.27669816524169205 valid 0.2729586232096756
LOSS train 0.27669816524169205 valid 0.272902742767607
LOSS train 0.27669816524169205 valid 0.27296372007281156
LOSS train 0.27669816524169205 valid 0.27287421002984047
LOSS train 0.27669816524169205 valid 0.2729646290248295
LOSS train 0.27669816524169205 valid 0.27292991076645096
LOSS train 0.27669816524169205 valid 0.2729235101728404
LOSS train 0.27669816524169205 valid 0.27301231805068343
LOSS train 0.27669816524169205 valid 0.2732001116506229
LOSS train 0.27669816524169205 valid 0.273325424613776
LOSS train 0.27669816524169205 valid 0.2734628990567478
LOSS train 0.27669816524169205 valid 0.2737810805659084
LOSS train 0.27669816524169205 valid 0.273786492290951
LOSS train 0.27669816524169205 valid 0.2737397319861572
LOSS train 0.27669816524169205 valid 0.27369167252020404
LOSS train 0.27669816524169205 valid 0.27363273836132407
LOSS train 0.27669816524169205 valid 0.2734985205240628
LOSS train 0.27669816524169205 valid 0.2732951548888529
LOSS train 0.27669816524169205 valid 0.2733574657030003
LOSS train 0.27669816524169205 valid 0.2732854345015117
LOSS train 0.27669816524169205 valid 0.2731859309605432
LOSS train 0.27669816524169205 valid 0.27307046981568034
LOSS train 0.27669816524169205 valid 0.27310161529497207
LOSS train 0.27669816524169205 valid 0.27317839818941037
LOSS train 0.27669816524169205 valid 0.27328122603265864
LOSS train 0.27669816524169205 valid 0.27319494775542014
LOSS train 0.27669816524169205 valid 0.2731830403987539
LOSS train 0.27669816524169205 valid 0.27327567038850653
LOSS train 0.27669816524169205 valid 0.27329557123481196
LOSS train 0.27669816524169205 valid 0.27340547565756174
LOSS train 0.27669816524169205 valid 0.2733240875693941
LOSS train 0.27669816524169205 valid 0.27344635174903154
LOSS train 0.27669816524169205 valid 0.27342670337128555
LOSS train 0.27669816524169205 valid 0.27359601900893815
LOSS train 0.27669816524169205 valid 0.27364265802553145
LOSS train 0.27669816524169205 valid 0.2735928342853849
LOSS train 0.27669816524169205 valid 0.27357464993642233
LOSS train 0.27669816524169205 valid 0.27345858529310096
LOSS train 0.27669816524169205 valid 0.27351285277999765
LOSS train 0.27669816524169205 valid 0.2734578896065553
LOSS train 0.27669816524169205 valid 0.27361963629920616
LOSS train 0.27669816524169205 valid 0.2736002720448355
LOSS train 0.27669816524169205 valid 0.2737061017909066
LOSS train 0.27669816524169205 valid 0.2737786571348184
LOSS train 0.27669816524169205 valid 0.2736422139601629
LOSS train 0.27669816524169205 valid 0.2735627029164165
LOSS train 0.27669816524169205 valid 0.2736110049952125
LOSS train 0.27669816524169205 valid 0.2735654582547677
LOSS train 0.27669816524169205 valid 0.2736270869916311
LOSS train 0.27669816524169205 valid 0.27354011103030174
LOSS train 0.27669816524169205 valid 0.2734188370382671
LOSS train 0.27669816524169205 valid 0.2735295373086746
LOSS train 0.27669816524169205 valid 0.2735161524230299
LOSS train 0.27669816524169205 valid 0.27365108935316657
LOSS train 0.27669816524169205 valid 0.27366899668224276
LOSS train 0.27669816524169205 valid 0.2735960668400873
LOSS train 0.27669816524169205 valid 0.2736358802401305
LOSS train 0.27669816524169205 valid 0.27373536081059174
LOSS train 0.27669816524169205 valid 0.27375819681206465
LOSS train 0.27669816524169205 valid 0.27365090330131353
LOSS train 0.27669816524169205 valid 0.27371036194009574
LOSS train 0.27669816524169205 valid 0.2736881769129208
LOSS train 0.27669816524169205 valid 0.2736708221513051
LOSS train 0.27669816524169205 valid 0.2736781384381983
LOSS train 0.27669816524169205 valid 0.2736872671659176
LOSS train 0.27669816524169205 valid 0.2737879137411439
LOSS train 0.27669816524169205 valid 0.27383242848269435
LOSS train 0.27669816524169205 valid 0.2737647702508583
LOSS train 0.27669816524169205 valid 0.27394920881939516
LOSS train 0.27669816524169205 valid 0.27387275072661316
LOSS train 0.27669816524169205 valid 0.27376320751051886
LOSS train 0.27669816524169205 valid 0.2736964631511504
LOSS train 0.27669816524169205 valid 0.273671011309008
LOSS train 0.27669816524169205 valid 0.273773101007867
LOSS train 0.27669816524169205 valid 0.2738482833798252
LOSS train 0.27669816524169205 valid 0.2738469352147409
LOSS train 0.27669816524169205 valid 0.2738581977187703
LOSS train 0.27669816524169205 valid 0.27383382062940204
LOSS train 0.27669816524169205 valid 0.2738081271317856
LOSS train 0.27669816524169205 valid 0.2737790034974323
LOSS train 0.27669816524169205 valid 0.27373818410806294
LOSS train 0.27669816524169205 valid 0.27364576647156164
LOSS train 0.27669816524169205 valid 0.2735954013058465
LOSS train 0.27669816524169205 valid 0.27384303969352747
LOSS train 0.27669816524169205 valid 0.2738962105218915
LOSS train 0.27669816524169205 valid 0.2738511698438942
LOSS train 0.27669816524169205 valid 0.27376847497324436
LOSS train 0.27669816524169205 valid 0.27368476066267355
LOSS train 0.27669816524169205 valid 0.27372246549430074
LOSS train 0.27669816524169205 valid 0.27363768266780036
LOSS train 0.27669816524169205 valid 0.2735590418094923
LOSS train 0.27669816524169205 valid 0.2735469053838063
LOSS train 0.27669816524169205 valid 0.27356786299866254
LOSS train 0.27669816524169205 valid 0.27356180628645893
LOSS train 0.27669816524169205 valid 0.27373037737020306
LOSS train 0.27669816524169205 valid 0.27377100410253813
LOSS train 0.27669816524169205 valid 0.2737985712890865
LOSS train 0.27669816524169205 valid 0.2738108128392497
LOSS train 0.27669816524169205 valid 0.2738471888317039
LOSS train 0.27669816524169205 valid 0.2737946497483386
LOSS train 0.27669816524169205 valid 0.27376086828285967
LOSS train 0.27669816524169205 valid 0.2738323748852667
LOSS train 0.27669816524169205 valid 0.2735904732683145
LOSS train 0.27669816524169205 valid 0.2736409779760864
LOSS train 0.27669816524169205 valid 0.2736613687587111
LOSS train 0.27669816524169205 valid 0.27360687767221625
LOSS train 0.27669816524169205 valid 0.27342058840012357
LOSS train 0.27669816524169205 valid 0.2733868347966801
LOSS train 0.27669816524169205 valid 0.2734385163884176
EPOCH 2:
  batch 1 loss: 0.19692152738571167
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.19970405846834183
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.19932695726553598
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.2056870236992836
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.21300356686115265
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.21256189793348312
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.20728291145392827
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.20683837495744228
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.20625771416558158
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.20644908547401428
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.20655307986519553
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.20778445899486542
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.20692490729001853
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.20771575506244386
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.2079864223798116
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.20955128781497478
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.2088516231845407
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.2097367743651072
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.20923533565119692
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.2091571070253849
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.20935119262763432
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.20909299701452255
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.20949221110862234
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.21120954118669033
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.21139553129673005
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.21265314175532415
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.21333623981034314
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.21298874276024954
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.21269645310681443
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.21336891055107116
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.2142080626180095
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.21398232970386744
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.21312772595521176
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.21396266712861903
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.2155524798801967
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.21570569525162378
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.21611828578485026
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.2155594206170032
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.21583204773756173
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.2157286174595356
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.21567584138091017
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.21565613824696767
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.2168980514587358
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.2167293747717684
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.21726759870847065
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.21691791648450104
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.21669790751122414
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.21592610608786345
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.21583039359170564
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.21594510674476625
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.21596199975294225
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.21581942989276007
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.2161343868048686
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.21594554682572684
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.215845264358954
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.2159011752477714
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.21656516246628343
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.21681936239374094
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.2172584854445215
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.2172742374241352
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.21721961776741217
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.21684254537666997
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.21704177132674626
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.21676211897283792
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.21659136643776528
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.21677887733235504
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.21655714311706487
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.21645168806700146
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.21665154851001242
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.21693406509501592
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.21687870651063784
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.21704751460088623
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.21742957422178086
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.2172710736458366
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.2175441712141037
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.21769620536973602
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.21759357912973923
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.21707320423462453
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.21708070854597453
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.2172552166506648
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.21741479175326264
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.21720953739997817
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.21734008282782083
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.2176657629509767
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.2175203172599568
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.21739420935858128
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.21725369030716776
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.2169768778099255
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.21716149184810982
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.21720068554083508
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.21724388085223817
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.21720068432066753
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.21746020003031657
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.21788502373593918
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.21810281151219418
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.21819280522565046
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.21776792652828178
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.21788934800697832
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.21807381300011067
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.21780785217881202
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.21777679144155862
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.21759437901132248
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.21772212875120847
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.21772083869347206
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.21737044396854582
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.21732959643287478
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.21739398319030476
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.21732455947332913
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.21754308632754404
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.21788262724876403
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.2176450281529813
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.21744185579674585
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.21709948685844388
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.21729505075174466
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.21705022233983745
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.2169784551550602
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.21700530098034784
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.2167365192356756
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.21652998468455145
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.21637475676834583
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.21626296417772278
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.21605523419184763
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.21617546144539748
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.21591314781577356
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.21577328753471375
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.2157917282884083
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.2158134443553414
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.21582005196250975
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.21574546662411948
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.21582438441423268
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.2155551438340704
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.2154457113056472
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.2154091083465662
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.2153198627838448
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.21511793026217707
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.21524868379620946
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.21551043795843194
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.21546938032775687
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.21561145621666805
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.21574182893548693
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.21564569266129893
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.2154587920702679
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.2155633837609858
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.21574966919918856
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.2156586034544583
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.21544264757061657
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.21540492195255903
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.21525760286965887
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.21537515931081452
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.21533678819735844
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.21517988200613994
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.21505281397778736
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.21485749130545098
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.21469512233486424
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.21465986361426692
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.2145869950644481
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.21453111595029284
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.21448413961673085
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.2147119829677186
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.21465115007013083
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.2143873189176832
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.21432544962491518
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.2144377604584021
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.2144324581434087
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.21444997119181083
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.21436081449669528
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.2141524578282933
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.21412704955963863
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.21414602296592217
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.21417776100775776
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.21407335892058255
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.2139692138447318
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.21383119818103108
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.21370599821381187
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.21363694259098598
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.21348065019331194
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.2133433409666611
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.21310361633809766
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.21305520274785644
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.2129990170399348
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.21301040827240075
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.21296648132604556
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.21267629500295296
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.2126652055784412
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.2126736781887106
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.2126057420366554
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.2127174419515273
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.2126484323055186
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.2127639447097425
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.21276812435765016
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.2127070992560911
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.21253708004951477
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.21264198561406505
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.21244168957484136
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.21237583580689554
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.21245900708801893
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.2125507529616961
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.2126714289188385
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.21263371370545583
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.21278382398188114
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.2126587733848771
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.2126329326541117
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.21257797669013734
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.2124578461197077
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.21216951943025356
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.21201001606809283
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.2120148749023244
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.21179794548795775
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.2116109402983953
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.2117933799113546
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.21169233752935418
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.21170032417999124
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.21170615086533096
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.21157522847719282
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.21160048863222433
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.21139966317073064
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.21137883936479893
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.21143481743718506
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.2115861836089391
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.2116970579732548
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.21179035090213447
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.21169137350610784
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.21160101850471155
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.2115230785150613
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.21133277197678885
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.21121732185108472
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.21114049024781467
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.2109394209426746
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.21091112240693455
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.21096153699833414
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.21094622272691685
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.21092861441188845
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.21090858746240068
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.21086413311397928
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.21077402398941364
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.2107569867018926
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.21081010733224168
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.21079376646701029
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.21093218864007973
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.2108447472875317
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.21081565413237607
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.21067054532776194
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.21065894223044437
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.21054492982440307
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.2105558268269714
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.21057367985083805
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.21048631190288405
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.21036535183027869
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.21031882221918988
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.2102703772187233
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.21026482073909258
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.2100807453195254
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.21005177232823352
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.20996042094596729
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.20996494614610486
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.20991391857387498
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.20976462542778787
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.2096750935496286
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.20962064549269363
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.20957076371862338
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.20959455151667541
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.20953380424557752
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.20941005329895382
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.2092138621391672
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.2091098189916251
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.209207604702254
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.20920910956931024
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.2091967286125048
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.20907798409461975
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.20893696701085127
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.20886815996407582
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.20885997019050753
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.208859580940816
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.20879806496583633
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.20868208083239467
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.20870531344975252
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.2086695863559358
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.20865366350618197
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.20864789770068234
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.20857562848499844
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.20852615273295771
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.20850134794170974
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.2084113654315261
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.20833725387781438
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.20838364582312735
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.20835176151949208
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.2082934755481494
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.20833669075121483
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.20823422888983492
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.20821934008392795
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.2081026950038176
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.2080557959741109
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.20810891789584438
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.208116122857243
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.20803151014497726
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.20791870476426305
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.20797671161918127
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.20791428346161875
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.20776098722398878
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.2077564154068629
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.2077827133807629
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.20767432979203218
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.2076142162773082
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.20755712256619804
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.20759376912820535
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.20757770767204123
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.2075433476254684
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.20744381269270723
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.20734723751406067
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.20728147957594165
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.2072108766944462
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.2072411159483286
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.20715610940045062
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.20698801902638878
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.2069698474236897
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.20693371647709533
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.20690879625288847
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.20684787068726881
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.2067293982053625
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.20674927150830627
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.20671126357862882
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.2066501393821669
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.20661353991127604
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.2065579252846447
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.20650554968760565
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.20642953969401084
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.20630408194633798
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.20625424448673318
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.2061662049822532
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.20618738148248558
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.20618088291310832
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.20616542942911745
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.20609584079311416
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.20603255307424567
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.20598332917512352
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.2059261170881135
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.20579412495523958
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.20572104939871286
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.20560041452403618
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.205563564861522
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.20549894122084564
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.20556756122070446
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.20549219924923978
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.20538033952200135
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.20531724946222443
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.20525213085502558
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.20525410075696127
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.20517393394277014
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.2051020596806163
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.20495459037167685
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.20488671589101481
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.20488862274214625
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.20484085710291822
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.20486917056269566
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.20486982317877486
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.20481154046366723
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.20474211742230156
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.20477548957537006
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.20469960288084982
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.20452510457899836
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.20442321033854233
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.20434973590611094
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.204255988775206
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.2041111605671736
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.20408322378380658
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.2040000131221417
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.20393244885618744
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.20389965127991594
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.20383512658801506
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.20381442264125155
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.2037538763165795
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.20367220793199795
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.20364524895480107
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.20362813838503577
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.20356709452470143
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.2035839288554927
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.20353659916303518
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.20347778641042255
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.20342969257473
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.2033007984882907
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.2032173533295709
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.2031390606498843
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.20314650751125096
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.20298639995356402
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.20288177527390516
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.202753245058455
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.20276864673894196
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.20275298264069655
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.20271491954596307
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.2027119912398167
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.20269070756252464
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.2026471838507117
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.20259381030655393
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.2025135098601961
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.2025099709818635
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.2025080846265109
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.2023774868520141
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.2023761121546803
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.20224355702710928
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.2022344883158803
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.20213544483939905
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.2020858083272455
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.20204371176937377
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.20199217857553228
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.20189427001240812
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.20181249177514626
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.2017389693336346
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.20172132653932945
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.20166673352782474
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.20168098378472213
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.20162349754441394
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.20155112832350638
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.20147200614286104
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.20149250177369601
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.20150730800197786
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.2015319447606229
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.20150410810963426
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.20140843289463142
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.20150335086842994
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.20143308894974846
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.2014699308931686
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.2014519329319633
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.20134794430248
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.20128297271593562
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.2012333227255765
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.20117539016993394
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.20109522688165482
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.20100425998462695
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.20092740071403398
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.20088998786238738
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.20083514291559737
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.20076756120693903
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.20063997574546322
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.20058673736961208
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.20045931229646177
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.20049769017811214
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.20051530259288422
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.20045748560532042
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.2002718649414243
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.20015930701047183
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.20011324699030442
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.20002686348638382
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.19997912310652485
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.20002665446215384
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.20004127188010162
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.2000180307291281
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.1999471437110997
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.19997405834562546
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.19994901165465734
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.19984958286086718
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.1997960074546596
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.1998441211582549
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.1997586658952252
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.19967936128706135
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.1996168261864683
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.19956635443591758
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.19952410363329318
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.19948332136960528
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.1993941284375253
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.19942213965822822
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.19934413013191907
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.19931989887253546
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.19917821869672503
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.19916967394087334
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.19910778898385265
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.19908033312965873
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.1990378395488737
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.1990223017035641
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.19899101753923684
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.1989727111414392
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.19895809472809425
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.1989501312659201
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.1989501312659201 valid 0.243365079164505
LOSS train 0.1989501312659201 valid 0.21901261061429977
LOSS train 0.1989501312659201 valid 0.22367175916830698
LOSS train 0.1989501312659201 valid 0.21494455263018608
LOSS train 0.1989501312659201 valid 0.2108937084674835
LOSS train 0.1989501312659201 valid 0.21335691213607788
LOSS train 0.1989501312659201 valid 0.22143754363059998
LOSS train 0.1989501312659201 valid 0.21836147271096706
LOSS train 0.1989501312659201 valid 0.21647592220041487
LOSS train 0.1989501312659201 valid 0.2201087012887001
LOSS train 0.1989501312659201 valid 0.21984557807445526
LOSS train 0.1989501312659201 valid 0.21870814884702364
LOSS train 0.1989501312659201 valid 0.21710172868691957
LOSS train 0.1989501312659201 valid 0.21721354446240834
LOSS train 0.1989501312659201 valid 0.21342495779196421
LOSS train 0.1989501312659201 valid 0.21471517626196146
LOSS train 0.1989501312659201 valid 0.21547693803029902
LOSS train 0.1989501312659201 valid 0.2155958827998903
LOSS train 0.1989501312659201 valid 0.21753961004708944
LOSS train 0.1989501312659201 valid 0.21815461069345474
LOSS train 0.1989501312659201 valid 0.21804328049932206
LOSS train 0.1989501312659201 valid 0.2165538018400019
LOSS train 0.1989501312659201 valid 0.21715752456499182
LOSS train 0.1989501312659201 valid 0.21638301635781923
LOSS train 0.1989501312659201 valid 0.2148377126455307
LOSS train 0.1989501312659201 valid 0.2146668634735621
LOSS train 0.1989501312659201 valid 0.21458443612964065
LOSS train 0.1989501312659201 valid 0.21569495062742913
LOSS train 0.1989501312659201 valid 0.21617947775742102
LOSS train 0.1989501312659201 valid 0.21701220472653707
LOSS train 0.1989501312659201 valid 0.21801669078488503
LOSS train 0.1989501312659201 valid 0.21789792273193598
LOSS train 0.1989501312659201 valid 0.2187488666086486
LOSS train 0.1989501312659201 valid 0.21819327596355886
LOSS train 0.1989501312659201 valid 0.21985370857375008
LOSS train 0.1989501312659201 valid 0.2196505976219972
LOSS train 0.1989501312659201 valid 0.22034063693639394
LOSS train 0.1989501312659201 valid 0.22075889220363215
LOSS train 0.1989501312659201 valid 0.22004607625496694
LOSS train 0.1989501312659201 valid 0.22014784328639508
LOSS train 0.1989501312659201 valid 0.22009806916481112
LOSS train 0.1989501312659201 valid 0.22051524974050976
LOSS train 0.1989501312659201 valid 0.21992654370707135
LOSS train 0.1989501312659201 valid 0.22026422721418468
LOSS train 0.1989501312659201 valid 0.2199137995640437
LOSS train 0.1989501312659201 valid 0.2208180417833121
LOSS train 0.1989501312659201 valid 0.22115906121882986
LOSS train 0.1989501312659201 valid 0.2212945825109879
LOSS train 0.1989501312659201 valid 0.2217300932626335
LOSS train 0.1989501312659201 valid 0.22104353845119476
LOSS train 0.1989501312659201 valid 0.22138755987672246
LOSS train 0.1989501312659201 valid 0.22119504786454713
LOSS train 0.1989501312659201 valid 0.22131846114149634
LOSS train 0.1989501312659201 valid 0.2217389209954827
LOSS train 0.1989501312659201 valid 0.22160172950137746
LOSS train 0.1989501312659201 valid 0.22154589449720724
LOSS train 0.1989501312659201 valid 0.22130123904922552
LOSS train 0.1989501312659201 valid 0.22104528931708173
LOSS train 0.1989501312659201 valid 0.22153338209047155
LOSS train 0.1989501312659201 valid 0.22099050804972648
LOSS train 0.1989501312659201 valid 0.22075797226585325
LOSS train 0.1989501312659201 valid 0.22109039296065608
LOSS train 0.1989501312659201 valid 0.2211401668332872
LOSS train 0.1989501312659201 valid 0.22184010711498559
LOSS train 0.1989501312659201 valid 0.2217733843968465
LOSS train 0.1989501312659201 valid 0.22176697285789432
LOSS train 0.1989501312659201 valid 0.22119818027339763
LOSS train 0.1989501312659201 valid 0.22149154631530538
LOSS train 0.1989501312659201 valid 0.22067092164703037
LOSS train 0.1989501312659201 valid 0.22082599933658328
LOSS train 0.1989501312659201 valid 0.2205626370621399
LOSS train 0.1989501312659201 valid 0.22072916250261995
LOSS train 0.1989501312659201 valid 0.2207858821708862
LOSS train 0.1989501312659201 valid 0.22059427000380852
LOSS train 0.1989501312659201 valid 0.22047190566857655
LOSS train 0.1989501312659201 valid 0.22112174351748667
LOSS train 0.1989501312659201 valid 0.22103248884925594
LOSS train 0.1989501312659201 valid 0.2209386164561296
LOSS train 0.1989501312659201 valid 0.22088344017916087
LOSS train 0.1989501312659201 valid 0.22033637333661318
LOSS train 0.1989501312659201 valid 0.21958296405680386
LOSS train 0.1989501312659201 valid 0.2197345257168863
LOSS train 0.1989501312659201 valid 0.2194583647581468
LOSS train 0.1989501312659201 valid 0.2195223667437122
LOSS train 0.1989501312659201 valid 0.21888637998524835
LOSS train 0.1989501312659201 valid 0.21834865264421285
LOSS train 0.1989501312659201 valid 0.2183709485434938
LOSS train 0.1989501312659201 valid 0.21800958534533327
LOSS train 0.1989501312659201 valid 0.21824260139733218
LOSS train 0.1989501312659201 valid 0.21839138451549742
LOSS train 0.1989501312659201 valid 0.21840162673494318
LOSS train 0.1989501312659201 valid 0.21837077241228975
LOSS train 0.1989501312659201 valid 0.21831409953614717
LOSS train 0.1989501312659201 valid 0.21857364967148354
LOSS train 0.1989501312659201 valid 0.21810625869976846
LOSS train 0.1989501312659201 valid 0.2181926411576569
LOSS train 0.1989501312659201 valid 0.21837075423334062
LOSS train 0.1989501312659201 valid 0.2185931243762678
LOSS train 0.1989501312659201 valid 0.218736982255271
LOSS train 0.1989501312659201 valid 0.21886068418622018
LOSS train 0.1989501312659201 valid 0.21908415013020582
LOSS train 0.1989501312659201 valid 0.21938424659710304
LOSS train 0.1989501312659201 valid 0.21926929360454522
LOSS train 0.1989501312659201 valid 0.21903636234884077
LOSS train 0.1989501312659201 valid 0.21930561874594007
LOSS train 0.1989501312659201 valid 0.21955057135168113
LOSS train 0.1989501312659201 valid 0.2192344970513727
LOSS train 0.1989501312659201 valid 0.21930833781758943
LOSS train 0.1989501312659201 valid 0.2197746378839563
LOSS train 0.1989501312659201 valid 0.21996577991680666
LOSS train 0.1989501312659201 valid 0.21974670417136974
LOSS train 0.1989501312659201 valid 0.21960390558732407
LOSS train 0.1989501312659201 valid 0.21978200075373186
LOSS train 0.1989501312659201 valid 0.21981821183049888
LOSS train 0.1989501312659201 valid 0.21984159000541853
LOSS train 0.1989501312659201 valid 0.22004008485839285
LOSS train 0.1989501312659201 valid 0.22028914718036977
LOSS train 0.1989501312659201 valid 0.22013119431370395
LOSS train 0.1989501312659201 valid 0.22008274558211574
LOSS train 0.1989501312659201 valid 0.21979064146677654
LOSS train 0.1989501312659201 valid 0.21972514898323817
LOSS train 0.1989501312659201 valid 0.21965025414208897
LOSS train 0.1989501312659201 valid 0.21985985362917426
LOSS train 0.1989501312659201 valid 0.22015299828302476
LOSS train 0.1989501312659201 valid 0.22005371868610382
LOSS train 0.1989501312659201 valid 0.22020241523545886
LOSS train 0.1989501312659201 valid 0.22011838290165728
LOSS train 0.1989501312659201 valid 0.2202410618774593
LOSS train 0.1989501312659201 valid 0.2205666700998942
LOSS train 0.1989501312659201 valid 0.22043859236515484
LOSS train 0.1989501312659201 valid 0.22041676756534867
LOSS train 0.1989501312659201 valid 0.22002999236186346
LOSS train 0.1989501312659201 valid 0.21974572808222664
LOSS train 0.1989501312659201 valid 0.21995856034666744
LOSS train 0.1989501312659201 valid 0.21987760078023982
LOSS train 0.1989501312659201 valid 0.21967045787502737
LOSS train 0.1989501312659201 valid 0.2194356727991661
LOSS train 0.1989501312659201 valid 0.21930932437164197
LOSS train 0.1989501312659201 valid 0.21902782668312676
LOSS train 0.1989501312659201 valid 0.21910735262291772
LOSS train 0.1989501312659201 valid 0.2191617780543388
LOSS train 0.1989501312659201 valid 0.2192330873558219
LOSS train 0.1989501312659201 valid 0.21935305822562504
LOSS train 0.1989501312659201 valid 0.21949314470920298
LOSS train 0.1989501312659201 valid 0.21937582328401764
LOSS train 0.1989501312659201 valid 0.21946992959878217
LOSS train 0.1989501312659201 valid 0.21932155350033117
LOSS train 0.1989501312659201 valid 0.2200982056155398
LOSS train 0.1989501312659201 valid 0.22016840583126016
LOSS train 0.1989501312659201 valid 0.2200087798635165
LOSS train 0.1989501312659201 valid 0.22017098163926838
LOSS train 0.1989501312659201 valid 0.21990899731846233
LOSS train 0.1989501312659201 valid 0.2199716174524594
LOSS train 0.1989501312659201 valid 0.21999700502915817
LOSS train 0.1989501312659201 valid 0.21993861602198694
LOSS train 0.1989501312659201 valid 0.2201202148810411
LOSS train 0.1989501312659201 valid 0.2201061791675106
LOSS train 0.1989501312659201 valid 0.22012857225122331
LOSS train 0.1989501312659201 valid 0.22012036767020915
LOSS train 0.1989501312659201 valid 0.219985805824399
LOSS train 0.1989501312659201 valid 0.21997015594695665
LOSS train 0.1989501312659201 valid 0.21978023685055015
LOSS train 0.1989501312659201 valid 0.21961486888078094
LOSS train 0.1989501312659201 valid 0.2194289281600859
LOSS train 0.1989501312659201 valid 0.21943434765844633
LOSS train 0.1989501312659201 valid 0.2193694615579513
LOSS train 0.1989501312659201 valid 0.21943948886351672
LOSS train 0.1989501312659201 valid 0.21938503001417434
LOSS train 0.1989501312659201 valid 0.21942847332305457
LOSS train 0.1989501312659201 valid 0.21957885728162876
LOSS train 0.1989501312659201 valid 0.2195515223065315
LOSS train 0.1989501312659201 valid 0.21943033122739128
LOSS train 0.1989501312659201 valid 0.21950401787812998
LOSS train 0.1989501312659201 valid 0.2194895394917192
LOSS train 0.1989501312659201 valid 0.2193766554764339
LOSS train 0.1989501312659201 valid 0.21940404015847229
LOSS train 0.1989501312659201 valid 0.21934824828374183
LOSS train 0.1989501312659201 valid 0.21940073806248353
LOSS train 0.1989501312659201 valid 0.2193306185013755
LOSS train 0.1989501312659201 valid 0.21933044559425777
LOSS train 0.1989501312659201 valid 0.21938104021944393
LOSS train 0.1989501312659201 valid 0.21938111993310216
LOSS train 0.1989501312659201 valid 0.21933185622666052
LOSS train 0.1989501312659201 valid 0.21939778635683266
LOSS train 0.1989501312659201 valid 0.21919800147816942
LOSS train 0.1989501312659201 valid 0.21916708101828894
LOSS train 0.1989501312659201 valid 0.21902787518055045
LOSS train 0.1989501312659201 valid 0.21895018711369088
LOSS train 0.1989501312659201 valid 0.21895880114149163
LOSS train 0.1989501312659201 valid 0.21914232930070476
LOSS train 0.1989501312659201 valid 0.21911309729695944
LOSS train 0.1989501312659201 valid 0.21912884549237788
LOSS train 0.1989501312659201 valid 0.21912221170460006
LOSS train 0.1989501312659201 valid 0.21894743170627615
LOSS train 0.1989501312659201 valid 0.2187093688127322
LOSS train 0.1989501312659201 valid 0.2187817590881367
LOSS train 0.1989501312659201 valid 0.2189736567475469
LOSS train 0.1989501312659201 valid 0.2188418690453876
LOSS train 0.1989501312659201 valid 0.21891283434839104
LOSS train 0.1989501312659201 valid 0.21887014165520668
LOSS train 0.1989501312659201 valid 0.21883348394092636
LOSS train 0.1989501312659201 valid 0.21878460329948085
LOSS train 0.1989501312659201 valid 0.21872840740997804
LOSS train 0.1989501312659201 valid 0.21876621217119926
LOSS train 0.1989501312659201 valid 0.21869725919351346
LOSS train 0.1989501312659201 valid 0.2186665638413244
LOSS train 0.1989501312659201 valid 0.21869486980680106
LOSS train 0.1989501312659201 valid 0.2186538097090446
LOSS train 0.1989501312659201 valid 0.21859241801015497
LOSS train 0.1989501312659201 valid 0.21871702954882666
LOSS train 0.1989501312659201 valid 0.2187791563464567
LOSS train 0.1989501312659201 valid 0.21877621472725328
LOSS train 0.1989501312659201 valid 0.21879506342008082
LOSS train 0.1989501312659201 valid 0.21870000883240567
LOSS train 0.1989501312659201 valid 0.21853969104068224
LOSS train 0.1989501312659201 valid 0.21845052522365693
LOSS train 0.1989501312659201 valid 0.2182607691271514
LOSS train 0.1989501312659201 valid 0.21828565753381188
LOSS train 0.1989501312659201 valid 0.21841988892860065
LOSS train 0.1989501312659201 valid 0.21848702390085567
LOSS train 0.1989501312659201 valid 0.21849430686208457
LOSS train 0.1989501312659201 valid 0.21848286580932033
LOSS train 0.1989501312659201 valid 0.21858030831599984
LOSS train 0.1989501312659201 valid 0.21868647681549191
LOSS train 0.1989501312659201 valid 0.21875720527436998
LOSS train 0.1989501312659201 valid 0.21887916432017773
LOSS train 0.1989501312659201 valid 0.21900006373810874
LOSS train 0.1989501312659201 valid 0.2190172973562751
LOSS train 0.1989501312659201 valid 0.21897100666985242
LOSS train 0.1989501312659201 valid 0.21908004050669463
LOSS train 0.1989501312659201 valid 0.21914509629016315
LOSS train 0.1989501312659201 valid 0.21914957290322618
LOSS train 0.1989501312659201 valid 0.2190851944582657
LOSS train 0.1989501312659201 valid 0.2190663627961762
LOSS train 0.1989501312659201 valid 0.2191782272242485
LOSS train 0.1989501312659201 valid 0.21903772077570527
LOSS train 0.1989501312659201 valid 0.21917090719007742
LOSS train 0.1989501312659201 valid 0.21908728446529693
LOSS train 0.1989501312659201 valid 0.2190559583104305
LOSS train 0.1989501312659201 valid 0.21905944583316644
LOSS train 0.1989501312659201 valid 0.2191062605479941
LOSS train 0.1989501312659201 valid 0.21892373935003911
LOSS train 0.1989501312659201 valid 0.21906914601845998
LOSS train 0.1989501312659201 valid 0.21918746829032898
LOSS train 0.1989501312659201 valid 0.21927333753936146
LOSS train 0.1989501312659201 valid 0.21919301134057162
LOSS train 0.1989501312659201 valid 0.21923700855811115
LOSS train 0.1989501312659201 valid 0.219223907878322
LOSS train 0.1989501312659201 valid 0.21914466575207
LOSS train 0.1989501312659201 valid 0.21917455130815505
LOSS train 0.1989501312659201 valid 0.2191775108119881
LOSS train 0.1989501312659201 valid 0.21924435477408152
LOSS train 0.1989501312659201 valid 0.2191655263010221
LOSS train 0.1989501312659201 valid 0.21913593696562325
LOSS train 0.1989501312659201 valid 0.21912192281554727
LOSS train 0.1989501312659201 valid 0.21907335839932784
LOSS train 0.1989501312659201 valid 0.21897876338040317
LOSS train 0.1989501312659201 valid 0.219125824554484
LOSS train 0.1989501312659201 valid 0.21911398427827017
LOSS train 0.1989501312659201 valid 0.21911259568654573
LOSS train 0.1989501312659201 valid 0.219229850161578
LOSS train 0.1989501312659201 valid 0.21918873039593223
LOSS train 0.1989501312659201 valid 0.21923698326254074
LOSS train 0.1989501312659201 valid 0.2192137258296663
LOSS train 0.1989501312659201 valid 0.2193363297660396
LOSS train 0.1989501312659201 valid 0.21932319641337358
LOSS train 0.1989501312659201 valid 0.21934593627961835
LOSS train 0.1989501312659201 valid 0.21941221338599476
LOSS train 0.1989501312659201 valid 0.21959300101024953
LOSS train 0.1989501312659201 valid 0.21970036289206257
LOSS train 0.1989501312659201 valid 0.21982749268357604
LOSS train 0.1989501312659201 valid 0.22015434399466305
LOSS train 0.1989501312659201 valid 0.22022078985914642
LOSS train 0.1989501312659201 valid 0.22020668792028497
LOSS train 0.1989501312659201 valid 0.22015801651911301
LOSS train 0.1989501312659201 valid 0.22009774556626444
LOSS train 0.1989501312659201 valid 0.21999683664163527
LOSS train 0.1989501312659201 valid 0.21980946505670068
LOSS train 0.1989501312659201 valid 0.21983839165566216
LOSS train 0.1989501312659201 valid 0.21974876863615853
LOSS train 0.1989501312659201 valid 0.21968239582942473
LOSS train 0.1989501312659201 valid 0.2195413073538043
LOSS train 0.1989501312659201 valid 0.2195790229964172
LOSS train 0.1989501312659201 valid 0.2196298288837285
LOSS train 0.1989501312659201 valid 0.21969654000642008
LOSS train 0.1989501312659201 valid 0.21965423512917298
LOSS train 0.1989501312659201 valid 0.21962835974601919
LOSS train 0.1989501312659201 valid 0.21968552651297715
LOSS train 0.1989501312659201 valid 0.21968399694634144
LOSS train 0.1989501312659201 valid 0.2198100031449877
LOSS train 0.1989501312659201 valid 0.2196885896814648
LOSS train 0.1989501312659201 valid 0.21977500428688035
LOSS train 0.1989501312659201 valid 0.21977060748448551
LOSS train 0.1989501312659201 valid 0.2199050243817219
LOSS train 0.1989501312659201 valid 0.21999557735556263
LOSS train 0.1989501312659201 valid 0.21992865627681887
LOSS train 0.1989501312659201 valid 0.21992842903241566
LOSS train 0.1989501312659201 valid 0.21986293257682915
LOSS train 0.1989501312659201 valid 0.21991344065371166
LOSS train 0.1989501312659201 valid 0.2198943299551805
LOSS train 0.1989501312659201 valid 0.21999914241193536
LOSS train 0.1989501312659201 valid 0.21995956722079524
LOSS train 0.1989501312659201 valid 0.22004307806491852
LOSS train 0.1989501312659201 valid 0.2201193532367286
LOSS train 0.1989501312659201 valid 0.21999599479260992
LOSS train 0.1989501312659201 valid 0.21993702898422876
LOSS train 0.1989501312659201 valid 0.2199726312964281
LOSS train 0.1989501312659201 valid 0.2199350076352621
LOSS train 0.1989501312659201 valid 0.21998091432654743
LOSS train 0.1989501312659201 valid 0.21989011404014402
LOSS train 0.1989501312659201 valid 0.21978166996474435
LOSS train 0.1989501312659201 valid 0.21988818698968643
LOSS train 0.1989501312659201 valid 0.2199131652189139
LOSS train 0.1989501312659201 valid 0.22000637579305915
LOSS train 0.1989501312659201 valid 0.2199769304385261
LOSS train 0.1989501312659201 valid 0.21991789902124223
LOSS train 0.1989501312659201 valid 0.2199843052527882
LOSS train 0.1989501312659201 valid 0.2200891709646339
LOSS train 0.1989501312659201 valid 0.22011290883008963
LOSS train 0.1989501312659201 valid 0.2200085654389113
LOSS train 0.1989501312659201 valid 0.2200827366002252
LOSS train 0.1989501312659201 valid 0.22006496393717595
LOSS train 0.1989501312659201 valid 0.22005410206391715
LOSS train 0.1989501312659201 valid 0.2200883636136114
LOSS train 0.1989501312659201 valid 0.22009000026262723
LOSS train 0.1989501312659201 valid 0.2201862923023891
LOSS train 0.1989501312659201 valid 0.2202546640273628
LOSS train 0.1989501312659201 valid 0.22017566178266595
LOSS train 0.1989501312659201 valid 0.22035377311851478
LOSS train 0.1989501312659201 valid 0.22026658889019127
LOSS train 0.1989501312659201 valid 0.22015560632205802
LOSS train 0.1989501312659201 valid 0.22010379262178778
LOSS train 0.1989501312659201 valid 0.22009389594689505
LOSS train 0.1989501312659201 valid 0.2201995826856105
LOSS train 0.1989501312659201 valid 0.2202249625280722
LOSS train 0.1989501312659201 valid 0.22026322409510612
LOSS train 0.1989501312659201 valid 0.2202997460563388
LOSS train 0.1989501312659201 valid 0.2202686962820369
LOSS train 0.1989501312659201 valid 0.22025408984813016
LOSS train 0.1989501312659201 valid 0.22023702575003398
LOSS train 0.1989501312659201 valid 0.22019062665375796
LOSS train 0.1989501312659201 valid 0.2201017708631984
LOSS train 0.1989501312659201 valid 0.22007130286262613
LOSS train 0.1989501312659201 valid 0.22031548155774905
LOSS train 0.1989501312659201 valid 0.22037512137406115
LOSS train 0.1989501312659201 valid 0.22037054761985822
LOSS train 0.1989501312659201 valid 0.22028157406993834
LOSS train 0.1989501312659201 valid 0.2202132777150335
LOSS train 0.1989501312659201 valid 0.2202475811741755
LOSS train 0.1989501312659201 valid 0.22017704793385098
LOSS train 0.1989501312659201 valid 0.220103699936826
LOSS train 0.1989501312659201 valid 0.22009321132844145
LOSS train 0.1989501312659201 valid 0.22009236709592025
LOSS train 0.1989501312659201 valid 0.22007725139458975
LOSS train 0.1989501312659201 valid 0.22021972856051486
LOSS train 0.1989501312659201 valid 0.22023728623818817
LOSS train 0.1989501312659201 valid 0.22026613955738164
LOSS train 0.1989501312659201 valid 0.2202361673949151
LOSS train 0.1989501312659201 valid 0.22024260498687084
LOSS train 0.1989501312659201 valid 0.22018306909335983
LOSS train 0.1989501312659201 valid 0.2201618901827989
LOSS train 0.1989501312659201 valid 0.22022962681331687
LOSS train 0.1989501312659201 valid 0.2200457782127969
LOSS train 0.1989501312659201 valid 0.22008836171129248
LOSS train 0.1989501312659201 valid 0.2200987207154705
LOSS train 0.1989501312659201 valid 0.220052059841612
LOSS train 0.1989501312659201 valid 0.21989556464902063
LOSS train 0.1989501312659201 valid 0.2198685988297929
LOSS train 0.1989501312659201 valid 0.2199179684001256
EPOCH 3:
  batch 1 loss: 0.15358129143714905
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.1571890339255333
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.1570693552494049
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.16316820681095123
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.17043028473854066
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.17013033231099448
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.1637641191482544
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.16402704641222954
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.16392343905236986
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.1636384218931198
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.16336589103395288
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.16366143027941385
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.1637464715884282
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.16345731701169694
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.1637449045976003
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.16508942376822233
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.16392404279288122
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.16529330280092028
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.16492950759435954
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.16439196467399597
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.1648823292482467
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.16481596637855878
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.16519646540932034
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.16702705497543016
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.16701078474521636
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.1675129412458493
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.16804090914902864
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.16762406804731914
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.16753167787502551
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.16800618718067806
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.16858716741684945
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.16832681139931083
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.16775551664106775
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.1684537617599263
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.16965595611504147
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.16968570152918497
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.16992830505242218
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.16931157323874926
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.16955605645974478
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.16919611655175687
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.1690997797541502
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.16919300563278653
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.17024077752301858
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.1698208786547184
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.1703593932920032
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.17008995912645175
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.16978170897098296
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.16915396569917598
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.1691298743291777
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.16917639285326003
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.16914314732832067
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.16933033128197378
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.16954969319532504
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.16969125524715142
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.16971441832455722
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.16977788534547603
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.17014278000906893
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.170098597376511
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.17063716937929896
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.1706333408753077
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.1704929319561505
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.1701579814957034
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.17045688510887205
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.17015230050310493
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.17004657479432914
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.17006260495294223
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.16989656860259042
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.1698780430152136
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.17010996523110763
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.17035057182822907
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.170425114707208
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.17045926095710862
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.17074189467789375
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.17057744796211655
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.17064640561739602
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.1707132166545642
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.17051046370685874
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.16997762406483674
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.17005194516121586
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.17008398286998272
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.17025345747853504
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.16994135608760322
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.1698632856090385
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.1702644115402585
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.17010406027821934
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.17018198880345323
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.17007923965481506
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.16987901130183178
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.1700652325421237
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.17010539041625128
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.17008365363210112
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.17009486695346626
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.1703762738935409
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.17058683313587877
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.17084144495035472
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.17088764626532793
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.17054128692936651
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.1705740633971837
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.17069405663495113
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.17043493404984475
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.17045196108888871
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.1702409829286968
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.17024131524331362
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.170358322130946
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.16993703146775563
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.16990068273724251
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.16997773207236674
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.16993938441629763
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.1701583942962349
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.17051638026129115
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.17032310151839042
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.17010635616523878
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.16979933985039197
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.1700335637780658
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.16980510794598122
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.16976409970686354
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.16984701245768458
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.1696571148300575
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.1694603390803858
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.1694511993477742
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.16931632715315859
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.16920136953475046
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.16934132830398838
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.1691264319083383
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.16897612845897675
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.1690111086955146
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.16906178619448595
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.1689861590275541
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.168962252694507
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.1689942981188114
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.16872365829598812
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.16864547404375943
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.16869523605905978
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.16856457337514677
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.16832323560008297
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.1682932280442294
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.16852433490057062
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.1684413934531419
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.1685567735124835
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.16873761362263134
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.16862639203561958
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.1685239204638441
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.1685084335870676
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.16864425575153696
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.16848110679922432
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.16833072247570507
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.16829017524411077
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.16821606890172572
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.16833895154847395
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.16836471259593963
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.1681490136890222
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.16804938135962738
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.16791409926087247
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.16772015508893248
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.16772016402213805
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.16760329672923455
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.16759680069176255
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.16748125860585442
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.16763873351444988
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.16764798406511544
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.16742755019146463
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.16734759040452815
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.1673988352523991
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.16737193023649657
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.1674049591476267
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.16737155382891736
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.16706849567725987
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.16702578930805126
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.16693057644473025
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.16698619026471587
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.16686060279607773
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.16679780657381513
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.16672619957627588
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.1665916845801918
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.1666074897136007
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.1664701051248068
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.1663313074047956
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.16609021925022094
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.16610134484548142
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.16602458212938573
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.1660277211616711
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.1659493253535622
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.16567711613543046
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.16564338160273823
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.16561003689830367
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.16556683463114563
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.1656386899278763
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.1655300052717645
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.16563723334875055
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.16568558396477448
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.16565755181287595
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.16553215426392853
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.16558191844218753
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.16538009185766436
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.1653189913584636
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.16540283071143286
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.1655173387926847
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.1656118490629726
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.1655770744660392
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.1657384593039751
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.16566769289436625
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.16567156611397715
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.16562319814865226
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.16559123540041493
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.16536443262565426
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.16520218782633253
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.1652109583362865
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.16501136281742498
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.16491535006527697
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.16513620813687643
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.16508762785608735
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.16510202110094843
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.16508264154055868
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.16495098291992027
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.16495412920796593
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.16479775951140457
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.16482214684585272
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.16491065898893076
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.16500415765259363
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.16505140628327022
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.16510276010942676
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.1650511074039313
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.16493594893692856
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.16491782019979187
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.164742809732755
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.16461813693816682
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.16461815957455908
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.16443188315290108
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.16439162145925923
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.16449132247463516
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.16456820054616764
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.16453044710616613
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.16449521600304756
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.16443385696436605
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.16438515906004195
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.1644150475513632
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.1643992249661357
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.1643766485029409
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.1644478337098864
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.16440741751963894
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.16445067002441874
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.16435105436720138
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.16436378256161027
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.1643054659676845
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.16428367364771512
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.16429285786864234
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.16423150592366695
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.16416839122652047
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.16410707694339943
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.16405649837851524
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.16408552285803266
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.16390413193712158
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.16386017821758633
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.16378933050501066
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.1638172606042787
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.16377973387716338
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.16364949703912327
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.1635819609428561
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.16352975610140208
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.1634914393035265
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.163507970071387
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.1634967849796055
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.16339041649615357
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.16320637314382827
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.16312634022730702
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.1631607807668528
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.16315959131673033
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.163131789095811
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.1630524096103406
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.16296690404415132
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.16288639280628894
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.16287251496139696
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.16294036920254046
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.16291236997085767
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.16281414958563717
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.16286045048331868
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.16279672710258608
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.1628095302328789
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.16281577437368344
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.16280515667583262
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.1627954087647679
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.16276856469558487
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.1627473814853931
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.16271023079752922
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.1627939123333546
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.16278274956491443
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.16274220358826974
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.16283934853143162
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.1627550621655573
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.16274155090595113
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.16269129693917803
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.16263767704367638
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.1627158007930977
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.16277401813236222
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.16269582782761524
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.16263355959106135
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.16270843860677597
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.16267493267187336
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.1625732296585638
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.162597979704539
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.16266178832299685
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.16261311526724834
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.16258373315578248
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.16253630331668414
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.16257328996892836
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.16254583975068884
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.16252742897027478
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.16245982389558444
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.16239024230963203
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.16231299225361118
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.16233496387096846
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.1623740848631431
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.1623447272724237
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.16225251560188403
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.1622403155243586
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.1622632367512848
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.16225065812703562
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.16222613881219108
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.1621589636821358
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.1621984866913408
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.162182463106708
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.16215726890549156
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.16214892863304622
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.1621316898568177
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.16213163178700668
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.16205868806027196
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.1619370610023128
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.16187425959491875
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.16185271325535325
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.16188827710169734
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.16191044006372868
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.16189256029944105
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.16181835319634313
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.161786150544168
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.1618169908870512
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.1618016593912173
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.16172973694451132
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.1616998564606235
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.16158087026316859
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.1615633645259282
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.16150175378032444
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.16163701666464583
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.16158897681677653
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.16152069083046774
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.16150505894768066
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.1615024139469414
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.1615080674331882
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.1614830974146895
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.1614457322097781
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.16133091898901122
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.16128319045502237
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.16131725477647374
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.16130921402919732
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.16138629743692565
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.1613869597164678
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.1613474688126465
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.16133836539937
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.16139850750898516
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.1613518859890178
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.16125748925324943
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.16120436802052396
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.1611755418061222
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.161110942336646
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.16102519567933057
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.16106132242369325
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.16102615558628827
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.1610303621241767
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.16103254997616875
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.16097856922766704
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.16093593914363835
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.16095032940576018
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.16092569302887685
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.1609051852179916
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.16093687484050817
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.16092779964208603
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.16093841553764773
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.16091511125827027
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.16092391298324973
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.16089185649691282
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.1607979797415043
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.16074305109855697
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.16069272306811122
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.16073439670994139
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.1606096645894771
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.16054476596318282
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.1604601809166256
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.1604748104893884
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.1604944403792165
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.16047949338173806
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.16049911139103082
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.16047576111753273
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.16046095460805357
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.160455016765279
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.1604084361174385
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.16043585689761972
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.1604648410822406
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.16035515449059098
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.1603725531592441
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.1602790758275149
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.16028430372476576
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.16024791707570415
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.1602113129413543
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.1601752041661118
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.1601678360585529
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.16011862780576872
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.16005672121723297
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.1599995553566724
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.16004450666699924
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.16002826986511062
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.16008389930172665
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.16005824408391967
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.16003067516586156
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.1599937385493849
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.1600468490578702
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.16010987579104413
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.16017799448365203
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.1601617808679311
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.16009781973071074
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.16022087370438906
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.16017805288235346
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.16021830873506368
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.16023874335803126
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.1601640249538647
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.16013372745716348
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.16010687614188474
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.160073174732112
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.1600414504072426
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.15997174389172938
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.15992799210242736
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.15989972574766292
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.15989513162808738
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.15986881149863755
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.15977850580463232
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.15975809591706447
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.1596750044274604
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.15973326659530673
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.15977331266250436
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.15974187956416988
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.15960814203040202
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.15952344535087998
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.15950087758621662
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.159438543577944
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.1594047296296124
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.15942942413191
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.15944814683681124
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.1594680447315154
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.15942975693701097
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.15949764663153992
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.15951357733765265
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.15946934065885013
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.15947222519674217
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.15953502393599633
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.15946745666907586
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.1594300027362313
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.15939720913276567
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.1593947150303345
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.15938082890059285
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.1593586131116969
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.1593204769660964
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.15939345717754053
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.1593678031043996
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.15936609197940146
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.1592463875434826
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.1592797782464788
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.15928538262203176
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.1592776279081091
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.1592957288409709
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.15933071732775778
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.15935708372704765
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.1593650349594177
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.15939692852350304
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.15947186583810943
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.15947186583810943 valid 0.22245997190475464
LOSS train 0.15947186583810943 valid 0.19165974855422974
LOSS train 0.15947186583810943 valid 0.19789237280686697
LOSS train 0.15947186583810943 valid 0.18781490623950958
LOSS train 0.15947186583810943 valid 0.18542346656322478
LOSS train 0.15947186583810943 valid 0.1902297909061114
LOSS train 0.15947186583810943 valid 0.19801863389355795
LOSS train 0.15947186583810943 valid 0.19598701782524586
LOSS train 0.15947186583810943 valid 0.19441938234700096
LOSS train 0.15947186583810943 valid 0.19882635921239852
LOSS train 0.15947186583810943 valid 0.19830957596952264
LOSS train 0.15947186583810943 valid 0.1966687353948752
LOSS train 0.15947186583810943 valid 0.19556768467793098
LOSS train 0.15947186583810943 valid 0.19515964495284216
LOSS train 0.15947186583810943 valid 0.19168774386246998
LOSS train 0.15947186583810943 valid 0.19345717877149582
LOSS train 0.15947186583810943 valid 0.19451946896665237
LOSS train 0.15947186583810943 valid 0.19393868992726007
LOSS train 0.15947186583810943 valid 0.19593251222058347
LOSS train 0.15947186583810943 valid 0.19714459404349327
LOSS train 0.15947186583810943 valid 0.19691876712299528
LOSS train 0.15947186583810943 valid 0.1955744827335531
LOSS train 0.15947186583810943 valid 0.19561913803867673
LOSS train 0.15947186583810943 valid 0.19526305856804052
LOSS train 0.15947186583810943 valid 0.19367500841617585
LOSS train 0.15947186583810943 valid 0.19368245739203233
LOSS train 0.15947186583810943 valid 0.193651274950416
LOSS train 0.15947186583810943 valid 0.1946875319949218
LOSS train 0.15947186583810943 valid 0.1952138471192327
LOSS train 0.15947186583810943 valid 0.19602627803881964
LOSS train 0.15947186583810943 valid 0.1969937196662349
LOSS train 0.15947186583810943 valid 0.19644385250285268
LOSS train 0.15947186583810943 valid 0.19696485589851032
LOSS train 0.15947186583810943 valid 0.19644051308141036
LOSS train 0.15947186583810943 valid 0.1982271581888199
LOSS train 0.15947186583810943 valid 0.19829344501097998
LOSS train 0.15947186583810943 valid 0.1992383659691424
LOSS train 0.15947186583810943 valid 0.19955045925943474
LOSS train 0.15947186583810943 valid 0.19875298746121237
LOSS train 0.15947186583810943 valid 0.19878814034163952
LOSS train 0.15947186583810943 valid 0.19858922645813082
LOSS train 0.15947186583810943 valid 0.1993232188480241
LOSS train 0.15947186583810943 valid 0.19874400940052298
LOSS train 0.15947186583810943 valid 0.19934403185140004
LOSS train 0.15947186583810943 valid 0.19906654126114315
LOSS train 0.15947186583810943 valid 0.1998813751599063
LOSS train 0.15947186583810943 valid 0.20019317180552382
LOSS train 0.15947186583810943 valid 0.20037282506624857
LOSS train 0.15947186583810943 valid 0.200975736184996
LOSS train 0.15947186583810943 valid 0.20023609399795533
LOSS train 0.15947186583810943 valid 0.20052743834607742
LOSS train 0.15947186583810943 valid 0.20041996670457032
LOSS train 0.15947186583810943 valid 0.2007703497162405
LOSS train 0.15947186583810943 valid 0.20117407457696068
LOSS train 0.15947186583810943 valid 0.20101478722962465
LOSS train 0.15947186583810943 valid 0.2009291486548526
LOSS train 0.15947186583810943 valid 0.20079644076656877
LOSS train 0.15947186583810943 valid 0.20062385884852246
LOSS train 0.15947186583810943 valid 0.20109949627165066
LOSS train 0.15947186583810943 valid 0.20062811076641082
LOSS train 0.15947186583810943 valid 0.20034758282489465
LOSS train 0.15947186583810943 valid 0.20065360924889963
LOSS train 0.15947186583810943 valid 0.20052765499031733
LOSS train 0.15947186583810943 valid 0.20128171890974045
LOSS train 0.15947186583810943 valid 0.2012281195475505
LOSS train 0.15947186583810943 valid 0.20109203886805158
LOSS train 0.15947186583810943 valid 0.20061566825233287
LOSS train 0.15947186583810943 valid 0.20094739338930914
LOSS train 0.15947186583810943 valid 0.20016698971174765
LOSS train 0.15947186583810943 valid 0.20042284386498588
LOSS train 0.15947186583810943 valid 0.20015811983128667
LOSS train 0.15947186583810943 valid 0.20029138587415218
LOSS train 0.15947186583810943 valid 0.20043612424641438
LOSS train 0.15947186583810943 valid 0.20028633322264697
LOSS train 0.15947186583810943 valid 0.20032795588175456
LOSS train 0.15947186583810943 valid 0.20101400974549746
LOSS train 0.15947186583810943 valid 0.2008318140522226
LOSS train 0.15947186583810943 valid 0.20075844266475776
LOSS train 0.15947186583810943 valid 0.20043620071079157
LOSS train 0.15947186583810943 valid 0.199772940389812
LOSS train 0.15947186583810943 valid 0.19921660828001705
LOSS train 0.15947186583810943 valid 0.19947495674941598
LOSS train 0.15947186583810943 valid 0.19911995799426574
LOSS train 0.15947186583810943 valid 0.19913119450211525
LOSS train 0.15947186583810943 valid 0.19847613064681782
LOSS train 0.15947186583810943 valid 0.19800404909738276
LOSS train 0.15947186583810943 valid 0.1979699376328238
LOSS train 0.15947186583810943 valid 0.1976254821162332
LOSS train 0.15947186583810943 valid 0.1978404459993491
LOSS train 0.15947186583810943 valid 0.19799499892526204
LOSS train 0.15947186583810943 valid 0.1979938840473091
LOSS train 0.15947186583810943 valid 0.19789532005139018
LOSS train 0.15947186583810943 valid 0.1977999218369043
LOSS train 0.15947186583810943 valid 0.1980774131544093
LOSS train 0.15947186583810943 valid 0.1976201720927891
LOSS train 0.15947186583810943 valid 0.19771520125990114
LOSS train 0.15947186583810943 valid 0.1977736283208906
LOSS train 0.15947186583810943 valid 0.19805367807952726
LOSS train 0.15947186583810943 valid 0.19825129737757674
LOSS train 0.15947186583810943 valid 0.19831645786762236
LOSS train 0.15947186583810943 valid 0.19848885291283674
LOSS train 0.15947186583810943 valid 0.1989521376934706
LOSS train 0.15947186583810943 valid 0.19886516425216083
LOSS train 0.15947186583810943 valid 0.19871673904932463
LOSS train 0.15947186583810943 valid 0.19890503471805937
LOSS train 0.15947186583810943 valid 0.19922355562448502
LOSS train 0.15947186583810943 valid 0.19892377315837645
LOSS train 0.15947186583810943 valid 0.19904771212626388
LOSS train 0.15947186583810943 valid 0.19956261780830697
LOSS train 0.15947186583810943 valid 0.1996333654631268
LOSS train 0.15947186583810943 valid 0.19955461143373368
LOSS train 0.15947186583810943 valid 0.19936757548046963
LOSS train 0.15947186583810943 valid 0.19953582561121577
LOSS train 0.15947186583810943 valid 0.19966117723991997
LOSS train 0.15947186583810943 valid 0.19961551518543907
LOSS train 0.15947186583810943 valid 0.1998837387253498
LOSS train 0.15947186583810943 valid 0.20009699922341567
LOSS train 0.15947186583810943 valid 0.19984747797755872
LOSS train 0.15947186583810943 valid 0.19970703325351746
LOSS train 0.15947186583810943 valid 0.1994446056584517
LOSS train 0.15947186583810943 valid 0.1993936893861156
LOSS train 0.15947186583810943 valid 0.19941034033650257
LOSS train 0.15947186583810943 valid 0.19961079518969466
LOSS train 0.15947186583810943 valid 0.19995550679103022
LOSS train 0.15947186583810943 valid 0.19974726378917695
LOSS train 0.15947186583810943 valid 0.19993073410458034
LOSS train 0.15947186583810943 valid 0.1998234446358493
LOSS train 0.15947186583810943 valid 0.1998352921800688
LOSS train 0.15947186583810943 valid 0.2001795374838881
LOSS train 0.15947186583810943 valid 0.20005495823346653
LOSS train 0.15947186583810943 valid 0.1999580970474782
LOSS train 0.15947186583810943 valid 0.19950606594934608
LOSS train 0.15947186583810943 valid 0.19920726210102999
LOSS train 0.15947186583810943 valid 0.19931795370222918
LOSS train 0.15947186583810943 valid 0.19918070567978752
LOSS train 0.15947186583810943 valid 0.19900021296651924
LOSS train 0.15947186583810943 valid 0.19877620063123913
LOSS train 0.15947186583810943 valid 0.19863661313834396
LOSS train 0.15947186583810943 valid 0.19832207927172132
LOSS train 0.15947186583810943 valid 0.19842279702425003
LOSS train 0.15947186583810943 valid 0.19845711888996423
LOSS train 0.15947186583810943 valid 0.19842965974354407
LOSS train 0.15947186583810943 valid 0.19851647661282465
LOSS train 0.15947186583810943 valid 0.19864386744383308
LOSS train 0.15947186583810943 valid 0.19856660807954854
LOSS train 0.15947186583810943 valid 0.1986444473062476
LOSS train 0.15947186583810943 valid 0.1985461571995093
LOSS train 0.15947186583810943 valid 0.19941009339448568
LOSS train 0.15947186583810943 valid 0.19948315470410674
LOSS train 0.15947186583810943 valid 0.1992915075023969
LOSS train 0.15947186583810943 valid 0.19956831791937746
LOSS train 0.15947186583810943 valid 0.19935225371859575
LOSS train 0.15947186583810943 valid 0.19936514261111715
LOSS train 0.15947186583810943 valid 0.19935179811406445
LOSS train 0.15947186583810943 valid 0.1992670096697346
LOSS train 0.15947186583810943 valid 0.1993635204166938
LOSS train 0.15947186583810943 valid 0.19942041614632697
LOSS train 0.15947186583810943 valid 0.19944045202264302
LOSS train 0.15947186583810943 valid 0.19942953657804044
LOSS train 0.15947186583810943 valid 0.19931013528257607
LOSS train 0.15947186583810943 valid 0.199290877173406
LOSS train 0.15947186583810943 valid 0.1990272162689103
LOSS train 0.15947186583810943 valid 0.19880651897447973
LOSS train 0.15947186583810943 valid 0.19864561944836523
LOSS train 0.15947186583810943 valid 0.19865392383300898
LOSS train 0.15947186583810943 valid 0.19853678659861346
LOSS train 0.15947186583810943 valid 0.19867844942087184
LOSS train 0.15947186583810943 valid 0.19869446231140978
LOSS train 0.15947186583810943 valid 0.1988005962597548
LOSS train 0.15947186583810943 valid 0.1989056540762677
LOSS train 0.15947186583810943 valid 0.19885653135372183
LOSS train 0.15947186583810943 valid 0.19872585077618443
LOSS train 0.15947186583810943 valid 0.19885282011734956
LOSS train 0.15947186583810943 valid 0.19881971017725167
LOSS train 0.15947186583810943 valid 0.19871839370046343
LOSS train 0.15947186583810943 valid 0.1987844854593277
LOSS train 0.15947186583810943 valid 0.19875202256407443
LOSS train 0.15947186583810943 valid 0.19879092878839943
LOSS train 0.15947186583810943 valid 0.19870716466584018
LOSS train 0.15947186583810943 valid 0.19867307742436727
LOSS train 0.15947186583810943 valid 0.19873725116582205
LOSS train 0.15947186583810943 valid 0.19866887774768766
LOSS train 0.15947186583810943 valid 0.19861155723939178
LOSS train 0.15947186583810943 valid 0.198619791832955
LOSS train 0.15947186583810943 valid 0.19843415448794494
LOSS train 0.15947186583810943 valid 0.19846046667906544
LOSS train 0.15947186583810943 valid 0.19830895561904194
LOSS train 0.15947186583810943 valid 0.19820369684949835
LOSS train 0.15947186583810943 valid 0.1982001861724904
LOSS train 0.15947186583810943 valid 0.1983226607504644
LOSS train 0.15947186583810943 valid 0.19827969777958557
LOSS train 0.15947186583810943 valid 0.19830862704354027
LOSS train 0.15947186583810943 valid 0.1982428524160632
LOSS train 0.15947186583810943 valid 0.1980676348369146
LOSS train 0.15947186583810943 valid 0.19782103666892417
LOSS train 0.15947186583810943 valid 0.19786028906094785
LOSS train 0.15947186583810943 valid 0.19804255906397922
LOSS train 0.15947186583810943 valid 0.19790894628474207
LOSS train 0.15947186583810943 valid 0.19797320506680552
LOSS train 0.15947186583810943 valid 0.19792310774326324
LOSS train 0.15947186583810943 valid 0.197907647831523
LOSS train 0.15947186583810943 valid 0.19786022396961062
LOSS train 0.15947186583810943 valid 0.1978726849473756
LOSS train 0.15947186583810943 valid 0.19802373486991023
LOSS train 0.15947186583810943 valid 0.19790478770325823
LOSS train 0.15947186583810943 valid 0.19791658931565517
LOSS train 0.15947186583810943 valid 0.1979358975150159
LOSS train 0.15947186583810943 valid 0.1978938810240764
LOSS train 0.15947186583810943 valid 0.19788991620666102
LOSS train 0.15947186583810943 valid 0.19799837810652596
LOSS train 0.15947186583810943 valid 0.19799452243258037
LOSS train 0.15947186583810943 valid 0.19796234678547336
LOSS train 0.15947186583810943 valid 0.197949446003202
LOSS train 0.15947186583810943 valid 0.19785176036513855
LOSS train 0.15947186583810943 valid 0.19774999937345816
LOSS train 0.15947186583810943 valid 0.19762197709469884
LOSS train 0.15947186583810943 valid 0.19739652201876662
LOSS train 0.15947186583810943 valid 0.19738644894656784
LOSS train 0.15947186583810943 valid 0.19752065150160766
LOSS train 0.15947186583810943 valid 0.1975403795865449
LOSS train 0.15947186583810943 valid 0.19749379353555618
LOSS train 0.15947186583810943 valid 0.19751350527948086
LOSS train 0.15947186583810943 valid 0.19758889486704173
LOSS train 0.15947186583810943 valid 0.1976693967756416
LOSS train 0.15947186583810943 valid 0.19787488791677688
LOSS train 0.15947186583810943 valid 0.19802683248984077
LOSS train 0.15947186583810943 valid 0.19813571036649696
LOSS train 0.15947186583810943 valid 0.19816384549465096
LOSS train 0.15947186583810943 valid 0.1981170397398253
LOSS train 0.15947186583810943 valid 0.19817648426346157
LOSS train 0.15947186583810943 valid 0.19827067761710196
LOSS train 0.15947186583810943 valid 0.19825167196064158
LOSS train 0.15947186583810943 valid 0.198222331478872
LOSS train 0.15947186583810943 valid 0.1982123309857825
LOSS train 0.15947186583810943 valid 0.1983322669216927
LOSS train 0.15947186583810943 valid 0.19820440642661968
LOSS train 0.15947186583810943 valid 0.1982457945241204
LOSS train 0.15947186583810943 valid 0.19815423214385489
LOSS train 0.15947186583810943 valid 0.19809904332939052
LOSS train 0.15947186583810943 valid 0.1981191371877988
LOSS train 0.15947186583810943 valid 0.19817420030283237
LOSS train 0.15947186583810943 valid 0.19802078081310287
LOSS train 0.15947186583810943 valid 0.19821147553224133
LOSS train 0.15947186583810943 valid 0.19830999047052664
LOSS train 0.15947186583810943 valid 0.19836417126412295
LOSS train 0.15947186583810943 valid 0.19825158159180387
LOSS train 0.15947186583810943 valid 0.1983339559090765
LOSS train 0.15947186583810943 valid 0.19832521304488182
LOSS train 0.15947186583810943 valid 0.19827858020502878
LOSS train 0.15947186583810943 valid 0.1982766877412796
LOSS train 0.15947186583810943 valid 0.19822817874619686
LOSS train 0.15947186583810943 valid 0.1983296527039437
LOSS train 0.15947186583810943 valid 0.1982561001193382
LOSS train 0.15947186583810943 valid 0.1982171011018002
LOSS train 0.15947186583810943 valid 0.19819326763059578
LOSS train 0.15947186583810943 valid 0.19814781844615936
LOSS train 0.15947186583810943 valid 0.19804137376959685
LOSS train 0.15947186583810943 valid 0.19822299913611524
LOSS train 0.15947186583810943 valid 0.19821635607816998
LOSS train 0.15947186583810943 valid 0.19822166120776763
LOSS train 0.15947186583810943 valid 0.19835568079546492
LOSS train 0.15947186583810943 valid 0.19832069665648555
LOSS train 0.15947186583810943 valid 0.19836125740986812
LOSS train 0.15947186583810943 valid 0.19838513253313123
LOSS train 0.15947186583810943 valid 0.1985262337158311
LOSS train 0.15947186583810943 valid 0.19851849289765036
LOSS train 0.15947186583810943 valid 0.1985576841491885
LOSS train 0.15947186583810943 valid 0.1986261476109277
LOSS train 0.15947186583810943 valid 0.19880363937868947
LOSS train 0.15947186583810943 valid 0.19889725678496892
LOSS train 0.15947186583810943 valid 0.19900475832808942
LOSS train 0.15947186583810943 valid 0.19937252581996076
LOSS train 0.15947186583810943 valid 0.19948030615245904
LOSS train 0.15947186583810943 valid 0.19948119806112163
LOSS train 0.15947186583810943 valid 0.19941568905656987
LOSS train 0.15947186583810943 valid 0.19935844784629517
LOSS train 0.15947186583810943 valid 0.19928434910757017
LOSS train 0.15947186583810943 valid 0.1990916428806113
LOSS train 0.15947186583810943 valid 0.19909725057822403
LOSS train 0.15947186583810943 valid 0.19898919968732764
LOSS train 0.15947186583810943 valid 0.19894082891983494
LOSS train 0.15947186583810943 valid 0.19875413343204673
LOSS train 0.15947186583810943 valid 0.19878556803132114
LOSS train 0.15947186583810943 valid 0.1988156496638983
LOSS train 0.15947186583810943 valid 0.1988544764225943
LOSS train 0.15947186583810943 valid 0.19885258683999935
LOSS train 0.15947186583810943 valid 0.19882503110357277
LOSS train 0.15947186583810943 valid 0.19885600342725715
LOSS train 0.15947186583810943 valid 0.1988336589303396
LOSS train 0.15947186583810943 valid 0.1989637354324604
LOSS train 0.15947186583810943 valid 0.19877524658576728
LOSS train 0.15947186583810943 valid 0.19884000182764172
LOSS train 0.15947186583810943 valid 0.1988549164842827
LOSS train 0.15947186583810943 valid 0.1989703263048412
LOSS train 0.15947186583810943 valid 0.199099540558912
LOSS train 0.15947186583810943 valid 0.19901482212181026
LOSS train 0.15947186583810943 valid 0.1990093198990581
LOSS train 0.15947186583810943 valid 0.19897511751459748
LOSS train 0.15947186583810943 valid 0.19901818595203669
LOSS train 0.15947186583810943 valid 0.19902262876431148
LOSS train 0.15947186583810943 valid 0.19907756768587817
LOSS train 0.15947186583810943 valid 0.19901648119388038
LOSS train 0.15947186583810943 valid 0.19909201808298382
LOSS train 0.15947186583810943 valid 0.1991776144622188
LOSS train 0.15947186583810943 valid 0.19906358303593807
LOSS train 0.15947186583810943 valid 0.19901834647445119
LOSS train 0.15947186583810943 valid 0.19906023799984773
LOSS train 0.15947186583810943 valid 0.19901885817964354
LOSS train 0.15947186583810943 valid 0.19905129349926143
LOSS train 0.15947186583810943 valid 0.19897614405039818
LOSS train 0.15947186583810943 valid 0.19887141964826552
LOSS train 0.15947186583810943 valid 0.1989694087742231
LOSS train 0.15947186583810943 valid 0.1990258229997592
LOSS train 0.15947186583810943 valid 0.19907940392661247
LOSS train 0.15947186583810943 valid 0.19901149608786145
LOSS train 0.15947186583810943 valid 0.1989634505272666
LOSS train 0.15947186583810943 valid 0.19905652307937574
LOSS train 0.15947186583810943 valid 0.19916992672022032
LOSS train 0.15947186583810943 valid 0.19919761580920145
LOSS train 0.15947186583810943 valid 0.19908372559584678
LOSS train 0.15947186583810943 valid 0.19917748116444203
LOSS train 0.15947186583810943 valid 0.19916320036842217
LOSS train 0.15947186583810943 valid 0.1991463076588539
LOSS train 0.15947186583810943 valid 0.19921963758490704
LOSS train 0.15947186583810943 valid 0.19921133967546317
LOSS train 0.15947186583810943 valid 0.19932943600818423
LOSS train 0.15947186583810943 valid 0.19942335779878342
LOSS train 0.15947186583810943 valid 0.19932591111013076
LOSS train 0.15947186583810943 valid 0.1995080826764411
LOSS train 0.15947186583810943 valid 0.19940660844246547
LOSS train 0.15947186583810943 valid 0.19927776665665953
LOSS train 0.15947186583810943 valid 0.1992378793656826
LOSS train 0.15947186583810943 valid 0.19923700034260391
LOSS train 0.15947186583810943 valid 0.19934542204983935
LOSS train 0.15947186583810943 valid 0.19933739424641453
LOSS train 0.15947186583810943 valid 0.1994206594037158
LOSS train 0.15947186583810943 valid 0.1994836878705803
LOSS train 0.15947186583810943 valid 0.19944767764157798
LOSS train 0.15947186583810943 valid 0.199439123888283
LOSS train 0.15947186583810943 valid 0.19941756729693974
LOSS train 0.15947186583810943 valid 0.199339463126974
LOSS train 0.15947186583810943 valid 0.19925819585720697
LOSS train 0.15947186583810943 valid 0.19924413949338063
LOSS train 0.15947186583810943 valid 0.19949994062961535
LOSS train 0.15947186583810943 valid 0.19957153645978457
LOSS train 0.15947186583810943 valid 0.19959187279374613
LOSS train 0.15947186583810943 valid 0.1994895562744278
LOSS train 0.15947186583810943 valid 0.199428735793322
LOSS train 0.15947186583810943 valid 0.19946728441810882
LOSS train 0.15947186583810943 valid 0.1993979896392141
LOSS train 0.15947186583810943 valid 0.19931238533085227
LOSS train 0.15947186583810943 valid 0.1993016508323225
LOSS train 0.15947186583810943 valid 0.19926431091243754
LOSS train 0.15947186583810943 valid 0.19924513587338777
LOSS train 0.15947186583810943 valid 0.19937545783083205
LOSS train 0.15947186583810943 valid 0.19938414003909305
LOSS train 0.15947186583810943 valid 0.1994105105390068
LOSS train 0.15947186583810943 valid 0.1993399039743333
LOSS train 0.15947186583810943 valid 0.19933880604077184
LOSS train 0.15947186583810943 valid 0.19927683559556802
LOSS train 0.15947186583810943 valid 0.19927507956767676
LOSS train 0.15947186583810943 valid 0.19935215186348276
LOSS train 0.15947186583810943 valid 0.19919851344314818
LOSS train 0.15947186583810943 valid 0.19924416581352988
LOSS train 0.15947186583810943 valid 0.19924798971169616
LOSS train 0.15947186583810943 valid 0.19922968505021652
LOSS train 0.15947186583810943 valid 0.19909225899614494
LOSS train 0.15947186583810943 valid 0.19907888227506823
LOSS train 0.15947186583810943 valid 0.19911899460040458
EPOCH 4:
  batch 1 loss: 0.131354421377182
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.13541657477617264
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.13705729941527048
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.14372852817177773
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.14983807802200316
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.15057904024918875
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.14343181358916418
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.14440253283828497
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.14404086189137566
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.1430829606950283
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.14287136020985516
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.14237072629233202
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.1426888297383602
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.14182515921337263
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.14242858638366063
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.1437610569410026
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.1424003374927184
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.14440022160609564
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.14367705426718058
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.1431143045425415
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.14364940708591825
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.14365121112628418
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.14403468370437622
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.14589287713170052
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.14581110417842866
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.14627586190517133
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.1470544558984262
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.1465468199125358
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.1465712852519134
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.14714136868715286
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.14776925261943571
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.14734896551817656
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.1470161464178201
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.14777903916204677
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.1488747545651027
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.14902435367306074
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.14926195265473546
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.14868147004591792
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.14909199338692886
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.1489828709512949
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.14915031012965413
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.14920829910607564
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.15007551291654275
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.1494719357314435
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.1503871876332495
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.15031817182898521
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.15011237101986052
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.14953590913986167
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.1495639817142973
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.14961262330412864
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.14954938593448378
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.14988307325312725
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.14996895601726928
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.15028497976837335
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.15054032572291115
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.15057336924863712
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.15075121859186574
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.15062903985381126
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.15126868564698656
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.15123558454215527
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.15113805759637083
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.1509044443167025
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.15135712578656182
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.15109049889724702
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.15104303921644505
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.15100892937996171
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.1508189307442352
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.1509891896344283
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.1512687980480816
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.15143001984272683
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.15171613103487122
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.15179269502146375
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.15200458701751005
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.15188080404658574
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.15186112215121586
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.15193324938024344
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.15174184129996734
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.1513003044976638
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.1514922062618823
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.151515622343868
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.1517648219510361
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.15137501851451107
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.15130685278809214
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.15189750093434537
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.15190068877795163
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.1520364001220049
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.15204854078333954
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.15192731038074603
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.15229308596822652
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.15240588593814108
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.15234236067140494
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.1523752542131621
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.15270602566901073
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.15281546963973247
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.15313645734598763
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.1531523490169396
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.15280990471544953
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.15281914600304194
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.15293956420036278
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.1526828344166279
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.15274118757483982
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.15259084149318583
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.1525429462634244
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.15273221510534102
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.15225350729056766
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.1522432658469902
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.1523530654539572
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.15224789814264686
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.15256889834316498
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.15298231582749974
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.15286473151262817
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.15257584057482226
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.1522818394994314
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.15252960198803953
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.15234043442684672
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.15232840334546977
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.15242084491456676
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.1523182226692216
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.15215825671408356
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.15228404042621455
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.1521310332146558
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.15209383786213201
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.15227850035923282
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.15211242761823437
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.1520238869190216
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.1520331632050257
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.1521785563136649
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.1520860109012574
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.152067070552545
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.15214677762526732
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.1518972627534211
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.15183086546533037
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.15202477619163973
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.15192082057248302
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.15169110237448305
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.1516216856920544
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.15186429562142295
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.15178140225833742
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.15193103805934782
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.15210385689777986
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.15200335130835255
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.15196661331074338
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.1519767370569956
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.15213496739872628
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.1519677719679372
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.15185339713137444
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.1518049012540149
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.15174368361162172
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.1519334005729464
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.15196260357896488
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.15172694559326236
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.15167628576684938
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.1515648627495454
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.15136746326824288
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.15139986949582254
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.15133689554073873
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.1513422639316814
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.15122796151834197
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.1513983831278183
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.15149450059980155
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.15135991397481527
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.15130511892062645
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.15133865323900444
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.15136129363644413
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.15143419350638535
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.1514531204319862
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.15114043902851151
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.15113874604659422
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.15103097894840692
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.15110865813844343
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.15096125485952835
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.1509509075346381
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.15090748641876817
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.15080664520976186
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.15086229034832546
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.15074563314291564
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.15060107704609801
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.1503698920852013
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.15043719749710413
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.15037352281312147
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.15037334339888714
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.15031320130923292
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.15005040938248398
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.1500187974990062
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.14999045363149127
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.15000253527234958
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.1500894694643862
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.1499731526016555
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.15011637531733388
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.15018232492239852
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.15021938112861824
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.1501129005337134
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.15017174280832468
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.1499898081510952
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.14996368575554628
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.150039957988323
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.1501877147912374
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.15032328112107335
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.15030378930682514
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.15049660515040159
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.15046385557052508
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.15051133963878793
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.15047476690244205
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.15047644608307117
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.15027748789729142
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.1501282167087481
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.15018045599909796
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.15002300626096818
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.1499620408675317
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.1501605299257097
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.15015884965516946
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.15019647278032214
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.15017610543490575
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.15006607422761828
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.15010713255682656
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.149982168028752
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.15004483119408657
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.15014740243690824
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.15025811155909272
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.15032198158177462
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.15034439539477837
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.15032227176266746
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.15023762034461102
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.15024327839325582
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.15009408626291487
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.1499911207145294
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.15004454395581973
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.14986735235965043
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.1498404579261505
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.14998658435500187
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.15008732302364333
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.15006745446087985
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.15004678249614944
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.14999536010954115
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.1499734753623922
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.15001073220776298
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.1500245228346893
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.15002829619065053
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.1501000079026282
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.15010436214506626
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.1502198083519441
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.15014506525490895
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.15019789528208996
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.15015396817785795
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.15016390170369828
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.1501954503175689
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.15013086095995265
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.1500852559843371
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.1500292675921237
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.1499865965247154
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.1500479888868522
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.14988395908758753
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.14985111255655176
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.14980696681446917
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.14987103004081576
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.1498574042925611
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.14974888715646612
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.14970597933775695
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.14965138181641296
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.1496466251806571
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.14967103779201762
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.149686649868279
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.14961394659126667
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.14943797663418631
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.14938391433009562
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.1494133924799306
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.14943098544739605
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.14942602504656385
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.14937149506186906
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.14932715299504776
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.14927561369439332
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.14927682102493503
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.14939022896599857
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.14937947763905038
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.1493044060739604
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.1493593363388293
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.14931831551910738
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.14934608101737584
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.14937297025141322
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.14940012696066074
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.14939963974032114
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.1494129166693975
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.14940745315050488
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.14937823189711066
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.14948819460053192
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.14948870921572605
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.14945857738264762
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.1495772528513852
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.14950606110908582
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.14951237386156774
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.14947896845049874
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.14943681710897244
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.14955412230292278
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.14963130599686078
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.149551270801132
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.14951127115637064
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.149578103387998
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.14955599058494473
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.1494651655339477
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.14949505604803562
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.14958884329395833
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.14955841323101757
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.14955599298669953
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.14953057896836022
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.14956235282245228
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.14955191185174424
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.14955276597885822
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.14950449413970693
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.1494781637992288
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.14941308899752556
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.14946507231311398
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.1495047671815906
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.14950464011286013
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.14942239303213017
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.14941669577170932
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.14948893312506284
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.1495115210064202
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.14949495933434498
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.1494360604209586
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.14948029958177358
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.1494892966756568
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.1494915889148016
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.1494893887694406
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.14945675821308
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.14948458068645917
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.1494182304599168
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.14930525490450203
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.149242003016719
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.14922649184621214
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.1492843767007192
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.14932928218582245
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.1493265289499099
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.14925591150919595
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.14923620299843257
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.14930529376464102
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.14930717057238022
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.1492706668129307
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.1492574419936485
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.1491495279875477
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.1491559736430645
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.14910563931437182
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.14925107521097564
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.14921783167141173
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.1491693238359551
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.14917543707550435
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.1491937501668241
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.14920981219763027
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.14919827647250275
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.14916900005244935
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.1490780656678336
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.14903458362800784
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.14908377225087446
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.14910602050361146
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.1492031950735103
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.14921362509190197
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.14920814042345862
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.14922716842145145
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.14930569667722926
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.14927231713423822
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.14920591103533903
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.14918351528386992
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.1491769609859635
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.14910745054237112
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.14904030293717488
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.14908886621259662
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.14908039122005629
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.14909920311591282
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.1491286580896248
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.1491033140075239
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.14906680487297677
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.14910037731224635
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.149092660716144
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.1490802132891586
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.1491457273695558
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.14916133709748586
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.14918381082726287
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.1491732240434667
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.14921357482671738
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.14918148608037854
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.14910402876373968
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.1490652851975496
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.14903199526417943
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.14909804655195216
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.14899450344576812
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.14894000018958922
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.148861902577735
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.14889451451310814
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.1489393473554825
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.14893748649518104
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.14896647420067052
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.148936444783912
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.1489402211024141
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.1489521098007986
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.14893844313352242
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.14899205069375943
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.14901549175306403
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.1489173604611786
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.14894543178192335
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.14888095329130502
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.14889200910925865
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.148871498772331
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.14885641215600778
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.14881744714587852
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.14882384598402693
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.14877884803730765
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.14872915730834593
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.14869244887289895
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.1487481056475172
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.14873825224949852
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.14881342369608763
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.1488102462387433
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.14878732559171695
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.14876765792075428
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.1488227461991103
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.14890507922833224
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.14896458555729344
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.14894882298344903
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.1488983704926865
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.1490405574263138
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.14900991214173182
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.14906311835076066
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.14908515220569774
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.1490223798944877
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.1490006530305968
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.14899004920440562
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.14895424573844027
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.148943442249326
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.14888940843814444
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.14885694707259708
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.14883417225507803
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.14884510178231308
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.14882978205396621
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.148759865767686
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.1487545474402366
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.1486777131584869
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.1487526403517898
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.14879565813857848
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.14877408919813426
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.14864787752984054
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.14858064666729082
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.14856973532038212
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.1485156952930252
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.14850083797430078
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.1485139757461913
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.1485502145263586
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.14857416402865953
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.14855252159655227
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.14863328861870936
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.14865668307168448
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.14862171504232619
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.1486442446576518
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.14872661786796773
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.14867214371694898
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.14863923772972587
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.1486176189157989
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.1486287076483693
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.14863373857619205
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.14861895033365774
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.14858759877988195
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.14868212070154108
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.14865820332876772
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.148666938178209
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.14855359753766792
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.14857985976890759
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.1486073744393164
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.1486032492133425
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.1486331040490363
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.14867560630743828
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.14871967507641454
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.14874355055867358
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.14878285182699277
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.148882158589944
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.148882158589944 valid 0.22612600028514862
LOSS train 0.148882158589944 valid 0.19289927929639816
LOSS train 0.148882158589944 valid 0.19959150751431784
LOSS train 0.148882158589944 valid 0.18839484453201294
LOSS train 0.148882158589944 valid 0.18603741228580475
LOSS train 0.148882158589944 valid 0.19128741572300592
LOSS train 0.148882158589944 valid 0.1992187201976776
LOSS train 0.148882158589944 valid 0.19756554067134857
LOSS train 0.148882158589944 valid 0.1962913821140925
LOSS train 0.148882158589944 valid 0.20069582909345626
LOSS train 0.148882158589944 valid 0.19992206177928232
LOSS train 0.148882158589944 valid 0.19788388535380363
LOSS train 0.148882158589944 valid 0.1967899237687771
LOSS train 0.148882158589944 valid 0.19637407149587358
LOSS train 0.148882158589944 valid 0.19276690979798636
LOSS train 0.148882158589944 valid 0.194520378485322
LOSS train 0.148882158589944 valid 0.19594855869517608
LOSS train 0.148882158589944 valid 0.19511928160985312
LOSS train 0.148882158589944 valid 0.1971682608127594
LOSS train 0.148882158589944 valid 0.1984839506447315
LOSS train 0.148882158589944 valid 0.19835802274090902
LOSS train 0.148882158589944 valid 0.1970492648807439
LOSS train 0.148882158589944 valid 0.19700024892454562
LOSS train 0.148882158589944 valid 0.19671552317837873
LOSS train 0.148882158589944 valid 0.1950054222345352
LOSS train 0.148882158589944 valid 0.19493704059949288
LOSS train 0.148882158589944 valid 0.1948987504950276
LOSS train 0.148882158589944 valid 0.1958771704563073
LOSS train 0.148882158589944 valid 0.19629728074731498
LOSS train 0.148882158589944 valid 0.19712697764237722
LOSS train 0.148882158589944 valid 0.19811120533174084
LOSS train 0.148882158589944 valid 0.19743493106216192
LOSS train 0.148882158589944 valid 0.1979360833312526
LOSS train 0.148882158589944 valid 0.1974128717008759
LOSS train 0.148882158589944 valid 0.1993038352046694
LOSS train 0.148882158589944 valid 0.19940677243802282
LOSS train 0.148882158589944 valid 0.20041229958469803
LOSS train 0.148882158589944 valid 0.2007484298787619
LOSS train 0.148882158589944 valid 0.19990064241947272
LOSS train 0.148882158589944 valid 0.19996191188693047
LOSS train 0.148882158589944 valid 0.19964738008452626
LOSS train 0.148882158589944 valid 0.20042952043669565
LOSS train 0.148882158589944 valid 0.19982548092686853
LOSS train 0.148882158589944 valid 0.20045603608543222
LOSS train 0.148882158589944 valid 0.2002410888671875
LOSS train 0.148882158589944 valid 0.20101004676974338
LOSS train 0.148882158589944 valid 0.20136241107545
LOSS train 0.148882158589944 valid 0.20154793746769428
LOSS train 0.148882158589944 valid 0.20227362732498014
LOSS train 0.148882158589944 valid 0.2015260151028633
LOSS train 0.148882158589944 valid 0.20188292568805172
LOSS train 0.148882158589944 valid 0.20182087329717782
LOSS train 0.148882158589944 valid 0.20219175039597279
LOSS train 0.148882158589944 valid 0.20259666994765954
LOSS train 0.148882158589944 valid 0.20247774747284977
LOSS train 0.148882158589944 valid 0.20239058216767652
LOSS train 0.148882158589944 valid 0.20229542856676536
LOSS train 0.148882158589944 valid 0.20216639005932316
LOSS train 0.148882158589944 valid 0.20268451132006565
LOSS train 0.148882158589944 valid 0.20222129598259925
LOSS train 0.148882158589944 valid 0.2019234519512927
LOSS train 0.148882158589944 valid 0.20217480582575645
LOSS train 0.148882158589944 valid 0.20201212498876783
LOSS train 0.148882158589944 valid 0.20280101941898465
LOSS train 0.148882158589944 valid 0.20275250146022208
LOSS train 0.148882158589944 valid 0.20258177455627557
LOSS train 0.148882158589944 valid 0.20212518813005134
LOSS train 0.148882158589944 valid 0.20247537752284722
LOSS train 0.148882158589944 valid 0.20169161908004596
LOSS train 0.148882158589944 valid 0.20195175749914987
LOSS train 0.148882158589944 valid 0.20167522866960982
LOSS train 0.148882158589944 valid 0.20180775763259995
LOSS train 0.148882158589944 valid 0.20195682061045137
LOSS train 0.148882158589944 valid 0.20182655771841873
LOSS train 0.148882158589944 valid 0.2018961743513743
LOSS train 0.148882158589944 valid 0.20262486526840612
LOSS train 0.148882158589944 valid 0.20238685530501527
LOSS train 0.148882158589944 valid 0.20230422092553896
LOSS train 0.148882158589944 valid 0.2019053857915009
LOSS train 0.148882158589944 valid 0.20125613417476415
LOSS train 0.148882158589944 valid 0.2007060871448046
LOSS train 0.148882158589944 valid 0.20101748952051487
LOSS train 0.148882158589944 valid 0.20060606516269316
LOSS train 0.148882158589944 valid 0.2005823850631714
LOSS train 0.148882158589944 valid 0.1998843889026081
LOSS train 0.148882158589944 valid 0.19939437889775566
LOSS train 0.148882158589944 valid 0.19933498505203204
LOSS train 0.148882158589944 valid 0.19898875887421044
LOSS train 0.148882158589944 valid 0.19924913581167714
LOSS train 0.148882158589944 valid 0.19941750350925658
LOSS train 0.148882158589944 valid 0.19942823101531018
LOSS train 0.148882158589944 valid 0.19929765666956487
LOSS train 0.148882158589944 valid 0.1992122684755633
LOSS train 0.148882158589944 valid 0.199517641137255
LOSS train 0.148882158589944 valid 0.1990515654024325
LOSS train 0.148882158589944 valid 0.19916977873072028
LOSS train 0.148882158589944 valid 0.1991949076812292
LOSS train 0.148882158589944 valid 0.19948569700425986
LOSS train 0.148882158589944 valid 0.19967920160052752
LOSS train 0.148882158589944 valid 0.19972220107913016
LOSS train 0.148882158589944 valid 0.19992927026630627
LOSS train 0.148882158589944 valid 0.20044825956517576
LOSS train 0.148882158589944 valid 0.20033361003236863
LOSS train 0.148882158589944 valid 0.20022369806583112
LOSS train 0.148882158589944 valid 0.20041040636244276
LOSS train 0.148882158589944 valid 0.20076571151895342
LOSS train 0.148882158589944 valid 0.20044564267742299
LOSS train 0.148882158589944 valid 0.2005635923533528
LOSS train 0.148882158589944 valid 0.20111163780776734
LOSS train 0.148882158589944 valid 0.2011433255943385
LOSS train 0.148882158589944 valid 0.20107153648728723
LOSS train 0.148882158589944 valid 0.20088020818574087
LOSS train 0.148882158589944 valid 0.201052004520872
LOSS train 0.148882158589944 valid 0.20119199914890423
LOSS train 0.148882158589944 valid 0.20113715957040373
LOSS train 0.148882158589944 valid 0.20142499511611872
LOSS train 0.148882158589944 valid 0.20163001871516562
LOSS train 0.148882158589944 valid 0.20136594822851278
LOSS train 0.148882158589944 valid 0.2011974703113572
LOSS train 0.148882158589944 valid 0.20092141044636566
LOSS train 0.148882158589944 valid 0.20084589596622246
LOSS train 0.148882158589944 valid 0.20087226419175258
LOSS train 0.148882158589944 valid 0.20106074747031297
LOSS train 0.148882158589944 valid 0.20142188427909727
LOSS train 0.148882158589944 valid 0.20120993685722352
LOSS train 0.148882158589944 valid 0.2014009216948161
LOSS train 0.148882158589944 valid 0.20127348003424997
LOSS train 0.148882158589944 valid 0.20128181111067533
LOSS train 0.148882158589944 valid 0.20164358823798423
LOSS train 0.148882158589944 valid 0.20150272250175477
LOSS train 0.148882158589944 valid 0.2013619103049504
LOSS train 0.148882158589944 valid 0.20088804027799403
LOSS train 0.148882158589944 valid 0.20057475197135954
LOSS train 0.148882158589944 valid 0.20067897872693502
LOSS train 0.148882158589944 valid 0.2005385449639073
LOSS train 0.148882158589944 valid 0.20035582287784884
LOSS train 0.148882158589944 valid 0.2001084151276707
LOSS train 0.148882158589944 valid 0.19996286615513373
LOSS train 0.148882158589944 valid 0.19964182344700793
LOSS train 0.148882158589944 valid 0.19974224801574433
LOSS train 0.148882158589944 valid 0.19977606537071527
LOSS train 0.148882158589944 valid 0.1997401268339493
LOSS train 0.148882158589944 valid 0.1998298863222549
LOSS train 0.148882158589944 valid 0.199979138871034
LOSS train 0.148882158589944 valid 0.19990927597572064
LOSS train 0.148882158589944 valid 0.19997633238361306
LOSS train 0.148882158589944 valid 0.19987717939882862
LOSS train 0.148882158589944 valid 0.20079844908134356
LOSS train 0.148882158589944 valid 0.20088907346229426
LOSS train 0.148882158589944 valid 0.20069156487782797
LOSS train 0.148882158589944 valid 0.20099490841492912
LOSS train 0.148882158589944 valid 0.2007982545581303
LOSS train 0.148882158589944 valid 0.20079270638282004
LOSS train 0.148882158589944 valid 0.20078750931984418
LOSS train 0.148882158589944 valid 0.20068949489824234
LOSS train 0.148882158589944 valid 0.20076579762957034
LOSS train 0.148882158589944 valid 0.20083007093068142
LOSS train 0.148882158589944 valid 0.20084857205046883
LOSS train 0.148882158589944 valid 0.20082983946275412
LOSS train 0.148882158589944 valid 0.20071857878938318
LOSS train 0.148882158589944 valid 0.2006797565621619
LOSS train 0.148882158589944 valid 0.2003971333672971
LOSS train 0.148882158589944 valid 0.2001602495192019
LOSS train 0.148882158589944 valid 0.19997906703047635
LOSS train 0.148882158589944 valid 0.19998622021891854
LOSS train 0.148882158589944 valid 0.19985673818961683
LOSS train 0.148882158589944 valid 0.20002864863344294
LOSS train 0.148882158589944 valid 0.20004773734226114
LOSS train 0.148882158589944 valid 0.20018821028915382
LOSS train 0.148882158589944 valid 0.20028299645465963
LOSS train 0.148882158589944 valid 0.20022209694511012
LOSS train 0.148882158589944 valid 0.20009431667452635
LOSS train 0.148882158589944 valid 0.20022377130613161
LOSS train 0.148882158589944 valid 0.2001841746527573
LOSS train 0.148882158589944 valid 0.20006385854312353
LOSS train 0.148882158589944 valid 0.20014187405732545
LOSS train 0.148882158589944 valid 0.2001057342957642
LOSS train 0.148882158589944 valid 0.20016028932975918
LOSS train 0.148882158589944 valid 0.20006356800401676
LOSS train 0.148882158589944 valid 0.2000404592189524
LOSS train 0.148882158589944 valid 0.20009354925945977
LOSS train 0.148882158589944 valid 0.20001525399121609
LOSS train 0.148882158589944 valid 0.19995366004321094
LOSS train 0.148882158589944 valid 0.1999722613102716
LOSS train 0.148882158589944 valid 0.19978778982484663
LOSS train 0.148882158589944 valid 0.19983945386384122
LOSS train 0.148882158589944 valid 0.19967098828943017
LOSS train 0.148882158589944 valid 0.19955479306109408
LOSS train 0.148882158589944 valid 0.19954346372652307
LOSS train 0.148882158589944 valid 0.19965194048065887
LOSS train 0.148882158589944 valid 0.19960275963338883
LOSS train 0.148882158589944 valid 0.19962257193401456
LOSS train 0.148882158589944 valid 0.19953733983743993
LOSS train 0.148882158589944 valid 0.19935123007936575
LOSS train 0.148882158589944 valid 0.19909332746114486
LOSS train 0.148882158589944 valid 0.19912755048396635
LOSS train 0.148882158589944 valid 0.19930709648858472
LOSS train 0.148882158589944 valid 0.19917853974332714
LOSS train 0.148882158589944 valid 0.19922426133299592
LOSS train 0.148882158589944 valid 0.19915523871779442
LOSS train 0.148882158589944 valid 0.1991385482436982
LOSS train 0.148882158589944 valid 0.1990920588521674
LOSS train 0.148882158589944 valid 0.19912416792561854
LOSS train 0.148882158589944 valid 0.19931036737911842
LOSS train 0.148882158589944 valid 0.1991676316755574
LOSS train 0.148882158589944 valid 0.1992010398975854
LOSS train 0.148882158589944 valid 0.19922049509154427
LOSS train 0.148882158589944 valid 0.1991790717181105
LOSS train 0.148882158589944 valid 0.19918021153319965
LOSS train 0.148882158589944 valid 0.1992872526248296
LOSS train 0.148882158589944 valid 0.19926813089452083
LOSS train 0.148882158589944 valid 0.19924379814908189
LOSS train 0.148882158589944 valid 0.19923181482044183
LOSS train 0.148882158589944 valid 0.19913609792417455
LOSS train 0.148882158589944 valid 0.19904520185880883
LOSS train 0.148882158589944 valid 0.19890884020262295
LOSS train 0.148882158589944 valid 0.19867638144899624
LOSS train 0.148882158589944 valid 0.1986612813598519
LOSS train 0.148882158589944 valid 0.19878508409136506
LOSS train 0.148882158589944 valid 0.19878780428658832
LOSS train 0.148882158589944 valid 0.19872639338355258
LOSS train 0.148882158589944 valid 0.1987580391200813
LOSS train 0.148882158589944 valid 0.19882533021037355
LOSS train 0.148882158589944 valid 0.19889931347487227
LOSS train 0.148882158589944 valid 0.19912888162665898
LOSS train 0.148882158589944 valid 0.19929215404312167
LOSS train 0.148882158589944 valid 0.19940318318190553
LOSS train 0.148882158589944 valid 0.1994443107723144
LOSS train 0.148882158589944 valid 0.19940226189954832
LOSS train 0.148882158589944 valid 0.19945424069528994
LOSS train 0.148882158589944 valid 0.19954443577822153
LOSS train 0.148882158589944 valid 0.19951665061044283
LOSS train 0.148882158589944 valid 0.19949845023165444
LOSS train 0.148882158589944 valid 0.19948448871190733
LOSS train 0.148882158589944 valid 0.19961517814626084
LOSS train 0.148882158589944 valid 0.19949302816037404
LOSS train 0.148882158589944 valid 0.19950865095929254
LOSS train 0.148882158589944 valid 0.19941880472567902
LOSS train 0.148882158589944 valid 0.19935590091110772
LOSS train 0.148882158589944 valid 0.19938676425566276
LOSS train 0.148882158589944 valid 0.19944464072152293
LOSS train 0.148882158589944 valid 0.1992985155464204
LOSS train 0.148882158589944 valid 0.19949849366896438
LOSS train 0.148882158589944 valid 0.19959062856973195
LOSS train 0.148882158589944 valid 0.19965329522989234
LOSS train 0.148882158589944 valid 0.19953338620139333
LOSS train 0.148882158589944 valid 0.19962375095257393
LOSS train 0.148882158589944 valid 0.19960216354698904
LOSS train 0.148882158589944 valid 0.19956463300080662
LOSS train 0.148882158589944 valid 0.19955701965093614
LOSS train 0.148882158589944 valid 0.19950579967631762
LOSS train 0.148882158589944 valid 0.19960858749728355
LOSS train 0.148882158589944 valid 0.1995348814330082
LOSS train 0.148882158589944 valid 0.1994892126462591
LOSS train 0.148882158589944 valid 0.1994703756243575
LOSS train 0.148882158589944 valid 0.1994208680698648
LOSS train 0.148882158589944 valid 0.19930284913875712
LOSS train 0.148882158589944 valid 0.19949296168809713
LOSS train 0.148882158589944 valid 0.19949662495533932
LOSS train 0.148882158589944 valid 0.19950289399578022
LOSS train 0.148882158589944 valid 0.19964808768011144
LOSS train 0.148882158589944 valid 0.19961919419410576
LOSS train 0.148882158589944 valid 0.19966481733684757
LOSS train 0.148882158589944 valid 0.199687796221538
LOSS train 0.148882158589944 valid 0.1998358670270668
LOSS train 0.148882158589944 valid 0.19983950559806107
LOSS train 0.148882158589944 valid 0.1998692238040631
LOSS train 0.148882158589944 valid 0.19994081951566597
LOSS train 0.148882158589944 valid 0.20012540040627733
LOSS train 0.148882158589944 valid 0.2002179029915068
LOSS train 0.148882158589944 valid 0.2003234396671457
LOSS train 0.148882158589944 valid 0.20071910425801487
LOSS train 0.148882158589944 valid 0.20084640105347057
LOSS train 0.148882158589944 valid 0.2008499601026521
LOSS train 0.148882158589944 valid 0.20078572923486884
LOSS train 0.148882158589944 valid 0.20072581251894217
LOSS train 0.148882158589944 valid 0.20065869993466331
LOSS train 0.148882158589944 valid 0.20045890048039045
LOSS train 0.148882158589944 valid 0.20046366783239508
LOSS train 0.148882158589944 valid 0.2003557076411588
LOSS train 0.148882158589944 valid 0.20031590653696094
LOSS train 0.148882158589944 valid 0.20011655198978195
LOSS train 0.148882158589944 valid 0.2001413203275667
LOSS train 0.148882158589944 valid 0.20016823131853426
LOSS train 0.148882158589944 valid 0.20019915490819698
LOSS train 0.148882158589944 valid 0.20021498276220334
LOSS train 0.148882158589944 valid 0.20019493560965468
LOSS train 0.148882158589944 valid 0.20021100590626398
LOSS train 0.148882158589944 valid 0.20018300934852612
LOSS train 0.148882158589944 valid 0.2003221528283481
LOSS train 0.148882158589944 valid 0.20010702378561407
LOSS train 0.148882158589944 valid 0.2001633154202814
LOSS train 0.148882158589944 valid 0.20018942684647167
LOSS train 0.148882158589944 valid 0.20030665078333446
LOSS train 0.148882158589944 valid 0.20044838932611175
LOSS train 0.148882158589944 valid 0.20035586717563705
LOSS train 0.148882158589944 valid 0.20035100143766563
LOSS train 0.148882158589944 valid 0.20031843769470317
LOSS train 0.148882158589944 valid 0.2003555090531059
LOSS train 0.148882158589944 valid 0.20036458800236384
LOSS train 0.148882158589944 valid 0.20041160072599137
LOSS train 0.148882158589944 valid 0.20034638599844168
LOSS train 0.148882158589944 valid 0.20042454857047243
LOSS train 0.148882158589944 valid 0.20051194428417243
LOSS train 0.148882158589944 valid 0.20040233403932853
LOSS train 0.148882158589944 valid 0.20036250011983261
LOSS train 0.148882158589944 valid 0.20040060914688856
LOSS train 0.148882158589944 valid 0.20036031267085633
LOSS train 0.148882158589944 valid 0.20039031608783697
LOSS train 0.148882158589944 valid 0.20032358155135185
LOSS train 0.148882158589944 valid 0.20021185024949897
LOSS train 0.148882158589944 valid 0.20032114186921182
LOSS train 0.148882158589944 valid 0.20039037631723447
LOSS train 0.148882158589944 valid 0.20045203786746713
LOSS train 0.148882158589944 valid 0.20037818021244472
LOSS train 0.148882158589944 valid 0.20033430864539328
LOSS train 0.148882158589944 valid 0.2004417309813695
LOSS train 0.148882158589944 valid 0.2005667598367487
LOSS train 0.148882158589944 valid 0.200603348240957
LOSS train 0.148882158589944 valid 0.20048587201163173
LOSS train 0.148882158589944 valid 0.20058705813231126
LOSS train 0.148882158589944 valid 0.20057949664429847
LOSS train 0.148882158589944 valid 0.2005634084113242
LOSS train 0.148882158589944 valid 0.20065008384394056
LOSS train 0.148882158589944 valid 0.20063578193004314
LOSS train 0.148882158589944 valid 0.20076881989013928
LOSS train 0.148882158589944 valid 0.2008736868608253
LOSS train 0.148882158589944 valid 0.2007647273017139
LOSS train 0.148882158589944 valid 0.20095037055232967
LOSS train 0.148882158589944 valid 0.20084204429929906
LOSS train 0.148882158589944 valid 0.20071125678785617
LOSS train 0.148882158589944 valid 0.2006675198404904
LOSS train 0.148882158589944 valid 0.20067088829504476
LOSS train 0.148882158589944 valid 0.20077989991018158
LOSS train 0.148882158589944 valid 0.20076117346535868
LOSS train 0.148882158589944 valid 0.20085290189655053
LOSS train 0.148882158589944 valid 0.20091896913171875
LOSS train 0.148882158589944 valid 0.20088831172186947
LOSS train 0.148882158589944 valid 0.2008841278630372
LOSS train 0.148882158589944 valid 0.20086237718077266
LOSS train 0.148882158589944 valid 0.20076938384264445
LOSS train 0.148882158589944 valid 0.20069239358281532
LOSS train 0.148882158589944 valid 0.2006837272522401
LOSS train 0.148882158589944 valid 0.20095109909253064
LOSS train 0.148882158589944 valid 0.20101374633934188
LOSS train 0.148882158589944 valid 0.20103617481930408
LOSS train 0.148882158589944 valid 0.20092599377336695
LOSS train 0.148882158589944 valid 0.2008649661880115
LOSS train 0.148882158589944 valid 0.2009028083188486
LOSS train 0.148882158589944 valid 0.20083199905497687
LOSS train 0.148882158589944 valid 0.20074167189604877
LOSS train 0.148882158589944 valid 0.2007360739040781
LOSS train 0.148882158589944 valid 0.2006890021538937
LOSS train 0.148882158589944 valid 0.20066483971257668
LOSS train 0.148882158589944 valid 0.20079504276665164
LOSS train 0.148882158589944 valid 0.20079835102464375
LOSS train 0.148882158589944 valid 0.2008275384662532
LOSS train 0.148882158589944 valid 0.20074828764080335
LOSS train 0.148882158589944 valid 0.2007533594509353
LOSS train 0.148882158589944 valid 0.20069539331727557
LOSS train 0.148882158589944 valid 0.20070150618407864
LOSS train 0.148882158589944 valid 0.2007890311146968
LOSS train 0.148882158589944 valid 0.20062950561361864
LOSS train 0.148882158589944 valid 0.20067678752181295
LOSS train 0.148882158589944 valid 0.20067780246473338
LOSS train 0.148882158589944 valid 0.20066498574174818
LOSS train 0.148882158589944 valid 0.20053038386785366
LOSS train 0.148882158589944 valid 0.20052016129636246
LOSS train 0.148882158589944 valid 0.20055599692391185
EPOCH 5:
  batch 1 loss: 0.13159950077533722
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.13258342444896698
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.13371956845124564
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.14185044541954994
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.14767620861530303
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.14771719028552374
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.13888295739889145
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.13936108816415071
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.13925358073578942
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.1381604053080082
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.1383697654713284
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.1374887110044559
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.13783966176784956
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.13670111926538603
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.1372446989019712
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.13845739187672734
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.13706743278924158
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.13905699799458185
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.13802521871893028
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.13740103468298911
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.13792583914030165
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.13795257901603525
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.1382823567027631
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.14005351377030215
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.13973341941833495
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.14023727178573608
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.14115895054958486
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.14081126770802907
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.14077108635984617
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.1414967472354571
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.14201543311918935
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.14158581336960196
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.14126241839293277
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.1420646664850852
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.1432477048465184
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.14323954905072847
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.14349618030560984
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.14287850907758662
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.14341722600735152
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.14328082147985696
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.14342446788782026
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.1435387097299099
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.1444514260389084
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.14385883205316283
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.14471115602387322
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.14472018441428308
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.1444168081308933
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.1438382047538956
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.1438636326668214
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.14394259780645371
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.14391091904219458
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.14434594116531885
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.1445436733511259
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.14492548533059932
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.14510656215927817
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.14519378995256765
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.1453954493790342
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.1453661869825988
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.14599986147072355
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.14594787309567134
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.14594678483048423
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.14579413903336372
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.14627545788174584
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.14602954522706568
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.14605242976775537
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.14597553511460623
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.14587001280108494
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.14598542890127966
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.1462596659211145
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.14646726271935873
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.146782410186781
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.14679796372850737
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.1470251158900457
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.14686551891468666
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.14680752178033193
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.14688592502161077
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.14671268556025122
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.14627542824317247
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.14646627159812783
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.14648983981460334
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.14681043871390967
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.14644485439469174
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.14636918220175318
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.1468946167400905
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.1469648589106167
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.14718906910613525
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.1471720514283783
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.1471564844250679
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.14754723497990813
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.14763926797442967
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.14761725100842152
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.14765203468825505
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.14791342279603403
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.14799841001946876
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.14834185681845014
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.1484203270326058
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.14809120415719515
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.14807636830575613
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.14820205119222102
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.14797361634671688
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.14806383899827996
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.14791113160112324
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.14784641860468875
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.14806490312688625
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.14762634053116752
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.14764002021753564
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.14778034583987476
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.1476682704631929
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.14798579631595438
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.14840508103370667
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.1482911272360398
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.14800913904660515
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.14769719679534962
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.14791819250635935
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.14767759189657542
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.14762681562068133
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.14772404749424028
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.1476292214277437
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.14748399992700384
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.14762518039594094
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.1474632210105904
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.14745333161754687
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.14764997865853272
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.14754187898530113
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.1474503932595253
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.1474414255529169
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.14757248530472358
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.14746694505447522
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.1474488999607951
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.14750766633794857
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.14728107957439568
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.14715113005403316
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.14736573465336533
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.14727637138384492
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.14702683273288938
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.1469468982232844
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.1471923208475983
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.14711919961416203
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.1472640053080998
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.1474727265004601
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.14736845449987032
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.1473233548163528
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.14733062689746176
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.14746056378094685
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.1472837120808404
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.14721028185258173
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.14717877676495078
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.14711873701496705
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.14731943122292526
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.1473584354420503
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.14712808705520947
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.14705049378895446
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.14696317945235696
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.146750527059103
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.14680241913564743
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.14671340947731948
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.14670141136190692
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.1465948456073109
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.1467720440723611
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.14688455136492848
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.14674333765269806
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.14671293646097183
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.14674845676114953
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.14677000045776367
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.14686506860183948
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.14685884073197122
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.14654021652158863
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.1465460355615332
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.1464088502720263
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.14648164519492318
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.1463380378415013
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.14633794050923613
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.14630849039278968
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.14617456264536957
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.1462524288892746
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.14614580512385478
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.14599364141454804
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.14577658164702104
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.14586546808980697
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.14578990331954425
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.14577608485577515
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.14570370185506212
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.1454540866075969
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.14543944568899664
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.1454435948987265
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.14542631384345792
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.14551092178266953
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.1453927993932937
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.14549372893161874
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.1455633081103626
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.14559601676401668
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.1454785868215064
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.14552678160099167
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.14533191550638258
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.14531502975867344
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.14541541093162128
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.14555835239778314
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.14572436181884824
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.14573325666051415
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.14588421456515788
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.14586587176097565
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.14589758438639122
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.14583352861439652
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.1458602107915224
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.1456730586363048
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.14554027659655774
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.1455839016803221
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.14542419721300787
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.14537534338720678
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.14559583621365682
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.14557940565861796
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.1456287163749056
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.14564232682118394
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.145516761831034
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.1456007433492084
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.14546777249348383
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.14554225577874116
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.1456676216778952
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.14576777683137215
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.1458343373442238
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.14586707811414926
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.14586872355760755
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.14579205886531837
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.14580987197613077
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.14566205594274734
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.14557278340896673
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.14564020780739806
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.14547299139463066
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.14544224905004668
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.14560984680834024
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.14574084250312863
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.14570244559054746
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.14569018266359623
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.14562548025168925
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.14560595322796638
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.14567306318904383
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.14565733895648883
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.1456361530395616
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.14568848007019594
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.14568705015505354
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.14579179818081164
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.14571622318842195
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.14578414781961913
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.1457365844398737
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.145755657279978
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.14577278255568288
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.14570800848456048
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.14565930410378403
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.14559860482632395
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.14556690707802772
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.14564398509928905
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.14547329251137045
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.1454490147265992
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.1454068633808395
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.14549068682918362
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.14546453664661385
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.14535483657038165
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.14530436761041945
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.145264774186961
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.1452532088240752
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.145270219964771
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.1452849039537761
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.14521328529817523
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.14505238712511279
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.1449962578854471
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.14503319796762967
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.14503427787443227
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.14503469551677134
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.14499707547705412
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.14496729992054128
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.14491656477600887
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.14491397372501738
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.1450364296034579
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.14501645867406887
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.1449357118118893
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.14501270821884923
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.14495133877553665
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.14498932892791658
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.1450014174999302
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.14503392702234644
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.14504803497065852
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.14506452168661652
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.145072809298763
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.14504873865402082
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.14516311379378302
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.14517814960408879
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.14516158165728174
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.14528364245779812
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.14521309421446085
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.1452185246194231
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.14520273280819668
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.1451709648874933
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.14529830250418635
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.1453807653263718
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.1452990784736003
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.1452638471398402
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.14533166650068077
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.14531108304458176
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.14523182749548882
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.14526445612311364
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.14536731598583172
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.14531928873220026
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.14532171883205375
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.14529771965585256
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.14532636443122487
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.14531358477531695
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.14529417901745836
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.14524150794589674
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.1452154337781147
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.14514908571877788
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.14520427845777806
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.1452472910332756
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.14524500431439366
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.14516204756915949
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.14516707857449848
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.1452394998337649
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.14525765048968678
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.14524747848323305
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.14519982493035846
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.14525054143741728
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.14526970392075655
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.14526196032392313
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.1452715520519221
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.14524717803722548
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.14529079822393565
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.14521456601246735
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.14509308303988308
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.145043510920936
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.1450364886129156
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.1450992700502728
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.14515316326542563
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.14515066570995083
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.14507621085948055
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.14506847676848936
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.14514645931880865
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.14514509018599278
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.14510380901197298
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.14509162586733435
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.14497277336222583
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.14498766816275963
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.14493419205950153
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.14510883109872802
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.14508040603413178
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.1450525616014073
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.14506385224884835
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.14508677702504774
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.14510712137053955
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.14508598329948968
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.14505884864695776
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.1449692259303161
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.14493137099209674
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.14497346225702626
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.14500072410609838
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.14510729102849287
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.14512711580790263
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.1451075624758273
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.14513884762338564
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.1452312260741295
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.14522399649563605
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.14516051134301555
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.14512327936712724
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.14512685892167013
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.14505489749356734
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.144984136898439
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.14504159778764802
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.1450370915763365
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.14506698470836765
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.14509125636971515
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.14504957570616145
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.1450067116602047
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.14504502557037333
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.1450289047213011
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.14500631208234435
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.1450738187700032
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.14509651716550193
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.14513413612037263
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.14512863083290484
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.1451639420692883
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.1451402342728386
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.14506336183924423
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.14503451520845959
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.14500450323389463
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.14508367331781213
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.1449825784075074
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.14494033217817157
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.1448678218909187
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.14488619236752046
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.14493499145142197
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.1449250517015592
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.14494233217376928
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.1449013469964647
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.1448975728672682
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.1449009110492301
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.14488588676310432
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.1449301736075667
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.14495958600456665
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.14486474562577095
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.14490120178416147
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.1448405071682201
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.14483668494969607
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.14482077588315617
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.1448008541518183
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.14476303290344644
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.14476534580387693
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.1447211292790778
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.1446774972263228
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.1446300223286673
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.14468425030217452
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.14466979640620262
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.14473429176865554
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.1447205244769725
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.14470150762970008
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.14467767313952595
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.14473375976805525
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.14480971123080655
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.14487349474802613
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.14486053364454127
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.14481552697397304
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.14495674209549206
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.14492342567869596
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.14498863123233416
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.1450150884793833
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.14495967809401505
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.1449369836493202
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.14493808681474013
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.14490731027274625
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.14489413206345583
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.14484293476861215
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.14480451131478334
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.14479126722313637
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.14480699958353196
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.1447959667140687
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.1447281942081231
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.14472691529357487
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.1446596417618894
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.14472480624093922
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.14477716860291068
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.1447458735679927
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.14462643783700765
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.14456185849214143
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.1445533448910497
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.14448716312781718
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.14446982213256052
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.14448091441446598
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.14450656527213837
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.14453654622684145
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.14452267973214988
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.14460582163051836
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.14462895153758254
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.14459215538369286
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.14462068455578747
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.1446856095134157
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.14463002433708436
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.1445902529529538
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.14456585481926634
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.14457835404104308
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.14458675930353954
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.14457620433064006
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.14456147650228346
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.14465142786502838
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.14463612046133154
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.14463420621889495
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.1445239765919568
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.1445548934540872
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.14458460785368438
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.14458751138059878
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.1446123434274559
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.14466022754199484
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.1447173080909481
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.1447480909367825
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.14477995348077924
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.1448729838860237
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.1448729838860237 valid 0.22765547037124634
LOSS train 0.1448729838860237 valid 0.1927604377269745
LOSS train 0.1448729838860237 valid 0.1985856592655182
LOSS train 0.1448729838860237 valid 0.18680740147829056
LOSS train 0.1448729838860237 valid 0.1838595151901245
LOSS train 0.1448729838860237 valid 0.18933150172233582
LOSS train 0.1448729838860237 valid 0.19720680798803056
LOSS train 0.1448729838860237 valid 0.19571672566235065
LOSS train 0.1448729838860237 valid 0.1944196985827552
LOSS train 0.1448729838860237 valid 0.1980646461248398
LOSS train 0.1448729838860237 valid 0.19714429703625766
LOSS train 0.1448729838860237 valid 0.19507267822821936
LOSS train 0.1448729838860237 valid 0.19387789185230547
LOSS train 0.1448729838860237 valid 0.19355758173125132
LOSS train 0.1448729838860237 valid 0.1899534543355306
LOSS train 0.1448729838860237 valid 0.1915645245462656
LOSS train 0.1448729838860237 valid 0.19325209540479324
LOSS train 0.1448729838860237 valid 0.19234043608109155
LOSS train 0.1448729838860237 valid 0.19437608044398458
LOSS train 0.1448729838860237 valid 0.19569042697548866
LOSS train 0.1448729838860237 valid 0.1956117373137247
LOSS train 0.1448729838860237 valid 0.19431883841753006
LOSS train 0.1448729838860237 valid 0.19424377122650976
LOSS train 0.1448729838860237 valid 0.1939621747781833
LOSS train 0.1448729838860237 valid 0.19222348868846895
LOSS train 0.1448729838860237 valid 0.19211162684055474
LOSS train 0.1448729838860237 valid 0.19205260221604947
LOSS train 0.1448729838860237 valid 0.19294357033712523
LOSS train 0.1448729838860237 valid 0.19326566930474906
LOSS train 0.1448729838860237 valid 0.19405132234096528
LOSS train 0.1448729838860237 valid 0.19504718674767402
LOSS train 0.1448729838860237 valid 0.19434191100299358
LOSS train 0.1448729838860237 valid 0.1947804851965471
LOSS train 0.1448729838860237 valid 0.1942616030573845
LOSS train 0.1448729838860237 valid 0.1961622659649168
LOSS train 0.1448729838860237 valid 0.1962377168238163
LOSS train 0.1448729838860237 valid 0.1972698935785809
LOSS train 0.1448729838860237 valid 0.19761770453892255
LOSS train 0.1448729838860237 valid 0.1967514000641994
LOSS train 0.1448729838860237 valid 0.19678845815360546
LOSS train 0.1448729838860237 valid 0.19644676003514266
LOSS train 0.1448729838860237 valid 0.1971858934987159
LOSS train 0.1448729838860237 valid 0.1965915817160939
LOSS train 0.1448729838860237 valid 0.19717278297651897
LOSS train 0.1448729838860237 valid 0.19701449043220945
LOSS train 0.1448729838860237 valid 0.19779272377490997
LOSS train 0.1448729838860237 valid 0.19813959300518036
LOSS train 0.1448729838860237 valid 0.1983376887316505
LOSS train 0.1448729838860237 valid 0.1990962037626578
LOSS train 0.1448729838860237 valid 0.19837388128042222
LOSS train 0.1448729838860237 valid 0.1987745390803206
LOSS train 0.1448729838860237 valid 0.1987219601869583
LOSS train 0.1448729838860237 valid 0.19907716535172373
LOSS train 0.1448729838860237 valid 0.19945614481413806
LOSS train 0.1448729838860237 valid 0.19941532205451618
LOSS train 0.1448729838860237 valid 0.1993685801114355
LOSS train 0.1448729838860237 valid 0.19931963870399877
LOSS train 0.1448729838860237 valid 0.19920795410871506
LOSS train 0.1448729838860237 valid 0.19975279397883658
LOSS train 0.1448729838860237 valid 0.19928818891445796
LOSS train 0.1448729838860237 valid 0.1989753055767935
LOSS train 0.1448729838860237 valid 0.19917521842064395
LOSS train 0.1448729838860237 valid 0.19899410364173709
LOSS train 0.1448729838860237 valid 0.19977891235612333
LOSS train 0.1448729838860237 valid 0.1997319228374041
LOSS train 0.1448729838860237 valid 0.199569227569031
LOSS train 0.1448729838860237 valid 0.1991288368381671
LOSS train 0.1448729838860237 valid 0.19948607266825788
LOSS train 0.1448729838860237 valid 0.19869285886702331
LOSS train 0.1448729838860237 valid 0.1989743132676397
LOSS train 0.1448729838860237 valid 0.1987200096039705
LOSS train 0.1448729838860237 valid 0.19883545260462496
LOSS train 0.1448729838860237 valid 0.19897715094154828
LOSS train 0.1448729838860237 valid 0.1988452304456685
LOSS train 0.1448729838860237 valid 0.19893126130104066
LOSS train 0.1448729838860237 valid 0.19970279069323288
LOSS train 0.1448729838860237 valid 0.19945596642308422
LOSS train 0.1448729838860237 valid 0.1993716076398507
LOSS train 0.1448729838860237 valid 0.19895678079580958
LOSS train 0.1448729838860237 valid 0.1983422489836812
LOSS train 0.1448729838860237 valid 0.1977881034951151
LOSS train 0.1448729838860237 valid 0.198131675763828
LOSS train 0.1448729838860237 valid 0.19769875759101777
LOSS train 0.1448729838860237 valid 0.19765976763197354
LOSS train 0.1448729838860237 valid 0.19693498366019305
LOSS train 0.1448729838860237 valid 0.1964371271951254
LOSS train 0.1448729838860237 valid 0.1963698343298901
LOSS train 0.1448729838860237 valid 0.19602870077572085
LOSS train 0.1448729838860237 valid 0.1963083330499992
LOSS train 0.1448729838860237 valid 0.19647367497285206
LOSS train 0.1448729838860237 valid 0.19651193016178006
LOSS train 0.1448729838860237 valid 0.1963844929376374
LOSS train 0.1448729838860237 valid 0.19631498138750753
LOSS train 0.1448729838860237 valid 0.19662174788561274
LOSS train 0.1448729838860237 valid 0.1961467350784101
LOSS train 0.1448729838860237 valid 0.19626510546853146
LOSS train 0.1448729838860237 valid 0.19626865251777098
LOSS train 0.1448729838860237 valid 0.19656009470321695
LOSS train 0.1448729838860237 valid 0.19673117694228587
LOSS train 0.1448729838860237 valid 0.19675632938742638
LOSS train 0.1448729838860237 valid 0.19698045321620336
LOSS train 0.1448729838860237 valid 0.19749538702707664
LOSS train 0.1448729838860237 valid 0.1973509795746757
LOSS train 0.1448729838860237 valid 0.1972969641479162
LOSS train 0.1448729838860237 valid 0.19749964418865384
LOSS train 0.1448729838860237 valid 0.19786544228499792
LOSS train 0.1448729838860237 valid 0.19754006511697145
LOSS train 0.1448729838860237 valid 0.197644150643437
LOSS train 0.1448729838860237 valid 0.19819662439713784
LOSS train 0.1448729838860237 valid 0.19823029935359954
LOSS train 0.1448729838860237 valid 0.1981480141749253
LOSS train 0.1448729838860237 valid 0.1979493967124394
LOSS train 0.1448729838860237 valid 0.19810011851049103
LOSS train 0.1448729838860237 valid 0.19825633602184162
LOSS train 0.1448729838860237 valid 0.19821155330409174
LOSS train 0.1448729838860237 valid 0.1985328438980826
LOSS train 0.1448729838860237 valid 0.19871540329395196
LOSS train 0.1448729838860237 valid 0.19845207780599594
LOSS train 0.1448729838860237 valid 0.19827952192110174
LOSS train 0.1448729838860237 valid 0.19798113517463206
LOSS train 0.1448729838860237 valid 0.19788099788437205
LOSS train 0.1448729838860237 valid 0.19791081352312057
LOSS train 0.1448729838860237 valid 0.1980812581089454
LOSS train 0.1448729838860237 valid 0.19844192278481299
LOSS train 0.1448729838860237 valid 0.1982277626991272
LOSS train 0.1448729838860237 valid 0.19841200822875613
LOSS train 0.1448729838860237 valid 0.198274999152957
LOSS train 0.1448729838860237 valid 0.19828869914636016
LOSS train 0.1448729838860237 valid 0.19865247537923414
LOSS train 0.1448729838860237 valid 0.1984855789404649
LOSS train 0.1448729838860237 valid 0.1983195463664659
LOSS train 0.1448729838860237 valid 0.19785859955079627
LOSS train 0.1448729838860237 valid 0.19753963349008918
LOSS train 0.1448729838860237 valid 0.19766452085615985
LOSS train 0.1448729838860237 valid 0.19751888745360904
LOSS train 0.1448729838860237 valid 0.19732855841079178
LOSS train 0.1448729838860237 valid 0.1970674693584442
LOSS train 0.1448729838860237 valid 0.1969195389445277
LOSS train 0.1448729838860237 valid 0.1966121143145527
LOSS train 0.1448729838860237 valid 0.1967059655913285
LOSS train 0.1448729838860237 valid 0.1967461514980235
LOSS train 0.1448729838860237 valid 0.1967165523641546
LOSS train 0.1448729838860237 valid 0.19680603484173756
LOSS train 0.1448729838860237 valid 0.1969583366687099
LOSS train 0.1448729838860237 valid 0.19688794664267836
LOSS train 0.1448729838860237 valid 0.19694693560061388
LOSS train 0.1448729838860237 valid 0.19683352356054346
LOSS train 0.1448729838860237 valid 0.1977699890330031
LOSS train 0.1448729838860237 valid 0.1978605723621061
LOSS train 0.1448729838860237 valid 0.19766277929147086
LOSS train 0.1448729838860237 valid 0.197970512292243
LOSS train 0.1448729838860237 valid 0.19778341349018247
LOSS train 0.1448729838860237 valid 0.1977816451998318
LOSS train 0.1448729838860237 valid 0.1977785283868963
LOSS train 0.1448729838860237 valid 0.1976716524170291
LOSS train 0.1448729838860237 valid 0.19773706602744567
LOSS train 0.1448729838860237 valid 0.19780189036184057
LOSS train 0.1448729838860237 valid 0.19781417258178133
LOSS train 0.1448729838860237 valid 0.1978073166976185
LOSS train 0.1448729838860237 valid 0.19769110586494207
LOSS train 0.1448729838860237 valid 0.1976385793145399
LOSS train 0.1448729838860237 valid 0.1973533747188839
LOSS train 0.1448729838860237 valid 0.19711832144509064
LOSS train 0.1448729838860237 valid 0.19690781141199717
LOSS train 0.1448729838860237 valid 0.19690205066493063
LOSS train 0.1448729838860237 valid 0.19678747016622358
LOSS train 0.1448729838860237 valid 0.19697578251361847
LOSS train 0.1448729838860237 valid 0.19697999395430088
LOSS train 0.1448729838860237 valid 0.19713105693371338
LOSS train 0.1448729838860237 valid 0.1972123532610781
LOSS train 0.1448729838860237 valid 0.19714658755308007
LOSS train 0.1448729838860237 valid 0.19702320987748545
LOSS train 0.1448729838860237 valid 0.19713975101537098
LOSS train 0.1448729838860237 valid 0.1970865797551199
LOSS train 0.1448729838860237 valid 0.19696134439536503
LOSS train 0.1448729838860237 valid 0.19704012547365643
LOSS train 0.1448729838860237 valid 0.19698504952050871
LOSS train 0.1448729838860237 valid 0.1970460680261087
LOSS train 0.1448729838860237 valid 0.19693423907517055
LOSS train 0.1448729838860237 valid 0.19691819772124292
LOSS train 0.1448729838860237 valid 0.19696497546704433
LOSS train 0.1448729838860237 valid 0.19688324867696552
LOSS train 0.1448729838860237 valid 0.19683659149975072
LOSS train 0.1448729838860237 valid 0.1968608075833839
LOSS train 0.1448729838860237 valid 0.1966841888588828
LOSS train 0.1448729838860237 valid 0.19673898151164415
LOSS train 0.1448729838860237 valid 0.19657775297521907
LOSS train 0.1448729838860237 valid 0.19645898329450728
LOSS train 0.1448729838860237 valid 0.196441603479562
LOSS train 0.1448729838860237 valid 0.19654205559115662
LOSS train 0.1448729838860237 valid 0.1964883738007221
LOSS train 0.1448729838860237 valid 0.1965068472394099
LOSS train 0.1448729838860237 valid 0.19640069481931202
LOSS train 0.1448729838860237 valid 0.1962126531090933
LOSS train 0.1448729838860237 valid 0.19594740179868844
LOSS train 0.1448729838860237 valid 0.19597848763271253
LOSS train 0.1448729838860237 valid 0.19615051982366494
LOSS train 0.1448729838860237 valid 0.19603360007808665
LOSS train 0.1448729838860237 valid 0.19608183624456876
LOSS train 0.1448729838860237 valid 0.19601248182356357
LOSS train 0.1448729838860237 valid 0.1960002398164711
LOSS train 0.1448729838860237 valid 0.1959544827442358
LOSS train 0.1448729838860237 valid 0.19599219518048422
LOSS train 0.1448729838860237 valid 0.19619080563094102
LOSS train 0.1448729838860237 valid 0.19604137459906137
LOSS train 0.1448729838860237 valid 0.19607552748571322
LOSS train 0.1448729838860237 valid 0.1960963944450093
LOSS train 0.1448729838860237 valid 0.19605914582140171
LOSS train 0.1448729838860237 valid 0.19605907541142698
LOSS train 0.1448729838860237 valid 0.19615804524648758
LOSS train 0.1448729838860237 valid 0.1961424920654975
LOSS train 0.1448729838860237 valid 0.1961308627758386
LOSS train 0.1448729838860237 valid 0.19612744667440513
LOSS train 0.1448729838860237 valid 0.19603831926795923
LOSS train 0.1448729838860237 valid 0.195949539334275
LOSS train 0.1448729838860237 valid 0.19580579114456972
LOSS train 0.1448729838860237 valid 0.19558125768663698
LOSS train 0.1448729838860237 valid 0.19557024620541738
LOSS train 0.1448729838860237 valid 0.1956899607698667
LOSS train 0.1448729838860237 valid 0.1956873966888948
LOSS train 0.1448729838860237 valid 0.19561979929785922
LOSS train 0.1448729838860237 valid 0.19565880902715632
LOSS train 0.1448729838860237 valid 0.19572080043666565
LOSS train 0.1448729838860237 valid 0.19579429824703506
LOSS train 0.1448729838860237 valid 0.19602827886740368
LOSS train 0.1448729838860237 valid 0.196191679125866
LOSS train 0.1448729838860237 valid 0.1962908320621247
LOSS train 0.1448729838860237 valid 0.196328536787054
LOSS train 0.1448729838860237 valid 0.19628218252325683
LOSS train 0.1448729838860237 valid 0.1963451458060223
LOSS train 0.1448729838860237 valid 0.19643409221203295
LOSS train 0.1448729838860237 valid 0.19640306017265238
LOSS train 0.1448729838860237 valid 0.19638707837321728
LOSS train 0.1448729838860237 valid 0.19636938504428944
LOSS train 0.1448729838860237 valid 0.1964913528650365
LOSS train 0.1448729838860237 valid 0.19637035306985096
LOSS train 0.1448729838860237 valid 0.19638269793886676
LOSS train 0.1448729838860237 valid 0.1962988717841501
LOSS train 0.1448729838860237 valid 0.19623491648101407
LOSS train 0.1448729838860237 valid 0.19627821358541647
LOSS train 0.1448729838860237 valid 0.1963356278249337
LOSS train 0.1448729838860237 valid 0.19619439591553586
LOSS train 0.1448729838860237 valid 0.19640081225599282
LOSS train 0.1448729838860237 valid 0.19648827355904658
LOSS train 0.1448729838860237 valid 0.19655686775032355
LOSS train 0.1448729838860237 valid 0.19643384836068967
LOSS train 0.1448729838860237 valid 0.19652381708264832
LOSS train 0.1448729838860237 valid 0.1964900618599307
LOSS train 0.1448729838860237 valid 0.19645330585150356
LOSS train 0.1448729838860237 valid 0.19644623136520387
LOSS train 0.1448729838860237 valid 0.19639146464516918
LOSS train 0.1448729838860237 valid 0.19649686989566637
LOSS train 0.1448729838860237 valid 0.1964189471463441
LOSS train 0.1448729838860237 valid 0.19637127865956525
LOSS train 0.1448729838860237 valid 0.1963575555413377
LOSS train 0.1448729838860237 valid 0.19629716099007055
LOSS train 0.1448729838860237 valid 0.19617094847478755
LOSS train 0.1448729838860237 valid 0.19636365494062735
LOSS train 0.1448729838860237 valid 0.19637152282197504
LOSS train 0.1448729838860237 valid 0.19637441331377395
LOSS train 0.1448729838860237 valid 0.19652843783641682
LOSS train 0.1448729838860237 valid 0.196502156328154
LOSS train 0.1448729838860237 valid 0.19654644589460396
LOSS train 0.1448729838860237 valid 0.19656486690721728
LOSS train 0.1448729838860237 valid 0.19671155855340777
LOSS train 0.1448729838860237 valid 0.19672087288664697
LOSS train 0.1448729838860237 valid 0.1967536010099261
LOSS train 0.1448729838860237 valid 0.19681978153426255
LOSS train 0.1448729838860237 valid 0.1970045546062817
LOSS train 0.1448729838860237 valid 0.19709595844701486
LOSS train 0.1448729838860237 valid 0.19720835906773035
LOSS train 0.1448729838860237 valid 0.1976112024858594
LOSS train 0.1448729838860237 valid 0.1977510982271516
LOSS train 0.1448729838860237 valid 0.19775783711106237
LOSS train 0.1448729838860237 valid 0.1976989915154197
LOSS train 0.1448729838860237 valid 0.19764154820122581
LOSS train 0.1448729838860237 valid 0.19758247091882064
LOSS train 0.1448729838860237 valid 0.19738285699122243
LOSS train 0.1448729838860237 valid 0.19739092282924173
LOSS train 0.1448729838860237 valid 0.1972884920026575
LOSS train 0.1448729838860237 valid 0.1972525658556575
LOSS train 0.1448729838860237 valid 0.19705027975934616
LOSS train 0.1448729838860237 valid 0.19707803307068222
LOSS train 0.1448729838860237 valid 0.1971133364967897
LOSS train 0.1448729838860237 valid 0.19713590192167382
LOSS train 0.1448729838860237 valid 0.19715843047503825
LOSS train 0.1448729838860237 valid 0.19714299576415417
LOSS train 0.1448729838860237 valid 0.19715354467431703
LOSS train 0.1448729838860237 valid 0.19712483903528497
LOSS train 0.1448729838860237 valid 0.1972754158850374
LOSS train 0.1448729838860237 valid 0.19705674984201124
LOSS train 0.1448729838860237 valid 0.19710782364214938
LOSS train 0.1448729838860237 valid 0.1971368242449321
LOSS train 0.1448729838860237 valid 0.19726185389116507
LOSS train 0.1448729838860237 valid 0.19740338315398007
LOSS train 0.1448729838860237 valid 0.19730664945736126
LOSS train 0.1448729838860237 valid 0.1973092192752594
LOSS train 0.1448729838860237 valid 0.19727205575112527
LOSS train 0.1448729838860237 valid 0.19730131597223888
LOSS train 0.1448729838860237 valid 0.1973123689989249
LOSS train 0.1448729838860237 valid 0.19735779605830628
LOSS train 0.1448729838860237 valid 0.19729012645633015
LOSS train 0.1448729838860237 valid 0.19736639619267027
LOSS train 0.1448729838860237 valid 0.1974519022593373
LOSS train 0.1448729838860237 valid 0.1973457673534018
LOSS train 0.1448729838860237 valid 0.19731011665334888
LOSS train 0.1448729838860237 valid 0.1973499003858442
LOSS train 0.1448729838860237 valid 0.19730956049321532
LOSS train 0.1448729838860237 valid 0.1973371148881017
LOSS train 0.1448729838860237 valid 0.19727555974837271
LOSS train 0.1448729838860237 valid 0.19716174383063792
LOSS train 0.1448729838860237 valid 0.19727699081294048
LOSS train 0.1448729838860237 valid 0.1973525594693784
LOSS train 0.1448729838860237 valid 0.1974209748730538
LOSS train 0.1448729838860237 valid 0.19735164997123536
LOSS train 0.1448729838860237 valid 0.19730721711169316
LOSS train 0.1448729838860237 valid 0.1974130711908972
LOSS train 0.1448729838860237 valid 0.19754440951272376
LOSS train 0.1448729838860237 valid 0.19758737320810277
LOSS train 0.1448729838860237 valid 0.1974796668626368
LOSS train 0.1448729838860237 valid 0.19758841209693861
LOSS train 0.1448729838860237 valid 0.1975829004778625
LOSS train 0.1448729838860237 valid 0.19756811220579473
LOSS train 0.1448729838860237 valid 0.19765910073930834
LOSS train 0.1448729838860237 valid 0.19764085476215068
LOSS train 0.1448729838860237 valid 0.19777357121186753
LOSS train 0.1448729838860237 valid 0.19787535135170006
LOSS train 0.1448729838860237 valid 0.19775916472440813
LOSS train 0.1448729838860237 valid 0.1979437722261191
LOSS train 0.1448729838860237 valid 0.19783362515948036
LOSS train 0.1448729838860237 valid 0.19770718489708858
LOSS train 0.1448729838860237 valid 0.19765734407736593
LOSS train 0.1448729838860237 valid 0.19766595820943872
LOSS train 0.1448729838860237 valid 0.19776988225782702
LOSS train 0.1448729838860237 valid 0.1977521530727842
LOSS train 0.1448729838860237 valid 0.1978483953114067
LOSS train 0.1448729838860237 valid 0.1979183455160888
LOSS train 0.1448729838860237 valid 0.1978941472500739
LOSS train 0.1448729838860237 valid 0.19789526358818235
LOSS train 0.1448729838860237 valid 0.1978715482441818
LOSS train 0.1448729838860237 valid 0.1977763005977502
LOSS train 0.1448729838860237 valid 0.19770464510248417
LOSS train 0.1448729838860237 valid 0.19770151037864017
LOSS train 0.1448729838860237 valid 0.1979700230755085
LOSS train 0.1448729838860237 valid 0.19802308004835378
LOSS train 0.1448729838860237 valid 0.19804461481254224
LOSS train 0.1448729838860237 valid 0.19793529825698403
LOSS train 0.1448729838860237 valid 0.19787391660542325
LOSS train 0.1448729838860237 valid 0.19790613335319782
LOSS train 0.1448729838860237 valid 0.1978317373565265
LOSS train 0.1448729838860237 valid 0.19775020443645977
LOSS train 0.1448729838860237 valid 0.19774870451709087
LOSS train 0.1448729838860237 valid 0.19770271286748287
LOSS train 0.1448729838860237 valid 0.19767735502814168
LOSS train 0.1448729838860237 valid 0.19780615014089664
LOSS train 0.1448729838860237 valid 0.19780654937363742
LOSS train 0.1448729838860237 valid 0.1978414719428669
LOSS train 0.1448729838860237 valid 0.19776595521239596
LOSS train 0.1448729838860237 valid 0.19777073347966984
LOSS train 0.1448729838860237 valid 0.19771576469971075
LOSS train 0.1448729838860237 valid 0.19772516075428834
LOSS train 0.1448729838860237 valid 0.1978165541070601
LOSS train 0.1448729838860237 valid 0.19765391658488712
LOSS train 0.1448729838860237 valid 0.19769064009517104
LOSS train 0.1448729838860237 valid 0.1976920198904325
LOSS train 0.1448729838860237 valid 0.1976758002257738
LOSS train 0.1448729838860237 valid 0.19754519693208325
LOSS train 0.1448729838860237 valid 0.19753535209304612
LOSS train 0.1448729838860237 valid 0.1975729177557033
EPOCH 6:
  batch 1 loss: 0.12819355726242065
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.13189692050218582
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.13083026309808096
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.13843675702810287
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.14455665349960328
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.1443578526377678
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.13579388388565608
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.13666946813464165
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.13593032956123352
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.13450861275196074
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.13443291864611887
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.1335447213302056
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.13416172277468902
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.13314893362777575
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.13423855553070704
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.13526780577376485
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.13375585394747116
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.1356107567747434
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.13483358684339022
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.13412304781377316
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.13456046971536817
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.13467860594391823
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.13495611917713415
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.13663420112182698
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.13628134697675706
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.13659051146644813
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.1372383686679381
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.13696010543831758
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.13694650988126622
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.13771295572320622
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.13822596087571112
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.13784892787225544
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.13766730983148923
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.13862160483703895
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.13971557638474874
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.14001856351064312
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.14025859756244197
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.1398514422931169
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.14046199963643
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.1402418937534094
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.14048504429619488
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.140615161330927
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.14137694378231846
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.14069476147944276
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.14168557359112632
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.14175687600737033
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.1415005551373705
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.1409553368575871
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.14114699786414905
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.14121025517582894
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.14121929935964883
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.14181740338412616
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.1418762152206223
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.14224998445974457
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.14255101802674205
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.1427055462928755
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.14296092785764158
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.14284401271363784
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.14342693997136616
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.14337192612389724
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.14327936253098192
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.14309111994601065
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.14350664698415333
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.1433343644021079
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.14328246151025478
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.14328630753990376
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.1431430735027612
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.14329995949040442
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.1436423733830452
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.1438111006149224
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.1441369731451424
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.14412034002857077
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.14435058577011708
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.1442280491260258
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.14410457442204158
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.14410389753940858
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.14395593774396104
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.1435384902243431
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.14371157230078418
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.1437455049715936
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.14399664959421865
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.14360146769663182
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.14350775889603487
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.14404893045624098
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.14410500316058888
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.14434551066437432
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.14439813621427822
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.1443489488552917
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.14471901836020223
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.1448513012793329
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.14478510814708667
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.14485723330922748
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.14510310881881303
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.1451957292379217
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.14549534352202165
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.14553590569024286
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.1451860205414369
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.1451736279592222
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.1453648695741037
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.14511825725436212
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.14523687206282473
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.14506364642989403
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.14499393481652714
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.1452215719394959
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.1447777666506313
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.1447345145609019
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.1448507160644665
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.14468939181555202
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.1450195400676596
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.1454439142210917
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.14534573585868957
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.1450773178865867
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.14478023528261522
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.14495614452058808
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.144753562885782
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.14474695034582039
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.14485735847399786
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.14475002935377218
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.1445859062070606
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.14476647973060608
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.14460300703432935
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.1445860009579385
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.1447449150245364
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.1446150835122793
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.14454459315538407
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.14453451790743405
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.1446399110860712
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.14452411158708856
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.14453423156063686
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.14463258620638114
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.14436868999068064
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.14425163879764802
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.1444558852485248
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.14433760312733365
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.14408662876597159
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.1439956836950253
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.1442006178689699
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.14409339152600453
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.1442328401499515
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.14446699028568608
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.14435442473660123
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.14430703238492282
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.14432252354763606
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.14449699559352464
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.1443297661070166
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.1442855044893206
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.14422187052008245
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.14415594255803404
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.14434013615718624
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.14436080758770306
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.14414077094256483
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.14407418643761621
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.14395204210787818
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.14373977937094576
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.14383731743981762
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.14375480942619154
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.14375174036071559
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.14364597776645346
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.1438204712650311
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.1439517511986196
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.1438199255777442
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.14378094857121693
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.1437751804392762
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.1438042540557501
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.14389889818249327
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.1438566747020526
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.14354616568652456
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.1435168825888208
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.14339093886007218
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.14346171315102016
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.14332952842726346
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.143293721471415
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.14324451147476372
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.14313955253910746
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.1431983015366963
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.14307233136655254
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.1429502142351226
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.14272422293263876
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.14280634810471668
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.142751823614041
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.14275636363424649
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.14268097486142273
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.1424270997770497
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.14237586643708788
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.14234028498868684
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.1423259800480258
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.1424124565194635
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.1422956044924386
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.14241451525656634
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.14252701719340524
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.1425576429638563
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.14243352607203028
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.1424882406552221
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.1422917101647436
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.14229136330959125
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.14238153808579154
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.14254609007520724
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.14270671103337798
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.14272142212894096
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.14289294503629207
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.14287325080057875
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.14290925757129594
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.1428687126912507
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.14292424310948335
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.14276268551262414
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.14263059929447267
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.14265807196137986
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.14250908637992465
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.1424661517214547
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.1426553635015374
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.14263666661288502
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.1426787782319874
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.1426485809431949
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.14252289704908835
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.1425890920467155
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.14246686023694496
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.14254914759765572
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.14268071136069954
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.14277337389449551
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.14283734302629125
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.1428838585296907
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.14286904708222226
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.1427895181515826
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.1428108179409589
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.14268319225973553
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.14258447780677702
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.14267019517227417
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.14250064356938788
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.14249094009529556
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.1426690506870332
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.14282630819391895
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.1427692373700697
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.14274278881084254
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.14266377318109202
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.14263930498285496
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.14269039830413915
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.1426805323186303
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.14267485820445694
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.14276560929031054
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.14276743071774642
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.14286319979493547
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.14277296962816854
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.14281392330495418
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.14277125833953014
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.1428076189391467
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.1428363744805499
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.1427743325107976
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.14273401644201048
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.14267506997987447
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.14264000600576401
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.1426980011610396
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.14251379033048
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.14248982021813336
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.1424365584247225
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.14250827896244386
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.14248229816439562
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.14236718303150703
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.1423374772822672
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.14227727737320897
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.1422588025434659
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.1422915418999862
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.142291942767742
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.14221487471925895
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.14205927904130836
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.14201749327047816
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.14202808179801568
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.14204185207684836
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.14203541964959743
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.1419896209217802
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.14194771917881788
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.14190903784384146
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.14191358887097416
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.14205106727151207
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.14201664282892743
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.14195555394346063
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.1420278613442096
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.1419786858752316
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.14200691176618604
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.1420127215885347
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.14203590245119163
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.14204558809668993
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.14204094104521664
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.14205737396179577
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.14203510973864877
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.14214602191197245
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.14216900325113244
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.14215896187759028
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.1422893236287766
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.1422118699519692
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.14222088494691357
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.1422107661354173
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.14217426722282417
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.14228639827958553
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.14236744967143553
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.14227682511180134
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.14225060938278566
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.14231934995462597
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.14230008895865223
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.1422252791581744
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.14226262663801512
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.1423526390446381
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.1423098292966552
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.1423040522207128
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.14227765078019156
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.14231389179581502
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.14229552851977692
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.14229401824722848
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.1422563476318663
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.14222161640627098
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.14215531911580792
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.14221234835229118
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.1422623323324399
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.14225687781652321
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.14218514365185597
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.14217370554568276
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.14225635389927066
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.14227342591488776
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.14224671310036438
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.1422084818906545
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.14226406961679458
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.14228618154273226
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.14229313154583392
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.142293822239427
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.14224907976609688
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.14229171940913568
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.1422092516912273
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.14208678923980175
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.1420350062047563
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.14202844068453305
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.142089874365113
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.14215169959558102
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.1421497120824923
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.14207662093836265
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.14207844027948235
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.14214855283498765
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.14214114172916328
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.142091332075327
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.14209178281870818
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.14197631142923614
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.14199608373729622
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.14194086379756676
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.14210915184247563
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.14208113089334165
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.14206204257992117
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.1420821452702301
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.14211183431089958
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.14215090941712905
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.14213229966317786
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.14211286948113183
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.14202415276850971
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.14197637711749797
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.1420344416348433
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.14206707737179045
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.14215863018301922
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.1421692721230883
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.14215716089676605
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.14218761771917343
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.1422731099889598
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.14224197358557109
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.14218630246404146
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.14216706270243654
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.14217590011697448
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.142117930571074
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.14206087282234495
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.1421219140699465
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.14212726785348412
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.14215168413739113
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.1421722683083752
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.1421452780483861
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.1421164128425959
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.14215409554882513
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.1421478849344997
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.1421258897388269
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.14219028678010492
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.14221007466316224
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.14223781763080587
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.14223229632137308
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.14226584286286087
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.14224740969002403
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.14217073919349596
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.14213448444141805
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.14209909641539864
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.1421830493266215
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.14208774189076698
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.14203386165491946
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.14195894054257807
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.14198433053354884
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.1420274302555421
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.14202894046328063
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.14203431829810143
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.14200341333742336
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.141994808230321
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.14200914943112064
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.14199639533452577
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.14205429205034353
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.14208928087368758
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.1419970182237457
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.14202196293290537
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.14196563261866868
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.141965997684747
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.14196970469859474
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.14194849732132694
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.14190647603190568
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.1419036971718663
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.14187466106664987
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.14183141850852615
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.14179455426679488
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.1418738683101301
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.14185570393550076
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.14192019681014667
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.14190651503575108
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.141874745433915
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.1418599941864718
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.1419076393155948
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.1419844911220562
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.1420451991725713
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.14202636235552155
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.14199556105516173
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.14213904292102644
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.14210530095511958
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.14215995918189545
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.14217962954917227
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.14212395315967835
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.1421057296003092
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.14210686082349103
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.1420688886772579
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.14205671206500547
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.14200869528141535
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.1419779711982587
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.14196237271954848
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.1419641739427352
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.14195200181738646
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.14189080276266255
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.14188888207215317
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.14182985209185503
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.14190000480194703
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.14196451243601346
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.14193676473343209
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.14182141396070666
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.14175253596834161
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.14175001572386747
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.1416961853455634
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.14167723947669258
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.1416877865589954
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.1417097906383236
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.14174045742627217
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.14172695437133712
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.14180393317448242
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.14182871364405533
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.14178876350323358
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.14182067393199302
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.14191010858105346
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.141854196079651
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.14181654972520694
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.14178197370780693
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.14179723026851812
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.14179407682930092
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.14178717884694644
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.14177007823857865
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.14186228555829627
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.14184477388729502
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.14184749948901015
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.14173799949511082
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.1417557533252342
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.14178438132168145
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.14178672093946024
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.1418212031271524
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.14188047221455818
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.1419346768146893
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.14196823305906134
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.14200327471056814
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.14210507775641093
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.14210507775641093 valid 0.23154929280281067
LOSS train 0.14210507775641093 valid 0.19529031217098236
LOSS train 0.14210507775641093 valid 0.20001485447088876
LOSS train 0.14210507775641093 valid 0.1877896636724472
LOSS train 0.14210507775641093 valid 0.18439887464046478
LOSS train 0.14210507775641093 valid 0.1900662804643313
LOSS train 0.14210507775641093 valid 0.19802815573556082
LOSS train 0.14210507775641093 valid 0.19674471206963062
LOSS train 0.14210507775641093 valid 0.19543727073404524
LOSS train 0.14210507775641093 valid 0.1985333204269409
LOSS train 0.14210507775641093 valid 0.19755861434069547
LOSS train 0.14210507775641093 valid 0.19544516255458197
LOSS train 0.14210507775641093 valid 0.19413130558454073
LOSS train 0.14210507775641093 valid 0.19385688858372824
LOSS train 0.14210507775641093 valid 0.19025330146153768
LOSS train 0.14210507775641093 valid 0.19171826355159283
LOSS train 0.14210507775641093 valid 0.19363157977076137
LOSS train 0.14210507775641093 valid 0.192609546913041
LOSS train 0.14210507775641093 valid 0.19463987726914256
LOSS train 0.14210507775641093 valid 0.1959252990782261
LOSS train 0.14210507775641093 valid 0.19586922654083797
LOSS train 0.14210507775641093 valid 0.1945126469839703
LOSS train 0.14210507775641093 valid 0.19444932173127713
LOSS train 0.14210507775641093 valid 0.19415370064477125
LOSS train 0.14210507775641093 valid 0.19239483177661895
LOSS train 0.14210507775641093 valid 0.19226893152181918
LOSS train 0.14210507775641093 valid 0.19220772164839287
LOSS train 0.14210507775641093 valid 0.1930035384637969
LOSS train 0.14210507775641093 valid 0.19326787210744004
LOSS train 0.14210507775641093 valid 0.19400997360547384
LOSS train 0.14210507775641093 valid 0.19500105131057002
LOSS train 0.14210507775641093 valid 0.19427957478910685
LOSS train 0.14210507775641093 valid 0.19464521155212866
LOSS train 0.14210507775641093 valid 0.1941079111660228
LOSS train 0.14210507775641093 valid 0.19604809369359696
LOSS train 0.14210507775641093 valid 0.19609736733966404
LOSS train 0.14210507775641093 valid 0.1971645274677792
LOSS train 0.14210507775641093 valid 0.19750589406804034
LOSS train 0.14210507775641093 valid 0.1966327088765609
LOSS train 0.14210507775641093 valid 0.19664928838610649
LOSS train 0.14210507775641093 valid 0.19632043853038694
LOSS train 0.14210507775641093 valid 0.1970107349611464
LOSS train 0.14210507775641093 valid 0.19645184555719064
LOSS train 0.14210507775641093 valid 0.19699634780937975
LOSS train 0.14210507775641093 valid 0.1969062477350235
LOSS train 0.14210507775641093 valid 0.1976901321955349
LOSS train 0.14210507775641093 valid 0.19805255849310693
LOSS train 0.14210507775641093 valid 0.19825755742688975
LOSS train 0.14210507775641093 valid 0.19905181958967325
LOSS train 0.14210507775641093 valid 0.19835217654705048
LOSS train 0.14210507775641093 valid 0.1987861581877166
LOSS train 0.14210507775641093 valid 0.19873989316133353
LOSS train 0.14210507775641093 valid 0.19905362629665518
LOSS train 0.14210507775641093 valid 0.19940088385785068
LOSS train 0.14210507775641093 valid 0.1994012569839304
LOSS train 0.14210507775641093 valid 0.1993923554463046
LOSS train 0.14210507775641093 valid 0.19937998997537712
LOSS train 0.14210507775641093 valid 0.19928673552028064
LOSS train 0.14210507775641093 valid 0.1998418834754976
LOSS train 0.14210507775641093 valid 0.19936538264155387
LOSS train 0.14210507775641093 valid 0.19903328477359208
LOSS train 0.14210507775641093 valid 0.199217475229694
LOSS train 0.14210507775641093 valid 0.19900256655518972
LOSS train 0.14210507775641093 valid 0.19979105982929468
LOSS train 0.14210507775641093 valid 0.199747002354035
LOSS train 0.14210507775641093 valid 0.19957281626535184
LOSS train 0.14210507775641093 valid 0.19915320557444843
LOSS train 0.14210507775641093 valid 0.19950604767483823
LOSS train 0.14210507775641093 valid 0.19870576759179434
LOSS train 0.14210507775641093 valid 0.199011706667287
LOSS train 0.14210507775641093 valid 0.1987711016980695
LOSS train 0.14210507775641093 valid 0.1988835266480843
LOSS train 0.14210507775641093 valid 0.19902613024189048
LOSS train 0.14210507775641093 valid 0.19889499770628438
LOSS train 0.14210507775641093 valid 0.19897847453753154
LOSS train 0.14210507775641093 valid 0.19979129692441538
LOSS train 0.14210507775641093 valid 0.19952562954518702
LOSS train 0.14210507775641093 valid 0.1994401052212104
LOSS train 0.14210507775641093 valid 0.199011813047566
LOSS train 0.14210507775641093 valid 0.19841618705540895
LOSS train 0.14210507775641093 valid 0.19785016333615338
LOSS train 0.14210507775641093 valid 0.1982091874247644
LOSS train 0.14210507775641093 valid 0.19776487224791423
LOSS train 0.14210507775641093 valid 0.19771562392512956
LOSS train 0.14210507775641093 valid 0.1969660001642564
LOSS train 0.14210507775641093 valid 0.19645734789759614
LOSS train 0.14210507775641093 valid 0.19637291767131324
LOSS train 0.14210507775641093 valid 0.19602622057903896
LOSS train 0.14210507775641093 valid 0.1963120729066013
LOSS train 0.14210507775641093 valid 0.19646558298004999
LOSS train 0.14210507775641093 valid 0.19652188888617925
LOSS train 0.14210507775641093 valid 0.19639836494689403
LOSS train 0.14210507775641093 valid 0.196334471785894
LOSS train 0.14210507775641093 valid 0.19664525938160876
LOSS train 0.14210507775641093 valid 0.19615368890134913
LOSS train 0.14210507775641093 valid 0.1962669976055622
LOSS train 0.14210507775641093 valid 0.19624719407755076
LOSS train 0.14210507775641093 valid 0.19653960834352338
LOSS train 0.14210507775641093 valid 0.19668893094616707
LOSS train 0.14210507775641093 valid 0.19670964673161506
LOSS train 0.14210507775641093 valid 0.19693712152466916
LOSS train 0.14210507775641093 valid 0.19743626550132154
LOSS train 0.14210507775641093 valid 0.19726917364643615
LOSS train 0.14210507775641093 valid 0.19725434023600358
LOSS train 0.14210507775641093 valid 0.1974620718331564
LOSS train 0.14210507775641093 valid 0.19783961112206838
LOSS train 0.14210507775641093 valid 0.19751218393027226
LOSS train 0.14210507775641093 valid 0.1976077589723799
LOSS train 0.14210507775641093 valid 0.19815888125962072
LOSS train 0.14210507775641093 valid 0.19819766770709646
LOSS train 0.14210507775641093 valid 0.19810369758455604
LOSS train 0.14210507775641093 valid 0.19789085018315486
LOSS train 0.14210507775641093 valid 0.19801752045091275
LOSS train 0.14210507775641093 valid 0.19817351549863815
LOSS train 0.14210507775641093 valid 0.19814439947190493
LOSS train 0.14210507775641093 valid 0.19850404303649377
LOSS train 0.14210507775641093 valid 0.19866055084599388
LOSS train 0.14210507775641093 valid 0.19838587434615118
LOSS train 0.14210507775641093 valid 0.19821220986983357
LOSS train 0.14210507775641093 valid 0.19789968468248845
LOSS train 0.14210507775641093 valid 0.19777140427719464
LOSS train 0.14210507775641093 valid 0.19780288001552956
LOSS train 0.14210507775641093 valid 0.19796870757893817
LOSS train 0.14210507775641093 valid 0.19833539089848917
LOSS train 0.14210507775641093 valid 0.19811987352371216
LOSS train 0.14210507775641093 valid 0.19830342890724303
LOSS train 0.14210507775641093 valid 0.19815225957885502
LOSS train 0.14210507775641093 valid 0.19816968543455005
LOSS train 0.14210507775641093 valid 0.1985372745020445
LOSS train 0.14210507775641093 valid 0.19835001803361452
LOSS train 0.14210507775641093 valid 0.1981733172904444
LOSS train 0.14210507775641093 valid 0.19772037852442625
LOSS train 0.14210507775641093 valid 0.1974010487696282
LOSS train 0.14210507775641093 valid 0.19754117193506726
LOSS train 0.14210507775641093 valid 0.19739054463527822
LOSS train 0.14210507775641093 valid 0.19719896914766116
LOSS train 0.14210507775641093 valid 0.19692899330254018
LOSS train 0.14210507775641093 valid 0.1967811087767283
LOSS train 0.14210507775641093 valid 0.19648381628149705
LOSS train 0.14210507775641093 valid 0.19657352726374352
LOSS train 0.14210507775641093 valid 0.1966142769612319
LOSS train 0.14210507775641093 valid 0.19658888278292938
LOSS train 0.14210507775641093 valid 0.19667692732560885
LOSS train 0.14210507775641093 valid 0.1968191290895144
LOSS train 0.14210507775641093 valid 0.19674530029296874
LOSS train 0.14210507775641093 valid 0.19679627924749296
LOSS train 0.14210507775641093 valid 0.19666837338282137
LOSS train 0.14210507775641093 valid 0.19762830808758736
LOSS train 0.14210507775641093 valid 0.19772015241968552
LOSS train 0.14210507775641093 valid 0.19752006888389587
LOSS train 0.14210507775641093 valid 0.19783358167338844
LOSS train 0.14210507775641093 valid 0.19765102167270684
LOSS train 0.14210507775641093 valid 0.19765386969045876
LOSS train 0.14210507775641093 valid 0.19765458285034476
LOSS train 0.14210507775641093 valid 0.19754161921239669
LOSS train 0.14210507775641093 valid 0.19759742820110077
LOSS train 0.14210507775641093 valid 0.19766693462611765
LOSS train 0.14210507775641093 valid 0.1976760538129867
LOSS train 0.14210507775641093 valid 0.1976831683760169
LOSS train 0.14210507775641093 valid 0.1975640376098454
LOSS train 0.14210507775641093 valid 0.1974972451325529
LOSS train 0.14210507775641093 valid 0.19721161896063957
LOSS train 0.14210507775641093 valid 0.19697266643398378
LOSS train 0.14210507775641093 valid 0.19672666408303308
LOSS train 0.14210507775641093 valid 0.19670679307345187
LOSS train 0.14210507775641093 valid 0.19660556890878333
LOSS train 0.14210507775641093 valid 0.1968079432398973
LOSS train 0.14210507775641093 valid 0.19680713534000374
LOSS train 0.14210507775641093 valid 0.19696602560359344
LOSS train 0.14210507775641093 valid 0.19704218945082497
LOSS train 0.14210507775641093 valid 0.19697201243269513
LOSS train 0.14210507775641093 valid 0.19685401291001675
LOSS train 0.14210507775641093 valid 0.19695255978603585
LOSS train 0.14210507775641093 valid 0.19689200398908263
LOSS train 0.14210507775641093 valid 0.1967634026493345
LOSS train 0.14210507775641093 valid 0.19684552172706885
LOSS train 0.14210507775641093 valid 0.19678727583696612
LOSS train 0.14210507775641093 valid 0.1968511166364959
LOSS train 0.14210507775641093 valid 0.196725397219871
LOSS train 0.14210507775641093 valid 0.19670883475078477
LOSS train 0.14210507775641093 valid 0.1967525041729047
LOSS train 0.14210507775641093 valid 0.19666798846734748
LOSS train 0.14210507775641093 valid 0.1966347414287713
LOSS train 0.14210507775641093 valid 0.19665889741609927
LOSS train 0.14210507775641093 valid 0.1964829379642332
LOSS train 0.14210507775641093 valid 0.196540103323998
LOSS train 0.14210507775641093 valid 0.19638852424481335
LOSS train 0.14210507775641093 valid 0.19626474325010118
LOSS train 0.14210507775641093 valid 0.19623465228963782
LOSS train 0.14210507775641093 valid 0.1963299688540007
LOSS train 0.14210507775641093 valid 0.19627699396373088
LOSS train 0.14210507775641093 valid 0.19629972598825893
LOSS train 0.14210507775641093 valid 0.19617641242365763
LOSS train 0.14210507775641093 valid 0.19598715253097496
LOSS train 0.14210507775641093 valid 0.19571399956177443
LOSS train 0.14210507775641093 valid 0.19574126912927142
LOSS train 0.14210507775641093 valid 0.1959154285906535
LOSS train 0.14210507775641093 valid 0.19580668534594353
LOSS train 0.14210507775641093 valid 0.19585843032328926
LOSS train 0.14210507775641093 valid 0.19578564003109933
LOSS train 0.14210507775641093 valid 0.19577490838605965
LOSS train 0.14210507775641093 valid 0.19573152021016224
LOSS train 0.14210507775641093 valid 0.19576974866425462
LOSS train 0.14210507775641093 valid 0.19597905092671805
LOSS train 0.14210507775641093 valid 0.19582300462373872
LOSS train 0.14210507775641093 valid 0.19585406548768572
LOSS train 0.14210507775641093 valid 0.1958815442479175
LOSS train 0.14210507775641093 valid 0.19584902204000032
LOSS train 0.14210507775641093 valid 0.19585066434869355
LOSS train 0.14210507775641093 valid 0.19594240827219828
LOSS train 0.14210507775641093 valid 0.19593123060549605
LOSS train 0.14210507775641093 valid 0.1959261492878761
LOSS train 0.14210507775641093 valid 0.1959339893060111
LOSS train 0.14210507775641093 valid 0.19585205823461585
LOSS train 0.14210507775641093 valid 0.1957664416972981
LOSS train 0.14210507775641093 valid 0.19561470941536957
LOSS train 0.14210507775641093 valid 0.19539639257615612
LOSS train 0.14210507775641093 valid 0.19538347928895863
LOSS train 0.14210507775641093 valid 0.1955059687840884
LOSS train 0.14210507775641093 valid 0.19550045241009106
LOSS train 0.14210507775641093 valid 0.19543247632850888
LOSS train 0.14210507775641093 valid 0.1954743141258085
LOSS train 0.14210507775641093 valid 0.19553282870305494
LOSS train 0.14210507775641093 valid 0.19560272359688366
LOSS train 0.14210507775641093 valid 0.19584441542625428
LOSS train 0.14210507775641093 valid 0.19601072100675213
LOSS train 0.14210507775641093 valid 0.19610356772523618
LOSS train 0.14210507775641093 valid 0.1961364584010944
LOSS train 0.14210507775641093 valid 0.19608812854019314
LOSS train 0.14210507775641093 valid 0.19616262698950976
LOSS train 0.14210507775641093 valid 0.19625447484063896
LOSS train 0.14210507775641093 valid 0.19621988880480157
LOSS train 0.14210507775641093 valid 0.19619992285838966
LOSS train 0.14210507775641093 valid 0.19617813583622631
LOSS train 0.14210507775641093 valid 0.19629747449083532
LOSS train 0.14210507775641093 valid 0.1961777468480296
LOSS train 0.14210507775641093 valid 0.19618570955493783
LOSS train 0.14210507775641093 valid 0.1961043480815006
LOSS train 0.14210507775641093 valid 0.19603807831409087
LOSS train 0.14210507775641093 valid 0.196082591637969
LOSS train 0.14210507775641093 valid 0.1961448233404595
LOSS train 0.14210507775641093 valid 0.1960071787114971
LOSS train 0.14210507775641093 valid 0.19621615209942492
LOSS train 0.14210507775641093 valid 0.19629685672335936
LOSS train 0.14210507775641093 valid 0.19636763741775434
LOSS train 0.14210507775641093 valid 0.19624094749853863
LOSS train 0.14210507775641093 valid 0.19633146981720018
LOSS train 0.14210507775641093 valid 0.19628366900067176
LOSS train 0.14210507775641093 valid 0.19624691956253895
LOSS train 0.14210507775641093 valid 0.1962409160733223
LOSS train 0.14210507775641093 valid 0.19618417239521604
LOSS train 0.14210507775641093 valid 0.19629115404354203
LOSS train 0.14210507775641093 valid 0.1962121789281076
LOSS train 0.14210507775641093 valid 0.19616354638197292
LOSS train 0.14210507775641093 valid 0.1961525154464385
LOSS train 0.14210507775641093 valid 0.19608617282938212
LOSS train 0.14210507775641093 valid 0.19595097967854733
LOSS train 0.14210507775641093 valid 0.1961485057376152
LOSS train 0.14210507775641093 valid 0.19615672822164293
LOSS train 0.14210507775641093 valid 0.19615276874257967
LOSS train 0.14210507775641093 valid 0.1963201480350275
LOSS train 0.14210507775641093 valid 0.19630187146991263
LOSS train 0.14210507775641093 valid 0.19634594334848934
LOSS train 0.14210507775641093 valid 0.19636546679292666
LOSS train 0.14210507775641093 valid 0.19651193056466446
LOSS train 0.14210507775641093 valid 0.19652262341259116
LOSS train 0.14210507775641093 valid 0.1965587522541539
LOSS train 0.14210507775641093 valid 0.19662219552851434
LOSS train 0.14210507775641093 valid 0.19681127097970047
LOSS train 0.14210507775641093 valid 0.19690135640126688
LOSS train 0.14210507775641093 valid 0.19702006508063566
LOSS train 0.14210507775641093 valid 0.19742825736894326
LOSS train 0.14210507775641093 valid 0.1975801795611888
LOSS train 0.14210507775641093 valid 0.19759244542487348
LOSS train 0.14210507775641093 valid 0.19753742786971004
LOSS train 0.14210507775641093 valid 0.19748401355700215
LOSS train 0.14210507775641093 valid 0.19743048515345646
LOSS train 0.14210507775641093 valid 0.19723133769609946
LOSS train 0.14210507775641093 valid 0.19724306186467516
LOSS train 0.14210507775641093 valid 0.19714385739394596
LOSS train 0.14210507775641093 valid 0.19710728245061487
LOSS train 0.14210507775641093 valid 0.19689894678956227
LOSS train 0.14210507775641093 valid 0.19692806715257596
LOSS train 0.14210507775641093 valid 0.19697203954130832
LOSS train 0.14210507775641093 valid 0.19698764713186967
LOSS train 0.14210507775641093 valid 0.19701177772733716
LOSS train 0.14210507775641093 valid 0.19700114911856967
LOSS train 0.14210507775641093 valid 0.19700914942142037
LOSS train 0.14210507775641093 valid 0.19697797143748064
LOSS train 0.14210507775641093 valid 0.19713486823542364
LOSS train 0.14210507775641093 valid 0.19691356960236003
LOSS train 0.14210507775641093 valid 0.19695952953132864
LOSS train 0.14210507775641093 valid 0.19699110909533582
LOSS train 0.14210507775641093 valid 0.19712281429848702
LOSS train 0.14210507775641093 valid 0.19726253980297154
LOSS train 0.14210507775641093 valid 0.19716171321232576
LOSS train 0.14210507775641093 valid 0.19716683050197384
LOSS train 0.14210507775641093 valid 0.19712691514084002
LOSS train 0.14210507775641093 valid 0.1971509919118722
LOSS train 0.14210507775641093 valid 0.19716441238919893
LOSS train 0.14210507775641093 valid 0.19720775953360967
LOSS train 0.14210507775641093 valid 0.19713739927438712
LOSS train 0.14210507775641093 valid 0.19720894722851984
LOSS train 0.14210507775641093 valid 0.19729477255360076
LOSS train 0.14210507775641093 valid 0.1971925086173855
LOSS train 0.14210507775641093 valid 0.19716313442373587
LOSS train 0.14210507775641093 valid 0.19720427832696646
LOSS train 0.14210507775641093 valid 0.19716268540783363
LOSS train 0.14210507775641093 valid 0.19718739809920488
LOSS train 0.14210507775641093 valid 0.19713072483578037
LOSS train 0.14210507775641093 valid 0.19701704660795893
LOSS train 0.14210507775641093 valid 0.19713743986227575
LOSS train 0.14210507775641093 valid 0.19721795613773332
LOSS train 0.14210507775641093 valid 0.19729348694442944
LOSS train 0.14210507775641093 valid 0.1972262307291939
LOSS train 0.14210507775641093 valid 0.19718283344107337
LOSS train 0.14210507775641093 valid 0.1972892439985877
LOSS train 0.14210507775641093 valid 0.1974262036148857
LOSS train 0.14210507775641093 valid 0.19747317925702815
LOSS train 0.14210507775641093 valid 0.19737251503393055
LOSS train 0.14210507775641093 valid 0.1974861095441836
LOSS train 0.14210507775641093 valid 0.19748312609721413
LOSS train 0.14210507775641093 valid 0.197465776013147
LOSS train 0.14210507775641093 valid 0.1975596316250754
LOSS train 0.14210507775641093 valid 0.1975407058917559
LOSS train 0.14210507775641093 valid 0.1976753127264099
LOSS train 0.14210507775641093 valid 0.19777568122115705
LOSS train 0.14210507775641093 valid 0.1976545882388586
LOSS train 0.14210507775641093 valid 0.19784068368307364
LOSS train 0.14210507775641093 valid 0.1977333747527816
LOSS train 0.14210507775641093 valid 0.19761002752536014
LOSS train 0.14210507775641093 valid 0.1975586631570954
LOSS train 0.14210507775641093 valid 0.19757174237354383
LOSS train 0.14210507775641093 valid 0.19767205051319328
LOSS train 0.14210507775641093 valid 0.19765549035214666
LOSS train 0.14210507775641093 valid 0.19775403734473956
LOSS train 0.14210507775641093 valid 0.19783148854882499
LOSS train 0.14210507775641093 valid 0.19781231628896218
LOSS train 0.14210507775641093 valid 0.1978194705240846
LOSS train 0.14210507775641093 valid 0.197793398841339
LOSS train 0.14210507775641093 valid 0.19769779909566002
LOSS train 0.14210507775641093 valid 0.19763081603580052
LOSS train 0.14210507775641093 valid 0.19763332952910886
LOSS train 0.14210507775641093 valid 0.19790866832400478
LOSS train 0.14210507775641093 valid 0.19795464000840118
LOSS train 0.14210507775641093 valid 0.19797447397012932
LOSS train 0.14210507775641093 valid 0.19786947669316438
LOSS train 0.14210507775641093 valid 0.1978077807131855
LOSS train 0.14210507775641093 valid 0.19783329989302123
LOSS train 0.14210507775641093 valid 0.19775599794728416
LOSS train 0.14210507775641093 valid 0.19768139579866686
LOSS train 0.14210507775641093 valid 0.1976844086735086
LOSS train 0.14210507775641093 valid 0.19764119378230388
LOSS train 0.14210507775641093 valid 0.19761388618393808
LOSS train 0.14210507775641093 valid 0.19774113268079893
LOSS train 0.14210507775641093 valid 0.19774232901046784
LOSS train 0.14210507775641093 valid 0.19778400129463827
LOSS train 0.14210507775641093 valid 0.19770813421164146
LOSS train 0.14210507775641093 valid 0.19770909903939388
LOSS train 0.14210507775641093 valid 0.1976510706047217
LOSS train 0.14210507775641093 valid 0.1976640060998066
LOSS train 0.14210507775641093 valid 0.19775796171216017
LOSS train 0.14210507775641093 valid 0.19759482288984556
LOSS train 0.14210507775641093 valid 0.19762844142023023
LOSS train 0.14210507775641093 valid 0.19763224578067048
LOSS train 0.14210507775641093 valid 0.19761355595836222
LOSS train 0.14210507775641093 valid 0.1974837922107946
LOSS train 0.14210507775641093 valid 0.19747407924707816
LOSS train 0.14210507775641093 valid 0.1975116790310154
EPOCH 7:
  batch 1 loss: 0.12926658987998962
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.13104651868343353
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.13081533213456473
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.13802440837025642
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.14327639639377593
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.14329934865236282
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.13484061083623342
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.13445378560572863
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.13325729386674035
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.13183843493461608
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.13159484754909168
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.13036969924966493
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.13114302662702707
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.13036007434129715
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.13116060396035512
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.13236297946423292
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.13085298371665618
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.13285441075762114
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.13223334950836083
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.13185602575540542
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.1323373033886864
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.13252125612714075
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.13269783102947733
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.13449657335877419
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.13415998339653015
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.13461996156435746
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.13547288709216648
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.13518900104931422
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.1349364144021067
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.1354733571410179
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.1359037055123237
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.1355688446201384
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.13530754952719717
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.13630443329320235
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.13747570727552685
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.13763312002023062
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.1379358285182231
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.13742817859900625
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.13827653343860918
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.13815664127469063
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.138328913144949
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.13850803141083037
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.13928850757521252
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.13873052055185492
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.13970408340295157
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.13993376590635465
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.13970813630743228
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.13925181732823452
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.13927878439426422
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.1392732974886894
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.13925235674661748
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.1397043111232611
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.13973399321987945
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.1401567914419704
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.1405172721906142
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.14063249475189618
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.14087293754544175
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.14088451271427088
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.1414755749500404
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.14143686592578888
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.14130385000197615
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.14109069926123466
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.14150468957802606
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.14139847154729068
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.14135545056599838
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.14133560815543839
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.1411631801235142
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.14137362557298996
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.14167809594368588
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.1418602521930422
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.14214453394983856
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.14210438604156175
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.14226697569024072
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.14213005735262021
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.1419459040959676
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.14205719256087354
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.1418715732825267
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.14139763323160318
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.14152407250072382
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.14157150723040104
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.1419150216711892
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.14148575912525013
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.14142945238265647
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.14209448111553988
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.14220459277138991
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.1424522331460964
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.14252819523386573
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.1424934578720819
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.14283299923277973
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.14295472809010082
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.1428756750710718
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.14285411256486955
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.14306653403146294
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.14320183981289256
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.14348065766849016
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.14353897616577646
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.14319266762930094
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.1431359052658081
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.1433199302415655
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.14310425721108913
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.14316336803212024
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.1429917008122977
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.14291109007249758
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.14309983464101186
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.14263234926121576
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.14260790180766358
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.14274268082090627
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.14256930013221722
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.142907746819728
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.14336189431223004
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.143233662722884
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.1429271722611572
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.1426151521437991
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.14284383936932213
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.14261989619420923
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.1425322011627
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.1426312706918798
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.14250443837905336
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.142337320982909
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.14251286710302036
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.14232604679736224
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.14230871487592087
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.14245207710721627
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.1423294204737871
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.14224573582410813
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.14225833478664596
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.14234205405777833
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.14222824125317857
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.14226328697084456
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.14238077373458788
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.14212778301639412
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.14200263583298886
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.1422266959023655
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.14214444471828974
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.1419096483124627
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.14186353630879345
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.14209761008293958
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.14203255817941998
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.14216660906513817
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.14236018508672715
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.14223619524046038
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.14218349712835232
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.14216867777017447
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.14232034536285532
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.1421444790630505
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.1420879060376997
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.14201135938467624
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.1419473249666594
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.14216394067410654
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.14218997672200204
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.14196627251557167
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.14189760166367418
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.1417740061879158
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.14152432276637522
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.14162526818052415
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.14156093701529196
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.14157327122179567
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.14146081391203252
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.14162201228194266
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.14172793100588024
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.1415948461486686
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.1415767487552431
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.14159452677504417
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.14165236155797795
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.14176592655254133
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.14176016462495528
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.14146349699554328
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.141446660139731
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.14132424902457458
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.14139695969574592
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.14124111417267057
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.14121316083122132
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.14114455581572705
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.14104954405933962
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.14110620187861578
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.14098577095534315
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.14084077618048016
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.14061517946505814
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.14069242617271466
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.14059206437733437
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.14058364203292362
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.14048370035303817
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.14024887466039815
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.14019465405979883
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.14018754306677225
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.14015401739587066
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.14025934837399956
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.1401233685381235
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.14018592652347353
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.140328766915359
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.1403713011382762
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.14027000203107795
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.1403196692621152
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.14012075081160388
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.1400955523053805
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.14019519238903813
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.14034514617042493
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.14051571258842344
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.14053125211491657
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.14073465880006553
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.1407282581895738
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.14075253789525222
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.14070126028924151
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.14074614655007334
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.14059088433661113
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.1404661983974929
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.1404724792725798
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.14034280736142626
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.1403041460226027
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.14053220958227203
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.14049502003108155
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.14055505215699943
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.14055738045436117
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.14043260341353506
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.14050652346638745
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.1404126298579353
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.14047675057902315
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.14063021297985262
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.14071552043772179
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.14077774323523046
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.14085196946406256
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.1408457480505243
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.14078119514100754
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.14078855457981782
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.1406835397084554
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.14060532855512822
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.14069152712034233
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.1405112698282066
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.14047210048640138
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.14064683318138121
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.14075698526132674
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.1407097935805033
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.1406770723316291
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.14062460460978696
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.1406018517752911
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.14064453384381229
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.1406394379677149
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.14061722804267868
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.14068137327497474
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.1406865181401372
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.14079339023200307
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.1407081458317347
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.14078228893103423
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.14074786782997553
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.14075316865833437
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.1407834559800179
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.14071618430768912
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.14069740858770186
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.14063347666617856
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.14060343712568282
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.14066896044400584
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.14049542665718093
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.14047736648043155
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.14042236415419992
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.14051244627026951
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.14049929298926145
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.1403845269160512
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.14035528742296752
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.14030296180008922
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.14029560731007504
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.14031368545417128
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.14033191412232304
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.14027043565144556
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.14011394737683464
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.1400582986339083
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.14006049121568973
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.14006192945920573
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.140054113301101
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.14001424281016603
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.1399804733141705
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.13994474596313006
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.13994415485135772
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.14006607024333415
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.1400279788042072
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.13995434847745028
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.14003558867219565
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.13998939204517255
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.14002763238742197
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.14004373918938381
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.14007040789084776
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.14009380467845875
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.14007290595389427
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.1400887088935704
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.14006389960856505
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.14017260325582404
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.1401870742023408
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.1401705180103355
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.14028882628513706
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.14021637922339786
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.14021713702843108
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.14018462045291036
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.1401334437605453
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.14024813747853548
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.14031619472163065
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.14023748297812574
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.14019550659970656
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.14026678315918856
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.14024960384672921
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.14016933475051038
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.14019983842968942
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.14030379586639594
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.14026028031347604
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.1402376286759235
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.14021825555123782
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.14022285821007902
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.14021314515007866
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.14020431022302335
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.14016036466731654
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.14013812124343367
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.14009241553083543
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.14016786917229557
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.140230433203471
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.14022890090371093
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.14016313580380882
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.14016622144078453
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.14026405209604698
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.1402988933050294
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.14027525179978437
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.14024349351101162
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.14030213318765164
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.14032330134204615
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.14033017497255196
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.14032435698590412
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.14028108483295382
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.14033266237148873
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.1402481212480668
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.14012212627524628
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.14007907965016075
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.14007930222072137
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.14014911340041594
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.14021766694470836
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.1402221791984805
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.140142006961791
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.1401507852230957
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.14023947947060883
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.14023919146330582
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.1401969583964843
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.14018014858107594
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.14006790285792675
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.14009526648065623
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.14002851232930014
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.14018505702764666
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.14015168677092293
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.14012001281560854
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.14013660740161288
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.1401604569457859
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.14019275145331445
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.14018101647667502
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.14015587805676938
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.14007439072643008
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.14002448963558572
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.14006812051361936
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.14010279815612367
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.14019565778852855
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.14020241064504838
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.14019671352475546
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.1402162271930056
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.14030849744238
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.14028317219782674
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.14023540943033166
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.1402208694047875
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.14022985298330612
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.14016360028059358
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.14010581740556838
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.14015762171108428
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.14015085504598956
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.140181254686874
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.1402073753390299
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.14016675026355396
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.1401323064960338
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.14017941172151874
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.1401815285245257
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.14017105883551667
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.14023831436898618
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.1402594187060992
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.14029005437376016
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.1402537515806899
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.14031232718043227
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.14031317714100464
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.14025357367568894
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.14022650604876946
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.1401933630762612
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.14027266743211148
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.14018421777291223
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.1401218292388049
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.1400440358050129
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.14007558856515614
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.14010870095688044
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.1400974458011686
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.14011410012459144
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.1400692936633249
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.14005982263811997
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.14007134174420938
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.14007524329091087
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.1401325599679464
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.14018178257075223
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.14008503439144163
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.14011317925836572
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.14006074097819796
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.140053576156497
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.14005685161028122
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.14002438679115095
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.13996948268514117
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.13998239789858902
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.1399470653430915
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.13989887019536765
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.13985683809261065
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.13993866754440115
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.13992982201077828
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.1399812377989292
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.13997442844502828
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.13994843221622186
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.13993201978486616
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.1399877806180629
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.14005764979554947
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.14012894476763904
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.14010728425759492
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.14008149406627604
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.1402204940917953
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.14018580642129694
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.1402494242136099
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.14026885068317727
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.14021178398360598
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.140193791550426
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.14018108145279043
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.1401428970827463
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.1401332074675962
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.14007891256745172
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.14004265134915328
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.14001797453262085
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.14001940873617916
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.14000759782545544
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.1399481577183578
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.13992995255485108
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.13987573791166832
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.13994789996756873
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.14001659880172743
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.13998354813211586
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.13986558592740386
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.1398127418858084
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.13980452866713747
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.13974799595065246
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.13972664799364642
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.1397235765885394
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.13975810229443433
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.13978656516681873
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.1397627458576388
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.13984591209529235
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.13987120795157015
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.13982590882314577
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.1398603602781264
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.13994018520863183
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.13989492079806382
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.13985780242518706
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.139838097514687
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.1398586026605284
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.13987546073399174
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.13987613257882897
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.13986459475067445
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.13994809583477352
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.1399185021050323
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.1399235532848866
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.13980456242100467
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.13982266812296262
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.139844263320969
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.1398347864814838
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.13986996446419067
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.1399272715147489
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.13998373572442577
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.14002072945871252
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.14005905388203901
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.14014921998762983
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.14014921998762983 valid 0.23372772336006165
LOSS train 0.14014921998762983 valid 0.19634561240673065
LOSS train 0.14014921998762983 valid 0.20017821590105692
LOSS train 0.14014921998762983 valid 0.18755707889795303
LOSS train 0.14014921998762983 valid 0.1837283968925476
LOSS train 0.14014921998762983 valid 0.18957816312710443
LOSS train 0.14014921998762983 valid 0.19756031249250686
LOSS train 0.14014921998762983 valid 0.19636018387973309
LOSS train 0.14014921998762983 valid 0.1951584153705173
LOSS train 0.14014921998762983 valid 0.1979143425822258
LOSS train 0.14014921998762983 valid 0.19684527949853378
LOSS train 0.14014921998762983 valid 0.1947437785565853
LOSS train 0.14014921998762983 valid 0.19338761728543502
LOSS train 0.14014921998762983 valid 0.19313785753079823
LOSS train 0.14014921998762983 valid 0.1895365277926127
LOSS train 0.14014921998762983 valid 0.19088339805603027
LOSS train 0.14014921998762983 valid 0.19292604572632732
LOSS train 0.14014921998762983 valid 0.19178323530488545
LOSS train 0.14014921998762983 valid 0.1937915784747977
LOSS train 0.14014921998762983 valid 0.19504304528236388
LOSS train 0.14014921998762983 valid 0.1949689459233057
LOSS train 0.14014921998762983 valid 0.19358203627846457
LOSS train 0.14014921998762983 valid 0.1935402582521024
LOSS train 0.14014921998762983 valid 0.19323326523105303
LOSS train 0.14014921998762983 valid 0.19145435690879822
LOSS train 0.14014921998762983 valid 0.19131280080630228
LOSS train 0.14014921998762983 valid 0.19123140805297428
LOSS train 0.14014921998762983 valid 0.19188466082726205
LOSS train 0.14014921998762983 valid 0.1920989047864388
LOSS train 0.14014921998762983 valid 0.19276971966028214
LOSS train 0.14014921998762983 valid 0.1937471580120825
LOSS train 0.14014921998762983 valid 0.19302835408598185
LOSS train 0.14014921998762983 valid 0.19332901094899033
LOSS train 0.14014921998762983 valid 0.19277478666866527
LOSS train 0.14014921998762983 valid 0.19475554312978471
LOSS train 0.14014921998762983 valid 0.1947637949552801
LOSS train 0.14014921998762983 valid 0.1958503292219059
LOSS train 0.14014921998762983 valid 0.19616784390650296
LOSS train 0.14014921998762983 valid 0.19530894588201475
LOSS train 0.14014921998762983 valid 0.19529972448945046
LOSS train 0.14014921998762983 valid 0.19498044070674153
LOSS train 0.14014921998762983 valid 0.19563388859941847
LOSS train 0.14014921998762983 valid 0.19512398748896842
LOSS train 0.14014921998762983 valid 0.19566834277727388
LOSS train 0.14014921998762983 valid 0.195633284913169
LOSS train 0.14014921998762983 valid 0.19643342624539914
LOSS train 0.14014921998762983 valid 0.19680293633582743
LOSS train 0.14014921998762983 valid 0.19700776475171247
LOSS train 0.14014921998762983 valid 0.19781329345946408
LOSS train 0.14014921998762983 valid 0.19714678347110748
LOSS train 0.14014921998762983 valid 0.19760157986014498
LOSS train 0.14014921998762983 valid 0.19756451478371254
LOSS train 0.14014921998762983 valid 0.1978488159629534
LOSS train 0.14014921998762983 valid 0.1981709047600075
LOSS train 0.14014921998762983 valid 0.19821362847631627
LOSS train 0.14014921998762983 valid 0.19822825571256025
LOSS train 0.14014921998762983 valid 0.1982445489419134
LOSS train 0.14014921998762983 valid 0.19815515084513302
LOSS train 0.14014921998762983 valid 0.19871588531187026
LOSS train 0.14014921998762983 valid 0.1982273002465566
LOSS train 0.14014921998762983 valid 0.19788808754233064
LOSS train 0.14014921998762983 valid 0.1980600609414039
LOSS train 0.14014921998762983 valid 0.1978262603755981
LOSS train 0.14014921998762983 valid 0.1986132103484124
LOSS train 0.14014921998762983 valid 0.19857336764152234
LOSS train 0.14014921998762983 valid 0.19839212998296274
LOSS train 0.14014921998762983 valid 0.19800062966880513
LOSS train 0.14014921998762983 valid 0.19834373847526662
LOSS train 0.14014921998762983 valid 0.1975337329550066
LOSS train 0.14014921998762983 valid 0.19785849260432378
LOSS train 0.14014921998762983 valid 0.19763331493021738
LOSS train 0.14014921998762983 valid 0.19773998649583924
LOSS train 0.14014921998762983 valid 0.19787610128317792
LOSS train 0.14014921998762983 valid 0.19773535289474437
LOSS train 0.14014921998762983 valid 0.1978211102883021
LOSS train 0.14014921998762983 valid 0.19866250200491203
LOSS train 0.14014921998762983 valid 0.19839821717181763
LOSS train 0.14014921998762983 valid 0.19830874525583708
LOSS train 0.14014921998762983 valid 0.19787442608724667
LOSS train 0.14014921998762983 valid 0.19728851206600667
LOSS train 0.14014921998762983 valid 0.19670480048214947
LOSS train 0.14014921998762983 valid 0.19707580746673956
LOSS train 0.14014921998762983 valid 0.19663126928260527
LOSS train 0.14014921998762983 valid 0.19658233438219344
LOSS train 0.14014921998762983 valid 0.19581140963470234
LOSS train 0.14014921998762983 valid 0.1953001907745073
LOSS train 0.14014921998762983 valid 0.1951986022036651
LOSS train 0.14014921998762983 valid 0.19484547373246064
LOSS train 0.14014921998762983 valid 0.1951273004995303
LOSS train 0.14014921998762983 valid 0.1952679701977306
LOSS train 0.14014921998762983 valid 0.19533970313412802
LOSS train 0.14014921998762983 valid 0.19522214972454568
LOSS train 0.14014921998762983 valid 0.19516114041369448
LOSS train 0.14014921998762983 valid 0.19547507594874564
LOSS train 0.14014921998762983 valid 0.19497245503099342
LOSS train 0.14014921998762983 valid 0.19508625085776052
LOSS train 0.14014921998762983 valid 0.19504036055397742
LOSS train 0.14014921998762983 valid 0.19533501930382788
LOSS train 0.14014921998762983 valid 0.1954627444948813
LOSS train 0.14014921998762983 valid 0.19548249512910842
LOSS train 0.14014921998762983 valid 0.19570285598240275
LOSS train 0.14014921998762983 valid 0.19618103229532055
LOSS train 0.14014921998762983 valid 0.19599037190664162
LOSS train 0.14014921998762983 valid 0.195996302108352
LOSS train 0.14014921998762983 valid 0.19621636455967312
LOSS train 0.14014921998762983 valid 0.19660625185044306
LOSS train 0.14014921998762983 valid 0.19627624304495125
LOSS train 0.14014921998762983 valid 0.19635837028423944
LOSS train 0.14014921998762983 valid 0.19690892674507351
LOSS train 0.14014921998762983 valid 0.19694399413737385
LOSS train 0.14014921998762983 valid 0.19683803899868116
LOSS train 0.14014921998762983 valid 0.19661830964365176
LOSS train 0.14014921998762983 valid 0.19673313213660654
LOSS train 0.14014921998762983 valid 0.1968933856278135
LOSS train 0.14014921998762983 valid 0.19687509692233543
LOSS train 0.14014921998762983 valid 0.197256147090731
LOSS train 0.14014921998762983 valid 0.19739058486416808
LOSS train 0.14014921998762983 valid 0.19710842873585427
LOSS train 0.14014921998762983 valid 0.19693507892744883
LOSS train 0.14014921998762983 valid 0.1966152172535658
LOSS train 0.14014921998762983 valid 0.19646531429665148
LOSS train 0.14014921998762983 valid 0.1964947272275315
LOSS train 0.14014921998762983 valid 0.19665111892107057
LOSS train 0.14014921998762983 valid 0.1970240437696057
LOSS train 0.14014921998762983 valid 0.19681045508384704
LOSS train 0.14014921998762983 valid 0.19699369004321476
LOSS train 0.14014921998762983 valid 0.19683259420507537
LOSS train 0.14014921998762983 valid 0.19685249240137637
LOSS train 0.14014921998762983 valid 0.1972253431183423
LOSS train 0.14014921998762983 valid 0.19702229155943943
LOSS train 0.14014921998762983 valid 0.19682866846787112
LOSS train 0.14014921998762983 valid 0.1963868863654859
LOSS train 0.14014921998762983 valid 0.19606051711659683
LOSS train 0.14014921998762983 valid 0.19621048211606582
LOSS train 0.14014921998762983 valid 0.19605987480393164
LOSS train 0.14014921998762983 valid 0.19586824220331275
LOSS train 0.14014921998762983 valid 0.19559140274994566
LOSS train 0.14014921998762983 valid 0.1954470803772194
LOSS train 0.14014921998762983 valid 0.1951631222911876
LOSS train 0.14014921998762983 valid 0.1952468253672123
LOSS train 0.14014921998762983 valid 0.19528321034096657
LOSS train 0.14014921998762983 valid 0.19525587243933074
LOSS train 0.14014921998762983 valid 0.19534319222390234
LOSS train 0.14014921998762983 valid 0.1954763252288103
LOSS train 0.14014921998762983 valid 0.19540263948769404
LOSS train 0.14014921998762983 valid 0.19544518728778787
LOSS train 0.14014921998762983 valid 0.19530514957142525
LOSS train 0.14014921998762983 valid 0.19628864748252406
LOSS train 0.14014921998762983 valid 0.19637795352695772
LOSS train 0.14014921998762983 valid 0.1961794106165568
LOSS train 0.14014921998762983 valid 0.19649126198118097
LOSS train 0.14014921998762983 valid 0.19631123689836577
LOSS train 0.14014921998762983 valid 0.19632218252210057
LOSS train 0.14014921998762983 valid 0.19632477400364814
LOSS train 0.14014921998762983 valid 0.19620830272474596
LOSS train 0.14014921998762983 valid 0.19625911355400696
LOSS train 0.14014921998762983 valid 0.19633274682008536
LOSS train 0.14014921998762983 valid 0.1963377359169948
LOSS train 0.14014921998762983 valid 0.19635713212894942
LOSS train 0.14014921998762983 valid 0.1962333869189024
LOSS train 0.14014921998762983 valid 0.1961592628348688
LOSS train 0.14014921998762983 valid 0.19587491112358776
LOSS train 0.14014921998762983 valid 0.19562952951785245
LOSS train 0.14014921998762983 valid 0.19535621719025983
LOSS train 0.14014921998762983 valid 0.19532824258009593
LOSS train 0.14014921998762983 valid 0.1952401506434004
LOSS train 0.14014921998762983 valid 0.19545222908079982
LOSS train 0.14014921998762983 valid 0.19544278901247752
LOSS train 0.14014921998762983 valid 0.1956047610594676
LOSS train 0.14014921998762983 valid 0.19567360816633
LOSS train 0.14014921998762983 valid 0.19560304292809894
LOSS train 0.14014921998762983 valid 0.19549297030235446
LOSS train 0.14014921998762983 valid 0.19557316187833776
LOSS train 0.14014921998762983 valid 0.19551077887587165
LOSS train 0.14014921998762983 valid 0.19538185673100608
LOSS train 0.14014921998762983 valid 0.1954648532820019
LOSS train 0.14014921998762983 valid 0.19540848848173173
LOSS train 0.14014921998762983 valid 0.19547598532746346
LOSS train 0.14014921998762983 valid 0.19534327689162845
LOSS train 0.14014921998762983 valid 0.1953226370116075
LOSS train 0.14014921998762983 valid 0.19536687845353923
LOSS train 0.14014921998762983 valid 0.19528408823432503
LOSS train 0.14014921998762983 valid 0.1952579348790841
LOSS train 0.14014921998762983 valid 0.19528239641500555
LOSS train 0.14014921998762983 valid 0.19510514325386769
LOSS train 0.14014921998762983 valid 0.19515795092428884
LOSS train 0.14014921998762983 valid 0.19501443414445868
LOSS train 0.14014921998762983 valid 0.19489380162447056
LOSS train 0.14014921998762983 valid 0.19485636591595948
LOSS train 0.14014921998762983 valid 0.19495110331397308
LOSS train 0.14014921998762983 valid 0.19489333462652736
LOSS train 0.14014921998762983 valid 0.19491938423986235
LOSS train 0.14014921998762983 valid 0.19478148654334904
LOSS train 0.14014921998762983 valid 0.19459081156966612
LOSS train 0.14014921998762983 valid 0.19431154024906647
LOSS train 0.14014921998762983 valid 0.1943329415303104
LOSS train 0.14014921998762983 valid 0.19450799198017507
LOSS train 0.14014921998762983 valid 0.19440670838259688
LOSS train 0.14014921998762983 valid 0.19446367616929003
LOSS train 0.14014921998762983 valid 0.19438569158315658
LOSS train 0.14014921998762983 valid 0.1943767462945103
LOSS train 0.14014921998762983 valid 0.19434181888504784
LOSS train 0.14014921998762983 valid 0.19438207230250823
LOSS train 0.14014921998762983 valid 0.19460059351780834
LOSS train 0.14014921998762983 valid 0.19444203762019552
LOSS train 0.14014921998762983 valid 0.19447193236894977
LOSS train 0.14014921998762983 valid 0.19450493372868802
LOSS train 0.14014921998762983 valid 0.19447063446904606
LOSS train 0.14014921998762983 valid 0.19447311595010985
LOSS train 0.14014921998762983 valid 0.19456256450641723
LOSS train 0.14014921998762983 valid 0.1945561096425305
LOSS train 0.14014921998762983 valid 0.19455534304088018
LOSS train 0.14014921998762983 valid 0.19457343864328985
LOSS train 0.14014921998762983 valid 0.19449399648425736
LOSS train 0.14014921998762983 valid 0.19440658868745317
LOSS train 0.14014921998762983 valid 0.19424917052189508
LOSS train 0.14014921998762983 valid 0.19403372910989594
LOSS train 0.14014921998762983 valid 0.19402104773379247
LOSS train 0.14014921998762983 valid 0.19414668581257127
LOSS train 0.14014921998762983 valid 0.19413695430213754
LOSS train 0.14014921998762983 valid 0.19406855483939744
LOSS train 0.14014921998762983 valid 0.19411182262607524
LOSS train 0.14014921998762983 valid 0.19416596142433148
LOSS train 0.14014921998762983 valid 0.19423517511625374
LOSS train 0.14014921998762983 valid 0.1944790393776364
LOSS train 0.14014921998762983 valid 0.19464883326956656
LOSS train 0.14014921998762983 valid 0.19473607628093417
LOSS train 0.14014921998762983 valid 0.19476515479516565
LOSS train 0.14014921998762983 valid 0.19471413352603995
LOSS train 0.14014921998762983 valid 0.194799525970998
LOSS train 0.14014921998762983 valid 0.19489331640206375
LOSS train 0.14014921998762983 valid 0.19485677489689712
LOSS train 0.14014921998762983 valid 0.19484082977147574
LOSS train 0.14014921998762983 valid 0.19481364268268275
LOSS train 0.14014921998762983 valid 0.19493105931484953
LOSS train 0.14014921998762983 valid 0.1948130390275333
LOSS train 0.14014921998762983 valid 0.19481593785406667
LOSS train 0.14014921998762983 valid 0.1947357209414995
LOSS train 0.14014921998762983 valid 0.19466529711769215
LOSS train 0.14014921998762983 valid 0.19471316083023946
LOSS train 0.14014921998762983 valid 0.19477929688093573
LOSS train 0.14014921998762983 valid 0.19464410022516881
LOSS train 0.14014921998762983 valid 0.19485594001081255
LOSS train 0.14014921998762983 valid 0.1949301694260269
LOSS train 0.14014921998762983 valid 0.1950030053756675
LOSS train 0.14014921998762983 valid 0.1948738817155846
LOSS train 0.14014921998762983 valid 0.19496034206407756
LOSS train 0.14014921998762983 valid 0.19490194200508057
LOSS train 0.14014921998762983 valid 0.19486679669364868
LOSS train 0.14014921998762983 valid 0.19486228024959565
LOSS train 0.14014921998762983 valid 0.1948018325752471
LOSS train 0.14014921998762983 valid 0.19490808354956762
LOSS train 0.14014921998762983 valid 0.1948270655196646
LOSS train 0.14014921998762983 valid 0.1947815610667852
LOSS train 0.14014921998762983 valid 0.194769298212201
LOSS train 0.14014921998762983 valid 0.1947007147828117
LOSS train 0.14014921998762983 valid 0.19455683903935356
LOSS train 0.14014921998762983 valid 0.19475633622139923
LOSS train 0.14014921998762983 valid 0.19476494267870562
LOSS train 0.14014921998762983 valid 0.194751244611465
LOSS train 0.14014921998762983 valid 0.19493086213581406
LOSS train 0.14014921998762983 valid 0.19492024405084493
LOSS train 0.14014921998762983 valid 0.19496630970742765
LOSS train 0.14014921998762983 valid 0.19498436327910784
LOSS train 0.14014921998762983 valid 0.19512994373744388
LOSS train 0.14014921998762983 valid 0.19514009614187972
LOSS train 0.14014921998762983 valid 0.1951796045240838
LOSS train 0.14014921998762983 valid 0.19523823945157565
LOSS train 0.14014921998762983 valid 0.1954296365878839
LOSS train 0.14014921998762983 valid 0.19551893969376882
LOSS train 0.14014921998762983 valid 0.19564454372958504
LOSS train 0.14014921998762983 valid 0.19605722508448012
LOSS train 0.14014921998762983 valid 0.19621820289355058
LOSS train 0.14014921998762983 valid 0.1962359839112219
LOSS train 0.14014921998762983 valid 0.19618547417900778
LOSS train 0.14014921998762983 valid 0.19613529239659724
LOSS train 0.14014921998762983 valid 0.19608448332827874
LOSS train 0.14014921998762983 valid 0.19588534759102966
LOSS train 0.14014921998762983 valid 0.19589933005285093
LOSS train 0.14014921998762983 valid 0.19580175834042685
LOSS train 0.14014921998762983 valid 0.1957622613771106
LOSS train 0.14014921998762983 valid 0.19555122072392322
LOSS train 0.14014921998762983 valid 0.19558184954597757
LOSS train 0.14014921998762983 valid 0.19563220142268797
LOSS train 0.14014921998762983 valid 0.19564204399000135
LOSS train 0.14014921998762983 valid 0.19566571410302516
LOSS train 0.14014921998762983 valid 0.1956588679905137
LOSS train 0.14014921998762983 valid 0.19566606797484887
LOSS train 0.14014921998762983 valid 0.1956369151912346
LOSS train 0.14014921998762983 valid 0.19579719078951868
LOSS train 0.14014921998762983 valid 0.1955761976565692
LOSS train 0.14014921998762983 valid 0.19561634018813093
LOSS train 0.14014921998762983 valid 0.19564947552038134
LOSS train 0.14014921998762983 valid 0.19578671024567415
LOSS train 0.14014921998762983 valid 0.1959240953295918
LOSS train 0.14014921998762983 valid 0.1958203174777933
LOSS train 0.14014921998762983 valid 0.19582590094519786
LOSS train 0.14014921998762983 valid 0.19578400344256586
LOSS train 0.14014921998762983 valid 0.1958013382245067
LOSS train 0.14014921998762983 valid 0.19581766789158186
LOSS train 0.14014921998762983 valid 0.1958596518269409
LOSS train 0.14014921998762983 valid 0.19579080196208512
LOSS train 0.14014921998762983 valid 0.19585979324166136
LOSS train 0.14014921998762983 valid 0.19594594566641668
LOSS train 0.14014921998762983 valid 0.1958453138832186
LOSS train 0.14014921998762983 valid 0.19582280302359387
LOSS train 0.14014921998762983 valid 0.1958634010743629
LOSS train 0.14014921998762983 valid 0.19582183731647282
LOSS train 0.14014921998762983 valid 0.1958451105167179
LOSS train 0.14014921998762983 valid 0.19579100190631804
LOSS train 0.14014921998762983 valid 0.1956784360086803
LOSS train 0.14014921998762983 valid 0.19580195376124138
LOSS train 0.14014921998762983 valid 0.19588621689107852
LOSS train 0.14014921998762983 valid 0.19596519966603845
LOSS train 0.14014921998762983 valid 0.1958982565100231
LOSS train 0.14014921998762983 valid 0.19585756108730654
LOSS train 0.14014921998762983 valid 0.19596537811334952
LOSS train 0.14014921998762983 valid 0.19610528385489243
LOSS train 0.14014921998762983 valid 0.1961558484545322
LOSS train 0.14014921998762983 valid 0.19606195008382202
LOSS train 0.14014921998762983 valid 0.1961771074763711
LOSS train 0.14014921998762983 valid 0.1961750557518894
LOSS train 0.14014921998762983 valid 0.19615540295931577
LOSS train 0.14014921998762983 valid 0.19624887737962934
LOSS train 0.14014921998762983 valid 0.19622954882108248
LOSS train 0.14014921998762983 valid 0.19636548950445432
LOSS train 0.14014921998762983 valid 0.19646587457496456
LOSS train 0.14014921998762983 valid 0.196342469397478
LOSS train 0.14014921998762983 valid 0.19652800905124757
LOSS train 0.14014921998762983 valid 0.19642442731243193
LOSS train 0.14014921998762983 valid 0.1963035103687707
LOSS train 0.14014921998762983 valid 0.19625000152961317
LOSS train 0.14014921998762983 valid 0.1962648828376879
LOSS train 0.14014921998762983 valid 0.19636381363975788
LOSS train 0.14014921998762983 valid 0.1963504798821549
LOSS train 0.14014921998762983 valid 0.19645015629274504
LOSS train 0.14014921998762983 valid 0.19653264231957737
LOSS train 0.14014921998762983 valid 0.19651475694228912
LOSS train 0.14014921998762983 valid 0.19652547781797988
LOSS train 0.14014921998762983 valid 0.19649782036157215
LOSS train 0.14014921998762983 valid 0.19640374126846838
LOSS train 0.14014921998762983 valid 0.19634021831709042
LOSS train 0.14014921998762983 valid 0.19634530730219693
LOSS train 0.14014921998762983 valid 0.19662480827334317
LOSS train 0.14014921998762983 valid 0.19666529444680697
LOSS train 0.14014921998762983 valid 0.1966840919387134
LOSS train 0.14014921998762983 valid 0.19658352176978197
LOSS train 0.14014921998762983 valid 0.19651986397374635
LOSS train 0.14014921998762983 valid 0.19654075049909958
LOSS train 0.14014921998762983 valid 0.1964632049202919
LOSS train 0.14014921998762983 valid 0.19639366873648773
LOSS train 0.14014921998762983 valid 0.19640085714953867
LOSS train 0.14014921998762983 valid 0.1963605326481649
LOSS train 0.14014921998762983 valid 0.1963311295724858
LOSS train 0.14014921998762983 valid 0.19645647906081778
LOSS train 0.14014921998762983 valid 0.19645872800035424
LOSS train 0.14014921998762983 valid 0.19650316426233083
LOSS train 0.14014921998762983 valid 0.1964287165989423
LOSS train 0.14014921998762983 valid 0.1964270004738008
LOSS train 0.14014921998762983 valid 0.19636676191455787
LOSS train 0.14014921998762983 valid 0.19638127241273337
LOSS train 0.14014921998762983 valid 0.19647566101498368
LOSS train 0.14014921998762983 valid 0.19631268564334586
LOSS train 0.14014921998762983 valid 0.19634520696414695
LOSS train 0.14014921998762983 valid 0.1963511493516295
LOSS train 0.14014921998762983 valid 0.196331020953551
LOSS train 0.14014921998762983 valid 0.19620214504182176
LOSS train 0.14014921998762983 valid 0.19619228034887626
LOSS train 0.14014921998762983 valid 0.19623247386640327
EPOCH 8:
  batch 1 loss: 0.12791593372821808
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.12637850269675255
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.12777652591466904
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.13399062491953373
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.14118844121694565
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.14125183100501695
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.13259099849632808
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.13266469351947308
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.13199704388777414
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.13064093440771102
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.13114344396374442
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.1298955207069715
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.13060106795567733
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.12940225643771036
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.130136768023173
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.1313717346638441
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.12998464878867655
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.13169841137197283
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.1310351749784068
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.13042691797018052
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.1307947337627411
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.130848460576751
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.13116849958896637
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.13299409051736197
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.13257627248764037
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.1329430123934379
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.13368659770047223
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.13346989612494195
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.13320577607072634
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.1337965076168378
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.13416321671778156
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.13388546602800488
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.1336239937579993
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.13458031635074055
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.13569757299763816
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.13576173658172289
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.13597535563481822
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.13553878449295698
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.13627715160449347
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.1360851326957345
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.13616361614407563
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.13630850027714456
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.13698117313689964
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.13650875860317188
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.13744822988907496
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.13773859891554582
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.1374387393923516
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.13685598162313303
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.1367330101071572
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.13682396650314332
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.1368222052560133
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.13738065069684616
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.13758310253890055
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.13788799389644904
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.13824402581561696
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.13836124938513553
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.13859743899420687
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.13854334323570647
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.13918790903131842
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.13917759731411933
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.1390257792883232
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.13891721180369776
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.13939547373188865
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.13932098634541035
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.13934583939038792
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.13933359267133655
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.1391784537639191
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.1393408591256422
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.13963006717571314
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.1397725250039782
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.1400348806465176
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.13997043607135615
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.14015977207111985
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.14004869215391777
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.1398115227619807
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.13983603723739324
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.1396736970969609
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.13921524402804863
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.13931966667311102
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.1393641290254891
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.13962638884046932
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.1392374600215656
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.13917628541050187
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.13983624854258128
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.13994495132390192
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.14013641206331032
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.14023799029574996
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.14025536061010577
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.14063832147067853
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.14076515485843022
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.14072087505361536
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.14070047324766283
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.14091503171510594
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.14105678825302326
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.14138436866433998
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.14146093341211477
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.14117561357537495
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.1411449992839171
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.1413351125789411
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.14112000420689583
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.1412064174909403
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.1410827160465951
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.14100850944958845
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.1411792875195925
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.14075788082111448
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.14066825406450145
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.14084670288819018
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.14066141084940345
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.1409760620615898
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.1413729048588059
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.14129083368692313
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.14102925573076522
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.14070631233991776
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.14094474571838714
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.1407328448217848
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.14065453198192449
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.1407771014377602
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.14067685155797813
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.14053080306083215
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.14072387758642435
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.14055415427635523
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.14053164538545687
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.14065208940244303
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.14052436867308232
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.14042073196172714
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.14039678711976325
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.14048454828384355
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.14040763856610283
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.1404266703382943
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.14058428137348247
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.14031974328610733
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.1401958472349427
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.14043492683790681
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.1403047291756566
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.1400556359578062
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.1399760628119111
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.14020013575353762
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.14013971199375996
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.14028380462806
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.1405114408582449
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.14038758281063526
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.1403578131026785
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.14032735623471387
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.14046463096100423
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.14027233257375915
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.14026623392758303
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.14019681452488414
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.1401454448901318
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.14038203266643037
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.14039465924104055
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.14016062070597088
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.14009829571372584
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.13998335473288118
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.13975514103839923
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.13982752734614956
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.1397422418380395
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.13973893272648952
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.13962988517706906
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.13981381780321492
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.13988930452615023
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.13977792039976356
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.13979029430099477
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.13978557430344857
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.1398438394705697
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.13992891839959404
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.13986005782183394
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.1395520510341593
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.13950917457363435
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.13937785163433594
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.1394594610613935
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.13930482543699924
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.13929466270776683
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.13925206687064529
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.13916134457478577
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.1392098354441779
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.13908823118121785
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.13895041242999545
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.1387384898458304
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.13879596000776612
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.138727776085337
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.13872120834022597
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.13864561645211754
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.13841202570115282
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.13838677302650784
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.13836827270082525
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.13836264185687547
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.13845962357712302
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.13833569782845517
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.13842400285617384
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.13857651752860922
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.1386338103348048
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.13853123147661486
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.138604463845337
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.13842403297264552
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.1384071870492055
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.13850430999787486
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.13865687164860935
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.13885330890465264
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.13885844375320416
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.13903321877121924
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.1390449999280237
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.13908280343702523
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.13902584731285209
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.13907800381090127
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.1389088783322311
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.13876465064229318
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.13878142193031773
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.1386277059522959
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.13860009075922258
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.13879290450186957
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.1387817962757219
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.1388331617246259
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.13881459062647933
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.13866721198102025
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.13874153612658036
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.13863235091169676
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.13870477017169724
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.13886008404810493
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.13897081461126945
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.13904041559858757
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.13910010119908536
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.13909679872764125
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.13902055929861795
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.1390500561600285
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.13891914635896682
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.13883213886776857
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.13892873468509329
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.13876499480714924
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.1387514139822477
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.13892005924945292
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.13906248936276416
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.13903181097502337
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.13901556296026246
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.13896499558264375
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.13894882668206032
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.13901922577139686
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.1389908242011875
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.13896511360502042
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.13906069271359983
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.13904028804972768
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.13914082601233638
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.13906005505195335
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.13911760159971293
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.13908539858997845
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.13911347808886548
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.13916986889955474
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.13911166220058796
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.13907199371005258
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.13900376981999502
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.13896139568090438
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.13902909376944203
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.13885986586175267
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.13882778284577985
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.13876870397742339
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.13884799416158713
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.13882143184309825
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.13869251763078488
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.13864432720019837
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.1385916403591863
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.13858411059929773
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.13860448544052825
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.1386001921109571
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.13853741168069295
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.1383874249277693
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.1383252484056185
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.13833245248498774
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.13832295281387
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.13832005547053777
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.1383001454811557
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.13824944606533757
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.13822816955647346
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.1382396477548515
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.1383778024942447
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.13833491710850793
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.13826617083766243
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.1383356366144574
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.13828547627056548
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.13831894131873151
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.13831750242086294
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.13835447606231485
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.1383794112880035
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.1383528622223976
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.13836232206846716
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.13833423934771982
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.13844771965553887
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.13845487459973022
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.13843334629768278
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.1385641678546866
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.13848360352037686
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.13849137651509252
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.13846338798909663
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.13843192012138564
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.13855614220729867
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.13863546076883265
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.13855882534536265
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.13852625466077714
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.13858107309349457
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.13855838075580212
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.13848540677773116
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.1385184584806363
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.13864644517831232
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.13860042210643655
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.13859060126366002
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.13857043426679938
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.13858618521299518
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.13857021810961703
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.13856179793805176
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.13851706127261187
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.1384852962104248
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.13843316267574987
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.1384914153737654
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.1385503135717068
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.13854918959803475
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.13847570766688913
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.13848280499851892
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.1385853638565993
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.13861236375777128
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.13858703894060362
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.13855304874970248
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.13860347014851868
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.13861041875828836
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.1386242667155236
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.13862584162238212
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.13858205181213074
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.1386346888083678
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.13855774657218003
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.13844260063010982
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.13840202296652446
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.13840955962344867
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.13845770354523804
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.13851929404224153
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.1385029504995748
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.13842740449103508
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.13843748125130545
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.1385173560968086
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.1385134556552484
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.13847584518374603
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.13846254820478032
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.1383668066477705
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.13838896054555389
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.1383441993611649
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.13852698667443286
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.13851011275115582
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.1384956790567484
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.13850650910450066
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.13853567358472443
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.13858009030994145
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.13855963553591022
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.13852331035885224
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.13843260441507613
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.1383905498432977
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.1384413377170197
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.1384794277062497
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.13856250559121874
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.13857246983638952
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.1385517893672994
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.1385856132839574
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.13868433459760757
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.1386590505999443
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.13861227989610697
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.1386073471312708
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.1386265205373751
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.13856619344023632
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.13850068964145995
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.1385540977732776
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.1385499693331171
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.13858037115117836
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.13863146191705827
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.13859710707089443
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.13855500102445886
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.13860150033010626
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.13858115018135117
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.13857994156692366
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.13864423619473681
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.1386765223145485
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.13870141225213067
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.13867860303238153
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.13873554992849235
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.13872759954360042
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.138645786027375
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.13861322756905567
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.13857855981323108
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.13865899971586607
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.13857016755112758
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.1385214314832316
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.13843383898697986
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.1384586083488563
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.13849515955626351
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.1384975396881986
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.1385061760361378
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.13846941048379444
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.13845913750784739
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.13849095063658465
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.138494292189022
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.13854236674459675
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.1385686806958131
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.138480939552826
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.13850571462257424
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.1384598267145623
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.13845549969002605
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.1384705224648081
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.13844460617769416
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.1383991382516939
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.1384046406180847
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.1383645909803885
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.13831192229328484
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.13825973509570597
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.13836126299757584
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.1383313870007368
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.13838546828525822
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.13839010241692953
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.13836522035083723
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.1383514079574234
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.13840136241509718
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.13847280277545193
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.1385322383318383
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.13851351685804142
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.13848836719989777
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.1386288446785442
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.13859503379180318
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.13866511273978724
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.13868383805474963
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.1386223675652317
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.13860389143931415
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.13859751687330357
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.1385576146099489
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.13854224938000673
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.13849057647112373
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.13846930458551243
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.13844983082178028
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.13845589794996443
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.13845287173710488
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.1383963565731434
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.13838663256044761
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.1383283445547367
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.13840095920983805
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.13847602296884873
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.13843940160998472
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.1383202745152769
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.13825978303158826
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.13825956444824095
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.13820702911299818
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.13818710398256914
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.13818231770382808
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.13820940036787077
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.13824310272330662
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.13822148755292765
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.13830749050248414
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.13832439892076967
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.1382813733153873
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.13829734372986924
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.1383796342838127
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.13833088554425482
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.13829528825923734
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.13827133800957228
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.13829480268453298
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.13830346651619843
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.13829469667772018
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.13828242448420305
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.13836491794689842
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.13834683147790375
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.1383493910362194
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.13823441734646102
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.13825301557843542
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.13826217021672957
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.13826268797844266
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.1382930586214239
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.13835377540662247
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.13840234548107647
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.1384400962673603
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.1384686646219778
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.13857219050938296
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.13857219050938296 valid 0.23654955625534058
LOSS train 0.13857219050938296 valid 0.19795982539653778
LOSS train 0.13857219050938296 valid 0.2012338787317276
LOSS train 0.13857219050938296 valid 0.18810106068849564
LOSS train 0.13857219050938296 valid 0.1838641494512558
LOSS train 0.13857219050938296 valid 0.1896776631474495
LOSS train 0.13857219050938296 valid 0.19774789895330155
LOSS train 0.13857219050938296 valid 0.19656839966773987
LOSS train 0.13857219050938296 valid 0.19541899032062954
LOSS train 0.13857219050938296 valid 0.1979353532195091
LOSS train 0.13857219050938296 valid 0.19675696031613785
LOSS train 0.13857219050938296 valid 0.1946235659221808
LOSS train 0.13857219050938296 valid 0.19324102997779846
LOSS train 0.13857219050938296 valid 0.19302459806203842
LOSS train 0.13857219050938296 valid 0.18942771355311075
LOSS train 0.13857219050938296 valid 0.1907042944803834
LOSS train 0.13857219050938296 valid 0.1928246967932757
LOSS train 0.13857219050938296 valid 0.19157800409528944
LOSS train 0.13857219050938296 valid 0.19361838855241476
LOSS train 0.13857219050938296 valid 0.19483577013015746
LOSS train 0.13857219050938296 valid 0.19470976364044917
LOSS train 0.13857219050938296 valid 0.19329168986190448
LOSS train 0.13857219050938296 valid 0.19328690352647201
LOSS train 0.13857219050938296 valid 0.19295587949454784
LOSS train 0.13857219050938296 valid 0.19115979552268983
LOSS train 0.13857219050938296 valid 0.19100116881040427
LOSS train 0.13857219050938296 valid 0.1909417653525317
LOSS train 0.13857219050938296 valid 0.19146947722349847
LOSS train 0.13857219050938296 valid 0.19167418418259458
LOSS train 0.13857219050938296 valid 0.19227833251158397
LOSS train 0.13857219050938296 valid 0.19325924400360353
LOSS train 0.13857219050938296 valid 0.19254666147753596
LOSS train 0.13857219050938296 valid 0.19280927696011282
LOSS train 0.13857219050938296 valid 0.192254315404331
LOSS train 0.13857219050938296 valid 0.19427263481276377
LOSS train 0.13857219050938296 valid 0.1942134996255239
LOSS train 0.13857219050938296 valid 0.1953212787976136
LOSS train 0.13857219050938296 valid 0.1956112863201844
LOSS train 0.13857219050938296 valid 0.19477983506826255
LOSS train 0.13857219050938296 valid 0.1947392325848341
LOSS train 0.13857219050938296 valid 0.19443726212513157
LOSS train 0.13857219050938296 valid 0.1950556774224554
LOSS train 0.13857219050938296 valid 0.1946029046247172
LOSS train 0.13857219050938296 valid 0.19513713432983917
LOSS train 0.13857219050938296 valid 0.1951416939496994
LOSS train 0.13857219050938296 valid 0.19596711675757947
LOSS train 0.13857219050938296 valid 0.19634103965252003
LOSS train 0.13857219050938296 valid 0.19653564194838205
LOSS train 0.13857219050938296 valid 0.19733020176692884
LOSS train 0.13857219050938296 valid 0.19669641464948653
LOSS train 0.13857219050938296 valid 0.19715853356847576
LOSS train 0.13857219050938296 valid 0.1971289303440314
LOSS train 0.13857219050938296 valid 0.19739642654949763
LOSS train 0.13857219050938296 valid 0.19769335400175164
LOSS train 0.13857219050938296 valid 0.19776501736857674
LOSS train 0.13857219050938296 valid 0.19780570827424526
LOSS train 0.13857219050938296 valid 0.1978370894988378
LOSS train 0.13857219050938296 valid 0.19773833916105074
LOSS train 0.13857219050938296 valid 0.19830833526991182
LOSS train 0.13857219050938296 valid 0.19779332255323728
LOSS train 0.13857219050938296 valid 0.19745558109439787
LOSS train 0.13857219050938296 valid 0.19761889932617063
LOSS train 0.13857219050938296 valid 0.197365126912556
LOSS train 0.13857219050938296 valid 0.19814800564199686
LOSS train 0.13857219050938296 valid 0.19810981750488282
LOSS train 0.13857219050938296 valid 0.19792300959428152
LOSS train 0.13857219050938296 valid 0.19756117307428103
LOSS train 0.13857219050938296 valid 0.19789229771670172
LOSS train 0.13857219050938296 valid 0.19707674824673196
LOSS train 0.13857219050938296 valid 0.19741518029144833
LOSS train 0.13857219050938296 valid 0.19720157540180314
LOSS train 0.13857219050938296 valid 0.1973044741898775
LOSS train 0.13857219050938296 valid 0.1974304242901606
LOSS train 0.13857219050938296 valid 0.19727107640859243
LOSS train 0.13857219050938296 valid 0.19735893229643503
LOSS train 0.13857219050938296 valid 0.19822475451387858
LOSS train 0.13857219050938296 valid 0.1979492192531561
LOSS train 0.13857219050938296 valid 0.19785430205938143
LOSS train 0.13857219050938296 valid 0.19741807500772837
LOSS train 0.13857219050938296 valid 0.19683522451668978
LOSS train 0.13857219050938296 valid 0.19622935667450045
LOSS train 0.13857219050938296 valid 0.19661581152822913
LOSS train 0.13857219050938296 valid 0.19617945673954057
LOSS train 0.13857219050938296 valid 0.1961283923259803
LOSS train 0.13857219050938296 valid 0.19533428847789763
LOSS train 0.13857219050938296 valid 0.19482439167277757
LOSS train 0.13857219050938296 valid 0.19470831854590054
LOSS train 0.13857219050938296 valid 0.19435740702531554
LOSS train 0.13857219050938296 valid 0.1946287707666333
LOSS train 0.13857219050938296 valid 0.19475986626413133
LOSS train 0.13857219050938296 valid 0.19483179803732986
LOSS train 0.13857219050938296 valid 0.19472043766923572
LOSS train 0.13857219050938296 valid 0.19465226415664919
LOSS train 0.13857219050938296 valid 0.19496659316281054
LOSS train 0.13857219050938296 valid 0.1944515286307586
LOSS train 0.13857219050938296 valid 0.19457568740472198
LOSS train 0.13857219050938296 valid 0.1945210781294046
LOSS train 0.13857219050938296 valid 0.19481730293862673
LOSS train 0.13857219050938296 valid 0.194929925630791
LOSS train 0.13857219050938296 valid 0.1949533624947071
LOSS train 0.13857219050938296 valid 0.19516431178786967
LOSS train 0.13857219050938296 valid 0.19562852514140747
LOSS train 0.13857219050938296 valid 0.19541932425452668
LOSS train 0.13857219050938296 valid 0.19543954371832883
LOSS train 0.13857219050938296 valid 0.1956813321227119
LOSS train 0.13857219050938296 valid 0.1960824429988861
LOSS train 0.13857219050938296 valid 0.19575079746335466
LOSS train 0.13857219050938296 valid 0.1958211025706044
LOSS train 0.13857219050938296 valid 0.19637245978784124
LOSS train 0.13857219050938296 valid 0.19639930616725576
LOSS train 0.13857219050938296 valid 0.19628038277497162
LOSS train 0.13857219050938296 valid 0.19605824976627315
LOSS train 0.13857219050938296 valid 0.19616123994367313
LOSS train 0.13857219050938296 valid 0.1963347905037696
LOSS train 0.13857219050938296 valid 0.1963281564090563
LOSS train 0.13857219050938296 valid 0.1967250433718336
LOSS train 0.13857219050938296 valid 0.19684224289197189
LOSS train 0.13857219050938296 valid 0.19655295865515532
LOSS train 0.13857219050938296 valid 0.1963814793765044
LOSS train 0.13857219050938296 valid 0.1960564692815145
LOSS train 0.13857219050938296 valid 0.1958896863805361
LOSS train 0.13857219050938296 valid 0.19591814487195405
LOSS train 0.13857219050938296 valid 0.1960696731883336
LOSS train 0.13857219050938296 valid 0.1964481994509697
LOSS train 0.13857219050938296 valid 0.19623688817024232
LOSS train 0.13857219050938296 valid 0.19641712130535216
LOSS train 0.13857219050938296 valid 0.19625434913034515
LOSS train 0.13857219050938296 valid 0.1962751231621951
LOSS train 0.13857219050938296 valid 0.19665702934875046
LOSS train 0.13857219050938296 valid 0.19643829866097523
LOSS train 0.13857219050938296 valid 0.19623219477766343
LOSS train 0.13857219050938296 valid 0.19580061440214966
LOSS train 0.13857219050938296 valid 0.19546073417466386
LOSS train 0.13857219050938296 valid 0.19562005229405502
LOSS train 0.13857219050938296 valid 0.19546901373951522
LOSS train 0.13857219050938296 valid 0.195273953873445
LOSS train 0.13857219050938296 valid 0.1949925706551893
LOSS train 0.13857219050938296 valid 0.19484995359527893
LOSS train 0.13857219050938296 valid 0.19457666436545282
LOSS train 0.13857219050938296 valid 0.19465347336871283
LOSS train 0.13857219050938296 valid 0.1946843922772306
LOSS train 0.13857219050938296 valid 0.19465454248055605
LOSS train 0.13857219050938296 valid 0.1947393624724208
LOSS train 0.13857219050938296 valid 0.19486422878172663
LOSS train 0.13857219050938296 valid 0.19478696430551595
LOSS train 0.13857219050938296 valid 0.19482071748743318
LOSS train 0.13857219050938296 valid 0.1946665232076126
LOSS train 0.13857219050938296 valid 0.1956553260820943
LOSS train 0.13857219050938296 valid 0.1957388959274996
LOSS train 0.13857219050938296 valid 0.195540280242761
LOSS train 0.13857219050938296 valid 0.19584878617959306
LOSS train 0.13857219050938296 valid 0.19566651472919866
LOSS train 0.13857219050938296 valid 0.19568624771108814
LOSS train 0.13857219050938296 valid 0.19568763402375308
LOSS train 0.13857219050938296 valid 0.19556594683277992
LOSS train 0.13857219050938296 valid 0.19561607428850272
LOSS train 0.13857219050938296 valid 0.19569234170351818
LOSS train 0.13857219050938296 valid 0.1956922161994101
LOSS train 0.13857219050938296 valid 0.1957216813879193
LOSS train 0.13857219050938296 valid 0.19559332011267544
LOSS train 0.13857219050938296 valid 0.1955161846202353
LOSS train 0.13857219050938296 valid 0.1952361351361981
LOSS train 0.13857219050938296 valid 0.19498615277691123
LOSS train 0.13857219050938296 valid 0.19469371492542872
LOSS train 0.13857219050938296 valid 0.1946615117968935
LOSS train 0.13857219050938296 valid 0.19458645339830813
LOSS train 0.13857219050938296 valid 0.19480613909081784
LOSS train 0.13857219050938296 valid 0.19478908837551162
LOSS train 0.13857219050938296 valid 0.1949479965239587
LOSS train 0.13857219050938296 valid 0.1950145745102097
LOSS train 0.13857219050938296 valid 0.1949460964802413
LOSS train 0.13857219050938296 valid 0.19484556873523912
LOSS train 0.13857219050938296 valid 0.19490762563110087
LOSS train 0.13857219050938296 valid 0.19484466417082424
LOSS train 0.13857219050938296 valid 0.1947145766871316
LOSS train 0.13857219050938296 valid 0.19479940848594363
LOSS train 0.13857219050938296 valid 0.1947414872504897
LOSS train 0.13857219050938296 valid 0.1948159316115165
LOSS train 0.13857219050938296 valid 0.194681072701289
LOSS train 0.13857219050938296 valid 0.19465407157937686
LOSS train 0.13857219050938296 valid 0.1947039753527931
LOSS train 0.13857219050938296 valid 0.19462421037011093
LOSS train 0.13857219050938296 valid 0.1946048849755949
LOSS train 0.13857219050938296 valid 0.19462863720305587
LOSS train 0.13857219050938296 valid 0.19444914725986687
LOSS train 0.13857219050938296 valid 0.1944984598986564
LOSS train 0.13857219050938296 valid 0.19435866288641557
LOSS train 0.13857219050938296 valid 0.19424296812491215
LOSS train 0.13857219050938296 valid 0.19420078262765572
LOSS train 0.13857219050938296 valid 0.19429707872240168
LOSS train 0.13857219050938296 valid 0.19423531346920273
LOSS train 0.13857219050938296 valid 0.19426513075207671
LOSS train 0.13857219050938296 valid 0.19411687213213333
LOSS train 0.13857219050938296 valid 0.19392381001686312
LOSS train 0.13857219050938296 valid 0.1936408505989955
LOSS train 0.13857219050938296 valid 0.19365778877114764
LOSS train 0.13857219050938296 valid 0.19383701107223628
LOSS train 0.13857219050938296 valid 0.19374364403763203
LOSS train 0.13857219050938296 valid 0.1938091270887672
LOSS train 0.13857219050938296 valid 0.1937240543216467
LOSS train 0.13857219050938296 valid 0.19371873147748597
LOSS train 0.13857219050938296 valid 0.19369052468550088
LOSS train 0.13857219050938296 valid 0.19373561625410182
LOSS train 0.13857219050938296 valid 0.19396060719793917
LOSS train 0.13857219050938296 valid 0.1938011455099757
LOSS train 0.13857219050938296 valid 0.19382728426491172
LOSS train 0.13857219050938296 valid 0.1938677050785166
LOSS train 0.13857219050938296 valid 0.1938293596299795
LOSS train 0.13857219050938296 valid 0.19383338364687833
LOSS train 0.13857219050938296 valid 0.19392241161494028
LOSS train 0.13857219050938296 valid 0.1939240895317629
LOSS train 0.13857219050938296 valid 0.19392569004645888
LOSS train 0.13857219050938296 valid 0.19395200628350037
LOSS train 0.13857219050938296 valid 0.19387311530168924
LOSS train 0.13857219050938296 valid 0.19377913177013398
LOSS train 0.13857219050938296 valid 0.19361502304673195
LOSS train 0.13857219050938296 valid 0.19339983501741964
LOSS train 0.13857219050938296 valid 0.193388161992808
LOSS train 0.13857219050938296 valid 0.19351312797091322
LOSS train 0.13857219050938296 valid 0.19349791881713
LOSS train 0.13857219050938296 valid 0.19342842172173894
LOSS train 0.13857219050938296 valid 0.19347489672201174
LOSS train 0.13857219050938296 valid 0.19352885579581752
LOSS train 0.13857219050938296 valid 0.19359906649749195
LOSS train 0.13857219050938296 valid 0.19384537120660147
LOSS train 0.13857219050938296 valid 0.1940189330978731
LOSS train 0.13857219050938296 valid 0.19410381745136782
LOSS train 0.13857219050938296 valid 0.19412667219314658
LOSS train 0.13857219050938296 valid 0.19407297827010592
LOSS train 0.13857219050938296 valid 0.19416768064965373
LOSS train 0.13857219050938296 valid 0.19426040738433986
LOSS train 0.13857219050938296 valid 0.19422126118222188
LOSS train 0.13857219050938296 valid 0.19420292084564977
LOSS train 0.13857219050938296 valid 0.19416965894464755
LOSS train 0.13857219050938296 valid 0.19428680367926335
LOSS train 0.13857219050938296 valid 0.19416467599191908
LOSS train 0.13857219050938296 valid 0.19416445694895235
LOSS train 0.13857219050938296 valid 0.19408620549600666
LOSS train 0.13857219050938296 valid 0.19400947500472288
LOSS train 0.13857219050938296 valid 0.19405733570456504
LOSS train 0.13857219050938296 valid 0.19412882570409182
LOSS train 0.13857219050938296 valid 0.19399276566653212
LOSS train 0.13857219050938296 valid 0.19420539259665298
LOSS train 0.13857219050938296 valid 0.19427736482170763
LOSS train 0.13857219050938296 valid 0.1943519810024573
LOSS train 0.13857219050938296 valid 0.19421977234807442
LOSS train 0.13857219050938296 valid 0.19430008717635383
LOSS train 0.13857219050938296 valid 0.1942350131369406
LOSS train 0.13857219050938296 valid 0.19420068259220047
LOSS train 0.13857219050938296 valid 0.19419711548089982
LOSS train 0.13857219050938296 valid 0.19413303681816238
LOSS train 0.13857219050938296 valid 0.19423979307923997
LOSS train 0.13857219050938296 valid 0.19415682177298624
LOSS train 0.13857219050938296 valid 0.19411636096990015
LOSS train 0.13857219050938296 valid 0.19410304766075284
LOSS train 0.13857219050938296 valid 0.19403356703696772
LOSS train 0.13857219050938296 valid 0.1938807612959049
LOSS train 0.13857219050938296 valid 0.1940821245197178
LOSS train 0.13857219050938296 valid 0.1940922291237415
LOSS train 0.13857219050938296 valid 0.1940646302814667
LOSS train 0.13857219050938296 valid 0.19425392207971479
LOSS train 0.13857219050938296 valid 0.1942525303545799
LOSS train 0.13857219050938296 valid 0.19430277108466218
LOSS train 0.13857219050938296 valid 0.1943192347658403
LOSS train 0.13857219050938296 valid 0.19446496434931484
LOSS train 0.13857219050938296 valid 0.19447197104082967
LOSS train 0.13857219050938296 valid 0.19451723066385318
LOSS train 0.13857219050938296 valid 0.19456975195390075
LOSS train 0.13857219050938296 valid 0.19476303026135527
LOSS train 0.13857219050938296 valid 0.19485480984052023
LOSS train 0.13857219050938296 valid 0.19498714788153604
LOSS train 0.13857219050938296 valid 0.1954044668442186
LOSS train 0.13857219050938296 valid 0.19557109557010316
LOSS train 0.13857219050938296 valid 0.19559383669691363
LOSS train 0.13857219050938296 valid 0.19554689645767212
LOSS train 0.13857219050938296 valid 0.19550009255391965
LOSS train 0.13857219050938296 valid 0.19545278200603994
LOSS train 0.13857219050938296 valid 0.19525509493814097
LOSS train 0.13857219050938296 valid 0.19526997614505043
LOSS train 0.13857219050938296 valid 0.19517309186714035
LOSS train 0.13857219050938296 valid 0.19512868330572
LOSS train 0.13857219050938296 valid 0.19491474161334071
LOSS train 0.13857219050938296 valid 0.19495039230521913
LOSS train 0.13857219050938296 valid 0.1950061754126784
LOSS train 0.13857219050938296 valid 0.1950123545370604
LOSS train 0.13857219050938296 valid 0.19503134509900233
LOSS train 0.13857219050938296 valid 0.1950297811513163
LOSS train 0.13857219050938296 valid 0.19503895855612224
LOSS train 0.13857219050938296 valid 0.19501292695223665
LOSS train 0.13857219050938296 valid 0.19517432507769814
LOSS train 0.13857219050938296 valid 0.1949558405019983
LOSS train 0.13857219050938296 valid 0.19499118411785935
LOSS train 0.13857219050938296 valid 0.19502603600863302
LOSS train 0.13857219050938296 valid 0.19516712741381456
LOSS train 0.13857219050938296 valid 0.19530338210574652
LOSS train 0.13857219050938296 valid 0.19519770804893327
LOSS train 0.13857219050938296 valid 0.19520206744422014
LOSS train 0.13857219050938296 valid 0.19515896088524953
LOSS train 0.13857219050938296 valid 0.19517068156231207
LOSS train 0.13857219050938296 valid 0.19519052267074585
LOSS train 0.13857219050938296 valid 0.195232483239665
LOSS train 0.13857219050938296 valid 0.19516692522740522
LOSS train 0.13857219050938296 valid 0.19523616036762892
LOSS train 0.13857219050938296 valid 0.1953234875966844
LOSS train 0.13857219050938296 valid 0.19522328264400607
LOSS train 0.13857219050938296 valid 0.1952052705622966
LOSS train 0.13857219050938296 valid 0.1952469369778804
LOSS train 0.13857219050938296 valid 0.19520459387015987
LOSS train 0.13857219050938296 valid 0.19522716746361124
LOSS train 0.13857219050938296 valid 0.19517543767729112
LOSS train 0.13857219050938296 valid 0.1950628500659366
LOSS train 0.13857219050938296 valid 0.1951895039051007
LOSS train 0.13857219050938296 valid 0.19527688508216565
LOSS train 0.13857219050938296 valid 0.19535585454884608
LOSS train 0.13857219050938296 valid 0.19528874348080347
LOSS train 0.13857219050938296 valid 0.19525122095512437
LOSS train 0.13857219050938296 valid 0.1953605614626069
LOSS train 0.13857219050938296 valid 0.19550003214452252
LOSS train 0.13857219050938296 valid 0.1955527420327955
LOSS train 0.13857219050938296 valid 0.1954616228118539
LOSS train 0.13857219050938296 valid 0.19557502160191165
LOSS train 0.13857219050938296 valid 0.1955734501066415
LOSS train 0.13857219050938296 valid 0.19554744351759046
LOSS train 0.13857219050938296 valid 0.195637666930755
LOSS train 0.13857219050938296 valid 0.19561863555358006
LOSS train 0.13857219050938296 valid 0.19575276765172467
LOSS train 0.13857219050938296 valid 0.1958534611383345
LOSS train 0.13857219050938296 valid 0.19573031384043577
LOSS train 0.13857219050938296 valid 0.19591558885429405
LOSS train 0.13857219050938296 valid 0.1958170114141522
LOSS train 0.13857219050938296 valid 0.19569817149387025
LOSS train 0.13857219050938296 valid 0.1956399865448475
LOSS train 0.13857219050938296 valid 0.19565623430339424
LOSS train 0.13857219050938296 valid 0.19575501246723587
LOSS train 0.13857219050938296 valid 0.19574536516595242
LOSS train 0.13857219050938296 valid 0.19584602911380075
LOSS train 0.13857219050938296 valid 0.19593187786351327
LOSS train 0.13857219050938296 valid 0.19591349350101145
LOSS train 0.13857219050938296 valid 0.19592851554222163
LOSS train 0.13857219050938296 valid 0.19590062065159572
LOSS train 0.13857219050938296 valid 0.19580832753013655
LOSS train 0.13857219050938296 valid 0.19574923229496383
LOSS train 0.13857219050938296 valid 0.19575820932756707
LOSS train 0.13857219050938296 valid 0.1960424695510504
LOSS train 0.13857219050938296 valid 0.19608243362627167
LOSS train 0.13857219050938296 valid 0.1961002875270182
LOSS train 0.13857219050938296 valid 0.19600453912696508
LOSS train 0.13857219050938296 valid 0.19593834748555874
LOSS train 0.13857219050938296 valid 0.19595669833329482
LOSS train 0.13857219050938296 valid 0.19587694717305046
LOSS train 0.13857219050938296 valid 0.1958116184697192
LOSS train 0.13857219050938296 valid 0.19582260836085136
LOSS train 0.13857219050938296 valid 0.1957839110248825
LOSS train 0.13857219050938296 valid 0.1957533068239352
LOSS train 0.13857219050938296 valid 0.1958773481174254
LOSS train 0.13857219050938296 valid 0.19588026697381158
LOSS train 0.13857219050938296 valid 0.19592671234066747
LOSS train 0.13857219050938296 valid 0.1958538397064422
LOSS train 0.13857219050938296 valid 0.19585003660249842
LOSS train 0.13857219050938296 valid 0.19578702491190697
LOSS train 0.13857219050938296 valid 0.1958029868695214
LOSS train 0.13857219050938296 valid 0.1958973682717065
LOSS train 0.13857219050938296 valid 0.19573426378003164
LOSS train 0.13857219050938296 valid 0.19576247089675494
LOSS train 0.13857219050938296 valid 0.1957723469358601
LOSS train 0.13857219050938296 valid 0.19575197441004666
LOSS train 0.13857219050938296 valid 0.1956237064476559
LOSS train 0.13857219050938296 valid 0.1956125987774652
LOSS train 0.13857219050938296 valid 0.19565353645541803
EPOCH 9:
  batch 1 loss: 0.12999925017356873
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.12948030978441238
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.12958879272143045
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.13690108805894852
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.14203904271125795
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.14113475382328033
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.13257115547146117
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.13258057553321123
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.1313304387860828
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.12991053760051727
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.12957427447492426
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.12863134096066156
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.12912753453621498
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.1281549222767353
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.12843757917483647
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.1296502952463925
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.12836573316770442
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.13028070661756727
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.12979252714859812
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.12924128100275994
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.12962186904180617
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.1299444098364223
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.13026007491609323
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.13238279148936272
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.13207813292741777
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.1322296356352476
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.13288208980251243
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.1326869728841952
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.13241094787572993
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.13285241648554802
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.13335417475431197
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.1329778099898249
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.13273539529605347
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.13374978150514996
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.13497222640684672
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.13526256361769307
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.13540827080204682
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.13495808075133123
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.13564976457601938
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.13545052353292703
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.1354735073883359
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.13569278890887895
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.13647652036228844
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.1358547198840163
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.13676335828171837
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.13703286178086116
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.13679625585358193
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.13625373334313431
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.13623402176462873
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.13630528673529624
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.13623210130368962
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.13670801528944418
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.13683410818284414
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.1371194641623232
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.13754390356215565
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.13769371341913939
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.1379121815164884
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.1378499666421578
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.13850105149766145
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.1385759332527717
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.13845559331725854
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.13834697116286523
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.13876262271688097
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.13869408832397312
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.13872792594707928
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.13872872597791933
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.13855670603798398
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.13878714019323096
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.13913192751183026
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.13931451748524393
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.1395592373651518
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.1394833406019542
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.13965148989060153
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.1394697166979313
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.13923915525277455
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.13928245086418956
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.1390699933875691
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.13852489185638917
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.13868907358072982
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.1386650327593088
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.13888590313770152
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.1385371323402335
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.13844476945428963
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.13897850701496714
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.1390571909792283
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.1392484215456386
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.1393640760717721
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.13935114070773125
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.1397017990605215
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.13979130056169298
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.13977979729463766
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.13978614398966666
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.13996340911234578
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.1400859127653406
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.1403596165933107
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.14038979727774858
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.140143493708876
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.1401588021188366
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.14035192282512934
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.14013080947101117
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.14019306557308328
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.14000453734222582
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.13986066743297484
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.14007109955239755
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.13963134920313244
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.13959119082340654
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.1397261223364099
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.13956812638099547
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.1398364911795756
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.14025967358188196
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.14018513819387368
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.13987484800496272
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.13956259824005904
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.1398026903993205
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.13956864404937494
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.1394858367355733
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.13968192722298142
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.1395966042527708
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.13941219962444626
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.13956709740062553
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.13938412378149584
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.1394084453827045
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.13957583128921386
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.13948182160815886
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.139404269695282
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.1393740229190342
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.13947140748106587
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.13937563833314925
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.13940025011236354
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.1395264100569945
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.13926181913787172
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.13908781726477723
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.139327234959692
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.1392095673328905
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.1389526579115126
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.1388944742434165
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.13913120274996235
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.13904856825652329
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.13916370326237712
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.13936712518334388
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.1392595538644926
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.1392076846579431
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.1391638654928941
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.1393209606822994
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.13911261502011069
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.13909832178933978
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.13901018128305875
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.1389765238540398
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.13920017511852636
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.13921505098541578
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.13899457301722457
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.13895981097103735
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.1388575158575002
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.1386179192983485
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.13868898456135104
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.13859371549617022
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.13862397673593205
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.13850700322397147
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.13868155208586147
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.1387740548234433
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.13866702073849507
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.13866769605212742
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.13864157335158506
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.13868952170014381
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.13880178892251216
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.13875335769121905
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.1384631555058999
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.13841631298973447
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.1383109611167005
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.13843410348190982
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.13830166292643686
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.13827666618623013
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.13824161658914103
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.13812984247831092
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.13821991703339986
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.1380928143520247
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.13794933650958335
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.13773475545510816
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.13779730362266135
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.13772495380706257
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.13771516940870338
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.1376169360764734
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.13737223993559353
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.13735122314613799
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.13735661442215377
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.13733489907556964
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.13747652392973875
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.13732241799539707
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.1373955046058332
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.13753904198345385
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.13757455505001606
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.13746846141293645
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.13751801655391338
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.13733572482141024
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.13730927262550746
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.13740191661885806
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.13756541030358543
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.13773013320234087
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.1377156012920878
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.13790085904300212
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.1379069815672452
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.13794337567126397
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.13786911362497678
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.13794073433268303
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.13775156881751083
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.137627620430826
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.1376314500103826
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.13750333102563253
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.13747452449000053
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.13770754436651866
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.13771693556794623
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.1377880039080134
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.13778572630994196
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.13764261478714854
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.13770191589760225
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.1376058684868945
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.13766228857402976
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.13781244061682202
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.13790400217385052
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.13794408657334067
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.13799149735211247
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.13797629396389197
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.13788768776061824
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.1379049870052508
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.13777842866049872
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.137691245043436
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.1377858434545311
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.13760856310265107
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.137607433780312
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.13777860869532046
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.13792480825087725
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.13787655635126705
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.13784768595460148
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.13779667427397183
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.1377621996910014
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.13782913856587167
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.13781624422546176
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.13779038460064336
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.13787010268436814
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.13783538844436408
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.13791481977429132
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.1378345629277308
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.1378898914336177
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.13784154522858683
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.13788044866250485
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.13790216872362587
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.1378677240629428
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.13784019198388822
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.13777149651184617
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.13773390060663224
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.1377887299573754
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.13761068883514593
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.13760376633273755
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.13755269633151415
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.13764901082305347
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.13763001104234718
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.1374993864374402
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.1374549201812393
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.13739216894371628
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.1373754671273323
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.13740926087473543
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.13741629539441516
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.13734426791337054
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.1371907347135923
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.13714643545308203
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.1371467807855373
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.13713603980197442
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.13712333078815866
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.13708143116262322
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.13706052019088358
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.13702922193436606
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.137048808611272
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.13717080715484234
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.1371360731440304
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.1370652402801947
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.13714390488314457
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.13708492389977625
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.13711540484063917
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.13710246534223625
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.1371463249570557
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.13716022365352012
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.13716061976044736
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.13717063637483246
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.13714806475794653
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.13725265423979677
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.13725421903329296
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.13722807249437227
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.13734154783499739
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.13725936387030724
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.13727194075954371
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.13723116985096553
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.13720001882477983
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.13730799536452765
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.13736325771022004
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.13729034371800342
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.13726554531604052
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.1373368942928234
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.13730442826479874
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.13724813824612958
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.13727374198536077
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.1373925968707598
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.13734469572637256
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.13734005647327246
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.13731543308025912
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.1373145746403053
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.1373091203714508
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.137308756478058
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.137288348628329
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.1372536032138133
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.13721482527832832
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.1372703627183123
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.13732131484609383
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.13731257241374006
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.13724173007497362
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.13724730700727494
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.13732761205940308
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.13736349381858995
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.13732914265784077
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.13729022929100407
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.1373747842852026
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.1373859403010841
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.13740617763366758
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.13739023539857598
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.137327735254794
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.13737884209706233
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.13730253388910937
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.1371850025261943
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.1371594306036103
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.1371536991108877
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.13722244629805738
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.13727494513790414
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.13727442630712527
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.13718460039333538
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.1372115447403428
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.13729763471368533
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.1372957115194627
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.13726043740967261
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.13724968674972918
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.1371538855333244
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.13717003622475793
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.13710479770412892
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.13725493233977704
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.13723364433289964
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.1372115092655254
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.1372416812872541
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.1372568866162631
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.13730011120309749
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.13727917278121257
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.13725407212374888
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.13716188669204712
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.13712642390673657
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.13717042794451118
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.13720992101994858
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.13730573473172
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.13731899870113587
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.13730388230989488
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.13732945964950807
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.1374208718621531
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.13739598390949803
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.1373551510895292
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.13733518955367424
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.13737157372068304
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.13731189821295173
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.13722840899212674
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.13728438221836745
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.13728086546551985
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.13733374790291045
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.13738099613186458
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.13735222451005202
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.13731367175240775
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.1373581371980536
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.13735309052932007
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.13733906910103064
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.13740372691642155
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.13744457640250524
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.13747279577829102
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.13744196187042748
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.13749684103661114
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.13748496371790728
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.1374294807448199
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.1373975504766612
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.1373560078910196
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.13742973005724948
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.1373425080673769
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.13729409813493879
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.13720255138574486
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.13722070880253492
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.13724846068347238
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.13723479688244183
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.13724100358593158
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.13720997529642662
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.13720435780301996
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.13724275371273056
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.13722505017661202
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.13726972211388094
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.1373181716172081
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.13723181445217253
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.13725586724116576
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.13720645234548956
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.13721492514014244
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.13722760150408805
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.1372135764923855
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.13716706533733727
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.13718823612769052
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.13715402533610663
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.13709029590187988
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.13705712218176236
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.13713078407168972
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.1371067449960557
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.13714221336129234
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.13715414849728563
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.13712730940467524
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.13710710563252682
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.13714712849201788
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.1372135344219495
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.13729028856883255
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.13726976701467158
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.13725325748609585
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.13739943794177084
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.13737415123198712
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.13744300141965976
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.13745381564819983
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.13740447299207073
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.1373938202401096
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.13739753521540585
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.13736979157921853
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.13735215999273281
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.13730668407107624
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.13726928032986768
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.13725533781703128
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.1372718140137168
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.1372680393461552
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.13721644224525875
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.1372001666558503
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.13714800968594934
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.13721805077832228
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.1372860838367137
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.13724740176168207
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.13713010571947945
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.1370673997158354
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.13705911015977665
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.13700402974745268
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.13698375526117687
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.13698397018015385
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.13700906592473555
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.13703790368854732
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.1370123635302454
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.13709558441769332
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.13712959653617013
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.1370774642460876
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.13710348600492772
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.13718068096951574
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.13713718432661715
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.13708384573000118
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.13707052447966167
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.13710205974220707
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.13711687138291886
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.13711230510512293
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.13709737530915567
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.13718974527133548
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.13716939907852327
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.13717626881870357
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.1370564894851289
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.13707503576859317
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.13710094840936762
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.13708793547787892
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.1371096741783032
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.13716780349739596
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.13722203401868532
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.13725259329410308
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.13727133055148358
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.13735617681453793
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.13735617681453793 valid 0.23438118398189545
LOSS train 0.13735617681453793 valid 0.19590575248003006
LOSS train 0.13735617681453793 valid 0.19937040905157724
LOSS train 0.13735617681453793 valid 0.1861882098019123
LOSS train 0.13735617681453793 valid 0.1820719301700592
LOSS train 0.13735617681453793 valid 0.18816901743412018
LOSS train 0.13735617681453793 valid 0.19622862764767238
LOSS train 0.13735617681453793 valid 0.19540448673069477
LOSS train 0.13735617681453793 valid 0.19446460902690887
LOSS train 0.13735617681453793 valid 0.19702951312065126
LOSS train 0.13735617681453793 valid 0.19583895802497864
LOSS train 0.13735617681453793 valid 0.1935606449842453
LOSS train 0.13735617681453793 valid 0.19223681550759536
LOSS train 0.13735617681453793 valid 0.19198644906282425
LOSS train 0.13735617681453793 valid 0.18845387597878774
LOSS train 0.13735617681453793 valid 0.18974670860916376
LOSS train 0.13735617681453793 valid 0.19185681904063506
LOSS train 0.13735617681453793 valid 0.1905873550309075
LOSS train 0.13735617681453793 valid 0.1926141593017076
LOSS train 0.13735617681453793 valid 0.1938161313533783
LOSS train 0.13735617681453793 valid 0.19361641293480283
LOSS train 0.13735617681453793 valid 0.19214685599912296
LOSS train 0.13735617681453793 valid 0.19213725043379742
LOSS train 0.13735617681453793 valid 0.19181779151161513
LOSS train 0.13735617681453793 valid 0.19006875693798064
LOSS train 0.13735617681453793 valid 0.18987426333702528
LOSS train 0.13735617681453793 valid 0.189833402633667
LOSS train 0.13735617681453793 valid 0.19028998059885843
LOSS train 0.13735617681453793 valid 0.19051707869973675
LOSS train 0.13735617681453793 valid 0.19106718351443608
LOSS train 0.13735617681453793 valid 0.1919818083124776
LOSS train 0.13735617681453793 valid 0.1912743654102087
LOSS train 0.13735617681453793 valid 0.19146509062160144
LOSS train 0.13735617681453793 valid 0.1908848960609997
LOSS train 0.13735617681453793 valid 0.19293731025287084
LOSS train 0.13735617681453793 valid 0.19283525231811735
LOSS train 0.13735617681453793 valid 0.19399898197199847
LOSS train 0.13735617681453793 valid 0.1942620163685397
LOSS train 0.13735617681453793 valid 0.1934701185195874
LOSS train 0.13735617681453793 valid 0.19342974349856376
LOSS train 0.13735617681453793 valid 0.19312587344064946
LOSS train 0.13735617681453793 valid 0.19375136672031312
LOSS train 0.13735617681453793 valid 0.1933230321074641
LOSS train 0.13735617681453793 valid 0.19387205994941972
LOSS train 0.13735617681453793 valid 0.19389969342284732
LOSS train 0.13735617681453793 valid 0.19473857043877893
LOSS train 0.13735617681453793 valid 0.19509750382697327
LOSS train 0.13735617681453793 valid 0.19528544011215368
LOSS train 0.13735617681453793 valid 0.19608471588212617
LOSS train 0.13735617681453793 valid 0.19547394633293153
LOSS train 0.13735617681453793 valid 0.19593546963205524
LOSS train 0.13735617681453793 valid 0.19588938355445862
LOSS train 0.13735617681453793 valid 0.19616532241398432
LOSS train 0.13735617681453793 valid 0.1964271612189434
LOSS train 0.13735617681453793 valid 0.1964742879975926
LOSS train 0.13735617681453793 valid 0.19652639675353253
LOSS train 0.13735617681453793 valid 0.19656751213366525
LOSS train 0.13735617681453793 valid 0.1964702577940349
LOSS train 0.13735617681453793 valid 0.1970482680757167
LOSS train 0.13735617681453793 valid 0.19650295997659364
LOSS train 0.13735617681453793 valid 0.19618355177465033
LOSS train 0.13735617681453793 valid 0.19634558188338433
LOSS train 0.13735617681453793 valid 0.19604122709660304
LOSS train 0.13735617681453793 valid 0.1968223499134183
LOSS train 0.13735617681453793 valid 0.19679078505589412
LOSS train 0.13735617681453793 valid 0.1965847001834349
LOSS train 0.13735617681453793 valid 0.19627023099073723
LOSS train 0.13735617681453793 valid 0.1965947151184082
LOSS train 0.13735617681453793 valid 0.19580475726853247
LOSS train 0.13735617681453793 valid 0.196168686236654
LOSS train 0.13735617681453793 valid 0.19594983357778736
LOSS train 0.13735617681453793 valid 0.19605433237221506
LOSS train 0.13735617681453793 valid 0.19621030333107464
LOSS train 0.13735617681453793 valid 0.1960448485773963
LOSS train 0.13735617681453793 valid 0.1961334220568339
LOSS train 0.13735617681453793 valid 0.19702554376501785
LOSS train 0.13735617681453793 valid 0.19675817776035953
LOSS train 0.13735617681453793 valid 0.1966674446295469
LOSS train 0.13735617681453793 valid 0.19619731473017343
LOSS train 0.13735617681453793 valid 0.19561229329556226
LOSS train 0.13735617681453793 valid 0.19499668681327206
LOSS train 0.13735617681453793 valid 0.19538581861955365
LOSS train 0.13735617681453793 valid 0.19497246178517857
LOSS train 0.13735617681453793 valid 0.1949131703447728
LOSS train 0.13735617681453793 valid 0.1941101296859629
LOSS train 0.13735617681453793 valid 0.19360776228267093
LOSS train 0.13735617681453793 valid 0.19346343083628292
LOSS train 0.13735617681453793 valid 0.1931195238774473
LOSS train 0.13735617681453793 valid 0.19336640064635974
LOSS train 0.13735617681453793 valid 0.19349224517742794
LOSS train 0.13735617681453793 valid 0.1935494249011134
LOSS train 0.13735617681453793 valid 0.1934314323184283
LOSS train 0.13735617681453793 valid 0.1933434187404571
LOSS train 0.13735617681453793 valid 0.19365691060715534
LOSS train 0.13735617681453793 valid 0.1931476232252623
LOSS train 0.13735617681453793 valid 0.19327087079485258
LOSS train 0.13735617681453793 valid 0.1932103830514495
LOSS train 0.13735617681453793 valid 0.19348527430271617
LOSS train 0.13735617681453793 valid 0.19359843839298596
LOSS train 0.13735617681453793 valid 0.1936368651688099
LOSS train 0.13735617681453793 valid 0.19382545396242992
LOSS train 0.13735617681453793 valid 0.19430813938379288
LOSS train 0.13735617681453793 valid 0.19409200476789937
LOSS train 0.13735617681453793 valid 0.19411022359362015
LOSS train 0.13735617681453793 valid 0.1943419785726638
LOSS train 0.13735617681453793 valid 0.1947494074702263
LOSS train 0.13735617681453793 valid 0.19443511935037988
LOSS train 0.13735617681453793 valid 0.19450380887698243
LOSS train 0.13735617681453793 valid 0.19504040301939762
LOSS train 0.13735617681453793 valid 0.19505820775573904
LOSS train 0.13735617681453793 valid 0.19495564658899564
LOSS train 0.13735617681453793 valid 0.1947210349940828
LOSS train 0.13735617681453793 valid 0.1948241422661638
LOSS train 0.13735617681453793 valid 0.19500898975029327
LOSS train 0.13735617681453793 valid 0.195005079196847
LOSS train 0.13735617681453793 valid 0.19539662373477015
LOSS train 0.13735617681453793 valid 0.19549594246424162
LOSS train 0.13735617681453793 valid 0.1951960028480675
LOSS train 0.13735617681453793 valid 0.19502472727238632
LOSS train 0.13735617681453793 valid 0.1947095412760973
LOSS train 0.13735617681453793 valid 0.19452900790478572
LOSS train 0.13735617681453793 valid 0.19456113960410729
LOSS train 0.13735617681453793 valid 0.19470242756169018
LOSS train 0.13735617681453793 valid 0.1950756787292419
LOSS train 0.13735617681453793 valid 0.1948591830730438
LOSS train 0.13735617681453793 valid 0.19504519930434605
LOSS train 0.13735617681453793 valid 0.19487730313943127
LOSS train 0.13735617681453793 valid 0.19489163276739419
LOSS train 0.13735617681453793 valid 0.19527614648027938
LOSS train 0.13735617681453793 valid 0.19504938343396552
LOSS train 0.13735617681453793 valid 0.19483601637468992
LOSS train 0.13735617681453793 valid 0.19440053245334915
LOSS train 0.13735617681453793 valid 0.19404977414392888
LOSS train 0.13735617681453793 valid 0.19420648077085836
LOSS train 0.13735617681453793 valid 0.19405225305645554
LOSS train 0.13735617681453793 valid 0.19386475850992343
LOSS train 0.13735617681453793 valid 0.19358036563779316
LOSS train 0.13735617681453793 valid 0.19344400862852731
LOSS train 0.13735617681453793 valid 0.19318991009708789
LOSS train 0.13735617681453793 valid 0.19327453385506357
LOSS train 0.13735617681453793 valid 0.19329369205532346
LOSS train 0.13735617681453793 valid 0.19325544559200045
LOSS train 0.13735617681453793 valid 0.19332866068486568
LOSS train 0.13735617681453793 valid 0.19344640109274122
LOSS train 0.13735617681453793 valid 0.19337271996613206
LOSS train 0.13735617681453793 valid 0.19340583927010838
LOSS train 0.13735617681453793 valid 0.19325716246147545
LOSS train 0.13735617681453793 valid 0.19425493206929517
LOSS train 0.13735617681453793 valid 0.1943350342296114
LOSS train 0.13735617681453793 valid 0.1941282140215238
LOSS train 0.13735617681453793 valid 0.19443682447963992
LOSS train 0.13735617681453793 valid 0.19425836666242072
LOSS train 0.13735617681453793 valid 0.1942675538312376
LOSS train 0.13735617681453793 valid 0.1942600587552244
LOSS train 0.13735617681453793 valid 0.1941315083734451
LOSS train 0.13735617681453793 valid 0.1941738108602854
LOSS train 0.13735617681453793 valid 0.1942522897841824
LOSS train 0.13735617681453793 valid 0.1942491625683217
LOSS train 0.13735617681453793 valid 0.19428198573724278
LOSS train 0.13735617681453793 valid 0.19414751222357154
LOSS train 0.13735617681453793 valid 0.19407115043690487
LOSS train 0.13735617681453793 valid 0.19379135350018373
LOSS train 0.13735617681453793 valid 0.19353825868638747
LOSS train 0.13735617681453793 valid 0.19324187434664586
LOSS train 0.13735617681453793 valid 0.19320925690911034
LOSS train 0.13735617681453793 valid 0.19313403919159647
LOSS train 0.13735617681453793 valid 0.19336542442530216
LOSS train 0.13735617681453793 valid 0.19335057692868368
LOSS train 0.13735617681453793 valid 0.19351289125941915
LOSS train 0.13735617681453793 valid 0.1935820534825325
LOSS train 0.13735617681453793 valid 0.1935144682899553
LOSS train 0.13735617681453793 valid 0.19340948301345803
LOSS train 0.13735617681453793 valid 0.19346647266018596
LOSS train 0.13735617681453793 valid 0.19340329660081315
LOSS train 0.13735617681453793 valid 0.19327801917280468
LOSS train 0.13735617681453793 valid 0.19337149667130274
LOSS train 0.13735617681453793 valid 0.19332777031060666
LOSS train 0.13735617681453793 valid 0.1934047065089258
LOSS train 0.13735617681453793 valid 0.19327145365363393
LOSS train 0.13735617681453793 valid 0.19324365084369977
LOSS train 0.13735617681453793 valid 0.19329169516076042
LOSS train 0.13735617681453793 valid 0.19320538468085802
LOSS train 0.13735617681453793 valid 0.19318165634173512
LOSS train 0.13735617681453793 valid 0.1931992425866749
LOSS train 0.13735617681453793 valid 0.1930252702655019
LOSS train 0.13735617681453793 valid 0.19308137396971384
LOSS train 0.13735617681453793 valid 0.1929508888307102
LOSS train 0.13735617681453793 valid 0.19285222055747153
LOSS train 0.13735617681453793 valid 0.19280204957439787
LOSS train 0.13735617681453793 valid 0.1928926837287451
LOSS train 0.13735617681453793 valid 0.19283661987456976
LOSS train 0.13735617681453793 valid 0.19286538823507726
LOSS train 0.13735617681453793 valid 0.19270608446758647
LOSS train 0.13735617681453793 valid 0.19251227133052864
LOSS train 0.13735617681453793 valid 0.19222785433133444
LOSS train 0.13735617681453793 valid 0.19224367921753804
LOSS train 0.13735617681453793 valid 0.19242702258117309
LOSS train 0.13735617681453793 valid 0.1923393364054988
LOSS train 0.13735617681453793 valid 0.19240862872432823
LOSS train 0.13735617681453793 valid 0.1923164249956608
LOSS train 0.13735617681453793 valid 0.19230386777896785
LOSS train 0.13735617681453793 valid 0.19228540220768145
LOSS train 0.13735617681453793 valid 0.1923427425435024
LOSS train 0.13735617681453793 valid 0.19256957759167634
LOSS train 0.13735617681453793 valid 0.19240484768297614
LOSS train 0.13735617681453793 valid 0.19243115179457712
LOSS train 0.13735617681453793 valid 0.19247783432548174
LOSS train 0.13735617681453793 valid 0.19244536836273396
LOSS train 0.13735617681453793 valid 0.19245744685522106
LOSS train 0.13735617681453793 valid 0.19253798453580764
LOSS train 0.13735617681453793 valid 0.19253496197162645
LOSS train 0.13735617681453793 valid 0.192537073327123
LOSS train 0.13735617681453793 valid 0.19256201904144646
LOSS train 0.13735617681453793 valid 0.19247705301391743
LOSS train 0.13735617681453793 valid 0.19238260131935742
LOSS train 0.13735617681453793 valid 0.19220874672410665
LOSS train 0.13735617681453793 valid 0.1919918060989424
LOSS train 0.13735617681453793 valid 0.19197758403393106
LOSS train 0.13735617681453793 valid 0.19210031553762688
LOSS train 0.13735617681453793 valid 0.19207162281328982
LOSS train 0.13735617681453793 valid 0.1919996356128028
LOSS train 0.13735617681453793 valid 0.1920423266049978
LOSS train 0.13735617681453793 valid 0.1920968517326988
LOSS train 0.13735617681453793 valid 0.19215964865205543
LOSS train 0.13735617681453793 valid 0.19240693562560612
LOSS train 0.13735617681453793 valid 0.1925786874067467
LOSS train 0.13735617681453793 valid 0.1926617369646543
LOSS train 0.13735617681453793 valid 0.1926877754822112
LOSS train 0.13735617681453793 valid 0.19263747243381485
LOSS train 0.13735617681453793 valid 0.19272866287957066
LOSS train 0.13735617681453793 valid 0.19282691948341601
LOSS train 0.13735617681453793 valid 0.19278121765317588
LOSS train 0.13735617681453793 valid 0.19277910517264846
LOSS train 0.13735617681453793 valid 0.1927411634570513
LOSS train 0.13735617681453793 valid 0.19286004640954607
LOSS train 0.13735617681453793 valid 0.19274286944734847
LOSS train 0.13735617681453793 valid 0.19273096134391013
LOSS train 0.13735617681453793 valid 0.19264982809790043
LOSS train 0.13735617681453793 valid 0.19255890232748565
LOSS train 0.13735617681453793 valid 0.192604149132967
LOSS train 0.13735617681453793 valid 0.1926866737020461
LOSS train 0.13735617681453793 valid 0.19255375960641655
LOSS train 0.13735617681453793 valid 0.19276476176187335
LOSS train 0.13735617681453793 valid 0.19283700734376907
LOSS train 0.13735617681453793 valid 0.1929072852037391
LOSS train 0.13735617681453793 valid 0.19276763747135797
LOSS train 0.13735617681453793 valid 0.1928485701923911
LOSS train 0.13735617681453793 valid 0.1927788625921934
LOSS train 0.13735617681453793 valid 0.19275839770414743
LOSS train 0.13735617681453793 valid 0.19274591338634492
LOSS train 0.13735617681453793 valid 0.1926802897833258
LOSS train 0.13735617681453793 valid 0.19279261211317683
LOSS train 0.13735617681453793 valid 0.19270918285658237
LOSS train 0.13735617681453793 valid 0.1926731155730608
LOSS train 0.13735617681453793 valid 0.1926588809373332
LOSS train 0.13735617681453793 valid 0.19259017787408084
LOSS train 0.13735617681453793 valid 0.19243171758920766
LOSS train 0.13735617681453793 valid 0.1926342574085376
LOSS train 0.13735617681453793 valid 0.1926532211105796
LOSS train 0.13735617681453793 valid 0.19262070472423848
LOSS train 0.13735617681453793 valid 0.19281026190039754
LOSS train 0.13735617681453793 valid 0.1928153488472218
LOSS train 0.13735617681453793 valid 0.19286928453372912
LOSS train 0.13735617681453793 valid 0.19288869591599161
LOSS train 0.13735617681453793 valid 0.19303775975164378
LOSS train 0.13735617681453793 valid 0.1930462726412859
LOSS train 0.13735617681453793 valid 0.19309094185463052
LOSS train 0.13735617681453793 valid 0.19313761975560614
LOSS train 0.13735617681453793 valid 0.1933312955066617
LOSS train 0.13735617681453793 valid 0.19342487388186985
LOSS train 0.13735617681453793 valid 0.19356129332222183
LOSS train 0.13735617681453793 valid 0.19397842763539622
LOSS train 0.13735617681453793 valid 0.19414802681613755
LOSS train 0.13735617681453793 valid 0.19417806660389378
LOSS train 0.13735617681453793 valid 0.1941357380693609
LOSS train 0.13735617681453793 valid 0.19409277684230736
LOSS train 0.13735617681453793 valid 0.19404195991448978
LOSS train 0.13735617681453793 valid 0.1938455521845989
LOSS train 0.13735617681453793 valid 0.1938570535952045
LOSS train 0.13735617681453793 valid 0.19375993769083705
LOSS train 0.13735617681453793 valid 0.19370844403407753
LOSS train 0.13735617681453793 valid 0.19348945569062065
LOSS train 0.13735617681453793 valid 0.19353156313036862
LOSS train 0.13735617681453793 valid 0.19359130611721898
LOSS train 0.13735617681453793 valid 0.193596257034101
LOSS train 0.13735617681453793 valid 0.1936151590589043
LOSS train 0.13735617681453793 valid 0.19361774426096406
LOSS train 0.13735617681453793 valid 0.19362772395834327
LOSS train 0.13735617681453793 valid 0.19360285038354075
LOSS train 0.13735617681453793 valid 0.19375562786028303
LOSS train 0.13735617681453793 valid 0.19353579583856248
LOSS train 0.13735617681453793 valid 0.19356788965324834
LOSS train 0.13735617681453793 valid 0.19360350580955935
LOSS train 0.13735617681453793 valid 0.19373995927320856
LOSS train 0.13735617681453793 valid 0.193883073633
LOSS train 0.13735617681453793 valid 0.19377553301888542
LOSS train 0.13735617681453793 valid 0.19377894192833692
LOSS train 0.13735617681453793 valid 0.19373934470967158
LOSS train 0.13735617681453793 valid 0.1937512253040454
LOSS train 0.13735617681453793 valid 0.19377794191241265
LOSS train 0.13735617681453793 valid 0.19381537776056715
LOSS train 0.13735617681453793 valid 0.1937496027113586
LOSS train 0.13735617681453793 valid 0.19381707360838898
LOSS train 0.13735617681453793 valid 0.19390798673817985
LOSS train 0.13735617681453793 valid 0.1938045298466917
LOSS train 0.13735617681453793 valid 0.1937948875372706
LOSS train 0.13735617681453793 valid 0.1938330259307588
LOSS train 0.13735617681453793 valid 0.19378777840114259
LOSS train 0.13735617681453793 valid 0.19381047597209228
LOSS train 0.13735617681453793 valid 0.19376279591552673
LOSS train 0.13735617681453793 valid 0.19365366842969053
LOSS train 0.13735617681453793 valid 0.19378267801724947
LOSS train 0.13735617681453793 valid 0.19387642058511131
LOSS train 0.13735617681453793 valid 0.19395140193070576
LOSS train 0.13735617681453793 valid 0.19388040890769354
LOSS train 0.13735617681453793 valid 0.19384936762006977
LOSS train 0.13735617681453793 valid 0.19396541838766274
LOSS train 0.13735617681453793 valid 0.19409758911567665
LOSS train 0.13735617681453793 valid 0.19415001435713333
LOSS train 0.13735617681453793 valid 0.19406161410734057
LOSS train 0.13735617681453793 valid 0.19417043652304236
LOSS train 0.13735617681453793 valid 0.19416777007513164
LOSS train 0.13735617681453793 valid 0.1941396255888068
LOSS train 0.13735617681453793 valid 0.19423070715533364
LOSS train 0.13735617681453793 valid 0.1942078448717411
LOSS train 0.13735617681453793 valid 0.1943453377101319
LOSS train 0.13735617681453793 valid 0.19444997057273117
LOSS train 0.13735617681453793 valid 0.1943294017053232
LOSS train 0.13735617681453793 valid 0.1945088338163486
LOSS train 0.13735617681453793 valid 0.19441491383494752
LOSS train 0.13735617681453793 valid 0.1942987959280475
LOSS train 0.13735617681453793 valid 0.19423963820718856
LOSS train 0.13735617681453793 valid 0.19425266032462363
LOSS train 0.13735617681453793 valid 0.1943550443577909
LOSS train 0.13735617681453793 valid 0.1943469809507256
LOSS train 0.13735617681453793 valid 0.1944519735074469
LOSS train 0.13735617681453793 valid 0.19453979422855094
LOSS train 0.13735617681453793 valid 0.19451745678863583
LOSS train 0.13735617681453793 valid 0.19453399160266976
LOSS train 0.13735617681453793 valid 0.19450682673384162
LOSS train 0.13735617681453793 valid 0.19441017054043208
LOSS train 0.13735617681453793 valid 0.19435291907243563
LOSS train 0.13735617681453793 valid 0.19436269494604438
LOSS train 0.13735617681453793 valid 0.1946513672727485
LOSS train 0.13735617681453793 valid 0.19469819263271662
LOSS train 0.13735617681453793 valid 0.1947190995850315
LOSS train 0.13735617681453793 valid 0.1946238977593029
LOSS train 0.13735617681453793 valid 0.19455594599418258
LOSS train 0.13735617681453793 valid 0.19457207474974986
LOSS train 0.13735617681453793 valid 0.19449069338185446
LOSS train 0.13735617681453793 valid 0.19442214704307056
LOSS train 0.13735617681453793 valid 0.1944344173270193
LOSS train 0.13735617681453793 valid 0.1943940231550179
LOSS train 0.13735617681453793 valid 0.19436763458501147
LOSS train 0.13735617681453793 valid 0.1944843441667691
LOSS train 0.13735617681453793 valid 0.194487592477477
LOSS train 0.13735617681453793 valid 0.19452921541131177
LOSS train 0.13735617681453793 valid 0.1944477063234292
LOSS train 0.13735617681453793 valid 0.19444220154066272
LOSS train 0.13735617681453793 valid 0.19437850014203126
LOSS train 0.13735617681453793 valid 0.19439430803143087
LOSS train 0.13735617681453793 valid 0.19448793807082412
LOSS train 0.13735617681453793 valid 0.19433248708070802
LOSS train 0.13735617681453793 valid 0.1943579705452526
LOSS train 0.13735617681453793 valid 0.19436832123423275
LOSS train 0.13735617681453793 valid 0.19435035217492308
LOSS train 0.13735617681453793 valid 0.19422511926951136
LOSS train 0.13735617681453793 valid 0.1942162427889264
LOSS train 0.13735617681453793 valid 0.1942524359558979
EPOCH 10:
  batch 1 loss: 0.12402918934822083
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.12554649263620377
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.12572111189365387
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.13478514924645424
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.14050227999687195
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.13994396726290384
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.13103171970163072
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.13083797879517078
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.1293610135714213
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.12781096994876862
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.127583385868506
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.12656508137782416
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.1272597874586399
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.12644143030047417
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.12728868573904037
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.12842889921739697
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.12684078602229848
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.12882069167163637
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.1284437391318773
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.12827353328466415
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.12850138119288854
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.12876957113092596
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.1290006106314452
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.1308654056241115
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.13036060124635696
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.1306719914651834
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.1313810599622903
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.13129750133625098
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.13111320728885717
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.13175818497935932
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.13209091535499018
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.1317379919346422
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.13144385114763724
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.1325171697227394
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.13363116064241953
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.1338808195044597
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.13409924889738495
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.1335812283581809
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.13432988161459947
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.13412194270640612
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.13424356590683867
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.1344027453590007
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.1351105381929597
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.13456751219928265
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.13548378199338912
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.1356936218621938
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.13542881814089228
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.13492113103469214
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.13479026604671868
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.13487869024276733
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.13497661229442148
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.1353672631084919
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.13553975185133377
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.1359370375672976
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.1363005971366709
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.13644961773284844
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.13653719660482908
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.13653280200629397
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.1370829555947902
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.13704497988025346
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.13685835336075455
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.1367652889701628
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.13718850390305595
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.13712956430390477
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.1370833002603971
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.13711522248658267
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.1369628706085148
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.13714166511507594
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.13742948442265607
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.13761001165424075
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.1378709766226755
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.1378210141426987
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.13804310743939385
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.13788642996066325
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.13770074238379795
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.13775518546371082
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.1375992059127077
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.13708721483365083
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.13718262126174155
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.13721149154007434
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.13745275949254449
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.13708493377013906
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.1370285262005875
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.13760304619513808
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.1376700339071891
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.13786012705328854
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.138000326680726
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.13809488265013153
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.1384747087788046
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.1385516758594248
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.13850014738656663
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.1385006754780593
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.13870854603667412
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.13887644884117106
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.13909139452796232
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.13910325251830122
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.13885698636475297
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.1388146820269069
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.1389506333554634
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.13874441511929037
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.13880506725889621
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.13870827393496737
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.1385422424928656
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.1387186601328162
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.13826573271126974
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.1381896692767458
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.13829415542221515
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.13815794733387451
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.138443202053735
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.13886493769558994
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.13875789913508269
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.13847770681604743
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.13813566119797463
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.13840767479779428
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.13816989427027496
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.13806031592961016
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.138250895927095
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.13815812817064382
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.13802667838685653
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.13821154832839966
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.13804776800319182
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.13807349633730825
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.13822972211169032
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.13811645596738784
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.13812266194820405
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.13808850861257976
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.13823568903085753
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.1381300232023932
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.13814556743054426
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.13830153443492377
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.1380386919579433
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.1379148339796247
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.13816219417000175
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.1380207833847893
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.13775931740248645
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.1376979574561119
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.1378985541145297
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.1378294773723768
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.13792821679183906
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.13810320485915456
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.13802357810608884
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.13798383791261995
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.1379385080996093
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.1381401385491093
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.13794999636452773
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.1379044521344851
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.13781686816491237
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.13776658133075043
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.13798507307999885
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.13801087826490402
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.1377888468914474
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.13774142982928375
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.13763508273690356
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.13739862346223422
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.13751857804675255
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.13742904489239058
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.1374603516546784
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.13735442089883587
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.137547327100106
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.13764043441042303
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.13749456970217805
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.13748912264903387
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.13750489568052116
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.137570796034685
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.1377128507151748
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.13766494204840027
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.13733724750087647
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.13730629498050326
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.13720850725851116
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.13732201360604343
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.13718682274832364
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.13720569993520892
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.13715545333534307
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.13705895935324416
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.13711155661514826
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.13697878063910388
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.13683471896048993
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.13660960728197954
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.1366785711416319
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.13661414500739838
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.13662563239671907
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.13652049136030805
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.13629068124815413
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.13627983038516148
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.13626187161819356
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.1362509596091445
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.1363848284444707
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.13622803427278996
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.136306470703511
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.1364475601989972
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.13649011067851052
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.13641125592403114
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.13646782317000966
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.13626869502909406
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.13624322020854707
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.13633100160074477
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.13648013683593818
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.1366443301014828
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.13664770706664378
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.13683074679225682
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.13686328359059433
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.13688718664026497
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.13680913798474326
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.13686110031809293
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.13668250692326847
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.13656839731972195
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.13656838752941233
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.13643788148720676
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.13641586062012678
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.13663831676046054
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.13663466122901835
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.13669798266634625
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.13669099630744244
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.1365456022322178
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.1366389749701633
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.1365401293668482
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.13659484547129425
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.136765944643305
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.1368383464475745
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.13689219856804066
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.13694855024641994
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.13693960447300663
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.13685609373543828
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.13687981691743648
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.1367733558681276
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.13667696697917658
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.13675906049653822
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.1366065320346439
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.13660067938040438
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.1367716429674107
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.13690179554156925
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.13686200578151078
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.13683737609775282
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.1367937276760737
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.13676863984858736
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.13681951424075386
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.136821289479984
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.13679139178340174
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.13686354196470651
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.1368352032576998
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.13693776300091962
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.13685920431224768
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.13691481061801009
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.13687933978365094
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.13688547565620773
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.13691689573773524
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.1368698795375071
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.1368549770465301
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.1367780333123054
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.13675660714507104
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.13682016857947962
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.1366308992580762
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.13663859429802347
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.13658652108485306
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.136687586646454
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.1366696774493903
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.13654259615487163
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.13652109213231145
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.13645837245990872
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.1364536084807836
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.13647426465005252
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.13646053346513792
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.13638910753192104
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.13624184079129587
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.13619447336444315
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.1362039793310757
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.13620349756452474
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.13619791213145008
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.13615566458843897
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.13612276514371235
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.13608234658892304
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.1360897825592581
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.13623009038058828
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.13617061967723562
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.1361035101522099
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.1361870816436367
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.13613728866895614
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.1361635718092644
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.13614906862004256
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.13616198892039913
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.13617380705804588
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.13617297409908147
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.1361737552990762
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.1361470174726466
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.1362382650898214
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.13627198421871745
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.13622806465272705
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.13634940472224522
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.1362742019823678
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.13628907571065016
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.1362385154282514
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.13620935084476862
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.13632283611509174
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.1363996237921877
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.1363102833850909
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.13627709702563448
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.13635148792856872
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.1363187077011438
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.13625170998150687
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.13627962191899617
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.13640406579274275
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.13636431300284846
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.13634572460903194
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.13633151935707583
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.13632729590916243
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.13630388053803663
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.1363059295893492
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.13627114533991008
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.13624144777125138
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.1362041956474704
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.13625846597159408
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.13631181667248407
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.13628198453983942
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.13622464574161608
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.13623218429940087
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.13631454154943365
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.13635781118264334
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.13631717192957987
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.13627264763120572
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.13634321154095233
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.13636104716133104
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.13638390063869288
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.13636689363249316
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.13633242833577555
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.13637169471153845
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.13628085885494035
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.1361593445278818
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.13614293094724417
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.13614136553553463
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.13622255237265066
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.1362578716928145
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.13625185122062644
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.13617060803346806
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.13619100103924375
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.13627459720444324
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.13628098974004388
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.13625195350509015
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.13624379856611143
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.13614873396875585
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.13615752609775347
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.1360985207592637
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.13624920003246843
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.13623723092301593
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.13622791740263618
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.13624938527743022
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.13628136276165184
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.13632452642539736
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.1362984862563939
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.13627605635491347
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.1361911095040185
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.13616519202703764
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.13620423906567422
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.1362201228273489
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.1363045666682518
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.1363174125762053
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.13631750446524513
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.13633662529018412
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.1364347381132275
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.13640591444079259
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.1363574696290824
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.1363390300453865
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.1363531929641468
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.13629277961805833
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.13621635282678263
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.1362606915913216
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.1362502748563967
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.13629173942174183
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.13632791961097848
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.13629865624151902
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.13625034156280594
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.13628487889416455
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.13627736232373663
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.13625783125692653
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.13632602784483827
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.1363595911860466
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.1363651862447249
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.13632924833016308
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.1363934049648898
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.13638257557688413
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.13632223063001508
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.1362933675878317
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.13624586880753178
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.13632473370969764
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.136251092403351
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.13620268762498708
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.13612648396912017
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.13614841317637638
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.136187202076322
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.13618363734995492
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.1362007541152147
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.136164042841443
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.1361625332917486
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.1361903310125414
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.13616948325168057
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.13622588013546377
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.13627298195103202
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.13618361638385043
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.1362156632857107
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.1361672178769769
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.13616259176284073
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.13618757358364333
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.13617310254134943
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.13613034603613483
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.13613469231099184
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.13609667451661311
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.13603502519318622
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.13599629811851047
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.13607702395130022
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.13606829924877814
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.1361107047192934
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.13611622055009043
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.13608677763021687
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.13605598766203367
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.13610662070017507
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.13618631704025957
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.13625541046405068
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.13624632587249902
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.13622581534551093
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.13637243140142116
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.13635167490158762
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.13640149588964331
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.13641746723538892
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.13634976816867825
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.13632181947523692
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.1363178576441372
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.1362819814346206
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.1362637499526178
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.13621401196676436
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.13618774163278388
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.13616872055585993
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.13618442768840391
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.13617883436381817
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.13612267478233672
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.13610272610791818
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.13605917262277384
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.13612685193640922
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.13620349619904154
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.13617229698115288
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.13606854041100636
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.13601036992940035
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.13601377471233983
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.1359563117079875
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.1359296130104743
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.13592050075195394
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.13593712705239822
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.135969900446756
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.13594780390627967
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.13604062009004078
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.13607627384049326
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.1360370502372583
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.13606313374843407
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.13614000655315092
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.13608383974492155
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.13603480048641758
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.13602009867573833
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.13603166657450952
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.13605262019571643
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.13605378379467795
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.13604270213959263
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.13613938925706823
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.13611787221436905
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.13612778720272567
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.13601229349493207
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.13603738353362885
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.13605834843330486
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.13604591666948643
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.13608087364751725
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.1361336931427065
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.1362060285103855
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.13624968251332323
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.13627152307840923
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.1363778547304919
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.1363778547304919 valid 0.23320214450359344
LOSS train 0.1363778547304919 valid 0.1937551125884056
LOSS train 0.1363778547304919 valid 0.19763954480489096
LOSS train 0.1363778547304919 valid 0.18395434692502022
LOSS train 0.1363778547304919 valid 0.17989339530467988
LOSS train 0.1363778547304919 valid 0.18621638665596643
LOSS train 0.1363778547304919 valid 0.19413307096276963
LOSS train 0.1363778547304919 valid 0.19340701028704643
LOSS train 0.1363778547304919 valid 0.19258205261495379
LOSS train 0.1363778547304919 valid 0.19501160234212875
LOSS train 0.1363778547304919 valid 0.1937829554080963
LOSS train 0.1363778547304919 valid 0.1914702206850052
LOSS train 0.1363778547304919 valid 0.19014338461252359
LOSS train 0.1363778547304919 valid 0.1898816909108843
LOSS train 0.1363778547304919 valid 0.1864247481028239
LOSS train 0.1363778547304919 valid 0.1877271942794323
LOSS train 0.1363778547304919 valid 0.189800822559525
LOSS train 0.1363778547304919 valid 0.18851353642013338
LOSS train 0.1363778547304919 valid 0.1905679365521983
LOSS train 0.1363778547304919 valid 0.19182488620281218
LOSS train 0.1363778547304919 valid 0.19157023585978009
LOSS train 0.1363778547304919 valid 0.1901200691407377
LOSS train 0.1363778547304919 valid 0.19009260120599167
LOSS train 0.1363778547304919 valid 0.18981112415591875
LOSS train 0.1363778547304919 valid 0.18804755926132202
LOSS train 0.1363778547304919 valid 0.18783257672419915
LOSS train 0.1363778547304919 valid 0.18779164331930656
LOSS train 0.1363778547304919 valid 0.18822274303862027
LOSS train 0.1363778547304919 valid 0.1884212421959844
LOSS train 0.1363778547304919 valid 0.18889810343583424
LOSS train 0.1363778547304919 valid 0.1898175867334489
LOSS train 0.1363778547304919 valid 0.18911010399460793
LOSS train 0.1363778547304919 valid 0.18925052158760303
LOSS train 0.1363778547304919 valid 0.188693930997568
LOSS train 0.1363778547304919 valid 0.19076387115887233
LOSS train 0.1363778547304919 valid 0.19062673300504684
LOSS train 0.1363778547304919 valid 0.1918232416784441
LOSS train 0.1363778547304919 valid 0.19205877341722188
LOSS train 0.1363778547304919 valid 0.1913040593648568
LOSS train 0.1363778547304919 valid 0.19123855754733085
LOSS train 0.1363778547304919 valid 0.19095085779341256
LOSS train 0.1363778547304919 valid 0.19156864356426967
LOSS train 0.1363778547304919 valid 0.1911724504343299
LOSS train 0.1363778547304919 valid 0.19171188873323528
LOSS train 0.1363778547304919 valid 0.1917530804872513
LOSS train 0.1363778547304919 valid 0.1926126476863156
LOSS train 0.1363778547304919 valid 0.19295185836071663
LOSS train 0.1363778547304919 valid 0.1931405970826745
LOSS train 0.1363778547304919 valid 0.1939062585636061
LOSS train 0.1363778547304919 valid 0.19330381572246552
LOSS train 0.1363778547304919 valid 0.19376599087434657
LOSS train 0.1363778547304919 valid 0.19371999914829546
LOSS train 0.1363778547304919 valid 0.19401957403938724
LOSS train 0.1363778547304919 valid 0.1942419688458796
LOSS train 0.1363778547304919 valid 0.19426868828860197
LOSS train 0.1363778547304919 valid 0.19434895100338118
LOSS train 0.1363778547304919 valid 0.19440591884286781
LOSS train 0.1363778547304919 valid 0.19430902908588277
LOSS train 0.1363778547304919 valid 0.19487756718013247
LOSS train 0.1363778547304919 valid 0.19430099899570147
LOSS train 0.1363778547304919 valid 0.19399478396431344
LOSS train 0.1363778547304919 valid 0.19416879285727778
LOSS train 0.1363778547304919 valid 0.19383959070084586
LOSS train 0.1363778547304919 valid 0.19461151398718357
LOSS train 0.1363778547304919 valid 0.1945728512910696
LOSS train 0.1363778547304919 valid 0.194350785604029
LOSS train 0.1363778547304919 valid 0.1940807444390966
LOSS train 0.1363778547304919 valid 0.19440583599840894
LOSS train 0.1363778547304919 valid 0.19360972487408182
LOSS train 0.1363778547304919 valid 0.19398258583886283
LOSS train 0.1363778547304919 valid 0.19376857515791773
LOSS train 0.1363778547304919 valid 0.19386764843430784
LOSS train 0.1363778547304919 valid 0.19401892019461278
LOSS train 0.1363778547304919 valid 0.19382901389051127
LOSS train 0.1363778547304919 valid 0.19391090750694276
LOSS train 0.1363778547304919 valid 0.19480926386619868
LOSS train 0.1363778547304919 valid 0.1945382621768233
LOSS train 0.1363778547304919 valid 0.19445598698579347
LOSS train 0.1363778547304919 valid 0.19398838753187203
LOSS train 0.1363778547304919 valid 0.19339175503700973
LOSS train 0.1363778547304919 valid 0.19277084554419105
LOSS train 0.1363778547304919 valid 0.1931727159677482
LOSS train 0.1363778547304919 valid 0.19276297092437744
LOSS train 0.1363778547304919 valid 0.192706245751608
LOSS train 0.1363778547304919 valid 0.19188442773678724
LOSS train 0.1363778547304919 valid 0.1913842451433803
LOSS train 0.1363778547304919 valid 0.191212500648937
LOSS train 0.1363778547304919 valid 0.19088119674812665
LOSS train 0.1363778547304919 valid 0.19111307651809092
LOSS train 0.1363778547304919 valid 0.1912286592854394
LOSS train 0.1363778547304919 valid 0.19128776366239067
LOSS train 0.1363778547304919 valid 0.19118117071364238
LOSS train 0.1363778547304919 valid 0.1910820238051876
LOSS train 0.1363778547304919 valid 0.19138766080141068
LOSS train 0.1363778547304919 valid 0.19087931749067807
LOSS train 0.1363778547304919 valid 0.1910164326739808
LOSS train 0.1363778547304919 valid 0.19094539072710215
LOSS train 0.1363778547304919 valid 0.19121599227798228
LOSS train 0.1363778547304919 valid 0.19133322302139166
LOSS train 0.1363778547304919 valid 0.19136908456683158
LOSS train 0.1363778547304919 valid 0.19154769462524074
LOSS train 0.1363778547304919 valid 0.19205631461797976
LOSS train 0.1363778547304919 valid 0.19183355454102302
LOSS train 0.1363778547304919 valid 0.1918578500357958
LOSS train 0.1363778547304919 valid 0.1920932521422704
LOSS train 0.1363778547304919 valid 0.19250353608491286
LOSS train 0.1363778547304919 valid 0.1921886832357567
LOSS train 0.1363778547304919 valid 0.192246969099398
LOSS train 0.1363778547304919 valid 0.19277848891161997
LOSS train 0.1363778547304919 valid 0.19277549602768637
LOSS train 0.1363778547304919 valid 0.19268221320869686
LOSS train 0.1363778547304919 valid 0.1924506008092846
LOSS train 0.1363778547304919 valid 0.19254963627431246
LOSS train 0.1363778547304919 valid 0.19275633729340738
LOSS train 0.1363778547304919 valid 0.19275070571381112
LOSS train 0.1363778547304919 valid 0.1931359622755955
LOSS train 0.1363778547304919 valid 0.19322595344139978
LOSS train 0.1363778547304919 valid 0.1929170986858465
LOSS train 0.1363778547304919 valid 0.19274418043489217
LOSS train 0.1363778547304919 valid 0.19242837404211363
LOSS train 0.1363778547304919 valid 0.19224461063373188
LOSS train 0.1363778547304919 valid 0.19228093265021434
LOSS train 0.1363778547304919 valid 0.1924127626225231
LOSS train 0.1363778547304919 valid 0.19279162249257487
LOSS train 0.1363778547304919 valid 0.1925714522600174
LOSS train 0.1363778547304919 valid 0.19276955499062462
LOSS train 0.1363778547304919 valid 0.19260448135259584
LOSS train 0.1363778547304919 valid 0.19260784646030515
LOSS train 0.1363778547304919 valid 0.1929941038752711
LOSS train 0.1363778547304919 valid 0.19276019930839539
LOSS train 0.1363778547304919 valid 0.1925471447806322
LOSS train 0.1363778547304919 valid 0.19211178747090427
LOSS train 0.1363778547304919 valid 0.19174910566412418
LOSS train 0.1363778547304919 valid 0.19191204278326746
LOSS train 0.1363778547304919 valid 0.19175972430794327
LOSS train 0.1363778547304919 valid 0.19157631246044354
LOSS train 0.1363778547304919 valid 0.191287597056723
LOSS train 0.1363778547304919 valid 0.1911525859133057
LOSS train 0.1363778547304919 valid 0.190901575007027
LOSS train 0.1363778547304919 valid 0.19098308075751577
LOSS train 0.1363778547304919 valid 0.19099788029565878
LOSS train 0.1363778547304919 valid 0.19095426325646925
LOSS train 0.1363778547304919 valid 0.19102368581961918
LOSS train 0.1363778547304919 valid 0.19113612092203563
LOSS train 0.1363778547304919 valid 0.1910686031497758
LOSS train 0.1363778547304919 valid 0.19109746395316843
LOSS train 0.1363778547304919 valid 0.19094219165188925
LOSS train 0.1363778547304919 valid 0.191945192479604
LOSS train 0.1363778547304919 valid 0.19202147434221817
LOSS train 0.1363778547304919 valid 0.19181448380152386
LOSS train 0.1363778547304919 valid 0.19212488139307263
LOSS train 0.1363778547304919 valid 0.19194453454723484
LOSS train 0.1363778547304919 valid 0.19195926578995448
LOSS train 0.1363778547304919 valid 0.19195036299816973
LOSS train 0.1363778547304919 valid 0.19181262350851488
LOSS train 0.1363778547304919 valid 0.1918546984402033
LOSS train 0.1363778547304919 valid 0.1919357029685549
LOSS train 0.1363778547304919 valid 0.1919254736998413
LOSS train 0.1363778547304919 valid 0.19196193557490343
LOSS train 0.1363778547304919 valid 0.1918247758410871
LOSS train 0.1363778547304919 valid 0.19174545716424907
LOSS train 0.1363778547304919 valid 0.1914671571166427
LOSS train 0.1363778547304919 valid 0.19121670530983276
LOSS train 0.1363778547304919 valid 0.19091671614385233
LOSS train 0.1363778547304919 valid 0.19088567811431306
LOSS train 0.1363778547304919 valid 0.19081432049173908
LOSS train 0.1363778547304919 valid 0.19104968011379242
LOSS train 0.1363778547304919 valid 0.19102457147978602
LOSS train 0.1363778547304919 valid 0.1911805019928859
LOSS train 0.1363778547304919 valid 0.1912468482466305
LOSS train 0.1363778547304919 valid 0.19117835225069035
LOSS train 0.1363778547304919 valid 0.19108064628617707
LOSS train 0.1363778547304919 valid 0.1911390950914063
LOSS train 0.1363778547304919 valid 0.19107580561747498
LOSS train 0.1363778547304919 valid 0.19094942833696094
LOSS train 0.1363778547304919 valid 0.19104734173213894
LOSS train 0.1363778547304919 valid 0.19101023236237002
LOSS train 0.1363778547304919 valid 0.19109376385975418
LOSS train 0.1363778547304919 valid 0.19096218582305163
LOSS train 0.1363778547304919 valid 0.19092744911710421
LOSS train 0.1363778547304919 valid 0.1909825815844931
LOSS train 0.1363778547304919 valid 0.19089152295510847
LOSS train 0.1363778547304919 valid 0.19087444709949805
LOSS train 0.1363778547304919 valid 0.19088861969826015
LOSS train 0.1363778547304919 valid 0.19071672147995716
LOSS train 0.1363778547304919 valid 0.1907738077864852
LOSS train 0.1363778547304919 valid 0.19065012611488608
LOSS train 0.1363778547304919 valid 0.1905566081404686
LOSS train 0.1363778547304919 valid 0.19050241841210258
LOSS train 0.1363778547304919 valid 0.19059291144734936
LOSS train 0.1363778547304919 valid 0.19053162642174365
LOSS train 0.1363778547304919 valid 0.1905635135869185
LOSS train 0.1363778547304919 valid 0.19039829731605212
LOSS train 0.1363778547304919 valid 0.19020556274455847
LOSS train 0.1363778547304919 valid 0.18992365774435874
LOSS train 0.1363778547304919 valid 0.18993399717978068
LOSS train 0.1363778547304919 valid 0.19011226061939587
LOSS train 0.1363778547304919 valid 0.19003064200432615
LOSS train 0.1363778547304919 valid 0.19010802744021968
LOSS train 0.1363778547304919 valid 0.19001334451138974
LOSS train 0.1363778547304919 valid 0.19000573752827904
LOSS train 0.1363778547304919 valid 0.18999544508976512
LOSS train 0.1363778547304919 valid 0.1900608982945898
LOSS train 0.1363778547304919 valid 0.1902947527374707
LOSS train 0.1363778547304919 valid 0.19013122203873425
LOSS train 0.1363778547304919 valid 0.19016293248215926
LOSS train 0.1363778547304919 valid 0.19021886831896317
LOSS train 0.1363778547304919 valid 0.19018748889748865
LOSS train 0.1363778547304919 valid 0.1902004727763993
LOSS train 0.1363778547304919 valid 0.1902823288525854
LOSS train 0.1363778547304919 valid 0.1902796520864794
LOSS train 0.1363778547304919 valid 0.19028348314031115
LOSS train 0.1363778547304919 valid 0.1903033222251095
LOSS train 0.1363778547304919 valid 0.19021660082529637
LOSS train 0.1363778547304919 valid 0.19011989880439847
LOSS train 0.1363778547304919 valid 0.1899412971817785
LOSS train 0.1363778547304919 valid 0.18972090099539077
LOSS train 0.1363778547304919 valid 0.18969555370031146
LOSS train 0.1363778547304919 valid 0.18981419238325667
LOSS train 0.1363778547304919 valid 0.18977566523985429
LOSS train 0.1363778547304919 valid 0.18970247280543745
LOSS train 0.1363778547304919 valid 0.1897476967927572
LOSS train 0.1363778547304919 valid 0.18980028403446814
LOSS train 0.1363778547304919 valid 0.18986691261774727
LOSS train 0.1363778547304919 valid 0.19012280338340334
LOSS train 0.1363778547304919 valid 0.19029760670609178
LOSS train 0.1363778547304919 valid 0.1903798125257576
LOSS train 0.1363778547304919 valid 0.1904021945308175
LOSS train 0.1363778547304919 valid 0.19035202154165792
LOSS train 0.1363778547304919 valid 0.19044800847768784
LOSS train 0.1363778547304919 valid 0.1905433972289552
LOSS train 0.1363778547304919 valid 0.1904936209321022
LOSS train 0.1363778547304919 valid 0.19049976746923422
LOSS train 0.1363778547304919 valid 0.19046061403221554
LOSS train 0.1363778547304919 valid 0.19057471441461685
LOSS train 0.1363778547304919 valid 0.19045716599892762
LOSS train 0.1363778547304919 valid 0.19044237505283035
LOSS train 0.1363778547304919 valid 0.19036390025074743
LOSS train 0.1363778547304919 valid 0.19026783807008335
LOSS train 0.1363778547304919 valid 0.19031684771180152
LOSS train 0.1363778547304919 valid 0.19040658600102817
LOSS train 0.1363778547304919 valid 0.19027222194208587
LOSS train 0.1363778547304919 valid 0.19049007393450404
LOSS train 0.1363778547304919 valid 0.19056673238023383
LOSS train 0.1363778547304919 valid 0.19063629860780676
LOSS train 0.1363778547304919 valid 0.19049003550676796
LOSS train 0.1363778547304919 valid 0.1905702345887659
LOSS train 0.1363778547304919 valid 0.19050189972885193
LOSS train 0.1363778547304919 valid 0.19048535650274362
LOSS train 0.1363778547304919 valid 0.1904670202732086
LOSS train 0.1363778547304919 valid 0.1903987715087564
LOSS train 0.1363778547304919 valid 0.19051505820382209
LOSS train 0.1363778547304919 valid 0.1904277846865032
LOSS train 0.1363778547304919 valid 0.1903925476463761
LOSS train 0.1363778547304919 valid 0.1903770138235653
LOSS train 0.1363778547304919 valid 0.19030731439124793
LOSS train 0.1363778547304919 valid 0.19014348814459625
LOSS train 0.1363778547304919 valid 0.19034797623175984
LOSS train 0.1363778547304919 valid 0.19036792581145828
LOSS train 0.1363778547304919 valid 0.19032843365119054
LOSS train 0.1363778547304919 valid 0.1905197952213872
LOSS train 0.1363778547304919 valid 0.19052829894855733
LOSS train 0.1363778547304919 valid 0.19058424427028844
LOSS train 0.1363778547304919 valid 0.19060351880210818
LOSS train 0.1363778547304919 valid 0.190749654173851
LOSS train 0.1363778547304919 valid 0.19075550094134824
LOSS train 0.1363778547304919 valid 0.1908058612310931
LOSS train 0.1363778547304919 valid 0.1908518248307171
LOSS train 0.1363778547304919 valid 0.19104244302418152
LOSS train 0.1363778547304919 valid 0.19114087533067775
LOSS train 0.1363778547304919 valid 0.191279447221668
LOSS train 0.1363778547304919 valid 0.19170225384261677
LOSS train 0.1363778547304919 valid 0.19187378190157614
LOSS train 0.1363778547304919 valid 0.19190694299274988
LOSS train 0.1363778547304919 valid 0.19186362597075377
LOSS train 0.1363778547304919 valid 0.19182578717236934
LOSS train 0.1363778547304919 valid 0.19177422546092354
LOSS train 0.1363778547304919 valid 0.1915775065263398
LOSS train 0.1363778547304919 valid 0.1915871920765087
LOSS train 0.1363778547304919 valid 0.19148695421005998
LOSS train 0.1363778547304919 valid 0.1914300827369147
LOSS train 0.1363778547304919 valid 0.1912080445822249
LOSS train 0.1363778547304919 valid 0.19125499126136092
LOSS train 0.1363778547304919 valid 0.1913185059289697
LOSS train 0.1363778547304919 valid 0.19132051305812703
LOSS train 0.1363778547304919 valid 0.19133575437785863
LOSS train 0.1363778547304919 valid 0.19134032331692633
LOSS train 0.1363778547304919 valid 0.1913523855101731
LOSS train 0.1363778547304919 valid 0.19132771211511948
LOSS train 0.1363778547304919 valid 0.19147728907650916
LOSS train 0.1363778547304919 valid 0.191255878234647
LOSS train 0.1363778547304919 valid 0.19128696615361188
LOSS train 0.1363778547304919 valid 0.19132420593561167
LOSS train 0.1363778547304919 valid 0.19146127968418356
LOSS train 0.1363778547304919 valid 0.19160509766158412
LOSS train 0.1363778547304919 valid 0.19149445767539577
LOSS train 0.1363778547304919 valid 0.19149332822171927
LOSS train 0.1363778547304919 valid 0.19145160648446755
LOSS train 0.1363778547304919 valid 0.19145945845838375
LOSS train 0.1363778547304919 valid 0.19149134715398153
LOSS train 0.1363778547304919 valid 0.19152491453082063
LOSS train 0.1363778547304919 valid 0.1914606758004782
LOSS train 0.1363778547304919 valid 0.19152749524061435
LOSS train 0.1363778547304919 valid 0.19161997451201865
LOSS train 0.1363778547304919 valid 0.19151833160001724
LOSS train 0.1363778547304919 valid 0.19150898800371519
LOSS train 0.1363778547304919 valid 0.19154752489992383
LOSS train 0.1363778547304919 valid 0.19149988531679302
LOSS train 0.1363778547304919 valid 0.19151749334096138
LOSS train 0.1363778547304919 valid 0.19147068569737097
LOSS train 0.1363778547304919 valid 0.19136125385952915
LOSS train 0.1363778547304919 valid 0.19148842808909905
LOSS train 0.1363778547304919 valid 0.19158460414066864
LOSS train 0.1363778547304919 valid 0.1916551391600044
LOSS train 0.1363778547304919 valid 0.1915831884221425
LOSS train 0.1363778547304919 valid 0.19155768484254426
LOSS train 0.1363778547304919 valid 0.1916769124355978
LOSS train 0.1363778547304919 valid 0.19180720369770843
LOSS train 0.1363778547304919 valid 0.1918622478609175
LOSS train 0.1363778547304919 valid 0.19177678595297037
LOSS train 0.1363778547304919 valid 0.19188475265309818
LOSS train 0.1363778547304919 valid 0.1918841568487031
LOSS train 0.1363778547304919 valid 0.19185447798978433
LOSS train 0.1363778547304919 valid 0.19194664309422174
LOSS train 0.1363778547304919 valid 0.19192380813451915
LOSS train 0.1363778547304919 valid 0.19206310278433233
LOSS train 0.1363778547304919 valid 0.19216576697082693
LOSS train 0.1363778547304919 valid 0.1920418200968969
LOSS train 0.1363778547304919 valid 0.192221629809826
LOSS train 0.1363778547304919 valid 0.19213092417427988
LOSS train 0.1363778547304919 valid 0.1920133739979966
LOSS train 0.1363778547304919 valid 0.19195192177073064
LOSS train 0.1363778547304919 valid 0.19196348218946485
LOSS train 0.1363778547304919 valid 0.19206275014641755
LOSS train 0.1363778547304919 valid 0.1920568016038012
LOSS train 0.1363778547304919 valid 0.19216597505978175
LOSS train 0.1363778547304919 valid 0.19225541924510584
LOSS train 0.1363778547304919 valid 0.1922368348614704
LOSS train 0.1363778547304919 valid 0.1922550901638723
LOSS train 0.1363778547304919 valid 0.19222635710064104
LOSS train 0.1363778547304919 valid 0.19212242130659893
LOSS train 0.1363778547304919 valid 0.192068262537669
LOSS train 0.1363778547304919 valid 0.19207973456765054
LOSS train 0.1363778547304919 valid 0.1923700337579777
LOSS train 0.1363778547304919 valid 0.19242422274057416
LOSS train 0.1363778547304919 valid 0.19244644618620074
LOSS train 0.1363778547304919 valid 0.1923517590812716
LOSS train 0.1363778547304919 valid 0.19228261632138285
LOSS train 0.1363778547304919 valid 0.1922981240346302
LOSS train 0.1363778547304919 valid 0.19221333137580326
LOSS train 0.1363778547304919 valid 0.1921467237238191
LOSS train 0.1363778547304919 valid 0.19215920626778493
LOSS train 0.1363778547304919 valid 0.19211750859579352
LOSS train 0.1363778547304919 valid 0.19209051999331866
LOSS train 0.1363778547304919 valid 0.19220539688224525
LOSS train 0.1363778547304919 valid 0.1922049242375272
LOSS train 0.1363778547304919 valid 0.19224842998827874
LOSS train 0.1363778547304919 valid 0.19216436568085707
LOSS train 0.1363778547304919 valid 0.19215802806831667
LOSS train 0.1363778547304919 valid 0.19209089448882474
LOSS train 0.1363778547304919 valid 0.19210553590280527
LOSS train 0.1363778547304919 valid 0.19219971802188546
LOSS train 0.1363778547304919 valid 0.19204903798326645
LOSS train 0.1363778547304919 valid 0.1920736883204062
LOSS train 0.1363778547304919 valid 0.1920876585457423
LOSS train 0.1363778547304919 valid 0.19207284436688396
LOSS train 0.1363778547304919 valid 0.19194967752418987
LOSS train 0.1363778547304919 valid 0.1919410063115799
LOSS train 0.1363778547304919 valid 0.19197448559085206
EPOCH 11:
  batch 1 loss: 0.1319155991077423
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.12645652145147324
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.1255340278148651
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.13372761383652687
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.13921039402484894
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.13889105866352716
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.13006563484668732
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.1297805141657591
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.12857405675782096
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.12695312425494193
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.12699220329523087
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.12555893883109093
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.12659483231030977
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.12580792659095355
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.12613703658183414
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.12758739246055484
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.12600466959616718
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.12804465823703343
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.12786767004351868
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.1275936782360077
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.12759265019780114
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.12798769094727255
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.12814460565214572
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.12992977909743786
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.12947356700897217
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.12987830547186044
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.13049945345631353
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.13022196452532495
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.12995017345609336
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.13055460005998612
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.1310375001161329
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.13067724625580013
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.1304620827237765
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.13156863543040612
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.1327922652874674
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.1328728755729066
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.13310944731976534
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.1326340438896104
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.13340846181679994
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.133197514526546
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.13328484954630457
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.13355867156670206
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.13421719937130464
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.13370033505965362
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.13458120740122265
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.1348356985203598
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.13448697534647394
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.13401034536461034
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.13386032563083025
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.13399121552705764
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.1339772641658783
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.13446985328426728
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.13465453009560424
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.13500188870562446
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.13541111946105958
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.13550281577876636
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.13574065555605971
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.13567438413356914
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.1362627512317593
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.13622881844639778
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.1360097722929032
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.13590095240262248
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.136282164426077
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.13617776543833315
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.13616162194655493
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.13618187335404483
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.13605843417680086
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.1362491046242854
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.13655905015226724
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.1367463001183101
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.1370610838205042
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.1369965680771404
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.13722039595858693
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.1370496770014634
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.136846621632576
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.13692879676818848
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.13673061422713392
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.13624637650373655
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.13639727480049374
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.13641867749392986
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.13663494476565607
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.13630230097872456
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.13621163107903608
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.13680793407062689
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.13688749048639745
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.13713549727270768
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.13715385371583633
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.13725060046735135
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.13771749338072337
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.13780100916822752
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.137761553974597
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.1377440827858189
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.13791877471952027
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.1381092257797718
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.13834153688267659
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.13836435469177863
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.13812424388435698
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.138075838359643
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.13825719231607939
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.1380539271235466
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.13803191674817908
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.1379014404819292
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.1377551003711895
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.137941889393215
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.13746120511066345
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.13735519497180884
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.137538217843693
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.13735386591266702
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.1376303255284598
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.13801994147625837
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.13793448459457708
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.13766392194000737
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.13731735549141874
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.13758463046529837
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.13731722157934437
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.13726256106948032
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.13742662864363092
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.137305437634557
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.1371503893692954
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.13731179367750884
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.13716202943531935
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.13716764131286105
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.13733205859496342
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.13726448826491833
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.1372262504696846
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.1371898947372323
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.13726996466165453
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.1371530840988271
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.13716878210620362
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.1373222728761343
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.13708165464282945
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.13694069828047897
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.13719826174857921
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.13707150107444221
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.13682958478177035
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.13679603229769888
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.13702159081279797
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.1369484138769516
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.13711505704860893
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.13730852822107928
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.1372224583376384
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.13722460459865315
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.13717894553096144
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.13735950122483903
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.13716558084405703
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.1371131162733248
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.13702944341768214
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.13692534171246193
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.13718127844317649
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.1372208611170451
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.13700055173098646
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.13694339730825864
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.1368264177558469
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.1366115075523977
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.13668662756681443
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.13658935399964833
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.1365923750077843
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.13648774847388268
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.13666461225388185
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.13676792471669613
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.1366544996636995
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.13665934491120738
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.13663896996002256
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.1367013884662855
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.13678787517728228
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.13674672688525844
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.13643913818690592
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.1363878541049503
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.1362819396091636
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.1363899433875785
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.13624935392399282
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.13620883365010106
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.13614199039219432
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.13602875168809944
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.13607517638376782
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.13593900483101606
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.1357778379809385
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.13558014503188348
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.13564482202243539
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.13558538767198722
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.13560725019946282
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.13555202817360124
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.1353272956495728
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.13528971159425768
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.13530966820749077
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.1353057505542873
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.13544083123697953
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.13530932022060485
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.13536531761998222
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.13550625423851767
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.13556915548927498
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.13547259631256262
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.13554130502315384
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.13533800437124735
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.13532010114345794
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.1354100972946201
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.13555718319216356
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.135696844207217
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.13569956976714445
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.1358971719816327
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.13591341047886
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.135958120189976
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.13588171019195924
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.13594238237276965
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.13578406285221983
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.13563386162797225
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.13562537297822427
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.13546783978549334
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.13542756195844075
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.13565689098267328
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.1356612024431545
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.1357475256301322
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.13575841367524555
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.13560268780756218
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.13570104667613672
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.13559192491488326
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.13563625950555097
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.13580445962351398
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.1358884732540884
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.13594444180754098
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.13599233035992714
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.13599206557547724
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.13591761342479508
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.13594997797294386
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.1358401526676284
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.13575216817908584
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.13583123040619402
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.13566251785347336
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.1356703791946303
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.13585793466671653
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.13598378847687792
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.1359424625105899
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.13592500172459515
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.13585491870076227
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.135821544331439
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.1358850301890555
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.13587762688911414
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.13584646447992124
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.13591583718440522
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.13589357960348328
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.13600371762553687
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.13593191955207792
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.13597163717442579
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.13592813927374903
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.1359645581975275
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.13600480804840723
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.1359488253711689
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.13592928687050457
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.13586736793618603
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.13585260805487634
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.13592204686061318
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.13576906135985775
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.13576115582418063
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.1357125599729264
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.1358026137246805
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.1357788977038581
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.1356516730344713
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.13561906665563583
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.13555164226693997
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.13552825135680346
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.13552432932616193
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.13553671413705548
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.13545458239747543
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.13531377570082745
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.13527432671695386
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.1353181794117716
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.1353103858645489
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.13531192734059114
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.13527638265745348
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.13523999190440886
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.13520820628562977
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.13523528541383498
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.13539489052998713
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.13532959096079325
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.1352746671167287
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.13536677771396394
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.13531085187132177
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.13533607350407745
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.13531348141290808
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.13535276539623736
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.13536859306364296
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.13533469411075538
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.1353418369491193
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.13532661420988365
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.13544658751864183
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.13546953975529105
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.13545004961174956
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.1355654203539921
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.13549036555842958
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.13549376983067085
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.13544334665811347
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.13541709784775563
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.13553695604459418
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.13560071861257358
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.13552063599986544
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.13548040973979072
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.13554066018223362
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.13551444050249636
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.13544347842401486
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.13546517943342526
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.13558943688473432
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.13552345027098592
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.13550245744658776
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.1354857857437118
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.13547233936728023
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.135455159555658
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.1354517703481528
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.13543620393551015
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.13540266283025248
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.135389759343478
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.13543686967858165
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.13547445315485582
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.1354392798849569
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.13538086663480778
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.13540011343974917
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.13549576060775714
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.13552774843827406
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.13550652626831577
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.13546952661302022
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.13554050410166382
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.13557197209273544
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.13557894134558507
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.13557437892668756
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.1355294890379832
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.1355805206986574
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.1354949357776554
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.13537780149905324
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.13535175924530116
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.13535994597028453
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.13543138501770569
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.1354940853045066
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.13547757297125926
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.13541035504043997
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.13542322379475583
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.13550316293292972
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.1355037872590834
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.13546267894679787
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.13545626560611837
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.1353544385383966
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.13536595447975047
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.1353112993395923
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.13547233184962942
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.13544459938568554
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.13544453617705163
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.1354731802707133
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.13549021271870315
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.13554484654829893
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.13552685229685799
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.13551232690541315
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.13543161996773312
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.13540558916041653
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.13545253724706444
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.1354715235256946
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.13555391246484497
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.13556309770530378
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.13554450593302758
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.13556196000061782
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.13565796409738795
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.135625152119687
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.13558108332670396
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.135544960972675
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.13555592895377408
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.13549712786743465
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.13541900804573362
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.13547101167783346
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.13547268978070692
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.13550474143483007
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.13554694187705932
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.13551678387372473
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.135469207469676
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.13550487640652695
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.13549478173816717
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.1354987846023276
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.13556784648388465
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.13559172886610033
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.1356214679817253
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.13557625451280836
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.13563182273948635
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.13562133878784002
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.13557126939688857
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.13554533668781515
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.13550759374081153
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.13559954057085297
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.13553513567118594
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.13549695996107994
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.1354205529615669
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.1354432882649646
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.1354906919229891
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.13548248814739727
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.13548673398983785
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.13546553962980695
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.1354645755218
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.13549274771116465
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.13547191250747836
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.1355297550747666
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.13558110997381836
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.13550045690323304
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.13551969086779422
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.13548205830250168
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.13548274416476488
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.13551187203114765
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.1354973725789222
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.13544981421962862
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.13546071551961475
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.1354246723983023
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.13537617583800418
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.13533990606848673
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.13541947672253146
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.13539756832685332
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.13543044221473904
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.1354326171632811
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.1354071938441795
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.13537569560525492
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.13542994615680354
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.13549885889851904
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.13556341314688325
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.13554779027434563
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.13551468669512626
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.13565977653910835
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.13562629004674298
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.13568066179115246
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.13569787248827836
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.13564179432589393
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.1356366232459275
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.1356496622281916
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.13561334625376223
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.13559474968798546
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.1355528591601091
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.13552026652094923
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.13549687242092087
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.13550826589082068
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.1354973541089782
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.1354482151894867
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.13543004868766678
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.1353863804504789
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.1354480551880434
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.1355289041450422
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.13549995822109043
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.13538960569404523
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.1353134070269086
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.1353163706675138
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.13526603724487227
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.1352551306343509
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.13524902578409728
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.13527107255512408
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.13529499119039073
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.13527506419876278
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.13537675418358827
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.1354065971876306
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.13535323676135805
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.1353865487216053
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.13547684994023457
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.13542547386147855
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.13538179224939598
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.1353609499368039
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.13537625298557573
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.13537786653187917
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.13537238931942194
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.13537825660248468
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.13546143313464912
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.13544197955902124
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.13544935662121999
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.1353313817962457
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.1353531864889223
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.13536853517896386
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.13535268684697255
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.13538714560502602
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.13543752676401383
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.1355023467337399
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.13553622013076821
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.13556615426155666
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.13566844020101984
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.13566844020101984 valid 0.2304743230342865
LOSS train 0.13566844020101984 valid 0.19075985997915268
LOSS train 0.13566844020101984 valid 0.194466695189476
LOSS train 0.13566844020101984 valid 0.18063011392951012
LOSS train 0.13566844020101984 valid 0.17645520269870757
LOSS train 0.13566844020101984 valid 0.18273353576660156
LOSS train 0.13566844020101984 valid 0.1905624887772969
LOSS train 0.13566844020101984 valid 0.1899215616285801
LOSS train 0.13566844020101984 valid 0.1891026364432441
LOSS train 0.13566844020101984 valid 0.19144642055034639
LOSS train 0.13566844020101984 valid 0.19013563340360468
LOSS train 0.13566844020101984 valid 0.18789834653337797
LOSS train 0.13566844020101984 valid 0.18659901160460252
LOSS train 0.13566844020101984 valid 0.18632589812789643
LOSS train 0.13566844020101984 valid 0.18294597367445628
LOSS train 0.13566844020101984 valid 0.18421374261379242
LOSS train 0.13566844020101984 valid 0.18621521223993862
LOSS train 0.13566844020101984 valid 0.18493231882651648
LOSS train 0.13566844020101984 valid 0.18697893070547203
LOSS train 0.13566844020101984 valid 0.1882123000919819
LOSS train 0.13566844020101984 valid 0.18792809475035893
LOSS train 0.13566844020101984 valid 0.18647969581864096
LOSS train 0.13566844020101984 valid 0.1864773391381554
LOSS train 0.13566844020101984 valid 0.1862094228466352
LOSS train 0.13566844020101984 valid 0.18448168218135833
LOSS train 0.13566844020101984 valid 0.18427160038397863
LOSS train 0.13566844020101984 valid 0.18423588849880077
LOSS train 0.13566844020101984 valid 0.18459338801247732
LOSS train 0.13566844020101984 valid 0.1847634793355547
LOSS train 0.13566844020101984 valid 0.18518603344758353
LOSS train 0.13566844020101984 valid 0.18609197870377572
LOSS train 0.13566844020101984 valid 0.18542677583172917
LOSS train 0.13566844020101984 valid 0.18555655488462158
LOSS train 0.13566844020101984 valid 0.18499268898192575
LOSS train 0.13566844020101984 valid 0.18705995466027941
LOSS train 0.13566844020101984 valid 0.18687995026508966
LOSS train 0.13566844020101984 valid 0.18808298255946185
LOSS train 0.13566844020101984 valid 0.18831193996103188
LOSS train 0.13566844020101984 valid 0.1875993074514927
LOSS train 0.13566844020101984 valid 0.18752970322966575
LOSS train 0.13566844020101984 valid 0.18725985615718654
LOSS train 0.13566844020101984 valid 0.1878554888424419
LOSS train 0.13566844020101984 valid 0.1874880211990933
LOSS train 0.13566844020101984 valid 0.18802209537137637
LOSS train 0.13566844020101984 valid 0.18809690276781718
LOSS train 0.13566844020101984 valid 0.18897692535234534
LOSS train 0.13566844020101984 valid 0.18930171144769548
LOSS train 0.13566844020101984 valid 0.18950647550324598
LOSS train 0.13566844020101984 valid 0.19022900352672656
LOSS train 0.13566844020101984 valid 0.18965141892433166
LOSS train 0.13566844020101984 valid 0.19012115662004433
LOSS train 0.13566844020101984 valid 0.19007059501913878
LOSS train 0.13566844020101984 valid 0.19037762509202058
LOSS train 0.13566844020101984 valid 0.19057714773548973
LOSS train 0.13566844020101984 valid 0.19059547619386152
LOSS train 0.13566844020101984 valid 0.19068517056959017
LOSS train 0.13566844020101984 valid 0.19074704458839015
LOSS train 0.13566844020101984 valid 0.19064259297888855
LOSS train 0.13566844020101984 valid 0.19120476811619128
LOSS train 0.13566844020101984 valid 0.19063006689151127
LOSS train 0.13566844020101984 valid 0.19033664806944425
LOSS train 0.13566844020101984 valid 0.1905198424093185
LOSS train 0.13566844020101984 valid 0.19018450805119105
LOSS train 0.13566844020101984 valid 0.19094421132467687
LOSS train 0.13566844020101984 valid 0.19088331713126255
LOSS train 0.13566844020101984 valid 0.19064891428658456
LOSS train 0.13566844020101984 valid 0.19039390247259566
LOSS train 0.13566844020101984 valid 0.1907194690669284
LOSS train 0.13566844020101984 valid 0.18993712508160135
LOSS train 0.13566844020101984 valid 0.1903387806245259
LOSS train 0.13566844020101984 valid 0.19012386383305133
LOSS train 0.13566844020101984 valid 0.19021932180556986
LOSS train 0.13566844020101984 valid 0.19036031279661883
LOSS train 0.13566844020101984 valid 0.19016152277991577
LOSS train 0.13566844020101984 valid 0.1902356344461441
LOSS train 0.13566844020101984 valid 0.19112520488469223
LOSS train 0.13566844020101984 valid 0.19086482230719035
LOSS train 0.13566844020101984 valid 0.19078740229209265
LOSS train 0.13566844020101984 valid 0.19032986084871653
LOSS train 0.13566844020101984 valid 0.189723639190197
LOSS train 0.13566844020101984 valid 0.18910618862252176
LOSS train 0.13566844020101984 valid 0.1895024720125082
LOSS train 0.13566844020101984 valid 0.1891055410525885
LOSS train 0.13566844020101984 valid 0.18906011237275033
LOSS train 0.13566844020101984 valid 0.188234343335909
LOSS train 0.13566844020101984 valid 0.18774561124832131
LOSS train 0.13566844020101984 valid 0.18756541190133696
LOSS train 0.13566844020101984 valid 0.18724054665389386
LOSS train 0.13566844020101984 valid 0.18745411405067766
LOSS train 0.13566844020101984 valid 0.18756110792358716
LOSS train 0.13566844020101984 valid 0.18762108937397107
LOSS train 0.13566844020101984 valid 0.18752736622548621
LOSS train 0.13566844020101984 valid 0.18742065428085225
LOSS train 0.13566844020101984 valid 0.18771715485986243
LOSS train 0.13566844020101984 valid 0.18721865690068196
LOSS train 0.13566844020101984 valid 0.18735645362176
LOSS train 0.13566844020101984 valid 0.18727955137638702
LOSS train 0.13566844020101984 valid 0.18755092844367027
LOSS train 0.13566844020101984 valid 0.18766738361481464
LOSS train 0.13566844020101984 valid 0.18770272590219975
LOSS train 0.13566844020101984 valid 0.18786381192431592
LOSS train 0.13566844020101984 valid 0.18837068894622372
LOSS train 0.13566844020101984 valid 0.18814028329351573
LOSS train 0.13566844020101984 valid 0.18816722392176205
LOSS train 0.13566844020101984 valid 0.18840736895799637
LOSS train 0.13566844020101984 valid 0.18882019642107892
LOSS train 0.13566844020101984 valid 0.18851653458637613
LOSS train 0.13566844020101984 valid 0.1885620116221684
LOSS train 0.13566844020101984 valid 0.18907246588293566
LOSS train 0.13566844020101984 valid 0.1890633812682195
LOSS train 0.13566844020101984 valid 0.1889779078396591
LOSS train 0.13566844020101984 valid 0.18875144588361895
LOSS train 0.13566844020101984 valid 0.1888510439116343
LOSS train 0.13566844020101984 valid 0.18907038834795617
LOSS train 0.13566844020101984 valid 0.1890625848070435
LOSS train 0.13566844020101984 valid 0.1894413892436644
LOSS train 0.13566844020101984 valid 0.1895186689037543
LOSS train 0.13566844020101984 valid 0.18920888379216194
LOSS train 0.13566844020101984 valid 0.1890407805057133
LOSS train 0.13566844020101984 valid 0.1887293432528774
LOSS train 0.13566844020101984 valid 0.18854200291978426
LOSS train 0.13566844020101984 valid 0.18857979450802334
LOSS train 0.13566844020101984 valid 0.18869528022965765
LOSS train 0.13566844020101984 valid 0.1890764145721351
LOSS train 0.13566844020101984 valid 0.18886184281110763
LOSS train 0.13566844020101984 valid 0.1890574741576399
LOSS train 0.13566844020101984 valid 0.1888863837977094
LOSS train 0.13566844020101984 valid 0.18889822653727606
LOSS train 0.13566844020101984 valid 0.18927747479011847
LOSS train 0.13566844020101984 valid 0.18903740443862402
LOSS train 0.13566844020101984 valid 0.18882663656054563
LOSS train 0.13566844020101984 valid 0.1883978039364923
LOSS train 0.13566844020101984 valid 0.18803441911039495
LOSS train 0.13566844020101984 valid 0.18820291184889737
LOSS train 0.13566844020101984 valid 0.18805598103337817
LOSS train 0.13566844020101984 valid 0.18787110438022545
LOSS train 0.13566844020101984 valid 0.18758436760110578
LOSS train 0.13566844020101984 valid 0.18745151545474495
LOSS train 0.13566844020101984 valid 0.18720346544500735
LOSS train 0.13566844020101984 valid 0.18728686118764537
LOSS train 0.13566844020101984 valid 0.18730512943039548
LOSS train 0.13566844020101984 valid 0.18726772662829344
LOSS train 0.13566844020101984 valid 0.18733136559074576
LOSS train 0.13566844020101984 valid 0.1874336469401088
LOSS train 0.13566844020101984 valid 0.18737370798300051
LOSS train 0.13566844020101984 valid 0.18740185436933007
LOSS train 0.13566844020101984 valid 0.18724105142208994
LOSS train 0.13566844020101984 valid 0.18824190500418883
LOSS train 0.13566844020101984 valid 0.18831373056109318
LOSS train 0.13566844020101984 valid 0.1881069578230381
LOSS train 0.13566844020101984 valid 0.18841229680160812
LOSS train 0.13566844020101984 valid 0.18823485061722367
LOSS train 0.13566844020101984 valid 0.18825337548855864
LOSS train 0.13566844020101984 valid 0.1882430664807945
LOSS train 0.13566844020101984 valid 0.18810298495715663
LOSS train 0.13566844020101984 valid 0.18814541471119112
LOSS train 0.13566844020101984 valid 0.18822715630766693
LOSS train 0.13566844020101984 valid 0.18820915536224087
LOSS train 0.13566844020101984 valid 0.18824737519025803
LOSS train 0.13566844020101984 valid 0.18811035924591124
LOSS train 0.13566844020101984 valid 0.18803637437753795
LOSS train 0.13566844020101984 valid 0.18776348412588792
LOSS train 0.13566844020101984 valid 0.18751695537311167
LOSS train 0.13566844020101984 valid 0.18721751527997052
LOSS train 0.13566844020101984 valid 0.18718412721698935
LOSS train 0.13566844020101984 valid 0.18711500888667912
LOSS train 0.13566844020101984 valid 0.18734510184940464
LOSS train 0.13566844020101984 valid 0.1873111186903857
LOSS train 0.13566844020101984 valid 0.1874679881175594
LOSS train 0.13566844020101984 valid 0.18753088734605733
LOSS train 0.13566844020101984 valid 0.18747194899975905
LOSS train 0.13566844020101984 valid 0.18737856017122434
LOSS train 0.13566844020101984 valid 0.18742818755738308
LOSS train 0.13566844020101984 valid 0.1873620673585897
LOSS train 0.13566844020101984 valid 0.1872394283754485
LOSS train 0.13566844020101984 valid 0.18734083210372113
LOSS train 0.13566844020101984 valid 0.18730616784196788
LOSS train 0.13566844020101984 valid 0.18739427906576167
LOSS train 0.13566844020101984 valid 0.18726541863639928
LOSS train 0.13566844020101984 valid 0.18723039871288671
LOSS train 0.13566844020101984 valid 0.1872965656015096
LOSS train 0.13566844020101984 valid 0.1872058047779969
LOSS train 0.13566844020101984 valid 0.18719511707135236
LOSS train 0.13566844020101984 valid 0.18720887953658466
LOSS train 0.13566844020101984 valid 0.18703988142110206
LOSS train 0.13566844020101984 valid 0.18709550441433026
LOSS train 0.13566844020101984 valid 0.18697860147704415
LOSS train 0.13566844020101984 valid 0.18689073919457325
LOSS train 0.13566844020101984 valid 0.1868374489996799
LOSS train 0.13566844020101984 valid 0.18692532671909584
LOSS train 0.13566844020101984 valid 0.18686176863796425
LOSS train 0.13566844020101984 valid 0.18689482445673397
LOSS train 0.13566844020101984 valid 0.1867208250368815
LOSS train 0.13566844020101984 valid 0.18652946751603147
LOSS train 0.13566844020101984 valid 0.1862519329557052
LOSS train 0.13566844020101984 valid 0.18625850348296213
LOSS train 0.13566844020101984 valid 0.18643881429724282
LOSS train 0.13566844020101984 valid 0.1863600086759437
LOSS train 0.13566844020101984 valid 0.18643943160472803
LOSS train 0.13566844020101984 valid 0.18634160477668046
LOSS train 0.13566844020101984 valid 0.1863336066775654
LOSS train 0.13566844020101984 valid 0.18633109777428136
LOSS train 0.13566844020101984 valid 0.18639740513022898
LOSS train 0.13566844020101984 valid 0.18662904199285835
LOSS train 0.13566844020101984 valid 0.1864704248745267
LOSS train 0.13566844020101984 valid 0.186505068165874
LOSS train 0.13566844020101984 valid 0.18656120240544352
LOSS train 0.13566844020101984 valid 0.1865295644921179
LOSS train 0.13566844020101984 valid 0.1865438638192615
LOSS train 0.13566844020101984 valid 0.18662199991799536
LOSS train 0.13566844020101984 valid 0.18661805171678417
LOSS train 0.13566844020101984 valid 0.18662300660222206
LOSS train 0.13566844020101984 valid 0.18664071184228842
LOSS train 0.13566844020101984 valid 0.1865507556108114
LOSS train 0.13566844020101984 valid 0.18645186323759166
LOSS train 0.13566844020101984 valid 0.18627178182618487
LOSS train 0.13566844020101984 valid 0.18605410081038276
LOSS train 0.13566844020101984 valid 0.18602463756816104
LOSS train 0.13566844020101984 valid 0.18614156030492696
LOSS train 0.13566844020101984 valid 0.1860991589386355
LOSS train 0.13566844020101984 valid 0.18602578810572085
LOSS train 0.13566844020101984 valid 0.1860706991776153
LOSS train 0.13566844020101984 valid 0.18611985996298727
LOSS train 0.13566844020101984 valid 0.1861874865966716
LOSS train 0.13566844020101984 valid 0.186443905863497
LOSS train 0.13566844020101984 valid 0.18662436013068773
LOSS train 0.13566844020101984 valid 0.18670385823244565
LOSS train 0.13566844020101984 valid 0.18672212066226884
LOSS train 0.13566844020101984 valid 0.18667360043812006
LOSS train 0.13566844020101984 valid 0.18677078132396158
LOSS train 0.13566844020101984 valid 0.18687019800339943
LOSS train 0.13566844020101984 valid 0.18681781729362135
LOSS train 0.13566844020101984 valid 0.1868298007375181
LOSS train 0.13566844020101984 valid 0.18679130462627125
LOSS train 0.13566844020101984 valid 0.186902820144562
LOSS train 0.13566844020101984 valid 0.1867877612111427
LOSS train 0.13566844020101984 valid 0.1867698872705552
LOSS train 0.13566844020101984 valid 0.18669106219621265
LOSS train 0.13566844020101984 valid 0.18659094858244374
LOSS train 0.13566844020101984 valid 0.18663751790300012
LOSS train 0.13566844020101984 valid 0.1867289816133709
LOSS train 0.13566844020101984 valid 0.1865992620895224
LOSS train 0.13566844020101984 valid 0.1868199405908094
LOSS train 0.13566844020101984 valid 0.18690013009138773
LOSS train 0.13566844020101984 valid 0.186970041753078
LOSS train 0.13566844020101984 valid 0.18681805305243507
LOSS train 0.13566844020101984 valid 0.1868966173244874
LOSS train 0.13566844020101984 valid 0.18682739994819125
LOSS train 0.13566844020101984 valid 0.18681200162832995
LOSS train 0.13566844020101984 valid 0.18678866890072823
LOSS train 0.13566844020101984 valid 0.18672032865038907
LOSS train 0.13566844020101984 valid 0.1868375437186351
LOSS train 0.13566844020101984 valid 0.1867505433001066
LOSS train 0.13566844020101984 valid 0.18671433008679256
LOSS train 0.13566844020101984 valid 0.18669607005282945
LOSS train 0.13566844020101984 valid 0.18662569599109702
LOSS train 0.13566844020101984 valid 0.18646106248583774
LOSS train 0.13566844020101984 valid 0.18666337387159812
LOSS train 0.13566844020101984 valid 0.18668394311399533
LOSS train 0.13566844020101984 valid 0.18663939773463287
LOSS train 0.13566844020101984 valid 0.18683425702468645
LOSS train 0.13566844020101984 valid 0.1868466947881775
LOSS train 0.13566844020101984 valid 0.18690569435348983
LOSS train 0.13566844020101984 valid 0.18692100135553064
LOSS train 0.13566844020101984 valid 0.1870688389775888
LOSS train 0.13566844020101984 valid 0.18707273981737016
LOSS train 0.13566844020101984 valid 0.18712420089079645
LOSS train 0.13566844020101984 valid 0.18716784335895262
LOSS train 0.13566844020101984 valid 0.18735363655706322
LOSS train 0.13566844020101984 valid 0.187453909466664
LOSS train 0.13566844020101984 valid 0.18759688963199453
LOSS train 0.13566844020101984 valid 0.18801869460217216
LOSS train 0.13566844020101984 valid 0.18819080787154782
LOSS train 0.13566844020101984 valid 0.18823130502209176
LOSS train 0.13566844020101984 valid 0.1881910210035064
LOSS train 0.13566844020101984 valid 0.18815663524403953
LOSS train 0.13566844020101984 valid 0.18810510239984155
LOSS train 0.13566844020101984 valid 0.1879121001622231
LOSS train 0.13566844020101984 valid 0.18792030116456385
LOSS train 0.13566844020101984 valid 0.1878212892317346
LOSS train 0.13566844020101984 valid 0.1877634537230607
LOSS train 0.13566844020101984 valid 0.1875409058803785
LOSS train 0.13566844020101984 valid 0.18759499303338384
LOSS train 0.13566844020101984 valid 0.18765963726795057
LOSS train 0.13566844020101984 valid 0.18765933934533804
LOSS train 0.13566844020101984 valid 0.18767289658422237
LOSS train 0.13566844020101984 valid 0.18767736776884425
LOSS train 0.13566844020101984 valid 0.18768901861686674
LOSS train 0.13566844020101984 valid 0.18766518078976435
LOSS train 0.13566844020101984 valid 0.18781344636247077
LOSS train 0.13566844020101984 valid 0.18759853525976955
LOSS train 0.13566844020101984 valid 0.1876263219659051
LOSS train 0.13566844020101984 valid 0.1876571803217048
LOSS train 0.13566844020101984 valid 0.18779365720898927
LOSS train 0.13566844020101984 valid 0.18793855887853494
LOSS train 0.13566844020101984 valid 0.18782791991189524
LOSS train 0.13566844020101984 valid 0.18782742665171223
LOSS train 0.13566844020101984 valid 0.1877859264562194
LOSS train 0.13566844020101984 valid 0.1877916629447985
LOSS train 0.13566844020101984 valid 0.18782429970800876
LOSS train 0.13566844020101984 valid 0.18785978361122632
LOSS train 0.13566844020101984 valid 0.1877988339309266
LOSS train 0.13566844020101984 valid 0.18786514732408838
LOSS train 0.13566844020101984 valid 0.18795770399370476
LOSS train 0.13566844020101984 valid 0.1878564702927089
LOSS train 0.13566844020101984 valid 0.1878464831879326
LOSS train 0.13566844020101984 valid 0.18788140621461
LOSS train 0.13566844020101984 valid 0.18783743178786397
LOSS train 0.13566844020101984 valid 0.18785226670573058
LOSS train 0.13566844020101984 valid 0.1878043606156303
LOSS train 0.13566844020101984 valid 0.1876955806394482
LOSS train 0.13566844020101984 valid 0.18782150571067363
LOSS train 0.13566844020101984 valid 0.18791856535802634
LOSS train 0.13566844020101984 valid 0.18798749269858286
LOSS train 0.13566844020101984 valid 0.18791443982294628
LOSS train 0.13566844020101984 valid 0.18789227304485026
LOSS train 0.13566844020101984 valid 0.18801241692402385
LOSS train 0.13566844020101984 valid 0.1881386266330128
LOSS train 0.13566844020101984 valid 0.18819533071267566
LOSS train 0.13566844020101984 valid 0.18811480852309614
LOSS train 0.13566844020101984 valid 0.18822263759151797
LOSS train 0.13566844020101984 valid 0.1882188975209405
LOSS train 0.13566844020101984 valid 0.18819077093811595
LOSS train 0.13566844020101984 valid 0.1882839846988151
LOSS train 0.13566844020101984 valid 0.18826146504053703
LOSS train 0.13566844020101984 valid 0.1884018645558986
LOSS train 0.13566844020101984 valid 0.18850183566684753
LOSS train 0.13566844020101984 valid 0.1883783990789841
LOSS train 0.13566844020101984 valid 0.18855543282890755
LOSS train 0.13566844020101984 valid 0.18846382157820643
LOSS train 0.13566844020101984 valid 0.18834676845646336
LOSS train 0.13566844020101984 valid 0.18828433740839182
LOSS train 0.13566844020101984 valid 0.18829283563492893
LOSS train 0.13566844020101984 valid 0.18839003170232574
LOSS train 0.13566844020101984 valid 0.18838772179920282
LOSS train 0.13566844020101984 valid 0.18849723619808043
LOSS train 0.13566844020101984 valid 0.18858675333707198
LOSS train 0.13566844020101984 valid 0.18857049697337772
LOSS train 0.13566844020101984 valid 0.1885893496747917
LOSS train 0.13566844020101984 valid 0.18855930446263622
LOSS train 0.13566844020101984 valid 0.18845298927812631
LOSS train 0.13566844020101984 valid 0.18839990332374099
LOSS train 0.13566844020101984 valid 0.18841004551934085
LOSS train 0.13566844020101984 valid 0.18869858735435924
LOSS train 0.13566844020101984 valid 0.18875510418328686
LOSS train 0.13566844020101984 valid 0.18878041356371317
LOSS train 0.13566844020101984 valid 0.1886881435510061
LOSS train 0.13566844020101984 valid 0.18861503336021954
LOSS train 0.13566844020101984 valid 0.1886291091669255
LOSS train 0.13566844020101984 valid 0.18854515890989984
LOSS train 0.13566844020101984 valid 0.18847859137163542
LOSS train 0.13566844020101984 valid 0.18849218672734092
LOSS train 0.13566844020101984 valid 0.18845155664189342
LOSS train 0.13566844020101984 valid 0.18842261084644807
LOSS train 0.13566844020101984 valid 0.18853631475022142
LOSS train 0.13566844020101984 valid 0.18853540410905073
LOSS train 0.13566844020101984 valid 0.18857889342792228
LOSS train 0.13566844020101984 valid 0.18849439413271138
LOSS train 0.13566844020101984 valid 0.18848561516296233
LOSS train 0.13566844020101984 valid 0.18841756451874972
LOSS train 0.13566844020101984 valid 0.18843122286892333
LOSS train 0.13566844020101984 valid 0.18852239153579453
LOSS train 0.13566844020101984 valid 0.18837776297790618
LOSS train 0.13566844020101984 valid 0.18840232608164406
LOSS train 0.13566844020101984 valid 0.18841687720524122
LOSS train 0.13566844020101984 valid 0.18840229016591292
LOSS train 0.13566844020101984 valid 0.1882808278505094
LOSS train 0.13566844020101984 valid 0.1882718050568972
LOSS train 0.13566844020101984 valid 0.18830606973348918
EPOCH 12:
  batch 1 loss: 0.12897330522537231
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.12816248834133148
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.12566735843817392
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.13475748524069786
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.14022941291332244
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.14010396848122278
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.13078543969563075
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.13035555370151997
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.12949846105443108
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.12778460681438447
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.12750260599634863
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.1267785970121622
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.12776694676050773
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.12656344526580401
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.12685293704271317
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.12834903365001082
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.1268759922069662
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.12870528631740147
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.12857273691578916
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.12802644819021225
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.1284447411696116
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.12877217002890326
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.12882774485194165
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.13088473491370678
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.13043556064367295
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.13062278897716448
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.13129539649795602
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.13101249800196715
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.13062027841806412
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.13105433806777
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.13155286951411155
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.1313232455868274
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.13102168747873016
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.13201641159899094
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.1331584849527904
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.13326499818099868
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.1330723553090482
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.1325602053027404
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.13338204912650278
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.13309149779379367
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.1332793977202439
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.13340581421341216
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.13399752627971562
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.13341330296613954
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.13434764047463735
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.13462873418693957
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.1343479445005985
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.13392603350803256
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.13384730064747286
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.1339068342745304
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.1338356503961133
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.13436044021867788
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.13449198780757077
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.13477919209334585
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.1352010348981077
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.13528217042663268
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.13543533429242016
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.13536478437740226
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.13605000672198958
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.13606913449863592
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.13587722292200463
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.13577768362818227
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.13608449283573362
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.1359483840642497
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.13598145326742758
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.13597171065030675
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.13575941891367757
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.13590967709965565
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.13617118253656055
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.13632312640547753
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.13658392565770888
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.13653118877361217
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.13678501413701333
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.1366200565889075
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.13642725904782613
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.13648034985128202
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.13627754229229766
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.13580797574458978
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.135922198242779
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.13595840465277434
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.13617860553441224
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.1358119170295029
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.1356903240084648
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.136275403645067
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.1363127902150154
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.13657200327792832
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.1366674923177423
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.13675141935660082
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.13720547843180345
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.137308200283183
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.13723842627727068
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.1371680074573859
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.13736134743498218
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.13751312147112602
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.13784926102349632
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.13787221419624984
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.13763013505136845
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.13758894580663467
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.13773622348754092
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.13755984455347062
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.13758168981807067
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.1374334860227856
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.1373144491638952
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.13750782673462078
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.1370416248838107
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.13697590306401253
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.1370915693239631
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.13691103292836082
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.13720174954025025
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.13761955391276967
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.13749314690226908
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.13719760832775915
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.13687452647538312
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.13716413734251992
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.1369391415430152
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.1368409206384215
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.13703655063087106
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.13691223223330612
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.136781842893913
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.13694052894910178
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.13673603479280944
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.13676565781724256
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.13693188682077376
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.1368600312139719
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.13685659569501876
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.13684162728133656
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.13696550882942093
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.13683716231025755
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.136895586238351
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.13702406550829227
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.13673609682849344
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.13660589396727807
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.1368743715093548
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.13672152684250874
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.13648426714870665
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.1364293853916666
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.13659779156429053
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.13653316667330437
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.13666679034773394
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.13686734092022693
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.1367739402550332
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.13676265307085614
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.13674125893341077
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.13691817042935225
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.1366812023109403
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.13667854263562046
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.13657499728154163
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.1364933008177055
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.13668401404315192
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.13672644133369127
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.13649646755283243
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.13649386780238465
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.13637025123522953
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.13613818310104409
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.13621573231873974
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.13612036693554658
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.13615066097800138
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.13600521379067929
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.13620289452608275
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.1363098609726876
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.1361879201501793
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.13616580235553377
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.13618891366237512
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.136242812135961
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.13635071204467253
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.13629893998963288
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.13598997119122636
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.13594404612446115
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.13583509914973785
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.13597921860568662
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.13584219433410824
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.13584158708189809
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.13578804862292515
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.13568906845717593
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.1357553575720106
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.13562097857621583
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.13547332887932406
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.13524483680055382
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.13530177935208687
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.13521416464613545
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.13522738428240982
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.13516288455371017
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.13493512448717337
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.13489072745584924
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.13489498979336506
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.13489470382531485
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.13501566632546205
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.13490614905319315
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.13496804347744695
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.1351188184399354
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.13519756269704608
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.13514904553691545
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.1351995267398617
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.13500616323087633
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.1349994590649238
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.13508353055435784
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.13525029396647728
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.1354051208255267
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.13540006350332767
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.1356046538054943
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.13562719197712134
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.13569057541023385
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.13560302816147873
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.13566529995524415
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.13550618516235816
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.13538353759976265
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.13537299517848064
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.13524564986045545
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.13518176826706343
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.13539082993354115
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.13538927923870311
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.1354692215275652
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.13547693091519003
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.13534007552210417
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.1354464244357375
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.1353180964856788
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.13539013585874013
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.1355161693186388
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.13560352726205843
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.1356529963626103
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.1357248943653042
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.13572863465896598
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.1356654800975804
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.13569797931372055
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.13557380421294107
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.13547059425474267
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.135550478022004
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.13537466336499182
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.1353549752303086
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.13556616364613824
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.1356918230588302
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.13562863429301772
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.13559224632420766
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.13553963919990084
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.13550380791755431
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.1355738653091051
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.13559252105433225
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.13554514437413015
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.13561076842341963
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.1355735246092081
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.13569484787115912
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.13561409151504847
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.13566967170424912
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.13562361422742977
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.1356597630649197
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.1356831349553616
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.13561873140967326
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.1355942480806862
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.1355200375478909
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.13551025435328484
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.13558747476909266
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.135416919690749
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.13540446116283478
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.1353645819847978
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.1354499246559891
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.1354155387962237
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.1352777525434698
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.13526040618849355
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.13519144066857555
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.1351909092699106
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.1351862310849387
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.1351905455172972
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.1351248116264325
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.13498033312234012
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.1349336644107441
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.13496485064016248
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.13497793654935636
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.13497880771200158
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.13495546227604927
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.1349158946838644
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.1349031043503557
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.1349281930255101
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.13506536892591378
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.13502032540901734
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.13496110593730754
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.1350566118253746
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.13500174325941272
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.13503681262620063
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.13500868344819675
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.13503180907240936
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.13505373561085332
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.13504494943305956
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.13504269218908183
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.13499797025406865
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.1351049498507851
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.13511849549058433
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.1350865629907269
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.13518924918025732
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.13511059363614317
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.13510406978171446
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.135049476553894
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.1350072388065188
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.1351109946547108
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.13518387459370554
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.13509190870543658
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.13505620282848138
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.13512203448549265
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.1350932540309509
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.13502958646187416
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.13505066821972528
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.13517354498075884
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.13511844290210712
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.13510167077429616
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.13507441470497533
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.13505340482367845
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.13505234895578397
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.13505023412090947
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.135017004734897
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.13498937712325249
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.13496305634898523
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.1350341521658698
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.1350936254916283
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.13508726951603692
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.135018739184946
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.13501191453801262
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.13509863339175907
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.13513960891435575
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.13510590489461738
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.13507354544434802
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.135160414269194
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.13519093426774223
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.13519551188494108
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.13517982999422232
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.13511400846879423
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.13518155444126861
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.13509062131398294
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.1349768393220158
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.13495454344353297
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.13493494252394037
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.1349963812891281
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.13504967780001573
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.13503131389348622
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.13494501473219903
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.1349580797651214
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.13504372899656866
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.13503721332000124
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.13501597668986645
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.1350201638360348
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.13493037880961523
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.13495470208718496
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.13488886732597155
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.13505764007132653
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.13503617997506617
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.1350245381120679
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.13504170665274495
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.13505555893902835
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.13510966582239878
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.1350987335003313
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.13508665785833895
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.13501054659485817
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.13497440444777833
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.13501577384092592
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.13504776763848475
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.13514302422602972
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.13515449447531094
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.13513568964567077
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.13514979242109784
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.13525155132542777
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.135222442178341
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.13518017042014335
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.1351565128383214
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.13518653929398206
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.1351184380079104
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.13503412601466364
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.13509424739096262
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.135094242770978
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.13513642060253211
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.13517012995789232
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.13513269117934917
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.13509709657849492
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.13513218143558245
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.13512071642664172
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.13513073848495535
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.13519457703765064
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.13524245750904082
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.13526127733131674
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.13523377638201498
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.13530367035320195
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.1353054082999129
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.135256760351752
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.13522506640182705
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.13518263411303466
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.1352577426959578
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.13518309344847998
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.13512246539066364
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.13503694165633132
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.13505178430052692
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.13509152451357276
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.1350866886942736
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.1350788139953063
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.1350517618800978
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.1350547517934928
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.13509215647710188
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.1350837209767799
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.13512136658912974
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.13517125679010694
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.13508393750608116
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.1351047386813104
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.1350697723583769
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.13507670706138014
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.1351031719189035
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.13508217524162572
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.13503298039545789
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.1350444019541587
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.1350107514012007
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.13493680023574478
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.1349021031793564
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.13498965185135603
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.13497320274779148
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.1350094121585532
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.13502365261896393
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.13499455814005681
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.1349683374924175
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.1350020732604651
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.13506015095725116
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.13511941727035895
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.13510434216947007
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.1350797101605737
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.1352147059715164
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.1351886647265582
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.1352401483901323
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.13525328433838502
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.13518341729333777
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.13517243174379165
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.13517700151485557
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.1351485105992203
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.13513729539841624
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.13509735069890447
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.13506381890990518
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.13504315351330956
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.13505889420028075
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.1350401395379945
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.1350024047955079
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.13498823118099967
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.13494814805600835
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.13501535304779305
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.13508511479577429
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.1350502715059067
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.13494484796950376
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.13487064206803387
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.13486505053143383
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.13480657511032546
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.1347865772099312
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.13479746263977643
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.13482748919658447
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.13486427970798562
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.13483292711127792
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.13492612919903227
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.13495248101469137
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.1349124611583021
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.13494490951035346
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.13502950018552023
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.1349821301222374
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.13494628152526947
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.13493783175945281
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.13495136967353655
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.13496166673087448
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.13496030893127992
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.13495369755494568
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.13504661830223125
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.1350180685455525
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.13503200931724532
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.1349118541821548
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.13492897686002583
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.13493772507995688
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.13491983052998655
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.1349552567625454
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.13499931453002822
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.13506766461105998
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.1351140506724094
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.13513968533771054
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.13524353242147777
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.13524353242147777 valid 0.23490417003631592
LOSS train 0.13524353242147777 valid 0.1941305249929428
LOSS train 0.13524353242147777 valid 0.1978959838549296
LOSS train 0.13524353242147777 valid 0.1836683303117752
LOSS train 0.13524353242147777 valid 0.17954129874706268
LOSS train 0.13524353242147777 valid 0.1860617995262146
LOSS train 0.13524353242147777 valid 0.19401994986193521
LOSS train 0.13524353242147777 valid 0.19335565343499184
LOSS train 0.13524353242147777 valid 0.19257979260550606
LOSS train 0.13524353242147777 valid 0.19484406411647798
LOSS train 0.13524353242147777 valid 0.19346936453472485
LOSS train 0.13524353242147777 valid 0.19126341491937637
LOSS train 0.13524353242147777 valid 0.18997898697853088
LOSS train 0.13524353242147777 valid 0.1896598967058318
LOSS train 0.13524353242147777 valid 0.18627661764621734
LOSS train 0.13524353242147777 valid 0.18752881791442633
LOSS train 0.13524353242147777 valid 0.18959430911961725
LOSS train 0.13524353242147777 valid 0.18829808135827383
LOSS train 0.13524353242147777 valid 0.19035003844060397
LOSS train 0.13524353242147777 valid 0.19160648062825203
LOSS train 0.13524353242147777 valid 0.19130249747208186
LOSS train 0.13524353242147777 valid 0.18982623314315622
LOSS train 0.13524353242147777 valid 0.18982092517873514
LOSS train 0.13524353242147777 valid 0.18955007816354433
LOSS train 0.13524353242147777 valid 0.18778834044933318
LOSS train 0.13524353242147777 valid 0.18758662446187094
LOSS train 0.13524353242147777 valid 0.18758486873573726
LOSS train 0.13524353242147777 valid 0.18793411286813871
LOSS train 0.13524353242147777 valid 0.18810491418016367
LOSS train 0.13524353242147777 valid 0.18851387004057565
LOSS train 0.13524353242147777 valid 0.18945014476776123
LOSS train 0.13524353242147777 valid 0.18875563610345125
LOSS train 0.13524353242147777 valid 0.18887268548662012
LOSS train 0.13524353242147777 valid 0.18828839896356359
LOSS train 0.13524353242147777 valid 0.1903801283666066
LOSS train 0.13524353242147777 valid 0.19018681802683407
LOSS train 0.13524353242147777 valid 0.191419182999714
LOSS train 0.13524353242147777 valid 0.1916471166830314
LOSS train 0.13524353242147777 valid 0.19092234892722887
LOSS train 0.13524353242147777 valid 0.19083573147654534
LOSS train 0.13524353242147777 valid 0.19058109165691747
LOSS train 0.13524353242147777 valid 0.1911734069387118
LOSS train 0.13524353242147777 valid 0.19081070949864942
LOSS train 0.13524353242147777 valid 0.19135811112143777
LOSS train 0.13524353242147777 valid 0.19144559270805783
LOSS train 0.13524353242147777 valid 0.19234430627978366
LOSS train 0.13524353242147777 valid 0.19267009642529995
LOSS train 0.13524353242147777 valid 0.19285888131707907
LOSS train 0.13524353242147777 valid 0.19357472840620546
LOSS train 0.13524353242147777 valid 0.1929996779561043
LOSS train 0.13524353242147777 valid 0.19348020676304312
LOSS train 0.13524353242147777 valid 0.19343463618021745
LOSS train 0.13524353242147777 valid 0.1937621587289954
LOSS train 0.13524353242147777 valid 0.19394210394885805
LOSS train 0.13524353242147777 valid 0.1939469052986665
LOSS train 0.13524353242147777 valid 0.1940450519323349
LOSS train 0.13524353242147777 valid 0.19412525834744437
LOSS train 0.13524353242147777 valid 0.19402257275992427
LOSS train 0.13524353242147777 valid 0.19458774616152552
LOSS train 0.13524353242147777 valid 0.1939916250606378
LOSS train 0.13524353242147777 valid 0.1936931329183891
LOSS train 0.13524353242147777 valid 0.19388806387301413
LOSS train 0.13524353242147777 valid 0.193534017318771
LOSS train 0.13524353242147777 valid 0.19430614914745092
LOSS train 0.13524353242147777 valid 0.19422892675949976
LOSS train 0.13524353242147777 valid 0.19397747358589462
LOSS train 0.13524353242147777 valid 0.19373260861012473
LOSS train 0.13524353242147777 valid 0.1940551303765353
LOSS train 0.13524353242147777 valid 0.1932643811771835
LOSS train 0.13524353242147777 valid 0.1936852659497942
LOSS train 0.13524353242147777 valid 0.19346545111965124
LOSS train 0.13524353242147777 valid 0.19355749007728365
LOSS train 0.13524353242147777 valid 0.19370393046777543
LOSS train 0.13524353242147777 valid 0.19349452752519297
LOSS train 0.13524353242147777 valid 0.19359185814857482
LOSS train 0.13524353242147777 valid 0.1945052484148427
LOSS train 0.13524353242147777 valid 0.19423689006210923
LOSS train 0.13524353242147777 valid 0.1941559163805766
LOSS train 0.13524353242147777 valid 0.19368882590456854
LOSS train 0.13524353242147777 valid 0.1930674882605672
LOSS train 0.13524353242147777 valid 0.19243840486915023
LOSS train 0.13524353242147777 valid 0.19284172701399502
LOSS train 0.13524353242147777 valid 0.192430334636964
LOSS train 0.13524353242147777 valid 0.19238660608728728
LOSS train 0.13524353242147777 valid 0.19153998143532697
LOSS train 0.13524353242147777 valid 0.19103724959977839
LOSS train 0.13524353242147777 valid 0.19085112316855068
LOSS train 0.13524353242147777 valid 0.19052415121008048
LOSS train 0.13524353242147777 valid 0.19073758533831392
LOSS train 0.13524353242147777 valid 0.1908405605289671
LOSS train 0.13524353242147777 valid 0.19090830870382078
LOSS train 0.13524353242147777 valid 0.19081430574474129
LOSS train 0.13524353242147777 valid 0.19070709584861673
LOSS train 0.13524353242147777 valid 0.19100169393610447
LOSS train 0.13524353242147777 valid 0.19049354132853055
LOSS train 0.13524353242147777 valid 0.19064282330994806
LOSS train 0.13524353242147777 valid 0.19056176156112828
LOSS train 0.13524353242147777 valid 0.19083571814152658
LOSS train 0.13524353242147777 valid 0.190956395382833
LOSS train 0.13524353242147777 valid 0.19099153131246566
LOSS train 0.13524353242147777 valid 0.19115452453641607
LOSS train 0.13524353242147777 valid 0.1916795789318926
LOSS train 0.13524353242147777 valid 0.1914428800923153
LOSS train 0.13524353242147777 valid 0.1914769713408672
LOSS train 0.13524353242147777 valid 0.1917208275624684
LOSS train 0.13524353242147777 valid 0.1921433078230552
LOSS train 0.13524353242147777 valid 0.1918336580568385
LOSS train 0.13524353242147777 valid 0.19187750667333603
LOSS train 0.13524353242147777 valid 0.192389672775881
LOSS train 0.13524353242147777 valid 0.19237097989429128
LOSS train 0.13524353242147777 valid 0.19229419314646506
LOSS train 0.13524353242147777 valid 0.19206378808511154
LOSS train 0.13524353242147777 valid 0.19216018348141053
LOSS train 0.13524353242147777 valid 0.19238758675361933
LOSS train 0.13524353242147777 valid 0.19238367624904798
LOSS train 0.13524353242147777 valid 0.19277276394182238
LOSS train 0.13524353242147777 valid 0.19284006278229576
LOSS train 0.13524353242147777 valid 0.19251891516022762
LOSS train 0.13524353242147777 valid 0.1923480311862561
LOSS train 0.13524353242147777 valid 0.19203382258613905
LOSS train 0.13524353242147777 valid 0.1918447178996299
LOSS train 0.13524353242147777 valid 0.19188492940586122
LOSS train 0.13524353242147777 valid 0.19199409862843955
LOSS train 0.13524353242147777 valid 0.19238162232983497
LOSS train 0.13524353242147777 valid 0.19216042172908782
LOSS train 0.13524353242147777 valid 0.19236835399790417
LOSS train 0.13524353242147777 valid 0.19219479398933922
LOSS train 0.13524353242147777 valid 0.19220943236723542
LOSS train 0.13524353242147777 valid 0.19258944401445316
LOSS train 0.13524353242147777 valid 0.19233986494632868
LOSS train 0.13524353242147777 valid 0.19212512819821598
LOSS train 0.13524353242147777 valid 0.1916864909457438
LOSS train 0.13524353242147777 valid 0.19131619623281004
LOSS train 0.13524353242147777 valid 0.19149040586468
LOSS train 0.13524353242147777 valid 0.19134522786846866
LOSS train 0.13524353242147777 valid 0.1911616176366806
LOSS train 0.13524353242147777 valid 0.19086796719662463
LOSS train 0.13524353242147777 valid 0.19073468554710996
LOSS train 0.13524353242147777 valid 0.1904805743222614
LOSS train 0.13524353242147777 valid 0.19056540695684296
LOSS train 0.13524353242147777 valid 0.19058372534758655
LOSS train 0.13524353242147777 valid 0.1905470919651045
LOSS train 0.13524353242147777 valid 0.1906061690378856
LOSS train 0.13524353242147777 valid 0.19070708793070582
LOSS train 0.13524353242147777 valid 0.19064909404721753
LOSS train 0.13524353242147777 valid 0.19067792522988908
LOSS train 0.13524353242147777 valid 0.19050780465813721
LOSS train 0.13524353242147777 valid 0.1915339861769934
LOSS train 0.13524353242147777 valid 0.19160826314215693
LOSS train 0.13524353242147777 valid 0.1913976263999939
LOSS train 0.13524353242147777 valid 0.19170843716094035
LOSS train 0.13524353242147777 valid 0.19152820541670448
LOSS train 0.13524353242147777 valid 0.19154860985045338
LOSS train 0.13524353242147777 valid 0.19153876351071641
LOSS train 0.13524353242147777 valid 0.19138981430761276
LOSS train 0.13524353242147777 valid 0.19142971120965788
LOSS train 0.13524353242147777 valid 0.19151643336198892
LOSS train 0.13524353242147777 valid 0.19149576335013668
LOSS train 0.13524353242147777 valid 0.191532553460613
LOSS train 0.13524353242147777 valid 0.19140100786462427
LOSS train 0.13524353242147777 valid 0.19132451231805434
LOSS train 0.13524353242147777 valid 0.19104930951639457
LOSS train 0.13524353242147777 valid 0.19079768831378843
LOSS train 0.13524353242147777 valid 0.19048789443402755
LOSS train 0.13524353242147777 valid 0.19045639859907554
LOSS train 0.13524353242147777 valid 0.19038658831492963
LOSS train 0.13524353242147777 valid 0.19062347603058385
LOSS train 0.13524353242147777 valid 0.19058883048239209
LOSS train 0.13524353242147777 valid 0.19075113887617573
LOSS train 0.13524353242147777 valid 0.19081150188165552
LOSS train 0.13524353242147777 valid 0.19075139572745875
LOSS train 0.13524353242147777 valid 0.19065681948911312
LOSS train 0.13524353242147777 valid 0.19070597061876618
LOSS train 0.13524353242147777 valid 0.19063668817966833
LOSS train 0.13524353242147777 valid 0.1905110377073288
LOSS train 0.13524353242147777 valid 0.19061740064485508
LOSS train 0.13524353242147777 valid 0.19058687042045055
LOSS train 0.13524353242147777 valid 0.19068031473441072
LOSS train 0.13524353242147777 valid 0.1905457915207527
LOSS train 0.13524353242147777 valid 0.19050537289844618
LOSS train 0.13524353242147777 valid 0.1905790885179741
LOSS train 0.13524353242147777 valid 0.19048484521252768
LOSS train 0.13524353242147777 valid 0.19047777151149478
LOSS train 0.13524353242147777 valid 0.1904912601508524
LOSS train 0.13524353242147777 valid 0.19031856325832572
LOSS train 0.13524353242147777 valid 0.19037405689877848
LOSS train 0.13524353242147777 valid 0.1902571066337473
LOSS train 0.13524353242147777 valid 0.1901695358150817
LOSS train 0.13524353242147777 valid 0.1901129630822984
LOSS train 0.13524353242147777 valid 0.19020208967359442
LOSS train 0.13524353242147777 valid 0.190136529388228
LOSS train 0.13524353242147777 valid 0.1901735421270132
LOSS train 0.13524353242147777 valid 0.18999392697539355
LOSS train 0.13524353242147777 valid 0.18979792465868683
LOSS train 0.13524353242147777 valid 0.18951489871893173
LOSS train 0.13524353242147777 valid 0.18951814462031638
LOSS train 0.13524353242147777 valid 0.18970348319123845
LOSS train 0.13524353242147777 valid 0.1896241417888439
LOSS train 0.13524353242147777 valid 0.18970613076758744
LOSS train 0.13524353242147777 valid 0.18960381768643855
LOSS train 0.13524353242147777 valid 0.1895992700288545
LOSS train 0.13524353242147777 valid 0.18959877271168302
LOSS train 0.13524353242147777 valid 0.1896676201685309
LOSS train 0.13524353242147777 valid 0.18990863384861573
LOSS train 0.13524353242147777 valid 0.18974650429516304
LOSS train 0.13524353242147777 valid 0.18978384173321491
LOSS train 0.13524353242147777 valid 0.18984369825625766
LOSS train 0.13524353242147777 valid 0.18980928388639137
LOSS train 0.13524353242147777 valid 0.18982674580726897
LOSS train 0.13524353242147777 valid 0.18990544073638463
LOSS train 0.13524353242147777 valid 0.1899039747449459
LOSS train 0.13524353242147777 valid 0.18990674896060294
LOSS train 0.13524353242147777 valid 0.189922894110702
LOSS train 0.13524353242147777 valid 0.1898333667037643
LOSS train 0.13524353242147777 valid 0.18973111371661341
LOSS train 0.13524353242147777 valid 0.18954382719541038
LOSS train 0.13524353242147777 valid 0.18932318433363865
LOSS train 0.13524353242147777 valid 0.18928780673293893
LOSS train 0.13524353242147777 valid 0.18940858536114977
LOSS train 0.13524353242147777 valid 0.18936288729310036
LOSS train 0.13524353242147777 valid 0.18928871684754056
LOSS train 0.13524353242147777 valid 0.18933698044972377
LOSS train 0.13524353242147777 valid 0.1893894837442535
LOSS train 0.13524353242147777 valid 0.18945821061996476
LOSS train 0.13524353242147777 valid 0.18972654706901973
LOSS train 0.13524353242147777 valid 0.18991444404937524
LOSS train 0.13524353242147777 valid 0.18999263210968825
LOSS train 0.13524353242147777 valid 0.19000863793649173
LOSS train 0.13524353242147777 valid 0.18995914494366625
LOSS train 0.13524353242147777 valid 0.1900586829237316
LOSS train 0.13524353242147777 valid 0.1901626679159346
LOSS train 0.13524353242147777 valid 0.19010907289539947
LOSS train 0.13524353242147777 valid 0.19012605587556128
LOSS train 0.13524353242147777 valid 0.19008830036872473
LOSS train 0.13524353242147777 valid 0.19020067351929684
LOSS train 0.13524353242147777 valid 0.19008279168757342
LOSS train 0.13524353242147777 valid 0.19006270440570414
LOSS train 0.13524353242147777 valid 0.18998309631808466
LOSS train 0.13524353242147777 valid 0.18987866163752568
LOSS train 0.13524353242147777 valid 0.18992503906289737
LOSS train 0.13524353242147777 valid 0.1900221597233254
LOSS train 0.13524353242147777 valid 0.18988897078786016
LOSS train 0.13524353242147777 valid 0.1901149186823103
LOSS train 0.13524353242147777 valid 0.19019853830581807
LOSS train 0.13524353242147777 valid 0.1902693395103727
LOSS train 0.13524353242147777 valid 0.19011182253195988
LOSS train 0.13524353242147777 valid 0.19019515203078266
LOSS train 0.13524353242147777 valid 0.19012223166083136
LOSS train 0.13524353242147777 valid 0.19010880667282395
LOSS train 0.13524353242147777 valid 0.19008274579048157
LOSS train 0.13524353242147777 valid 0.19001281789811958
LOSS train 0.13524353242147777 valid 0.19013134017586708
LOSS train 0.13524353242147777 valid 0.19004178477134628
LOSS train 0.13524353242147777 valid 0.19000197137434652
LOSS train 0.13524353242147777 valid 0.1899837065561145
LOSS train 0.13524353242147777 valid 0.18991137045668438
LOSS train 0.13524353242147777 valid 0.18974060881230617
LOSS train 0.13524353242147777 valid 0.18995067817989245
LOSS train 0.13524353242147777 valid 0.18997149986419898
LOSS train 0.13524353242147777 valid 0.18992207675026013
LOSS train 0.13524353242147777 valid 0.1901241387329796
LOSS train 0.13524353242147777 valid 0.19014106082324764
LOSS train 0.13524353242147777 valid 0.19020213271728487
LOSS train 0.13524353242147777 valid 0.19021814965614767
LOSS train 0.13524353242147777 valid 0.19036888946902078
LOSS train 0.13524353242147777 valid 0.1903711355158261
LOSS train 0.13524353242147777 valid 0.19042464711246418
LOSS train 0.13524353242147777 valid 0.19047380911547745
LOSS train 0.13524353242147777 valid 0.19065939610110783
LOSS train 0.13524353242147777 valid 0.1907636672810272
LOSS train 0.13524353242147777 valid 0.1909082087643472
LOSS train 0.13524353242147777 valid 0.19134081889163046
LOSS train 0.13524353242147777 valid 0.19151629491166755
LOSS train 0.13524353242147777 valid 0.19156089609991894
LOSS train 0.13524353242147777 valid 0.19151901770721783
LOSS train 0.13524353242147777 valid 0.19148873823924342
LOSS train 0.13524353242147777 valid 0.19144146773789333
LOSS train 0.13524353242147777 valid 0.1912462156560781
LOSS train 0.13524353242147777 valid 0.191255274425698
LOSS train 0.13524353242147777 valid 0.19115451175187315
LOSS train 0.13524353242147777 valid 0.19109284893685813
LOSS train 0.13524353242147777 valid 0.19086304428518241
LOSS train 0.13524353242147777 valid 0.190918873613799
LOSS train 0.13524353242147777 valid 0.1909850272284427
LOSS train 0.13524353242147777 valid 0.1909830131028828
LOSS train 0.13524353242147777 valid 0.19099687342043523
LOSS train 0.13524353242147777 valid 0.19100163093011968
LOSS train 0.13524353242147777 valid 0.1910109070336653
LOSS train 0.13524353242147777 valid 0.19098523764461794
LOSS train 0.13524353242147777 valid 0.19113613305420712
LOSS train 0.13524353242147777 valid 0.19091639957067483
LOSS train 0.13524353242147777 valid 0.1909437251519667
LOSS train 0.13524353242147777 valid 0.1909742087329207
LOSS train 0.13524353242147777 valid 0.1911141525219087
LOSS train 0.13524353242147777 valid 0.19126170767565906
LOSS train 0.13524353242147777 valid 0.1911485968409358
LOSS train 0.13524353242147777 valid 0.19114521863283934
LOSS train 0.13524353242147777 valid 0.19110273674830494
LOSS train 0.13524353242147777 valid 0.1911057397274668
LOSS train 0.13524353242147777 valid 0.1911405282219251
LOSS train 0.13524353242147777 valid 0.19117299276728963
LOSS train 0.13524353242147777 valid 0.19110980940772998
LOSS train 0.13524353242147777 valid 0.1911760308463188
LOSS train 0.13524353242147777 valid 0.19127143512627012
LOSS train 0.13524353242147777 valid 0.19116932471267511
LOSS train 0.13524353242147777 valid 0.19115902624683442
LOSS train 0.13524353242147777 valid 0.19119411114761029
LOSS train 0.13524353242147777 valid 0.19115009753928555
LOSS train 0.13524353242147777 valid 0.19116240455301833
LOSS train 0.13524353242147777 valid 0.19111504444191532
LOSS train 0.13524353242147777 valid 0.19100412894100238
LOSS train 0.13524353242147777 valid 0.19113043557183865
LOSS train 0.13524353242147777 valid 0.19123083433975427
LOSS train 0.13524353242147777 valid 0.19130123511051675
LOSS train 0.13524353242147777 valid 0.19122599637697613
LOSS train 0.13524353242147777 valid 0.19120619012207923
LOSS train 0.13524353242147777 valid 0.1913327475075466
LOSS train 0.13524353242147777 valid 0.1914621242468462
LOSS train 0.13524353242147777 valid 0.19152096390350484
LOSS train 0.13524353242147777 valid 0.19144094022922217
LOSS train 0.13524353242147777 valid 0.19155158455312438
LOSS train 0.13524353242147777 valid 0.19154844326632364
LOSS train 0.13524353242147777 valid 0.19151768863385676
LOSS train 0.13524353242147777 valid 0.19161640852689743
LOSS train 0.13524353242147777 valid 0.19159276054455684
LOSS train 0.13524353242147777 valid 0.1917371610922316
LOSS train 0.13524353242147777 valid 0.19183619890008863
LOSS train 0.13524353242147777 valid 0.19170809469026764
LOSS train 0.13524353242147777 valid 0.19189000378807264
LOSS train 0.13524353242147777 valid 0.191800019280477
LOSS train 0.13524353242147777 valid 0.19167993692653057
LOSS train 0.13524353242147777 valid 0.19161565229296684
LOSS train 0.13524353242147777 valid 0.19162405391892157
LOSS train 0.13524353242147777 valid 0.19171983231149034
LOSS train 0.13524353242147777 valid 0.1917204139837578
LOSS train 0.13524353242147777 valid 0.19183459501003935
LOSS train 0.13524353242147777 valid 0.19192597150625743
LOSS train 0.13524353242147777 valid 0.19191181329051418
LOSS train 0.13524353242147777 valid 0.19193315171914116
LOSS train 0.13524353242147777 valid 0.19190313071012496
LOSS train 0.13524353242147777 valid 0.1917939422067659
LOSS train 0.13524353242147777 valid 0.1917395753818646
LOSS train 0.13524353242147777 valid 0.1917509902757389
LOSS train 0.13524353242147777 valid 0.1920456725584213
LOSS train 0.13524353242147777 valid 0.19210467429264733
LOSS train 0.13524353242147777 valid 0.1921327680435484
LOSS train 0.13524353242147777 valid 0.19204137485034184
LOSS train 0.13524353242147777 valid 0.19196550963425088
LOSS train 0.13524353242147777 valid 0.1919794779025381
LOSS train 0.13524353242147777 valid 0.19189279952219554
LOSS train 0.13524353242147777 valid 0.19182540005088872
LOSS train 0.13524353242147777 valid 0.19183916666290976
LOSS train 0.13524353242147777 valid 0.19179840056653064
LOSS train 0.13524353242147777 valid 0.19176878443377166
LOSS train 0.13524353242147777 valid 0.19188379876210657
LOSS train 0.13524353242147777 valid 0.19188181289963507
LOSS train 0.13524353242147777 valid 0.19192856266385033
LOSS train 0.13524353242147777 valid 0.1918397104606948
LOSS train 0.13524353242147777 valid 0.1918317217763752
LOSS train 0.13524353242147777 valid 0.1917610328644514
LOSS train 0.13524353242147777 valid 0.19177581683585518
LOSS train 0.13524353242147777 valid 0.19186660820278673
LOSS train 0.13524353242147777 valid 0.19172218310603098
LOSS train 0.13524353242147777 valid 0.1917487158552631
LOSS train 0.13524353242147777 valid 0.1917640983650129
LOSS train 0.13524353242147777 valid 0.191750466538583
LOSS train 0.13524353242147777 valid 0.191627653604957
LOSS train 0.13524353242147777 valid 0.19161846954375505
LOSS train 0.13524353242147777 valid 0.19165425034880962
EPOCH 13:
  batch 1 loss: 0.12585997581481934
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.12553304433822632
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.12511330594619116
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.1339475978165865
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.13975935131311418
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.1397055151561896
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.1306659664426531
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.13108752481639385
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.129432855380906
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.12699431777000428
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.12653105367313733
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.1253609135746956
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.126823012645428
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.1260386263685567
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.12594175785779954
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.12714388640597463
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.12584301478722515
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.1279922624429067
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.1276654393265122
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.12709911949932576
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.1273357275696028
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.1276935592971065
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.1274796999667002
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.1294219298288226
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.12913528233766555
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.1295105519776161
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.13020067220484768
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.12989969471735613
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.1297272800885398
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.13028828129172326
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.13079804254155006
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.1305921683087945
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.1303306847359195
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.13129905326401486
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.1323466541511672
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.1324650514870882
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.1325787710579666
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.13219381849232473
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.13295180923663652
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.13252084497362376
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.13271597191327955
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.13297352673751967
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.13362289254748544
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.1329856323586269
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.1338667172524664
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.1341979657502278
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.13390217214188677
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.13343613998343548
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.13325576408177006
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.13331899121403695
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.13329297727813907
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.1337549197845734
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.13391140932744405
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.13417735323309898
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.13455367183143443
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.13475337345153093
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.13493683136868895
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.13494742828710327
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.13554740343558586
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.13557492059965928
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.13530919639790645
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.13526657704384096
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.13556167530635047
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.13543576910160482
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.13543989933454073
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.1355042839140603
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.13526884010478632
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.13547231958192937
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.13572360359240268
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.1359226045863969
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.1362090937688317
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.13616404413349098
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.13647160770958416
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.13632075408020536
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.136089443564415
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.1361524892088614
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.13590114892690214
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.1353898911904066
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.135462792425216
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.13550236653536557
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.1358010557330685
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.1354484673498607
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.13528031245412597
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.13590239192403497
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.13598546447122797
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.136235382581173
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.136299644650399
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.1363717887381261
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.13684853149598905
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.13694045320153236
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.13693460994041884
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.13692950028116288
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.13713197610391084
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.1372982644812858
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.13758747867847743
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.1376149799519529
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.13737777397804654
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.1373334592398332
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.13751348175785758
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.13731066808104514
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.137322206308346
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.1372155157082221
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.13706979618489162
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.13729543468126884
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.13683904792581286
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.1367722914185164
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.13693284904845407
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.13673785535825622
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.13701686604854163
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.13749908141114495
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.13739849090039194
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.13711940610249126
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.1367703801499004
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.13703416863031553
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.13676331820695298
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.13664965068214927
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.13684243492336354
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.13673253609972486
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.13656367543114334
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.1367358066762487
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.13656373032607322
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.1365965055759813
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.13675629868497693
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.1366685619878192
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.13664448815584182
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.13664022342316687
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.13677493365496163
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.13662690302589908
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.136720809421336
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.13686556569658792
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.13657353499918493
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.13641037639569153
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.1366455120811785
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.13649333090479696
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.13625905960798262
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.13623049017041922
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.13641801612438076
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.13631284598639046
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.13644831423922407
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.1366175851119416
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.13650838856367356
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.13650145554836368
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.1364391945771404
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.13664193911891845
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.13641369080749052
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.13636928944759172
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.13626479463917868
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.13621466486035166
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.13645860812808044
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.13651677558819453
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.1362737910143587
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.13625028165743538
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.1360743655001416
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.135848866170877
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.135926767414616
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.13579821701233202
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.13580056028381274
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.13566941442557529
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.13583077241977057
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.13597740777768194
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.135877338394245
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.13586797818173596
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.13587512363868257
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.13596211396521185
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.1360549677050475
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.13600601556609912
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.13569724778393785
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.13561924937225522
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.13547501911425733
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.1355925869415788
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.13547278775109184
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.1354482338525528
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.1353712760644152
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.13525053529047418
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.1353261151909828
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.1351793501102789
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.13502118744924244
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.13481335463316252
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.13487577017958605
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.1348058237797684
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.13483130800131277
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.13474550687677259
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.13450826085656067
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.13448235392570496
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.1344874927320996
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.13449637840191522
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.13461466069846229
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.13445906372780494
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.1345029599452145
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.13466227321248306
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.13472072998578635
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.13466848467942327
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.13471053289780344
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.13452222870336367
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.13447756006931647
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.13457033216801226
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.13473017351125097
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.13487273255231405
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.134868348401096
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.13505712267011405
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.1350626537381713
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.13508473546935781
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.13501286062554185
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.13509040614407436
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.13491831301915935
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.13479140798733072
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.13475832220724815
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.13460665425428978
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.13458316577108284
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.13481747699635369
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.13481371853306395
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.1348855878525185
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.13488813856957663
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.13472048322034774
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.13481296560792036
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.13470136981319497
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.13477993897304008
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.13495209942990488
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.13507019582132226
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.13511838818138297
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.1351624335503686
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.13517897777460716
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.13509288637360115
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.1351362272564854
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.13501570569144355
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.1349094397330706
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.1349964073576066
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.13481846948464712
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.13482386407373254
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.1350437571173129
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.13516869554013916
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.1351030844681222
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.13509555212162083
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.13504891046601483
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.13500990258886458
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.1351094112684161
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.13511594004520383
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.13508017968480326
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.1351517922708679
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.13510225927457215
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.13521992371908362
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.135145528565261
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.13519415903238602
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.13517372786510187
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.1352151797134049
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.13524484701030623
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.13520703353138588
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.13520638428388104
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.1351265842955275
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.1351055757701397
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.13519096591201912
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.13500674199017268
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.13502195218335028
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.13496375834847998
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.13504429412823096
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.13500527915311977
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.1348793370946372
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.1348507741624995
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.13476971137017357
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.13475810369619956
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.13477030744726173
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.13476880071044878
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.13469669508956683
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.13454267066536527
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.13450104293395887
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.1345344833693558
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.13453758424252607
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.13452640089637308
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.13449837297865894
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.13445645474173404
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.13442509912806683
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.13444808046059573
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.1346051416499711
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.13452768529744913
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.13445154826749456
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.13455127166125222
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.1344969388541332
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.13452529207729608
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.13451501815229333
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.13454750986503702
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.13458416356416783
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.13457849080152545
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.13458022019783095
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.13453467221746981
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.13465361537640555
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.134670655210535
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.134631542534363
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.13473001717486316
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.13466443026045202
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.13468377859941844
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.13462844990270653
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.13460152588580568
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.13472006714079568
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.1347944519805665
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.13471193689916094
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.13468953420886318
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.13474807426684635
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.134725114518164
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.1346721155288626
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.13466282561421394
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.1347823184590007
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.13469904155427256
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.13469068536368928
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.13467159693276412
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.13466718150455442
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.13465680905124722
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.13465975494357585
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.1346417854890808
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.13460016339824424
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.1345798652739294
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.13463396234048525
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.1346882024111274
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.13466854624378796
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.13460845417182915
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.13461353497372733
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.13469877011508127
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.13474875850726378
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.13470562341265707
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.13466939946700787
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.13475104933604598
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.1347772907059512
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.13480538287147972
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.13478113378718173
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.13471680985372744
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.13477851468783159
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.13468549626827972
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.13456390697839427
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.13454127979532973
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.1345373715492005
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.13459888082562071
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.13464213191922697
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.1346246890095343
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.1345431833295851
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.13456310742272587
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.1346269742766423
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.1346204617459859
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.13460464085775595
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.13459553596007048
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.13451441559819696
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.13453773794805302
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.13447757041262043
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.1346382905037431
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.13461845854715426
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.13460375128183946
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.13463454157977864
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.1346396690805179
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.13468882791504735
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.1346644351323103
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.1346470888031588
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.1345720761375768
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.13452902642155645
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.13457171587188813
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.13459593992648652
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.13468714094178824
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.13470346241349906
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.13469313371800976
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.1347233089370554
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.1348112696500797
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.1347755523486722
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.13473366954260402
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.13471189133018008
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.13473266338445863
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.13467464440042978
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.13459164190750855
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.1346461004590335
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.13465075593828504
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.13469298041808833
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.13472439389189947
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.13470217734979098
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.1346624139192942
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.13469661623641166
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.13469593282989276
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.13469062701626694
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.1347591128339742
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.1348071024020513
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.13483076399945199
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.13479531933837607
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.13486703197476724
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.1348681004427983
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.13483458790732059
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.1347996604059312
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.1347648378715153
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.13485928255257346
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.13477945756555224
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.1347330072096416
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.13463690687800936
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.13465372066781195
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.13469365158492758
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.13468269919521705
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.1347042259115439
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.1346707924476365
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.13467198587497886
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.13472124191034235
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.13470576253487979
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.1347553046821039
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.13479622065870447
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.1347059986040334
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.13473316501058527
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.1347042807100112
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.13469996927306055
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.13473146104084285
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.13470780699333149
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.13465907354434725
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.13466823386895185
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.13463707003328534
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.1345718740279158
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.13453169769312298
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.13461788099113048
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.1345990671614563
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.13462165774005216
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.13463341542621599
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.13459871515848684
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.13456008886381732
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.1346084322006518
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.13467400833425752
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.13474345449000025
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.13473489464258404
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.13471103562977896
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.13485569551338158
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.13482777683862618
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.13488222968337654
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.13491276289685078
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.13484874596691582
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.1348366428447782
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.13483669670189127
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.13479891674759242
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.13477495630256464
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.1347284672137733
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.1346978202010646
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.1346775658948477
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.1346857381461668
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.1346819551523637
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.13463335262297482
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.1346155648132623
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.13456969751023698
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.13463684003971038
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.1347166393250568
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.13468770545027028
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.13458355090205382
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.13451480133966967
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.13451024960903896
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.13445534166032913
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.13442798665226446
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.13440836523030256
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.13442608787772362
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.13447268106744964
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.13444795876208035
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.1345397682827232
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.13457235801790765
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.13452602023879687
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.13455506833786446
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.13462040937645772
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.1345690753779664
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.13451066586939775
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.13448655408817334
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.13449933602098832
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.13451478113491636
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.1345110310477461
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.13451741408548584
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.13460954087583915
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.13458039871782645
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.13458739627491345
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.13447209014748393
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.13449467824579314
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.13451441680231402
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.13450476680702406
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.1345358024764112
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.13459550083065644
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.13467952421606222
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.134725573437011
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.13475694992992782
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.13485906528845681
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.13485906528845681 valid 0.23337441682815552
LOSS train 0.13485906528845681 valid 0.19256963580846786
LOSS train 0.13485906528845681 valid 0.1961058328549067
LOSS train 0.13485906528845681 valid 0.18187331408262253
LOSS train 0.13485906528845681 valid 0.1777355194091797
LOSS train 0.13485906528845681 valid 0.18414626022179922
LOSS train 0.13485906528845681 valid 0.19198318890162877
LOSS train 0.13485906528845681 valid 0.1913682520389557
LOSS train 0.13485906528845681 valid 0.1906114402744505
LOSS train 0.13485906528845681 valid 0.19274804145097732
LOSS train 0.13485906528845681 valid 0.19131832366639917
LOSS train 0.13485906528845681 valid 0.1891795260210832
LOSS train 0.13485906528845681 valid 0.18791273694771987
LOSS train 0.13485906528845681 valid 0.18758286003555572
LOSS train 0.13485906528845681 valid 0.18428511520226795
LOSS train 0.13485906528845681 valid 0.18547749053686857
LOSS train 0.13485906528845681 valid 0.1875210728715448
LOSS train 0.13485906528845681 valid 0.18620620419581732
LOSS train 0.13485906528845681 valid 0.1882104991297973
LOSS train 0.13485906528845681 valid 0.18943123370409012
LOSS train 0.13485906528845681 valid 0.18910748405115946
LOSS train 0.13485906528845681 valid 0.18761314790357242
LOSS train 0.13485906528845681 valid 0.1876209468945213
LOSS train 0.13485906528845681 valid 0.18733691858748594
LOSS train 0.13485906528845681 valid 0.18560195744037628
LOSS train 0.13485906528845681 valid 0.18541322247340128
LOSS train 0.13485906528845681 valid 0.18542181341736405
LOSS train 0.13485906528845681 valid 0.18572597205638885
LOSS train 0.13485906528845681 valid 0.18587945093368663
LOSS train 0.13485906528845681 valid 0.18626782447099685
LOSS train 0.13485906528845681 valid 0.18720986766199912
LOSS train 0.13485906528845681 valid 0.18653961643576622
LOSS train 0.13485906528845681 valid 0.18664048386342597
LOSS train 0.13485906528845681 valid 0.18605218082666397
LOSS train 0.13485906528845681 valid 0.18811766122068677
LOSS train 0.13485906528845681 valid 0.187909667690595
LOSS train 0.13485906528845681 valid 0.1891346364407926
LOSS train 0.13485906528845681 valid 0.1893565642990564
LOSS train 0.13485906528845681 valid 0.18866324539367968
LOSS train 0.13485906528845681 valid 0.18857530355453492
LOSS train 0.13485906528845681 valid 0.18833023854872075
LOSS train 0.13485906528845681 valid 0.18890216982080824
LOSS train 0.13485906528845681 valid 0.18855631836625034
LOSS train 0.13485906528845681 valid 0.18910947171124545
LOSS train 0.13485906528845681 valid 0.1892166558239195
LOSS train 0.13485906528845681 valid 0.19011733162662256
LOSS train 0.13485906528845681 valid 0.19044566439821364
LOSS train 0.13485906528845681 valid 0.19063198690613112
LOSS train 0.13485906528845681 valid 0.19131971986926333
LOSS train 0.13485906528845681 valid 0.19076353788375855
LOSS train 0.13485906528845681 valid 0.19124765694141388
LOSS train 0.13485906528845681 valid 0.19119885506538245
LOSS train 0.13485906528845681 valid 0.19151336938705085
LOSS train 0.13485906528845681 valid 0.19168193169214107
LOSS train 0.13485906528845681 valid 0.1916846576062116
LOSS train 0.13485906528845681 valid 0.19178736342915467
LOSS train 0.13485906528845681 valid 0.1918766158714629
LOSS train 0.13485906528845681 valid 0.1917732059955597
LOSS train 0.13485906528845681 valid 0.19233367301649967
LOSS train 0.13485906528845681 valid 0.19173584431409835
LOSS train 0.13485906528845681 valid 0.19144865308628709
LOSS train 0.13485906528845681 valid 0.19164969868236972
LOSS train 0.13485906528845681 valid 0.19129290088774667
LOSS train 0.13485906528845681 valid 0.1920549883507192
LOSS train 0.13485906528845681 valid 0.191967151256708
LOSS train 0.13485906528845681 valid 0.19170651697751248
LOSS train 0.13485906528845681 valid 0.1914725781821493
LOSS train 0.13485906528845681 valid 0.19179270920508049
LOSS train 0.13485906528845681 valid 0.19102114244647647
LOSS train 0.13485906528845681 valid 0.19144789342369353
LOSS train 0.13485906528845681 valid 0.1912252774960558
LOSS train 0.13485906528845681 valid 0.19131085876789358
LOSS train 0.13485906528845681 valid 0.1914514954367729
LOSS train 0.13485906528845681 valid 0.1912419719470514
LOSS train 0.13485906528845681 valid 0.19134425044059752
LOSS train 0.13485906528845681 valid 0.19225015687315086
LOSS train 0.13485906528845681 valid 0.19198965639263005
LOSS train 0.13485906528845681 valid 0.1919053764297412
LOSS train 0.13485906528845681 valid 0.1914466834520992
LOSS train 0.13485906528845681 valid 0.19082996789366008
LOSS train 0.13485906528845681 valid 0.19020755625801322
LOSS train 0.13485906528845681 valid 0.1906058470286974
LOSS train 0.13485906528845681 valid 0.19020005306565618
LOSS train 0.13485906528845681 valid 0.19016262037413462
LOSS train 0.13485906528845681 valid 0.18931853569605772
LOSS train 0.13485906528845681 valid 0.18882275953195815
LOSS train 0.13485906528845681 valid 0.1886352770801248
LOSS train 0.13485906528845681 valid 0.18831454395231875
LOSS train 0.13485906528845681 valid 0.18852357055698887
LOSS train 0.13485906528845681 valid 0.18862084589070743
LOSS train 0.13485906528845681 valid 0.18869001680350567
LOSS train 0.13485906528845681 valid 0.18860018180440302
LOSS train 0.13485906528845681 valid 0.18849153968916144
LOSS train 0.13485906528845681 valid 0.1887763631629183
LOSS train 0.13485906528845681 valid 0.18827306071394367
LOSS train 0.13485906528845681 valid 0.18842060111152628
LOSS train 0.13485906528845681 valid 0.18833480567968996
LOSS train 0.13485906528845681 valid 0.18861199086722064
LOSS train 0.13485906528845681 valid 0.188728378803441
LOSS train 0.13485906528845681 valid 0.18876575164496898
LOSS train 0.13485906528845681 valid 0.18892238362885938
LOSS train 0.13485906528845681 valid 0.18944250185992204
LOSS train 0.13485906528845681 valid 0.18920117913229953
LOSS train 0.13485906528845681 valid 0.18924024378737578
LOSS train 0.13485906528845681 valid 0.1894873599920954
LOSS train 0.13485906528845681 valid 0.18990765814230126
LOSS train 0.13485906528845681 valid 0.1896004031333968
LOSS train 0.13485906528845681 valid 0.18963699546401147
LOSS train 0.13485906528845681 valid 0.19013349978475397
LOSS train 0.13485906528845681 valid 0.19011368744752624
LOSS train 0.13485906528845681 valid 0.19004331858039977
LOSS train 0.13485906528845681 valid 0.1898136460222304
LOSS train 0.13485906528845681 valid 0.18990634981773596
LOSS train 0.13485906528845681 valid 0.19013783747428342
LOSS train 0.13485906528845681 valid 0.190138046702613
LOSS train 0.13485906528845681 valid 0.19052613340318203
LOSS train 0.13485906528845681 valid 0.19058507114139378
LOSS train 0.13485906528845681 valid 0.1902651992010868
LOSS train 0.13485906528845681 valid 0.19009933094767964
LOSS train 0.13485906528845681 valid 0.18979104353735846
LOSS train 0.13485906528845681 valid 0.18959876076002752
LOSS train 0.13485906528845681 valid 0.1896396510791583
LOSS train 0.13485906528845681 valid 0.18973731419177559
LOSS train 0.13485906528845681 valid 0.1901236905325805
LOSS train 0.13485906528845681 valid 0.18990824121236802
LOSS train 0.13485906528845681 valid 0.19011231127475936
LOSS train 0.13485906528845681 valid 0.18993694851482948
LOSS train 0.13485906528845681 valid 0.1899573394912295
LOSS train 0.13485906528845681 valid 0.19032837629549262
LOSS train 0.13485906528845681 valid 0.19007309795572208
LOSS train 0.13485906528845681 valid 0.18985790929949012
LOSS train 0.13485906528845681 valid 0.18942622326766
LOSS train 0.13485906528845681 valid 0.18905627856353172
LOSS train 0.13485906528845681 valid 0.1892316311931432
LOSS train 0.13485906528845681 valid 0.1890898707288283
LOSS train 0.13485906528845681 valid 0.18890693482449827
LOSS train 0.13485906528845681 valid 0.1886148727396979
LOSS train 0.13485906528845681 valid 0.18848487362265587
LOSS train 0.13485906528845681 valid 0.18823571755088492
LOSS train 0.13485906528845681 valid 0.1883209865540266
LOSS train 0.13485906528845681 valid 0.18833861193547013
LOSS train 0.13485906528845681 valid 0.18830746386042782
LOSS train 0.13485906528845681 valid 0.18836254022129767
LOSS train 0.13485906528845681 valid 0.18846118714039525
LOSS train 0.13485906528845681 valid 0.18840586558498185
LOSS train 0.13485906528845681 valid 0.1884369560810801
LOSS train 0.13485906528845681 valid 0.1882613479178779
LOSS train 0.13485906528845681 valid 0.18928198773111846
LOSS train 0.13485906528845681 valid 0.1893570582638651
LOSS train 0.13485906528845681 valid 0.18914913142720857
LOSS train 0.13485906528845681 valid 0.18945641514678666
LOSS train 0.13485906528845681 valid 0.18927918889216686
LOSS train 0.13485906528845681 valid 0.1893005465078198
LOSS train 0.13485906528845681 valid 0.18929249633635795
LOSS train 0.13485906528845681 valid 0.18914405669896833
LOSS train 0.13485906528845681 valid 0.1891846196869245
LOSS train 0.13485906528845681 valid 0.18927019412160678
LOSS train 0.13485906528845681 valid 0.18924460112105443
LOSS train 0.13485906528845681 valid 0.18928193516513836
LOSS train 0.13485906528845681 valid 0.18915476971305906
LOSS train 0.13485906528845681 valid 0.189078759471452
LOSS train 0.13485906528845681 valid 0.18880955759941795
LOSS train 0.13485906528845681 valid 0.18856054375325243
LOSS train 0.13485906528845681 valid 0.18824914683837715
LOSS train 0.13485906528845681 valid 0.18821868034023226
LOSS train 0.13485906528845681 valid 0.18815205986779857
LOSS train 0.13485906528845681 valid 0.18838617477766767
LOSS train 0.13485906528845681 valid 0.18834869080178795
LOSS train 0.13485906528845681 valid 0.1885124521230805
LOSS train 0.13485906528845681 valid 0.18857032415621422
LOSS train 0.13485906528845681 valid 0.1885152832545035
LOSS train 0.13485906528845681 valid 0.18842038689830967
LOSS train 0.13485906528845681 valid 0.1884614323518869
LOSS train 0.13485906528845681 valid 0.18839071421273823
LOSS train 0.13485906528845681 valid 0.18826852598360608
LOSS train 0.13485906528845681 valid 0.18837406329641287
LOSS train 0.13485906528845681 valid 0.18834718762986405
LOSS train 0.13485906528845681 valid 0.1884460308997149
LOSS train 0.13485906528845681 valid 0.18831187349291487
LOSS train 0.13485906528845681 valid 0.18827122073206637
LOSS train 0.13485906528845681 valid 0.1883506688047509
LOSS train 0.13485906528845681 valid 0.18825749467526162
LOSS train 0.13485906528845681 valid 0.1882550010019964
LOSS train 0.13485906528845681 valid 0.18826994271544012
LOSS train 0.13485906528845681 valid 0.18810052489106718
LOSS train 0.13485906528845681 valid 0.18815489549950887
LOSS train 0.13485906528845681 valid 0.18804303771831135
LOSS train 0.13485906528845681 valid 0.18795931993171255
LOSS train 0.13485906528845681 valid 0.18790314559425628
LOSS train 0.13485906528845681 valid 0.18799054140323088
LOSS train 0.13485906528845681 valid 0.18792471309138842
LOSS train 0.13485906528845681 valid 0.18796397113086036
LOSS train 0.13485906528845681 valid 0.18778303346640088
LOSS train 0.13485906528845681 valid 0.1875883550143119
LOSS train 0.13485906528845681 valid 0.18730715883847995
LOSS train 0.13485906528845681 valid 0.18730821585928908
LOSS train 0.13485906528845681 valid 0.18749426157795232
LOSS train 0.13485906528845681 valid 0.18741718788791184
LOSS train 0.13485906528845681 valid 0.18749697510171776
LOSS train 0.13485906528845681 valid 0.18739093091338874
LOSS train 0.13485906528845681 valid 0.18738621254613744
LOSS train 0.13485906528845681 valid 0.1873904458145694
LOSS train 0.13485906528845681 valid 0.18745779737903567
LOSS train 0.13485906528845681 valid 0.18769866421672643
LOSS train 0.13485906528845681 valid 0.18753769757544123
LOSS train 0.13485906528845681 valid 0.18757491547939847
LOSS train 0.13485906528845681 valid 0.18763287750563185
LOSS train 0.13485906528845681 valid 0.1875973711721599
LOSS train 0.13485906528845681 valid 0.18761567575366875
LOSS train 0.13485906528845681 valid 0.18769185794960885
LOSS train 0.13485906528845681 valid 0.18769025763770417
LOSS train 0.13485906528845681 valid 0.1876933563078912
LOSS train 0.13485906528845681 valid 0.18770910879955605
LOSS train 0.13485906528845681 valid 0.1876211338023716
LOSS train 0.13485906528845681 valid 0.18751831293799157
LOSS train 0.13485906528845681 valid 0.18733007871304397
LOSS train 0.13485906528845681 valid 0.18711238024833565
LOSS train 0.13485906528845681 valid 0.1870768693950745
LOSS train 0.13485906528845681 valid 0.18719788627129166
LOSS train 0.13485906528845681 valid 0.18714941966940057
LOSS train 0.13485906528845681 valid 0.1870758653833316
LOSS train 0.13485906528845681 valid 0.18712452359430426
LOSS train 0.13485906528845681 valid 0.18717709093484108
LOSS train 0.13485906528845681 valid 0.1872443690829511
LOSS train 0.13485906528845681 valid 0.18751152492231793
LOSS train 0.13485906528845681 valid 0.18770026557345307
LOSS train 0.13485906528845681 valid 0.18777649324502188
LOSS train 0.13485906528845681 valid 0.18779039111707294
LOSS train 0.13485906528845681 valid 0.18774186149564892
LOSS train 0.13485906528845681 valid 0.18784278783461322
LOSS train 0.13485906528845681 valid 0.18794943395502123
LOSS train 0.13485906528845681 valid 0.18789526174679913
LOSS train 0.13485906528845681 valid 0.18791566191185186
LOSS train 0.13485906528845681 valid 0.18787721967977336
LOSS train 0.13485906528845681 valid 0.18798817475425436
LOSS train 0.13485906528845681 valid 0.18787091986228854
LOSS train 0.13485906528845681 valid 0.18784971689247382
LOSS train 0.13485906528845681 valid 0.18776960530075706
LOSS train 0.13485906528845681 valid 0.18766232018814924
LOSS train 0.13485906528845681 valid 0.18770666013782222
LOSS train 0.13485906528845681 valid 0.18780495563359675
LOSS train 0.13485906528845681 valid 0.18767500733417913
LOSS train 0.13485906528845681 valid 0.1878993490657198
LOSS train 0.13485906528845681 valid 0.1879832302754531
LOSS train 0.13485906528845681 valid 0.18805580172611744
LOSS train 0.13485906528845681 valid 0.18789592479181483
LOSS train 0.13485906528845681 valid 0.18797677842832286
LOSS train 0.13485906528845681 valid 0.18790137951051997
LOSS train 0.13485906528845681 valid 0.18788930106474214
LOSS train 0.13485906528845681 valid 0.1878625102341175
LOSS train 0.13485906528845681 valid 0.18779154955984587
LOSS train 0.13485906528845681 valid 0.1879089152706521
LOSS train 0.13485906528845681 valid 0.18782011767032117
LOSS train 0.13485906528845681 valid 0.18777962542307658
LOSS train 0.13485906528845681 valid 0.18776277458550883
LOSS train 0.13485906528845681 valid 0.18769178827642463
LOSS train 0.13485906528845681 valid 0.18751908372000498
LOSS train 0.13485906528845681 valid 0.1877261781126492
LOSS train 0.13485906528845681 valid 0.18774666188421396
LOSS train 0.13485906528845681 valid 0.18769367523491381
LOSS train 0.13485906528845681 valid 0.18789882629653046
LOSS train 0.13485906528845681 valid 0.1879189796745777
LOSS train 0.13485906528845681 valid 0.18798107563202826
LOSS train 0.13485906528845681 valid 0.1879952470066421
LOSS train 0.13485906528845681 valid 0.18814725749335198
LOSS train 0.13485906528845681 valid 0.18814929325441668
LOSS train 0.13485906528845681 valid 0.18820263983939917
LOSS train 0.13485906528845681 valid 0.18825174570639633
LOSS train 0.13485906528845681 valid 0.18843209657638046
LOSS train 0.13485906528845681 valid 0.1885373614176556
LOSS train 0.13485906528845681 valid 0.18868257009125283
LOSS train 0.13485906528845681 valid 0.1891121490420226
LOSS train 0.13485906528845681 valid 0.18928899681502645
LOSS train 0.13485906528845681 valid 0.18933754232134262
LOSS train 0.13485906528845681 valid 0.1892989718643102
LOSS train 0.13485906528845681 valid 0.18927072201841982
LOSS train 0.13485906528845681 valid 0.18922643683554893
LOSS train 0.13485906528845681 valid 0.18903587943465588
LOSS train 0.13485906528845681 valid 0.1890442993318308
LOSS train 0.13485906528845681 valid 0.18894498526517833
LOSS train 0.13485906528845681 valid 0.18888259875286517
LOSS train 0.13485906528845681 valid 0.18865358747594746
LOSS train 0.13485906528845681 valid 0.1887112204314542
LOSS train 0.13485906528845681 valid 0.18877641869787598
LOSS train 0.13485906528845681 valid 0.18877217348730355
LOSS train 0.13485906528845681 valid 0.18878649683161217
LOSS train 0.13485906528845681 valid 0.18879197945474332
LOSS train 0.13485906528845681 valid 0.1887993896121366
LOSS train 0.13485906528845681 valid 0.1887729521389651
LOSS train 0.13485906528845681 valid 0.18892277322452644
LOSS train 0.13485906528845681 valid 0.1887063252198737
LOSS train 0.13485906528845681 valid 0.1887310655302789
LOSS train 0.13485906528845681 valid 0.188758458776889
LOSS train 0.13485906528845681 valid 0.1888982302122781
LOSS train 0.13485906528845681 valid 0.18904336188304222
LOSS train 0.13485906528845681 valid 0.18893162249209913
LOSS train 0.13485906528845681 valid 0.1889279448373952
LOSS train 0.13485906528845681 valid 0.18888692620316608
LOSS train 0.13485906528845681 valid 0.1888867387653992
LOSS train 0.13485906528845681 valid 0.1889230032513539
LOSS train 0.13485906528845681 valid 0.18895600045539215
LOSS train 0.13485906528845681 valid 0.18889269097060557
LOSS train 0.13485906528845681 valid 0.1889579364224629
LOSS train 0.13485906528845681 valid 0.1890511494374981
LOSS train 0.13485906528845681 valid 0.18895023907305764
LOSS train 0.13485906528845681 valid 0.18894094591534216
LOSS train 0.13485906528845681 valid 0.18897347130391032
LOSS train 0.13485906528845681 valid 0.18893094605149388
LOSS train 0.13485906528845681 valid 0.18894185731036764
LOSS train 0.13485906528845681 valid 0.18889401720896845
LOSS train 0.13485906528845681 valid 0.18878444753754944
LOSS train 0.13485906528845681 valid 0.18890966673214465
LOSS train 0.13485906528845681 valid 0.1890113540112782
LOSS train 0.13485906528845681 valid 0.18908269544410858
LOSS train 0.13485906528845681 valid 0.18900833099134384
LOSS train 0.13485906528845681 valid 0.18899057652282564
LOSS train 0.13485906528845681 valid 0.18911777410879496
LOSS train 0.13485906528845681 valid 0.18924550124028194
LOSS train 0.13485906528845681 valid 0.18930569690503296
LOSS train 0.13485906528845681 valid 0.18922976509202272
LOSS train 0.13485906528845681 valid 0.18933902316468526
LOSS train 0.13485906528845681 valid 0.18933364589299476
LOSS train 0.13485906528845681 valid 0.18930313274864072
LOSS train 0.13485906528845681 valid 0.1894030952481208
LOSS train 0.13485906528845681 valid 0.18937996252224995
LOSS train 0.13485906528845681 valid 0.1895244337526933
LOSS train 0.13485906528845681 valid 0.18962198908267036
LOSS train 0.13485906528845681 valid 0.18949477440427717
LOSS train 0.13485906528845681 valid 0.18967516668208825
LOSS train 0.13485906528845681 valid 0.18958611061627215
LOSS train 0.13485906528845681 valid 0.1894680963677222
LOSS train 0.13485906528845681 valid 0.18940294567630234
LOSS train 0.13485906528845681 valid 0.18941080290544499
LOSS train 0.13485906528845681 valid 0.1895037726861631
LOSS train 0.13485906528845681 valid 0.18950741182067501
LOSS train 0.13485906528845681 valid 0.18962152450833292
LOSS train 0.13485906528845681 valid 0.18971259553726066
LOSS train 0.13485906528845681 valid 0.18970008143747347
LOSS train 0.13485906528845681 valid 0.1897219758601667
LOSS train 0.13485906528845681 valid 0.18969220639589954
LOSS train 0.13485906528845681 valid 0.1895839295997298
LOSS train 0.13485906528845681 valid 0.18953003353107045
LOSS train 0.13485906528845681 valid 0.18954092878902618
LOSS train 0.13485906528845681 valid 0.1898343335880443
LOSS train 0.13485906528845681 valid 0.1898910059661105
LOSS train 0.13485906528845681 valid 0.18992034111150427
LOSS train 0.13485906528845681 valid 0.1898319228882405
LOSS train 0.13485906528845681 valid 0.1897536439756895
LOSS train 0.13485906528845681 valid 0.18976654989756964
LOSS train 0.13485906528845681 valid 0.18968040685568538
LOSS train 0.13485906528845681 valid 0.18961327680070855
LOSS train 0.13485906528845681 valid 0.18962704756466503
LOSS train 0.13485906528845681 valid 0.18958750369275257
LOSS train 0.13485906528845681 valid 0.1895565424472262
LOSS train 0.13485906528845681 valid 0.18966935455379352
LOSS train 0.13485906528845681 valid 0.18966863000911943
LOSS train 0.13485906528845681 valid 0.18971523515233138
LOSS train 0.13485906528845681 valid 0.18962662069740907
LOSS train 0.13485906528845681 valid 0.189618326059457
LOSS train 0.13485906528845681 valid 0.18954756617960003
LOSS train 0.13485906528845681 valid 0.18956137361866615
LOSS train 0.13485906528845681 valid 0.18964884782117375
LOSS train 0.13485906528845681 valid 0.18950804552838166
LOSS train 0.13485906528845681 valid 0.18953487438733105
LOSS train 0.13485906528845681 valid 0.18954984564078997
LOSS train 0.13485906528845681 valid 0.18953536017263523
LOSS train 0.13485906528845681 valid 0.1894142701815844
LOSS train 0.13485906528845681 valid 0.18940509230141406
LOSS train 0.13485906528845681 valid 0.18944002803021331
EPOCH 14:
  batch 1 loss: 0.12727059423923492
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.12724362313747406
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.12535060942173004
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.1351623684167862
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.14053671956062316
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.13944611698389053
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.13090072572231293
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.13052045181393623
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.12851460526386896
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.12643889859318733
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.12662731991572815
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.12552226210633913
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.12667811490022218
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.1257153611098017
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.12608351210753124
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.12728455755859613
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.1260921332766028
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.12821398013167912
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.12805244012882835
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.12763467840850354
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.1277048797124908
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.12814877622506834
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.12819840953401898
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.13014497390637794
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.12965726971626282
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.12985182266968948
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.13066029603834506
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.13040169222014292
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.13019199669361115
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.13046566843986512
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.13100025490407022
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.13077480718493462
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.13058868727900766
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.13159267078427708
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.13265535490853445
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.1327220176657041
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.13254532298526248
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.13206501148248972
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.1328653674095105
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.1325167639181018
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.13257589627329897
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.13286499164643742
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.13350963471240776
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.13293488018892027
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.13385572069221072
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.13407569734946542
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.13364540040493011
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.1331978109665215
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.13304687108920546
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.13304321274161338
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.13303704340668285
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.1334742567000481
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.13381658874030383
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.1340800798325627
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.1344877550547773
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.13465547042765788
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.13489670228017003
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.13492416320689793
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.13540573759099184
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.13545616356035073
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.13519065643920272
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.135064662944886
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.13539004349519337
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.13523106393404305
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.13519916924146505
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.1352736125841285
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.13504442661555846
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.13529309170211062
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.1356219267067702
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.13583180010318757
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.13609106817715605
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.13613113264242807
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.13633210732512277
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.13614504542705175
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.13595948497454324
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.13605694080653943
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.13585316254334015
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.13536755100656778
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.135433495516264
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.13539874525740742
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.13556264754798678
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.13520840564515532
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.1350853328245232
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.13566622208981288
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.13578633315422955
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.13601787041786106
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.13606035281186815
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.1361832862550562
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.1366660807574733
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.13672510965002907
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.13670846525129382
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.13667563786325249
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.13684605494622262
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.1369835395128169
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.1372843397291083
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.13731065904721618
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.13703800146419978
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.13705665184831134
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.1372220271615067
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.13704357840120793
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.13705779648948424
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.13690410386405738
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.13678849994846917
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.13703144205590853
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.13656093726555507
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.1364317888499431
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.13658629504041137
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.1364268215580119
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.1367387816036513
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.13718190037391403
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.13711939893058828
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.13681437128356524
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.13644029529748763
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.13669806927965397
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.13645875622396883
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.13637122332022109
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.13654793404106402
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.1364438566616026
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.13627660744080022
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.13647894406070313
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.1362751077645081
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.13629826823951768
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.1364386907316805
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.13637184754254356
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.13634752434492112
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.13635372092563008
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.13645419366951064
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.13633741618832573
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.1363987360698308
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.1365608664086232
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.13626837906719164
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.13609858946592518
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.1363407907293255
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.13618619856772138
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.1359399359535288
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.13586372969781652
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.13608603451373805
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.1359979334300843
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.1361329093468275
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.13634156797613417
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.1362603670316385
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.13625369504304
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.13621342015433144
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.13639137066072887
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.13619245285617895
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.13616637227265802
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.13607414031312579
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.13598856168824272
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.136188298663837
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.13622913241386414
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.1360054166605141
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.1359896951502091
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.1358298568261994
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.1355912960678726
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.1356774723818225
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.13558522907969278
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.1355598364855833
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.13546109147652796
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.13561980365394796
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.13575323899276553
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.13560903808159858
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.13560173922666796
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.13561245666508295
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.13569141074833346
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.13577588078650563
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.1357239986130272
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.1353927246646253
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.1353280126516308
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.13518887978686384
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.1353081629556768
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.1351630121295215
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.13516846792988998
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.13512608512288574
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.13503030203711028
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.13507071839911597
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.13491436280310154
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.13475289347313218
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.13453151210305397
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.1345879942511713
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.13452913194066948
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.13453590610573962
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.1344425946392201
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.13419786909890305
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.13415025854888168
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.1341550274475201
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.13415556561241868
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.1342866768173993
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.13413584466151734
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.1341935588963448
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.13435032763763477
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.13440791603314314
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.1343537934978182
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.13439241047813485
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.13419769489273584
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.13418642412393522
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.1342725190429055
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.1344250482956165
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.13460391248115386
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.13459893477022947
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.13480636440217494
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.13481718805891957
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.13486545171478007
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.1347773175301223
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.13483109708656282
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.13467119565097296
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.1345304037326748
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.13451462876105655
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.13438610911655885
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.1343572187509263
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.13457832485437393
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.13457342141894932
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.13463785864834515
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.13467629855507415
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.13451108813425092
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.13458422924890073
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.1344756660293098
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.13454915600300935
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.13470516493971194
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.13479410244585716
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.13485944396392865
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.1349200800196078
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.13492045877081854
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.13485647891668995
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.1349020549096167
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.13479128426975673
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.1346880724999757
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.13478398493733176
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.13462039220489955
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.13463308157879192
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.13482637042584628
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.13498419897380845
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.1349285110060511
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.13491319855395303
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.13487618359235617
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.13482477550810956
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.1349109308951992
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.13491849324622737
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.13489107580996362
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.13494949709920204
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.13490738415469725
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.13501838152586673
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.13492981806274287
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.1349587129589952
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.13492253172348756
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.13494597338900274
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.13497807782113067
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.13495530700876646
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.13495461991237057
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.13487911532561941
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.13486699399352073
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.13496606452650284
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.13478651879325745
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.13476899142557452
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.1347286310780236
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.13483810340072594
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.13480667481780984
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.13468207033343815
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.13467160245591356
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.13458273030501075
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.13458690978586674
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.1345872469389119
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.13458472942464225
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.13451467032328304
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.13436755766584116
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.13430050496222837
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.13432508805080465
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.1343322798386495
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.13431393074344344
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.13430606878490697
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.1342752830573806
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.1342571134441893
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.1342933031401652
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.13445325858972884
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.1343952320570493
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.1343340654535727
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.134425129228528
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.1343582719564438
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.13438261337846302
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.13434850977313134
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.1343747405069215
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.13440110224209648
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.13437274013850706
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.1343791886058376
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.13435926676636012
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.1344822890402978
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.1344902879604093
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.13446141493860436
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.1345734710080756
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.1344915038058502
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.13450661044696283
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.13446613332045446
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.13442294802261542
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.13454399685098856
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.13461441534007487
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.1345294395998373
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.13449415476438967
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.1345608149363537
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.134537315633673
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.13447789192498727
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.13448688494662445
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.13462456340508602
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.13455451936121807
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.1345599725990012
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.13454007136782534
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.134539582397117
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.13449430042037777
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.13450813633222922
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.13449921221895653
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.13446255197031212
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.13444622822346225
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.13451188428992245
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.13455078980097404
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.134516793841752
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.13444225831775908
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.13445702578340257
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.13454006088874007
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.13457397430289056
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.13454743437234712
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.13451720629366215
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.13459529606625437
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.13462143709355054
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.13464016101745344
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.1346278744216305
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.1345713662650482
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.1346424793967834
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.1345501343256857
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.13442781294157746
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.13439268134988663
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.13440116155201903
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.13446474967129302
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.1345181838578328
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.13450262114315867
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.13443291804811977
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.1344665580702399
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.13453847721441467
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.1345517627806181
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.13451898380981356
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.13451351331183192
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.13442778226900243
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.1344410711351563
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.13438113842192284
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.1345500796161897
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.1345285594289574
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.13452013763923978
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.1345359987538794
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.13453290430624362
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.13458450567997154
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.1345579580500208
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.1345505328438002
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.13445805507046835
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.13441891280504373
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.13446359945969147
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.13448992734590265
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.13456812719840788
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.13459016823432815
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.13457241156295444
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.1345945203087243
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.13468818220845813
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.1346600573914629
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.13461843120555084
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.13460409050998265
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.13462172926130875
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.13455848594917083
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.134477626818877
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.13453158560681017
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.13453147381218405
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.134569436717748
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.13459850260582956
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.13456276018768146
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.1345185557732711
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.13455960403555486
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.13455057801097953
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.13454045706875523
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.13460187709586505
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.13464044241110484
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.13465978185388636
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.13462398926522434
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.13468014799728595
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.13467024278514933
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.13462639852966132
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.1345916403325524
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.1345589311881215
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.13464575698574902
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.13457274565007538
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.1345192046521546
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.13443188274694232
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.13445114090230115
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.1344823886246718
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.13448484163434463
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.1345021593456085
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.13447145344046377
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.1344806640503966
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.13451988843409463
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.13451484462180113
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.13455911440939844
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.13460617068440023
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.1345226208043639
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.13453385424823616
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.13449795643115103
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.13449426161125302
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.1345246969501574
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.13450299779796482
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.13444474274777893
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.13445379561053053
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.13441393338603738
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.13435721103780962
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.13431673374310757
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.1344052585681864
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.1343814275229764
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.13441223011511128
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.1344154720396311
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.13438200485865467
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.13435234257513906
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.13438378179058935
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.13444751869124102
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.13451167649159637
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.1345086821870838
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.13447120694808984
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.13459714338503476
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.13456779443437145
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.1346238239396213
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.13463418349906167
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.13457401475514644
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.13456225913580297
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.13456260753028534
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.1345210626126735
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.13451280518144856
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.1344632707779931
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.13443199564249086
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.13441511371801065
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.13442772101360242
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.13441232829872105
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.13436767224902094
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.13435304377760207
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.13432545045326497
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.13439200531452075
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.13447604909777913
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.13445454590941128
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.13434789362013475
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.13428032306784934
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.13426862967933387
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.13422153682797744
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.13419957830661036
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.13420450974356485
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.13422281884410409
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.13426601828280585
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.134241266921996
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.13433987978247128
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.13437380721348696
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.1343248019284672
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.13435061104149618
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.1344278525330324
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.1343759876279115
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.13433664344612198
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.13430879680992483
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.13432524394113243
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.13433943269876214
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.13434770974985377
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.1343449316675367
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.13444163230774195
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.13442256317803244
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.13443282148216193
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.1343177572600785
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.13434491924748854
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.13437152461659524
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.13435489912977033
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.1343978363422306
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.13445693093678382
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.1345393417327643
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.13458362175746166
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.1346031245627221
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.13469442777272503
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.13469442777272503 valid 0.23306402564048767
LOSS train 0.13469442777272503 valid 0.1916593685746193
LOSS train 0.13469442777272503 valid 0.19520833591620126
LOSS train 0.13469442777272503 valid 0.1807793565094471
LOSS train 0.13469442777272503 valid 0.17662256360054016
LOSS train 0.13469442777272503 valid 0.18308323621749878
LOSS train 0.13469442777272503 valid 0.19085607997008733
LOSS train 0.13469442777272503 valid 0.1902537252753973
LOSS train 0.13469442777272503 valid 0.1894998633199268
LOSS train 0.13469442777272503 valid 0.19153365790843963
LOSS train 0.13469442777272503 valid 0.19008509679274124
LOSS train 0.13469442777272503 valid 0.18796210239330927
LOSS train 0.13469442777272503 valid 0.18672386155678675
LOSS train 0.13469442777272503 valid 0.1863747558423451
LOSS train 0.13469442777272503 valid 0.1831049660841624
LOSS train 0.13469442777272503 valid 0.18431061692535877
LOSS train 0.13469442777272503 valid 0.1863594931714675
LOSS train 0.13469442777272503 valid 0.18500131203068626
LOSS train 0.13469442777272503 valid 0.18701390843642385
LOSS train 0.13469442777272503 valid 0.18826311826705933
LOSS train 0.13469442777272503 valid 0.18792580564816794
LOSS train 0.13469442777272503 valid 0.1864342208613049
LOSS train 0.13469442777272503 valid 0.18642373123894568
LOSS train 0.13469442777272503 valid 0.1861506700515747
LOSS train 0.13469442777272503 valid 0.18441222369670868
LOSS train 0.13469442777272503 valid 0.18424537376715586
LOSS train 0.13469442777272503 valid 0.1842613192620101
LOSS train 0.13469442777272503 valid 0.1845492741891316
LOSS train 0.13469442777272503 valid 0.18469298245577975
LOSS train 0.13469442777272503 valid 0.18504897952079774
LOSS train 0.13469442777272503 valid 0.18600291686673318
LOSS train 0.13469442777272503 valid 0.18532752338796854
LOSS train 0.13469442777272503 valid 0.18540846488692544
LOSS train 0.13469442777272503 valid 0.18483073439668207
LOSS train 0.13469442777272503 valid 0.18689536154270173
LOSS train 0.13469442777272503 valid 0.1866859868168831
LOSS train 0.13469442777272503 valid 0.18792365491390228
LOSS train 0.13469442777272503 valid 0.18814057111740112
LOSS train 0.13469442777272503 valid 0.1874638012586496
LOSS train 0.13469442777272503 valid 0.18736633956432341
LOSS train 0.13469442777272503 valid 0.18712159991264343
LOSS train 0.13469442777272503 valid 0.18769592898232595
LOSS train 0.13469442777272503 valid 0.18735614072444828
LOSS train 0.13469442777272503 valid 0.1879099759865891
LOSS train 0.13469442777272503 valid 0.18802408675352733
LOSS train 0.13469442777272503 valid 0.18892746062382407
LOSS train 0.13469442777272503 valid 0.18925348114460072
LOSS train 0.13469442777272503 valid 0.18943265421936908
LOSS train 0.13469442777272503 valid 0.19010378511584536
LOSS train 0.13469442777272503 valid 0.18955349385738374
LOSS train 0.13469442777272503 valid 0.1900395856184118
LOSS train 0.13469442777272503 valid 0.1899879867067704
LOSS train 0.13469442777272503 valid 0.19030075843604105
LOSS train 0.13469442777272503 valid 0.19045659320222008
LOSS train 0.13469442777272503 valid 0.19045950201424686
LOSS train 0.13469442777272503 valid 0.19057563319802284
LOSS train 0.13469442777272503 valid 0.19067409791444478
LOSS train 0.13469442777272503 valid 0.1905673392887773
LOSS train 0.13469442777272503 valid 0.19112558056742457
LOSS train 0.13469442777272503 valid 0.19051951790849367
LOSS train 0.13469442777272503 valid 0.19023496596539607
LOSS train 0.13469442777272503 valid 0.19043949727089174
LOSS train 0.13469442777272503 valid 0.1900751060909695
LOSS train 0.13469442777272503 valid 0.19083518674597144
LOSS train 0.13469442777272503 valid 0.1907499705369656
LOSS train 0.13469442777272503 valid 0.19048858592004486
LOSS train 0.13469442777272503 valid 0.19026360938798137
LOSS train 0.13469442777272503 valid 0.1905776494566132
LOSS train 0.13469442777272503 valid 0.18980480104252911
LOSS train 0.13469442777272503 valid 0.1902313164302281
LOSS train 0.13469442777272503 valid 0.1900110060060528
LOSS train 0.13469442777272503 valid 0.19009026595287853
LOSS train 0.13469442777272503 valid 0.19022550444080405
LOSS train 0.13469442777272503 valid 0.1900047667928644
LOSS train 0.13469442777272503 valid 0.1901162385940552
LOSS train 0.13469442777272503 valid 0.19102158436649724
LOSS train 0.13469442777272503 valid 0.1907536019752552
LOSS train 0.13469442777272503 valid 0.19067024000180074
LOSS train 0.13469442777272503 valid 0.19021784617931028
LOSS train 0.13469442777272503 valid 0.18959796614944935
LOSS train 0.13469442777272503 valid 0.18897482716006997
LOSS train 0.13469442777272503 valid 0.1893762724428642
LOSS train 0.13469442777272503 valid 0.18896884677639927
LOSS train 0.13469442777272503 valid 0.1889332795426959
LOSS train 0.13469442777272503 valid 0.18808279300437253
LOSS train 0.13469442777272503 valid 0.18758505738751832
LOSS train 0.13469442777272503 valid 0.18739269074352308
LOSS train 0.13469442777272503 valid 0.18707482499832456
LOSS train 0.13469442777272503 valid 0.18728327433044992
LOSS train 0.13469442777272503 valid 0.1873771220445633
LOSS train 0.13469442777272503 valid 0.18745255863273536
LOSS train 0.13469442777272503 valid 0.18736985180041063
LOSS train 0.13469442777272503 valid 0.18726136495349227
LOSS train 0.13469442777272503 valid 0.1875422237084267
LOSS train 0.13469442777272503 valid 0.18703684053922953
LOSS train 0.13469442777272503 valid 0.1871900112989048
LOSS train 0.13469442777272503 valid 0.18709999829837956
LOSS train 0.13469442777272503 valid 0.18737948609858143
LOSS train 0.13469442777272503 valid 0.18749344574682641
LOSS train 0.13469442777272503 valid 0.18752510741353035
LOSS train 0.13469442777272503 valid 0.18768548655627978
LOSS train 0.13469442777272503 valid 0.18821108560351765
LOSS train 0.13469442777272503 valid 0.1879662311482198
LOSS train 0.13469442777272503 valid 0.18800941109657288
LOSS train 0.13469442777272503 valid 0.18825959023975192
LOSS train 0.13469442777272503 valid 0.1886809582136712
LOSS train 0.13469442777272503 valid 0.18836970320928878
LOSS train 0.13469442777272503 valid 0.18840072414389364
LOSS train 0.13469442777272503 valid 0.18889500761250838
LOSS train 0.13469442777272503 valid 0.1888667720285329
LOSS train 0.13469442777272503 valid 0.18880106656401008
LOSS train 0.13469442777272503 valid 0.1885725830548576
LOSS train 0.13469442777272503 valid 0.1886612494698668
LOSS train 0.13469442777272503 valid 0.18889641591854262
LOSS train 0.13469442777272503 valid 0.18889765428460162
LOSS train 0.13469442777272503 valid 0.1892905898135284
LOSS train 0.13469442777272503 valid 0.1893445374364527
LOSS train 0.13469442777272503 valid 0.18901982133166265
LOSS train 0.13469442777272503 valid 0.18885118395340542
LOSS train 0.13469442777272503 valid 0.18854205707708996
LOSS train 0.13469442777272503 valid 0.18834768568188692
LOSS train 0.13469442777272503 valid 0.18838785393316237
LOSS train 0.13469442777272503 valid 0.18847835621213524
LOSS train 0.13469442777272503 valid 0.18886461493469053
LOSS train 0.13469442777272503 valid 0.18864802742004394
LOSS train 0.13469442777272503 valid 0.1888516363170412
LOSS train 0.13469442777272503 valid 0.1886780215060617
LOSS train 0.13469442777272503 valid 0.18869645230006427
LOSS train 0.13469442777272503 valid 0.18906449047169943
LOSS train 0.13469442777272503 valid 0.1888062387704849
LOSS train 0.13469442777272503 valid 0.18859135399337942
LOSS train 0.13469442777272503 valid 0.18816359002481808
LOSS train 0.13469442777272503 valid 0.18779310785738149
LOSS train 0.13469442777272503 valid 0.18797041320089083
LOSS train 0.13469442777272503 valid 0.18783056095794395
LOSS train 0.13469442777272503 valid 0.18764779760557063
LOSS train 0.13469442777272503 valid 0.1873556372240512
LOSS train 0.13469442777272503 valid 0.18722639384045117
LOSS train 0.13469442777272503 valid 0.18697406019238258
LOSS train 0.13469442777272503 valid 0.187055723794869
LOSS train 0.13469442777272503 valid 0.18707136704143904
LOSS train 0.13469442777272503 valid 0.1870393196881657
LOSS train 0.13469442777272503 valid 0.1870924499068227
LOSS train 0.13469442777272503 valid 0.18719168866260183
LOSS train 0.13469442777272503 valid 0.1871392188401058
LOSS train 0.13469442777272503 valid 0.187168687582016
LOSS train 0.13469442777272503 valid 0.1869883650825137
LOSS train 0.13469442777272503 valid 0.1880123873014708
LOSS train 0.13469442777272503 valid 0.1880869027352173
LOSS train 0.13469442777272503 valid 0.18788147290547688
LOSS train 0.13469442777272503 valid 0.18819021260896265
LOSS train 0.13469442777272503 valid 0.18801302845148662
LOSS train 0.13469442777272503 valid 0.1880386320788876
LOSS train 0.13469442777272503 valid 0.18803166714194533
LOSS train 0.13469442777272503 valid 0.18787993750264567
LOSS train 0.13469442777272503 valid 0.1879196468836222
LOSS train 0.13469442777272503 valid 0.18800569558219546
LOSS train 0.13469442777272503 valid 0.18797762163832218
LOSS train 0.13469442777272503 valid 0.18801714320602658
LOSS train 0.13469442777272503 valid 0.18789165923371912
LOSS train 0.13469442777272503 valid 0.18781386214013426
LOSS train 0.13469442777272503 valid 0.18754521012306213
LOSS train 0.13469442777272503 valid 0.18729656148541923
LOSS train 0.13469442777272503 valid 0.18698224906877772
LOSS train 0.13469442777272503 valid 0.18695312079155083
LOSS train 0.13469442777272503 valid 0.18688820185790578
LOSS train 0.13469442777272503 valid 0.18712482686171275
LOSS train 0.13469442777272503 valid 0.1870877172443129
LOSS train 0.13469442777272503 valid 0.1872489514964572
LOSS train 0.13469442777272503 valid 0.18730330633766512
LOSS train 0.13469442777272503 valid 0.18724782494773642
LOSS train 0.13469442777272503 valid 0.18715488945328912
LOSS train 0.13469442777272503 valid 0.18719429604579949
LOSS train 0.13469442777272503 valid 0.187121847665858
LOSS train 0.13469442777272503 valid 0.18699961977345603
LOSS train 0.13469442777272503 valid 0.18710364054211162
LOSS train 0.13469442777272503 valid 0.18707762860645683
LOSS train 0.13469442777272503 valid 0.187180614956979
LOSS train 0.13469442777272503 valid 0.1870451192449591
LOSS train 0.13469442777272503 valid 0.18700166758563783
LOSS train 0.13469442777272503 valid 0.1870839094424116
LOSS train 0.13469442777272503 valid 0.18698954885150051
LOSS train 0.13469442777272503 valid 0.18699172017027121
LOSS train 0.13469442777272503 valid 0.1870062748051208
LOSS train 0.13469442777272503 valid 0.18683745893272194
LOSS train 0.13469442777272503 valid 0.18689238151875875
LOSS train 0.13469442777272503 valid 0.18678219735941148
LOSS train 0.13469442777272503 valid 0.18669889153952293
LOSS train 0.13469442777272503 valid 0.1866421141321697
LOSS train 0.13469442777272503 valid 0.18673154954847537
LOSS train 0.13469442777272503 valid 0.1866622679520652
LOSS train 0.13469442777272503 valid 0.1867033642871926
LOSS train 0.13469442777272503 valid 0.18652156033973002
LOSS train 0.13469442777272503 valid 0.18632817037941254
LOSS train 0.13469442777272503 valid 0.18604746552614065
LOSS train 0.13469442777272503 valid 0.18604528234929454
LOSS train 0.13469442777272503 valid 0.18622915741755877
LOSS train 0.13469442777272503 valid 0.18615353303124207
LOSS train 0.13469442777272503 valid 0.1862348631099241
LOSS train 0.13469442777272503 valid 0.18612514719367027
LOSS train 0.13469442777272503 valid 0.1861223832736561
LOSS train 0.13469442777272503 valid 0.18612876239389475
LOSS train 0.13469442777272503 valid 0.18619832279059687
LOSS train 0.13469442777272503 valid 0.18644201229600346
LOSS train 0.13469442777272503 valid 0.18628127269628572
LOSS train 0.13469442777272503 valid 0.1863183924584713
LOSS train 0.13469442777272503 valid 0.18637740748803971
LOSS train 0.13469442777272503 valid 0.1863401156778519
LOSS train 0.13469442777272503 valid 0.1863607508999309
LOSS train 0.13469442777272503 valid 0.18643783572174255
LOSS train 0.13469442777272503 valid 0.18643834586674568
LOSS train 0.13469442777272503 valid 0.18644153425153695
LOSS train 0.13469442777272503 valid 0.186455530180058
LOSS train 0.13469442777272503 valid 0.18636878795712908
LOSS train 0.13469442777272503 valid 0.1862652369709902
LOSS train 0.13469442777272503 valid 0.18607607518357258
LOSS train 0.13469442777272503 valid 0.1858589186371746
LOSS train 0.13469442777272503 valid 0.18582053619240402
LOSS train 0.13469442777272503 valid 0.1859423033723004
LOSS train 0.13469442777272503 valid 0.18588953946124423
LOSS train 0.13469442777272503 valid 0.18581668574076432
LOSS train 0.13469442777272503 valid 0.18586734534652383
LOSS train 0.13469442777272503 valid 0.18592045867122342
LOSS train 0.13469442777272503 valid 0.18598985924784625
LOSS train 0.13469442777272503 valid 0.18626127428478664
LOSS train 0.13469442777272503 valid 0.18645192003619354
LOSS train 0.13469442777272503 valid 0.18652628047088168
LOSS train 0.13469442777272503 valid 0.1865387538676722
LOSS train 0.13469442777272503 valid 0.18649027844703875
LOSS train 0.13469442777272503 valid 0.18659272595592166
LOSS train 0.13469442777272503 valid 0.18669972107523963
LOSS train 0.13469442777272503 valid 0.1866440069701137
LOSS train 0.13469442777272503 valid 0.1866659960137928
LOSS train 0.13469442777272503 valid 0.18662898471722236
LOSS train 0.13469442777272503 valid 0.18673853443024005
LOSS train 0.13469442777272503 valid 0.18662011743349544
LOSS train 0.13469442777272503 valid 0.18659808948824677
LOSS train 0.13469442777272503 valid 0.1865187446985926
LOSS train 0.13469442777272503 valid 0.18641035003143375
LOSS train 0.13469442777272503 valid 0.18645576859513918
LOSS train 0.13469442777272503 valid 0.1865541455780322
LOSS train 0.13469442777272503 valid 0.1864255937047241
LOSS train 0.13469442777272503 valid 0.18665254606631557
LOSS train 0.13469442777272503 valid 0.1867368976112272
LOSS train 0.13469442777272503 valid 0.18680985533461278
LOSS train 0.13469442777272503 valid 0.186647611420329
LOSS train 0.13469442777272503 valid 0.18672796257353022
LOSS train 0.13469442777272503 valid 0.1866524903283965
LOSS train 0.13469442777272503 valid 0.18664048001708755
LOSS train 0.13469442777272503 valid 0.18661175954341888
LOSS train 0.13469442777272503 valid 0.18653753459216113
LOSS train 0.13469442777272503 valid 0.18665419193723845
LOSS train 0.13469442777272503 valid 0.18656462902136942
LOSS train 0.13469442777272503 valid 0.1865238134316572
LOSS train 0.13469442777272503 valid 0.1865074227253596
LOSS train 0.13469442777272503 valid 0.1864365132059902
LOSS train 0.13469442777272503 valid 0.18626174255335842
LOSS train 0.13469442777272503 valid 0.18647030590809593
LOSS train 0.13469442777272503 valid 0.18648941814899445
LOSS train 0.13469442777272503 valid 0.1864335559308529
LOSS train 0.13469442777272503 valid 0.1866411282984233
LOSS train 0.13469442777272503 valid 0.1866631092119763
LOSS train 0.13469442777272503 valid 0.18672657222575562
LOSS train 0.13469442777272503 valid 0.1867413122884252
LOSS train 0.13469442777272503 valid 0.18689293085404163
LOSS train 0.13469442777272503 valid 0.18689407436247157
LOSS train 0.13469442777272503 valid 0.18694912633869085
LOSS train 0.13469442777272503 valid 0.186999809664132
LOSS train 0.13469442777272503 valid 0.18717762822333764
LOSS train 0.13469442777272503 valid 0.18728516846895218
LOSS train 0.13469442777272503 valid 0.18743000198774232
LOSS train 0.13469442777272503 valid 0.1878614421824322
LOSS train 0.13469442777272503 valid 0.18803992043266365
LOSS train 0.13469442777272503 valid 0.188089535782372
LOSS train 0.13469442777272503 valid 0.188051293221387
LOSS train 0.13469442777272503 valid 0.18802431160989014
LOSS train 0.13469442777272503 valid 0.18798413173386336
LOSS train 0.13469442777272503 valid 0.18779527851574712
LOSS train 0.13469442777272503 valid 0.1878030405890557
LOSS train 0.13469442777272503 valid 0.18770194788064276
LOSS train 0.13469442777272503 valid 0.1876388900114548
LOSS train 0.13469442777272503 valid 0.18741013572042717
LOSS train 0.13469442777272503 valid 0.1874673566369623
LOSS train 0.13469442777272503 valid 0.18753405652520522
LOSS train 0.13469442777272503 valid 0.18752808793072115
LOSS train 0.13469442777272503 valid 0.18754233292245365
LOSS train 0.13469442777272503 valid 0.18754801727857323
LOSS train 0.13469442777272503 valid 0.18755396597811747
LOSS train 0.13469442777272503 valid 0.18752704424119737
LOSS train 0.13469442777272503 valid 0.18767801700481054
LOSS train 0.13469442777272503 valid 0.1874602737602909
LOSS train 0.13469442777272503 valid 0.18748373584183928
LOSS train 0.13469442777272503 valid 0.18751132686593833
LOSS train 0.13469442777272503 valid 0.18765223239149367
LOSS train 0.13469442777272503 valid 0.1877970297457808
LOSS train 0.13469442777272503 valid 0.18768427938826987
LOSS train 0.13469442777272503 valid 0.18767888201808286
LOSS train 0.13469442777272503 valid 0.1876368391633834
LOSS train 0.13469442777272503 valid 0.18763356872625575
LOSS train 0.13469442777272503 valid 0.18767221594850222
LOSS train 0.13469442777272503 valid 0.1877035955059964
LOSS train 0.13469442777272503 valid 0.18764036193193978
LOSS train 0.13469442777272503 valid 0.18770522577534415
LOSS train 0.13469442777272503 valid 0.18779786225212247
LOSS train 0.13469442777272503 valid 0.18769909617353658
LOSS train 0.13469442777272503 valid 0.18768899459270091
LOSS train 0.13469442777272503 valid 0.1877215766266037
LOSS train 0.13469442777272503 valid 0.18767915332278648
LOSS train 0.13469442777272503 valid 0.18768701261108361
LOSS train 0.13469442777272503 valid 0.1876380447418459
LOSS train 0.13469442777272503 valid 0.1875269839426329
LOSS train 0.13469442777272503 valid 0.1876506147762904
LOSS train 0.13469442777272503 valid 0.1877529948188093
LOSS train 0.13469442777272503 valid 0.18782362617125178
LOSS train 0.13469442777272503 valid 0.18774930153574262
LOSS train 0.13469442777272503 valid 0.1877343951241125
LOSS train 0.13469442777272503 valid 0.18786182082977956
LOSS train 0.13469442777272503 valid 0.1879905788872227
LOSS train 0.13469442777272503 valid 0.18805171810908003
LOSS train 0.13469442777272503 valid 0.18797784000635148
LOSS train 0.13469442777272503 valid 0.1880873942115225
LOSS train 0.13469442777272503 valid 0.18808206180052728
LOSS train 0.13469442777272503 valid 0.18805005537467107
LOSS train 0.13469442777272503 valid 0.18815115687471848
LOSS train 0.13469442777272503 valid 0.1881292362855031
LOSS train 0.13469442777272503 valid 0.18827362335715558
LOSS train 0.13469442777272503 valid 0.1883699409094061
LOSS train 0.13469442777272503 valid 0.188241487823245
LOSS train 0.13469442777272503 valid 0.18842243935380662
LOSS train 0.13469442777272503 valid 0.18833521925138705
LOSS train 0.13469442777272503 valid 0.18821665212466998
LOSS train 0.13469442777272503 valid 0.1881504941237978
LOSS train 0.13469442777272503 valid 0.18815928581240657
LOSS train 0.13469442777272503 valid 0.18825107509504535
LOSS train 0.13469442777272503 valid 0.1882557382334524
LOSS train 0.13469442777272503 valid 0.188371212665169
LOSS train 0.13469442777272503 valid 0.18846307500356738
LOSS train 0.13469442777272503 valid 0.18845205361673817
LOSS train 0.13469442777272503 valid 0.18847473220663435
LOSS train 0.13469442777272503 valid 0.18844475285971865
LOSS train 0.13469442777272503 valid 0.18833640292068382
LOSS train 0.13469442777272503 valid 0.18828327670606257
LOSS train 0.13469442777272503 valid 0.1882944642715482
LOSS train 0.13469442777272503 valid 0.18858886583773202
LOSS train 0.13469442777272503 valid 0.18864619870116744
LOSS train 0.13469442777272503 valid 0.1886760266874567
LOSS train 0.13469442777272503 valid 0.1885891666673446
LOSS train 0.13469442777272503 valid 0.18851031328754864
LOSS train 0.13469442777272503 valid 0.18852225308944298
LOSS train 0.13469442777272503 valid 0.18843470007181168
LOSS train 0.13469442777272503 valid 0.18836751809486976
LOSS train 0.13469442777272503 valid 0.18838208973069082
LOSS train 0.13469442777272503 valid 0.1883428116189184
LOSS train 0.13469442777272503 valid 0.18831135151581577
LOSS train 0.13469442777272503 valid 0.18842360830642807
LOSS train 0.13469442777272503 valid 0.18842121159260192
LOSS train 0.13469442777272503 valid 0.18846926756766663
LOSS train 0.13469442777272503 valid 0.18838094129076216
LOSS train 0.13469442777272503 valid 0.18837270016125648
LOSS train 0.13469442777272503 valid 0.18830112926661968
LOSS train 0.13469442777272503 valid 0.18831468396239665
LOSS train 0.13469442777272503 valid 0.18840128520904983
LOSS train 0.13469442777272503 valid 0.18826153431056944
LOSS train 0.13469442777272503 valid 0.18828869316276614
LOSS train 0.13469442777272503 valid 0.18830434369714294
LOSS train 0.13469442777272503 valid 0.1882901780022298
LOSS train 0.13469442777272503 valid 0.18816983314073704
LOSS train 0.13469442777272503 valid 0.18815996206325034
LOSS train 0.13469442777272503 valid 0.1881947250870185
EPOCH 15:
  batch 1 loss: 0.12678925693035126
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.12701284140348434
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.12744185825188956
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.13559263572096825
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.1401325821876526
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.1395751213033994
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.1310042611190251
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.130522932857275
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.12901002334223854
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.12721894904971123
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.12691231546076862
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.1256127543747425
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.1266508732850735
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.1257258877158165
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.1260159562031428
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.12745648715645075
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.12628919587415807
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.12816360096136728
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.12801574405870939
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.12773173712193966
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.12836099841764995
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.12843058495358986
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.12844521034023035
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.1303607259566585
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.12992394894361495
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.13012391357467726
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.1307814422029036
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.130663288757205
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.1303167856972793
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.1308412457505862
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.13132756371651927
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.13105347845703363
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.1305566102717862
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.13146626445300438
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.1325248903461865
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.13266243723531565
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.13261818986486745
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.13218767470435092
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.13295157139117902
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.1325755650177598
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.1327490168737202
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.13305547248039926
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.13366700656885325
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.13314069892195138
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.13401369667715496
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.13430761788850246
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.1339540332555771
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.13353192526847124
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.13348296132622933
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.13356779277324676
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.13355688487782197
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.13402712001250341
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.13421257374421605
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.13461688878359618
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.13499754938212308
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.1351565412644829
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.13533092695370055
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.13527095574757148
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.13584490372972974
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.13594887008269627
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.13575243791107272
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.13559510739099595
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.13590954421531587
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.13575258443597704
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.13572262330697132
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.1357365877113559
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.13555749216631277
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.1357030280153541
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.1359385399930719
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.1361107246151992
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.13633913840626327
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.13626619914753568
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.13647480727466818
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.13634435902978922
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.13614878932634988
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.13618307795963788
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.13591179774179088
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.13539555401374131
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.13551921463465388
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.1355119688436389
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.13578315483935086
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.13540177165371617
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.13525825352912926
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.1359182930595818
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.13608999594169505
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.13625053359672082
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.1363169234888307
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.1364095447246324
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.13687395371412964
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.13689958908491665
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.1368233017050303
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.13679151617638444
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.13701425725093452
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.13717702863381265
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.13743870250488582
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.13748340751044452
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.13725746131127642
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.13722498144726364
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.13741320168430154
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.13721341878175736
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.13715445626490186
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.13703669476158478
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.13691891787700283
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.13711485295341566
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.13663330021358672
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.13653542382537193
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.13669944672941048
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.13651288183474983
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.13680191604642694
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.1372386062009768
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.1371819494409604
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.13692739133590034
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.1365865957552353
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.1368262844258233
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.13655572099530178
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.13643538662842636
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.1366547830721252
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.13657891630368718
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.13639710893651016
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.1365752924233675
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.1363981281183968
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.13643009838510733
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.13660826673352622
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.13652905821800232
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.13648031258583068
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.13644246485025163
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.13653087921029938
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.1363843534491025
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.13644840614509213
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.1365937818128329
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.1363140433344222
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.13612115146084267
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.13639639810960097
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.1362384324754352
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.13598299821217855
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.13594112501424901
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.1361672677045321
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.1361285896188971
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.13628592904951933
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.13649064100214414
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.13638584130833334
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.13638004361533781
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.13633394475791838
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.13652066198281115
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.13632285096522034
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.13626918640651114
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.13616250004289912
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.13608416446761504
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.13629390684350226
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.1363227706650893
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.13609656933325015
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.13605946032820562
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.13593460604840635
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.13567660695740155
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.135717179361851
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.13559897420689082
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.13558352159656537
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.13547011002709594
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.13566873602147372
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.13580211605876685
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.13568369313056425
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.13564449447540589
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.1356333299465706
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.13568823648298659
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.13581827066161417
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.1357716963772314
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.1354563076517539
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.1353955833862225
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.1352618920379842
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.1354022000642384
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.13530643833310982
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.13526388344376586
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.13519210618183103
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.13509614560110816
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.13511238540921894
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.1349468667229468
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.13477867610205366
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.13455533818080184
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.13462095424616138
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.13454394212199583
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.1345711430709665
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.1344882063158266
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.1342512015762225
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.13421697299117627
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.1342412046484045
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.13425226349343536
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.1343736972878961
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.13420711219944853
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.13424798530876322
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.13440773142011542
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.13447405288668826
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.13439954801773032
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.13443489534867242
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.1342408946264036
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.1342262991345846
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.13433782197535038
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.1345195154172515
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.13464183574824623
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.134646262617866
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.1348388598486781
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.13485935741840904
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.13490674958223164
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.1348142222377467
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.1348826356378256
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.13470728172034752
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.1345610569764688
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.1345280984389609
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.13438763390653408
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.13435976856062856
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.1346194509239424
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.13460899592858355
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.13466830087720222
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.1347089861060532
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.134532111340037
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.1346341418665509
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.1345333399389077
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.13458071769245208
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.13474010016529933
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.134823452085937
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.13484575426713988
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.13491732283280447
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.13492205332931098
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.13483938644953372
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.1348799810678299
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.13477879967954423
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.13467170314583105
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.13478820751285764
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.13461206386094554
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.1345944833742479
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.1348093706952489
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.13494148376313123
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.13488352244141802
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.13487229355542957
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.13481730365982422
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.13478938540879717
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.13488035071325505
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.1348810258482579
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.13484813019382855
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.13492651978920694
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.13488241561378042
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.13499177234919735
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.134906895089248
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.1349534686087581
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.13492870770516943
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.13496051649658047
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.13500735036483624
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.13497155126531116
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.13496949685917747
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.13488508548482833
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.13485951581597327
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.13494057552629257
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.13476808617512384
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.1347757338181786
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.13472166554782333
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.13479770670334498
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.13477090917876922
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.13464605126979287
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.134621315599643
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.13453380855584238
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.13453681566394293
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.13454048758037246
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.13452663539930154
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.13446353455233484
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.13430541265530116
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.13424973291046213
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.1342824811774089
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.1342830015032479
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.13425516314915756
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.1342172774738982
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.13417391467977452
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.13415070092546105
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.1341579347191488
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.1343113355365865
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.1342507523559306
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.1341830375790596
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.13429515800722266
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.13424218104419294
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.13426821753918697
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.1342333394726972
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.1342616163992456
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.13428115094365597
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.13425068052630898
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.1342546143453871
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.13423700764460464
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.13435728437545008
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.13436227909230686
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.1343251509585447
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.13442797402644324
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.13434990985273904
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.13435086872557114
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.13428789751021722
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.1342487627359694
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.13437072085947713
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.13443514686964808
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.13434454321356143
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.13431207782815438
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.13438672537253762
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.13434827620251066
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.13428521791728443
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.13430319018661976
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.13441643745103152
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.13434877727698807
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.1343345953637224
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.13431598086792387
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.13430601952017332
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.1342801009274386
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.13429361841480583
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.13426068788031478
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.13422478872884824
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.1341994322836399
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.13423713459558426
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.13429346214979887
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.13427559867168007
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.1342147741300665
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.1342262949498873
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.13432586690576015
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.13434952857084453
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.1343249480163901
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.13429221057779736
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.13436936670914293
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.1343976861776964
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.1344336509612036
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.1344154892783416
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.1343548953257225
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.13441878277521868
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.13431569927034934
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.13419629041025763
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.13417577266511393
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.1341613222219299
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.13423039895115477
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.13429403845277077
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.13427835397691612
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.13420802498006965
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.1342319794846866
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.13432381304342356
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.1343465167585583
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.13432181451900777
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.13431185536836027
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.13422744116776109
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.13424318559906062
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.13417976299339027
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.1343428814533161
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.1343186710562025
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.13431271156945893
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.1343272067066552
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.13432829891670647
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.1343926901174897
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.13437733931959361
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.13436096987785787
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.1342807851731777
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.1342367607346627
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.13429336827671665
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.13433361709877722
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.13442570949762556
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.13444573839365598
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.1344392564817426
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.13445563816807182
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.13456235524246146
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.13453572704184355
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.13449554186728266
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.13447841011256062
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.1344837794564047
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.13441494965192044
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.1343271014495538
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.13437748602400088
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.13437463605745895
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.13441354005879214
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.1344321196615372
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.13440008427150205
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.13435368215715562
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.1343951785982137
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.1343910503852111
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.1343968425535325
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.13446015482120974
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.13449656788508096
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.13450625875687344
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.13446191925585113
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.13453087356513138
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.13453231492426276
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.13447878960716098
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.1344389077520433
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.13440714244053
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.13450195052838512
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.1344271961133927
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.13437487815881705
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.13427668894819644
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.13428717385736855
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.13432450500345722
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.13432744049627554
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.13432620935715162
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.1342888696266867
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.13429191282817296
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.13433299439250665
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.1343285692146587
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.1343691478801679
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.13440409705343873
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.13431668442953143
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.13433686269437847
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.13429718780188932
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.1342879520729184
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.1343246601391909
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.13430524506230854
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.1342680016463211
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.1342790299417949
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.13423182647905232
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.13415925327706807
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.134113537975992
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.13420033390980726
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.13418099446299606
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.13422028137052933
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.13422279558405098
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.1341991684426671
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.1341653937974507
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.1342117721799779
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.13429382886872235
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.13436636896445775
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.13435935604272128
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.1343296353290811
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.13446947685076116
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.13444291656570775
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.13450261516908002
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.13451928753070358
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.13445423971465292
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.134451190695026
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.13446116975125144
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.1344179012091227
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.13441067359975126
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.134366913811765
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.13433580758757802
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.13430907268856848
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.13432995131425127
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.13431799249654566
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.1342624145587928
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.1342435645857989
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.13419448250326618
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.13426515388242696
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.1343305405203756
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.13429804821605007
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.13419360769297375
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.1341205983338031
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.13411167003805674
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.1340546656855091
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.1340308057367129
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.13402093535809367
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.134042921746045
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.13407150463873496
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.13403420771995114
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.13413646286686084
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.13416600288752722
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.13412913474771712
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.13415493071079254
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.13424029791381506
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.1341951790103323
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.13414823749648316
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.13413386289235
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.13415092965097805
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.13417211921653288
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.13416965666946887
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.1341659939626723
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.13426975847586342
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.13426070332268572
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.1342687363296876
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.13415391139734384
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.13417766742600962
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.13418177268517914
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.1341583228034523
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.13419584049274885
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.13425175848807025
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.13431315082731024
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.13437002716546362
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.13439106185092034
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.13448409720371335
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.13448409720371335 valid 0.2318243533372879
LOSS train 0.13448409720371335 valid 0.1900847926735878
LOSS train 0.13448409720371335 valid 0.19386552770932516
LOSS train 0.13448409720371335 valid 0.1792871206998825
LOSS train 0.13448409720371335 valid 0.17518636882305144
LOSS train 0.13448409720371335 valid 0.18186993648608527
LOSS train 0.13448409720371335 valid 0.18956188431807927
LOSS train 0.13448409720371335 valid 0.18902458436787128
LOSS train 0.13448409720371335 valid 0.1882639444536633
LOSS train 0.13448409720371335 valid 0.19021164625883102
LOSS train 0.13448409720371335 valid 0.18878086453134363
LOSS train 0.13448409720371335 valid 0.18665121868252754
LOSS train 0.13448409720371335 valid 0.18546517537190363
LOSS train 0.13448409720371335 valid 0.18507441026823862
LOSS train 0.13448409720371335 valid 0.18183641533056896
LOSS train 0.13448409720371335 valid 0.18306764494627714
LOSS train 0.13448409720371335 valid 0.18511471415267272
LOSS train 0.13448409720371335 valid 0.1837503355410364
LOSS train 0.13448409720371335 valid 0.18574140965938568
LOSS train 0.13448409720371335 valid 0.1870434857904911
LOSS train 0.13448409720371335 valid 0.18669510526316507
LOSS train 0.13448409720371335 valid 0.18521373922174628
LOSS train 0.13448409720371335 valid 0.18516174347504324
LOSS train 0.13448409720371335 valid 0.1849037930369377
LOSS train 0.13448409720371335 valid 0.1831734699010849
LOSS train 0.13448409720371335 valid 0.18303248572808045
LOSS train 0.13448409720371335 valid 0.18305056459373897
LOSS train 0.13448409720371335 valid 0.1833449181701456
LOSS train 0.13448409720371335 valid 0.18347870841108518
LOSS train 0.13448409720371335 valid 0.18380956103404364
LOSS train 0.13448409720371335 valid 0.18477038798793669
LOSS train 0.13448409720371335 valid 0.18407563492655754
LOSS train 0.13448409720371335 valid 0.18412980527588815
LOSS train 0.13448409720371335 valid 0.18356448499595418
LOSS train 0.13448409720371335 valid 0.185625525031771
LOSS train 0.13448409720371335 valid 0.1854278523888853
LOSS train 0.13448409720371335 valid 0.18669191808313937
LOSS train 0.13448409720371335 valid 0.1869103041918654
LOSS train 0.13448409720371335 valid 0.18624569246402153
LOSS train 0.13448409720371335 valid 0.18614260703325272
LOSS train 0.13448409720371335 valid 0.18589746770335408
LOSS train 0.13448409720371335 valid 0.1864855640700885
LOSS train 0.13448409720371335 valid 0.1861384569905525
LOSS train 0.13448409720371335 valid 0.1866974305700172
LOSS train 0.13448409720371335 valid 0.18681638340155285
LOSS train 0.13448409720371335 valid 0.18772092655948971
LOSS train 0.13448409720371335 valid 0.18804035224813095
LOSS train 0.13448409720371335 valid 0.1882163705304265
LOSS train 0.13448409720371335 valid 0.18888108736398268
LOSS train 0.13448409720371335 valid 0.18833422631025315
LOSS train 0.13448409720371335 valid 0.1888229093130897
LOSS train 0.13448409720371335 valid 0.1887631702881593
LOSS train 0.13448409720371335 valid 0.18908411433111946
LOSS train 0.13448409720371335 valid 0.18923325698684762
LOSS train 0.13448409720371335 valid 0.18923363306305624
LOSS train 0.13448409720371335 valid 0.18935298014964377
LOSS train 0.13448409720371335 valid 0.18946174153110437
LOSS train 0.13448409720371335 valid 0.1893586814403534
LOSS train 0.13448409720371335 valid 0.18991845467333066
LOSS train 0.13448409720371335 valid 0.1893076367676258
LOSS train 0.13448409720371335 valid 0.18902627830622626
LOSS train 0.13448409720371335 valid 0.1892281150625598
LOSS train 0.13448409720371335 valid 0.18885930426537045
LOSS train 0.13448409720371335 valid 0.18961133644916117
LOSS train 0.13448409720371335 valid 0.18952398712818438
LOSS train 0.13448409720371335 valid 0.18925664506175302
LOSS train 0.13448409720371335 valid 0.1890419837254197
LOSS train 0.13448409720371335 valid 0.18934730308897355
LOSS train 0.13448409720371335 valid 0.1885758951522302
LOSS train 0.13448409720371335 valid 0.1890089179788317
LOSS train 0.13448409720371335 valid 0.18879541586822188
LOSS train 0.13448409720371335 valid 0.18886899037493599
LOSS train 0.13448409720371335 valid 0.18900999059415843
LOSS train 0.13448409720371335 valid 0.18878557633709264
LOSS train 0.13448409720371335 valid 0.18891005396842955
LOSS train 0.13448409720371335 valid 0.18981730937957764
LOSS train 0.13448409720371335 valid 0.1895463000257294
LOSS train 0.13448409720371335 valid 0.18946525970330605
LOSS train 0.13448409720371335 valid 0.1890077083548413
LOSS train 0.13448409720371335 valid 0.18838268890976906
LOSS train 0.13448409720371335 valid 0.18776508098767128
LOSS train 0.13448409720371335 valid 0.18817644929740487
LOSS train 0.13448409720371335 valid 0.1877700839057026
LOSS train 0.13448409720371335 valid 0.18773807106273516
LOSS train 0.13448409720371335 valid 0.1868878746733946
LOSS train 0.13448409720371335 valid 0.18638844129651091
LOSS train 0.13448409720371335 valid 0.18619579419322396
LOSS train 0.13448409720371335 valid 0.18587939949198204
LOSS train 0.13448409720371335 valid 0.18608887778239303
LOSS train 0.13448409720371335 valid 0.18617906040615506
LOSS train 0.13448409720371335 valid 0.18625936419754238
LOSS train 0.13448409720371335 valid 0.1861803813473038
LOSS train 0.13448409720371335 valid 0.18607517708373325
LOSS train 0.13448409720371335 valid 0.186345396999349
LOSS train 0.13448409720371335 valid 0.18584069531214864
LOSS train 0.13448409720371335 valid 0.1859975646560391
LOSS train 0.13448409720371335 valid 0.18589965584351845
LOSS train 0.13448409720371335 valid 0.18617483122008188
LOSS train 0.13448409720371335 valid 0.18629120440796168
LOSS train 0.13448409720371335 valid 0.18632052823901177
LOSS train 0.13448409720371335 valid 0.18647980512958942
LOSS train 0.13448409720371335 valid 0.18701292457533816
LOSS train 0.13448409720371335 valid 0.1867669557483451
LOSS train 0.13448409720371335 valid 0.18682033353700087
LOSS train 0.13448409720371335 valid 0.18706466541403816
LOSS train 0.13448409720371335 valid 0.18748339237469547
LOSS train 0.13448409720371335 valid 0.18717152117011704
LOSS train 0.13448409720371335 valid 0.1872039825000145
LOSS train 0.13448409720371335 valid 0.18769615792900052
LOSS train 0.13448409720371335 valid 0.18766286657615142
LOSS train 0.13448409720371335 valid 0.18760598283093255
LOSS train 0.13448409720371335 valid 0.18737373713936126
LOSS train 0.13448409720371335 valid 0.18746107763948694
LOSS train 0.13448409720371335 valid 0.18769978640372292
LOSS train 0.13448409720371335 valid 0.1877004968083423
LOSS train 0.13448409720371335 valid 0.18809314765806856
LOSS train 0.13448409720371335 valid 0.18814173366269496
LOSS train 0.13448409720371335 valid 0.18781384408979093
LOSS train 0.13448409720371335 valid 0.18764284703911854
LOSS train 0.13448409720371335 valid 0.18733483466009299
LOSS train 0.13448409720371335 valid 0.18713969164643407
LOSS train 0.13448409720371335 valid 0.18718228281521407
LOSS train 0.13448409720371335 valid 0.18726551750811135
LOSS train 0.13448409720371335 valid 0.18764981122747545
LOSS train 0.13448409720371335 valid 0.18742599856853484
LOSS train 0.13448409720371335 valid 0.18762897901118747
LOSS train 0.13448409720371335 valid 0.18745348209471213
LOSS train 0.13448409720371335 valid 0.18746918183751404
LOSS train 0.13448409720371335 valid 0.1878332722094632
LOSS train 0.13448409720371335 valid 0.18757300078868866
LOSS train 0.13448409720371335 valid 0.1873585361105795
LOSS train 0.13448409720371335 valid 0.18693309186985999
LOSS train 0.13448409720371335 valid 0.18656320993165323
LOSS train 0.13448409720371335 valid 0.18674138566451287
LOSS train 0.13448409720371335 valid 0.18660220007101694
LOSS train 0.13448409720371335 valid 0.1864222075571032
LOSS train 0.13448409720371335 valid 0.1861291001312924
LOSS train 0.13448409720371335 valid 0.18599961179754007
LOSS train 0.13448409720371335 valid 0.18574707081420816
LOSS train 0.13448409720371335 valid 0.18582425628389632
LOSS train 0.13448409720371335 valid 0.18583635478577715
LOSS train 0.13448409720371335 valid 0.18579928545464933
LOSS train 0.13448409720371335 valid 0.1858486217427087
LOSS train 0.13448409720371335 valid 0.18595118541270494
LOSS train 0.13448409720371335 valid 0.18590530095429256
LOSS train 0.13448409720371335 valid 0.18593278823241796
LOSS train 0.13448409720371335 valid 0.18575477985297742
LOSS train 0.13448409720371335 valid 0.1867837480999328
LOSS train 0.13448409720371335 valid 0.18685942308214687
LOSS train 0.13448409720371335 valid 0.18665354947249094
LOSS train 0.13448409720371335 valid 0.1869689036678794
LOSS train 0.13448409720371335 valid 0.1867940182748594
LOSS train 0.13448409720371335 valid 0.18682165288068112
LOSS train 0.13448409720371335 valid 0.18681336552291722
LOSS train 0.13448409720371335 valid 0.1866591073812977
LOSS train 0.13448409720371335 valid 0.1866945115228494
LOSS train 0.13448409720371335 valid 0.18678320877870935
LOSS train 0.13448409720371335 valid 0.18675377238777618
LOSS train 0.13448409720371335 valid 0.1867957813372402
LOSS train 0.13448409720371335 valid 0.18667334476485847
LOSS train 0.13448409720371335 valid 0.186594782501274
LOSS train 0.13448409720371335 valid 0.18632380370004678
LOSS train 0.13448409720371335 valid 0.1860762482589008
LOSS train 0.13448409720371335 valid 0.1857604964113817
LOSS train 0.13448409720371335 valid 0.185734397353548
LOSS train 0.13448409720371335 valid 0.1856697555406984
LOSS train 0.13448409720371335 valid 0.1859131799843497
LOSS train 0.13448409720371335 valid 0.18587925116575899
LOSS train 0.13448409720371335 valid 0.1860408885239144
LOSS train 0.13448409720371335 valid 0.1860932940069367
LOSS train 0.13448409720371335 valid 0.18603496959334925
LOSS train 0.13448409720371335 valid 0.18594223737370136
LOSS train 0.13448409720371335 valid 0.18598530058226834
LOSS train 0.13448409720371335 valid 0.18591174808726912
LOSS train 0.13448409720371335 valid 0.18579130845410483
LOSS train 0.13448409720371335 valid 0.18589577400548893
LOSS train 0.13448409720371335 valid 0.18587168764933354
LOSS train 0.13448409720371335 valid 0.18597426809621662
LOSS train 0.13448409720371335 valid 0.18583589925446323
LOSS train 0.13448409720371335 valid 0.18578821735249626
LOSS train 0.13448409720371335 valid 0.18586912249003984
LOSS train 0.13448409720371335 valid 0.18577098371563378
LOSS train 0.13448409720371335 valid 0.1857767915302287
LOSS train 0.13448409720371335 valid 0.18578984807042973
LOSS train 0.13448409720371335 valid 0.1856221001695942
LOSS train 0.13448409720371335 valid 0.1856783011587717
LOSS train 0.13448409720371335 valid 0.18557022137437912
LOSS train 0.13448409720371335 valid 0.18548863143362898
LOSS train 0.13448409720371335 valid 0.18543048605086312
LOSS train 0.13448409720371335 valid 0.18551703691482543
LOSS train 0.13448409720371335 valid 0.18544717351491538
LOSS train 0.13448409720371335 valid 0.1854896740987897
LOSS train 0.13448409720371335 valid 0.1853051611024481
LOSS train 0.13448409720371335 valid 0.18511399480792665
LOSS train 0.13448409720371335 valid 0.18483359867181534
LOSS train 0.13448409720371335 valid 0.1848303937942398
LOSS train 0.13448409720371335 valid 0.18501144337472578
LOSS train 0.13448409720371335 valid 0.18493735075297982
LOSS train 0.13448409720371335 valid 0.18501853748182556
LOSS train 0.13448409720371335 valid 0.18490832962095738
LOSS train 0.13448409720371335 valid 0.18490911664357826
LOSS train 0.13448409720371335 valid 0.1849157441370558
LOSS train 0.13448409720371335 valid 0.18498765145029342
LOSS train 0.13448409720371335 valid 0.1852326483703127
LOSS train 0.13448409720371335 valid 0.18507265513989984
LOSS train 0.13448409720371335 valid 0.1851102267798868
LOSS train 0.13448409720371335 valid 0.18516845029333365
LOSS train 0.13448409720371335 valid 0.18513162713497877
LOSS train 0.13448409720371335 valid 0.18515468691809897
LOSS train 0.13448409720371335 valid 0.1852320522069931
LOSS train 0.13448409720371335 valid 0.1852322816848755
LOSS train 0.13448409720371335 valid 0.1852366329645211
LOSS train 0.13448409720371335 valid 0.18524954156696516
LOSS train 0.13448409720371335 valid 0.18516268511520367
LOSS train 0.13448409720371335 valid 0.18506010500497597
LOSS train 0.13448409720371335 valid 0.18486823879734235
LOSS train 0.13448409720371335 valid 0.18465090189386624
LOSS train 0.13448409720371335 valid 0.18461014672156867
LOSS train 0.13448409720371335 valid 0.18473260478886294
LOSS train 0.13448409720371335 valid 0.18467634625055573
LOSS train 0.13448409720371335 valid 0.18460402290475855
LOSS train 0.13448409720371335 valid 0.18465867873516167
LOSS train 0.13448409720371335 valid 0.18471307350915644
LOSS train 0.13448409720371335 valid 0.18478253077981727
LOSS train 0.13448409720371335 valid 0.18505780729982588
LOSS train 0.13448409720371335 valid 0.1852490709418744
LOSS train 0.13448409720371335 valid 0.18532014149138581
LOSS train 0.13448409720371335 valid 0.18533240958002575
LOSS train 0.13448409720371335 valid 0.18528397836799704
LOSS train 0.13448409720371335 valid 0.18538709639207177
LOSS train 0.13448409720371335 valid 0.1854974650072329
LOSS train 0.13448409720371335 valid 0.18544062137089926
LOSS train 0.13448409720371335 valid 0.18546544034593607
LOSS train 0.13448409720371335 valid 0.18543185650283456
LOSS train 0.13448409720371335 valid 0.1855384551464243
LOSS train 0.13448409720371335 valid 0.18542044338280872
LOSS train 0.13448409720371335 valid 0.1853946397701899
LOSS train 0.13448409720371335 valid 0.18531468811155366
LOSS train 0.13448409720371335 valid 0.18520359458035504
LOSS train 0.13448409720371335 valid 0.18525171137104432
LOSS train 0.13448409720371335 valid 0.18535031186594508
LOSS train 0.13448409720371335 valid 0.18522414472723794
LOSS train 0.13448409720371335 valid 0.18545315025274645
LOSS train 0.13448409720371335 valid 0.1855365388461801
LOSS train 0.13448409720371335 valid 0.18560744414524155
LOSS train 0.13448409720371335 valid 0.18544234632234263
LOSS train 0.13448409720371335 valid 0.18552732491782803
LOSS train 0.13448409720371335 valid 0.18545100140956142
LOSS train 0.13448409720371335 valid 0.18543942314553932
LOSS train 0.13448409720371335 valid 0.18540785598754883
LOSS train 0.13448409720371335 valid 0.18533069738353866
LOSS train 0.13448409720371335 valid 0.1854484866535853
LOSS train 0.13448409720371335 valid 0.18535821169261404
LOSS train 0.13448409720371335 valid 0.18531751802821797
LOSS train 0.13448409720371335 valid 0.18530052935375887
LOSS train 0.13448409720371335 valid 0.18522806005785242
LOSS train 0.13448409720371335 valid 0.18505172118370636
LOSS train 0.13448409720371335 valid 0.1852604062982308
LOSS train 0.13448409720371335 valid 0.18527914790803401
LOSS train 0.13448409720371335 valid 0.18522465475476704
LOSS train 0.13448409720371335 valid 0.18543325473065578
LOSS train 0.13448409720371335 valid 0.18545593110887149
LOSS train 0.13448409720371335 valid 0.1855199148333118
LOSS train 0.13448409720371335 valid 0.18553708251001258
LOSS train 0.13448409720371335 valid 0.18569076162464213
LOSS train 0.13448409720371335 valid 0.18569180302153854
LOSS train 0.13448409720371335 valid 0.18574869543425598
LOSS train 0.13448409720371335 valid 0.1858012910876701
LOSS train 0.13448409720371335 valid 0.185976363746207
LOSS train 0.13448409720371335 valid 0.18608436711408474
LOSS train 0.13448409720371335 valid 0.1862291026159406
LOSS train 0.13448409720371335 valid 0.1866627547013409
LOSS train 0.13448409720371335 valid 0.1868422561403596
LOSS train 0.13448409720371335 valid 0.18689488369400484
LOSS train 0.13448409720371335 valid 0.18685638189315795
LOSS train 0.13448409720371335 valid 0.1868303873832675
LOSS train 0.13448409720371335 valid 0.18679252218468526
LOSS train 0.13448409720371335 valid 0.1866044735522579
LOSS train 0.13448409720371335 valid 0.1866110034527317
LOSS train 0.13448409720371335 valid 0.18650980943015644
LOSS train 0.13448409720371335 valid 0.18644706507170328
LOSS train 0.13448409720371335 valid 0.18621690758893675
LOSS train 0.13448409720371335 valid 0.186273999775467
LOSS train 0.13448409720371335 valid 0.1863437684541437
LOSS train 0.13448409720371335 valid 0.1863365183535375
LOSS train 0.13448409720371335 valid 0.18635290347925432
LOSS train 0.13448409720371335 valid 0.18635902901663598
LOSS train 0.13448409720371335 valid 0.18636219868332976
LOSS train 0.13448409720371335 valid 0.18633476773657187
LOSS train 0.13448409720371335 valid 0.18648696092182193
LOSS train 0.13448409720371335 valid 0.18626572173792882
LOSS train 0.13448409720371335 valid 0.18628829919210035
LOSS train 0.13448409720371335 valid 0.18631549538097283
LOSS train 0.13448409720371335 valid 0.18645602381046938
LOSS train 0.13448409720371335 valid 0.18660177247504056
LOSS train 0.13448409720371335 valid 0.1864886558488817
LOSS train 0.13448409720371335 valid 0.18648229627195834
LOSS train 0.13448409720371335 valid 0.186439596991611
LOSS train 0.13448409720371335 valid 0.18643528076019575
LOSS train 0.13448409720371335 valid 0.1864769072085619
LOSS train 0.13448409720371335 valid 0.1865058158867779
LOSS train 0.13448409720371335 valid 0.18644098441628432
LOSS train 0.13448409720371335 valid 0.18650500764056008
LOSS train 0.13448409720371335 valid 0.18659634704358483
LOSS train 0.13448409720371335 valid 0.18649918058368026
LOSS train 0.13448409720371335 valid 0.18648913432083097
LOSS train 0.13448409720371335 valid 0.18652215961621715
LOSS train 0.13448409720371335 valid 0.1864798270504583
LOSS train 0.13448409720371335 valid 0.1864862402714186
LOSS train 0.13448409720371335 valid 0.1864378305452485
LOSS train 0.13448409720371335 valid 0.18632778326510616
LOSS train 0.13448409720371335 valid 0.18645050269193375
LOSS train 0.13448409720371335 valid 0.18655615599867634
LOSS train 0.13448409720371335 valid 0.18662498950673517
LOSS train 0.13448409720371335 valid 0.18655082803396952
LOSS train 0.13448409720371335 valid 0.18653787924802
LOSS train 0.13448409720371335 valid 0.18666653802812289
LOSS train 0.13448409720371335 valid 0.18679553802189586
LOSS train 0.13448409720371335 valid 0.18685665481423136
LOSS train 0.13448409720371335 valid 0.18678441031370313
LOSS train 0.13448409720371335 valid 0.18689493540198632
LOSS train 0.13448409720371335 valid 0.18689019017800781
LOSS train 0.13448409720371335 valid 0.18685778271948958
LOSS train 0.13448409720371335 valid 0.1869610903761637
LOSS train 0.13448409720371335 valid 0.18693920940160752
LOSS train 0.13448409720371335 valid 0.18708412328655002
LOSS train 0.13448409720371335 valid 0.18717863018815306
LOSS train 0.13448409720371335 valid 0.1870480278143432
LOSS train 0.13448409720371335 valid 0.18722742825384198
LOSS train 0.13448409720371335 valid 0.18714142457553834
LOSS train 0.13448409720371335 valid 0.18702179105710406
LOSS train 0.13448409720371335 valid 0.1869554314762354
LOSS train 0.13448409720371335 valid 0.18696519340928253
LOSS train 0.13448409720371335 valid 0.18705486245230288
LOSS train 0.13448409720371335 valid 0.18705979654593255
LOSS train 0.13448409720371335 valid 0.1871788364480294
LOSS train 0.13448409720371335 valid 0.18727253605155633
LOSS train 0.13448409720371335 valid 0.18726231243747932
LOSS train 0.13448409720371335 valid 0.18728527670676728
LOSS train 0.13448409720371335 valid 0.1872554922147709
LOSS train 0.13448409720371335 valid 0.18714682642053648
LOSS train 0.13448409720371335 valid 0.1870931184126271
LOSS train 0.13448409720371335 valid 0.18710416435934712
LOSS train 0.13448409720371335 valid 0.18739867775679328
LOSS train 0.13448409720371335 valid 0.18745702792336974
LOSS train 0.13448409720371335 valid 0.18748898164643718
LOSS train 0.13448409720371335 valid 0.1874032780442183
LOSS train 0.13448409720371335 valid 0.18732533045113087
LOSS train 0.13448409720371335 valid 0.1873362531677358
LOSS train 0.13448409720371335 valid 0.18724830599767822
LOSS train 0.13448409720371335 valid 0.1871805113137957
LOSS train 0.13448409720371335 valid 0.18719475439072333
LOSS train 0.13448409720371335 valid 0.18715508811956763
LOSS train 0.13448409720371335 valid 0.18712529095973673
LOSS train 0.13448409720371335 valid 0.1872346462708124
LOSS train 0.13448409720371335 valid 0.1872299536434787
LOSS train 0.13448409720371335 valid 0.1872797130661852
LOSS train 0.13448409720371335 valid 0.18719040192455552
LOSS train 0.13448409720371335 valid 0.18718239766724595
LOSS train 0.13448409720371335 valid 0.1871111347236567
LOSS train 0.13448409720371335 valid 0.18712411104724677
LOSS train 0.13448409720371335 valid 0.18721110499283886
LOSS train 0.13448409720371335 valid 0.18707377430739153
LOSS train 0.13448409720371335 valid 0.18710153531487828
LOSS train 0.13448409720371335 valid 0.18711676301613245
LOSS train 0.13448409720371335 valid 0.18710272622808732
LOSS train 0.13448409720371335 valid 0.18698382355293072
LOSS train 0.13448409720371335 valid 0.1869737962058381
LOSS train 0.13448409720371335 valid 0.18700995603147239
bichrom_seq(
  (conv1d): Conv1d(4, 256, kernel_size=(24,), stride=(1,))
  (relu): ReLU()
  (batchNorm1d): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (maxPool1d): MaxPool1d(kernel_size=15, stride=15, padding=0, dilation=1, ceil_mode=True)
  (lstm): LSTM(256, 32, batch_first=True)
  (tanh): Tanh()
  (model_dense_repeat): Sequential(
    (0): Linear(in_features=32, out_features=512, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.5, inplace=False)
    (3): Linear(in_features=512, out_features=512, bias=True)
    (4): ReLU()
    (5): Dropout(p=0.5, inplace=False)
    (6): Linear(in_features=512, out_features=512, bias=True)
    (7): ReLU()
    (8): Dropout(p=0.5, inplace=False)
  )
  (linear): Linear(in_features=512, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)
bimodal_network(
  (base_model): bichrom_seq(
    (conv1d): Conv1d(4, 256, kernel_size=(24,), stride=(1,))
    (relu): ReLU()
    (batchNorm1d): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (maxPool1d): MaxPool1d(kernel_size=15, stride=15, padding=0, dilation=1, ceil_mode=True)
    (lstm): LSTM(256, 32, batch_first=True)
    (tanh): Tanh()
    (model_dense_repeat): Sequential(
      (0): Linear(in_features=32, out_features=512, bias=True)
      (1): ReLU()
      (2): Dropout(p=0.5, inplace=False)
      (3): Linear(in_features=512, out_features=512, bias=True)
      (4): ReLU()
      (5): Dropout(p=0.5, inplace=False)
      (6): Linear(in_features=512, out_features=512, bias=True)
      (7): ReLU()
      (8): Dropout(p=0.5, inplace=False)
    )
    (linear): Linear(in_features=512, out_features=1, bias=True)
    (sigmoid): Sigmoid()
  )
  (linear): Linear(in_features=512, out_features=1, bias=True)
  (tanh): Tanh()
  (model): bichrom_chrom(
    (_reshape): _reshape()
    (conv1d): Conv1d(12, 15, kernel_size=(1,), stride=(1,), padding=valid)
    (relu): ReLU()
    (lstm): LSTM(15, 5, batch_first=True)
    (relu2): ReLU()
    (linear): Linear(in_features=5, out_features=1, bias=True)
    (tanh): Tanh()
  )
  (linear2): Linear(in_features=2, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)
0.4426910299003322
0.632890365448505
